{
  "tech": "mongodb",
  "count": 318,
  "examples": [
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 68958221,
      "title": "MongoParseError: options useCreateIndex, useFindAndModify are not supported",
      "problem": "I tried to run it and it said an error like the title. and\nthis is my code:\n```\n`const URI = process.env.MONGODB_URL;\n\nmongoose.connect(URI, {\n   useCreateIndex: true, \n   useFindAndModify: false, \n   useNewUrlParser: true, \n   useUnifiedTopology: true \n}, err => {\n   if(err) throw err;\n   console.log('Connected to MongoDB!!!')\n})\n`\n```\nI set the MONGODB_URL in .env :\n```\n`MONGODB_URL = mongodb+srv://username:@cluster0.accdl.mongodb.net/website?retryWrites=true&w=majority\n`\n```\nHow to fix it?",
      "solution": "From the Mongoose 6.0 docs:\n\nuseNewUrlParser, useUnifiedTopology, useFindAndModify, and useCreateIndex are no longer supported options. Mongoose 6 always behaves as if useNewUrlParser, useUnifiedTopology, and useCreateIndex are true, and useFindAndModify is false. Please remove these options from your code.",
      "question_score": 121,
      "answer_score": 223,
      "created_at": "2021-08-27T21:00:39",
      "url": "https://stackoverflow.com/questions/68958221/mongoparseerror-options-usecreateindex-usefindandmodify-are-not-supported"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 66225004,
      "title": "What is meant by &quot;+srv&quot; in the mongoDb connection string",
      "problem": "I am new to MongoDB and just encountered two types of connection string.\n\nmongodb://[username:password@]host1[:port1][,host2[:port2],...[,hostN[:portN]]][/[database][?options]]\n\nmongodb+srv://[username:password@]host[/[database][?options]]\n\nI know about the 1st one. But unfamiliar with the (+srv) in the 2nd.\n```\n`   let connectionUrl;\n      if (username && password)\n        connectionUrl = `mongodb://${username}:${password}@${host}:${\n          port || 27017\n        }/${databaseName}`;\n      else\n        connectionUrl = `mongodb://${host}:${\n          port || 27017\n        }/${databaseName}`;\n      console.log(connectionUrl, \"connectionUrlconnectionUrl\");\n      let connection = await mongoose.createConnection(connectionUrl, {\n        useNewUrlParser: true,\n      });\n      return connection;\n`\n```\nNow the problem user can enter username, password, hostname, etc...\nBut is there any way to know when to add  (+srv) because I was trying with localhost and with MongoDB atlas.\nAtlas works fine with +srv but in the case of localhost, it's throwing an error.",
      "solution": "in MongoDB 3.6 is introduced the concept of a seed list that is specified using DNS records, specifically SRV and TXT records. You will recall from using replica sets with MongoDB that the client must specify at least one replica set member (and may specify several of them) when connecting. This allows a client to connect to a replica set even if one of the nodes that the client specifies is unavailable\nYou can see an example of this URL on a 2.2.12 or later connection string\n\nNote that without the SRV record configuration we must list several nodes (in the case of Atlas we always include all the cluster members, though this is not required). We also have to specify the ssl and replicaSet options\nWith the 3.4 or earlier driver, we have to specify all the options on the command line using the MongoDB URI syntax.\nThe use of SRV records eliminates the requirement for every client to pass in a complete set of state information for the cluster. Instead, a single SRV record identifies all the nodes associated with the cluster (and their port numbers) and an associated TXT record defines the options for the URI.\n\ncheck the Reference",
      "question_score": 56,
      "answer_score": 31,
      "created_at": "2021-02-16T14:12:48",
      "url": "https://stackoverflow.com/questions/66225004/what-is-meant-by-srv-in-the-mongodb-connection-string"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 74747476,
      "title": "DeprecationWarning: Mongoose: the `strictQuery` option will be switched back to `false` by default in Mongoose 7",
      "problem": "I am just making a database called Fruits from my `app.js` and connecting the database to MongoDB using Mongoose.\n`const mongoose = require(\"mongoose\");\n\nmongoose.connect(\"mongodb://localhost:27017/fruitsDB\", {useNewUrlParser: true});\n\nmongoose.set('strictQuery', false);\n\nconst fruitSchema = new mongoose.Schema({\n    name: String,\n    rating: Number,\n    review: String\n});\n\nconst Fruit = mongoose.model(\"Fruit\", fruitSchema);\n\nconst fruit = new Fruit({\n    name: \"Apple\",\n    rating: 7,\n    review: \"Taste Good\"\n});\n\nfruit.save();\n`\nWhenever I try `node app.js`, I am getting DeprecationWarning. Even though I tried using `mongoose.set('strictQuery', true);`, the same error continues as follows:\n`(node:15848) [MONGOOSE] DeprecationWarning: Mongoose: the `strictQuery` option w\nill be switched back to `false` by default in Mongoose 7. Use `mongoose.set('str\nictQuery', false);` if you want to prepare for this change. Or use `mongoose.set\n('strictQuery', true);` to suppress this warning.\n(Use `node --trace-deprecation ...` to show where the warning was created)\nD:\\Web Development\\FruitsProject\\node_modules\\mongoose\\lib\\drivers\\node-mongodb-\nnative\\collection.js:158\n          const err = new MongooseError(message);\n                      ^\n\nMongooseError: Operation `fruits.insertOne()` buffering timed out after 10000ms\n    at Timeout. (D:\\Web Development\\FruitsProject\\node_modules\\mongoo\nse\\lib\\drivers\\node-mongodb-native\\collection.js:158:23)\n    at listOnTimeout (node:internal/timers:564:17)\n    at process.processTimers (node:internal/timers:507:7)\n\nNode.js v18.12.1\n`\nAnd then the second error also continues fruits.insertOne().\nBecause of this, my MongoDB database is not getting updated.\n`test> show dbs\nadmin    40.00 KiB\nconfig  108.00 KiB\nlocal    40.00 KiB\nshopDB   72.00 KiB\n`\nI just want to fix this error. But I don't know where to fix this error. For the second part of the error, it seems like it is coming from the node_modules itself. How can I fix this error?",
      "solution": "```\n`mongoose.set(\"strictQuery\", false);\n\nmongoose.connect(process.env.MONGO_URL);\n`\n```\nOR\n```\n`mongoose.set(\"strictQuery\", false);\nmongoose.connect(process.env.MONGO_URL, () => {\n  console.log(\"Connected to MongoDB\");\n});\n`\n```\n`const connectDB = async () => {\n    try {\n        mongoose.set('strictQuery', false);\n        await mongoose.connect(db, {\n            useNewUrlParser: true,\n            useUnifiedTopology: true,\n        });\n        console.log('MongoDB Connected...');\n    } catch (err) {\n        console.error(err.message);\n        // make the process fail\n        process.exit(1);\n    }\n`",
      "question_score": 53,
      "answer_score": 91,
      "created_at": "2022-12-09T20:24:21",
      "url": "https://stackoverflow.com/questions/74747476/deprecationwarning-mongoose-the-strictquery-option-will-be-switched-back-to"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 68927857,
      "title": "Mongoexport auth error using mechanism &quot;SCRAM-SHA-1&quot;",
      "problem": "I have taken over undocumented Mongo 4.4.8 cluster (PSA). I am trying to tidy it up and test thouroughly.\nAn original connection string:\n```\n`MONGODB_URI=mongodb://${USER}:${PASS}@10.0.0.3:27017,10.0.0.6:27017,10.0.0.2:27017/bud?replicaSet=bud-replica&authSource=admin\n`\n```\nI have enabled localhost and socket connection. I can log in from cmdline with\n```\n`mongo -u ${USER} -p ${PASS}\nMongoDB shell version v4.4.8\nconnecting to: mongodb://127.0.0.1:27017/?compressors=disabled&gssapiServiceName=mongodb\nImplicit session: session { \"id\" : UUID(\"492e331b-417e-458a-83c7-9db6eaae0869\") }\nMongoDB server version: 4.4.8\n`\n```\nI can switch db to bud and perform the queries. But if I run just\n```\n`mongo\n`\n```\nthen the authentication with the same credentials does not work:\n```\n`bud-replica:PRIMARY> db.auth('admin','admin');\nError: Authentication failed.\n0\n`\n```\nI tried to search for users but shows there arent any:\n```\n`bud-replica:PRIMARY> db.getUsers()\n[ ]\nbud-replica:PRIMARY> use bud\nswitched to db bud\nbud-replica:PRIMARY> db.getUsers()\n[ ]\n`\n```\nThis is `mongod.conf` security part:\n```\n`security:\n   authorization: enabled\n   keyFile: \"/etc/bud-rs\"\n`\n```\nFinally I need to export my data before doing experiments. Though the cmd line interface looks similar, mongoexport cannot fetch the data, regardless I set user/password or skip these arguments.\n```\n`mongoexport -h localhost --db=bud -u ${USER} -p ${PASS} -c=accidents --jsonArray > accidents.json\n2021-08-25T19:30:30.631+0200    could not connect to server: connection() error occured during connection handshake: auth error: sasl conversation error: unable to authenticate using mechanism \"SCRAM-SHA-1\": (AuthenticationFailed) Authentication failed.\nmongoexport -h localhost --db=bud -u ${USER} -p ${PASS} -c=accidents --jsonArray --authenticationDatabase \u201cadmin\u201d > accidents.json\n2021-08-25T19:36:18.738+0200    could not connect to server: connection() error occured during connection handshake: auth error: sasl conversation error: unable to authenticate using mechanism \"SCRAM-SHA-1\": (AuthenticationFailed) Authentication failed.\nroot@10:~# mongoexport -h localhost --db=bud -u ${USER} -p ${PASS} -c=accidents --jsonArray --authenticationDatabase \u201cbud\u201d > accidents.json\n2021-08-25T19:38:21.174+0200    could not connect to server: connection() error occured during connection handshake: auth error: sasl conversation error: unable to authenticate using mechanism \"SCRAM-SHA-1\": (AuthenticationFailed) Authentication failed.\n`\n```\nI am really confused and I failed to find a solution on Google or SO.\nSecond relevant question:\nIf I need to create new user, shall I do it on all replicas or it is automatically synchronized?\n1st update\nThis is the workaround, but my questions are still valid. I want to understand.\n```\n`root@10:~# mongoexport --db=bud -u ${USER} -p ${PASS} -c=accidents --jsonArray \"mongodb://admin:admin@10.0.0.3:27017/bud?authSource=admin\" > accidents.json\n2021-08-25T20:46:54.777+0200    connected to: mongodb://[**REDACTED**]@10.0.0.3:27017/bud?authSource=admin\n2021-08-25T20:46:55.778+0200    [........................]  bud.accidents  0/4379  (0.0%)\n2021-08-25T20:46:56.497+0200    [########################]  bud.accidents  4379/4379  (100.0%)\n2021-08-25T20:46:56.497+0200    exported 4379 records\n`\n```\n2nd update\n```\n`bud-replica:PRIMARY> use admin\nbud-replica:PRIMARY> show collections\nsystem.keys\nsystem.users\nsystem.version\nbud-replica:PRIMARY> db.system.users.find()\n{ \"_id\" : \"admin.admin\", \"userId\" : UUID(\"769e4f5c-6f46-4153-857e-47d7d8730066\"), \"user\" : \"admin\", \"db\" : \"admin\", \"credentials\" : { \"SCRAM-SHA-1\" : { \"iterationCount\" : 10000, \"salt\" : \"32/AP4019eome36j8n\n`\n```",
      "solution": "The user credential was created in the admin database.\nWhen connecting with the mongo shell, switch with `use admin` before running `db.auth`\nThe mongoexport command that worked used `authSource=admin` in the connection string.\nAdd `--authenticationDatabase=admin` to the other command line to direct it to use the admin database for auth as well.",
      "question_score": 40,
      "answer_score": 128,
      "created_at": "2021-08-25T20:02:54",
      "url": "https://stackoverflow.com/questions/68927857/mongoexport-auth-error-using-mechanism-scram-sha-1"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 72549668,
      "title": "How to do custom repository using TypeORM (MongoDB) in NestJS?",
      "problem": "I have a question. With `@EntityRepository` decorator being marked as deprecated in `typeorm@^0.3.6`, what is now the recommended or TypeScript-friendly way to create a custom repository for an entity in NestJS? A custom repository before would look like this:\n```\n`// users.repository.ts\nimport { EntityRepository, Repository } from 'typeorm';\nimport { User } from './user.entity';\n\n@EntityRepository(User)\nexport class UsersRepository extends Repository {\n  async createUser(firstName: string, lastName: string): Promise {\n    const user = this.create({\n      firstName,\n      lastName,\n    });\n\n    await this.save(user);\n\n    return user;\n  }\n}\n`\n```\nAnd since NestJS is by default configured with TypeScript support, I will be able to call `usersRepository.createUser()` without an issue in a service like this:\n```\n`// users.service.ts\nimport { Injectable } from '@nestjs/common';\nimport { InjectRepository } from '@nestjs/typeorm';\nimport { User } from './user.entity';\nimport { UsersRepository } from './users.repository';\n\n@Injectable()\nexport class UsersService {\n  constructor(\n    @InjectRepository(UsersRepository)\n    private readonly usersRepository: UsersRepository,\n  ) {}\n\n  async createUser(firstName: string, lastName: string): Promise {\n    return this.usersRepository.createUser(firstName, lastName);\n  }\n}\n`\n```\nThis is how the modules would import the custom repository:\n```\n`// users.module.ts\nimport { Module } from '@nestjs/common';\nimport { TypeOrmModule } from '@nestjs/typeorm';\nimport { UsersController } from './users.controller';\nimport { UsersRepository } from './users.repository';\nimport { UsersService } from './users.service';\n\n@Module({\n  imports: [TypeOrmModule.forFeature([UsersRepository])],\n  controllers: [UsersController],\n  providers: [UsersService],\n  exports: [UsersService],\n})\nexport class UsersModule {}\n`\n```\nAlso the reason why I mentioned MongoDB here is because I tried using `typeorm@0.2` where `@EntityRepository` is still supported but I receive an error when I tried to import it in the module stating `Repository not found` or something. Do note, if I chose `postgresql` as my database in TypeORM with the same changes above, I don't have this issue. Hence I went to check the latest only to find out it is already deprecated, I also didn't find any example in NestJS documentation.",
      "solution": "I think I found a solution to this which allows to call custom methods but also inherited ones. It seems that this \"issue\" is not too popular yet but there is definitely some chatter about it within the typeorm GitHub threads: https://github.com/typeorm/typeorm/issues/9013\nThe following solution uses MySQL as underlying database driver but I assume it will work for MongoDB as well.\nteam.repository.ts\n```\n`import {DataSource, Repository} from 'typeorm';\nimport {Injectable} from '@nestjs/common';\nimport {Team} from '@Domain/Team/Models/team.entity';\n\n@Injectable()\nexport class TeamRepository extends Repository\n{\n    constructor(private dataSource: DataSource)\n    {\n        super(Team, dataSource.createEntityManager());\n    }\n\n    /**\n     * Add a basic where clause to the query and return the first result.\n     */\n    async firstWhere(column: string, value: string | number, operator = '='): Promise\n    {\n        return await this.createQueryBuilder()\n                         .where(`Team.${column} ${operator} :value`, {value: value})\n                         .getOne();\n    }\n}\n\n`\n```\nteam.service.ts\n```\n`import {Injectable} from '@nestjs/common';\nimport {Team} from '@Domain/Team/Models/team.entity';\nimport {TeamRepository} from '@Domain/Team/Repositories/team.repository';\n\n@Injectable()\nexport class TeamService\n{\n    constructor(\n        private teamRepository: TeamRepository,\n    )\n    {\n    }\n\n    async create(): Promise\n    {\n        const team: Team = await this.teamRepository.firstWhere('id', 1);\n\n        return this.teamRepository.save(team);\n    }\n}\n`\n```\nteam.module.ts\n```\n`import {Module} from '@nestjs/common';\nimport {TeamService} from '@Domain/Team/Services/team.service';\nimport {TypeOrmModule} from '@nestjs/typeorm';\nimport {Team} from '@Domain/Team/Models/team.entity';\nimport {TeamRepository} from '@Domain/Team/Repositories/team.repository';\n\n@Module({\n            imports:   [TypeOrmModule.forFeature([Team])],\n            exports:   [TeamService],\n            providers: [TeamService, TeamRepository],\n        })\nexport class TeamModule\n{\n}\n`\n```",
      "question_score": 31,
      "answer_score": 68,
      "created_at": "2022-06-08T19:07:29",
      "url": "https://stackoverflow.com/questions/72549668/how-to-do-custom-repository-using-typeorm-mongodb-in-nestjs"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 70185942,
      "title": "Why I am getting &quot;NotImplementedError: Database objects do not implement truth value testing or bool().&quot; while running makemigration command in Django",
      "problem": "I am trying to connect Django with MongoDB using Djongo. I have changed the Database parameter but I am getting this error:\n\nNotImplementedError: Database objects do not implement truth value testing or bool().\n\nwhen I am running the `makemigration` command.\nPlease can anybody explain why I am getting this error and how to resolve it?\nI have included my settings.py file, the error log, and a screenshot of the MongoDB Compass setup.\nsettings.py\n```\n`\"\"\"\nDjango settings for Chatify project.\n\nGenerated by 'django-admin startproject' using Django 3.2.9.\n\nFor more information on this file, see\nhttps://docs.djangoproject.com/en/3.2/topics/settings/\n\nFor the full list of settings and their values, see\nhttps://docs.djangoproject.com/en/3.2/ref/settings/\n\"\"\"\n\nfrom pathlib import Path\n\n# Build paths inside the project like this: BASE_DIR / 'subdir'.\nBASE_DIR = Path(__file__).resolve().parent.parent\n\n# Quick-start development settings - unsuitable for production\n# See https://docs.djangoproject.com/en/3.2/howto/deployment/checklist/\n\n# SECURITY WARNING: keep the secret key used in production secret!\nSECRET_KEY = 'django-insecure-1k4mo05el_0112guspx^004n-i&3h#u4gyev#27u)tkb8t82_%'\n\n# SECURITY WARNING: don't run with debug turned on in production!\nDEBUG = True\n\nALLOWED_HOSTS = []\n\n# Application definition\n\nINSTALLED_APPS = [\n    'django.contrib.admin',\n    'django.contrib.auth',\n    'django.contrib.contenttypes',\n    'django.contrib.sessions',\n    'django.contrib.messages',\n    'django.contrib.staticfiles',\n    'api.apps.ApiConfig',\n]\n\nMIDDLEWARE = [\n    'django.middleware.security.SecurityMiddleware',\n    'django.contrib.sessions.middleware.SessionMiddleware',\n    'django.middleware.common.CommonMiddleware',\n    'django.middleware.csrf.CsrfViewMiddleware',\n    'django.contrib.auth.middleware.AuthenticationMiddleware',\n    'django.contrib.messages.middleware.MessageMiddleware',\n    'django.middleware.clickjacking.XFrameOptionsMiddleware',\n]\n\nROOT_URLCONF = 'Chatify.urls'\n\nTEMPLATES = [\n    {\n        'BACKEND': 'django.template.backends.django.DjangoTemplates',\n        'DIRS': [],\n        'APP_DIRS': True,\n        'OPTIONS': {\n            'context_processors': [\n                'django.template.context_processors.debug',\n                'django.template.context_processors.request',\n                'django.contrib.auth.context_processors.auth',\n                'django.contrib.messages.context_processors.messages',\n            ],\n        },\n    },\n]\n\nWSGI_APPLICATION = 'Chatify.wsgi.application'\n\n# Database\n# https://docs.djangoproject.com/en/3.2/ref/settings/#databases\n\n# DATABASES = {\n#     'default': {\n#         'ENGINE': 'django.db.backends.sqlite3',\n#         'NAME': BASE_DIR / 'db.sqlite3',\n#     }\n# }\nDATABASES = {\n       'default': {\n           'ENGINE': 'djongo',\n           'NAME': 'users',\n       }\n}\n\n# Password validation\n# https://docs.djangoproject.com/en/3.2/ref/settings/#auth-password-validators\n\nAUTH_PASSWORD_VALIDATORS = [\n    {\n        'NAME': 'django.contrib.auth.password_validation.UserAttributeSimilarityValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.MinimumLengthValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.CommonPasswordValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.NumericPasswordValidator',\n    },\n]\n\n# Internationalization\n# https://docs.djangoproject.com/en/3.2/topics/i18n/\n\nLANGUAGE_CODE = 'en-us'\n\nTIME_ZONE = 'UTC'\n\nUSE_I18N = True\n\nUSE_L10N = True\n\nUSE_TZ = True\n\n# Static files (CSS, JavaScript, Images)\n# https://docs.djangoproject.com/en/3.2/howto/static-files/\n\nSTATIC_URL = '/static/'\n\n# Default primary key field type\n# https://docs.djangoproject.com/en/3.2/ref/settings/#default-auto-field\n\nDEFAULT_AUTO_FIELD = 'django.db.models.BigAutoField'\n`\n```\nError Log\n`File \"manage.py\", line 22, in \n    main()\n  File \"manage.py\", line 18, in main\n    execute_from_command_line(sys.argv)\n  File \"C:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\django\\core\\management\\__init__.py\", line 419, in execute_from_command_line\n    utility.execute()\n  File \"C:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\django\\core\\management\\__init__.py\", line 413, in execute\n    self.fetch_command(subcommand).run_from_argv(self.argv)\n  File \"C:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\django\\core\\management\\base.py\", line 367, in run_from_argv\n    connections.close_all()\n  File \"C:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\django\\db\\utils.py\", line 213, in close_all\n    connection.close()\n  File \"C:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\django\\utils\\asyncio.py\", line 33, in inner\n    return func(*args, **kwargs)\n  File \"C:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\django\\db\\backends\\base\\base.py\", line 294, in close\n    self._close()\n  File \"C:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\djongo\\base.py\", line 208, in _close\n    if self.connection:\n  File \"C:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pymongo\\database.py\", line 829, in __bool__\n    raise NotImplementedError(\"Database objects do not implement truth \"\nNotImplementedError: Database objects do not implement truth value testing or bool(). Please compare with None instead: database is not None\n`\nMongoDB Compass",
      "solution": "The problem is with the new version of pymongo (4.0 from 29.11.2021) which is not supported by Djongo 1.3.6.\nYou need to install pymongo 3.12.1.",
      "question_score": 28,
      "answer_score": 87,
      "created_at": "2021-12-01T15:24:18",
      "url": "https://stackoverflow.com/questions/70185942/why-i-am-getting-notimplementederror-database-objects-do-not-implement-truth-v"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 68123923,
      "title": "PyMongo [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate",
      "problem": "I'm using Python 3.9.5 and PyMongo 3.11.4. The version of my MongoDB database is 4.4.6. I'm using Windows 8.1\nI'm learning MongoDB and I have a cluster set up in Atlas that I connect to. Whenever I try to insert a document into a collection, a `ServerSelectionTimeoutError` is raised, and inside its parentheses there are several `[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate`.\nTroubleshooting TLS Errors in the PyMongo docs weren't too much help as they only provided tips for Linux and macOS users.\nIt's worth mentioning that if I set `tlsAllowInvalidCertificates=True` when initializing my `MongoClient`, everything works fine. That sounds insecure, and while I am working on a small project, I would still like to develop good habits and not override any security measures in place, so I'm hoping there is an alternative to that.\nFrom all the searching I've done, I'm guessing that I'm missing certain certificates, or that Python can't find them. I've looked into the `certifi` package, but this part of the docs makes it seem that should only be necessary if I'm using Python 2.x, which I'm not.\nSo yeah, I'm kind of stuck right now.",
      "solution": "Well, I eventually decided to install `certifi` and it worked.\n`client = MongoClient(CONNECTION_STRING, tlsCAFile=certifi.where())`\nWish the docs were a bit clearer on this, but maybe I just didn't look hard enough.",
      "question_score": 26,
      "answer_score": 56,
      "created_at": "2021-06-25T01:59:42",
      "url": "https://stackoverflow.com/questions/68123923/pymongo-ssl-certificate-verify-failed-certificate-verify-failed-unable-to-ge"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 75586474,
      "title": "Mongoose stopped accepting callbacks for some of its functions",
      "problem": "I've been using callbacks for .save() and .findOne() for a few days now and just today I encounter these errors:\n```\n`throw new MongooseError('Model.prototype.save() no longer accepts a callback')\n\nMongooseError: Model.prototype.save() no longer accepts a callback\n`\n```\nand\n`MongooseError: Model.findOne() no longer accepts a callback`\nIt's really awkward given that callbacks are still accepted in the docs at least for .findOne().\n```\n`app.post(\"/register\", (req, res) => {\n    const newUser = new User({\n        email: req.body.username,\n        password: req.body.password\n    });\n\n    newUser.save((err) => {\n        if (err) console.log(err)\n        else res.render(\"secrets\");\n    });\n});\n`\n```\nThis is what used to work for me, using express and mongoose. Please let me know how to fix it.",
      "solution": "MongooseError: Model.find() no longer accepts a callback\n\nSince the callback function has been deprecated from now onwards.\nIf you are using these functions with callbacks, use async/await or promises if async functions don't work for you.\n```\n`app.get(\"/articles\", async (req, res) => {\n  try {\n    const articles = await Article.find({ });\n    res.send(articles);\n    console.log(articles);\n  } catch (err) {\n    console.log(err);\n  }\n});\n`\n```",
      "question_score": 25,
      "answer_score": 34,
      "created_at": "2023-02-28T00:18:28",
      "url": "https://stackoverflow.com/questions/75586474/mongoose-stopped-accepting-callbacks-for-some-of-its-functions"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 70818543,
      "title": "Mongo DB deployment not working in kubernetes because processor doesn&#39;t have AVX support",
      "problem": "I am trying to deploy a `mongo db` deployment together with service, as follows:\n```\n`apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mongo-deployment\n  labels:\n    app: mongo\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: mongo\n  template:\n    metadata:\n      labels:\n        app: mongo\n    spec:\n      containers:\n      - name: mongo\n        image: mongo:5.0\n        ports:\n        - containerPort: 27017\n        env:\n        - name: MONGO_INITDB_ROOT_USERNAME\n          valueFrom:\n            secretKeyRef: \n              name: mongo-secret\n              key: mongo-user\n        - name: MONGO_INITDB_ROOT_PASSWORD\n          valueFrom:\n            secretKeyRef: \n              name: mongo-secret\n              key: mongo-password\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: mongo-service\nspec:\n  selector:\n    app: mongo\n  ports:\n    - protocol: TCP\n      port: 27017\n      targetPort: 27017\n`\n```\nEven though everything seems to be configured right and deployed, it gets to a `CrashLoopBackOff` state instead of `Running`, using a `kubectl logs ` I get the following error:\n```\n`MongoDB 5.0+ requires a CPU with AVX support, and your current system does not appear to have that!\n`\n```\nDoes anybody know what to do?",
      "solution": "To solve this issue I had to run an older `mongo-db` docker image version (4.4.6), as follows:\n```\n`image: mongo:4.4.6\n`\n```\nReference:\nMongo 5.0.0 crashes but 4.4.6 works #485",
      "question_score": 25,
      "answer_score": 42,
      "created_at": "2022-01-23T02:43:44",
      "url": "https://stackoverflow.com/questions/70818543/mongo-db-deployment-not-working-in-kubernetes-because-processor-doesnt-have-avx"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 66049860,
      "title": "Cannot connect to MongoDB because of wrong URI",
      "problem": "I was trying to run mongoDB on node server\nFull Code here from MongoDB:\n\nMy mongo version: 4.4.3\nNode version: v15.7.0\n\nI've imported get started code from MongoDB, and here's the code:\n```\n`const { MongoClient } = require(\"mongodb\");\n// Connection URI\nconst uri =\n  \"mongodb+srv://sample-hostname:27017/?poolSize=20&writeConcern=majority\";\n// Create a new MongoClient\nconst client = new MongoClient(uri);\nasync function run() {\n  try {\n    // Connect the client to the server\n    await client.connect();\n    // Establish and verify connection\n    await client.db(\"admin\").command({ ping: 1 });\n    console.log(\"Connected successfully to server\");\n  } finally {\n    // Ensures that the client will close when you finish/error\n    await client.close();\n  }\n}\nrun().catch(console.dir);\n`\n```\nOn terminal, when i run \"node app.js\", it throws me following error:\n```\n`> (node:79653) Warning: Accessing non-existent property 'MongoError' of\n> module exports inside circular dependency (Use `node --trace-warnings\n> ...` to show where the warning was created) MongoParseError: URI does\n> not have hostname, domain name and tld\n>     at parseSrvConnectionString (/home/harmony/Desktop/FruitsProject/node_modules/mongodb/lib/core/uri_parser.js:50:21)\n>     at parseConnectionString (/home/harmony/Desktop/FruitsProject/node_modules/mongodb/lib/core/uri_parser.js:594:12)\n>     at connect (/home/harmony/Desktop/FruitsProject/node_modules/mongodb/lib/operations/connect.js:284:3)\n>     at /home/harmony/Desktop/FruitsProject/node_modules/mongodb/lib/mongo_client.js:225:5\n>     at maybePromise (/home/harmony/Desktop/FruitsProject/node_modules/mongodb/lib/utils.js:681:3)\n>     at MongoClient.connect (/home/harmony/Desktop/FruitsProject/node_modules/mongodb/lib/mongo_client.js:221:10)\n>     at run (/home/harmony/Desktop/FruitsProject/app.js:12:18)\n>     at Object. (/home/harmony/Desktop/FruitsProject/app.js:21:1)\n`\n```",
      "solution": "The error `Accessing non-existent property 'MongoError' of > module exports inside circular dependency` is caused by a bug in mongodb 3.6.4\nIt's already reported here\nBack to version 3.6.3 works for me:\n```\n`npm uninstall mongodb --save\n`\n```\nInstall version 3.6.3\n```\n`npm i mongodb@3.6.3\n`\n```",
      "question_score": 25,
      "answer_score": 40,
      "created_at": "2021-02-04T17:34:45",
      "url": "https://stackoverflow.com/questions/66049860/cannot-connect-to-mongodb-because-of-wrong-uri"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 69797985,
      "title": "All charts apps need to be terminated before the project can be deleted",
      "problem": "I wanted to delete the organization in my MongoDB account but when I try to do that, MongoDB forces me to delete all the projects first. I deleted all my projects except one. When I try to delete this project, a pop up appears like this\n\nBut even if I terminate the cluster and delete the charts, I'm unable to get rid of this pop which is preventing me from deleting the project.\nYou can see I don't have any charts here\n\nNor do I have any clusters",
      "solution": "I got the same issue I deleted the project using these simple step..\n\nIn project dashboard click on three dot icon which is left of delete icon.\nClick on \"Visit Project Settings\"\nScroll down to the bottom\nYou will see an option \"Delete Charts\" click on red Delete button\nCome back to project dashboard and now you can delete it without any error",
      "question_score": 23,
      "answer_score": 87,
      "created_at": "2021-11-01T14:58:33",
      "url": "https://stackoverflow.com/questions/69797985/all-charts-apps-need-to-be-terminated-before-the-project-can-be-deleted"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 65680842,
      "title": "&quot;Error: MongooseError: Operation `users.insertOne()` buffering timed out after 10000ms&quot;,",
      "problem": "I'm running MongoDB Atlas on node express and I got this error when I tested with postman.\n```\n`const express = require('express');\nconst cors = require('cors');\nconst mongoose = require('mongoose');\n\nrequire('dotenv').config();\n\nconst app = express();\nconst port = process.env.PORT || 5000;\n\napp.use(cors());\napp.use(express.json());\n\nconst uri = process.env.ATLAS_URI;\nmongoose.connect(uri, { useNewUrlParser: true, useCreateIndex: true }\n);\nconst connection = mongoose.connection;\nconnection.once('open', () => {\n  console.log(\"MongoDB database connection established successfully\");\n})\n\nconst exercisesRouter = require('./routes/exercises');\nconst usersRouter = require('./routes/users');\n\napp.use('/exercises', exercisesRouter);\napp.use('/users', usersRouter);\n\napp.listen(port, () => {\n    console.log(`Server is running on port: ${port}`);\n});\n`\n```\nThis is my `.env`, I'm guessing the problem might be here too, Kindly help:\n```\n`ATLAS_URI=mongodb+srv://userone:useronepassword1234@cluster0.swye5.mongodb.net/?retryWrites=true&w=majority\n`\n```",
      "solution": "In my case, I had to go to Atlas, and reset my whitelisted IP to the correct address.\nThen I restarted my local server and tried posting again on postman... And it worked!",
      "question_score": 23,
      "answer_score": 17,
      "created_at": "2021-01-12T10:06:06",
      "url": "https://stackoverflow.com/questions/65680842/error-mongooseerror-operation-users-insertone-buffering-timed-out-after-1"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 68456504,
      "title": "Mongo driver with @Testcontainers, throws exceptions after tests run successfully",
      "problem": "I'm setting up integration tests in a sample spring boot kotlin project, using testcontainers:mongodb.\nI've set up a MongoDBContaine, and everything works as expected - the app connects to the mongodb and tests with repositories (e.g. save, delete) work perfectly fine, but I've noticed that after the tests ran (successfully) mongodb.driver throws an exception in the end - would seem like the container isn't being gracefully closed/stopped - is that possible?\nExample of how I start the container\n```\n`companion object {\n        @Container\n        var mongoDBContainer = MongoDBContainer(\"mongo:4.4.2\")\n\n        @JvmStatic\n        @DynamicPropertySource\n        fun setProperties(registry: DynamicPropertyRegistry) {\n            registry.add(\"spring.data.mongodb.uri\") { mongoDBContainer.replicaSetUrl }\n        }\n    }\n`\n```\nTest method\n```\n`@Test fun someTest() {\n   autowiredRepository.save(document)\n   ...\n   ...\n}\n`\n```\nAs for the running class, I'm simply using the @Testcontainers annotation on top of it with @SpringBootTest, nothing else.\nThe exception I'm getting in the end is\n```\n`com.mongodb.MongoSocketReadException: Prematurely reached end of stream\n    at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:112) ~[mongodb-driver-core-4.2.3.jar:na]\n    at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:131) ~[mongodb-driver-core-4.2.3.jar:na]\n    at com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:647) ~[mongodb-driver-core-4.2.3.jar:na]\n    at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:512) ~[mongodb-driver-core-4.2.3.jar:na]\n    at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:355) ~[mongodb-driver-core-4.2.3.jar:na]\n    at com.mongodb.internal.connection.InternalStreamConnection.receive(InternalStreamConnection.java:315) ~[mongodb-driver-core-4.2.3.jar:na]\n    at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:215) ~[mongodb-driver-core-4.2.3.jar:na]\n    at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:144) ~[mongodb-driver-core-4.2.3.jar:na]\n    at java.base/java.lang.Thread.run(Thread.java:832) ~[na:na]\n\n`\n```",
      "solution": "I was facing the same issue and solved it by removing the annotation `@Container` from my container and starting it manually as a Singleton.\nThe reason is that `@Container` was starting multiple containers. The first one was being successfully used by the tests and the other ones were throwing the `com.mongodb.MongoSocketReadException: Prematurely reached end of stream` exception.\nReplacing this:\n`\n    @Testcontainers  \n    @DataMongoTest(excludeAutoConfiguration = EmbeddedMongoAutoConfiguration.class)  \n    class UserRepositoryTest {  \n    \n        @Container  \n        static MongoDBContainer mongoDBContainer = new MongoDBContainer(\"mongo:5.0.10\");  \n    \n        @DynamicPropertySource  \n        static void setProperties(DynamicPropertyRegistry registry) {  \n            registry.add(\"spring.data.mongodb.uri\", mongoDBContainer::getReplicaSetUrl);  \n        }\n    }\n\n`\nwith this\n`\n    @Testcontainers  \n    @DataMongoTest(excludeAutoConfiguration = EmbeddedMongoAutoConfiguration.class)  \n    class UserRepositoryTest {  \n    \n        static MongoDBContainer mongoDBContainer = new MongoDBContainer(\"mongo:5.0.13\");\n    \n        static {\n            mongoDBContainer.start();\n        }\n    \n        @DynamicPropertySource  \n        static void setProperties(DynamicPropertyRegistry registry) {  \n            registry.add(\"spring.data.mongodb.uri\", mongoDBContainer::getReplicaSetUrl);  \n        }\n    }\n\n`",
      "question_score": 21,
      "answer_score": 15,
      "created_at": "2021-07-20T16:19:00",
      "url": "https://stackoverflow.com/questions/68456504/mongo-driver-with-testcontainers-throws-exceptions-after-tests-run-successfull"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 66179884,
      "title": "How can I remove this deprecation warning in MongoDB and why is it happening?",
      "problem": "I am just trying to create an API and connect to it inside my app.js file but every time I run app.js I am getting this deprecation warning. I have checked all of the deprecation warnings in mongoose and MongoDB but I cannnot find any that match mine. The warning that I am getting is,\n`(node:16864) Warning: Accessing non-existent property 'MongoError' of module exports inside circular dependency`\n`(Use node --trace-warnings ... to show where the warning was created)`\n`(node:16864) DeprecationWarning: Listening to events on the Db class has been deprecated and will be removed in the next major version.`\nWhy am I getting this deprecation warning and how can I remove it?",
      "solution": "I guess this warning occurs with mongoose 5.11.16 version. If you want to avoid seeing them until the bug gets fixed, instead you can go for mongoose version 5.11.15. Uninstall mongoose 5.11.16 and install npm install mongoose@5.11.15",
      "question_score": 21,
      "answer_score": 12,
      "created_at": "2021-02-12T23:31:12",
      "url": "https://stackoverflow.com/questions/66179884/how-can-i-remove-this-deprecation-warning-in-mongodb-and-why-is-it-happening"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 67498836,
      "title": "Docker - chown: changing ownership of &#39;/data/db&#39;: Operation not permitted",
      "problem": "I am trying to run my application using Docker and here is my yml file content to run the mongo container.\n```\n` services:\n   mongodb:\n    image: mongo:3.4\n    #    ports:\n    #        - \"27017:27017\"\n    volumes:\n      - ./data/mongo:/data/db\n    restart: always\n`\n```\nAnd getting this error in contianer: (Saw this error after running docker logs command)\n\nchown: changing ownership of '/data/db': Operation not permitted\n\nThe host has ./data/mongo folder and here are the details.\n```\n`drwxrwxrwx  2 nfsnobody nfsnobody 4096 May 11 23:13 mongo\n`\n```\nI tried to run this on the host as suggested in one of the forums.\n```\n`sudo chgrp 1000 ./data/mongo\n`\n```\nNot sure how this would help to solve the issue because the error we get is insdide the container folder not the one from host, anyway i tried..\nBut got this response :\n\nchgrp: changing group of \u2018mongo\u2019: Operation not permitted\n\nHow to solve this issue? is there any solution other than \"chgrp\"? Thank you.\nHere is the full docker-compose.yml file\n```\n`## You can generate a custom docker compose file automatically on http://reportportal.io/download (Step 2)\n\n## This is example of Docker Compose for ReportPortal\n## Do not forget to configure data volumes for production usage\n\n## Execute 'docker-compose -p reportportal up -d --force-recreate'\n## to start all containers in daemon mode\n## Where:\n##      '-p reportportal' -- specifies container's prefix (project name)\n##      '-d' -- enables daemon mode\n##      '--force-recreate' -- forces re-recreating of all containers\n\nversion: '2'\n\nservices:\n\n  mongodb:\n    image: mongo:3.4\n    #    ports:\n    #        - \"27017:27017\"\n    volumes:\n      - ./data/mongo:/data/db\n    restart: always\n\n  registry:\n    image: consul:1.0.6\n    volumes:\n      - ./data/consul:/usr/share/consul/data\n#    ports:\n#      - \"8500:8500\"\n#      - \"8300:8300\"\n#      - \"53:8600/udp\"\n    command: \"agent -server -bootstrap-expect=1 -ui -client 0.0.0.0\"\n    environment:\n      - 'CONSUL_LOCAL_CONFIG={\"leave_on_terminate\": true}'\n    restart: always\n\n  uat:\n    image: reportportal/service-authorization:4.2.0\n    #ports:\n    #  - \"9999:9999\"\n    depends_on:\n      - mongodb\n    environment:\n      - RP_PROFILES=docker\n      - RP_SESSION_LIVE=86400 #in seconds\n    #      - RP_MONGO_URI=mongodb://localhost:27017\n    restart: always\n\n  ### Another option for gateway\n  ### Can be used instead of traefik\n  #  gateway:\n  #    image: fabiolb/fabio:1.5.8-go1.10\n  #    ports:\n  #      - \"9998:9998\" # GUI/management\n  #      - \"8080:9999\" # HTTP exposed\n  #    environment:\n  #      - FABIO_REGISTRY_CONSUL_ADDR=registry:8500\n  #      - FABIO_REGISTRY_CONSUL_REGISTER_NAME=gateway\n  #      - FABIO_PROXY_ADDR=:9999;rt=300s;wt=300s\n  #    restart: always\n\n  gateway:\n    image: traefik:1.6.6\n    ports:\n      - \"4444:8080\" # HTTP exposed\n      - \"8081:8081\" # HTTP Administration exposed\n#    expose:\n#      - '8080'\n    command:\n      - --consulcatalog.endpoint=registry:8500\n      - --defaultEntryPoints=http\n      - --entryPoints=Name:http Address::8080\n      - --web\n      - --web.address=:8081\n    restart: always\n\n  index:\n    image: reportportal/service-index:4.2.0\n    environment:\n      - RP_SERVER_PORT=8080\n      - RP_PROXY_CONSUL=true\n    depends_on:\n      - registry\n      - gateway\n    restart: always\n\n  api:\n    image: reportportal/service-api:4.3.0\n    depends_on:\n      - mongodb\n    environment:\n      - RP_PROFILES=docker\n      - JAVA_OPTS=-Xmx1g -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp\n    #      - RP_MONGO_URI=mongodb://localhost:27017\n    restart: always\n\n  ui:\n    image: reportportal/service-ui:4.3.0\n    environment:\n      - RP_SERVER.PORT=8080\n      - RP_CONSUL.TAGS=urlprefix-/ui opts strip=/ui\n      - RP_CONSUL.ADDRESS=registry:8500\n    restart: always\n\n  analyzer:\n    image: reportportal/service-analyzer:4.3.0\n    depends_on:\n      - registry\n      - gateway\n      - elasticsearch\n    restart: always\n\n  elasticsearch:\n    image: docker.elastic.co/elasticsearch/elasticsearch-oss:6.1.1\n    restart: always\n    volumes:\n      - ./data/elasticsearch:/usr/share/elasticsearch/data\n    environment:\n      - bootstrap.memory_lock=true\n    ulimits:\n      memlock:\n        soft: -1\n        hard: -1\n      nofile:\n        soft: 65536\n        hard: 65536\n  #    ports:\n  #        - \"9200:9200\"\n\n  jira:\n    image: reportportal/service-jira:4.0.0\n    environment:\n      - RP_PROFILES=docker\n    #     - RP_MONGO_URI=mongodb://localhost:27017\n    restart: always\n\n  rally:\n    image: reportportal/service-rally:4.3.0\n    environment:\n      - RP_PROFILES=docker\n    #     - RP_MONGO_URI=mongodb://localhost:27017\n    restart: always\n`\n```",
      "solution": "Mongo startup script changes ownership on files in `/data/configdb` and `/data/db` if ran as root. Try running it as `nfsnobody` (the owner of local `./data/mongo`) to skip this step:\n`services:\n  mongodb:\n    user: \"nfsnobody\" # insert either uid or name of the user\n`",
      "question_score": 19,
      "answer_score": 11,
      "created_at": "2021-05-12T09:12:17",
      "url": "https://stackoverflow.com/questions/67498836/docker-chown-changing-ownership-of-data-db-operation-not-permitted"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 73656873,
      "title": "Unable to install mongodb in ubuntu 22.04 (mongodb-org, libssl1.1)",
      "problem": "On following installation instructions from MongoDB official site.\n\nWhile installing mongodb-org package getting the following error\n\n```\n`sudo apt install -y mongodb-org\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nSome packages could not be installed. This may mean that you have\nrequested an impossible situation or if you are using the unstable\ndistribution that some required packages have not yet been created\nor been moved out of Incoming.\nThe following information may help to resolve the situation:\n\nThe following packages have unmet dependencies:\n mongodb-org-mongos : Depends: libssl1.1 (>= 1.1.1) but it is not installable\n mongodb-org-server : Depends: libssl1.1 (>= 1.1.1) but it is not installable\n mongodb-org-shell : Depends: libssl1.1 (>= 1.1.1) but it is not installable\nE: Unable to correct problems, you have held broken packages.\n`\n```\n\nI tried installing libssl1.1 package using apt but I was unable to do so it throws following error.\n\n```\n`E: Package 'libssl1.1' has no installation candidate\n`\n```\nplease someone help.",
      "solution": "It seems mongodb require specific version of libssl1.1\n\nyou can download debian file of that version and install using following commands. Most probably it will resolve this issue.\n\n```\n`sudo wget http://archive.ubuntu.com/ubuntu/pool/main/o/openssl/libssl1.1_1.1.1f-1ubuntu2_amd64.deb\nsudo dpkg -i libssl1.1_1.1.1f-1ubuntu2_amd64.deb\n`\n```",
      "question_score": 18,
      "answer_score": 68,
      "created_at": "2022-09-09T04:54:33",
      "url": "https://stackoverflow.com/questions/73656873/unable-to-install-mongodb-in-ubuntu-22-04-mongodb-org-libssl1-1"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 68917683,
      "title": "mongoose.connect(uri, ConnectOptions) does not recognize useNewUrlParser and other options",
      "problem": "My GitHub repo: https://github.com/safiullah7/legan\nBranch: redux\nI'm following this tutorial: https://tomanagle.medium.com/build-a-rest-api-with-node-js-typescript-mongodb-b6c898d70d61\nand I'm unable to connect to my mongodb.\nHere's my code file where I'm trying to connect with the mongodb:\n```\n`import config from \"config\";\nimport log from \"../logger\";\n\nfunction connect() {\n  const dbUri = config.get(\"dbUri\") as string;\n\n  return mongoose\n    .connect(dbUri, {\n      useNewUrlParser: true,\n      useUnifiedTopology: true,\n    })\n    .then(() => {\n      log.info(\"Database connected\");\n    })\n    .catch((error) => {\n      log.error(\"db error\", error);\n      process.exit(1);\n    });\n}\n\nexport default connect;\n`\n```\nCompiler gives the following error:\n```\n`No overload matches this call.\n  Overload 1 of 3, '(uri: string, callback: CallbackWithoutResult): void', gave the following error.\n    Argument of type '{ useNewUrlParser: boolean; useUnifiedTopology: boolean; }' is not assignable to parameter of type 'CallbackWithoutResult'.\n      Object literal may only specify known properties, and 'useNewUrlParser' does not exist in type 'CallbackWithoutResult'.\n  Overload 2 of 3, '(uri: string, options?: ConnectOptions | undefined): Promise', gave the following error.\n    Argument of type '{ useNewUrlParser: boolean; useUnifiedTopology: boolean; }' is not assignable to parameter of type 'ConnectOptions'.\n      Object literal may only specify known properties, and 'useNewUrlParser' does not exist in type 'ConnectOptions'.\n`\n```\nI'm new to typescript and node/mongoos. Would appreciate your help.",
      "solution": "The new version of `mongoose` (the latest version when I wrote this post is `6.0.2`) has the following type definitions for the `connect()` function.\n```\n`/** Opens Mongoose's default connection to MongoDB, see [connections docs](https://mongoosejs.com/docs/connections.html) */\nexport function connect(uri: string, options: ConnectOptions, callback: CallbackWithoutResult): void;\nexport function connect(uri: string, callback: CallbackWithoutResult): void;\nexport function connect(uri: string, options?: ConnectOptions): Promise;\n`\n```\nThe `options object` you're passing to the `connect()` function\n```\n`{\n  useNewUrlParser: true,\n  useUnifiedTopology: true,\n}\n`\n```\nis, therefore, should have a type of `ConnectOptions`.\nCurrent definition of the `ConnectOptions` is as follows:\n```\n`interface ConnectOptions extends mongodb.MongoClientOptions {\n  /** Set to false to [disable buffering](http://mongoosejs.com/docs/faq.html#callback_never_executes) on all models associated with this connection. */\n  bufferCommands?: boolean;\n  /** The name of the database you want to use. If not provided, Mongoose uses the database name from connection string. */\n  dbName?: string;\n  /** username for authentication, equivalent to `options.auth.user`. Maintained for backwards compatibility. */\n  user?: string;\n  /** password for authentication, equivalent to `options.auth.password`. Maintained for backwards compatibility. */\n  pass?: string;\n  /** Set to false to disable automatic index creation for all models associated with this connection. */\n  autoIndex?: boolean;\n  /** Set to `true` to make Mongoose automatically call `createCollection()` on every model created on this connection. */\n  autoCreate?: boolean;\n}\n`\n```\nLooking at the new definition of the `connect` and `ConnectOptions`, we can see that there is no definition of `useNewUrlParser` or `useUnifiedTopology` inside the `ConnectOptions`. That is the reason we got such an error. You can delete the options `useNewUrlParser: true,` and `useUnifiedTopology: true,` and your code should be able to connect to your MongoDB. If you want to pass in some options to the `connect()` function, then they should follow the new type definition of the `ConnectOptions`.",
      "question_score": 17,
      "answer_score": 24,
      "created_at": "2021-08-25T08:11:08",
      "url": "https://stackoverflow.com/questions/68917683/mongoose-connecturi-connectoptions-does-not-recognize-usenewurlparser-and-oth"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 68742794,
      "title": "MongoDB failed (result: core-dump)?",
      "problem": "I'm attempting to setup MongoDB on Ubuntu 20.04.02 LTS by following the documentation. I have ensured this is a fresh install. However an error persists when I verify if the MongoDB install started succesfully by typing in:\n\nsudo systemctl status mongod\n\n\u25cf mongod.service - MongoDB Database Server\nLoaded: loaded (/lib/systemd/system/mongod.service; disabled; vendor prese>\nActive: failed (Result: core-dump) since Wed 2021-08-11 12:59:20 UTC; 49s >\nDocs: https://docs.mongodb.org/manual\nProcess: 3190 ExecStart=/usr/bin/mongod --config /etc/mongod.conf (code=dum>    Main PID: 3190 (code=dumped, signal=ILL)\nAug 11 12:59:19 discorddomagoj systemd1: Started MongoDB Database\nServer. Aug 11 12:59:20 discorddomagoj systemd1: mongod.service:\nMain process exited,> Aug 11 12:59:20 discorddomagoj systemd1:\nmongod.service: Failed with result '>\n\nPicture of problem for clarity:",
      "solution": "MongoDB 5+ need CPU with AVX instruction, downgrade to MongoDB 4 is the solution.\n1.Stop the `mongod` process by issuing the following command:-\n```\n`    sudo service mongod stop\n`\n```\n2.Remove any MongoDB packages that you had previously installed:-\n```\n`    sudo apt-get purge mongodb-org*\n`\n```\n3.Remove MongoDB databases and log files:-\n```\n`    sudo rm -r /var/log/mongodb\n    sudo rm -r /var/lib/mongodb\n`\n```\n4.Then reinstall mangodb 4.4.8\n5.Import the public key used by the package management system:-\n```\n`    wget -qO - https://www.mongodb.org/static/pgp/server-4.4.asc | sudo apt-key add -\n`\n```\n6.The following instruction is for Ubuntu 20.04 (Focal):-\n```\n`    echo \"deb [ arch=amd64,arm64 ] https://repo.mongodb.org/apt/ubuntu focal/mongodb-org/4.4 multiverse\" | sudo tee /etc/apt/sources.list.d/mongodb-org-4.4.list\n`\n```\n7.Update Apt\n```\n`    sudo apt-get update\n`\n```\n8.Install mongodb\n```\n`    sudo apt-get install mongodb-org=4.4.8 mongodb-org-server=4.4.8 mongodb-org-shell=4.4.8 mongodb-org-mongos=4.4.8 mongodb-org-tools=4.4.8\n`\n```\n9.Use `mongod --version` to check its succesfully installed\n10.If u encounter any error while using `mongod`\n```\n`    sudo mkdir /data\n    cd /data\n    sudo mkdir db\n    sudo pkill -f mongod\n`\n```\n11.Then use sudo `mongod` command.",
      "question_score": 17,
      "answer_score": 31,
      "created_at": "2021-08-11T15:16:32",
      "url": "https://stackoverflow.com/questions/68742794/mongodb-failed-result-core-dump"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 76686267,
      "title": "What is the new way to declare Mongo ObjectId with PyDantic v2.0^?",
      "problem": "This week, I started working with MongoDB and Flask, so I found a helpful article on how to use them together by using PyDantic library to define MongoDB's models. However, the article is somewhat outdated, mostly could be updated to new PyDantic's version, but the problem is that the ObjectId is a third party field and that changed drastically between versions.\nThe article defines the ObjectId using the following code:\n`from bson import ObjectId\nfrom pydantic.json import ENCODERS_BY_TYPE\n\nclass PydanticObjectId(ObjectId):\n    \"\"\"\n    Object Id field. Compatible with Pydantic.\n    \"\"\"\n\n    @classmethod\n    def __get_validators__(cls):\n        yield cls.validate\n   \n    #The validator is doing nothing\n    @classmethod\n    def validate(cls, v):\n        return PydanticObjectId(v)\n\n    #Here you modify the schema to tell it that it will work as an string\n    @classmethod\n    def __modify_schema__(cls, field_schema: dict):\n        field_schema.update(\n            type=\"string\",\n            examples=[\"5eb7cf5a86d9755df3a6c593\", \"5eb7cfb05e32e07750a1756a\"],\n        )\n\n#Here you encode the ObjectId as a string\nENCODERS_BY_TYPE[PydanticObjectId] = str\n`\nIn the past, this code worked well. However, I recently discovered that the latest version of PyDantic has a more complex way of defining custom data types. I've tried following the Pydantic documentation, but I'm still confused and haven't been able to implement it successfully.\nI've tried the implementation to do the implementation for third party types, but it's not working. It's almost the same code of the documentation, but changing ints for strings, and the third party callabels for ObjectId. Again, I'm not sure why it's not working.\n`from bson import ObjectId\nfrom pydantic_core import core_schema \nfrom typing import Annotated, Any\nfrom pydantic import BaseModel, GetJsonSchemaHandler, ValidationError\n\nfrom pydantic.json_schema import JsonSchemaValue\n\nclass PydanticObjectId(ObjectId):\n    \"\"\"\n    Object Id field. Compatible with Pydantic.\n    \"\"\"\n\n    x: str\n\n    def __init__(self):\n        self.x = ''\n\nclass _ObjectIdPydanticAnnotation:\n    @classmethod\n    def __get_pydantic_core_schema__(\n            cls,\n            _source_type: Any,\n            _handler: ObjectId[[Any], core_schema.CoreSchema],\n        ) -> core_schema.CoreSchema:\n\n        @classmethod\n        def validate_object_id(cls, v: ObjectId) -> PydanticObjectId:\n            if not ObjectId.is_valid(v):\n                raise ValueError(\"Invalid objectid\")\n            return PydanticObjectId(v)\n        \n        from_str_schema = core_schema.chain_schema(\n            [\n                core_schema.str_schema(),\n                core_schema.no_info_plain_validator_function(validate_object_id),\n            ]\n        )\n        return core_schema.json_or_python_schema(\n            json_schema=from_str_schema,\n            python_schema=core_schema.union_schema(\n                [\n                    # check if it's an instance first before doing any further work\n                    core_schema.is_instance_schema(PydanticObjectId),\n                    from_str_schema,\n                ]\n            ),\n            serialization=core_schema.plain_serializer_function_ser_schema(\n                lambda instance: instance.x\n            ),\n        )\n    @classmethod\n    def __get_pydantic_json_schema__(\n        cls, _core_schema: core_schema.CoreSchema, handler: GetJsonSchemaHandler\n    ) -> JsonSchemaValue:\n        # Use the same schema that would be used for `int`\n        return handler(core_schema.int_schema())\n\n`\nI've searched for answers on StackOverflow, but all the answers I've found refer to older versions of Pydantic and use code that's similar to what I pasted above. If anyone knows of an alternative solution or can provide clear guidance on how to define a custom data type in the latest version of PyDantic, I would greatly appreciate it.\n\nUpdate\nA constant error that I'm getting because I'm not creating right the ObjectId type is this\nUnable to generate pydantic-core schema for . Set `arbitrary_types_allowed=True` in the model_config to ignore this error or implement `__get_pydantic_core_schema__` on your type to fully support it.\nIf you got this error by calling handler() within `__get_pydantic_core_schema__` then you likely need to call `handler.generate_schema()` since we do not call `__get_pydantic_core_schema__` on `` otherwise to avoid infinite recursion.\nFor further information visit https://errors.pydantic.dev/2.0.2/u/schema-for-unknown-type\nAnd the answer is to declare it as an unknown type, but I don't want it, I want to declare it as an ObjectId.",
      "solution": "Generally best to ask questions like this on pydantic's GitHub discussions.\nYour solution is pretty close, I think you just have the wrong core schema.\nI think our documentation on using custom types via `Annotated` cover this fairly well, but just to help you, here is a working implementation:\n`from typing import Annotated, Any\n\nfrom bson import ObjectId\nfrom pydantic_core import core_schema\n\nfrom pydantic import BaseModel\n\nfrom pydantic.json_schema import JsonSchemaValue\n\nclass ObjectIdPydanticAnnotation:\n    @classmethod\n    def validate_object_id(cls, v: Any, handler) -> ObjectId:\n        if isinstance(v, ObjectId):\n            return v\n\n        s = handler(v)\n        if ObjectId.is_valid(s):\n            return ObjectId(s)\n        else:\n            raise ValueError(\"Invalid ObjectId\")\n\n    @classmethod\n    def __get_pydantic_core_schema__(cls, source_type, _handler) -> core_schema.CoreSchema:\n        assert source_type is ObjectId\n        return core_schema.no_info_wrap_validator_function(\n            cls.validate_object_id, \n            core_schema.str_schema(), \n            serialization=core_schema.to_string_ser_schema(),\n        )\n\n    @classmethod\n    def __get_pydantic_json_schema__(cls, _core_schema, handler) -> JsonSchemaValue:\n        return handler(core_schema.str_schema())\n\nclass Model(BaseModel):\n    id: Annotated[ObjectId, ObjectIdPydanticAnnotation]\n\nprint(Model(id='64b7abdecf2160b649ab6085'))\nprint(Model(id='64b7abdecf2160b649ab6085').model_dump_json())\nprint(Model(id=ObjectId()))\nprint(Model.model_json_schema())\nprint(Model(id='foobar'))  # will error\n`",
      "question_score": 17,
      "answer_score": 15,
      "created_at": "2023-07-14T11:07:30",
      "url": "https://stackoverflow.com/questions/76686267/what-is-the-new-way-to-declare-mongo-objectid-with-pydantic-v2-0"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 69126879,
      "title": "Typescript: deep keyof of a nested object, with related type",
      "problem": "I'm looking for a way to have all keys / values pair of a nested object.\n(For the autocomplete of MongoDB dot notation key / value type)\n```\n`interface IPerson {\n    name: string;\n    age: number;\n    contact: {\n        address: string;\n        visitDate: Date;\n    }\n}\n`\n```\nHere is what I want to achieve, to make it becomes:\n```\n`type TPerson = {\n    name: string;\n    age: number;\n    contact: { address: string; visitDate: Date; }\n    \"contact.address\": string;\n    \"contact.visitDate\": Date;\n}\n`\n```\nWhat I have tried:\nIn this answer, I can get the key with `Leaves`.\nSo it becomes `'name' | 'age' | 'contact.address' | 'contact.visitDate'`.\nAnd in another answer from @jcalz, I can get the deep, related value type, with `DeepIndex`.\nIs it possible to group them together, to become type like `TPerson`?\nModified 9/14: The use cases, need and no need:\nWhen I start this question, I was thinking it could be as easy as something like `[K in keyof T]: T[K];`, with some clever transformation. But I was wrong. Here is what I need:\n1. Index Signature\nSo the interface\n```\n`interface IPerson {\n    contact: {\n        address: string;\n        visitDate: Date;\n    }[]\n}\n`\n```\nbecomes\n```\n`type TPerson = {\n    [x: `contact.${number}.address`]: string;\n    [x: `contact.${number}.visitDate`]: Date;\n    contact: {\n        address: string;\n        visitDate: Date;\n    }[];\n}\n`\n```\nNo need to check for valid `number`, the nature of Array / Index Signature should allow any number of elements.\n2. Tuple\nThe interface\n```\n`interface IPerson {\n    contact: [string, Date]\n}\n`\n```\nbecomes\n```\n`type TPerson = {\n    [x: `contact.0`]: string;\n    [x: `contact.1`]: Date;\n    contact: [string, Date];\n}\n`\n```\nTuple should be the one which cares about valid index numbers.\n3. Readonly\n`readonly` attributes should be removed from the final structure.\n```\n`interface IPerson {\n    readonly _id: string;\n    age: number;\n    readonly _created_date: Date;\n}\n`\n```\nbecomes\n```\n`type TPerson = {\n    age: number;\n}\n`\n```\nThe use case is for MongoDB, the `_id`, `_created_date` cannot be modified after the data has been created. `_id: never` is not working in this case, since it will block the creation of `TPerson`.\n4. Optional\n```\n`interface IPerson {\n    contact: {\n        address: string;\n        visitDate?: Date;\n    }[];        \n}\n`\n```\nbecomes\n```\n`type TPerson = {\n    [x: `contact.${number}.address`]: string;\n    [x: `contact.${number}.visitDate`]?: Date;\n    contact: {\n        address: string;\n        visitDate?: Date;\n    }[];\n}\n`\n```\nIt's sufficient just to bring the optional flags to transformed structure.\n5. Intersection\n```\n`interface IPerson {\n    contact: { address: string; } & { visitDate: Date; }\n}\n`\n```\nbecomes\n```\n`type TPerson = {\n    [x: `contact.address`]: string;\n    [x: `contact.visitDate`]?: Date;\n    contact: { address: string; } & { visitDate: Date; }\n}\n`\n```\n6. Possible to Specify Types as Exception\nThe interface\n```\n`interface IPerson {\n    birth: Date;\n}\n`\n```\nbecomes\n```\n`type TPerson = {\n    birth: Date;\n}\n`\n```\nnot\n```\n`type TPerson = {\n    age: Date;\n    \"age.toDateString\": () => string;\n    \"age.toTimeString\": () => string;\n    \"age.toLocaleDateString\": {\n    ...\n}\n`\n```\nWe can give a list of Types to be the end node.\nHere is what I don't need:\n\nUnion. It could be too complex with it.\nClass related keyword. No need to handle keywords ex: private / abstract .\nAll the rest I didn't write it here.",
      "solution": "Below is the full implementation I have of `Flatten` which transforms a type possibly-nested `T` into a \"flattened\" version whose keys are the dotted paths through the original `T`.  The `O` type is an optional type where you can specify a (union of) object type(s) to leave as-is without flattening them.  In your example, this is just `Date`, but you could have other types.\nWarning: it's hideously ugly and probably fragile. There are edge cases all over the place.  The pieces that make it up involve weird type manipulations that either don't always do what one might expect, or are impenetrable to all but the most seasoned TypeScript veterans, or both.\nIn light of that, there is no such thing as a \"canonical\" answer to this question, other than possibly \"please don't do this\".  But I'm happy to present my version.\nHere it is:\n\n```\n`type Flatten = Writable, O> extends infer U ?\n    U extends O ? U : U extends object ?\n    ValueOf, K, O>) => void }>\n    | ((x: U) => void) extends (x: infer I) => void ?\n    { [K in keyof I]: I[K] } : never : U : never;\n`\n```\nThe basic approach here is to take your `T` type, and return it as-is if it's not an object or if it extends `O`.  Otherwise, we remove any `readonly` properties, and transform any arrays or tuples into a version without all the array methods (like `push()` and `map()`) and get `U`.  We then flatten each property in that.  We have a key `K` and a flattened property `Flatten`; we want to prepend the key `K` to the dotted paths in `Flatten`, and when we're done with all that we want to intersect these flattened objects (with the unflattened object too) all together to be one big object.\nNote that convincing the compiler to produce an intersection involves conditional type inference in contravariant positions (see Transform union type to intersection type), which is where those `(x: XXX) => void)` and `extends (x: infer I) => void` pieces come in.  It makes the compiler take all the different `XXX` values and intersect them to get `I`.\nAnd while an intersection like `{foo: string} & {bar: number} & {baz: boolean}` is what we want conceptually, it's uglier than the equivalent `{foo: string; bar: number; baz: boolean}` so I do some more conditional type mapping with `{ [K in keyof I]: I[K] }` instead of just `I` (see How can I see the full expanded contract of a Typescript type?).\nThis code generally distributes over unions, so optional properties may end up spawning unions (like `{a?: {b: string}}` could produce `{\"a.b\": string; a?: {b: string}} | {\"a\": undefined, a?: {b: string}}`, and while this might not be the representation you were going for, it should work (since, for example, `\"a.b\"` might not exist as a key if `a` is optional).\n\nThe `Flatten` definition depends on helper type functions that I will present here with various levels of description:\n```\n`type Writable = T extends O ? T : {\n    [P in keyof T as IfEquals]: T[P]\n}\n\ntype IfEquals =\n    (() => T extends X ? 1 : 2) extends\n    (() => T extends Y ? 1 : 2) ? A : B;\n`\n```\nThe `Writable` returns a version of `T` with the `readonly` properties removed (unless `T extends O` in which case we leave it alone).  It comes from TypeScript conditional types - filter out readonly properties / pick only required properties.\nNext:\n```\n`type Cleanup =\n    0 extends (1 & T) ? unknown :\n    T extends readonly any[] ?\n    (Exclude extends never ?\n        { [k: `${number}`]: T[number] } : Omit) : T;\n`\n```\nThe `Cleanup` type turns the `any` type into the `unknown` type (since `any` really fouls up type manipulation), turns tuples into objects with just individual numericlike keys (`\"0\"` and `\"1\"`, etc), and turns other arrays into just a single index signature.\nNext:\n```\n`type PrefixKeys =\n    V extends O ? { [P in K]: V } : V extends object ?\n    { [P in keyof V as\n        `${Extract}.${Extract}`]: V[P] } :\n    { [P in K]: V };\n`\n```\n`PrefixKeys` prepends the key `K` to the path in `V`'s property keys... unless `V` extends `O` or `V` is not an object.  It uses template literal types to do so.\nFinally:\n```\n`type ValueOf = T[keyof T]\n`\n```\nturns a type `T` into a union of its properties. See Is there a `valueof` similar to `keyof` in TypeScript?.\nWhew! \ud83d\ude05\n\nSo, there you go. You can verify how closely this conforms to your stated use cases.  But it's very complicated and fragile and I wouldn't really recommend using it in any production code environment without a lot of testing.\nPlayground link to code",
      "question_score": 17,
      "answer_score": 10,
      "created_at": "2021-09-10T05:26:30",
      "url": "https://stackoverflow.com/questions/69126879/typescript-deep-keyof-of-a-nested-object-with-related-type"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 66200136,
      "title": "TypeError: Object of type function is not JSON serializable when using flask_jwt_extended int RESTful API",
      "problem": "I'm building a REST API using flask. I'm using postman for testing a route that creates a new item in my database, but only if the user is logged in. The routes for registering and login are working well, the last one returns the token using flask_jwt_extended module. When I send a post request to my \"/api/notes\" (creates a new note in database) I get the error bellow:\n\" (...) raise TypeError(f'Object of type {o.class.name} '\nTypeError: Object of type function is not JSON serializable\"\nfor the request I'm using the authorization tab of postman. type: Bearer Token, and my token in the field (tried with and without quotation marks)\nI had faced this error this morning, before implementing my one-many relantionship, but I got it working by replacing my VERY_LONG_TOKEN with \"VERY_LONG_TOKEN\" in the Barear token field. I thought that because the token includes dots, it was interpreting as a function. But after implementing the relationship, I went to test and got this error again.\nmy note.py file:\n```\n`from flask import request, Response, jsonify\nfrom app.models import User, Note\nfrom flask_restful import Resource\nfrom flask_jwt_extended import jwt_required, get_jwt_identity\n\nclass NotesApi(Resource):\n    def get(self):\n        notes = Note.objects().to_json()\n        return Response(notes, mimetype=\"application/json\", status=200)\n\n    @jwt_required  \n    def post(self):  # post method I'm making a request for\n        print(\"fool\")  # this doesn't get printed  ->  not reaching \n        user_id = get_jwt_identity()\n        data = request.get_json(force=True)\n        if data:\n            user = User.objects(id=user_id) # logged in user\n            note = Note(**data, user_author=user) # creates note with the author\n            note.save()\n            user.update(push__notes=note) # add this note to users notes\n            user.save()\n            id = str(note.id)\n            return {'id': id}, 200\n        else:\n            return {'error': 'missing data'}, 400\n`\n```\nmy models.py:\n```\n`from app import db  # using mongodb\nfrom datetime import datetime\nfrom flask_bcrypt import generate_password_hash, check_password_hash\n\nclass Note(db.Document):\n    title = db.StringField(max_length=120,required=True)\n    content = db.StringField(required=True)\n    status = db.BooleanField(required=True, default=False)\n    date_modified = db.DateTimeField(default=datetime.utcnow)\n    user_author = db.ReferenceField('User')\n    \nclass User(db.Document):\n    username = db.StringField(max_length=100, required=True, unique=True)\n    email = db.StringField(max_length=120, required=True, unique=True)\n    password = db.StringField(required=True)\n    remember_me = db.BooleanField(default=False)\n    notes = db.ListField(db.ReferenceField('Note', reverse_delete_rule=db.PULL)) # one-many relationship\n\n    def hash_password(self):\n        self.password = generate_password_hash(self.password).decode('utf8')\n\n    def check_password(self, password):\n        return check_password_hash(self.password, password)\n\nUser.register_delete_rule(Note, 'user_author', db.CASCADE)\n`\n```\ninit.py:\n```\n`from flask import Flask\nfrom config import Config  # my config class to set MONGOBD_HOST and SECRET_CLASS\nfrom flask_mongoengine import MongoEngine\nfrom flask_restful import Api\nfrom flask_bcrypt import Bcrypt\nfrom flask_jwt_extended import JWTManager\n\napp = Flask(__name__)\napp.config.from_object(Config)\ndb = MongoEngine(app)\napi = Api(app)\nbcrypt = Bcrypt(app)\njwt = JWTManager(app)\n\nfrom app.resources.routes import initialize_routes\ninitialize_routes(api) \n`\n```\nresources/routes.py:\n```\n`from .note import NotesApi, NoteApi\nfrom .auth import SignupApi, LoginApi\n\ndef initialize_routes(api):\n    api.add_resource(NotesApi, '/api/notes')\n    api.add_resource(NoteApi, '/api/note/')\n    api.add_resource(SignupApi, '/api/auth/signup')\n    api.add_resource(LoginApi, '/api/auth/login')\n`\n```\nfolder structure:\n```\n`app\n  |_ resources\n      |_ auth.py  # signup working well, login also working, return a token (type = String)\n      |_ note.py\n      |_ routes.py\n  |_ __init__.py\n  |_ models.py\nconfig.py\nappname.py  #just import app and do a app.run()\n`\n```\nbody of my post request:\n```\n`{\n   \"title\": \"test0\",\n   \"content\": \"test0\"  \n}\n`\n```\nDid anyone faced it before or know how to solve it?\nEdit: added more code info",
      "solution": "Looks like flask-jwt-extended released a new version over the weekend. As part of the API changes, the `@jwt_required` decorator is now `@jwt_required()`\nhttps://flask-jwt-extended.readthedocs.io/en/stable/v4_upgrade_guide.html#api-changes",
      "question_score": 16,
      "answer_score": 35,
      "created_at": "2021-02-14T22:05:37",
      "url": "https://stackoverflow.com/questions/66200136/typeerror-object-of-type-function-is-not-json-serializable-when-using-flask-jwt"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 69397039,
      "title": "pymongo [SSL: CERTIFICATE_VERIFY_FAILED]: certificate has expired on Mongo Atlas",
      "problem": "I am using MongoDB(Mongo Atlas) in my Django app. All was working fine till yesterday. But today, when I ran the server, it is showing me the following error on console\n```\n`Exception in thread django-main-thread:\nTraceback (most recent call last):\n  File \"c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\threading.py\", line 973, in _bootstrap_inner\n    self.run()\n  File \"c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\threading.py\", line 910, in run\n    self._target(*self._args, **self._kwargs)\n  File \"C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python39\\Lib\\site-packages\\django\\utils\\autoreload.py\", line 64, in wrapper\n    fn(*args, **kwargs)\n  File \"C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python39\\Lib\\site-packages\\django\\core\\management\\commands\\runserver.py\", line 121, in inner_run\n    self.check_migrations()\n  File \"C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python39\\Lib\\site-packages\\django\\core\\management\\base.py\", line 486, in check_migrations\n    executor = MigrationExecutor(connections[DEFAULT_DB_ALIAS])\n  File \"C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python39\\Lib\\site-packages\\django\\db\\migrations\\executor.py\", line 18, in __init__\n    self.loader = MigrationLoader(self.connection)\n  File \"C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python39\\Lib\\site-packages\\django\\db\\migrations\\loader.py\", line 53, in __init__\n    self.build_graph()\n  File \"C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python39\\Lib\\site-packages\\django\\db\\migrations\\loader.py\", line 220, in build_graph\n    self.applied_migrations = recorder.applied_migrations()\n  File \"C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python39\\Lib\\site-packages\\django\\db\\migrations\\recorder.py\", line 77, in applied_migrations\n    if self.has_table():\n  File \"C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python39\\Lib\\site-packages\\django\\db\\migrations\\recorder.py\", line 56, in has_table\n    tables = self.connection.introspection.table_names(cursor)\n  File \"C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python39\\Lib\\site-packages\\django\\db\\backends\\base\\introspection.py\", line 52, in table_names\n    return get_names(cursor)\n  File \"C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python39\\Lib\\site-packages\\django\\db\\backends\\base\\introspection.py\", line 47, in get_names\n    return sorted(ti.name for ti in self.get_table_list(cursor)\n  File \"C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python39\\Lib\\site-packages\\djongo\\introspection.py\", line 47, in get_table_list\n    for c in cursor.db_conn.list_collection_names()\n  File \"C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python39\\Lib\\site-packages\\pymongo\\database.py\", line 880, in list_collection_names\n    for result in self.list_collections(session=session, **kwargs)]\n  File \"C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python39\\Lib\\site-packages\\pymongo\\database.py\", line 842, in list_collections\n    return self.__client._retryable_read(\n  File \"C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python39\\Lib\\site-packages\\pymongo\\mongo_client.py\", line 1514, in _retryable_read\n    server = self._select_server(\n  File \"C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python39\\Lib\\site-packages\\pymongo\\mongo_client.py\", line 1346, in _select_server\n    server = topology.select_server(server_selector)\n  File \"C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python39\\Lib\\site-packages\\pymongo\\topology.py\", line 244, in select_server\n    return random.choice(self.select_servers(selector,\n  File \"C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python39\\Lib\\site-packages\\pymongo\\topology.py\", line 202, in select_servers\n    server_descriptions = self._select_servers_loop(\n  File \"C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python39\\Lib\\site-packages\\pymongo\\topology.py\", line 218, in _select_servers_loop\n    raise ServerSelectionTimeoutError(\npymongo.errors.ServerSelectionTimeoutError: cluster0-shard-00-02.mny7y.mongodb.net:27017: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1129),cluster0-shard-00-01.mny7y.mongodb.net:27017: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1129),cluster0-shard-00-00.mny7y.mongodb.net:27017: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1129), Timeout: 30s, Topology Description: , , ]>\n`\n```\nI am using djongo as the database engine\n```\n`DATABASES = {\n    'default': {\n            'ENGINE': 'djongo',\n            'NAME': 'DbName',\n            'ENFORCE_SCHEMA': False,\n            'CLIENT': {\n                'host': 'mongodb+srv://username:password@cluster0.mny7y.mongodb.net/DbName?retryWrites=true&w=majority'\n            }  \n    }\n}\n`\n```\nAnd following dependencies are being used in the app\n```\n`dj-database-url==0.5.0\nDjango==3.2.5\ndjangorestframework==3.12.4\ndjango-cors-headers==3.7.0\ngunicorn==20.1.0\npsycopg2==2.9.1\npytz==2021.1\nwhitenoise==5.3.0\ndjongo==1.3.6\ndnspython==2.1.0\n`\n```\nWhat should be done in order to resolve this error?",
      "solution": "This is because of a root CA Let\u2019s Encrypt uses (and Mongo Atals uses Let's Encrypt) has expired on 2020-09-30 - namely the \"IdentTrust DST Root CA X3\" one.\nThe fix is to manually install in the Windows certificate store the \"ISRG Root X1\" and \"ISRG Root X2\" root certificates, and the \"Let\u2019s Encrypt R3\" intermediate one - link to their official site - https://letsencrypt.org/certificates/\nCopy from the comments: download the .der field from the 1st category, download, double click and follow the wizard to install it.",
      "question_score": 15,
      "answer_score": 17,
      "created_at": "2021-09-30T20:10:09",
      "url": "https://stackoverflow.com/questions/69397039/pymongo-ssl-certificate-verify-failed-certificate-has-expired-on-mongo-atlas"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 67799847,
      "title": "Mongoose/NestJs can&#39;t access createdAt even though {timestamps: true}",
      "problem": "I'm currently using Mongoose and NestJs and I'm struggling a bit regarding accessing the createdAt property.\nThis is my user.schema.ts\n```\n`@Schema({ timestamps: true})\nexport class User {\n  @Prop({ required: true })\n  name!: string;\n\n  @Prop({ required: true })\n  email!: string;\n}\n\nexport const UserSchema = SchemaFactory.createForClass(User);\n`\n```\nand in my user.service.ts\n```\n`public async getUser(\n    id: string,\n  ): Promise {\n    const user = await this.userModel.findOne({ id });\n\n    if (!user) {\n      throw new NotFoundException();\n    }\n\n    console.log(user.createdAt) // Property 'createdAt' does not exist on type 'User' .ts(2339)\n  }\n`\n```\nSo basically I've set timestamps to true but I'm still unable to access the createdAt property. By the way I also have a custom id which works fine so please ignore that in my service.ts\nI've tried setting `@Prop() createdAt?: Date` to the schema but it still hasn't worked.\nI've also tested this schema using MongoMemoryServer and Jest which shows that it returns createdAt.\nAny help as to why I can't access the createdAt property would be greatly appreciated!",
      "solution": "I tested your code, adding `@Prop() createdAt?: Date` should be able to access `createdAt`.\nThe only thing I spot from your code where you cannot access createdAt is the `id` you pass to the query. The key should be `_id`\n```\n`public async getUser(\n    id: string,\n): Promise {\n    const user = await this.userModel.findOne({ _id: id });\n\n    if (!user) {\n      throw new NotFoundException();\n    }\n\n    console.log(user.createdAt)\n}\n`\n```\nHere is the screenshot of my testing using your code:",
      "question_score": 15,
      "answer_score": 9,
      "created_at": "2021-06-02T08:15:24",
      "url": "https://stackoverflow.com/questions/67799847/mongoose-nestjs-cant-access-createdat-even-though-timestamps-true"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 69412049,
      "title": "Mongoose TypeScript ObjectId casting - Type &#39;string&#39; is not assignable to type &#39;Condition&lt;ObjectId | undefined&gt;&#39;",
      "problem": "I am having issues with mongoose queries when I query by an ObjectId field. Passing `mongoose.Types.ObjectId` or `string` both throws errors. TypeScript only stops complaining when I cast the ObjectId to `mongoose.Schema.Types.ObjectId`, which then crashes at runtime since it's not a valid ObjectId.\n```\n`interface DocumentWithTimestamp extends Document {\n  createdAt: Date;\n  updatedAt: Date;\n}\n`\n```\nFirst of all, I noticed that Document defines `_id` as `ObjectId | undefined`, which already complicates everything.\nI define my Schemas as follows:\n```\n`export interface LanguageSchema extends DocumentWithTimestamp {\n  langCode: string;\n  locale: string;\n}\n\ninterface LanguageModel extends Model {\n  mapToLanguage(language: LeanDocument | LanguageSchema): Language;\n}\n\nconst languageSchema = new Schema(\n  {\n    langCode: { type: String, required: true, trim: true },\n    locale: { type: String, unique: true, required: true, trim: true },\n  },\n  { collection: 'language', timestamps: true, versionKey: false },\n);\n\nlanguageSchema.statics.mapToLanguage = function (language: LeanDocument | LanguageSchema): Language {\n  return {\n    id: language._id?.toString() || '', // why is _id conditional...?\n    langCode: language.langCode,\n    locale: language.locale,\n  };\n};\n\nexport const LanguageModel = model('Language', languageSchema);\n`\n```\nNow querying my LanguageModel by _id or any other ObjectId field throws type errors:\n```\n`export async function findLanguagesByIds(languageIds: string[]) {\n  return LanguageModel.find({ _id: { $in: languageIds } }).lean();\n}\n`\n```\nError:\n```\n`Argument of type '{ _id: { $in: string[]; }; }' is not assignable to parameter of type 'Callback'.\n      Object literal may only specify known properties, and '_id' does not exist in type 'Callback'.\n`\n```\nWhen I try to cast string[] to Schema.Types.ObjectId[] array, the error goes away but this cannot be the solution, since it crashes at runtime (Schema.Types.ObjectId is not a valid ObjectId).\n```\n`const languages = await LanguageModel.find({\n  _id: { $in: languageIds.map((id) => new mongoose.Schema.Types.ObjectId(id)) },\n}).lean();\n`\n```\nIf I cast to `mongoose.Types.ObjectId`(to a real ObjectId) it throws errors again...\nAny help is appreciated!",
      "solution": "I found the solution to my problem.\nmongoose Document is defined as follows:\n```\n`class Document {\n  constructor(doc?: any);\n\n  /** This documents _id. */\n  _id?: T;\n\n  // ...\n  }\n`\n```\nI extended it using `Document`. The ObjectId type however was the Schema type, e.g.\n```\n`import { ObjectId } from \"mongoose\";\n`\n```\nBut it actually should have been the actual ObjectId Type of mongoose and not the Schema type!\nFIX:\n```\n`import { Types } from \"mongoose\"\n\ninterface DocumentWithTimestamp extends Document {\n  createdAt: Date;\n  updatedAt: Date;\n}\n`\n```",
      "question_score": 15,
      "answer_score": 10,
      "created_at": "2021-10-01T23:15:38",
      "url": "https://stackoverflow.com/questions/69412049/mongoose-typescript-objectid-casting-type-string-is-not-assignable-to-type"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 74447979,
      "title": "MongoServerError: Cannot do exclusion on field date in inclusion projection",
      "problem": "I was working with the MongoDB Atlas Server...\nand encountered this error...\nWhat does it mean...?\nCan someone explain in simple words plz...\nThis was the query i was trying...\n```\n`db.posts.find({}, {title: 1, date: 0})\n`\n```\nThe structure of the posts in database is as follows:\n```\n`[\n  {\n    _id: ObjectId(\"63739044de169f6d0h2e6a3d\"),\n    title: 'Post 2',\n    body: 'a news post',\n    category: 'News',\n    likes: 1,\n    tags: [ 'news', 'events' ],\n    date: 'Tue Nov 15 2022 18:53:24 GMT+0530 (India Standard Time)'\n  },\n  {\n    _id: ObjectId(\"63739271de179f5d0e31e5b2\"),\n    title: 'Post 1',\n    body: 'hey there, hemant here',\n    category: 'random',\n    likes: 1,\n    tags: [ 'random', 'events' ],\n    date: 'Tue Nov 15 2022 18:41:24 GMT+0530 (India Standard Time)'\n  }\n]\n\n`\n```\nBut I got an error which says...\n```\n`MongoServerError: Cannot do exclusion on field date in inclusion projection\n`\n```\nI was trying to get all the document objects excluding the date parameter and including the title parameter but got an error...",
      "solution": "According to the documentation, first argument in `find` is filter and second is `projection`. `projection` allows you to specify fields to return.`_id` is the only field which you need to explicitly exclude in the projection. For all other fields you just need to state the inclusion. You will have to follow the below format.\n```\n`db.posts.find({}, {title: 1, body:1, category:1, likes:1, tags:1})\n`\n```",
      "question_score": 14,
      "answer_score": 15,
      "created_at": "2022-11-15T16:13:01",
      "url": "https://stackoverflow.com/questions/74447979/mongoservererror-cannot-do-exclusion-on-field-date-in-inclusion-projection"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 69267875,
      "title": "Cannot Populate path in Mongoose while trying to join two documents",
      "problem": "MongooseError: Cannot populate path `loaned_to` because it is not in your schema. Set the `strictPopulate` option to false to override.\nI've tried to join two documents in mongodb using mongoose in nodejs, But unfortunately this error occurs. My mongoose version is 6.0.6\nBook Schema\n```\n`const mongoose = require('mongoose');\n\nconst BookSchema = new mongoose.Schema({\n    \"name\": {type: String, required: true},\n    \"author_name\": {type: String, required: true},\n    \"published_date\": {type: Date, required: false},\n    \"copies\": [\n        {\n            \"isbn_number\": {type: String, required: true},\n            \"status\": {type: String, required: true, default: \"Available\"},\n            \"due_back\": {type: Date, required: false},\n            \"loaned_to\": {type: mongoose.Schema.Types.ObjectId, required: false, ref: \"User\"}\n        },\n    ]\n})\n\nconst Book = mongoose.model(\"Book\", BookSchema);\nmodule.exports = Book;\n`\n```\nUser Schema\n```\n`const mongoose = require('mongoose');\n\nconst UserSchema = new mongoose.Schema({\n    \"first_name\": {type: String, required: true},\n    \"last_name\": {type: String, required: true},\n    \"phone_number\": {type:  String, required: true},\n    \"address\": {type:  String, required: false},\n    \"user_name\":{type: String, required: true},\n    \"password\": {type:  String, required: true},\n    \"email\": {type:  String, required: true},\n    \"notifications\": [\n        {\n            \"notification_id\" : {type:\"string\", required:true},\n            \"notification\": {type: \"string\", required: true}\n        },\n    ]\n})\n\nconst User = mongoose.model(\"User\", UserSchema);\nmodule.exports = User;\n`\n```\nMy code to join documents\n```\n`exports.getAllBooks = async (req, res) => {\n    try {\n        let data = await BookModel.findOne().populate(\"loaned_to\");\n        res.status(200).send({data: [...data], success: true})\n    } catch (err) {\n        console.log(err)\n        res.status(404).send({success: false, msg: err.message})\n    }\n}\n`\n```",
      "solution": "```\n`exports.getAllBooks = async (req, res) => {\n  try {\n    let data = await BookModel.findOne().populate({\n      path: 'copies.loaned_to',\n      select:\n        'first_name lastName phone_number address user_name email notifications',\n    });\n    res.status(200).json({ data: [...data], success: true });\n  } catch (err) {\n    console.log(err);\n    res.status(500).json({ success: false, msg: err.message });\n  }\n};\n`\n```",
      "question_score": 14,
      "answer_score": 23,
      "created_at": "2021-09-21T13:10:19",
      "url": "https://stackoverflow.com/questions/69267875/cannot-populate-path-in-mongoose-while-trying-to-join-two-documents"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 66496119,
      "title": "pecl fails installing mongodb driver on Apple Silicon (M1)",
      "problem": "I have php 7.4 installed on my macbook pro m1\n```\n`% php -v\nPHP 7.4.15 (cli) (built: Feb 26 2021 09:28:23) ( NTS )\nCopyright (c) The PHP Group\nZend Engine v3.4.0, Copyright (c) Zend Technologies\n    with Zend OPcache v7.4.15, Copyright (c), by Zend Technologies\n`\n```\nI'm trying to install the mongodb driver running:\n```\n`sudo pecl install mongodb\n`\n```\nBut fails after a while:\n```\n`mp/pear/temp/mongodb/src/contrib/ -DHAVE_CONFIG_H -g -O2 -c /private/tmp/pear/temp/mongodb/php_phongo.c  -fno-common -DPIC -o .libs/php_phongo.o\nIn file included from /private/tmp/pear/temp/mongodb/php_phongo.c:29:\nIn file included from /opt/homebrew/Cellar/php@7.4/7.4.15_1/include/php/ext/spl/spl_iterators.h:24:\n/opt/homebrew/Cellar/php@7.4/7.4.15_1/include/php/ext/pcre/php_pcre.h:25:10: fatal error: 'pcre2.h' file not found\n#include \"pcre2.h\"\n         ^~~~~~~~~\n1 error generated.\nmake: *** [php_phongo.lo] Error 1\nERROR: `make' failed\n`\n```\nI tried with\n```\n`arch -x86_64 sudo pecl install mongodb\n`\n```\nbut results in the same error.\nSomebody knows how can I solve this please? Or if I can install the mongodb driver without using  pecl. Thanks in advance.",
      "solution": "I have finally solved my problem.\nI followed these steps from https://github.com/mongodb/mongo-php-driver/issues/1159\nAfter installing a newer PHP version where previously I used 7.3.24 and updated to 7.4.16.\nStill had the same issue but diffrent file:\n\nfatal error: 'pcre2.h' file not found #include \"pcre2.h\"\n\nFrom this error I tried to check the files in the MacOs Big Sur File system and in the end I found where the pcre2.h is located, and oddly it was being called in the pcre folder, so I manually copied from the pcre2 folder to pcre\nThe solution that I used:\n`$cp /opt/homebrew/Cellar/pcre2/10.36/include/pcre2.h /opt/homebrew/Cellar/php\\@7.4/7.4.16/include/php/ext/pcre/pcre2.h\n`\nThen, I installed mongodb using\n`$brew install mongodb\n`\nAfter that, I once again tried\n`$sudo pecl install mongodb\n`\nFinally, it worked. However, I'm not sure if this is a good way to solve the problem by manually adding a header file to the directory.\n(Sorry I'm still new to the MacOs Environment, just bought a mac mini m1 last week for developing my programming skills)\nIf you were to install php multiple versions from homebrew make sure you are using the current version as you wanted to. You could check the version by using which php.\nTo switch from 7.4 to 5.6\n`$brew unlink php@7.4\n$brew link php@5.6 --force\n`\ncredits: https://www.markhesketh.com/switching-multiple-php-versions-on-macos/\nNew error\n\nSuccess Installation:",
      "question_score": 13,
      "answer_score": 40,
      "created_at": "2021-03-05T17:26:27",
      "url": "https://stackoverflow.com/questions/66496119/pecl-fails-installing-mongodb-driver-on-apple-silicon-m1"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 69585273,
      "title": "How can I delete my MongoDB Atlas Project?",
      "problem": "When I click delete button it shows an:\nError Message :\n\"Project Name\" has running Atlas clusters. All Atlas clusters need to be terminated before the project can be deleted. As Shown in picture below :",
      "solution": "You just need to terminate the cluster before deleting the project. Follow these steps:\n\nClick three dots (as seen on the picture) and then click \"Terminate\".\nClick on the leaf on the top left side.\nThen you can delete the project.",
      "question_score": 13,
      "answer_score": 30,
      "created_at": "2021-10-15T15:11:54",
      "url": "https://stackoverflow.com/questions/69585273/how-can-i-delete-my-mongodb-atlas-project"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 68392064,
      "title": "Error when running mongo image - docker-entrypoint.sh: line 381",
      "problem": "After installing Ubuntu v20 and then installing docker:\n```\n`$ docker network create test-network\n\n$ docker pull mongo:latest\n\n$ docker run --network test-network --name mongodb \\\n    -e MONGO_INITDB_ROOT_USERNAME=admin \\\n    -e MONGO_INITDB_ROOT_PASSWORD=pawwrord \\\n    mongo\n`\n```\nI got an error like this:\n```\n`/usr/local/bin/docker-entrypoint.sh: line 381:    25 Illegal instruction     (core dumped) \"${mongodHackedArgs[@]}\" --fork\n`\n```\nDo you know what the problem is? I need some guidance to investigate the issue.\nUPDATE\nI don't have any problem with other tags.\nSpecifically, only when I want to run Mongo with the latest tag, I got this error.",
      "solution": "MongoDB 5.0 requires a Sandy Bridge or newer CPU. Get a newer processor or use an older version of MongoDB.\nhttps://jira.mongodb.org/browse/SERVER-54407",
      "question_score": 13,
      "answer_score": 16,
      "created_at": "2021-07-15T12:24:08",
      "url": "https://stackoverflow.com/questions/68392064/error-when-running-mongo-image-docker-entrypoint-sh-line-381"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 67656468,
      "title": "mongorestore 0 document(s) restored successfully. 0 document(s) failed to restore",
      "problem": "Im trying to migrate mongo, from local to atlas. I did mongodump, it creted `.gz` archive properly.\nHowever when i try to restore like below :\n```\n`mongorestore --uri=\"mongodb+srv://:@xxxxx.ipmg6.mongodb.net/myDbName\" --archive=mongodump_2021-05-22_09-05-44.gz --gzip\n\n`\n```\nI see output\n```\n`2021-05-23T07:25:01.340+0200    The --db and --collection flags are deprecated for this use-case; please use --nsInclude instead, i.e. with --nsInclude=${DATABASE}.${COLLECTION}\n2021-05-23T07:25:01.350+0200    preparing collections to restore from\n2021-05-23T07:25:01.360+0200    0 document(s) restored successfully. 0 document(s) failed to restore.\n\n`\n```\nNo error but also no documents processed. Any idea what is wrong? And i something there is no error or warning from mongo?",
      "solution": "Ok solution is that `--nsInclude` was missing. If one want to restore whole db then `--nsInclude=\"*\"` should be used.",
      "question_score": 13,
      "answer_score": 17,
      "created_at": "2021-05-23T07:32:49",
      "url": "https://stackoverflow.com/questions/67656468/mongorestore-0-documents-restored-successfully-0-documents-failed-to-restor"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 74110777,
      "title": "@DynamicPropertySource not being invoked (Kotlin, Spring Boot and TestContainers)",
      "problem": "I'm trying to define a `@TestConfiguration` class that is executed once before all integration tests to run a MongoDB TestContainer in Kotlin in a Spring Boot project.\nHere is the code:\n```\n`import org.springframework.boot.test.context.TestConfiguration\nimport org.springframework.test.context.DynamicPropertyRegistry\nimport org.springframework.test.context.DynamicPropertySource\nimport org.testcontainers.containers.MongoDBContainer\nimport org.testcontainers.utility.DockerImageName\n\n@TestConfiguration\nclass TestContainerMongoConfig {\n\n  companion object {\n\n    @JvmStatic\n    private val MONGO_CONTAINER: MongoDBContainer = MongoDBContainer(DockerImageName.parse(\"mongo\").withTag(\"latest\")).withReuse(true)\n\n    @JvmStatic\n    @DynamicPropertySource\n    private fun emulatorProperties(registry: DynamicPropertyRegistry) {\n      registry.add(\"spring.data.mongodb.uri\", MONGO_CONTAINER::getReplicaSetUrl)\n    }\n\n    init { MONGO_CONTAINER.start() }\n\n  }\n\n}\n`\n```\nThe issue seems to be that `emulatorProperties` method is not being called.\nThe regular flow should be that the container is started and then the properties are set.\nThe first step happens, the second does not.\nI know there is an alternative for which I can do this configuration in each functional test class but I don't like it as it adds not needed noise to the test class.\nFor example, with a Java project that uses Postgres I managed to make it work with the following code:\n```\n`import javax.sql.DataSource;\n\nimport org.springframework.boot.jdbc.DataSourceBuilder;\nimport org.springframework.boot.test.context.TestConfiguration;\nimport org.springframework.context.annotation.Bean;\nimport org.testcontainers.containers.PostgreSQLContainer;\nimport org.testcontainers.utility.DockerImageName;\n\n@TestConfiguration\npublic class PostgresqlTestContainersConfig {\n\n  static final PostgreSQLContainer POSTGRES_CONTAINER;\n  private final static DockerImageName IMAGE = DockerImageName.parse(\"postgres\").withTag(\"latest\");\n\n  static {\n    POSTGRES_CONTAINER = new PostgreSQLContainer(IMAGE);\n    POSTGRES_CONTAINER.start();\n  }\n\n  @Bean\n  DataSource dataSource() {\n    return DataSourceBuilder.create()\n        .username(POSTGRES_CONTAINER.getUsername())\n        .password(POSTGRES_CONTAINER.getPassword())\n        .driverClassName(POSTGRES_CONTAINER.getDriverClassName())\n        .url(POSTGRES_CONTAINER.getJdbcUrl())\n        .build();\n  }\n}\n`\n```\nI'm trying to achieve the same thing but in Kotlin and using MongoDB.\nAny idea on what may be the issue causing the `@DynamicPropertySource` not being called?",
      "solution": "`@DynamicPropertySource` is part of the Spring-Boot context lifecycle. Since you want to replicate the Java setup in a way, it is not required to use `@DynamicPropertySource`. Instead you can follow the Singleton Container Pattern, and replicate it in Kotlin as well.\nInstead of setting the config on the registry, you can set them as a System property and Spring Autoconfig will pick it up:\n`    init { \n      MONGO_CONTAINER.start() \n      System.setProperty(\"spring.data.mongodb.uri\", MONGO_CONTAINER.getReplicaSetUrl());\n    }\n`",
      "question_score": 12,
      "answer_score": 9,
      "created_at": "2022-10-18T14:12:25",
      "url": "https://stackoverflow.com/questions/74110777/dynamicpropertysource-not-being-invoked-kotlin-spring-boot-and-testcontainers"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 71400834,
      "title": "Failed to unlink socket file [MongoDB]",
      "problem": "I am new to MongoDB & I am trying to install MongoDb-community@4.2 to use mongo on Mac, I proceeded to all these steps by steps :\n\n`sudo brew install mongodb`\n\n`mkdir -p /data/db`\n\n`sudo chown -R -un' /data/db`\n\n`mongo` or `mongod`\n\nuntil i got this on terminal :\n```\n`{\"t\":{\"$date\":\"2022-03-08T20:19:25.376+01:00\"},\"s\":\"I\",  \"c\":\"CONTROL\",  \"id\":21951,   \"ctx\":\"initandlisten\",\"msg\":\"Options set by command line\",\"attr\":{\"options\":{}}}\n{\"t\":{\"$date\":\"2022-03-08T20:19:25.377+01:00\"},\"s\":\"E\",  \"c\":\"NETWORK\",  \"id\":23024,   \"ctx\":\"initandlisten\",\"msg\":\"Failed to unlink socket file\",\"attr\":{\"path\":\"/tmp/mongodb-27017.sock\",\"error\":\"Permission denied\"}}\n{\"t\":{\"$date\":\"2022-03-08T20:19:25.377+01:00\"},\"s\":\"F\",  \"c\":\"-\",        \"id\":23091,   \"ctx\":\"initandlisten\",\"msg\":\"Fatal assertion\",\"attr\":{\"msgid\":40486,\"file\":\"src/mongo/transport/transport_layer_asio.cpp\",\"line\":989}}\n{\"t\":{\"$date\":\"2022-03-08T20:19:25.377+01:00\"},\"s\":\"F\",  \"c\":\"-\",        \"id\":23092,   \"ctx\":\"initandlisten\",\"msg\":\"\\n\\n***aborting after fassert() failure\\n\\n\"}\nmac@macs-MacBook-Pro ~ % mongod --dbpath ~/data/db\n{\"t\":{\"$date\":\"2022-03-08T20:21:23.974+01:00\"},\"s\":\"I\",  \"c\":\"CONTROL\",  \"id\":23285,   \"ctx\":\"thread1\",\"msg\":\"Automatically disabling TLS 1.0, to force-enable TLS 1.0 specify --sslDisabledProtocols 'none'\"}\n{\"t\":{\"$date\":\"2022-03-08T20:21:23.974+01:00\"},\"s\":\"I\",  \"c\":\"NETWORK\",  \"id\":4915701, \"ctx\":\"thread1\",\"msg\":\"Initialized wire specification\",\"attr\":{\"spec\":{\"incomingExternalClient\":{\"minWireVersion\":0,\"maxWireVersion\":13},\"incomingInternalClient\":{\"minWireVersion\":0,\"maxWireVersion\":13},\"outgoing\":{\"minWireVersion\":0,\"maxWireVersion\":13},\"isInternalClient\":true}}}\n{\"t\":{\"$date\":\"2022-03-08T20:21:23.974+01:00\"},\"s\":\"W\",  \"c\":\"ASIO\",     \"id\":22601,   \"ctx\":\"thread1\",\"msg\":\"No TransportLayer configured during NetworkInterface startup\"}\n{\"t\":{\"$date\":\"2022-03-08T20:21:23.974+01:00\"},\"s\":\"I\",  \"c\":\"NETWORK\",  \"id\":4648602, \"ctx\":\"thread1\",\"msg\":\"Implicit TCP FastOpen in use.\"}\n{\"t\":{\"$date\":\"2022-03-08T20:21:23.976+01:00\"},\"s\":\"W\",  \"c\":\"ASIO\",     \"id\":22601,   \"ctx\":\"thread1\",\"msg\":\"No TransportLayer configured during NetworkInterface startup\"}\n{\"t\":{\"$date\":\"2022-03-08T20:21:23.976+01:00\"},\"s\":\"I\",  \"c\":\"REPL\",     \"id\":5123008, \"ctx\":\"thread1\",\"msg\":\"Successfully registered PrimaryOnlyService\",\"attr\":{\"service\":\"TenantMigrationDonorService\",\"ns\":\"config.tenantMigrationDonors\"}}\n{\"t\":{\"$date\":\"2022-03-08T20:21:23.976+01:00\"},\"s\":\"I\",  \"c\":\"REPL\",     \"id\":5123008, \"ctx\":\"thread1\",\"msg\":\"Successfully registered PrimaryOnlyService\",\"attr\":{\"service\":\"TenantMigrationRecipientService\",\"ns\":\"config.tenantMigrationRecipients\"}}\n{\"t\":{\"$date\":\"2022-03-08T20:21:23.976+01:00\"},\"s\":\"I\",  \"c\":\"CONTROL\",  \"id\":5945603, \"ctx\":\"thread1\",\"msg\":\"Multi threading initialized\"}\n{\"t\":{\"$date\":\"2022-03-08T20:21:23.976+01:00\"},\"s\":\"I\",  \"c\":\"CONTROL\",  \"id\":4615611, \"ctx\":\"initandlisten\",\"msg\":\"MongoDB starting\",\"attr\":{\"pid\":69610,\"port\":27017,\"dbPath\":\"/Users/mac/data/db\",\"architecture\":\"64-bit\",\"host\":\"macs-MacBook-Pro.local\"}}\n{\"t\":{\"$date\":\"2022-03-08T20:21:23.976+01:00\"},\"s\":\"I\",  \"c\":\"CONTROL\",  \"id\":23403,   \"ctx\":\"initandlisten\",\"msg\":\"Build Info\",\"attr\":{\"buildInfo\":{\"version\":\"5.0.6\",\"gitVersion\":\"212a8dbb47f07427dae194a9c75baec1d81d9259\",\"modules\":[],\"allocator\":\"system\",\"environment\":{\"distarch\":\"x86_64\",\"target_arch\":\"x86_64\"}}}}\n{\"t\":{\"$date\":\"2022-03-08T20:21:23.976+01:00\"},\"s\":\"I\",  \"c\":\"CONTROL\",  \"id\":51765,   \"ctx\":\"initandlisten\",\"msg\":\"Operating System\",\"attr\":{\"os\":{\"name\":\"Mac OS X\",\"version\":\"20.1.0\"}}}\n{\"t\":{\"$date\":\"2022-03-08T20:21:23.976+01:00\"},\"s\":\"I\",  \"c\":\"CONTROL\",  \"id\":21951,   \"ctx\":\"initandlisten\",\"msg\":\"Options set by command line\",\"attr\":{\"options\":{\"storage\":{\"dbPath\":\"/Users/mac/data/db\"}}}}\n{\"t\":{\"$date\":\"2022-03-08T20:21:23.977+01:00\"},\"s\":\"E\",  \"c\":\"NETWORK\",  \"id\":23024,   \"ctx\":\"initandlisten\",\"msg\":\"Failed to unlink socket file\",\"attr\":{\"path\":\"/tmp/mongodb-27017.sock\",\"error\":\"Permission denied\"}}\n{\"t\":{\"$date\":\"2022-03-08T20:21:23.977+01:00\"},\"s\":\"F\",  \"c\":\"-\",        \"id\":23091,   \"ctx\":\"initandlisten\",\"msg\":\"Fatal assertion\",\"attr\":{\"msgid\":40486,\"file\":\"src/mongo/transport/transport_layer_asio.cpp\",\"line\":989}}\n{\"t\":{\"$date\":\"2022-03-08T20:21:23.977+01:00\"},\"s\":\"F\",  \"c\":\"-\",        \"id\":23092,   \"ctx\":\"initandlisten\",\"msg\":\"\\n\\n***aborting after fassert() failure\\n\\n\"}```\n`\n```",
      "solution": "Most probably you missed to shutdown properly your mongod previous time so there is socket file left in the tmp folder that you need to remove manually before you try to start the mongod process again:\n```\n`    rm /tmp/mongodb-27017.sock\n`\n```",
      "question_score": 12,
      "answer_score": 16,
      "created_at": "2022-03-08T21:00:46",
      "url": "https://stackoverflow.com/questions/71400834/failed-to-unlink-socket-file-mongodb"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 67860273,
      "title": "Preflight request doesn&#39;t pass access control check: It does not have HTTP ok status problem and i don&#39;t know why, i imported everything right",
      "problem": "I'm getting this error where it says \"Response to preflight request doesn't pass access control check: It does not have HTTP ok status.\" Anyone have any idea on what could it be?\nThis is my code on backend:\napp.js\n```\n`app.delete(\"api/posts/:id\", (req,res,next) =>{\n  console.log(req.params.id);\n  res.status(200).json({message: 'post deleted!'})\n})\n\n`\n```\nThis is the code on frontend:\nposts.service.ts:\n```\n`deletePost(postId: string) {\n    this.http\n      .delete('http://localhost:3000/api/posts/' + postId)\n      .subscribe(() => {\n        console.log('deleted');\n      });\n  }\n\n`\n```\npost-list.component.ts\n```\n`onDelete(postId: string) {\n    this.postsService.deletePost(postId);\n  }\n`\n```\nAnd post-list.component.html:\n```\n`DELETE\n`\n```\nNote that I DID ALLOW CORS in my code, here's the proof:\n```\n`app.use((req, res, next) => {\n  res.setHeader(\"Access-Control-Allow-Origin\", \"*\");\n  res.setHeader(\n    \"Access-Control-Allow-Headers\",\n    \"Origin, X-Requested-With, Content-Type, Accept\"\n  );\n  res.setHeader(\n    \"Access-Control-Allow-Methods\",\n    \"GET, POST, PATCH, PUT, DELETE, OPTIONS\"\n  );\n  next();\n});\n`\n```\nHere's a picture of the problem:",
      "solution": "Preflight requests are not handeled as \"normal\" request. They have the http OPTIONS header.So after your code snippet where you set the CORS header just add the following lines:\n```\n`app.options('/*', (_, res) => {\n    res.sendStatus(200);\n});\n`\n```\nThis just sets the status to 200 and should fix your problem.",
      "question_score": 12,
      "answer_score": 16,
      "created_at": "2021-06-06T16:39:42",
      "url": "https://stackoverflow.com/questions/67860273/preflight-request-doesnt-pass-access-control-check-it-does-not-have-http-ok-st"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 74013936,
      "title": "Prisma - The provided database string is invalid. MongoDB connection string error",
      "problem": "I'm being told I have an invalid connection string for my MongoDB data provider.\nSpecifically, I'm getting this: `The provided database string is invalid. MongoDB connection string error: Missing delimiting slash between hosts and options in database URL.`\nMy problem, however, is that my connection string does have a delimiting slash: it's this: `mongodb://:@cluster0..mongodb.net/?retryWrites=true&w=majority`\nWhat's going on? Is there anything I'm missing?",
      "solution": "It's missing database name after host: `mongodb://:@cluster0..mongodb.net/?retryWrites=true&w=majority`",
      "question_score": 11,
      "answer_score": 26,
      "created_at": "2022-10-10T13:07:06",
      "url": "https://stackoverflow.com/questions/74013936/prisma-the-provided-database-string-is-invalid-mongodb-connection-string-erro"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 66748716,
      "title": "&quot;$lookup with &#39;pipeline&#39; may not specify &#39;localField&#39; or &#39;foreignField&#39;&quot;",
      "problem": "I'm new to mongoose & mongoDB, I have following query, which when run throws the follow error. It would be great if some can help me handle the issue and still get the same output\n```\n`//interview.model.js => mongodb show name as interview\nmodule.exports = mongoose.model('Interview', interviewSchema);\n//candidate.model.js => mongodb show name as candidate\nmodule.exports = mongoose.model('Candidate', candidateSchema);\n\nconst [result, err] = await of(Interview.aggregate([\n         {\n           $match: {\n             ...filterQuery,\n           }\n         },\n         {\n           $lookup: {\n             'from': 'candidates',\n             'localField': 'candidateId',\n             'foreignField': '_id',\n             'as': 'candidateId',\n             pipeline: [\n               { $match: { 'candidateId.interviewCount': 0 }},\n               { $project: { firstName:1, status:1, avatar:1 } }\n             ]\n           }\n         },\n       ]))\n`\n```",
      "solution": "MongoDB 4.4 or below versions:\nYou can either use any one syntax of `$lookup` from 1) localField/ForeignField or 2) lookup with pipeline,\n\n`let` to declare a variable `candidateId` you can use this id inside `pipeline`, that is called localField\n`pipeline` push expression match using `$expr` and `$eq` because we are matching internal fields\n`$$` reference will allow to use variables in pipeline that is declared `let`\n\n```\n`const [result, err] = await of(Interview.aggregate([\n  { $match: { ...filterQuery } },\n  { \n    $lookup: {\n      'from': 'candidates',\n      'as': 'candidateId',\n      'let': { candidateId: \"$candidateId\" },\n      'pipeline': [\n        { \n          $match: { \n            $expr: { $eq: [\"$$candidateId\", \"$_id\"] },\n            'candidateId.interviewCount': 0\n          } \n        },\n        { $project: { firstName:1, status:1, avatar:1 } }\n      ]\n    }\n  }\n]))\n`\n```\nMongoDB 5.0 or above versions:\nYou query looks good as per latest version of mongodb, You can upgrade your mongodb version to mongodb latest version.",
      "question_score": 11,
      "answer_score": 33,
      "created_at": "2021-03-22T16:05:52",
      "url": "https://stackoverflow.com/questions/66748716/lookup-with-pipeline-may-not-specify-localfield-or-foreignfield"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 65953381,
      "title": "How to use .env with ES6 module with node js and express application?",
      "problem": "I need your help on how I can use .env file on this application. here is my problem: I am building an app using ES6 module in my node express app. I am facing a problem while storing my variable in .env file, both these two ways below are giving this error : `MongooseError: The uri parameter to openUri() must be a string, got \"undefined\". Make sure the first parameter to mongoose.connect() or mongoose.createConnection() is a string. did not connect`. But when I only use the plain string connect is working, that means that I am not using the dotenv file correctly:\n1-\n```\n`import {} from \"dotenv/config.js\";\nimport express from \"express\";\nimport mongoose from \"mongoose\";\nimport cors from \"cors\";\n\nconst app=express()\n...\n//DB config\nmongoose.connect(process.env.CONNECTION_URL,\n    {\n      useCreateIndex: true,\n      useNewUrlParser: true,\n      useUnifiedTopology: true,\n    })\n\napp.listen(port,()=>console.log(`server on ${port}`) \n`\n```\n2-\n```\n`import dotenv from \"dotenv\";\nimport express from \"express\";\nimport mongoose from \"mongoose\";\nimport cors from \"cors\";\n\ndotenv.config();\n\nconst app=express()\n...\n//DB config\nmongoose.connect(process.env.CONNECTION_URL,\n    {\n      useCreateIndex: true,\n      useNewUrlParser: true,\n      useUnifiedTopology: true,\n    })\n\napp.listen(port,()=>console.log(`server on ${port}`)\n`\n```",
      "solution": "There are several methods to import dotenv configs, 2u4u has been answered one of the way to import dotenv, and you can try this one below too for your future reference.\n```\n`import { configDotenv } from \"dotenv\";\nimport express from \"express\";\n\nconst app = express();\nconfigDotenv();\n\napp.get(\"/\" , (req, res) => {\n    return res.json(\"HELLO WORLD\")\n});\n\napp.listen(process.env.PORT, () => {\n    console.log(`This app listens to http://localhost:${process.env.PORT}`)\n});\n`\n```\nSince the `configDotenv` is a function, and I checked with `console.log` for a better clarification though, then you need to call below. That's it and worked for me too.",
      "question_score": 11,
      "answer_score": 1,
      "created_at": "2021-01-29T12:13:06",
      "url": "https://stackoverflow.com/questions/65953381/how-to-use-env-with-es6-module-with-node-js-and-express-application"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 71225451,
      "title": "How Disable logs in mongodb?",
      "problem": "How can I disable logs in MongoDB?\nI'm using a container, and trying to hide the logs using the following code in `docker-compose.yml`\n```\n`mongodb:\n  image: mongo\n  logging:\n    driver: \"none\"\n`\n```\nI tried without quotes but it doesn't work.",
      "solution": "Well, I have been following this thread I've omitted the whole log and it's much better in my case:\n```\n`mongo:\n    command: mongod --quiet --logpath /dev/null \n`\n```",
      "question_score": 11,
      "answer_score": 21,
      "created_at": "2022-02-22T18:09:31",
      "url": "https://stackoverflow.com/questions/71225451/how-disable-logs-in-mongodb"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 70298707,
      "title": "MongoDB Compass: Current topology does not support sessions",
      "problem": "All of a sudden I am getting an error - 'Current topology does not support sessions' on MongoDB Compass. I have never seen this before on MongoDB Compass!!!\nBelow are the details on version/server\nMongoDB Compass Version: 1.29.5 (1.29.5)\nMongoDB Version: MongoDB 3.0.6 Community\nCluster : Standalone\nHost : AWS EC2\n.",
      "solution": "I believe this is an issue due to the mongoDB version is not compatible with the latest version of MongoDB Compass.\nSolution: Downgraded MongoDB Compass version to 1.28.4 (1.28.4).\nLink - https://github.com/mongodb-js/compass/releases?q=1.28.4&expanded=true ... look for required installer under Assets.",
      "question_score": 11,
      "answer_score": 15,
      "created_at": "2021-12-10T02:28:54",
      "url": "https://stackoverflow.com/questions/70298707/mongodb-compass-current-topology-does-not-support-sessions"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 68083050,
      "title": "Problem with connecting mongo express service in docker",
      "problem": "I have a docker configuration where I started nginx server. Now I have to run mongo and mongo-express to be able to connect to my database. When I put my configuration for those two in docker-compose.yml and try docker compose up, mongo service starts but mongo express shows this error\n```\n`Could not connect to database using connectionString: mongodb://root:pass@mongodb:27017/\" mongo-express_1 | (node:8) UnhandledPromiseRejectionWarning: MongoNetworkError: failed to connect to server [mongodb:27017] on first connect\n`\n```\nAny help is appreciated, here are my configurations.\ndocker-compose.yml\n```\n`version: \"3\"\n\nservices:\napp:\n  build:\n    context: .\n    dockerfile: Dockerfile\n  image: cryptoterminal\n  container_name: app\n  restart: unless-stopped\n  volumes:\n    - ./:/var/www\n\nwebserver:\n  build:\n    context: .\n    dockerfile: Dockerfile_Nginx\n  image: nginx\n  container_name: webserver\n  restart: unless-stopped\n  ports:\n    - \"8080:80\"\n  volumes: \n    - ./:/var/www\n    - ./config/nginx/:/etc/nginx/conf.d/\n  depends_on:\n    - app\nmongo:\n  image: mongo\n  environment:\n      - MONGO_INITDB_ROOT_USERNAME=root\n      - MONGO_INITDB_ROOT_PASSWORD=pass\nmongo-express:\n  image: mongo-express\n  environment:\n    - ME_CONFIG_MONGODB_SERVER=mongodb\n    - ME_CONFIG_MONGODB_ENABLE_ADMIN=true\n    - ME_CONFIG_MONGODB_ADMINUSERNAME=root\n    - ME_CONFIG_MONGODB_ADMINPASSWORD=pass\n    - ME_CONFIG_BASICAUTH_USERNAME=admin\n    - ME_CONFIG_BASICAUTH_PASSWORD=admin123\n  depends_on:\n      - mongo\n  ports:\n    - \"8888:8081\"\n`\n```\nDockerfile\n```\n`FROM php:7.4-fpm-alpine\n\nWORKDIR /var/www\n\nRUN apk update && apk add \\\n\nbuild-base \\\nvim\n\nRUN addgroup -g 1000 -S www && \\\nadduser -u 1000 -S www -G www\n\nUSER www\n\nCOPY --chown=www:www . /var/www\n\nEXPOSE 9000\n`\n```\nDockerfile_Nginx\n```\n`FROM nginx:alpine\n\nCOPY ./config/nginx/app.conf /etc/nginx/conf.d/app.conf\n\nCOPY . /var/www\n`\n```\n.env file\n```\n`MONGO_DB_URI=mongodb://root:pass@mongodb:27017/crypto-terminal?authSource=admin\n`\n```\napp.conf file\n```\n`server {\n    listen 80;\n    index index.php index.html;\n    error_log /var/log/nginx/error.log;\n    access_log /var/log/nginx/access.log;\n    root /var/www;\n    location ~ \\.php$ {\n        try_files $uri =404;\n        fastcgi_split_path_info ^(.+\\.php)(/.+)$;\n        fastcgi_pass app:9000;\n        fastcgi_index index.php;\n        include fastcgi_params;\n        fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name;\n        fastcgi_param PATH_INFO $fastcgi_path_info;\n    }\n    location / {\n        try_files $uri $uri/ /index.php?$query_string;\n    }\n}\n`\n```\nfolder structure\n```\n`project_folder\n    php\n        config\n            nginx\n                app.conf\n    .env\n    Dockerfile\n    Dockerfile_Nginx\n    docker-compose.yml\n    index.php\n`\n```",
      "solution": "In your docker-compose file, you call the mongo service `mongo`. That's the name you can address it as on the docker network. In your connection string, you've said\n```\n`mongodb://root:pass@mongodb:27017/\"\n`\n```\nIt should be\n```\n`mongo://root:pass@mongo:27017/\"\n`\n```\nSince the connection string is built using the environment variables in your docker-compose file, the thing to change is\n```\n`- ME_CONFIG_MONGODB_SERVER=mongodb\n`\n```\nto\n```\n`- ME_CONFIG_MONGODB_SERVER=mongo\n`\n```",
      "question_score": 11,
      "answer_score": 13,
      "created_at": "2021-06-22T13:51:32",
      "url": "https://stackoverflow.com/questions/68083050/problem-with-connecting-mongo-express-service-in-docker"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 69278930,
      "title": "Cant read data from collection in MongoDB Atlas Trigger",
      "problem": "New to MongoDB, very new to Atlas. I'm trying to set up a trigger such that it reads all the data from a collection named `Config`. This is my attempt:\n```\n`exports = function(changeEvent) {\n  const mongodb = context.services.get(\"Cluster0\");\n  const db = mongodb.db(\"TestDB\");\n  var collection = db.collection(\"Config\");\n  config_docs = collection.find().toArray(); \n  console.log(JSON.stringify(config_docs));\n}\n`\n```\nthe function is part of an automatically created realm application called `Triggers_RealmApp`, which has `Cluster0` as a named linked data source. When I go into Collections in Cluster0, `TestDB.Config` is one of the collections.\nSome notes:\n\nit's not throwing an error, but simply returning `{}`.\nWhen I change `context.services.get(\"Cluster0\");` to something else, it throws an error\nWhen I change `\"TestDB\"` to a db that doesnt exist, or `\"Config\"` to a collection which doesn't exist, I get the same output; `{}`\nI've tried creating new Realm apps, manually creating services, creating new databases and new collections, etc. I keep bumping into the same issue.\nThe mongo docs reference promises and awaits, which I haven't seen in any examples (link). I tried experimenting with that a bit and got nowhere. From what I can tell, what I've already done is the typical way of doing it.\n\nImages:\nCollection:\n\nLinked Data Source:",
      "solution": "I ended up taking it up with MongoDB directly, `.find()` is asynchronous and I was handling it incorrectly. Here is the reply straight from the horses mouth:\n\nAs I understand it, you are not getting your expected results from the query you posted above. I know it can be confusing when you are just starting out with a new technology and can't get something to work!\nThe issue is that the collection.find() function is an asynchronous function. That means it sends out the request but does not wait for the reply before continuing. Instead, it returns a Promise, which is an object that describes the current status of the operation. Since a Promise really isn't an array, your statment collection.find().toArray() is returning an empty object. You write this empty object to the console.log and end your function, probably before the asynchronous call even returns with your data.\nThere are a couple of ways to deal with this. The first is to make your function an async function and use the await operator to tell your function to wait for the collection.find() function to return before continuing.\n```\n`exports = async function(changeEvent) {\n  const mongodb = context.services.get(\"Cluster0\");\n  const db = mongodb.db(\"TestDB\");\n  var collection = db.collection(\"Config\");\n  config_docs = await collection.find().toArray(); \n  console.log(JSON.stringify(config_docs));\n\n};\n`\n```\nNotice the async keyword on the first line, and the await keyword on the second to last line.\nThe second method is to use the .then function to process the results when they return:\n```\n`exports = function(changeEvent) {\n  const mongodb = context.services.get(\"Cluster0\");\n  const db = mongodb.db(\"TestDB\");\n  var collection = db.collection(\"Config\");\n  collection.find().toArray().then(config_docs => {\n    console.log(JSON.stringify(config_docs));\n  });\n};\n`\n```",
      "question_score": 11,
      "answer_score": 5,
      "created_at": "2021-09-22T07:59:13",
      "url": "https://stackoverflow.com/questions/69278930/cant-read-data-from-collection-in-mongodb-atlas-trigger"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 69015811,
      "title": "ReferenceError: TextEncoder is not defined Node.js with mongoose",
      "problem": "the problem seems to be with mongoose & mongodb packages as it works fine when\n```\n`mongoose.connect('mongodb+srv://mydb:@cluster0.w1opr.mongodb.net/test?retryWrites=true&w=majority');\n`\n```\nis removed\nit also works fine on repl.it cloud env\nhere is my code\n```\n`var express = require('express');\nvar ejs = require('ejs');\nvar app = express();\nvar bodyParser = require('body-parser');\nvar mongoose = require('mongoose');\nmongoose.connect('mongodb+srv://mydb:@cluster0.w1opr.mongodb.net/test? \nretryWrites=true&w=majority');\napp.set('view engine','ejs')\napp.use(bodyParser.urlencoded({extended: true}));\n.\n.\n.\napp.listen(3000,function(){\nconsole.log('Server is running on port 3000');\n});\n`\n```",
      "solution": "Actually mongoose 6 requires Node 12 or higher, so this is expected behavior. Mongoose 6 does not support Node 10.So updating Node version will fix the issue. It also fix the problem by downgrading mongoose version to 5.",
      "question_score": 10,
      "answer_score": 35,
      "created_at": "2021-09-01T16:35:34",
      "url": "https://stackoverflow.com/questions/69015811/referenceerror-textencoder-is-not-defined-node-js-with-mongoose"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 71073107,
      "title": "How to fix the error (TypeError: Cannot assign to read only property &#39;map&#39; of object &#39;#&lt;QueryCursor&gt;&#39;)",
      "problem": "I am sending my data to MongoDB via Mongoose. Now, during the fetch of API route for it, an error is thrown.\nCode\n`const addChoice = async (e) => {\n    try {\n        e.preventDefault();\n        const res = await fetch(\"/api/sendChoice\", {\n            method: \"POST\",\n            headers: {\n                \"Content-Type\": \"application/json\",\n            },\n            body: JSON.stringify({\n                choiceSeq: choice,\n                checkSubmit: true,\n            }),\n        });\n        console.log(res);\n        router.push(\"/home\");\n    } catch (error) {\n        console.log(error);\n    }\n};\n`\nThe error is happening at `const res = await fetch(\"/api/sendChoice\" ,{`\nIn terminal server the error\n\nerror - TypeError: Cannot assign to read only property 'map' of object\n'#'\n\nIn the inspect element the error is as\n\nI can't find anything related to fix this issue, I don't even understand what the error means to try to resolve it myself.\nSome other related code from my project:\napi/sendChoice\n`import { getSession } from \"next-auth/client\";\nimport dbConnect from \"../../helpers/dbConnect\";\nimport Choice from \"../../models/Choice\";\n\nexport default async function sendChoice(req, res) {\n    try {\n        const session = await getSession({ req });\n        await dbConnect();\n        if (!session) {\n            res.status(401).send(\"You are not signed in\");\n            return;\n        }\n        if (req.method === \"POST\") {\n            console.log(req.body);\n            const { choiceSeq, checkSubmit } = req.body;\n            console.log(choiceSeq, checkSubmit);\n            const userId = session.user.id;\n            const nameP = session.user.name;\n            const choice = new Choice({\n                user: userId,\n                name: nameP,\n                choiceSeq,\n                checkSubmit,\n            });\n            await choice.save();\n            res.status(200).send(\"Choice saved\");\n        } else {\n            res.status(400).send(\"Bad request\");\n        }\n    }\n    catch (error) {\n        console.log(error);\n    }\n}\n`\nThe MongoDB schema\n`import mongoose, { Schema } from 'mongoose';\n\nconst ChoiceSchema = new Schema({\n    user: {\n        type: Schema.Types.ObjectId,\n        ref: 'User',\n    },\n    name: {\n        type: String,\n    },\n    choiceSeq: {\n        type: Array,\n        default: [],\n    },\n    checkSubmit: {\n        type: Boolean,\n    }\n});\n\nmongoose.models = {};\n\nexport default mongoose.model('Choice', ChoiceSchema);\n`",
      "solution": "the latest update to version 17.5.0 is the one causing this error. You must reinstall node js to version 16.14.0 LTS. You should always work with LTS versions",
      "question_score": 10,
      "answer_score": 29,
      "created_at": "2022-02-10T23:35:32",
      "url": "https://stackoverflow.com/questions/71073107/how-to-fix-the-error-typeerror-cannot-assign-to-read-only-property-map-of-ob"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 66918510,
      "title": "mongodump.exe and mongorestore.exe files are missing",
      "problem": "Yesterday I reinstalled Windows 10 on my PC and downloaded and installed the current version of MongoDB Enterprise from the MongoDB website. After installing MongoDB, it works fine. But the mongodump.exe and mongorestore.exe files are missing in the C:\\Program Files\\MongoDB\\Server\\4.4\\bin directory. So, I cannot restore my previous database and continue my work. Am I doing anything wrong? How can I get those files back so that I can restore my previous database?",
      "solution": "Starting with Mongo 4.4 the database tools are not part of the MongoDB Server package anymore.\nYou have to download MongoDB Database Tools separately from https://www.mongodb.com/try/download/database-tools",
      "question_score": 10,
      "answer_score": 29,
      "created_at": "2021-04-02T13:17:51",
      "url": "https://stackoverflow.com/questions/66918510/mongodump-exe-and-mongorestore-exe-files-are-missing"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 76013265,
      "title": "How to do a MongoDB 6 single node replicaset with Docker compose?",
      "problem": "On my local Mac M1 PRO, I use for several month now a Docker compose to mount a single node replicaset based on MongoDB 5.\n```\n`version: \"3.9\"\nservices:\n  mongodb:\n    image: mongo:5\n    command: --replSet rs0\n    ports:\n      - '28017:27017'\n    healthcheck:\n      test: echo 'db.runCommand(\"ping\").ok' | mongo localhost:27017/admin --quiet\n      interval: 2s\n      timeout: 3s\n      retries: 5\n\n  mongo-init:\n    image: mongo:5\n    restart: \"no\"\n    depends_on:\n      mongodb:\n        condition: service_healthy\n    command: >\n      mongo --host mongodb:27017 --eval\n      '\n      rs.initiate( {\n        _id : \"rs0\",\n        members: [\n          { _id: 0, host: \"localhost:27017\" }\n        ]\n      })\n      '\n`\n```\nIt works well and I have a simple MongoDB 5 replicaset. Now, I want the same thing with MongoDB 6. So, I modify the image from mongodb:5 to mongodb:6 but the replicaset didn't mount.\nI have this error:\n```\n`{\"t\":{\"$date\":\"2023-04-14T08:52:52.326+00:00\"},\"s\":\"I\",  \"c\":\"-\", \"id\":4939300, \"ctx\":\"monitoring-keys-for-HMAC\",\"msg\":\"Failed to refresh key cache\",\"attr\":{\"error\":\"NotYetInitialized: Cannot use non-local read concern until replica set is finished initializing.\",\"nextWakeupMillis\":24600}}\n`\n```\nI don't need TLS or encryption fancy feature.\nWhat is wrong with my configuration?",
      "solution": "The mongo6 docker container has changed some internals, here is a working version for me\nPort differences arent relevant except for setting your correct ones in the commands/healthchecks.\n```\n`  mongo:\n    image: mongo:6\n    command: [--replSet, my-replica-set, --bind_ip_all, --port, \"30001\"]\n    ports:\n      - 30001:30001\n    healthcheck:\n      test: test $$(mongosh --port 30001 --quiet --eval \"try {rs.initiate({_id:'my-replica-set',members:[{_id:0,host:\\\"mongo:30001\\\"}]})} catch(e) {rs.status().ok}\") -eq 1\n      interval: 10s\n      start_period: 30s\n`\n```",
      "question_score": 10,
      "answer_score": 17,
      "created_at": "2023-04-14T10:56:52",
      "url": "https://stackoverflow.com/questions/76013265/how-to-do-a-mongodb-6-single-node-replicaset-with-docker-compose"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 68526200,
      "title": "No AuthProvider for DEFAULT defined",
      "problem": "My MongoDB Compass has updated to version 1.28.1 and now I can't connect to my mongo host. The error is\n`No AuthProvider for DEFAULT defined.`\nI don't use authentication, so my connection string is without username and password. How to fix the problem?",
      "solution": "When you first create new connection, the connection string looks like\n```\n`mongodb://some-remote-host/database\n`\n```\nThen MongoDb Compass saves connection to favorites modifying the connection string to\n```\n`mongodb://some-remote-host:27017/database?readPreference=primary&authSource=database&appname=MongoDB%20Compass&directConnection=true&ssl=false\n`\n```\nTo make MongoDB Compass connect again you need to remove this parameter from connection string:\n```\n`&authSource=database\n`\n```",
      "question_score": 10,
      "answer_score": 9,
      "created_at": "2021-07-26T09:47:49",
      "url": "https://stackoverflow.com/questions/68526200/no-authprovider-for-default-defined"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 66067320,
      "title": "Running mongorestore on Docker once the container starts",
      "problem": "I'm trying to set up a container running MongoDB that gets populated with data using mongorestore when it starts up. The idea is to quickly set up a dummy database for testing and mocking.\nMy Dockerfile looks like this:\n```\n`FROM mongo:bionic\nCOPY ./db-dump/mydatabase/* /db-dump/\n`\n```\nand docker-compose.yml looks like this:\n```\n`version: \"3.1\"\n  \nservices:\n  mongo:\n    build: ./mongo\n    command: mongorestore -d mydatabase ./db-dump\n    ports:\n      - \"27017:27017\"\n`\n```\nIf I run this with `docker-compose up`, it pauses for a while and then I get an error saying:\n```\n`error connecting to host: could not connect to server: server selection error: server selection timeout, current topology: { Type: Single, Servers: [{ Addr: localhost:27017, Type: Unknown, State: Connected, Average RTT: 0, Last error: connection() : dial tcp 127.0.0.1:27017: connect: connection refused }, ] }\n`\n```\nOpening a CLI on the container and running the exact same command works without any issues, however. I've tried adding `-h` with the name of the container or 127.0.0.1, and it doesn't make a difference. Why isn't this command able to connect when it works fine once the container is running?",
      "solution": "There is a better way than overriding the default command - using `/docker-entrypoint-initdb.d`:\n\nWhen a container is started for the first time it will execute files with extensions `.sh` and `.js` that are found in `/docker-entrypoint-initdb.d`. Files will be executed in alphabetical order. `.js` files will be executed by mongo using the database specified by the MONGO_INITDB_DATABASE variable, if it is present, or test otherwise. You may also switch databases within the `.js` script.\n\n[Source]\nSo you simply write that command into a file named `mongorestore.sh`:\n```\n`mongorestore -d mydatabase /db-dump\n`\n```\nand then mount it inside along with the dump file:\n`version: \"3.1\"\n  \nservices:\n  mongo:\n    image: mongo:bionic\n    ports:\n      - \"27017:27017\"\n    volumes:\n      - ./mongorestore.sh:/docker-entrypoint-initdb.d/mongorestore.sh\n      - ./db-dump:/db-dump\n`\nYou don't even need a Dockerfile.",
      "question_score": 10,
      "answer_score": 18,
      "created_at": "2021-02-05T17:53:02",
      "url": "https://stackoverflow.com/questions/66067320/running-mongorestore-on-docker-once-the-container-starts"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 68617654,
      "title": "Error: Problem: nothing provides /usr/libexec/platform-python needed by mongodb-org-database-tools-extra-4.4.5-1.el8.x86_64",
      "problem": "I am using Fedora Linux and when i want to update MongoDB tools (mongodb-org-tools) or my packages via `sudo dnf update`\ni always get error like this:\n```\n`Error: \n Problem: problem with installed package mongodb-org-database-tools-extra-4.4.4-1.el8.x86_64\n  - cannot install the best update candidate for package mongodb-org-database-tools-extra-4.4.4-1.el8.x86_64\n  - nothing provides /usr/libexec/platform-python needed by mongodb-org-database-tools-extra-4.4.5-1.el8.x86_64\n  - nothing provides /usr/libexec/platform-python needed by mongodb-org-database-tools-extra-4.4.6-1.el8.x86_64\n  - nothing provides /usr/libexec/platform-python needed by mongodb-org-database-tools-extra-4.4.7-1.el8.x86_64\n(try to add '--skip-broken' to skip uninstallable packages)\n`\n```\ni had similar error for updating mongodb and i solved them by `sudo dnf upgrade mongodb-org-mongos --best --allowerasing`.\nbut still i have problem with mongodb tools",
      "solution": "I also had problems installing Mongodb on Fedora 33. These problems occurred when I had the following code in /etc/yum.repos.d/mongodb-org.repo :\n```\n`[Mongodb]\nname=MongoDB Repository\nbaseurl=https://repo.mongodb.org/yum/redhat/8/mongodb-org/4.4/x86_64/\ngpgcheck=1\nenabled=1\ngpgkey=https://www.mongodb.org/static/pgp/server-4.4.asc\n`\n```\nBut if I use this repository instead (i.e. replace the above code in /etc/yum.repos.d/mongodb-org.repo with the code below), it all works fine:\n```\n`[Mongodb]\nname=MongoDB Repository\nbaseurl=https://repo.mongodb.org/yum/amazon/2013.03/mongodb-org/4.4/x86_64\ngpgcheck=1\nenabled=1\ngpgkey=https://www.mongodb.org/static/pgp/server-4.4.asc\n`\n```\nNext install mongodb:\n```\n`sudo dnf install mongodb-org\n`\n```\nStart the service:\n```\n`sudo service start mongod\n`\n```\nAfter starting the service as above, you can then use the usual systemctl commands to stop, start and show status of the service. The above command to start the service is needed only once.\n```\n`sudo systemctl stop mongod\nsudo systemctl start mongod\nsudo systemctl status mongod\n`\n```\nFurther note on Fedora 34:\nThe above does not work on Fedora 34 as mongodb-org-shell dependencies on an older version of openssl causes problems:\n```\n`- nothing provides libcrypto.so.10()(64bit) needed by mongodb-org-shell-4.4.0-1.amzn1.x86_64\n- nothing provides libssl.so.10()(64bit) needed by mongodb-org-shell-4.4.0-1.amzn1.x86_64 \n`\n```\nI conclude from https://jira.mongodb.org/browse/SERVER-58870 that\nthe Mongodb team do not plan to support their product on Fedora going forward, as Mongodb 5.0 is also not supported on Fedora 34, although a workaround is suggested. I'm therefore going to consider other NoSQL options.",
      "question_score": 10,
      "answer_score": 7,
      "created_at": "2021-08-02T09:09:16",
      "url": "https://stackoverflow.com/questions/68617654/error-problem-nothing-provides-usr-libexec-platform-python-needed-by-mongodb"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 77659527,
      "title": "MongoDB via Brew Services &quot;undefined method `plist_startup&#39;&quot;",
      "problem": "When I run MongoDB, I usually start it manually (i.e. it's not part of my login startup items) and I'm good about stopping the service before I shutdown.\nI recently restarted my laptop and received an error upon running:\n`brew services run mongodb/brew/mongodb-community\n`\nThe error message reads:\n\nError: undefined method `plist_startup' for #\n\nI'm not entirely sure what happened. I haven't installed any major updates or made any modifications to my environment.\nWhat I've Tried\nI completely uninstalled MongoDB before installing it again:\n`# Uninstall each component of mongodb-community...\nbrew uninstall mongodb/brew/mongodb-community\nbrew uninstall mongodb/brew/mongodb-database-tools\nbrew uninstall mongosh\n\n# Reinstall all of the above...\nbrew install mongodb/brew/mongodb-community\n`\nCurrent Install\n`mongod --version`\n`db version v7.0.2\nBuild Info: {\n    \"version\": \"7.0.2\",\n    \"gitVersion\": \"02b3c655e1302209ef046da6ba3ef6749dd0b62a\",\n    \"modules\": [],\n    \"allocator\": \"system\",\n    \"environment\": {\n        \"distarch\": \"x86_64\",\n        \"target_arch\": \"x86_64\"\n    }\n}\n`\nCould be Homebrew...?\nI'll be honest, sometimes on these late nights, I'm on autopilot. I may or may not have run a `brew upgrade` at some point. I'll look into whether or not this happened. In the meantime, when I get the log for MongoDB in Homebrew, I don't even see a commit that would have impacted me:\n`brew log mongodb/brew/mongodb-community\n\n# Yields...\n\ncommit f33a59b6642f6a9f47f84b390dd71c386998cce6\nAuthor: Zack Winter \nDate:   Wed Oct 4 23:24:26 2023 +0000\n\n    Update paths with mr script\n\ncommit 7f3db6dbe9231300cc61645c68686a970b575f1f\nAuthor: Zack Winter \nDate:   Tue Oct 3 20:00:06 2023 +0000\n\n    SERVER-80537 update formulas for 7.0.1 release\n\ncommit 5055c34131148681885ea2241abccfc603596295\nAuthor: Alexander Neben \nDate:   Fri Aug 18 10:54:02 2023 -0700\n`",
      "solution": "Recently Homebrew updated to 4.2, which changed the way it starts services. It required the mongodb formula to explicitly define a service block, even if just points to the service file.\nThe fix was merged in https://github.com/mongodb/homebrew-brew/commit/2d0bfe19214d1f5071decd238b29306a6e82fff2",
      "question_score": 10,
      "answer_score": 2,
      "created_at": "2023-12-14T11:41:49",
      "url": "https://stackoverflow.com/questions/77659527/mongodb-via-brew-services-undefined-method-plist-startup"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 75577962,
      "title": "Can&#39;t install jenssegers/mongodb in Laravel 10",
      "problem": "I am new to Laravel. I want to connect to MongoDB using Laravel 10 which require jenssegers/mongodb to be installed. When I run command `composer require jenssegers/mongodb 3.8.0 --ignore-platform-reqs` in the terminal, I got error like this:\n```\n`PHP Warning:  PHP Startup: Unable to load dynamic library 'php_mongodb.dll' (tried: D:\\ProgramFiles\\xampp\\php\\ext\\php_mongodb.dll (The specified module could not be found), D:\\ProgramFiles\\xampp\\php\\ext\\php_php_mongodb.dll.dll (The specified module could not be found)) in Unknown on line 0\n\nWarning: PHP Startup: Unable to load dynamic library 'php_mongodb.dll' (tried: D:\\ProgramFiles\\xampp\\php\\ext\\php_mongodb.dll (The specified module could not be found), D:\\ProgramFiles\\xampp\\php\\ext\\php_php_mongodb.dll.dll (The specified module could not be found)) in Unknown on line 0\nInfo from https://repo.packagist.org: #StandWithUkraine\n./composer.json has been updated\nRunning composer update jenssegers/mongodb\nLoading composer repositories with package information\nUpdating dependencies\nYour requirements could not be resolved to an installable set of packages.\n\n  Problem 1\n    - Root composer.json requires jenssegers/mongodb 3.8.0 -> satisfiable by jenssegers/mongodb[v3.8.0].\n    - jenssegers/mongodb v3.8.0 requires illuminate/support ^8.0 -> found illuminate/support[v8.0.0, ..., v8.83.27] but these were not loaded, likely because it conflicts with another require.\n\nInstallation failed, reverting ./composer.json and ./composer.lock to their original content.\n`\n```\nNote that I use PHP 8.1.10 and download php_mongodb.dll (download from this link and it support PHP 7.3) to `php\\ext` folder and add `extension=php_mongodb.dll` to `php.ini` file but it look like this module could not be found as show in the result above.\nThis is my composer.json:\n```\n`{\n    \"name\": \"laravel/laravel\",\n    \"type\": \"project\",\n    \"description\": \"The Laravel Framework.\",\n    \"keywords\": [\"framework\", \"laravel\"],\n    \"license\": \"MIT\",\n    \"require\": {\n        \"php\": \"^8.1\",\n        \"guzzlehttp/guzzle\": \"^7.2\",\n        \"laravel/framework\": \"^10.0\",\n        \"laravel/sanctum\": \"^3.2\",\n        \"laravel/tinker\": \"^2.8\",\n        \"laravel/ui\": \"^4.2\"\n    },\n    \"require-dev\": {\n        \"fakerphp/faker\": \"^1.9.1\",\n        \"laravel/pint\": \"^1.0\",\n        \"laravel/sail\": \"^1.18\",\n        \"mockery/mockery\": \"^1.4.4\",\n        \"nunomaduro/collision\": \"^7.0\",\n        \"phpunit/phpunit\": \"^10.0\",\n        \"spatie/laravel-ignition\": \"^2.0\"\n    },\n    \"autoload\": {\n        \"psr-4\": {\n            \"App\\\\\": \"app/\",\n            \"Database\\\\Factories\\\\\": \"database/factories/\",\n            \"Database\\\\Seeders\\\\\": \"database/seeders/\"\n        }\n    },\n    \"autoload-dev\": {\n        \"psr-4\": {\n            \"Tests\\\\\": \"tests/\"\n        }\n    },\n    \"scripts\": {\n        \"post-autoload-dump\": [\n            \"Illuminate\\\\Foundation\\\\ComposerScripts::postAutoloadDump\",\n            \"@php artisan package:discover --ansi\"\n        ],\n        \"post-update-cmd\": [\n            \"@php artisan vendor:publish --tag=laravel-assets --ansi --force\"\n        ],\n        \"post-root-package-install\": [\n            \"@php -r \\\"file_exists('.env') || copy('.env.example', '.env');\\\"\"\n        ],\n        \"post-create-project-cmd\": [\n            \"@php artisan key:generate --ansi\"\n        ]\n    },\n    \"extra\": {\n        \"laravel\": {\n            \"dont-discover\": []\n        }\n    },\n    \"config\": {\n        \"optimize-autoloader\": true,\n        \"preferred-install\": \"dist\",\n        \"sort-packages\": true,\n        \"allow-plugins\": {\n            \"pestphp/pest-plugin\": true,\n            \"php-http/discovery\": true\n        }\n    },\n    \"minimum-stability\": \"stable\",\n    \"prefer-stable\": true\n}\n`\n```\nI had tried another command like `composer require jenssegers/mongodb:dev-develop --ignore-platform-reqs`, `composer require jenssegers/mongodb 3.8 --ignore-platform-reqs`, `composer require jenssegers/mongodb:* --ignore-platform-reqs`, `composer require jenssegers/mongodb:*` or similar but it didn't help.\nHow do I solve this problem?\nAny help would be appreciated.",
      "solution": "Update October 2023\nThe package now has a new version 4 that supports laravel 10. The package itself got renamed to `mongodb/laravel-mongodb` as the package was transferred over to MongoDB Inc. Link to repo: https://github.com/mongodb/laravel-mongodb\nSo to install it on laravel 10, you can run the following command assuming you have setup the correct PHP MongoDB extension version\n`composer require mongodb/laravel-mongodb\n`\nUpdate\nSo I later found out that the PHP MongoDB extension dll files can be downloaded directly from their github repo. So visit this link and you can get the MongoDB extension 1.15 for Windows ( Make sure to get the correct extension for your PHP version and that its thread safe. It's mentioned in the file name ). So downloading the MongoDB extension 1.15 will allow to overcome the 3rd and 4th issues.\nSo if you use the 1.15 version of the mongodb extension, running\n`composer require jenssegers/mongodb:dev-master\n`\nshould be enough to fix composer issues and get your application running\nOld Answer\nAt this moment, here are the issues\n\nYou are using laravel 10. The `jenssegers/mongodb` package currently (at the time of writing this answer ) does not have any stable versions that support laravel 10 ( There is a new released planned soon with support for laravel 10, but no release date announced ). So you would need to use their master branch which has added support for laravel 10 ( master branch is still undergoing development, so expect breaking changes ). You can install the master branch code using  `composer require jenssegers/mongodb:dev-master`\n\nYou are using PHP 8.1, but you installed the MongoDB extension for PHP7.3, and they are not compatible. You need to download a compatible version, preferable version 1.13 ( the latest windows version ). You can download it from here https://pecl.php.net/package/mongodb/1.13.0/windows ( Be sure to download the thread safe version )\n\n`jenssegers/mongodb` will try to install the latest version of `mongodb/mongodb`, which requires the PHP extension version 1.15. Unfortunately, version 1.15 is not yet released for windows, so you will have to use version 1.12 of the package which will run on version 1.13 of the extension  `composer require mongodb/mongodb:^1.12 jenssegers/mongodb:dev-master`\n\nNow the last issue is that the master branch of `jenssegers/mongodb` also requires MongoDB extension 1.15. Unfortunately, there's probably no safe way around this. You could run  `composer require mongodb/mongodb:^1.12 jenssegers/mongodb:dev-master --ignore-platform-reqs`  and get it to install, but it may or may not work in all cases.\n\nSo the options you have are\n\nUse WSL2 or docker to get a linux environment. You can then install the 1.15 version of the PHP extension and you can just run  `composer require jenssegers/mongodb:dev-master`  and get it to work\nStick to windows and downgrade to laravel 9. Then you may run  `composer require mongodb/mongodb:^1.12 jenssegers/mongodb`\nIgnore the platform requirements and hope that it works and run  `composer require mongodb/mongodb:^1.12 jenssegers/mongodb:dev-master --ignore-platform-reqs`\n\nHope this helps",
      "question_score": 10,
      "answer_score": 13,
      "created_at": "2023-02-27T08:55:11",
      "url": "https://stackoverflow.com/questions/75577962/cant-install-jenssegers-mongodb-in-laravel-10"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 73152973,
      "title": "Failed to start up WiredTiger under any compatibility version. This may be due to an unsupported upgrade or downgrade",
      "problem": "Getting this error on Ubuntu 20.04.\nI had origianlly installed the default version 3.0.6 on ubuntu.\nI have purged it and installed 5.0.9. But now I get this error in logs and mongod won't start.\n`\"Failed to start up WiredTiger under any compatibility version. This may be due to an unsupported upgrade or downgrade.\"`",
      "solution": "It's a massive gap between versions. You need to upgrade all versions in-between one at a time: https://www.mongodb.com/docs/manual/release-notes/5.0-upgrade-standalone/#upgrade-version-path:\n\nTo upgrade an existing MongoDB deployment to 5.0, you must be running a 4.4-series release.\nTo upgrade from a version earlier than the 4.4-series, you must successively upgrade major releases until you have upgraded to 4.4-series. For example, if you are running a 4.2-series, you must upgrade first to 4.4 before you can upgrade to 5.0.\n\nOr you can export data from 3.0.6 , start 5.0.9 with empty data directory (Storage.dbPath option in config), import data to 5.0.9",
      "question_score": 10,
      "answer_score": 13,
      "created_at": "2022-07-28T14:32:02",
      "url": "https://stackoverflow.com/questions/73152973/failed-to-start-up-wiredtiger-under-any-compatibility-version-this-may-be-due-t"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 69237272,
      "title": "Document Read and insert with locking/transaction in nodejs with mongodb",
      "problem": "In reservation system, only 5 different user can create bookings. If 100 user call booking api at same time than how to handle concurrency with locking. I am using nodejs with mongodb. I went through mongo concurrency article and transactions in mongodb, but cannot find any sample coding solution with locking.\nI have achieved solution with Optimistic concurrency control (when there is low contention for the resource - This can be easily implemented using versionNumber or timeStamp field).\nThank you in advance for suggesting me solution with locking.\nNow the algorithm is:\nStep 1: Get userAllowedNumber from userSettings collection.\n`//Query\ndb.getCollection('userSettings').find({})\n\n//Response Data\n{ \"userAllowedNumber\": 5 }\n`\nStep 2, Get current bookedCount from bookings collection.\n`//Query\ndb.getCollection('bookings').count({ })\n\n//Response Data\n2\n`\nStep 3, if `bookedCount \n`//Query\ndb.getCollection('bookings').create({ user_id: \"usr_1\" })\n`",
      "solution": "I had indepth discussion about locking with transaction in mongodb community. In the conclusion, I learn and found the limitation of transaction. There is no lock we can use to handle concurrent request for this task.\nYou can see the full Mongodb community conversation at this link\nhttps://www.mongodb.com/community/forums/t/implementing-locking-in-transaction-for-read-and-write/127845\nGithub demo code with Jmeter testing shows the limitation and not able to handle concurrent request for this task.\nhttps://github.com/naisargparmar/concurrencyMongo\nNew suggestion are still welcome and appreciate",
      "question_score": 10,
      "answer_score": 4,
      "created_at": "2021-09-18T20:10:28",
      "url": "https://stackoverflow.com/questions/69237272/document-read-and-insert-with-locking-transaction-in-nodejs-with-mongodb"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 70730514,
      "title": "Unable to connect to the database. Retrying",
      "problem": "I'm trying to connect to the database, seems like the set-up is correct, but for some reason, it says that it is not available.\n`app.module.ts`\n```\n`import { Module } from \"@nestjs/common\"\nimport { MongooseModule } from \"@nestjs/mongoose\";\nimport { ConfigModule } from \"../config\";\nimport { CreatorModule } from \"./creator.module\";\n\n@Module({\n    imports: [\n        MongooseModule.forRoot('mongodb://localhost:27017/snaptoon', {\n            useCreateIndex: true,\n            useUnifiedTopology: true,\n            useNewUrlParser: true,\n        }),\n        CreatorModule,\n    ],\n    controllers: [],\n    providers: []\n})\n\nexport class AppModule {}\n`\n```\nThe error is: `ERROR [MongooseModule] Unable to connect to the database. Retrying (9)...`\nI'm using `'@nestjs/mongoose': '9.0.2'`",
      "solution": "I solved by updating manually mongoose version to 6.2.2\n```\n` WARN @nestjs/mongoose@9.0.2 requires a peer of mongoose@^6.0.2 but none is installed. You must install peer dependencies yourself.\n`\n```\nI realize due to this error on npm install\njust use:\n```\n` npm install mongoose@6.2.2 --save\n`\n```",
      "question_score": 9,
      "answer_score": 5,
      "created_at": "2022-01-16T14:32:11",
      "url": "https://stackoverflow.com/questions/70730514/unable-to-connect-to-the-database-retrying"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 68242250,
      "title": "Unable to run mongo express with docker compose",
      "problem": "This is my first post in node js and docker so bear with me. I am running a mongo and mongo express container with the docker-compose but mongo express is not running. When I run mongo and mongo express without docker-compose it works perfectly. So, I think I am having some issues in docker-compose or maybe node js code\ndocker-compose.yaml\n```\n`version: '3'\nservices:\n  mongodb:\n    image: mongo\n    ports:\n      - 27017:27017\n    environment:\n      - MONGO_INITDB_ROOT_USERNAME=admin\n      - MONGO_INITDB_ROOT_PASSWORD=password\n  mongo-express:\n    image: mongo-express\n    ports:\n      - 8080:8081\n    environment:\n      - ME_CONFIG_MONGODB_ADMINUSERNAME=admin\n      - ME_CONFIG_MONGODB_ADMINPASSWORD=password\n      - ME_CONFIG_MONGODB_SERVER=mongodb\n`\n```\nserver.js\n```\n`let express = require('express');\nlet path = require('path');\nlet fs = require('fs');\nlet MongoClient = require('mongodb').MongoClient;\nlet bodyParser = require('body-parser');\nlet app = express();\n\napp.use(bodyParser.urlencoded({\n  extended: true\n}));\napp.use(bodyParser.json());\n\napp.get('/', function (req, res) {\n    res.sendFile(path.join(__dirname, \"index.html\"));\n  });\n\napp.get('/profile-picture', function (req, res) {\n  let img = fs.readFileSync(path.join(__dirname, \"images/profile-1.jpg\"));\n  res.writeHead(200, {'Content-Type': 'image/jpg' });\n  res.end(img, 'binary');\n});\n\n// use when starting application locally\nlet mongoUrlLocal = \"mongodb://admin:password@localhost:27017\";\n\n// use when starting application as docker container\nlet mongoUrlDocker = \"mongodb://admin:password@mongodb\";\n\n// pass these options to mongo client connect request to avoid DeprecationWarning for current Server Discovery and Monitoring engine\nlet mongoClientOptions = { useNewUrlParser: true, useUnifiedTopology: true };\n\n// \"user-account\" in demo with docker. \"my-db\" in demo with docker-compose\nlet databaseName = \"my-db\";\n\napp.post('/update-profile', function (req, res) {\n  let userObj = req.body;\n\n  MongoClient.connect(mongoUrlLocal, mongoClientOptions, function (err, client) {\n    if (err) throw err;\n\n    let db = client.db(databaseName);\n    userObj['userid'] = 1;\n\n    let myquery = { userid: 1 };\n    let newvalues = { $set: userObj };\n\n    db.collection(\"users\").updateOne(myquery, newvalues, {upsert: true}, function(err, res) {\n      if (err) throw err;\n      client.close();\n    });\n\n  });\n  // Send response\n  res.send(userObj);\n});\n\napp.get('/get-profile', function (req, res) {\n  let response = {};\n  // Connect to the db\n  MongoClient.connect(mongoUrlLocal, mongoClientOptions, function (err, client) {\n    if (err) throw err;\n\n    let db = client.db(databaseName);\n\n    let myquery = { userid: 1 };\n\n    db.collection(\"users\").findOne(myquery, function (err, result) {\n      if (err) throw err;\n      response = result;\n      client.close();\n\n      // Send response\n      res.send(response ? response : {});\n    });\n  });\n});\n\napp.listen(3000, function () {\n  console.log(\"app listening on port 3000!\");\n});\n`\n```\nIf I run docker ps I can only see mongo is running\n```\n`CONTAINER ID   IMAGE     COMMAND                  CREATED          STATUS          PORTS                                           NAMES\nd20c4784d316   mongo     \"docker-entrypoint.s\u2026\"   43 seconds ago   Up 38 seconds   0.0.0.0:27017->27017/tcp, :::27017->27017/tcp   nodeapplications_mongodb_1\n`\n```\nAnd when I run my docker-compose with the below command I see this log where I suspect an issue. Any help is appreciated\ndocker-compose -f docker-compose.yaml up\nLogs\n```\n`mongo-express_1  | Welcome to mongo-express\nmongo-express_1  | ------------------------\nmongo-express_1  | \nmongo-express_1  | \nmongo-express_1  | (node:7) [MONGODB DRIVER] Warning: Current Server Discovery and Monitoring engine is deprecated, and will be removed in a future version. To use the new Server Discover and Monitoring engine, pass option { useUnifiedTopology: true } to the MongoClient constructor.\nmongo-express_1  | Could not connect to database using connectionString: mongodb://admin:password@mongodb:27017/\"\nmongo-express_1  | (node:7) UnhandledPromiseRejectionWarning: MongoNetworkError: failed to connect to server [mongodb:27017] on first connect [Error: connect ECONNREFUSED 172.19.0.3:27017\nmongo-express_1  |     at TCPConnectWrap.afterConnect [as oncomplete] (net.js:1144:16) {\nmongo-express_1  |   name: 'MongoNetworkError'\nmongo-express_1  | }]\nmongo-express_1  |     at Pool. (/node_modules/mongodb/lib/core/topologies/server.js:438:11)\nmongo-express_1  |     at Pool.emit (events.js:314:20)\nmongo-express_1  |     at /node_modules/mongodb/lib/core/connection/pool.js:562:14\nmongo-express_1  |     at /node_modules/mongodb/lib/core/connection/pool.js:995:11\nmongo-express_1  |     at /node_modules/mongodb/lib/core/connection/connect.js:32:7\nmongo-express_1  |     at callback (/node_modules/mongodb/lib/core/connection/connect.js:280:5)\nmongo-express_1  |     at Socket. (/node_modules/mongodb/lib/core/connection/connect.js:310:7)\nmongo-express_1  |     at Object.onceWrapper (events.js:421:26)\nmongo-express_1  |     at Socket.emit (events.js:314:20)\nmongo-express_1  |     at emitErrorNT (internal/streams/destroy.js:92:8)\nmongo-express_1  |     at emitErrorAndCloseNT (internal/streams/destroy.js:60:3)\nmongo-express_1  |     at processTicksAndRejections (internal/process/task_queues.js:84:21)\nmongo-express_1  | (node:7) UnhandledPromiseRejectionWarning: Unhandled promise rejection. This error originated either by throwing inside of an async function without a catch block, or by rejecting a promise which was not handled with .catch(). To terminate the node process on unhandled promise rejection, use the CLI flag `--unhandled-rejections=strict` (see https://nodejs.org/api/cli.html#cli_unhandled_rejections_mode). (rejection id: 1)\n`\n```\nAs suggested by @Blunderchips Update 1\nserver.js\n```\n`let express = require('express');\nlet path = require('path');\nlet fs = require('fs');\nlet MongoClient = require('mongodb').MongoClient;\nlet bodyParser = require('body-parser');\nlet app = express();\n\nconst dbServer = process.env.ME_CONFIG_MONGODB_SERVER;\nconst dbPassword = process.env.ME_CONFIG_MONGODB_ADMINPASSWORD;\nconst dbUserName = process.env.ME_CONFIG_MONGODB_ADMINUSERNAME;\nconst dbPort = process.env.ME_CONFIG_MONGODB_PORT;\n\napp.use(bodyParser.urlencoded({\n  extended: true\n}));\napp.use(bodyParser.json());\n\napp.get('/', function (req, res) {\n    res.sendFile(path.join(__dirname, \"index.html\"));\n  });\n\napp.get('/profile-picture', function (req, res) {\n  let img = fs.readFileSync(path.join(__dirname, \"images/profile-1.jpg\"));\n  res.writeHead(200, {'Content-Type': 'image/jpg' });\n  res.end(img, 'binary');\n});\n\n// use when starting application locally\n//let mongoUrlLocal = \"mongodb://admin:password@localhost:27017\";\n\n// use when starting application as docker container\nlet mongoUrlDocker = `mongodb://${dbUserName}:${dbPassword}@${dbServer}:${dbPort}`;//\"mongodb://admin:password@mongodb:27017\";//\"mongodb://admin:password@mongodb\";\n\n// pass these options to mongo client connect request to avoid DeprecationWarning for current Server Discovery and Monitoring engine\nlet mongoClientOptions = { useNewUrlParser: true, useUnifiedTopology: true };\n\n// \"user-account\" in demo with docker. \"my-db\" in demo with docker-compose\nlet databaseName = \"my-db\";\n\napp.post('/update-profile', function (req, res) {\n  let userObj = req.body;\n\n  MongoClient.connect(mongoUrlDocker, mongoClientOptions, function (err, client) {\n    if (err) throw err;\n\n    let db = client.db(databaseName);\n    userObj['userid'] = 1;\n\n    let myquery = { userid: 1 };\n    let newvalues = { $set: userObj };\n\n    db.collection(\"users\").updateOne(myquery, newvalues, {upsert: true}, function(err, res) {\n      if (err) throw err;\n      client.close();\n    });\n\n  });\n  // Send response\n  res.send(userObj);\n});\n\napp.get('/get-profile', function (req, res) {\n  let response = {};\n  // Connect to the db\n  MongoClient.connect(mongoUrlDocker, mongoClientOptions, function (err, client) {\n    if (err) throw err;\n\n    let db = client.db(databaseName);\n\n    let myquery = { userid: 1 };\n\n    db.collection(\"users\").findOne(myquery, function (err, result) {\n      if (err) throw err;\n      response = result;\n      client.close();\n\n      // Send response\n      res.send(response ? response : {});\n    });\n  });\n});\n\napp.listen(3000, function () {\n  console.log(\"app listening on port 3000!\");\n});\n`\n```\ndocker-compose.yaml\n```\n`version: '3'\nservices:\n  mongodb:\n    image: mongo\n    ports:\n      - 27017:27017\n    environment:\n      - MONGO_INITDB_ROOT_USERNAME=admin\n      - MONGO_INITDB_ROOT_PASSWORD=password\n  mongo-express:\n    image: mongo-express\n    ports:\n      - 8080:8081\n    environment:\n      - ME_CONFIG_MONGODB_ADMINUSERNAME=admin\n      - ME_CONFIG_MONGODB_ADMINPASSWORD=password\n      - ME_CONFIG_MONGODB_SERVER=mongodb\n    links: \n        - mongodb:mongodb\n`\n```\nI still can't see mongo express running\n```\n`docker ps\nCONTAINER ID   IMAGE     COMMAND                  CREATED              STATUS              PORTS                                           NAMES\n23428dc0c3a1   mongo     \"docker-entrypoint.s\u2026\"   About a minute ago   Up About a minute   0.0.0.0:27017->27017/tcp, :::27017->27017/tcp   nodeapplications_mongodb_1\n`\n```\nLogs\n```\n`mongo-express_1  | Welcome to mongo-express\nmongo-express_1  | ------------------------\nmongo-express_1  | \nmongo-express_1  | \nmongo-express_1  | (node:7) [MONGODB DRIVER] Warning: Current Server Discovery and Monitoring engine is deprecated, and will be removed in a future version. To use the new Server Discover and Monitoring engine, pass option { useUnifiedTopology: true } to the MongoClient constructor.\nmongo-express_1  | Could not connect to database using connectionString: mongodb://admin:password@mongodb:27017/\"\nmongo-express_1  | (node:7) UnhandledPromiseRejectionWarning: MongoNetworkError: failed to connect to server [mongodb:27017] on first connect [Error: connect ECONNREFUSED 172.23.0.2:27017\nmongo-express_1  |     at TCPConnectWrap.afterConnect [as oncomplete] (net.js:1144:16) {\nmongo-express_1  |   name: 'MongoNetworkError'\nmongo-express_1  | }]\nmongo-express_1  |     at Pool. (/node_modules/mongodb/lib/core/topologies/server.js:438:11)\nmongo-express_1  |     at Pool.emit (events.js:314:20)\nmongo-express_1  |     at /node_modules/mongodb/lib/core/connection/pool.js:562:14\nmongo-express_1  |     at /node_modules/mongodb/lib/core/connection/pool.js:995:11\nmongo-express_1  |     at /node_modules/mongodb/lib/core/connection/connect.js:32:7\nmongo-express_1  |     at callback (/node_modules/mongodb/lib/core/connection/connect.js:280:5)\nmongo-express_1  |     at Socket. (/node_modules/mongodb/lib/core/connection/connect.js:310:7)\nmongo-express_1  |     at Object.onceWrapper (events.js:421:26)\nmongo-express_1  |     at Socket.emit (events.js:314:20)\nmongo-express_1  |     at emitErrorNT (internal/streams/destroy.js:92:8)\nmongo-express_1  |     at emitErrorAndCloseNT (internal/streams/destroy.js:60:3)\nmongo-express_1  |     at processTicksAndRejections (internal/process/task_queues.js:84:21)\nmongo-express_1  | (node:7) UnhandledPromiseRejectionWarning: Unhandled promise rejection. This error originated either by throwing inside of an async function without a catch block, or by rejecting a promise which was not handled with .catch(). To terminate the node process on unhandled promise rejection, use the CLI flag `--unhandled-rejections=strict` (see https://nodejs.org/api/cli.html#cli_unhandled_rejections_mode). (rejection id: 1)\nmongo-express_1  | (node:7) [DEP0018] DeprecationWarning: Unhandled promise rejections are deprecated. In the future, promise rejections that are not handled will terminate the Node.js process with a non-zero exit code.\n`\n```\nUpdate 2\n```\n`let express = require('express');\nlet path = require('path');\nlet fs = require('fs');\nlet MongoClient = require('mongodb').MongoClient;\nlet bodyParser = require('body-parser');\nlet app = express();\n\napp.use(bodyParser.urlencoded({\n  extended: true\n}));\napp.use(bodyParser.json());\n\napp.get('/', function (req, res) {\n    res.sendFile(path.join(__dirname, \"index.html\"));\n  });\n\napp.get('/profile-picture', function (req, res) {\n  let img = fs.readFileSync(path.join(__dirname, \"images/profile-1.jpg\"));\n  res.writeHead(200, {'Content-Type': 'image/jpg' });\n  res.end(img, 'binary');\n});\n\n// use when starting application locally\n//let mongoUrlLocal = \"mongodb://admin:password@localhost:27017\";\n\n// use when starting application as docker container\nlet mongoUrlDocker = \"mongodb://admin:password@mongodb:27017\";\n\n// pass these options to mongo client connect request to avoid DeprecationWarning for current Server Discovery and Monitoring engine\nlet mongoClientOptions = { useNewUrlParser: true, useUnifiedTopology: true };\n\n// \"user-account\" in demo with docker. \"my-db\" in demo with docker-compose\nlet databaseName = \"my-db\";\n\napp.post('/update-profile', function (req, res) {\n  let userObj = req.body;\n\n  MongoClient.connect(mongoUrlDocker, mongoClientOptions, function (err, client) {\n    if (err) throw err;\n\n    let db = client.db(databaseName);\n    userObj['userid'] = 1;\n\n    let myquery = { userid: 1 };\n    let newvalues = { $set: userObj };\n\n    db.collection(\"users\").updateOne(myquery, newvalues, {upsert: true}, function(err, res) {\n      if (err) throw err;\n      client.close();\n    });\n\n  });\n  // Send response\n  res.send(userObj);\n});\n\napp.get('/get-profile', function (req, res) {\n  let response = {};\n  // Connect to the db\n  MongoClient.connect(mongoUrlDocker, mongoClientOptions, function (err, client) {\n    if (err) throw err;\n\n    let db = client.db(databaseName);\n\n    let myquery = { userid: 1 };\n\n    db.collection(\"users\").findOne(myquery, function (err, result) {\n      if (err) throw err;\n      response = result;\n      client.close();\n\n      // Send response\n      res.send(response ? response : {});\n    });\n  });\n});\n\napp.listen(3000, function () {\n  console.log(\"app listening on port 3000!\");\n});\n`\n```\nUpdate 3\ndocker-compose\n```\n`version: '3'\nservices:\n  mongodb:\n    image: mongo\n    ports:\n      - 27017:27017\n    environment:\n      - MONGO_INITDB_ROOT_USERNAME=admin\n      - MONGO_INITDB_ROOT_PASSWORD=password\n  mongo-express:\n    image: mongo-express\n    ports:\n      - 8080:8081\n    environment:\n      - ME_CONFIG_MONGODB_ADMINUSERNAME=admin\n      - ME_CONFIG_MONGODB_ADMINPASSWORD=password\n      - ME_CONFIG_MONGODB_SERVER=mongodb\n    links: \n        - mongodb:mongodb\n    restart: on-failure\n`\n```\nFull Logs\n```\n` mongo-express_1  | Welcome to mongo-express\nmongo-express_1  | ------------------------\nmongo-express_1  | \nmongo-express_1  | \nmongodb_1        | {\"t\":{\"$date\":\"2021-07-04T10:41:58.806+00:00\"},\"s\":\"I\",  \"c\":\"STORAGE\",  \"id\":22318,   \"ctx\":\"SignalHandler\",\"msg\":\"Shutting down session sweeper thread\"}\nmongodb_1        | {\"t\":{\"$date\":\"2021-07-04T10:41:58.806+00:00\"},\"s\":\"I\",  \"c\":\"STORAGE\",  \"id\":22319,   \"ctx\":\"SignalHandler\",\"msg\":\"Finished shutting down session sweeper thread\"}\nmongodb_1        | {\"t\":{\"$date\":\"2021-07-04T10:41:58.807+00:00\"},\"s\":\"I\",  \"c\":\"STORAGE\",  \"id\":22322,   \"ctx\":\"SignalHandler\",\"msg\":\"Shutting down checkpoint thread\"}\nmongodb_1        | {\"t\":{\"$date\":\"2021-07-04T10:41:58.807+00:00\"},\"s\":\"I\",  \"c\":\"STORAGE\",  \"id\":22323,   \"ctx\":\"SignalHandler\",\"msg\":\"Finished shutting down checkpoint thread\"}\nmongodb_1        | {\"t\":{\"$date\":\"2021-07-04T10:41:58.807+00:00\"},\"s\":\"I\",  \"c\":\"STORAGE\",  \"id\":4795902, \"ctx\":\"SignalHandler\",\"msg\":\"Closing WiredTiger\",\"attr\":{\"closeConfig\":\"leak_memory=true,\"}}\nmongodb_1        | {\"t\":{\"$date\":\"2021-07-04T10:41:58.810+00:00\"},\"s\":\"I\",  \"c\":\"STORAGE\",  \"id\":22430,   \"ctx\":\"SignalHandler\",\"msg\":\"WiredTiger message\",\"attr\":{\"message\":\"[1625395318:810568][28:0x7f50eec9b700], close_ckpt: [WT_VERB_CHECKPOINT_PROGRESS] saving checkpoint snapshot min: 48, snapshot max: 48 snapshot count: 0, oldest timestamp: (0, 0) , meta checkpoint timestamp: (0, 0)\"}}\nmongo-express_1  | (node:7) [MONGODB DRIVER] Warning: Current Server Discovery and Monitoring engine is deprecated, and will be removed in a future version. To use the new Server Discover and Monitoring engine, pass option { useUnifiedTopology: true } to the MongoClient constructor.\nmongo-express_1  | Could not connect to database using connectionString: mongodb://admin:password@mongodb:27017/\"\nmongo-express_1  | (node:7) UnhandledPromiseRejectionWarning: MongoNetworkError: failed to connect to server [mongodb:27017] on first connect [Error: connect ECONNREFUSED 172.27.0.2:27017\nmongo-express_1  |     at TCPConnectWrap.afterConnect [as oncomplete] (net.js:1144:16) {\nmongo-express_1  |   name: 'MongoNetworkError'\nmongo-express_1  | }]\nmongo-express_1  |     at Pool. (/node_modules/mongodb/lib/core/topologies/server.js:438:11)\nmongo-express_1  |     at Pool.emit (events.js:314:20)\nmongo-express_1  |     at /node_modules/mongodb/lib/core/connection/pool.js:562:14\nmongo-express_1  |     at /node_modules/mongodb/lib/core/connection/pool.js:995:11\nmongo-express_1  |     at /node_modules/mongodb/lib/core/connection/connect.js:32:7\nmongo-express_1  |     at callback (/node_modules/mongodb/lib/core/connection/connect.js:280:5)\nmongo-express_1  |     at Socket. (/node_modules/mongodb/lib/core/connection/connect.js:310:7)\nmongo-express_1  |     at Object.onceWrapper (events.js:421:26)\nmongo-express_1  |     at Socket.emit (events.js:314:20)\nmongo-express_1  |     at emitErrorNT (internal/streams/destroy.js:92:8)\nmongo-express_1  |     at emitErrorAndCloseNT (internal/streams/destroy.js:60:3)\nmongo-express_1  |     at processTicksAndRejections (internal/process/task_queues.js:84:21)\nmongo-express_1  | (node:7) UnhandledPromiseRejectionWarning: Unhandled promise rejection. This error originated either by throwing inside of an async function without a catch block, or by rejecting a promise which was not handled with .catch(). To terminate the node process on unhandled promise rejection, use the CLI flag `--unhandled-rejections=strict` (see https://nodejs.org/api/cli.html#cli_unhandled_rejections_mode). (rejection id: 1)\nmongo-express_1  | (node:7) [DEP0018] DeprecationWarning: Unhandled promise rejections are deprecated. In the future, promise rejections that are not handled will terminate the Node.js process with a non-zero exit code.\nmongodb_1        | {\"t\":{\"$date\":\"2021-07-04T10:42:00.871+00:00\"},\"s\":\"I\",  \"c\":\"STORAGE\",  \"id\":4795901, \"ctx\":\"SignalHandler\",\"msg\":\"WiredTiger closed\",\"attr\":{\"durationMillis\":2064}}\nmongodb_1        | {\"t\":{\"$date\":\"2021-07-04T10:42:00.871+00:00\"},\"s\":\"I\",  \"c\":\"STORAGE\",  \"id\":22279,   \"ctx\":\"SignalHandler\",\"msg\":\"shutdown: removing fs lock...\"}\nmongodb_1        | {\"t\":{\"$date\":\"2021-07-04T10:42:00.872+00:00\"},\"s\":\"I\",  \"c\":\"-\",        \"id\":4784931, \"ctx\":\"SignalHandler\",\"msg\":\"Dropping the scope cache for shutdown\"}\nmongodb_1        | {\"t\":{\"$date\":\"2021-07-04T10:42:00.873+00:00\"},\"s\":\"I\",  \"c\":\"FTDC\",     \"id\":4784926, \"ctx\":\"SignalHandler\",\"msg\":\"Shutting down full-time data capture\"}\nmongodb_1        | {\"t\":{\"$date\":\"2021-07-04T10:42:00.873+00:00\"},\"s\":\"I\",  \"c\":\"FTDC\",     \"id\":20626,   \"ctx\":\"SignalHandler\",\"msg\":\"Shutting down full-time diagnostic data capture\"}\nmongodb_1        | {\"t\":{\"$date\":\"2021-07-04T10:42:00.878+00:00\"},\"s\":\"I\",  \"c\":\"CONTROL\",  \"id\":20565,   \"ctx\":\"SignalHandler\",\"msg\":\"Now exiting\"}\nmongodb_1        | {\"t\":{\"$date\":\"2021-07-04T10:42:00.879+00:00\"},\"s\":\"I\",  \"c\":\"CONTROL\",  \"id\":23138,   \"ctx\":\"SignalHandler\",\"msg\":\"Shutting down\",\"attr\":{\"exitCode\":0}}\nnodeapplications_mongo-express_1 exited with code 0\nmongodb_1        | \nmongodb_1        | MongoDB init process complete; ready for start up.\n`\n```",
      "solution": "As suggested by @David Maze by adding restart: unless-stopped it worked\n```\n`version: '3'\nservices:\n  mongodb:\n    image: mongo\n    ports:\n      - 27017:27017\n    environment:\n      - MONGO_INITDB_ROOT_USERNAME=admin\n      - MONGO_INITDB_ROOT_PASSWORD=password\n  mongo-express:\n    image: mongo-express\n    ports:\n      - 8081:8081\n    environment:\n      - ME_CONFIG_MONGODB_ADMINUSERNAME=admin\n      - ME_CONFIG_MONGODB_ADMINPASSWORD=password\n      - ME_CONFIG_MONGODB_SERVER=mongodb\n    restart: unless-stopped\n`\n```",
      "question_score": 9,
      "answer_score": 19,
      "created_at": "2021-07-04T08:24:11",
      "url": "https://stackoverflow.com/questions/68242250/unable-to-run-mongo-express-with-docker-compose"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 67042963,
      "title": "Cannot overwrite mongoose model once compiled with Nextjs",
      "problem": "Before you close this question, I have read several forums that have the same question as I have but my issue is way different. Even when Im not trying to do anything, even save a model, it still gives me an error of:\n```\n`cannot overwrite \"mongoose\" model once compiled\n`\n```\nI have a feeling there is something wrong with my schemas because when it was still simpler, it worked fine but as I tried to make it more complex it started to give me that error. Here is my mongoose code:\n```\n`import mongoose from 'mongoose'\n\nconst flashcardItemSchema = new mongoose.Schema({\n    term: {\n        type:String,\n        required: true,\n        min:1\n    },\n    description: {\n        type:String,\n        required:true,\n        min:1\n    }\n});\nconst FlashcardItem = mongoose.model(\"flashcardItem\", flashcardItemSchema);\n\nconst flashcardSetSchema = new mongoose.Schema({\n    title: {\n        type: String,\n        min: 1,\n    },\n    flashcards:[flashcardItemSchema],\n})\n\nconst FlashcardSet = mongoose.model('flashcardSet', flashcardSetSchema )\n\nexport {FlashcardItem, FlashcardSet}\n`\n```\nI connect to my database when the server runs, so it doesn't disconnect from time to time.\nUPDATE\nI realized that I'm using nextjs builtin api, meaning the api directory is inside the page directory. I only get the error once the pages get recompiled.",
      "solution": "So it turns out that the error came from nextjs trying to remake the model every render. There is an answer here: Mongoose/NextJS - Model is not defined / Cannot overwrite model once compiled\nbut I thought the code was too long and all that fixed mine was just a single line. When trying to save a model in nextjs, it should be written like this:\n```\n`const modelName = mongoose.models.modelName || mongoose.model('modelName', flashcardSetSchema )  \n`\n```",
      "question_score": 9,
      "answer_score": 21,
      "created_at": "2021-04-11T10:41:52",
      "url": "https://stackoverflow.com/questions/67042963/cannot-overwrite-mongoose-model-once-compiled-with-nextjs"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 72480395,
      "title": "Connecting to AWS documentDB using mongoose giving error: &#39;MongoParseError: option ssl_ca_certs is not supported&#39;",
      "problem": "Hello I am just trying to connect to my documentDb using mongoose; hosted on AWS. From my local pc, I am try to do it like:\n```\n`const URI = 'mongodb://username:npassword@docdb-2022-05-31-18-46-43.cluster-cnyrbefiq91q.eu-west-2.docdb.amazonaws.com:27017/?ssl=true&replicaSet=rs0&readPreference=secondaryPreferred&retryWrites=false'\n\nmongoose.connect(URI, { useNewUrlParser: true , ssl: true});\n`\n```\nI am getting the error:\n```\n`MongoParseError: option ssl_ca_certs is not supported\n`\n```\nCan any1 explain to me what I am doing wrong?",
      "solution": "DocumentDB supports TLS protocol. It worked for me when I\n1/ downloaded the TLS public key using\n```\n`wget https://s3.amazonaws.com/rds-downloads/rds-combined-ca-bundle.pem\n`\n```\n2/ changed the parameter `ssl=true` to `tls=true` in the connection string,\n3/ and updated the params to connect() method.\n```\n`const uri = 'mongodb://:@:27017/?tls=true&replicaSet=rs0&readPreference=secondaryPreferred&retryWrites=false'\nconst client = new MongoClient(uri,{tlsCAFile: `rds-combined-ca-bundle.pem`});\n\ntry{\n    await client.connect();\n}\n`\n```\nI referred to this DocumentDB documentation.",
      "question_score": 9,
      "answer_score": 13,
      "created_at": "2022-06-02T19:52:41",
      "url": "https://stackoverflow.com/questions/72480395/connecting-to-aws-documentdb-using-mongoose-giving-error-mongoparseerror-opti"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 74221005,
      "title": "Mongodb connection failed in local with node version 18.12.0?",
      "problem": "I am seeing one issue with node version `18.12.0` and mongodb `6.0.2`. I already build a nestjs application with mongodb. Here I use `@nestjs/mongoose(v- 9.0.2)` and `mongoose (v-6.7.0)`\nHere I can see that when I upgrade node js to latest lts version then I am not able to connect to mongodb. It show an error like `unable to connect to database`.\nBut When I downgrade to node version `16.18.0` then it working fine. My question is that you guys already face this issue or I am only person getting this issue. If you know that then actually where is the problem occurred?\nHere is my connection code-\n```\n`MongooseModule.forRoot(\"mongodb://localhost:27017/nekmart\", {\n      connectionFactory: (connection) => {\n        connection.plugin(slug, { number: true });\n        return connection\n      }\n}),\n`\n```",
      "solution": "Had same problem after upgrading to NodeJs 18.12.1; followed other blogs/comments and apparently\nchanging the Uri from `mongodb://localhost:27017/test_db` to `mongodb://127.0.0.1:27017/test_db` works.\nWithout getting into the specifics reasons it appears that `localhost` is rejected due to some changes in NodeJS.",
      "question_score": 9,
      "answer_score": 14,
      "created_at": "2022-10-27T12:58:34",
      "url": "https://stackoverflow.com/questions/74221005/mongodb-connection-failed-in-local-with-node-version-18-12-0"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 69506203,
      "title": "MongoDB FindOptions when using Typescript",
      "problem": "I am converting a JS project to TS and am struggling with FindOptions when I am querying a collection. I just want to get the Ids of all elements of a collection. This is the TS code   that is causing TS errors:\n`import { Collection, Db, Document, MongoClient } from \"mongodb\";\n\ntype MyDoc = {\n    _id: object,\n    title: string,\n    image: string,\n    description: string\n} \n    \nconst client: MongoClient = await MongoClient.connect(\"mongodb://127.0.0.1:27017\");\nconst db: Db = client.db(\"mydb\");\nconst myCollection: Collection = db.collection(\"myCollection\");\nconst ids: object[] = await myCollection.find({}, { _id: 1 }).toArray();\nclient.close();\n`\nThe problem there is the `{_id: 1}` FindOption that is working fine in JS. This is the error message:\n```\n`Argument of type '{ _id: number; }' is not assignable to parameter of type 'FindOptions'.\n      Object literal may only specify known properties, and '_id' does not exist in type 'FindOptions'.\n`\n```",
      "solution": "From `find` (Mongo docs),\n`find(filter: Filter, options?: FindOptions): FindCursor\n`\nThe second parameter has to be `FindOptions` type.\nYou need to pass value of `FindOptions` type to second parameter and with FindOptions.projection to specify the return field(s).\n\nprojection: Projection\nThe fields to return in the query. Object of fields to either include or exclude (one of, not both), {'a':1, 'b': 1} or {'a': 0, 'b': 0}\n\n`const ids: object[] = await myCollection.find({}, { projection: { _id: 1 } }).toArray();\n`",
      "question_score": 9,
      "answer_score": 11,
      "created_at": "2021-10-09T13:13:25",
      "url": "https://stackoverflow.com/questions/69506203/mongodb-findoptions-when-using-typescript"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 68167039,
      "title": "How to check if key exists in MongoDB",
      "problem": "I am trying to check if a key exists in a MongoDB collection. Basically, I need to map an array of strings to a specific key. If the key exists, I want to update the list by adding a new value, otherwise create a new key with an initial value (If a new key is added, it will only be added with 1 value initially).\nI have found some examples online, though I haven't been able to get it to work locally. Here is my code below (I am using the official Go MongoDB driver):\n```\n`key:= \"some_key\"\ndatabase := client.Database(\"dbName\")\nkeysCollection := database.Collection(\"keys\")\nkeysCollection.Find(nil, {key:{$exists:true}});\n`\n```\nI'm running into 2 issues around this component `{key: {$exists:true}}`\n\n`invalid character U+0024 '$'`: when trying to check if the `key` exists, at `{$exists:true}`, though this seems to be what the MongoDB documentation supports to check if the key itself exists, without checking for an exact value mapped to it\n`syntax error: unexpected {, expecting expression`: at the beginning of `{key: {$exists:true}}`, it looks like I cannot pass a struct?\n\nThis is my first time working with MongoDB, and I have very little experience in GoLang, and am stuck on this seeming small issue.\nAm I going about this the right way? If so, how can I move past these issues?",
      "solution": "To check if a key exists in a collection the following are the queries in `mongo` shell and golang respectively. Assume a collection of sample documents as:\n```\n`{ _id: 1, arr: [ \"red\", \"blue\" ] }\n{ _id: 2  }\n`\n```\nThe query:\n```\n`db.collection.find( { \"arr\": { \"$exists\": true } } )\n`\n```\nSee the usage of $exists.\n```\n`var result bson.M\nfilter := bson.D{{ \"arr\", bson.D{{ \"$exists\", true }} }}\nerr = collection.FindOne(context.TODO(), filter).Decode(&result)\nif err != nil {\n    log.Fatal(err)\n}\nfmt.Printf(\"Found a single document: %+v\\n\", result)\n`\n```\n\nI am trying to check if a key exists in a MongoDB collection.  I need\nto map an array of strings to a specific key.  If the key exists, I\nwant to update the list by adding a new value, otherwise create a new\nkey with an initial value (If a new key is added, it will only be\nadded with 1 value initially).\n\nTo update a field based upon a condition, you need to use an Update With Aggregation Pipeline. The following shell and golang queries update all documents with the condition - if the array field exists, it adds the value from `newValue` or if the field doesn''t exist, creates a new field `arr` with the value from`newValue`.\n```\n`var newValue = \"white\"\n\ndb.collection.updateMany(\n  { },\n  [ { \n       $set: { \n           arr: { \n               $concatArrays: [ \n                   { $ifNull: [ \"$arr\", [ ] ] }, \n                   [ newValue ] \n               ] \n           } \n       } \n  } ]\n)\n`\n```\nThe golang update:\n```\n`newValue := \"white\"\nfilter := bson.D{{}}\n\npipe := bson.D{{ \"$set\", bson.D{{ \"arr\", bson.D{{ \"$concatArrays\", bson.A{ bson.D{{\"$ifNull\",bson.A{ \"$arr\", bson.A{} }}}, bson.A{ newValue } } }} }} }}\n    \nupdateResult, errr := collection.UpdateMany(ctx, filter, mongo.Pipeline{ pipe })\n    \nif errr != nil {\n    log.Fatal(errr)\n}\nfmt.Printf(\"Matched %v documents and updated %v documents.\\n\", updateResult.MatchedCount, updateResult.ModifiedCount)\n`\n```",
      "question_score": 9,
      "answer_score": 10,
      "created_at": "2021-06-28T18:36:44",
      "url": "https://stackoverflow.com/questions/68167039/how-to-check-if-key-exists-in-mongodb"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 66374757,
      "title": "Top-level use of w, wtimeout, j, and fsync is deprecated. Use writeConcern instead",
      "problem": "I am using mongodb with loopback and on successful connection I am getting below warning\n```\n`Top-level use of w, wtimeout, j, and fsync is deprecated. Use writeConcern instead.\n`\n```\nloopback version: 5.5.0\nHow to get rid of this? Kindly help",
      "solution": "Apparently this warning was introduced in Mongo Driver v3.6.4 so the basis solution will be to down grade to v3.6.3. There is a PR that try to stop the warning outputs, and also a report on the Jira for the Node Driver. https://jira.mongodb.org/browse/NODE-3114\nUpdate:\nThe problem have been fixed with the version 3.6.5",
      "question_score": 9,
      "answer_score": 5,
      "created_at": "2021-02-25T19:55:31",
      "url": "https://stackoverflow.com/questions/66374757/top-level-use-of-w-wtimeout-j-and-fsync-is-deprecated-use-writeconcern-inste"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 73875169,
      "title": "Mongo Atlas Random Message: &quot;BadValue: SCRAM-SHA-256 authentication is disabled&quot;",
      "problem": "I'm getting the following random message on my Mongo Atlas \"Access History\" view:\n\"FAILED BadValue: SCRAM-SHA-256 authentication is disabled\"\nThis message is attached to my application IP hosted at AWS. I basically got this message first and then the success message at the same timestamp.\n\nInstance MongoAtlas - Cluster Tier M40 General\nReplica Set - 3 nodes\nDriver mongoose: \u201c^6.5.1\u201d\nMongo version as per Atlas view: 5.0.12\nMongo String URI: mongodb+srv://xyz:ABCDEFGH@clusterabc.abcde.mongodb.net/my_db\n\nIs that an issue while connecting to Mongo?",
      "solution": "This error `BadValue: SCRAM-SHA-256 authentication is disabled` caused by the Mongo-atlas does NOT support `SCRAM-SHA-256`, but does support `SCRAM-SHA-1`.\n\nNotably, MongoDB authentication protocols do not use SHA-1 as a raw hash function for passwords or digital signatures, but rather as an HMAC construction in, e.g., SASL SCRAM-SHA-1. While many common uses of SHA-1 have been deprecated or sunset by standards organizations, these do not typically apply to HMAC functions.\n\nGenerally speaking, this error happendd on the user `mms-automation` or ` mms-monitoring-agent`. Both of them are used for Atlas internal tasks including monitoring.\n\nThe source of this message is that mms-automation user initially attempts authentication using SCRAM-SHA-256 which Atlas doesn\u2019t support, causing the \u201cBadValue: SCRAM-SHA-256 authentication is disabled\u201d message, before falling back to SCRAM-SHA-1.\n\nNote that there is NO detrimental effect to the operation of the database, and this informational message is provided for your own auditing purposes.\n\nIf there is connection issue, you could change the authentication mechanism to `SCRAM-SHA-1`. In a connection string, specify `authMechanism=SCRAM-SHA-1`\n```\n`mongodb://username:password@host:port/database?authMechanism=SCRAM-SHA-1\n`\n```\n\nSource: https://www.mongodb.com/community/forums/t/badvalue-scram-sha-256-authentication-is-disabled/143254/4",
      "question_score": 9,
      "answer_score": 3,
      "created_at": "2022-09-28T03:01:11",
      "url": "https://stackoverflow.com/questions/73875169/mongo-atlas-random-message-badvalue-scram-sha-256-authentication-is-disabled"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 76321221,
      "title": "Error &quot;ImportError: cannot import name &#39;_get_object_size&#39; from &#39;bson&#39; &quot;",
      "problem": "when running the below file , I am getting error \"ImportError: cannot import name 'get_object_size' from 'bson' (C:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\bson_init.py)\"\ncode:\n`from flask import Flask, request, jsonify\nfrom flask_pymongo import PyMongo\n# from bson.objectid import ObjectId\n```\n`app = Flask(__name__)\napp.config['MONGO_URI'] = connectionstring\nmongo = PyMongo(app)\n\n# Create a new to-do item\n@app.route('/api/todo', methods=['POST'])\ndef create_todo():\n    # data = request.json\n\n    # Create a new to-do item in the database\n new_todo = {\n        # 'task': data['task'],\n        # 'due_date': data['due_date'],\n        # 'completed': False\n        \"task\": \"study\",\n        \"date\": 18,\n        \"completed\": False\n    }\n\n result = mongo.db.todos.insert_one(new_todo)\n print(result,\"has been created\")`\n`\n```\nI am creating a todo list using flask environment.",
      "solution": "uninstall both pymongo and bson\nand install just pymongo, pymongo automatically installs bson\n```\n`pip uninstall pymongo\n`\n```\n```\n`pip uninstall bson\n`\n```\n```\n`pip install pymongo\n`\n```",
      "question_score": 8,
      "answer_score": 41,
      "created_at": "2023-05-24T09:49:35",
      "url": "https://stackoverflow.com/questions/76321221/error-importerror-cannot-import-name-get-object-size-from-bson"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 73344053,
      "title": "Getting &quot;The &#39;mongodb&#39; provider is not supported with this command&quot; Error when try to do mongoDB migrate with Prisma",
      "problem": "I'm developing some simple Todo App BE using NestJS with Prisma ORM and use MongoDB as the DB. I'm using a FREE and SHARED MongoDB cluster that is hosted in MongoDB Altas cloud. Also I added `0.0.0.0/0` to the network access tab so anyone can connect to the DB.\nschema.prisma file\n```\n`// This is your Prisma schema file,\n// learn more about it in the docs: https://pris.ly/d/prisma-schema\n\ndatasource db {\n  provider = \"mongodb\"\n  url      = env(\"DATABASE_URL\")\n}\n\ngenerator client {\n  provider = \"prisma-client-js\"\n}\n\nmodel Task {\n  id      String   @id @default(auto()) @map(\"_id\") @db.ObjectId\n  name    String?\n  description String?\n  status  TaskStatus @default(TODO)\n}\n\nenum TaskStatus {\n  TODO\n  INPROGRESS\n  DONE\n}\n`\n```\n.env file\n```\n`DATABASE_URL=\"mongodb+srv://:@todoappdb.jfo3m2c.mongodb.net/?retryWrites=true&w=majority\"\n`\n```\nBut when I try to run `npx prisma migrate dev --name init` command it gives following output\n```\n`D:\\todoapp-backend>npx prisma migrate dev --name init\nEnvironment variables loaded from .env\nPrisma schema loaded from prisma\\schema.prisma\nDatasource \"db\"\n\nError: The \"mongodb\" provider is not supported with this command. For more info see https://www.prisma.io/docs/concepts/database-connectors/mongodb\n   0: migration_core::state::DevDiagnostic\n             at migration-engine\\core\\src\\state.rs:250\n`\n```\nCan someone point me what is the problem?",
      "solution": "After reading some content, found that `prisma migrate` commands are for the SQL databases only since they have a rigid table structure. But MongoDB is a document database and those data are unstructured\nSo rather than running `prisma migrate` command we can use following command\n```\n`npx prisma generate\n`\n```\nThis command creates the Prisma client that gives type-safe access to our database.\nReference - https://www.youtube.com/watch?v=b4nxOv91vWI&ab_channel=Prisma",
      "question_score": 8,
      "answer_score": 23,
      "created_at": "2022-08-13T13:58:52",
      "url": "https://stackoverflow.com/questions/73344053/getting-the-mongodb-provider-is-not-supported-with-this-command-error-when-t"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 65599791,
      "title": "MongooseError: Operation `orders.deleteMany()` buffering timed out after 10000ms",
      "problem": "when I run my app with `npm run seeder`\nthen I have face this error\nI have checked my database connection carefully, it's ok.\nalso, I have checked my ordermodels file it's also ok. I have used MongoDB compass there is nothing problem. I don't know why showing `buffering timed out`.\n```\n`MongooseError: Operation `orders.deleteMany()` buffering timed out after 10000ms\n`\n```\n\nseeder.js\n\n```\n`    import mongoose from \"mongoose\";\n    import dotenv from \"dotenv\";\n    import colors from \"colors\";\n    import users from \"./data/users.js\";\n    import products from \"./data/products.js\";\n    import User from \"./models/userModel.js\";\n    import Product from \"./models/productModel.js\";\n    import Order from \"./models/orderModel.js\";\n    import connectDB from \"./config/db.js\";\n    \n    dotenv.config();\n    connectDB();\n    \n    const importData = async () => {\n      try {\n        await Order.deleteMany();\n        await Product.deleteMany();\n        await User.deleteMany();\n    \n        const createUsers = await User.insertMany(users);\n        const adminUser = createUsers[0]._id;\n        const sampleProducts = products.map((product) => {\n          return { ...product, user: adminUser };\n        });\n        await Product.insertMany(sampleProducts);\n    \n        console.log(\"Data Imported\".green.inverse);\n        process.exit();\n      } catch (error) {\n        console.error(`${error}`.red.inverse);\n        process.exit(1);\n      }\n    };\n    \n    const DeleteData = async () => {\n      try {\n        await Order.deleteMany();\n        await Product.deleteMany();\n        await User.deleteMany();\n    \n        console.log(\"Data Deleted\".red.inverse);\n        process.exit();\n      } catch (error) {\n        console.error(`${error}`.red.inverse);\n        process.exit(1);\n      }\n    };\n    \n    if (process.argv[2] === \"-d\") {\n      DeleteData();\n    } else {\n      importData();\n    }\n`\n```",
      "solution": "I have the same issue and I just did a research and I find that your MongoDB are trying to execute the function `User.deleteMany()` before the database is connected.\njust put an await before `connectDB()`;\n```\n`await connectDB();\n`\n```",
      "question_score": 8,
      "answer_score": 15,
      "created_at": "2021-01-06T17:43:27",
      "url": "https://stackoverflow.com/questions/65599791/mongooseerror-operation-orders-deletemany-buffering-timed-out-after-10000ms"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 70766870,
      "title": "Next.js with-mongodb convert to TypeScript",
      "problem": "I installed next.js app with mongodb template:\n```\n`npx create-next-app --e with-mongodb my-app\n`\n```\nand installed TypeScript as well.\nAnd i need to convert `/lib/mongodb.js` to TypeScript\nCurrently it looks like this, almost no type change\n```\n`// lib/mongodb.ts\n\nimport { MongoClient } from 'mongodb'\n\nconst uri = process.env.MONGODB_URI\nconst options = {}\n\nlet client\nlet clientPromise: MongoClient\n\nif (!process.env.MONGODB_URI) {\n  throw new Error('Please add your Mongo URI to .env.local')\n}\nif (process.env.NODE_ENV === 'development') {\n  if (!global._mongoClientPromise) {\n    client = new MongoClient(uri, options)\n    global._mongoClientPromise = client.connect()\n  }\n  clientPromise = global._mongoClientPromise\n} else {\n  client = new MongoClient(uri, options)\n  clientPromise = client.connect()\n}\n\nexport default clientPromise\n\n`\n```\nAnd it is yelling:",
      "solution": "I ran into the same problem yesterday. Luckily, found a solution!\n```\n`import { MongoClient } from \"mongodb\";\n\nif (!process.env.MONGODB_URI) {\n  throw new Error(\"Please add your Mongo URI to .env.local\");\n}\n\nconst uri: string = process.env.MONGODB_URI;\nlet client: MongoClient;\nlet clientPromise: Promise;\n\nif (process.env.NODE_ENV === \"development\") {\n  // In development mode, use a global variable so that the value\n  // is preserved across module reloads caused by HMR (Hot Module Replacement).\n\n  let globalWithMongoClientPromise = global as typeof globalThis & {\n    _mongoClientPromise: Promise;\n  };\n\n  if (!globalWithMongoClientPromise._mongoClientPromise) {\n    client = new MongoClient(uri);\n    globalWithMongoClientPromise._mongoClientPromise = client.connect();\n  }\n\n  clientPromise = globalWithMongoClientPromise._mongoClientPromise;\n} else {\n  // In production mode, it's best to not use a global variable.\n  client = new MongoClient(uri);\n  clientPromise = client.connect();\n}\n\n// Export a module-scoped MongoClient promise. By doing this in a\n// separate module, the client can be shared across functions.\nexport default clientPromise;\n`\n```",
      "question_score": 8,
      "answer_score": 29,
      "created_at": "2022-01-19T08:40:27",
      "url": "https://stackoverflow.com/questions/70766870/next-js-with-mongodb-convert-to-typescript"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 70192191,
      "title": "What causes Mongoose `updateMany` to return `{ acknowledged: false }`",
      "problem": "I'm trying to set up notifications for an Express app using MongoDB.\nI have an API endpoint where I `$push` a user's id to the `readBy` field in MongoDB to \"mark\" it as read after retrieving a user's notifications. When I make a request to this endpoint, it returns `200` and their notifications, but it isn't making any updates to the notification document in MongoDB. `console.log`ing the query response in the callback gave me `{ acknowledged: false }`. According to the Mongoose docs, `acknowledged` is a `Boolean indicating everything went smoothly`, but information about what `acknowledged` is and at which point in the query/write process caused it to occur was sparse. Since it didn't return any errors I couldn't find a way to troubleshoot it.\nWould someone be able to shed some light on what exactly `acknowledged: false` is and in what typically causes it, and why it doesn't throw an error.\nModel:\n```\n`const notificationSchema = new Schema({\n  timestamp: {\n    type: Date,\n    required: true\n  },\n  type: {\n    type: String,\n    required: true,\n    enum: [\n      'newCustomer',\n      'contractSigned',\n      'invoicePaid',\n      'warrantyExp',\n      'assignedProject'\n    ]\n  },\n  recipients: [{\n    type: Schema.Types.ObjectId,\n    ref: 'Employee',\n    required: true,\n  }],\n  customer: {\n    type: Schema.Types.ObjectId,\n    ref: 'Customer',\n    required: true,\n  },\n  readBy: [{\n    type: String\n  }],\n  uuid: {\n    type: String,\n    default: uuid.v4,\n    immutable: true,\n    required: true,\n  },\n  company: {\n    type: Schema.Types.ObjectId, ref: 'Company'\n  }\n});\n`\n```\nRoute:\n```\n`router.get(\"/notification/all\", withAuth, async (req, res) => {\n  const FOURTEEN_DAYS = new Date().setDate(new Date().getDate() + 14);\n  try {\n    const { uuid, userId } = req.loggedInUser;\n\n    // Fetch notifications that have the user as a recipient.\n    Notification.find({\n      recipients: userId,\n    })\n      .populate(\"customer\")\n      .exec((err, notifs) => {\n        if (err)\n          return res.status(500).json({\n            success: false,\n            message: \"Error: Failed to retrieve notifications.\",\n          });\n\n        const result = [];\n        const notifIds = [];\n\n        for (const notif of notifs) {\n          // Filter notif\n          result.push({\n            timestamp: notif.timestamp,\n            customer: notif.customer,\n            type: notif.type,\n            read: notif.readBy.includes(uuid),\n          });\n          // Add the user as read\n          notifIds.push(notif.uuid);\n        }\n\n        console.log(notifIds);\n\n        /* THIS RETURNS ACKNOWLEDGED: FALSE */         \n        // Write to DB that user has read these notifications\n        Notification.updateMany(\n          { uuid: { $in: notifIds } },\n          { $push: { readBy: uuid } },\n          (err, resultUpdate) => {\n            if (err)\n              return res.status(500).json({\n                success: false,\n                message:\n                  \"Error: Failed to add check off notifications as read.\",\n              });\n\n            console.log(resultUpdate);\n\n            // Delete notifications past 14 days and has been read by all recipients\n            Notification.deleteMany(\n              {\n                timestamp: { $gte: FOURTEEN_DAYS },\n                $expr: {\n                  $eq: [{ $size: \"$readBy\" }, { $size: \"$recipients\" }],\n                },\n              },\n              (err) => {\n                if (err)\n                  return res.status(500).json({\n                    success: false,\n                    message: \"Error: Failed to delete old notifications.\",\n                  });\n\n                return res.status(200).json({\n                  success: true,\n                  notifications: result,\n                  message: \"Fetched notifications\",\n                });\n              }\n            );\n          }\n        );\n      });\n  } catch (err) {\n    res.status(500).json({ success: false, message: err.toString() });\n  }\n});\n`\n```",
      "solution": "So it turns out that this issue was unrelated to write concern. `acknowledged: false` was being returned because the value we were trying to `$push` was `undefined`. So essentially Mongoose was refusing to write `undefined` values, but doesn't throw an error for the input value being undefined. Putting this here in case someone else runs into this issue.",
      "question_score": 8,
      "answer_score": 19,
      "created_at": "2021-12-01T23:43:55",
      "url": "https://stackoverflow.com/questions/70192191/what-causes-mongoose-updatemany-to-return-acknowledged-false"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 70782360,
      "title": "How to use embedded MongoDB with SpringBoot v2.6.2?",
      "problem": "I'm using Spring Boot v2.6.2 and Java v17 and trying to test my MongoConnection without having a MongoDBService running because it should be tested with embedded in-memory MongoDB on the build machine, no need to set up an extra MongoDB service there.\nFor sure on the productive system, it should use a full MongoDB.\nI try to get this easy example running:\n```\n`@DataMongoTest\npublic class MongoTest\n{\n    @Autowired\n    private UserRepository userRepository;\n\n    @AfterEach\n    void cleanUpDatabase()\n    {\n        this.userRepository.deleteAll();\n    }\n\n    @Test\n    void bootstrapTestDataWithMongoTemplate() {\n        final var restaurant = new User( \"123\", \"ABC\", \"DEF\" );\n        this.userRepository.insert( restaurant );\n\n        final var found = this.userRepository.findByFirstName( \"ABC\" );\n\n        System.out.println( found );\n    }\n}\n`\n```\n`UserRepository` is an interface that extends `MongoRepository`. `User` itself is just a `DBEntity` with `@Document` annotation. If I let the SpringBoot application run, not in test mode, everything works fine, because MongoDB is running at the specified location. But for the test, I want to let it run as an in-memory DB.\nBut Springboot wants to connect for the test.\n\n`    2022-01-20 08:31:57.489  INFO 3976 --- [           main] org.mongodb.driver.cluster               : Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n2022-01-20 08:31:57.521  INFO 3976 --- [localhost:27017] org.mongodb.driver.cluster               : Exception in monitor thread while connecting to server localhost:27017\n\ncom.mongodb.MongoSocketOpenException: Exception opening socket\n    at com.mongodb.internal.connection.SocketStream.open(SocketStream.java:70) ~[mongodb-driver-core-4.4.0.jar:na]\n    at com.mongodb.internal.connection.InternalStreamConnection.open(InternalStreamConnection.java:180) ~[mongodb-driver-core-4.4.0.jar:na]\n    at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:188) ~[mongodb-driver-core-4.4.0.jar:na]\n    at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:152) ~[mongodb-driver-core-4.4.0.jar:na]\n    at java.base/java.lang.Thread.run(Thread.java:833) ~[na:na]\nCaused by: java.net.ConnectException: Connection refused: no further information\n    at java.base/sun.nio.ch.Net.pollConnect(Native Method) ~[na:na]\n    at java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672) ~[na:na]\n    at java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(NioSocketImpl.java:542) ~[na:na]\n    at java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:597) ~[na:na]\n    at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327) ~[na:na]\n    at java.base/java.net.Socket.connect(Socket.java:633) ~[na:na]\n    at com.mongodb.internal.connection.SocketStreamHelper.initialize(SocketStreamHelper.java:107) ~[mongodb-driver-core-4.4.0.jar:na]\n    at com.mongodb.internal.connection.SocketStream.initializeSocket(SocketStream.java:79) ~[mongodb-driver-core-4.4.0.jar:na]\n    at com.mongodb.internal.connection.SocketStream.open(SocketStream.java:65) ~[mongodb-driver-core-4.4.0.jar:na]\n    ... 4 common frames omitted\n`\nAny suggestions on how to do this? Is `@DataMongoTest` testing in the wrong way? I thought the integrated flapdoodle dependency would inject it automatically in the test case.\nThe pom itself is also not that complex:\n`\n        17\n    \n    \n        \n            org.springframework.boot\n            spring-boot-starter-actuator\n        \n        \n            org.springframework.boot\n            spring-boot-starter-data-mongodb\n        \n        \n            org.springframework.boot\n            spring-boot-starter-security\n        \n        \n            org.springframework.boot\n            spring-boot-starter-web\n        \n        \n            org.springframework.boot\n            spring-boot-devtools\n            true\n        \n\n        \n            org.projectlombok\n            lombok\n            true\n        \n        \n            org.springframework.boot\n            spring-boot-starter-test\n            test\n        \n        \n            org.springframework.security\n            spring-security-test\n            test\n        \n        \n            io.springfox\n            springfox-swagger2\n            3.0.0\n        \n        \n            com.github.openjson\n            openjson\n            1.0.11\n        \n        \n            com.github.erosb\n            everit-json-schema\n            1.14.0\n        \n    \n`",
      "solution": "The documentation states:\n\n2.2.4. Embedded Mongo\nSpring Boot offers auto-configuration for Embedded Mongo. To use it in\nyour Spring Boot application, add a dependency on\nde.flapdoodle.embed:de.flapdoodle.embed.mongo and set the\nspring.mongodb.embedded.version property to match the version of\nMongoDB that your application will use in production.     The default\ndownload configuration allows access to most of the versions listed in\nEmbedded Mongo\u2019s Version class as well as some others. Configuring an\ninaccessible version will result in an error when attempting to\ndownload the server. Such an error can be corrected by defining an\nappropriately configured DownloadConfigBuilderCustomizer bean.\nThe port that Mongo listens on can be configured by setting the\nspring.data.mongodb.port property. To use a randomly allocated free\nport, use a value of 0. The MongoClient created by\nMongoAutoConfiguration is automatically configured to use the randomly\nallocated port.   If you do not configure a custom port, the embedded\nsupport uses a random port (rather than 27017) by default.\nIf you have SLF4J on the classpath, the output produced by Mongo is\nautomatically routed to a logger named\norg.springframework.boot.autoconfigure.mongo.embedded.EmbeddedMongo.\nYou can declare your own IMongodConfig and IRuntimeConfig beans to\ntake control of the Mongo instance\u2019s configuration and logging\nrouting. The download configuration can be customized by declaring a\nDownloadConfigBuilderCustomizer bean.\n\nTherefore add the below dependency in scope:test if you want it to only be applied for Tests.\n```\n`    \n        de.flapdoodle.embed\n        de.flapdoodle.embed.mongo\n        test\n    \n`\n```\nAs well you need to set the version in your application.properties file:\n```\n`spring.mongodb.embedded.version=4.0.21\n`\n```\nhttps://docs.spring.io/spring-boot/docs/current/reference/html/data.html#data.nosql.mongodb.embedded\n\nWorking Example:\n```\n`\n\n    4.0.0\n    \n        org.springframework.boot\n        spring-boot-starter-parent\n        2.6.2\n         \n    \n    com.example.mongodb.embedded\n    mongodb-app\n    0.0.1-SNAPSHOT\n    mongodb-app\n    Demo project for usage of embedded mongodb\n    \n        11\n    \n    \n        \n            org.springframework.boot\n            spring-boot-starter-data-mongodb\n        \n        \n            org.springframework.boot\n            spring-boot-starter-web\n        \n        \n            org.projectlombok\n            lombok\n            1.18.16\n            provided\n        \n        \n            org.springframework.boot\n            spring-boot-starter-test\n            test\n        \n        \n            de.flapdoodle.embed\n            de.flapdoodle.embed.mongo\n            test\n        \n\n    \n\n    \n        \n            \n                org.springframework.boot\n                spring-boot-maven-plugin\n            \n            \n                org.apache.maven.plugins\n                maven-surefire-plugin\n                \n                    \n                        integration-test\n                        \n                            test\n                        \n                        integration-test\n                        \n                            \n                                none\n                            \n                            \n                                **/*IT.java\n                            \n                        \n                    \n                \n            \n        \n    \n\n`\n```\nUserRepository:\n```\n`public interface UserRepository extends MongoRepository {\n\n    //Spring converts this to Regex findByFirstnameRegex(String firstname)  {\"firstname\" : {\"$regex\" : firstname }}\n    // automatically\n    public List findByFirstName(String firstName);\n\n}\n`\n```\nUser:\n```\n`@Data\n@Builder\npublic class User {\n\n    @Id\n    private Long id;\n\n    private String firstName;\n    private String lastName;\n\n}\n`\n```\nTest:\n```\n`@DataMongoTest\n@TestInstance(TestInstance.Lifecycle.PER_CLASS)\npublic class UserControllerIT {\n\n    @Autowired\n    private UserRepository userRepository;\n\n    @BeforeAll\n    public void setup(){\n\n        userRepository.save(User.builder().id(1L).firstName(\"James\").lastName(\"Bond\").build());\n        userRepository.save(User.builder().id(2L).firstName(\"James\").lastName(\"Farley\").build());\n        userRepository.save(User.builder().id(3L).firstName(\"Marley\").lastName(\"Hemp\").build());\n        userRepository.save(User.builder().id(4L).firstName(\"James\").lastName(\"Bond\").build());\n\n    }\n\n    @Test\n    public void test_getById_successfull() throws Exception {\n        Assertions.assertEquals(\"James\", userRepository.findByFirstName(\"James\").get(0).getFirstName());\n    }\n\n}\n`\n```\nsrc/test/resources/application.properties\n```\n`spring.data.mongodb.database=test\nspring.data.mongodb.port=27017\nspring.mongodb.embedded.version=4.0.2\n`\n```",
      "question_score": 8,
      "answer_score": 24,
      "created_at": "2022-01-20T08:42:15",
      "url": "https://stackoverflow.com/questions/70782360/how-to-use-embedded-mongodb-with-springboot-v2-6-2"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 70985855,
      "title": "MongoDB aggregate match non-empty array",
      "problem": "I have a collection in a MongoDB that contains a field \"events\" which is an array. I need to write an aggregate query for this that checks for the events array to not be empty, but can't find a way to do this.\nI want something like:\n```\n`db.collection.aggregate([\n    { \n        $match: { \n            events: {\n                \"$empty\": false \n            }\n        }\n    }\n]);\n`\n```",
      "solution": "After some digging around and having tried several options (including a nasty project of $gte: 0 of the $size followed by a match on that projected field) I eventually found the following makes sense and actually works:\n```\n`db.collection.aggregate([\n    { \n        $match: { \n            \"events.0\": {\n                \"$exists\": true \n            }\n        }\n    }\n]);\n`\n```",
      "question_score": 8,
      "answer_score": 19,
      "created_at": "2022-02-04T12:45:32",
      "url": "https://stackoverflow.com/questions/70985855/mongodb-aggregate-match-non-empty-array"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 76570896,
      "title": "ImportError: cannot import name &#39;JSONEncoder&#39; from &#39;flask.json&#39;",
      "problem": "I'm following a course on full-stack with Flask. My init.py looks like:\n```\n`from flask import Flask\nfrom config import Config\nfrom flask_mongoengine import MongoEngine\n\napp = Flask(__name__)\napp.config.from_object(Config)\n\ndb = MongoEngine()\ndb.init_app(app)\n\nfrom application import routes\n`\n```\nHowever, when importing `from flask_mongoengine import MongoEngine`, I'm getting an ImportError:\n```\n`ImportError: cannot import name 'JSONEncoder' from 'flask.json' \n`\n```\nMy venv looks like:\n```\n`blinker==1.6.2\nclick==8.1.3\ncolorama==0.4.6\ndnspython==2.3.0\nemail-validator==2.0.0.post2\nFlask==2.3.2\nflask-mongoengine==1.0.0\nFlask-WTF==1.1.1\nidna==3.4\nitsdangerous==2.1.2\nJinja2==3.1.2\nMarkupSafe==2.1.3\nmongoengine==0.27.0\npymongo==4.4.0\npython-dotenv==1.0.0\nWerkzeug==2.3.6\nWTForms==3.0.1\n`\n```\nIs there anything I can do here to avoid this conflict? Thanks!",
      "solution": "flask_mongoengine seems to be not currently maintained and does not work with current Flask versions. If you absolutely must use it, you need to downgrade your Flask version, which may (and likely will) get you into other trouble.\nThere is an issue on github regarding your error message:\nhttps://github.com/MongoEngine/flask-mongoengine/issues/522\nThe deprecation warning came with Flask 2.2.0 in 08/2022:\nFlask Changes\nAfter a brief look at the repo, it seems the maintainer was already on it: https://github.com/MongoEngine/flask-mongoengine/blob/master/flask_mongoengine/json.py",
      "question_score": 8,
      "answer_score": 3,
      "created_at": "2023-06-28T09:23:31",
      "url": "https://stackoverflow.com/questions/76570896/importerror-cannot-import-name-jsonencoder-from-flask-json"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 70739146,
      "title": "How can I solve a &quot;MongoError: E11000 duplicate key error collection&quot; error in Mongo?",
      "problem": "I have created a new Mongo collection to a new project, but when I try to create a new user, I get the following error:\n```\n`MongoError: E11000 duplicate key error collection: GestorMedico.users index: username_1 dup key: { username: null }\n`\n```\nMy User schema is the following:\n```\n`const { Schema, model } = require(\"mongoose\");\n\nconst userSchema = new Schema({\n    nombreEmpresa: {\n        type: String,\n        unique: true,\n        required: true\n    },\n    email: {\n        type: String,\n        required: true\n    },\n    telefono: {\n        type: String,\n        required: true\n    },\n    contrase\u00f1a: {\n        type: String,\n        required: true\n    }\n}, {\n    timestamps: true,\n    versionKey: false\n});\n\nmodule.exports = model(\"Users\", userSchema);\n`\n```\nMy function is the following:\n```\n`userCtrl.createUser = async (req, res) => {\n    const newUser = new User(req.body);\n    \n    newUser.contrase\u00f1a = await crypt.encryptPassword(newUser.contrase\u00f1a);\n    \n    await newUser.save();\n    \n    res.status(200).json({\n        status: \"OK\",\n        message: \"User created\"\n    });\n};\n`\n```\nAnd my collection looks like:\n\nI have reused the backend of one of my old projects, which have a \"username\" in a schema.",
      "solution": "The Error E11000 is thrown when there are unique fields (indexes) that you try to save while there is another one already in the Database.\nsince the field seems to be username which is not visible in the userSchema, I am assuming you still have that index existing in your Database.\nIn mongo Compass on the page u made a screenshot from, go to the tab \"Indexes\", refresh and delete the username index in case it is there.\nIf that does not solve it proceed to print out the indexes via code and check there, if there is one that your don't want use `db.collection.dropIndex()` to delete it.\nDocs here\nAlso make sure that in your req.body the `nombreEmpresa` field is transferred since it is required when you want to save the document.\nI hope that solves it ;)",
      "question_score": 8,
      "answer_score": 14,
      "created_at": "2022-01-17T10:32:25",
      "url": "https://stackoverflow.com/questions/70739146/how-can-i-solve-a-mongoerror-e11000-duplicate-key-error-collection-error-in-m"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 69542263,
      "title": "IntellijIdea Mongo connection too slow and lack of capabilities",
      "problem": "I'm trying to use IntellijIdea in order to connect to MongoDB but it seems to work too slow. A simple read request might take up to 5 secs meanwhile Robo 3T works almost instantly. Is it a common and known behavior (some issue with mongo driver for example) or is it my local issue?\nAlso I can't find how to manage collections\\databases via GUI. Let's say I want to create a new database: I right-click in order to get a context menu, go to \"new\" section and everything I can do is to add a new datasource, driver or just jump to console.\nAlso I can't find db users for the given database. There is just no such folder under selected db.\nCan I do such kind of management via IntellijIdea database GUI?",
      "solution": "Unfortunately we found a problem with latest MongoDB driver, which causes slow operations. Please open up data source properties, switch to Drivers tab, select MongoDB and switch to v.1.11.\nAnd I've created 2 feature request based on your feedback, please follow and vote to get noticed on any updates:\n\nFor database management GUI https://youtrack.jetbrains.com/issue/DBE-14252\nFor user list https://youtrack.jetbrains.com/issue/DBE-14253",
      "question_score": 8,
      "answer_score": 19,
      "created_at": "2021-10-12T16:24:12",
      "url": "https://stackoverflow.com/questions/69542263/intellijidea-mongo-connection-too-slow-and-lack-of-capabilities"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 66038563,
      "title": "TypeScript declarations for environment variables not being recognized",
      "problem": "I am trying to connect to MongoDB when starting my node.js server. My server is in `src/server.ts`, `connectDB()` is in `src/config/db.ts` and my `.env`, and `environment.d.ts` are in the root directory. However, when trying to connect to the DB it still says my `MONGO_URI`, declared in `.env`, is of type `string | undefined`.\n`environment.d.ts`:\n```\n`declare namespace NodeJS {\n  export interface ProcessEnv {\n    PORT?: string;\n    NODE_ENV: 'development' | 'production';\n    MONGO_URI: string;\n  }\n}\n`\n```\n`src/server`:\n```\n`import dotenv from 'dotenv';\nimport express from 'express';\nimport connectDB from './config/db';\n\ndotenv.config({ path: '../.env' });\nconnectDB();\nconst app = express();\n\n.....\n`\n```\n`src/config/db.ts`:\n```\n`import mongoose from 'mongoose';\n\nconst connectDB = async () => {\n  try {\n    const conn = await mongoose.connect(process.env.MONGO_URI, {\n      useUnifiedTopology: true,\n      useNewUrlParser: true,\n      useCreateIndex: true,\n    });\n\n    console.log(`MongoDB connected: ${conn.connection.host}`);\n  } catch (error) {\n    console.error(`ERROR: ${error.message}`);\n    process.exit(1);\n  }\n};\n\nexport default connectDB;\n`\n```\nFull error code:\n```\n`TSError: \u2a2f Unable to compile TypeScript:\nsrc/config/db.ts:5:41 - error TS2769: No overload matches this call.\n  Overload 1 of 3, '(uri: string, callback: (err: CallbackError) => void): void', gave the following error.\n    Argument of type 'string | undefined' is not assignable to parameter of type 'string'.\n      Type 'undefined' is not assignable to type 'string'.\n  Overload 2 of 3, '(uri: string, options?: ConnectOptions | undefined): Promise', gave the following error.\n    Argument of type 'string | undefined' is not assignable to parameter of type 'string'.\n      Type 'undefined' is not assignable to type 'string'.\n\n5     const conn = await mongoose.connect(process.env.MONGO_URI, {\n`\n```\nI tried declaring the `dotenv.config()` within `db.ts` and removing the path option when declaring it in `server.ts`. Hovering over the environment variable in VSCode shows `(property) NodeJS.ProcessEnv.MONGO_URI: string`. So I really thought this was setup right but I must be missing something.",
      "solution": "This error may be due to your tsconfig.json file rules. Most probably due to `\"strictNullChecks\": true` you may have put this rule to true.\nThere are two simple solutions:\n\nput this rule to false like this `\"strictNullChecks\": false`.\nOr add `!` immediately after `process.env.MONGO_URI` like this: `process.env.MONGO_URI!`. The symbol `!` insures your typescript transpiler the value will not be undefined.",
      "question_score": 8,
      "answer_score": 6,
      "created_at": "2021-02-04T03:30:43",
      "url": "https://stackoverflow.com/questions/66038563/typescript-declarations-for-environment-variables-not-being-recognized"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 65863013,
      "title": "Issues With MongoDB storage (9001 already in use)",
      "problem": "I am getting this error when trying to run the command\n`mongod --dbpath=/var/lib/mongodb`\n```\n`{\"t\":{\"$date\":\"2021-01-23T11:23:41.733-07:00\"},\"s\":\"I\",  \"c\":\"CONTROL\",  \"id\":51765,   \"ctx\":\"initandlisten\",\"msg\":\"Operating System\",\"attr\":{\"os\":{\"name\":\"Ubuntu\",\"version\":\"20.04\"}}}\n{\"t\":{\"$date\":\"2021-01-23T11:23:41.733-07:00\"},\"s\":\"I\",  \"c\":\"CONTROL\",  \"id\":21951,   \"ctx\":\"initandlisten\",\"msg\":\"Options set by command line\",\"attr\":{\"options\":{\"storage\":{\"dbPath\":\"/var/lib/mongodb\"}}}}\n{\"t\":{\"$date\":\"2021-01-23T11:23:41.734-07:00\"},\"s\":\"E\",  \"c\":\"STORAGE\",  \"id\":20568,   \"ctx\":\"initandlisten\",\"msg\":\"Error setting up listener\",\"attr\":{\"error\":{\"code\":9001,\"codeName\":\"SocketException\",\"errmsg\":\"Address already in use\"}}}\n`\n```\nI do see the issue is\n`\"attr\":{\"error\":{\"code\":9001,\"codeName\":\"SocketException\",\"errmsg\":\"Address already in use\"}}}`\nBut I don't know how to resolve this?\nI tried other suggestions from another SO post that says to kill the process and restart. I did that and it did not work for me.\nI also tried running mongod on a different port\n```\n`mongod --dbpath=/var/lib/mongodb --port 27019\n`\n```\nwhich gives me this error\n```\n`{\"t\":{\"$date\":\"2021-01-23T11:27:29.535-07:00\"},\"s\":\"E\",  \"c\":\"STORAGE\",  \"id\":20557,   \"ctx\":\"initandlisten\",\"msg\":\"DBException in initAndListen, terminating\",\"attr\":{\"error\":\"IllegalOperation: Attempted to create a lock file on a read-only directory: /var/lib/mongodb\"}}\n`\n```\nSo it's a read only directory so I ran\n```\n`sudo chmod -R 777 /var/lib/mongodb\n`\n```\nThen I get this error in the log\n```\n`\"ctx\":\"initandlisten\",\"msg\":\"DBException in initAndListen, terminating\",\"attr\":{\"error\":\"DBPathInUse: Unable to lock the lock file: /var/lib/mongodb/mongod.lock (Resource temporarily unavailable). Another mongod instance is already running on the /var/lib/mongodb directory\"}}\n`\n```\nSo I try to find out which other process is running\n```\n`ps -eaf | grep mongod\n\nmongodb   722732       1  0 11:09 ?        00:00:10 /usr/bin/mongod --config /etc/mongod.conf\nsrt       725772  720106  0 11:34 pts/2    00:00:00 grep --color=auto --exclude-dir=.bzr --exclude-dir=CVS --exclude-dir=.git --exclude-dir=.hg --exclude-dir=.svn --exclude-dir=.idea --exclude-dir=.tox mongod\n`\n```\nBut idk what to do from here.",
      "solution": "You have already the default auto started mongod instance running that use the default /etc/mongod.conf file\nYou may stop this instance via:\n```\n`  service mongod stop\n`\n```\nor\n```\n`  kill -2 722732\n`\n```\nor\n```\n` mongo --port 27017\n >use admin\n >db.shutdownServer()\n`\n```\nor\n```\n` systemctl stop mongod\n`\n```\nYou can modify the /etc/mongod.conf file with your needs and start it again with\n```\n`  service mongod start\n`\n```\nor:\n```\n` systemctl start mongod   \n`\n```",
      "question_score": 8,
      "answer_score": 11,
      "created_at": "2021-01-23T19:36:26",
      "url": "https://stackoverflow.com/questions/65863013/issues-with-mongodb-storage-9001-already-in-use"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 68674897,
      "title": "MongoDB docker replica set connection error &quot;Host not found&quot;",
      "problem": "I have a local MongoDB replica set created following this SO answer.\nThe docker-compose file:\n`services:\n  mongo1:\n    container_name: mongo1\n    image: mongo:4.2\n    ports:\n      - 27017:27017\n    restart: always\n    command: [\"--bind_ip_all\", \"--replSet\", \"rs\" ]\n  mongo2:\n    container_name: mongo2\n    image: mongo:4.2\n    ports:\n      - 27018:27017\n    restart: always\n    command: [\"--bind_ip_all\", \"--replSet\", \"rs\" ]\n  mongo3:\n    container_name: mongo3\n    image: mongo:4.2\n    ports:\n      - 27019:27017\n    restart: always\n    command: [\"--bind_ip_all\", \"--replSet\", \"rs\" ]\n  replica_set:\n    image: mongo:4.2\n    container_name: replica_set\n    depends_on:\n      - mongo1\n      - mongo2\n      - mongo3\n    volume:\n      - ./initiate_replica_set.sh:/initiate_replica_set.sh\n    entrypoint: \n      - /initiate_replica_set.sh\n`\nThe initiate_replica_set.sh file:\n`#!/bin/bash\n\necho \"Starting replica set initialize\"\nuntil mongo --host mongo1 --eval \"print(\\\"waited for connection\\\")\"\ndo\n    sleep 2\ndone\necho \"Connection finished\"\necho \"Creating replica set\"\nmongo --host mongo1 \nThe replica set is brought up successfully and runs fine, but it errors when I try to connect to the replica set:\n`$ mongo \"mongodb://localhost:27017,localhost:27018,localhost:27019/?replicaSet=rs\"\nMongoDB shell version v5.0.2\nconnecting to: mongodb://localhost:27017,localhost:27018,localhost:27019/?compressors=disabled&gssapiServiceName=mongodb&replicaSet=rs\n{\"t\":{\"$date\":\"2021-08-05T21:35:40.667Z\"},\"s\":\"I\",  \"c\":\"NETWORK\",  \"id\":4333208, \"ctx\":\"ReplicaSetMonitor-TaskExecutor\",\"msg\":\"RSM host selection timeout\",\"attr\":{\"replicaSet\":\"rs\",\"error\":\"FailedToSatisfyReadPreference: Could not find host matching read preference { mode: \\\"nearest\\\" } for set rs\"}}\nError: Could not find host matching read preference { mode: \"nearest\" } for set rs, rs/localhost:27017,localhost:27018,localhost:27019 :\nconnect@src/mongo/shell/mongo.js:372:17\n@(connect):2:6\nexception: connect failed\nexiting with code 1\n`\nMore verbose log:\n`{\n  \"t\": {\n    \"$date\": \"2021-08-05T21:35:54.531Z\"\n  },\n  \"s\": \"I\",\n  \"c\": \"-\",\n  \"id\": 4333222,\n  \"ctx\": \"ReplicaSetMonitor-TaskExecutor\",\n  \"msg\": \"RSM received error response\",\n  \"attr\": {\n    \"host\": \"mongo1:27017\",\n    \"error\": \"HostUnreachable: Error connecting to mongo1:27017 :: caused by :: Could not find address for mongo1:27017: SocketException: Host not found (authoritative)\",\n    \"replicaSet\": \"rs\",\n    \"response\": \"{}\"\n  }\n}\n`\nWhat is the cause of the problem and how do I fix it?",
      "solution": "There are some partial answers on this issue from various places, here is what I think as a complete answer.\nThe Cause\n\nMongo clients use the hostnames listed in the replica set config, not the seed list\nAlthough the connection string is `\"mongodb://localhost:27017,localhost:27018,localhost:27019/?replicaSet=rs\"`, mongo client does not connect to the members of the replica set with seed addresses `localhost:27017` etc, instead the client connects to the members in the replica config set returned from the seed hosts, i.e., the ones in the `rs.initiate` call. This is why the error message is `Error connecting to mongo1:27017` instead of `Error connecting to localhost:27017`.\n\nContainer hostnames are not addressable outside the container network\nA mongo client inside the same container network as the mongo server containers can connect to the server via addresses like `mongo1:27017`; however, a client on the host, which is outside of the container network, can not resolve `mongo1` to an IP. The typical solution for this problem is proxy, see Access docker container from host using containers name for details.\n\nThe Fix\nBecause the problem involves docker networking and docker networking varies between Linux and Mac. The fixes are different on the two platforms.\nLinux\nThe proxy fix (via 3rd party software or modifying `/etc/hosts` file) works fine but sometimes is not viable, e.g., running on remote CI hosts. A simple self-contained portable solution is to update the `intiate_replia_set.sh` script to initiate the replica set with member IPs instead of hostnames.\nintiate_replia_set.sh\n`echo \"Starting replica set initialization\"\nuntil mongo --host mongo1 --eval \"print(\\\"waited for connection\\\")\"\ndo\n   sleep 2\ndone\necho \"Connection finished\"\necho \"Creating replica set\"\n\nMONGO1IP=$(getent hosts mongo1 | awk '{ print $1 }')\nMONGO2IP=$(getent hosts mongo2 | awk '{ print $1 }')\nMONGO3IP=$(getent hosts mongo3 | awk '{ print $1 }')\n\nread -r -d '' CMD \nThis way the mongo replica set members have container IP instead of hostname in their addresses. And the container IP is reachable from the host.\nAlternatively, we can assign static IP to each container explicitly in the docker-compose file, and use static IPs when initiating the replica set. It is a similar fix but with more work.\nMac\nThe above solution unfortunately does not work for Mac, because docker container IP on Mac is not exposed on the host network interface. https://docs.docker.com/docker-for-mac/networking/#per-container-ip-addressing-is-not-possible\nThe easiest way to make it work is to add following mapping in `/etc/hosts` file:\n`127.0.0.1   mongo1 mongo2 mongo3\n`",
      "question_score": 8,
      "answer_score": 13,
      "created_at": "2021-08-06T02:45:57",
      "url": "https://stackoverflow.com/questions/68674897/mongodb-docker-replica-set-connection-error-host-not-found"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 68517126,
      "title": "First time with MongoDB + Docker - Set up from docker compose",
      "problem": "I'd like to try a project I found on GitHub, so I installed MongoDB on MacOS and now I'm trying to understand how to set up it correctly through the docker compose file in the directory. This is the docker file:\n```\n`version: '3'\nservices:\n# replica set 1\n  mongors1n1:\n    container_name: mongors1n1\n    image: mongo\n    command: mongod --shardsvr --replSet mongors1 --dbpath /data/db --port 27017\n    ports:\n      - 27017:27017\n    expose:\n      - \"27017\"\n    volumes:\n      - ~/mongo_cluster/data1:/data/db\n\n  mongors1n2:\n    container_name: mongors1n2\n    image: mongo\n    command: mongod --shardsvr --replSet mongors1 --dbpath /data/db --port 27017\n    ports:\n      - 27027:27017\n    expose:\n      - \"27017\"\n    volumes:\n      - ~/mongo_cluster/data2:/data/db\n\n  mongors1n3:\n    container_name: mongors1n3\n    image: mongo\n    command: mongod --shardsvr --replSet mongors1 --dbpath /data/db --port 27017\n    ports:\n      - 27037:27017\n    expose:\n      - \"27017\"\n\n    volumes:\n      - ~/mongo_cluster/data3:/data/db\n\n# replica set 2\n  mongors2n1:\n    container_name: mongors2n1\n    image: mongo\n    command: mongod --shardsvr --replSet mongors2 --dbpath /data/db --port 27017\n    ports:\n      - 27047:27017\n    expose:\n      - \"27017\"\n    volumes:\n      - ~/mongo_cluster/data4:/data/db\n\n  mongors2n2:\n    container_name: mongors2n2\n    image: mongo\n    command: mongod --shardsvr --replSet mongors2 --dbpath /data/db --port 27017\n    ports:\n      - 27057:27017\n    expose:\n      - \"27017\"\n    volumes:\n      - ~/mongo_cluster/data5:/data/db\n\n  mongors2n3:\n    container_name: mongors2n3\n    image: mongo\n    command: mongod --shardsvr --replSet mongors2 --dbpath /data/db --port 27017\n    ports:\n      - 27067:27017\n    expose:\n      - \"27017\"\n\n    volumes:\n      - ~/mongo_cluster/data6:/data/db\n\n  # mongo config server\n  mongocfg1:\n    container_name: mongocfg1\n    image: mongo\n    command: mongod --configsvr --replSet mongors1conf --dbpath /data/db --port 27017\n    expose:\n      - \"27017\"\n    volumes:\n      - ~/mongo_cluster/config1:/data/db\n\n  mongocfg2:\n    container_name: mongocfg2\n    image: mongo\n    command: mongod --configsvr --replSet mongors1conf --dbpath /data/db --port 27017\n    expose:\n      - \"27017\"\n    volumes:\n      - ~/mongo_cluster/config2:/data/db\n\n  mongocfg3:\n    container_name: mongocfg3\n    image: mongo\n    command: mongod --configsvr --replSet mongors1conf --dbpath /data/db --port 27017\n\n    expose:\n      - \"27017\"\n    volumes:\n      - ~/mongo_cluster/config3:/data/db\n\n# mongos router\n  mongos1:\n    container_name: mongos1\n    image: mongo\n    depends_on:\n      - mongocfg1\n      - mongocfg2\n    command: mongos --configdb mongors1conf/mongocfg1:27017,mongocfg2:27017,mongocfg3:27017 --port 27017\n    ports:\n      - 27019:27017\n    expose:\n      - \"27017\"\n\n  mongos2:\n    container_name: mongos2\n    image: mongo\n    depends_on:\n      - mongocfg1\n      - mongocfg2\n    command: mongos --configdb mongors1conf/mongocfg1:27017,mongocfg2:27017,mongocfg3:27017 --port 27017\n    ports:\n      - 27020:27017\n    expose:\n      - \"27017\"\n\n# TODO after running docker-compose\n# conf = rs.config();\n# conf.members[0].priority = 2;\n# rs.reconfig(conf);\n`\n```\nAnd this is the script to run and create the shards etc..:\n```\n`#!/bin/sh\ndocker-compose up\n# configure our config servers replica set\ndocker exec -it mongocfg1 bash -c \"echo 'rs.initiate({_id: \\\"mongors1conf\\\",configsvr: true, members: [{ _id : 0, host : \\\"mongocfg1\\\" },{ _id : 1, host : \\\"mongocfg2\\\" }, { _id : 2, host : \\\"mongocfg3\\\" }]})' | mongo\"\n\n# building replica shard\ndocker exec -it mongors1n1 bash -c \"echo 'rs.initiate({_id : \\\"mongors1\\\", members: [{ _id : 0, host : \\\"mongors1n1\\\" },{ _id : 1, host : \\\"mongors1n2\\\" },{ _id : 2, host : \\\"mongors1n3\\\" }]})' | mongo\"\ndocker exec -it mongors2n1 bash -c \"echo 'rs.initiate({_id : \\\"mongors2\\\", members: [{ _id : 0, host : \\\"mongors2n1\\\" },{ _id : 1, host : \\\"mongors2n2\\\" },{ _id : 2, host : \\\"mongors2n3\\\" }]})' | mongo\"\n\n# we add shard to the routers\ndocker exec -it mongos1 bash -c \"echo 'sh.addShard(\\\"mongors1/mongors1n1\\\")' | mongo \"\ndocker exec -it mongos1 bash -c \"echo 'sh.addShard(\\\"mongors2/mongors2n1\\\")' | mongo \"\n`\n```\nIf I try to run directly the script I get the errors:\n\nmongos1     | {\"t\":{\"$date\":\"2021-07-25T09:03:56.101+00:00\"},\"s\":\"I\",  \"c\":\"-\",        \"id\":4333222, \"ctx\":\"ReplicaSetMonitor-TaskExecutor\",\"msg\":\"RSM received error response\",\"attr\":{\"host\":\"mongocfg3:27017\",\"error\":\"HostUnreachable: Error connecting to mongocfg3:27017 (172.18.0.2:27017) :: caused by :: Connection refused\",\"replicaSet\":\"mongors1conf\",\"response\":\"{}\"}}\n\nmongos1     | {\"t\":{\"$date\":\"2021-07-25T09:03:56.101+00:00\"},\"s\":\"I\",\n\"c\":\"NETWORK\",  \"id\":4712102,\n\"ctx\":\"ReplicaSetMonitor-TaskExecutor\",\"msg\":\"Host failed in replica\nset\",\"attr\":{\"replicaSet\":\"mongors1conf\",\"host\":\"mongocfg3:27017\",\"error\":{\"code\":6,\"codeName\":\"HostUnreachable\",\"errmsg\":\"Error\nconnecting to mongocfg3:27017 (172.18.0.2:27017) :: caused by ::\nConnection\nrefused\"},\"action\":{\"dropConnections\":true,\"requestImmediateCheck\":false,\"outcome\":{\"host\":\"mongocfg3:27017\",\"success\":false,\"errorMessage\":\"HostUnreachable:\nError connecting to mongocfg3:27017 (172.18.0.2:27017) :: caused by ::\nConnection refused\"}}}}\n\nAnd other errors like:\n\nmongos1     | {\"t\":{\"$date\":\"2021-07-25T09:05:39.743+00:00\"},\"s\":\"I\",\n\"c\":\"-\",        \"id\":4939300,\n\"ctx\":\"monitoring-keys-for-HMAC\",\"msg\":\"Failed to refresh key\ncache\",\"attr\":{\"error\":\"FailedToSatisfyReadPreference: Could not find\nhost matching read preference { mode: \"nearest\" } for set\nmongors1conf\",\"nextWakeupMillis\":1800}}\n\nShouldn't docker configure all the files without the user has to? Or do I need to create something manually like the database etc.?\nEDIT: Here there are the first errors that show up when I run the script: log",
      "solution": "So here is an attempt at helping.. For the most part, the docker compose yaml file is pretty close, with exception of some minor port and binding parameters.  There is an expectation that initialization will be additional commands.   Example:\n\ndocker-compose up the environment\nrun some scripts to init the environment\n\n... but this was already part of the original post.\nSo here is a docker compose file\ndocker-compose.yml\n```\n`version: '3'\nservices:\n # mongo config server\n  mongocfg1:\n    container_name: mongocfg1\n    hostname: mongocfg1\n    image: mongo\n    command: mongod --configsvr --replSet mongors1conf --dbpath /data/db --port 27019 --bind_ip_all\n    volumes:\n      - ~/mongo_cluster/config1:/data/db\n\n  mongocfg2:\n    container_name: mongocfg2\n    hostname: mongocfg2\n    image: mongo\n    command: mongod --configsvr --replSet mongors1conf --dbpath /data/db --port 27019 --bind_ip_all\n    volumes:\n      - ~/mongo_cluster/config2:/data/db\n\n  mongocfg3:\n    container_name: mongocfg3\n    hostname: mongocfg3\n    image: mongo\n    command: mongod --configsvr --replSet mongors1conf --dbpath /data/db --port 27019 --bind_ip_all\n    volumes:\n      - ~/mongo_cluster/config3:/data/db\n\n# replica set 1\n  mongors1n1:\n    container_name: mongors1n1\n    hostname: mongors1n1\n    image: mongo\n    command: mongod --shardsvr --replSet mongors1 --dbpath /data/db --port 27018 --bind_ip_all\n    volumes:\n      - ~/mongo_cluster/data1:/data/db\n\n  mongors1n2:\n    container_name: mongors1n2\n    hostname: mongors1n2\n    image: mongo\n    command: mongod --shardsvr --replSet mongors1 --dbpath /data/db --port 27018 --bind_ip_all\n    volumes:\n      - ~/mongo_cluster/data2:/data/db\n\n  mongors1n3:\n    container_name: mongors1n3\n    hostname: mongors1n3\n    image: mongo\n    command: mongod --shardsvr --replSet mongors1 --dbpath /data/db --port 27018 --bind_ip_all\n    volumes:\n      - ~/mongo_cluster/data3:/data/db\n\n# replica set 2\n  mongors2n1:\n    container_name: mongors2n1\n    hostname: mongors2n1\n    image: mongo\n    command: mongod --shardsvr --replSet mongors2 --dbpath /data/db --port 27018 --bind_ip_all\n    volumes:\n      - ~/mongo_cluster/data4:/data/db\n\n  mongors2n2:\n    container_name: mongors2n2\n    hostname: mongors2n2\n    image: mongo\n    command: mongod --shardsvr --replSet mongors2 --dbpath /data/db --port 27018 --bind_ip_all\n    volumes:\n      - ~/mongo_cluster/data5:/data/db\n\n  mongors2n3:\n    container_name: mongors2n3\n    hostname: mongors2n3\n    image: mongo\n    command: mongod --shardsvr --replSet mongors2 --dbpath /data/db --port 27018 --bind_ip_all\n    volumes:\n      - ~/mongo_cluster/data6:/data/db\n\n# mongos router\n  mongos1:\n    container_name: mongos1\n    hostname: mongos1\n    image: mongo\n    depends_on:\n      - mongocfg1\n      - mongocfg2\n    command: mongos --configdb mongors1conf/mongocfg1:27019,mongocfg2:27019,mongocfg3:27019 --port 27017 --bind_ip_all\n    ports:\n      - 27017:27017\n\n  mongos2:\n    container_name: mongos2\n    hostname: mongos2\n    image: mongo\n    depends_on:\n      - mongocfg1\n      - mongocfg2\n    command: mongos --configdb mongors1conf/mongocfg1:27019,mongocfg2:27019,mongocfg3:27019 --port 27017 --bind_ip_all\n    ports:\n      - 27016:27017\n`\n```\n... and some scripts to finalize the initialization...\n```\n`docker-compose up -d\n`\n```\n... Give it a few seconds to wind up, then issue...\n```\n`# Init the replica sets (use the MONGOS host)\ndocker exec -it mongos1 bash -c \"echo 'rs.initiate({_id: \\\"mongors1conf\\\",configsvr: true, members: [{ _id : 0, host : \\\"mongocfg1:27019\\\", priority: 2 },{ _id : 1, host : \\\"mongocfg2:27019\\\" }, { _id : 2, host : \\\"mongocfg3:27019\\\" }]})' | mongo --host mongocfg1:27019\"\ndocker exec -it mongos1 bash -c \"echo 'rs.initiate({_id : \\\"mongors1\\\", members: [{ _id : 0, host : \\\"mongors1n1:27018\\\", priority: 2 },{ _id : 1, host : \\\"mongors1n2:27018\\\" },{ _id : 2, host : \\\"mongors1n3:27018\\\" }]})' | mongo --host mongors1n1:27018\"\ndocker exec -it mongos1 bash -c \"echo 'rs.initiate({_id : \\\"mongors2\\\", members: [{ _id : 0, host : \\\"mongors2n1:27018\\\", priority: 2 },{ _id : 1, host : \\\"mongors2n2:27018\\\" },{ _id : 2, host : \\\"mongors2n3:27018\\\" }]})' | mongo --host mongors2n1:27018\"\n`\n```\n... again, give 10-15 seconds to allow the system to adjust to recent commands...\n```\n`# ADD TWO SHARDS (mongors1, and mongors2)\ndocker exec -it mongos1 bash -c \"echo 'sh.addShard(\\\"mongors1/mongors1n1:27018,mongors1n2:27018,mongors1n2:27018\\\")' | mongo\"\ndocker exec -it mongos1 bash -c \"echo 'sh.addShard(\\\"mongors2/mongors2n1:27018,mongors2n2:27018,mongors2n3:27018\\\")' | mongo\"\n`\n```\nNow, try to connect to the mongos from the host with docker running (assumes you have mongo shell installed on this host).  Use 2 mongos hosts as the seed list.\n```\n`mongo --host \"localhost:27017,localhost:27016\"\n`\n```\nComments\nNotice how the priority for node0 is set to a priority of 2 in the init() call?\nNotice how the config servers are all port 27019 - this follows recommendations by MongoDB.\nNotice how the shard servers are all port 27018 - again, following mongo recommendations.\nThe mongos expose 2 ports 27017 (the natural port for MongoDB) and also port 27016 (a secondary mongos for high availability).\nThe config servers and the shard servers do not expose their respective ports - for security reasons.  Should be using the mongos to get to the data.  If need to have these ports open for administrative purposes simply add to the docker compose file.\nThe replica-set intercommunication is not using authentication.  This is a security no-no.  Need to decide which auth mechanism is best for your scenario - can use keyfile (just a text file that is identical among the members of the replica set) or x509 certs.  If going with x509 then you need to include the CA.cert in each docker container for reference along with the individual cert per server with proper host name alignment.  Would need to add the startup configuration item for the mongod processes to use whichever auth method was selected.\nLogging is not specified.  It probably makes sense to set the logging output of the mongod and mongos to the default location of /var/log/mongodb/mongod.log and /var/log/mongodb/mongos.log for these.  Without specifying a logging strategy I believe mongo logs to standard out, which is suppressed if running `docker-compose up -d`.\nSuperuser:  No users are yet created on the system. Usually for every replica set I stand up before adding it to a sharded cluster I like to add a super user account - one having root access - so if I need to make administrative changes at the replica set level I can.  With the docker-compose approach you can create a super user from the mongos perspective and perform most all operations needed on a sharded cluster, but still - I like having the replica set user available.\nOS tunables - Mongo likes to take up all the system resources.  For a shared ecosystem where one physical host is hosting a bunch of mongo processes, you might want to consider specifying the wiredTiger cache size, etc.  WiredTiger by default wants (`System Memory Size` - 1 GB) / 2.  Also, you would benefit from setting ulimits to proper values - i.e., 64000 file handles per user is a good start - mongo potentially likes to use a lot of files.  Also, filesystem should be mounted somewhere having xfs.  This strategy is using the host system users home directory for database data directories.  A more thoughtful approach could be welcomed here.\nAnything else?\nI am sure I am missing something.  If you have any questions, please leave a comment and I will reply.\nUpdate 1\nThe above docker-compose.yml file was missing the hostname attribute for some of the hosts, and this was causing balancer issues, so I have edited the docker-compose.yml to include hostname on all hosts.\nAlso, the addShard() method only referred to one host of the replica set.  For completeness I added the other hosts to the addShard() method described above.\nFollowing these steps will result in a brand new sharded cluster, but there are no user databases yet.  As such, no user databases are sharded.  So let's take a moment to add a database and shard it, then view the shard distributions (A.K.A., balancer results).\nWe must connect to the database via the mongos (as described above).  This example assumes the use of the mongo shell.\n```\n`mongo --host \"localhost:27017,localhost:27016\"\n`\n```\nDatabases in Mongo can be created a variety of ways.  While there is no explicit database create command, there is an explicit create collection command (db.createCollection()).  We must first set the database context using a 'use ' command...\n```\n`use mydatabase\ndb.createCollection(\"mycollection\")\n`\n```\n... but rather than use this command we can create a database and collection by creating an index on a non-existing collection.  (If you already created the collection, no worries, this next command should still be successful).\n```\n`use mydatabase\ndb.mycollection.createIndex({lastName: 1, creationDate: 1})\n`\n```\nIn this example, I created a compound index on two fields...\n\nlastName\ncreationDate\n\n... on a collection that does not yet exist, on a database that does not yet exist.  Once I issue this command, both the database and the collection will be created.  Furthermore, I now have the basis for a shard key - the key to which sharding distribution will be based.  This shard key will be based on this new index having these two fields.\nShard the database\nAssuming I have issued the createIndex command, I can now turn on sharding at the database and issue the shardCollection command...\n```\n`sh.enableSharding(\"mydatabase\")\nsh.shardCollection(\"mydatabase.mycollection\", { \"lastName\": 1, \"creationDate\": 1})\n`\n```\nNotice how the command 'shardCollection()' refers to our indexed fields created earlier?  Assuming sharding has been successfully applied, we can now view the distribution of data by issuing the sh.status() command\n```\n`sh.status()\n`\n```\nExample of output: (new collection, no data yet, thus no real distribution of data - need to insert more than 64MB of data such that there is more than one chunk to distribute)\n```\n`mongos> sh.status()\n--- Sharding Status --- \n  sharding version: {\n    \"_id\" : 1,\n    \"minCompatibleVersion\" : 5,\n    \"currentVersion\" : 6,\n    \"clusterId\" : ObjectId(\"6101c030a98b2cc106034695\")\n  }\n  shards:\n        {  \"_id\" : \"mongors1\",  \"host\" : \"mongors1/mongors1n1:27018,mongors1n2:27018,mongors1n3:27018\",  \"state\" : 1,  \"topologyTime\" : Timestamp(1627504744, 1) }\n        {  \"_id\" : \"mongors2\",  \"host\" : \"mongors2/mongors2n1:27018,mongors2n2:27018,mongors2n3:27018\",  \"state\" : 1,  \"topologyTime\" : Timestamp(1627504753, 1) }\n  active mongoses:\n        \"5.0.1\" : 2\n  autosplit:\n        Currently enabled: yes\n  balancer:\n        Currently enabled: yes\n        Currently running: no\n        Failed balancer rounds in last 5 attempts: 0\n        Migration results for the last 24 hours: \n                No recent migrations\n  databases:\n        {  \"_id\" : \"config\",  \"primary\" : \"config\",  \"partitioned\" : true }\n        {  \"_id\" : \"mydatabase\",  \"primary\" : \"mongors2\",  \"partitioned\" : true,  \"version\" : {  \"uuid\" : UUID(\"bc890722-00c6-4cbe-a3e1-eab9692faf93\"),  \"timestamp\" : Timestamp(1627504768, 2),  \"lastMod\" : 1 } }\n                mydatabase.mycollection\n                        shard key: { \"lastName\" : 1, \"creationDate\" : 1 }\n                        unique: false\n                        balancing: true\n                        chunks:\n                                mongors2    1\n                        { \"lastName\" : { \"$minKey\" : 1 }, \"creationDate\" : { \"$minKey\" : 1 } } -->> { \"lastName\" : { \"$maxKey\" : 1 }, \"creationDate\" : { \"$maxKey\" : 1 } } on : mongors2 Timestamp(1, 0) \n`\n```\nInsert some data\nTo test out the sharding we can add some test data.  Again, we want to distribute by lastName, and creationDate.\nIn mongoshell we can run javascript.  Here is a script that will create test records such that data will be split and balanced.  This will create 500,000 fake records.  We need more than 64MB of data to create another chunk to balance.  500,000 records will make approx. 5 chunks.  This takes a couple of minutes to run and complete.\n```\n`use mydatabase\n\nfunction randomInteger(min, max) {\n    return Math.floor(Math.random() * (max - min) + min);\n} \n\nfunction randomAlphaNumeric(length) {\n  var result = [];\n  var characters = 'abcdef0123456789';\n  var charactersLength = characters.length;\n\n  for ( var i = 0; i Give a few minutes and review in the mongoshell, if we now look at the shard status we should see chunks distributed across both shards...\n```\n`sh.status()\n`\n```\n... we should see something similar to ...\n```\n`mongos> sh.status()\n--- Sharding Status --- \n  sharding version: {\n    \"_id\" : 1,\n    \"minCompatibleVersion\" : 5,\n    \"currentVersion\" : 6,\n    \"clusterId\" : ObjectId(\"6101c030a98b2cc106034695\")\n  }\n  shards:\n        {  \"_id\" : \"mongors1\",  \"host\" : \"mongors1/mongors1n1:27018,mongors1n2:27018,mongors1n3:27018\",  \"state\" : 1,  \"topologyTime\" : Timestamp(1627504744, 1) }\n        {  \"_id\" : \"mongors2\",  \"host\" : \"mongors2/mongors2n1:27018,mongors2n2:27018,mongors2n3:27018\",  \"state\" : 1,  \"topologyTime\" : Timestamp(1627504753, 1) }\n  active mongoses:\n        \"5.0.1\" : 2\n  autosplit:\n        Currently enabled: yes\n  balancer:\n        Currently enabled: yes\n        Currently running: yes\n        Collections with active migrations: \n                config.system.sessions started at Wed Jul 28 2021 20:44:25 GMT+0000 (UTC)\n        Failed balancer rounds in last 5 attempts: 0\n        Migration results for the last 24 hours: \n                60 : Success\n  databases:\n        {  \"_id\" : \"config\",  \"primary\" : \"config\",  \"partitioned\" : true }\n                config.system.sessions\n                        shard key: { \"_id\" : 1 }\n                        unique: false\n                        balancing: true\n                        chunks:\n                                mongors1    965\n                                mongors2    59\n                        too many chunks to print, use verbose if you want to force print\n        {  \"_id\" : \"mydatabase\",  \"primary\" : \"mongors2\",  \"partitioned\" : true,  \"version\" : {  \"uuid\" : UUID(\"bc890722-00c6-4cbe-a3e1-eab9692faf93\"),  \"timestamp\" : Timestamp(1627504768, 2),  \"lastMod\" : 1 } }\n                mydatabase.mycollection\n                        shard key: { \"lastName\" : 1, \"creationDate\" : 1 }\n                        unique: false\n                        balancing: true\n                        chunks:\n                                mongors1    2\n                                mongors2    3\n                        { \"lastName\" : { \"$minKey\" : 1 }, \"creationDate\" : { \"$minKey\" : 1 } } -->> {\n                            \"lastName\" : \"00001276\",\n                            \"creationDate\" : ISODate(\"2021-07-28T20:42:00.867Z\")\n                        } on : mongors1 Timestamp(2, 0) \n                        {\n                            \"lastName\" : \"00001276\",\n                            \"creationDate\" : ISODate(\"2021-07-28T20:42:00.867Z\")\n                        } -->> {\n                            \"lastName\" : \"623292c2\",\n                            \"creationDate\" : ISODate(\"2021-07-28T20:42:01.046Z\")\n                        } on : mongors1 Timestamp(3, 0) \n                        {\n                            \"lastName\" : \"623292c2\",\n                            \"creationDate\" : ISODate(\"2021-07-28T20:42:01.046Z\")\n                        } -->> {\n                            \"lastName\" : \"c3f2a99a\",\n                            \"creationDate\" : ISODate(\"2021-07-28T20:42:06.474Z\")\n                        } on : mongors2 Timestamp(3, 1) \n                        {\n                            \"lastName\" : \"c3f2a99a\",\n                            \"creationDate\" : ISODate(\"2021-07-28T20:42:06.474Z\")\n                        } -->> {\n                            \"lastName\" : \"ed75c36c\",\n                            \"creationDate\" : ISODate(\"2021-07-28T20:42:03.984Z\")\n                        } on : mongors2 Timestamp(1, 6) \n                        {\n                            \"lastName\" : \"ed75c36c\",\n                            \"creationDate\" : ISODate(\"2021-07-28T20:42:03.984Z\")\n                        } -->> { \"lastName\" : { \"$maxKey\" : 1 }, \"creationDate\" : { \"$maxKey\" : 1 } } on : mongors2 Timestamp(2, 1) \n`\n```\n... Here we can see evidence of balancing activites. See label \"chunks\" for mongors1 and mongors2.  While it is balancing our test collection it is also pre-splitting and balancing a different collection for session data.  I believe this is a one-time system automation.\nI hope these details help.  Please let me know if you have any other questions.",
      "question_score": 8,
      "answer_score": 12,
      "created_at": "2021-07-25T11:10:18",
      "url": "https://stackoverflow.com/questions/68517126/first-time-with-mongodb-docker-set-up-from-docker-compose"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 72048051,
      "title": "PyMongo Auth Failure: {&#39;ok&#39;: 0.0, &#39;errmsg&#39;: &#39;Authentication failed.&#39;, &#39;code&#39;: 18, &#39;codeName&#39;: &#39;AuthenticationFailed&#39;",
      "problem": "Curious thing, I am running SLS Python Lambdas in a Docker container with the MongoDB instance in a separate container. I am able to authenticate from an exposed port on my host to connect to the instance.\nHowever, from inside SLS container for whatever reason it does not want to connect :S\n`# mongodb://root:password@mongodb/test_db?retryWrites=true&w=majority\ndb_uri = \"mongodb://\" + usr + \":\" + pwd + \"@\" + url + \"/test_db?retryWrites=true&w=majority\"\nclient = pymongo.MongoClient(db_uri)\n\ndb = client[mongo_db_name]\ncollection = db[mongo_collection_name]\n`\n```\n`version: \"3.8\"\nservices:\n  mongodb:\n    image : mongo\n    environment:\n      - MONGO_INITDB_ROOT_USERNAME=root\n      - MONGO_INITDB_ROOT_PASSWORD=password\n      - MONGO_INITDB_DATABASE=test_db\n      - PUID=1000\n      - PGID=1000\n    ports: \n      - \"27000:27017\"\n    restart: unless-stopped\n    volumes:\n      - ./mongodb/init.js:/docker-entrypoint.initdb.d/init.js\n\n  python:\n    build: python\n    depends_on:\n      - mongodb\n    ports: \n      - \"3000:3000\"\n    restart: unless-stopped\n    volumes:\n      - ./python:/usr/src/app\n`\n```",
      "solution": "Needed to add authSource\n(https://pymongo.readthedocs.io/en/stable/examples/authentication.html#default-database-and-authsource)\n`db_uri = \"mongodb://\" + usr + \":\" + pwd + \"@\" + \\\n    url + \"/test_db?authSource=admin&retryWrites=true&w=majority\"\n`",
      "question_score": 8,
      "answer_score": 10,
      "created_at": "2022-04-28T19:37:09",
      "url": "https://stackoverflow.com/questions/72048051/pymongo-auth-failure-ok-0-0-errmsg-authentication-failed-code-18"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 70296667,
      "title": "How do I connect to MongoDB, running in Github codespaces, using MongoDB Compass?",
      "problem": "I'm trying out Github codespaces, specifically the \"Node.js & Mongo DB\" default settings.\nThe port is forwarded, and my objective is to connect with MongoDB Compass running on my local machine.\nThe address forwarded to `27017` is something like `https://.githubpreview.dev/`\nMy attempt\nI attempted to use the following connection string, but it did not work in MongoDB compass. It failed with `No addresses found at host`. I'm actually unsure about how I even determine if MongoDB is actually running in the Github codespace?\n```\n`mongodb+srv://root:example@.githubpreview.dev/\n`\n```\n.devcontainer files\n\n`docker-compose.yml`\n```\n`version: '3.8'\n\nservices:\n  app:\n    build:\n      context: .\n      dockerfile: Dockerfile\n      args:\n        # Update 'VARIANT' to pick an LTS version of Node.js: 16, 14, 12.\n        # Append -bullseye or -buster to pin to an OS version.\n        # Use -bullseye variants on local arm64/Apple Silicon.\n        VARIANT: \"16\"\n    volumes:\n      - ..:/workspace:cached\n    init: true\n\n    # Overrides default command so things don't shut down after the process ends.\n    command: sleep infinity\n\n    # Runs app on the same network as the database container, allows \"forwardPorts\" in devcontainer.json function.\n    network_mode: service:db\n    # Uncomment the next line to use a non-root user for all processes.\n    # user: node\n\n    # Use \"forwardPorts\" in **devcontainer.json** to forward an app port locally. \n    # (Adding the \"ports\" property to this file will not forward from a Codespace.)\n\n  db:\n    image: mongo:latest\n    restart: unless-stopped\n    volumes:\n      - mongodb-data:/data/db\n    # Uncomment to change startup options\n    environment:\n     MONGO_INITDB_ROOT_USERNAME: root\n     MONGO_INITDB_ROOT_PASSWORD: example\n     MONGO_INITDB_DATABASE: foo\n\n    # Add \"forwardPorts\": [\"27017\"] to **devcontainer.json** to forward MongoDB locally.\n    # (Adding the \"ports\" property to this file will not forward from a Codespace.)\n\nvolumes:\n  mongodb-data: null\n\n`\n```\nAnd a `devcontainer.json` file\n```\n`// For format details, see https://aka.ms/devcontainer.json. For config options, see the README at:\n// https://github.com/microsoft/vscode-dev-containers/tree/v0.203.0/containers/javascript-node-mongo\n// Update the VARIANT arg in docker-compose.yml to pick a Node.js version\n{\n    \"name\": \"Node.js & Mongo DB\",\n    \"dockerComposeFile\": \"docker-compose.yml\",\n    \"service\": \"app\",\n    \"workspaceFolder\": \"/workspace\",\n\n    // Set *default* container specific settings.json values on container create.\n    \"settings\": {},\n\n    // Add the IDs of extensions you want installed when the container is created.\n    \"extensions\": [\n        \"dbaeumer.vscode-eslint\",\n        \"mongodb.mongodb-vscode\" \n    ],\n\n    // Use 'forwardPorts' to make a list of ports inside the container available locally.\n    \"forwardPorts\": [3000, 27017],\n\n    // Use 'postCreateCommand' to run commands after the container is created.\n    // \"postCreateCommand\": \"yarn install\",\n\n    // Comment out connect as root instead. More info: https://aka.ms/vscode-remote/containers/non-root.\n    \"remoteUser\": \"node\",\n    \"features\": {\n        \"git\": \"os-provided\"\n    }\n}\n`\n```\nand finally  a Docker file:\n```\n`# [Choice] Node.js version (use -bullseye variants on local arm64/Apple Silicon): 16, 14, 12, 16-bullseye, 14-bullseye, 12-bullseye, 16-buster, 14-buster, 12-buster\nARG VARIANT=16-bullseye\nFROM mcr.microsoft.com/vscode/devcontainers/javascript-node:0-${VARIANT}\n\n# Install MongoDB command line tools if on buster and x86_64 (arm64 not supported)\nARG MONGO_TOOLS_VERSION=5.0\nRUN . /etc/os-release \\\n    && if [ \"${VERSION_CODENAME}\" = \"buster\" ] && [ \"$(dpkg --print-architecture)\" = \"amd64\" ]; then \\\n        curl -sSL \"https://www.mongodb.org/static/pgp/server-${MONGO_TOOLS_VERSION}.asc\" | gpg --dearmor > /usr/share/keyrings/mongodb-archive-keyring.gpg \\\n        && echo \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/mongodb-archive-keyring.gpg] http://repo.mongodb.org/apt/debian $(lsb_release -cs)/mongodb-org/${MONGO_TOOLS_VERSION} main\" | tee /etc/apt/sources.list.d/mongodb-org-${MONGO_TOOLS_VERSION}.list \\\n        && apt-get update && export DEBIAN_FRONTEND=noninteractive \\\n        && apt-get install -y mongodb-database-tools mongodb-mongosh \\\n        && apt-get clean -y && rm -rf /var/lib/apt/lists/*; \\\n    fi\n\n# [Optional] Uncomment this section to install additional OS packages.\n# RUN apt-get update && export DEBIAN_FRONTEND=noninteractive \\\n#     && apt-get -y install --no-install-recommends \n\n# [Optional] Uncomment if you want to install an additional version of node using nvm\n# ARG EXTRA_NODE_VERSION=10\n# RUN su node -c \"source /usr/local/share/nvm/nvm.sh && nvm install ${EXTRA_NODE_VERSION}\"\n\n# [Optional] Uncomment if you want to install more global node modules\n# RUN su node -c \"npm install -g \"\n`\n```\nUpdate\nI also posted here in the MongoDB community, but no help...",
      "solution": "As @iravinandan said you need to set up a tunnel.\nPublishing a port alone won't help as all incoming requests are  going through an http proxy.\nIf you `dig CNAME .githubpreview.dev` you will see it's  github-codespaces.app.online.visualstudio.com.  You can put anything in the githubpreview.dev subdomain and it will still be resolved on the DNS level.\nThe proxy relies on HTTP Host header to route the request to correct upstream so it will work for HTTP protocols only.\nTo use any other protocol (MongoDb wire protocol in your case) you need to set up a TCP tunnel from codespaces to your machine.\nSimplest set up - direct connection\nAt the time of writing the default Node + Mongo codespace uses Debian buster, so ssh port forwarding would be the obvious choice. In the codespace/VSCode terminal:\n```\n`ssh -R 27017:localhost:27017 your_public_ip\n`\n```\nThen in your compas connect to\n```\n`mongodb://localhost:27017\n`\n```\n\nIt will require your local machine to run sshd of course, have a white IP (or at least your router should forward incoming ssh traffic to your computer) and allow it in the firewall. You can pick any port if 27017 is already being used locally.\nIt's the simplest set up but it exposes your laptop to the internet, and it's just a matter of time to get it infected.\nA bit more secure - jumpbox in the middle\nTo keep your local system behind DMZ you can set up a jumpbox instead - a minimalistic disposable linux box somewhere in the internet, which will be used to chain 2 tunnels:\n\nRemote port forwarding from codespace to the jumpbox\nLocal port forwarding from your laptop to the jumpbox\n\nThe same\n```\n`mongodb://localhost:27017\n`\n```\non mongo compas.\nThe jumpbox have to expose sshd to the internet, but you can minimise risks by hardening its security. After all it doesn't do anything but proxy traffic. EC2   nano will be more than enough, just keep in mind large data transfers might be expensive.\nHassle-free tunnel-as-a-service\nSomething you can try in 5 min. ngrok has been around for more than a decade and it does exactly this - it sells tunnels (with some free tier sufficient for the demo).\nIn your codespace/VScode terminal:\n```\n`npm i ngrok --save-dev\n`\n```\nTo avoid installing every time but ensure you don't ship with production code.\nYou will need to register an account on ngrok (SSO with github will do) to get an authentication code and pass it to the codespaces/VSCode terminal:\n```\n`./node_modules/.bin/ngrok authtoken \n`\n```\nPlease remember it saves the token to the home directory which will be wiped after rebuild. Once authorised you can open the tunnel in the codespaces/VSCode terminal:\n```\n`./node_modules/.bin/ngrok tcp 27017\n`\n```\nCodespace will automatically forward the port:\n\nAnd the terminal will show you some stats (mind the free tier limit) and connection string:\n\nThe subdomain and port will change every time you open the tunnel.\nFrom the image above the connection parameters for mongodb compas will be:\n```\n`mongodb://0.tcp.ngrok.io:18862\n`\n```\nwith authorization parameters on mongodb level as needed.\nAgain, keep in mind you leave your mongodb exposed to the internet (0.tcp.ngrok.io:18862), and mongo accepts unauthenticated connections.\nI wouldn't leave it open for longer than necessary.\nUse built-in mongodb client\nThe node + mongo environment comes with handy VScode plugins pre-installed:\n\nOf course it lacks many of compas analytical tools but it works out of the box and is sufficient for development.\nJust open the plugin and connect to localhost:\n\nCompass D.I.Y\nThe best option to get compass functionality without compromising security and achieve zero-config objective is to host compass yourself. It's an electron application and works perfectly in a browser in Mongodb Atlas.\nThe source code is available at https://github.com/mongodb-js/compass.\nWith a bit of effort you can craft a docker image to host compass,  include this image into docker-compose, and forward the port in devcontainer.json\nGithub codespaces will take care of authentication (keep the forwarded port private so only owner of the space will have access to it). All communication from desktop to compass will be over https, and compass to mongodb will be local to the docker network. Security-wise it will be on par with VSCode mongodb plugin",
      "question_score": 8,
      "answer_score": 9,
      "created_at": "2021-12-09T22:02:41",
      "url": "https://stackoverflow.com/questions/70296667/how-do-i-connect-to-mongodb-running-in-github-codespaces-using-mongodb-compass"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 68196502,
      "title": "Failed to connect to a server with Golang due x509 certificate relies on legacy Common Name field",
      "problem": "I'm trying to connect on a mongodb server, to connect I have to provide a CA cert file and also tls cert file.\nWhen I use the following command I don't have issue\n```\n`$ mongo --host customhost:port DB --authenticationDatabase=DB -u ACCOUNT -p PWD --tls --tlsCAFile /etc/ca-files/new-mongo.ca.crt --tlsCertificateKeyFile /etc/ca-files/new-mongo-client.pem \n`\n```\nBut when I try to connect with mongo (and also tested with just a tls client) I have the following error:\n```\n`failed to connect: x509: certificate relies on legacy Common Name field, use SANs or temporarily enable Common Name matching with GODEBUG=x509ignoreCN=0\n`\n```\nIf I use the env variable everything works well but I would like to know how to fix it without having to use it.\n`const CONFIG_DB_CA = \"/etc/ca-files/new-mongo.ca.crt\"\n\nfunc main() {\n    cer, err := tls.LoadX509KeyPair(\"mongo-server.crt\", \"mongo-server.key\")\n    if err != nil {\n        log.Println(err)\n        return\n    }\n\n    roots := x509.NewCertPool()\n    ca, err := ioutil.ReadFile(CONFIG_DB_CA)\n    if err != nil {\n        fmt.Printf(\"Failed to read or open CA File: %s.\\n\", CONFIG_DB_CA)\n        return\n    }\n    roots.AppendCertsFromPEM(ca)\n\n    tlsConfig := &tls.Config{\n        Certificates: []tls.Certificate{cer},\n        RootCAs:      roots,\n    }\n\n    conn, err := tls.Dial(\"tcp\", \"customhost:port\", tlsConfig)\n    if err != nil {\n        fmt.Printf(\"failed to connect: %v.\\n\", err)\n        return\n    }\n\n    err = conn.VerifyHostname(\"customhost\")\n    if err != nil {\n        panic(\"Hostname doesn't match with certificate: \" + err.Error())\n    }\n    for i, cert := range conn.ConnectionState().PeerCertificates {\n        prefix := fmt.Sprintf(\"CERT%d::\", i+1)\n        fmt.Printf(\"%sIssuer: %s\\n\", prefix, cert.Issuer)\n        fmt.Printf(\"%sExpiry: %v\\n\", prefix, cert.NotAfter.Format(time.RFC850))\n        fmt.Printf(\"%sDNSNames: %v\\n\\n\", prefix, cert.DNSNames)\n    }\n    \n    fmt.Printf(\"Success!\")\n}\n\n`\nCertificates:\n`$ openssl x509 -in /etc/ca-files/new-mongo.ca.crt -text -noout\nCertificate:\n    Data:\n        Version: 3 (0x2)\n        Serial Number:\n            ....\n        Signature Algorithm: sha256WithRSAEncryption\n        Issuer: C = FR, ST = IDF, L = Paris, O = COMP, OU = IT, CN = newmongo\n        Validity\n            Not Before: Jun 30 13:02:12 2021 GMT\n            Not After : Jun 30 13:02:12 2023 GMT\n        Subject: C = FR, ST = IDF, L = Paris, O = COMP, OU = IT, CN = newmongo\n\n...\n\n        X509v3 extensions:\n            X509v3 Subject Key Identifier: \n                ...\n            X509v3 Authority Key Identifier: \n                ....\n\n            X509v3 Basic Constraints: critical\n                CA:TRUE\n`\n`$ openssl x509 -in /etc/ca-files/newmongo-client.pem -text -noout \nCertificate:\n    Data:\n        Version: 3 (0x2)\n        Serial Number:\n            ...\n        Signature Algorithm: sha256WithRSAEncryption\n        Issuer: C = FR, ST = IDF, L = Paris, O = COMP, OU = IT, CN = newmongo\n        Validity\n            Not Before: Jun 30 13:17:25 2021 GMT\n            Not After : Jun 30 13:17:25 2023 GMT\n        Subject: C = FR, ST = IDF, L = Paris, O = COMP, OU = IT, CN = newmongo-client\n...\n        X509v3 extensions:\n            X509v3 Subject Alternative Name: \n                DNS:customhost:port, DNS:customhost, DNS:newmongo-client\n`\nI'm a bit stuck and don't know if the problem is my code configuration of tls and the way I loaded certificates or if it comes from the SSL certificate misconfiguration but from what certificates look fine.\nI feel like loaded certificate are ignored for any reason.",
      "solution": "You need to fix the problem at the source and generate a certificate with a `DNS` `SAN` field - then the `Go` runtime check will disappear.\nThis is achievable with `openssl` but is tricky as it requires a config file - as SAN field options are too broad to fit into simple command-line options.\nThe general gist is, create a CSR:\n```\n`openssl req -new \\\n    -subj \"${SUBJ_PREFIX}/CN=${DNS}/emailAddress=${EMAIL}\" \\\n            -key \"${KEY}\" \\\n    -addext \"subjectAltName = DNS:${DNS}\" \\\n    -out \"${CSR}\"\n`\n```\nand then sign the `CSR` with your `root CA`:\n```\n`openssl ca \\\n        -create_serial \\\n                -cert \"${ROOT_CRT}\" \\\n        -keyfile \"${ROOT_KEY}\" \\\n                -days \"${CERT_LIFETIME}\" \\\n                -in \"${CSR}\" \\\n        -batch \\\n        -config \"${CA_CONF}\" \\\n                -out \"${CRT}\"\n`\n```\n`CA_CONF` referenced above looks something like this:\n```\n`[ ca ]\ndefault_ca      = my_ca\n\n[ my_ca ]\ndir             = ./db\ndatabase            = $dir/index.txt\nserial              = $dir/serial\nnew_certs_dir   = $dir/tmp\nx509_extensions = my_cert\nname_opt            = ca_default\ncert_opt            = ca_default\ndefault_md          = default\npolicy              = policy_match\n# 'copy_extensions' will copy over SAN (\"X509v3 Subject Alternative Name\") from CSR\ncopy_extensions = copy\n\n[ my_cert ]\nbasicConstraints        = CA:FALSE\nnsComment               = \"generated by https://github.com/me/my-pki\"\nsubjectKeyIdentifier    = hash\nauthorityKeyIdentifier  = keyid,issuer\n\n[ policy_match ]\n# ensure CSR fields match that of delivered Cert\ncountryName             = match\nstateOrProvinceName     = match\norganizationName        = match\norganizationalUnitName  = optional\ncommonName              = supplied\nemailAddress            = optional\n`\n```\nInspecting the resulting server cert:\n```\n`openssl x509 -in server.crt -noout -text\n`\n```\nshould then have a `SAN` section like:\n```\n`X509v3 Subject Alternative Name: \n    DNS:myserver.com\n`\n```",
      "question_score": 8,
      "answer_score": 9,
      "created_at": "2021-06-30T16:15:35",
      "url": "https://stackoverflow.com/questions/68196502/failed-to-connect-to-a-server-with-golang-due-x509-certificate-relies-on-legacy"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 74058894,
      "title": "event loop is closed in a celery worker",
      "problem": "I am facing multiple issues using an async ODM inside my celery worker\nFirst i wasn't able to init my database models using celery worker signal\ni am using beanie for the db connection.\nFirst Implementation\n```\n`from asyncer import syncify\nfrom asgiref.sync import async_to_sync \nclient = AsyncIOMotorClient(\n    DATABASE_URL, uuidRepresentation=\"standard\" )\n    db = client[DB_NAME]\n\nasync def db_session():\n        await init_beanie(\n        database=db,\n        document_models=[Project, User],\n    )\n@worker_ready.connect\ndef startup_celery_ecosystem(**kwargs):\n            logger.info('Startup celery worker process')\n            async_to_sync(db_session)()\n            logger.info('FINISHED : Startup celery worker process')\nasync def get_users():\n    users = User.find()\n    users_list = await users.to_list()\n    return users_list\n\n@celery_app.task\ndef pool_db():\n    async_to_sync(get_users)()\n    #syncify(get_users)() same error User class is not initialized yet (init_beanie should have already initialized all the models )\n`\n```\nWith this implementation i could not access my database using the User and Project class and it raises an error as if User and Project haven't been instantiated yet\nThe workaround is to call db_session() at the module level which solve the problem with database models instantiation, But now when querying the database i get the following error from my celery task\n\nRuntimeError: Event loop is closed\n\nSecond Implementation\n```\n`from asyncer import syncify\nfrom asgiref.sync import async_to_sync client = AsyncIOMotorClient(\nDATABASE_URL, uuidRepresentation=\"standard\" )\ndb = client[DB_NAME]\n\nasync def db_session():\n        await init_beanie(\n        database=db,\n        document_models=[Project, User],\n    )\n# now  init_beanie at module level\nasync_to_sync(db_session)()\n\nasync def get_users():\n    users = User.find()\n    users_list = await users.to_list()\n    return users_list\n\n@celery_app.task\ndef pool_db():\n    # this raises the following Runtime error RuntimeError('Event loop is closed')\n    async_to_sync(get_users)()\n    #syncify(get_users)() same error \n`\n```\ni am not very familiar with how asyncio is implemented and how asyncer and asgiref allows to run async code inside a sync thread which left me confused, any help would be appriciated",
      "solution": "After many investigation using flower for monitoring workers and logging the workers Id ( processes ids) it turns out that Celery worker itself does not process any tasks, it spawns other child processes ( this is my case because i am using the default executor pool which is prefork), while the signal ( worker_ready.connect ) is only run on the supervisor process Celery worker and not the childs, and since processes are isoleted memory wise, this means that you can't have access to db connection or any initialized ressources from the child processes.\nNow i am using celery with gevent which only spawn 1 process, because initially my project doesn't require CPU heavy tasks which means i don't need all the cpu power provided by the prefork pool",
      "question_score": 8,
      "answer_score": 3,
      "created_at": "2022-10-13T18:14:46",
      "url": "https://stackoverflow.com/questions/74058894/event-loop-is-closed-in-a-celery-worker"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 66760225,
      "title": "Error serializing Next.js in getStaticProps function?",
      "problem": "I'm using `getStaticProps()` function and I get this error for no reason:\n\nError: Error serializing `.posts[0]` returned from `getStaticProps` in\n\"/\". Reason: `object` (\"[object Object]\") cannot be serialized as\nJSON. Please only return JSON serializable data types.\n\nI'm also using a mongoDb database, the `connectDb()` function runs the `mongoose.connect()` function and connects to the database. also, the `console.log()`s return valid JSON format data, I don't know what is causing this issue, here is my code:\n```\n`export const getStaticProps: GetStaticProps = async (\n  context: GetStaticPropsContext\n) => {\n  await connectDb()\n  const count = await PostModel.countDocuments()\n  const posts = await PostModel.find()\n  console.log(posts)\n  console.log(count)\n  return {\n    props: { posts: posts, count: count },\n    revalidate: 10,\n  }\n}\n`\n```",
      "solution": "Use lean it will convert in a plain JavaScript object.\n```\n`const posts = await PostModel.find().lean();\n`\n```\nor you can try serialization via .toJSON",
      "question_score": 8,
      "answer_score": 2,
      "created_at": "2021-03-23T10:13:44",
      "url": "https://stackoverflow.com/questions/66760225/error-serializing-next-js-in-getstaticprops-function"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 75410705,
      "title": "Please report this issue to the mongodb/brew - PR to fix it?",
      "problem": "brew upgrade:\n```\n`Warning: Calling plist_options is deprecated! Use service.require_root instead.\nPlease report this issue to the mongodb/brew tap (not Homebrew/brew or Homebrew/core), or even better, submit a PR to fix it:\n  /usr/local/Homebrew/Library/Taps/mongodb/homebrew-brew/Formula/mongodb-community.rb:55\n`\n```\nbrew doctor:\n```\n`Please note that these warnings are just used to help the Homebrew maintainers\nwith debugging if you file an issue. If everything you use Homebrew for is\nworking fine: please don't worry or file an issue; just ignore this. Thanks!\n\nWarning: No Cask quarantine support available: unknown reason.\nWarning: Calling plist_options is deprecated! Use service.require_root instead.\nPlease report this issue to the mongodb/brew tap (not Homebrew/brew or Homebrew/core), or even better, submit a PR to fix it:\n  /usr/local/Homebrew/Library/Taps/mongodb/homebrew-brew/Formula/mongodb-community.rb:55\n`\n```\nmongodb-community:\n```\n`brew list mongodb-community\n/usr/local/Cellar/mongodb-community/6.0.4/bin/install_compass\n/usr/local/Cellar/mongodb-community/6.0.4/bin/mongod\n/usr/local/Cellar/mongodb-community/6.0.4/bin/mongos\n/usr/local/Cellar/mongodb-community/6.0.4/homebrew.mxcl.mongodb-community.plist\n/usr/local/Cellar/mongodb-community/6.0.4/MPL-2\n/usr/local/Cellar/mongodb-community/6.0.4/THIRD-PARTY-NOTICES\n`\n```\nWhat causes this 'report'? Does it need \"a PR to fix it\"?",
      "solution": "The issue has now been fixed in the Mongo's homebrew package.\nSee following pull requests:\nhttps://github.com/mongodb/homebrew-brew/pull/176\nand\nhttps://github.com/mongodb/homebrew-brew/pull/174",
      "question_score": 8,
      "answer_score": 1,
      "created_at": "2023-02-10T12:47:59",
      "url": "https://stackoverflow.com/questions/75410705/please-report-this-issue-to-the-mongodb-brew-pr-to-fix-it"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 65623404,
      "title": "Error: querySrv ENOTFOUND _mongodb._tcp.dbname.fzofb.mongodb.net",
      "problem": "I'm learning Node.js and just started working with MongoDB.\nI'm making a connection with the MongoDB Cluster I've created\n```\n`const dbURI = 'mongodb+srv://testuser:test1234@nodelearning.fzofb.mongodb.net/mydb?retryWrites=true&w=majority';\nmongoose.connect(dbURI, { useNewUrlParser: true, useUnifiedTopology: true })\n    .then((result) => console.log('connected to db'))\n    .catch((err) => console.log(err));\n`\n```\nWhen I run it `nodemon app` I get this error:\n\nError: querySrv ENOTFOUND _mongodb._tcp.mydb.fzofb.mongodb.net\nat QueryReqWrap.onresolve [as oncomplete] (node:dns:206:19) {   errno: undefined,   code: 'ENOTFOUND',   syscall: 'querySrv',\nhostname: '_mongodb._tcp.mydb.fzofb.mongodb.net' }",
      "solution": "The error indicates that there is no error in the code. This leaves you with three potential possibilities:\n\nEnsure you have MongoDB installed on your computer.\nMake sure you're connected to wifi that is not public.\nMake sure you have allowed the IP in network access of MongoDB as shown in the image below:\n\nIn my case, I was connected to public wifi in a coworking space. I change my connection to my personal hotspot and it worked.",
      "question_score": 7,
      "answer_score": 6,
      "created_at": "2021-01-08T05:19:25",
      "url": "https://stackoverflow.com/questions/65623404/error-querysrv-enotfound-mongodb-tcp-dbname-fzofb-mongodb-net"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 65826119,
      "title": "how to restart mongodb in terminal?",
      "problem": "I installed mongodb in VS code by `npm install mongodb`. And again I downloaded mongodb from official website and installed it as a windows service since I don't know if installing in vscode with `npm` is enough. It worked for a while.\nand now it doesn't work as my database connection with mongoose fails and nothing is running in 27017 port in localhost. Seems like server stopped.\nhow to restart mongodb server? what's the CLI/terminal command etc for that?",
      "solution": "Go to services via start button and check for the MongoDB services shown in below pic. And rest steps you can follow which @apoorva has mentioned.",
      "question_score": 7,
      "answer_score": 8,
      "created_at": "2021-01-21T12:08:34",
      "url": "https://stackoverflow.com/questions/65826119/how-to-restart-mongodb-in-terminal"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 66346206,
      "title": "Mongo Error QueryExceededMemoryLimitNoDiskUseAllowed",
      "problem": "Getting this error, when I am using this query:\n```\n`db2.collection('candata')\n.aggregate([{$sort:{time:-1}},{$group:{_id:{batteryId:\"$batteryId\"},soctime:{$first:\"$time\"},GPSStatus:{$first:\"$GPSStatus\"},CANStatus:{$first:\"$CANStatus\"},soc: { $first : \"$socpercentage\" },charging_status: { $first : \"$charging_status\" },status: { $first : \"$status\" }}},{$sort:{'_id.batteryId':1}},{$lookup:{from : \"swap_table\",localField : \"_id.batteryId\",foreignField:\"batteryId\",as:\"swap\"}},{$lookup:{from : \"battery_status_table\",localField : \"_id.batteryId\",foreignField:\"batteryId\",as:\"battery_statustb\"}}])\n`\n```\nWhat change do I need to do? I do not want to change the query, so how to increase the limit of this, or I need to go some data archived method?",
      "solution": "Add `allowDiskUse` and set it to true at the end of your query so it becomes like the following:\n```\n` db2.collection('candata')\n.aggregate([{$sort:{time:-1}},{$group:{_id:{batteryId:\"$batteryId\"},soctime:{$first:\"$time\"},GPSStatus:{$first:\"$GPSStatus\"},CANStatus:{$first:\"$CANStatus\"},soc: { $first : \"$socpercentage\" },charging_status: { $first : \"$charging_status\" },status: { $first : \"$status\" }}},{$sort:{'_id.batteryId':1}},{$lookup:{from : \"swap_table\",localField : \"_id.batteryId\",foreignField:\"batteryId\",as:\"swap\"}},{$lookup:{from : \"battery_status_table\",localField : \"_id.batteryId\",foreignField:\"batteryId\",as:\"battery_statustb\"}}]).allowDiskUse(true)\n`\n```",
      "question_score": 7,
      "answer_score": 8,
      "created_at": "2021-02-24T08:10:43",
      "url": "https://stackoverflow.com/questions/66346206/mongo-error-queryexceededmemorylimitnodiskuseallowed"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 71467630,
      "title": "FastAPI issues with MongoDB - TypeError: &#39;ObjectId&#39; object is not iterable",
      "problem": "I am having some issues inserting into MongoDB via FastAPI.\nThe below code works as expected. Notice how the `response` variable has not been used in `response_to_mongo()`.\nThe `model` is an sklearn ElasticNet model.\n`app = FastAPI()\n\ndef response_to_mongo(r: dict):\n    client = pymongo.MongoClient(\"mongodb://mongo:27017\")\n    db = client[\"models\"]\n    model_collection = db[\"example-model\"]\n    model_collection.insert_one(r)\n\n@app.post(\"/predict\")\nasync def predict_model(features: List[float]):\n\n    prediction = model.predict(\n        pd.DataFrame(\n            [features],\n            columns=model.feature_names_in_,\n        )\n    )\n\n    response = {\"predictions\": prediction.tolist()}\n    response_to_mongo(\n        {\"predictions\": prediction.tolist()},\n    )\n    return response\n`\nHowever when I write `predict_model()` like this and pass the `response` variable to `response_to_mongo()`:\n`@app.post(\"/predict\")\nasync def predict_model(features: List[float]):\n\n    prediction = model.predict(\n        pd.DataFrame(\n            [features],\n            columns=model.feature_names_in_,\n        )\n    )\n\n    response = {\"predictions\": prediction.tolist()}\n    response_to_mongo(\n        response,\n    )\n    return response\n`\nI get an error stating that:\n```\n`TypeError: 'ObjectId' object is not iterable\n`\n```\nFrom my reading, it seems that this is due to BSON/JSON issues between FastAPI and Mongo. However, why does it work in the first case when I do not use a variable? Is this due to the asynchronous nature of FastAPI?",
      "solution": "As per the documentation:\n\nWhen a document is inserted a special key, `\"_id\"`, is automatically\nadded if the document doesn\u2019t already contain an `\"_id\"` key. The value\nof `\"_id\"` must be unique across the collection. `insert_one()` returns an\ninstance of InsertOneResult. For more information on \"_id\", see the\ndocumentation on _id.\n\nThus, in the second case of the example you provided, when you pass the dictionary to the `insert_one()` function, Pymongo will add to your dictionary the unique identifier (i.e., `ObjectId`) necessary to retrieve the data from the database; and hence, when returning the response from the endpoint, the `ObjectId` fails getting serialized\u2014since, as described in this answer in detail, FastAPI, by default, will automatically convert that return value into JSON-compatible data using the `jsonable_encoder` (to ensure that objects that are not serializable are converted to a `str`), and then return a `JSONResponse`, which uses the standard `json` library to serialize the data.\nSolution 1\nUse the approach demonstrated here, by having the `ObjectId` converted to `str` by default, and hence, you can return the `response` as usual inside your endpoint.\n`# place these at the top of your .py file\nimport pydantic\nfrom bson import ObjectId\npydantic.json.ENCODERS_BY_TYPE[ObjectId]=str\n\nreturn response # as usual\n`\nSolution 2\nDump the loaded `BSON` to valid `JSON` string and then reload it as `dict`, as described here and here.\n`from bson import json_util\nimport json\n\nresponse = json.loads(json_util.dumps(response))\nreturn response\n`\nSolution 3\nDefine a custom `JSONEncoder`, as described here, to convert the `ObjectId` into `str`:\n`import json\nfrom bson import ObjectId\n\nclass JSONEncoder(json.JSONEncoder):\n    def default(self, o):\n        if isinstance(o, ObjectId):\n            return str(o)\n        return json.JSONEncoder.default(self, o)\n\nresponse = JSONEncoder().encode(response)\nreturn response\n`\nSolution 4\nYou can have a separate output model without the 'ObjectId' (`_id`) field, as described in the documentation. You can declare the model used for the response with the parameter `response_model` in the decorator of your endpoint. Example:\n`from pydantic import BaseModel\n\nclass ResponseBody(BaseModel):\n    name: str\n    age: int\n\n@app.get('/', response_model=ResponseBody)\ndef main():\n    # response sample\n    response = {'_id': ObjectId('53ad61aa06998f07cee687c3'), 'name': 'John', 'age': '25'}\n    return response\n`\nSolution 5\nRemove the `\"_id\"` entry from the `response` dictionary before returning it (see here on how to remove a key from a `dict`):\n`response.pop('_id', None)\nreturn response\n`",
      "question_score": 7,
      "answer_score": 15,
      "created_at": "2022-03-14T13:15:45",
      "url": "https://stackoverflow.com/questions/71467630/fastapi-issues-with-mongodb-typeerror-objectid-object-is-not-iterable"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 74900751,
      "title": "Warning: No available formula with the name &quot;mongodb-community@6.0&quot;",
      "problem": "Trying to install mongodb server on my mac using Brew but getting this error.\n\nUpdated my brew to latest version, still getting this error.\nCommand i used:\n`brew install mongodb-community@6.0`",
      "solution": "I figured it out myself. Here is the solution which I found out. I am using mac os on intel processor.\nI first run\n```\n`rm -fr $(brew --repo homebrew/core)\n`\n```\nthen\n```\n`brew tap homebrew/core\n`\n```\nthen\n```\n`brew tap mongodb/brew\n`\n```\nthen installed it\n```\n`brew install mongodb-community@6.0\n`\n```\nThis worked for me.",
      "question_score": 7,
      "answer_score": 14,
      "created_at": "2022-12-23T15:28:24",
      "url": "https://stackoverflow.com/questions/74900751/warning-no-available-formula-with-the-name-mongodb-community6-0"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 68996929,
      "title": "MongooseError: Query was already executed: Todo.updateOne({ _id: new ObjectId(&quot;612df063a8f",
      "problem": "I updated mongoose to latest version (6.0.2) and now I'm recieving this error and crush the application whenever `.updateOne()` is executed. But object update inside the database. My code:\n`async(req,res) => {\n    await Todo.updateOne(\n        {_id : req.params.id},\n        {\n            $set : {\n                status : \"inactive\"\n            }\n        },\n        (updateError) => {\n            // if updateError exist\n            if (updateError) {\n                // print error\n                printError(updateError);\n\n                // response the error\n                res.status(500).json({\n                    error : \"There was a Server Side Error!\"\n                });\n            } else {\n                // if error dose not exist\n                // print message\n                console.log(\"|===> \ud83d\uddc2\ufe0f  Data Was Updated successfully! \ud83d\uddc2\ufe0f",
      "solution": "Your async/await should look like this:\n\r\n\r\n`async (req,res) => {\n  try {\n      const result = await Todo.updateOne(\n          {_id : req.params.id},\n          {\n              $set : {\n                  status : \"inactive\"\n              }\n          },\n          );\n      }\n      console.log('success', result)\n      res.json({message: \"Todo Was Update successfully!\", result })\n  } catch (err) {\n     console.log('error', err)\n     res.status(500).json({error:'There was a Server Side Error!'})\n  }\n}`",
      "question_score": 7,
      "answer_score": 3,
      "created_at": "2021-08-31T12:18:45",
      "url": "https://stackoverflow.com/questions/68996929/mongooseerror-query-was-already-executed-todo-updateone-id-new-objectid6"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 65948083,
      "title": "mongoDB whitelist IP",
      "problem": "I am seeing similar posts, however none are helping me solve my problem.\nFollowing a Udemy tutorial that builds a MERN application from scratch, I got stuck on the mongoose connection.\nHere is my index.js code:\n```\n`const express = require(\"express\");\nconst mongoose = require(\"mongoose\");\n\nconst app = express();\n\napp.use(express.json());\n\napp.listen(5000, () => console.log(\"Server started on port 5000\"));\n\napp.use(\"/snippet\", require(\"./routers/snippetRouter\"));\n\nmongoose.connect(\"mongodb+srv://snippetUser:_password_@\n  snippet-manager.sometext.mongodb.net/main?retryWrites=\n  true&w=majority\", {\n    useNewUrlParser: true,\n    useUnifiedTopology: true\n}, (err) => {\n  if (err) return console.log(\"error here \" + err);\n  console.log(\"Connected to MongoDB\");\n});\n`\n```\nHere is the error I am getting:\n```\n`Server started on port 5000\nerror here MongooseServerSelectionError: Could not connect to any \nservers in your MongoDB Atlas cluster. One common reason is \nthat you're trying to access the database from an IP that isn't \nwhitelisted. Make sure your current IP address is on your Atlas \ncluster's IP whitelist:\nhttps://docs.atlas.mongodb.com/security-whitelist/ \n`\n```\nAs stated, I am seeing similar errors relating to an IP that isn't whitelisted.\nHowever, in my mongoDB account, it seems that my IP is already whitelisted:\n\nIn the screenshot above, the blank part is where my IP is located (right before it says \"includes your current IP address\").\nSince my IP is listed there, does that not mean my IP is whitelisted?\nIf not, how do I whitelist my IP?",
      "solution": "After a couple of days of frustration, I went into Mongo Atlas, then into Network Access and changed the setting to \"allow access from anywhere\" (shown as `0.0.0.0/0` below). It removed my IP address and changed it to a wildcard IP address.\n\nThis was a deviation from the tutorial I am following on Udemy, but it did work, and I can finally proceed with the rest of the course.",
      "question_score": 7,
      "answer_score": 10,
      "created_at": "2021-01-29T03:45:02",
      "url": "https://stackoverflow.com/questions/65948083/mongodb-whitelist-ip"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 68608923,
      "title": "atlas mongodb serverless and aws lambda connection issue",
      "problem": "I have got a serverless db on atlas (https://www.mongodb.com/serverless). I used the connection string recommended by ATLAS:\n```\n`mongodb+srv://:@xyz.esxbh.mongodb.net/myFirstDatabase?retryWrites=true&w=majority\n`\n```\nhowever as soon as i try to create a record, i get the following error:\n```\n`{\"errorType\":\"Runtime.UnhandledPromiseRejection\",\"errorMessage\":\"MongoParseError: Text record must only set `authSource` or `replicaSet`\",\"reason\":{\"errorType\":\"MongoParseError\",\"errorMessage\":\"Text record must only set `authSource` or `replicaSet`\",\"name\":\"MongoParseError\",\"stack\":[\"MongoParseError: Text record must only set `authSource` or `replicaSet`\",\"\n`\n```\nI don't think that the connection string is correct, on the other hand the dns entry for the server does reply with 2 servers.\nI tried dropping the '+srv' part, however in that case the save function from mongoose just hangs forever timing out the lambda function.\nI could not find any similar problem on google.\nThe TXT entry record from the dns server shows:\n```\n`\"TXT    \"authSource=admin&loadBalanced=true\"\n`\n```\nHow have you configured the serverless database to work?\nThe code that generates the error  depends on mongoose and is as follows:\n```\n`        try {\n          const customer = new Customer(cust);\n          console.log('new cusotmer created');\n          const returnedCustomer = await customer.save();\n          console.log(returnedCustomer);\n          return serverResponse(200, returnedCustomer);\n        } catch(err){\n          console.log(err);\n          return errorHandler(500, err)\n        }\n`\n```\nIt seems that the connection to the database is fine:\n```\n`try {\n    await dbTools.connectMongoose();\n    console.log('*** connected ***');\n\n} catch(err){\n    console.log('error when connecting');\n    return errorHandler(500, err);\n}\n`\n```\nNow, looking at the source code, nothing really too complicated:\n```\n`if (Object.keys(record).some(key => key !== 'authSource' && key !== 'replicaSet')) {\n  return callback(\n    new MongoParseError('Text record must only set `authSource` or `replicaSet`')\n  );\n}\n`\n```\nI am now really struggling to understand what's wrong as authSource seems to be present in the TXT record.",
      "solution": "Upgrading mongoose to the latest version worked for me in Nodejs.\n\nRemove \"mongoose\" from package.json.\nReinstall \"npm i mongoose\"\n\"mongoose version 6.0.5\" should work.\n\nWorked on 10-Sep-2021",
      "question_score": 7,
      "answer_score": 11,
      "created_at": "2021-08-01T11:03:19",
      "url": "https://stackoverflow.com/questions/68608923/atlas-mongodb-serverless-and-aws-lambda-connection-issue"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 67229795,
      "title": "What do the timeline annotation lines actually represent in the mongodb atlas Metrics?",
      "problem": "I'm currently investigating a mongo incident in our Atlas cluster. I am trying to figure out why it decided to upgrade for a period of time. I know that something happened around the memory so I've been looking at the System Memory metric in Atlas. In the charts there are lines added that are described as DISPLAY TIMELINE ANNOTATIONS.\n\nI can see one of these lines around the time of my \"incident\". The problem is nothing seems to explain what these lines represent? Looking on the mongo Atlas docs it simply states:\n\nDirects Ops Manager to display or hide chart annotations. Chart annotations consist of colored vertical lines that indicate server events, such as a server restart or a transition in member state.\n\nSo what exactly is a \"server event\"? How do I find these and what do the various colours mean?",
      "solution": "I raised a support ticket with mongo to get this information. It seems it is shown on the chart but it's a bit hidden because you need to press the `i` in the top left, which only appears when you hover your mouse cursor over the chart:\n\nRed - Server restart\nOrange - server is now primary\nBrown server is now secondary",
      "question_score": 7,
      "answer_score": 12,
      "created_at": "2021-04-23T14:17:20",
      "url": "https://stackoverflow.com/questions/67229795/what-do-the-timeline-annotation-lines-actually-represent-in-the-mongodb-atlas-me"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 79820424,
      "title": ".NET 9 -&gt; .NET 10: MongoDB query using array.Contains throws NotSupportedException",
      "problem": "After upgrading from .NET 9 to .NET 10, a MongoDB query that used to work now throws a `System.NotSupportedException` during query execution. I'm looking for pointers whether this is a runtime change in .NET 10, a MongoDB driver bug, or something I'm doing wrong?\nUsing: .NET 10, MongoDB.Driver 3.5.0\nCode:\n```\n`public static async Task> GetUsers(Guid[] ids)\n{\n    return await Collection.Find(p => ids.Contains(p.UserId)).ToListAsync();\n}\n`\n```\nException:\n\nSystem.NotSupportedException\nHResult=0x80131515\nMessage=Specified method is not supported.\nSource=System.Private.CoreLib\nStackTrace:\nat System.Reflection.RuntimeMethodInfo.ThrowNoInvokeException()\nat System.Reflection.RuntimeMethodInfo.Invoke(Object obj, BindingFlags invokeAttr, Binder binder, Object[] parameters, CultureInfo culture)\nat System.Delegate.DynamicInvokeImpl(Object[] args)\nat MongoDB.Driver.Linq.Linq3Implementation.Misc.PartialEvaluator.SubtreeEvaluator.Evaluate(Expression expression)\nat MongoDB.Driver.Linq.Linq3Implementation.Misc.PartialEvaluator.SubtreeEvaluator.Visit(Expression expression)\nat System.Dynamic.Utils.ExpressionVisitorUtils.VisitArguments(ExpressionVisitor visitor, IArgumentProvider nodes)\nat System.Linq.Expressions.ExpressionVisitor.VisitMethodCall(MethodCallExpression node)\nat MongoDB.Driver.Linq.Linq3Implementation.Misc.PartialEvaluator.SubtreeEvaluator.Visit(Expression expression)\nat System.Linq.Expressions.ExpressionVisitor.VisitLambda[T](Expression`1 node)   at MongoDB.Driver.Linq.Linq3Implementation.Misc.PartialEvaluator.SubtreeEvaluator.Visit(Expression expression)   at MongoDB.Driver.Linq.Linq3Implementation.Misc.PartialEvaluator.EvaluatePartially(Expression expression)   at MongoDB.Driver.Linq.LinqProviderAdapter.TranslateExpressionToFilter[TDocument](Expression`1 expression, IBsonSerializer`1 documentSerializer, IBsonSerializerRegistry serializerRegistry, ExpressionTranslationOptions translationOptions)    at MongoDB.Driver.ExpressionFilterDefinition`1.Render(RenderArgs`1 args)   at MongoDB.Driver.MongoCollectionImpl`1.CreateFindOperation[TProjection](FilterDefinition`1 filter, FindOptions`2 options)\nat MongoDB.Driver.MongoCollectionImpl`1.FindAsync[TProjection](IClientSessionHandle session, FilterDefinition`1 filter, FindOptions`2 options, CancellationToken cancellationToken)   at MongoDB.Driver.MongoCollectionImpl`1.d__55`1.MoveNext()   at MongoDB.Driver.IAsyncCursorSourceExtensions.d__17`1.MoveNext()\n\nWorking:\n```\n`var filter = Builders.Filter.In(p => p.UserId, ids);\n`\n```\n\nIs this a behavioral change in .NET 10 that prevents using a captured array's `Contains` method inside MongoDB LINQ expressions? The same code worked on .NET 9 but now throws `System.NotSupportedException`.\n\nAre LINQ methods that operate on captured collections (for example `Contains`, `Any`, `Count`) no longer safe to use inside expressions passed to `IMongoCollection.Find(...)` with the MongoDB.Driver? If so, which methods are affected?\n\nIs `Builders.Filter.In(...)` the recommended long-term solution for this scenario, or should the LINQ provider accept `array.Contains(x)` again? Are there performance or correctness differences I should be aware of when switching to `Filter.In`?",
      "solution": "C# 14 introduces first-class support for `System.Span` and `System.ReadOnlySpan`:\n\nC# 14 recognizes the relationship and supports some conversions between `ReadOnlySpan`, `Span`, and `T[]`. The span types can be extension method receivers, compose with other conversions, and help with generic type inference scenarios.\n\nWhich results in `Contains` in `ids.Contains(p.UserId)` treated as `MemoryExtensions.Contains(this ReadOnlySpan span, T value)` which seems not to be supported by the driver for now.\nAs a workaround you can try using `Enumerable.Contains` explicitly:\n`await Collection.Find(p => Enumerable.Contains(ids, p.UserId))\n    .ToListAsync()\n`\nOr change `GetUsers` signature to accept `IEnumerable` (or `IReadOnlyCollection`, etc.):\n`public static async Task> GetUsers(IEnumerable ids)\n{\n    return await Collection.Find(p => ids.Contains(p.UserId)).ToListAsync();\n}\n`\nOr change the language version used by the application:\n`\n    13\n\n`\nUPD\nTo address at least some questions by Ivan Petrov\nBut why are arrays getting the extension methods of spans?\nThis is how the feature was designed - from the feature spec:\n\nAn implicit span conversion permits `array_types`, `System.Span`, `System.ReadOnlySpan`, and `string` to be converted between each other as follows\n...\nWe also add implicit span conversion to the list of acceptable implicit conversions on the first parameter of an extension method when determining applicability\n\nAnd get them first before `IEnumerable` methods?\nThis will require a bit more investigation on my side, but as far as I understand one of them should be first and it should be the span one otherwise feature would quite less useful (or completely useless).\nAlso found this issue @github, which basically covers the question and is resolved \"by design\".\nNotes\n\nRider provides diagnostics for this:\n\nThe resolution for this invocation has changed in C# 14 due to a breaking change in overload resolution with spans. This can cause runtime exceptions when compiled with interpretation.\n\nAt the moment of writing Entity Framework Core in versions earlier than 10 has the same problem - see for example First class span's break EF Core issue @github\n\nOther ORMs working on top of expressions will have the same problem if not updated to handle new methods:\n\nCosmosDb: CosmosDb ExpressionVisitor (SubtreeEvaluator) fails on Azure, works on local machine",
      "question_score": 7,
      "answer_score": 10,
      "created_at": "2025-11-14T21:28:42",
      "url": "https://stackoverflow.com/questions/79820424/net-9-net-10-mongodb-query-using-array-contains-throws-notsupportedexcepti"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 72579620,
      "title": "Fetching data from server in Remix.run",
      "problem": "I was exploring Remix.run and making a sample app. I came across an issue that has been bothering me for some time now. Correct me if I am wrong: `action()` is for handling Form submission, and `loader()` is for fetching data initially.\nFor my application, I used mongoose to connect MongoDB, defined models, and defined querying function in a `query.server.ts` file. I want to fetch data from the database through a function defined in the `query.server.ts` file when an image is clicked on the UI. How can I do that without using forms? I cannot pre-fetch the data without knowing what image was clicked by the user.",
      "solution": "You can create a resource route. These are like regular routes, but don't export a default component (no UI).\nYou can use the `useFetcher` hook and call `fetcher.load()` to call your resource route. The data is in `fetcher.data`.\n`// routes/query-data.ts\nexport const loader: LoaderFunction = async ({request}) => {\n  const url = new URL(request.url)\n  const img = url.searchParams.get('img')\n  const data = await getData(img)\n  return json(data)\n}\n\n// routes/route.tsx\nexport default function Route() {\n  const fetcher = useFetcher()\n\n  const handleImgClick = (e) => {\n    const img = e.target\n    fetcher.load(`/query-data?img=${img.attr('src')}`)\n  }\n\n  return (\n    \n      \n      ```\n{ JSON.stringify(fetcher.data, null, 2) }\n```\n    \n  )\n}\n`",
      "question_score": 7,
      "answer_score": 10,
      "created_at": "2022-06-10T22:40:33",
      "url": "https://stackoverflow.com/questions/72579620/fetching-data-from-server-in-remix-run"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 67998926,
      "title": "Getting U_STRINGPREP_PROHIBITED_ERROR when trying to deploy MongoDB on Kubernetes",
      "problem": "Trying to set up MongoDB in a Kubernetes cluster on my local machine using Minikube but getting the following error. (I tried multiple MongoDB images; latest, 5.0.0, 4.0, etc. The problem continued.)\nMy `mongodb-secret.yaml` is as follows:\n```\n`apiVersion: v1\nkind: Secret\nmetadata:\n  name: mongodb-secret\ntype: Opaque\ndata:\n  mongo-root-username: dXNlcm5hbWUNCg==\n  mongo-root-password: cGFzc3dvcmQNCg==\n`\n```\nMy `mongodb-deployment.yaml` file is as follows:\n```\n`apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mongodb-deployment\n  labels:\n    apps: mongodb\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: mongodb\n  template:\n    metadata:\n      labels:\n        app: mongodb\n    spec:\n      containers:\n      - name: mongodb\n        image: mongo:latest\n        ports:\n        - containerPort: 27017\n        env:\n        - name: MONGO_INITDB_ROOT_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: mongodb-secret\n              key: mongo-root-username \n        - name: MONGO_INITDB_ROOT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mongodb-secret\n              key: mongo-root-password\n`\n```\nThe output when I run `kubectl get all`:\n```\n`NAME                                      READY   STATUS             RESTARTS   AGE\npod/mongodb-deployment-6c587ddcbb-lnk2q   0/1     CrashLoopBackOff   6          7m56s\n\nNAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE\nservice/kubernetes   ClusterIP   10.96.0.1            443/TCP   6d\n\nNAME                                 READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/mongodb-deployment   0/1     1            0           14m\n\nNAME                                            DESIRED   CURRENT   READY   AGE\nreplicaset.apps/mongodb-deployment-59ff8f9fd7   0         0         0       9m1s\nreplicaset.apps/mongodb-deployment-6c587ddcbb   1         1         0       7m56s\nreplicaset.apps/mongodb-deployment-7fc5cbcf9c   0         0         0       10m\nreplicaset.apps/mongodb-deployment-8f6675bc5    0         0         0       14m\n`\n```\nThe output when I run `kubectl logs mongodb-deployment-6c587ddcbb-lnk2q`:\n```\n`about to fork child process, waiting until server is ready for connections.\nforked process: 28\n\n{\"t\":{\"$date\":\"2021-06-16T08:15:33.738+00:00\"},\"s\":\"I\",  \"c\":\"CONTROL\",  \"id\":20698,   \"ctx\":\"main\",\"msg\":\"***** SERVER RESTARTED *****\"}\n{\"t\":{\"$date\":\"2021-06-16T08:15:33.740+00:00\"},\"s\":\"I\",  \"c\":\"CONTROL\",  \"id\":23285,   \"ctx\":\"main\",\"msg\":\"Automatically disabling TLS 1.0, to force-enable TLS 1.0 specify --sslDisabledProtocols 'none'\"}\n{\"t\":{\"$date\":\"2021-06-16T08:15:33.741+00:00\"},\"s\":\"W\",  \"c\":\"ASIO\",     \"id\":22601,   \"ctx\":\"main\",\"msg\":\"No TransportLayer configured during NetworkInterface startup\"}\n{\"t\":{\"$date\":\"2021-06-16T08:15:33.741+00:00\"},\"s\":\"I\",  \"c\":\"NETWORK\",  \"id\":4648601, \"ctx\":\"main\",\"msg\":\"Implicit TCP FastOpen unavailable. If TCP FastOpen is required, set tcpFastOpenServer, tcpFastOpenClient, and tcpFastOpenQueueSize.\"}\n{\"t\":{\"$date\":\"2021-06-16T08:15:33.741+00:00\"},\"s\":\"I\",  \"c\":\"STORAGE\",  \"id\":4615611, \"ctx\":\"initandlisten\",\"msg\":\"MongoDB starting\",\"attr\":{\"pid\":28,\"port\":27017,\"dbPath\":\"/data/db\",\"architecture\":\"64-bit\",\"host\":\"mongodb-deployment-6c587ddcbb-lnk2q\"}}\n{\"t\":{\"$date\":\"2021-06-16T08:15:33.741+00:00\"},\"s\":\"I\",  \"c\":\"CONTROL\",  \"id\":23403,   \"ctx\":\"initandlisten\",\"msg\":\"Build Info\",\"attr\":{\"buildInfo\":{\"version\":\"4.4.6\",\"gitVersion\":\"72e66213c2c3eab37d9358d5e78ad7f5c1d0d0d7\",\"openSSLVersion\":\"OpenSSL 1.1.1  11 Sep 2018\",\"modules\":[],\"allocator\":\"tcmalloc\",\"environment\":{\"distmod\":\"ubuntu1804\",\"distarch\":\"x86_64\",\"target_arch\":\"x86_64\"}}}}\n{\"t\":{\"$date\":\"2021-06-16T08:15:33.741+00:00\"},\"s\":\"I\",  \"c\":\"CONTROL\",  \"id\":51765,   \"ctx\":\"initandlisten\",\"msg\":\"Operating System\",\"attr\":{\"os\":{\"name\":\"Ubuntu\",\"version\":\"18.04\"}}}\n{\"t\":{\"$date\":\"2021-06-16T08:15:33.741+00:00\"},\"s\":\"I\",  \"c\":\"CONTROL\",  \"id\":21951,   \"ctx\":\"initandlisten\",\"msg\":\"Options set by command line\",\"attr\":{\"options\":{\"net\":{\"bindIp\":\"127.0.0.1\",\"port\":27017,\"tls\":{\"mode\":\"disabled\"}},\"processManagement\":{\"fork\":true,\"pidFilePath\":\"/tmp/docker-entrypoint-temp-mongod.pid\"},\"systemLog\":{\"destination\":\"file\",\"logAppend\":true,\"path\":\"/proc/1/fd/1\"}}}}\n{\"t\":{\"$date\":\"2021-06-16T08:15:33.741+00:00\"},\"s\":\"I\",  \"c\":\"STORAGE\",  \"id\":22297,   \"ctx\":\"initandlisten\",\"msg\":\"Using the XFS filesystem is strongly recommended with the WiredTiger storage engine. See http://dochub.mongodb.org/core/prodnotes-filesystem\",\"tags\":[\"startupWarnings\"]}\n{\"t\":{\"$date\":\"2021-06-16T08:15:33.742+00:00\"},\"s\":\"I\",  \"c\":\"STORAGE\",  \"id\":22315,   \"ctx\":\"initandlisten\",\"msg\":\"Opening WiredTiger\",\"attr\":{\"config\":\"create,cache_size=12285M,session_max=33000,eviction=(threads_min=4,threads_max=4),config_base=false,statistics=(fast),log=(enabled=true,archive=true,path=journal,compressor=snappy),file_manager=(close_idle_time=100000,close_scan_interval=10,close_handle_minimum=250),statistics_log=(wait=0),verbose=[recovery_progress,checkpoint_progress,compact_progress],\"}}\n{\"t\":{\"$date\":\"2021-06-16T08:15:33.855+00:00\"},\"s\":\"I\",  \"c\":\"STORAGE\",  \"id\":22430,   \"ctx\":\"initandlisten\",\"msg\":\"WiredTiger message\",\"attr\":{\"message\":\"[1623831333:855500][28:0x7f583f1b6ac0], txn-recover: [WT_VERB_RECOVERY | WT_VERB_RECOVERY_PROGRESS] Set global recovery timestamp: (0, 0)\"}}\n{\"t\":{\"$date\":\"2021-06-16T08:15:33.855+00:00\"},\"s\":\"I\",  \"c\":\"STORAGE\",  \"id\":22430,   \"ctx\":\"initandlisten\",\"msg\":\"WiredTiger message\",\"attr\":{\"message\":\"[1623831333:855566][28:0x7f583f1b6ac0], txn-recover: [WT_VERB_RECOVERY | WT_VERB_RECOVERY_PROGRESS] Set global oldest timestamp: (0, 0)\"}}\n{\"t\":{\"$date\":\"2021-06-16T08:15:33.865+00:00\"},\"s\":\"I\",  \"c\":\"STORAGE\",  \"id\":4795906, \"ctx\":\"initandlisten\",\"msg\":\"WiredTiger opened\",\"attr\":{\"durationMillis\":123}}\n{\"t\":{\"$date\":\"2021-06-16T08:15:33.865+00:00\"},\"s\":\"I\",  \"c\":\"RECOVERY\", \"id\":23987,   \"ctx\":\"initandlisten\",\"msg\":\"WiredTiger recoveryTimestamp\",\"attr\":{\"recoveryTimestamp\":{\"$timestamp\":{\"t\":0,\"i\":0}}}}\n{\"t\":{\"$date\":\"2021-06-16T08:15:33.886+00:00\"},\"s\":\"I\",  \"c\":\"STORAGE\",  \"id\":4366408, \"ctx\":\"initandlisten\",\"msg\":\"No table logging settings modifications are required for existing WiredTiger tables\",\"attr\":{\"loggingEnabled\":true}}\n{\"t\":{\"$date\":\"2021-06-16T08:15:33.887+00:00\"},\"s\":\"I\",  \"c\":\"STORAGE\",  \"id\":22262,   \"ctx\":\"initandlisten\",\"msg\":\"Timestamp monitor starting\"}\n{\"t\":{\"$date\":\"2021-06-16T08:15:33.894+00:00\"},\"s\":\"W\",  \"c\":\"CONTROL\",  \"id\":22120,   \"ctx\":\"initandlisten\",\"msg\":\"Access control is not enabled for the database. Read and write access to data and configuration is unrestricted\",\"tags\":[\"startupWarnings\"]}\n{\"t\":{\"$date\":\"2021-06-16T08:15:33.894+00:00\"},\"s\":\"W\",  \"c\":\"CONTROL\",  \"id\":22178,   \"ctx\":\"initandlisten\",\"msg\":\"/sys/kernel/mm/transparent_hugepage/enabled is 'always'. We suggest setting it to 'never'\",\"tags\":[\"startupWarnings\"]}\n{\"t\":{\"$date\":\"2021-06-16T08:15:33.894+00:00\"},\"s\":\"I\",  \"c\":\"STORAGE\",  \"id\":20320,   \"ctx\":\"initandlisten\",\"msg\":\"createCollection\",\"attr\":{\"namespace\":\"admin.system.version\",\"uuidDisposition\":\"provided\",\"uuid\":{\"uuid\":{\"$uuid\":\"5118c0eb-1d88-4b4a-837c-ec78ce97b710\"}},\"options\":{\"uuid\":{\"$uuid\":\"5118c0eb-1d88-4b4a-837c-ec78ce97b710\"}}}}\n{\"t\":{\"$date\":\"2021-06-16T08:15:33.908+00:00\"},\"s\":\"I\",  \"c\":\"INDEX\",    \"id\":20345,   \"ctx\":\"initandlisten\",\"msg\":\"Index build: done building\",\"attr\":{\"buildUUID\":null,\"namespace\":\"admin.system.version\",\"index\":\"_id_\",\"commitTimestamp\":{\"$timestamp\":{\"t\":0,\"i\":0}}}}\n{\"t\":{\"$date\":\"2021-06-16T08:15:33.908+00:00\"},\"s\":\"I\",  \"c\":\"COMMAND\",  \"id\":20459,   \"ctx\":\"initandlisten\",\"msg\":\"Setting featureCompatibilityVersion\",\"attr\":{\"newVersion\":\"4.4\"}}\n{\"t\":{\"$date\":\"2021-06-16T08:15:33.908+00:00\"},\"s\":\"I\",  \"c\":\"STORAGE\",  \"id\":20536,   \"ctx\":\"initandlisten\",\"msg\":\"Flow Control is enabled on this deployment\"}\n{\"t\":{\"$date\":\"2021-06-16T08:15:33.909+00:00\"},\"s\":\"I\",  \"c\":\"STORAGE\",  \"id\":20320,   \"ctx\":\"initandlisten\",\"msg\":\"createCollection\",\"attr\":{\"namespace\":\"local.startup_log\",\"uuidDisposition\":\"generated\",\"uuid\":{\"uuid\":{\"$uuid\":\"5e09a18c-0532-4fad-8981-283eb3f2f429\"}},\"options\":{\"capped\":true,\"size\":10485760}}}\n{\"t\":{\"$date\":\"2021-06-16T08:15:33.924+00:00\"},\"s\":\"I\",  \"c\":\"INDEX\",    \"id\":20345,   \"ctx\":\"initandlisten\",\"msg\":\"Index build: done building\",\"attr\":{\"buildUUID\":null,\"namespace\":\"local.startup_log\",\"index\":\"_id_\",\"commitTimestamp\":{\"$timestamp\":{\"t\":0,\"i\":0}}}}\n{\"t\":{\"$date\":\"2021-06-16T08:15:33.924+00:00\"},\"s\":\"I\",  \"c\":\"FTDC\",     \"id\":20625,   \"ctx\":\"initandlisten\",\"msg\":\"Initializing full-time diagnostic data capture\",\"attr\":{\"dataDirectory\":\"/data/db/diagnostic.data\"}}\n{\"t\":{\"$date\":\"2021-06-16T08:15:33.925+00:00\"},\"s\":\"I\",  \"c\":\"CONTROL\",  \"id\":20712,   \"ctx\":\"LogicalSessionCacheReap\",\"msg\":\"Sessions collection is not set up; waiting until next sessions reap interval\",\"attr\":{\"error\":\"NamespaceNotFound: config.system.sessions does not exist\"}}\n{\"t\":{\"$date\":\"2021-06-16T08:15:33.925+00:00\"},\"s\":\"I\",  \"c\":\"STORAGE\",  \"id\":20320,   \"ctx\":\"LogicalSessionCacheRefresh\",\"msg\":\"createCollection\",\"attr\":{\"namespace\":\"config.system.sessions\",\"uuidDisposition\":\"generated\",\"uuid\":{\"uuid\":{\"$uuid\":\"8149a51f-47fd-4180-a6a4-310574dfae9b\"}},\"options\":{}}}\n{\"t\":{\"$date\":\"2021-06-16T08:15:33.925+00:00\"},\"s\":\"I\",  \"c\":\"NETWORK\",  \"id\":23015,   \"ctx\":\"listener\",\"msg\":\"Listening on\",\"attr\":{\"address\":\"/tmp/mongodb-27017.sock\"}}\n{\"t\":{\"$date\":\"2021-06-16T08:15:33.926+00:00\"},\"s\":\"I\",  \"c\":\"NETWORK\",  \"id\":23015,   \"ctx\":\"listener\",\"msg\":\"Listening on\",\"attr\":{\"address\":\"127.0.0.1\"}}\n{\"t\":{\"$date\":\"2021-06-16T08:15:33.926+00:00\"},\"s\":\"I\",  \"c\":\"NETWORK\",  \"id\":23016,   \"ctx\":\"listener\",\"msg\":\"Waiting for connections\",\"attr\":{\"port\":27017,\"ssl\":\"off\"}}\nchild process started successfully, parent exiting\n{\"t\":{\"$date\":\"2021-06-16T08:15:33.948+00:00\"},\"s\":\"I\",  \"c\":\"INDEX\",    \"id\":20345,   \"ctx\":\"LogicalSessionCacheRefresh\",\"msg\":\"Index build: done building\",\"attr\":{\"buildUUID\":null,\"namespace\":\"config.system.sessions\",\"index\":\"_id_\",\"commitTimestamp\":{\"$timestamp\":{\"t\":0,\"i\":0}}}}\n{\"t\":{\"$date\":\"2021-06-16T08:15:33.948+00:00\"},\"s\":\"I\",  \"c\":\"INDEX\",    \"id\":20345,   \"ctx\":\"LogicalSessionCacheRefresh\",\"msg\":\"Index build: done building\",\"attr\":{\"buildUUID\":null,\"namespace\":\"config.system.sessions\",\"index\":\"lsidTTLIndex\",\"commitTimestamp\":{\"$timestamp\":{\"t\":0,\"i\":0}}}}\n{\"t\":{\"$date\":\"2021-06-16T08:15:33.961+00:00\"},\"s\":\"I\",  \"c\":\"NETWORK\",  \"id\":22943,   \"ctx\":\"listener\",\"msg\":\"Connection accepted\",\"attr\":{\"remote\":\"127.0.0.1:55144\",\"connectionId\":1,\"connectionCount\":1}}\n{\"t\":{\"$date\":\"2021-06-16T08:15:33.961+00:00\"},\"s\":\"I\",  \"c\":\"NETWORK\",  \"id\":51800,   \"ctx\":\"conn1\",\"msg\":\"client metadata\",\"attr\":{\"remote\":\"127.0.0.1:55144\",\"client\":\"conn1\",\"doc\":{\"application\":{\"name\":\"MongoDB Shell\"},\"driver\":{\"name\":\"MongoDB Internal Client\",\"version\":\"4.4.6\"},\"os\":{\"type\":\"Linux\",\"name\":\"Ubuntu\",\"architecture\":\"x86_64\",\"version\":\"18.04\"}}}}\n{\"t\":{\"$date\":\"2021-06-16T08:15:33.964+00:00\"},\"s\":\"I\",  \"c\":\"NETWORK\",  \"id\":22944,   \"ctx\":\"conn1\",\"msg\":\"Connection ended\",\"attr\":{\"remote\":\"127.0.0.1:55144\",\"connectionId\":1,\"connectionCount\":0}}\n{\"t\":{\"$date\":\"2021-06-16T08:15:34.003+00:00\"},\"s\":\"I\",  \"c\":\"NETWORK\",  \"id\":22943,   \"ctx\":\"listener\",\"msg\":\"Connection accepted\",\"attr\":{\"remote\":\"127.0.0.1:55146\",\"connectionId\":2,\"connectionCount\":1}}\n{\"t\":{\"$date\":\"2021-06-16T08:15:34.004+00:00\"},\"s\":\"I\",  \"c\":\"NETWORK\",  \"id\":51800,   \"ctx\":\"conn2\",\"msg\":\"client metadata\",\"attr\":{\"remote\":\"127.0.0.1:55146\",\"client\":\"conn2\",\"doc\":{\"application\":{\"name\":\"MongoDB Shell\"},\"driver\":{\"name\":\"MongoDB Internal Client\",\"version\":\"4.4.6\"},\"os\":{\"type\":\"Linux\",\"name\":\"Ubuntu\",\"architecture\":\"x86_64\",\"version\":\"18.04\"}}}}\nuncaught exception: Error: couldn't add user: Error preflighting normalization: U_STRINGPREP_PROHIBITED_ERROR :\n_getErrorWithCode@src/mongo/shell/utils.js:25:13\nDB.prototype.createUser@src/mongo/shell/db.js:1386:11\n@(shell):1:1\nError saving history file: FileOpenFailed Unable to open() file /home/mongodb/.dbshell: No such file or directory\n{\"t\":{\"$date\":\"2021-06-16T08:15:34.016+00:00\"},\"s\":\"I\",  \"c\":\"NETWORK\",  \"id\":22944,   \"ctx\":\"conn2\",\"msg\":\"Connection ended\",\"attr\":{\"remote\":\"127.0.0.1:55146\",\"connectionId\":2,\"connectionCount\":0}}\n`\n```\nWhat am I missing?",
      "solution": "Checking your secret values, something's wrong:\n```\n`$ echo  dXNlcm5hbWUNCg==| base64 --decode | cat -e\nusername^M$\n$ echo  cGFzc3dvcmQNCg== | base64 --decode | cat -e\npassword^M$\n`\n```\nTry re-creating your secret without those un-printable characters (`\\r` as well as `\\n`).\nMaybe, instead of pre-encoding your secret, you could try:\n```\n`apiVersion: v1\nkind: Secret\nmetadata:\n  name: mongodb-secret\ntype: Opaque\nstringData:\n  mongo-root-username: username\n  mongo-root-password: password\n`\n```",
      "question_score": 7,
      "answer_score": 10,
      "created_at": "2021-06-16T10:21:38",
      "url": "https://stackoverflow.com/questions/67998926/getting-u-stringprep-prohibited-error-when-trying-to-deploy-mongodb-on-kubernete"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 71880907,
      "title": "MongoDB transaction with @NestJs/mongoose not working",
      "problem": "I really need your help. My MongoDB transaction with @NestJs/mongoose not working...When My stripe payment fails rollback is not working... Still, my order collection saved the data...How can I fix this issue..?\n```\n`  async create(orderData: CreateOrderServiceDto): Promise {\n    const session = await this.connection.startSession();\n    session.startTransaction();\n    try {\n      const createOrder = new this.orderModel(orderData);\n      const order = await createOrder.save();\n\n      await this.stripeService.charge(\n        orderData.amount,\n        orderData.paymentMethodId,\n        orderData.stripeCustomerId,\n      );\n      await session.commitTransaction();\n      return order;\n    } catch (error) {\n      await session.abortTransaction();\n      throw error;\n    } finally {\n      await session.endSession();\n    }\n  }\n`\n```",
      "solution": "I had the same issue and i found that on github: Mongo DB Transactions With Mongoose & Nestjs\nSo I think, according this issue, you have to call the `create` method of your model, like that:\n```\n`const order = await this.orderModel.create(orderData, { session });\n`\n```\nas you can see, the `Model.create` method has an overload with `SaveOptions` as parameter:\n```\n`create(docs: (AnyKeys | AnyObject)[], options?: SaveOptions): Promise[]>;\n`\n```\nit takes an optional `SaveOptions` parameter that can contain the session:\n```\n`interface SaveOptions {\n  checkKeys?: boolean;\n  j?: boolean;\n  safe?: boolean | WriteConcern;\n  session?: ClientSession | null;\n  timestamps?: boolean;\n  validateBeforeSave?: boolean;\n  validateModifiedOnly?: boolean;\n  w?: number | string;\n  wtimeout?: number;\n}\n`\n```\nPlease note that `Model.save()` can also take a `SaveOptions` parameter.\nSo you can also do as you did like that:\n```\n`const createOrder = new this.orderModel(orderData);\nconst order = await createOrder.save({ session });\n`\n```\nA little further...\nAs i do many things that require a transaction, I came up with this helper to avoid many code duplication:\n```\n`import { InternalServerErrorException } from \"@nestjs/common\"\nimport { Connection, ClientSession } from \"mongoose\"\n\nexport const mongooseTransactionHandler = async (\n  method: (session: ClientSession) => Promise,\n  onError: (error: any) => any,\n  connection: Connection, session?: ClientSession\n): Promise => {\n  const isSessionFurnished = session === undefined ? false : true\n  if (isSessionFurnished === false) {\n    session = await connection.startSession()\n    session.startTransaction()\n  }\n\n  let error\n  let result: T\n  try {\n    result = await method(session)\n\n    if (isSessionFurnished === false) {\n      await session.commitTransaction()\n    }\n  } catch (err) {\n    error = err\n    if (isSessionFurnished === false) {\n      await session.abortTransaction()\n    }\n  } finally {\n    if (isSessionFurnished === false) {\n      await session.endSession()\n    }\n\n    if (error) {\n      onError(error)\n    }\n\n    return result\n  }\n}\n`\n```\nDetails\nthe optional parameter `session` is in case you are doing nested nested transaction.\nthat's why i check if the session is provided. If it is, it means we are in a nested transaction. So we'll let the main transaction commit, abort and end the session.\nExample\nfor example: you delete a `User` model, and then the user's avatar which is a `File` model.\n```\n`/** UserService **/\nasync deleteById(id: string): Promise {\n  const transactionHandlerMethod = async (session: ClientSession): Promise => {\n    const user = await this.userModel.findOneAndDelete(id, { session })\n    await this.fileService.deleteById(user.avatar._id.toString(), session)\n  }\n\n  const onError = (error: any) => {\n    throw error\n  }\n\n  await mongooseTransactionHandler(\n    transactionHandlerMethod,\n    onError,\n    this.connection\n  )\n}\n\n/** FileService **/\nasync deleteById(id: string, session?: ClientSession): Promise {\n  const transactionHandlerMethod = async (session: ClientSession): Promise => {\n    await this.fileModel.findOneAndRemove(id, { session })\n  }\n\n  const onError = (error: any) => {\n    throw error\n  }\n\n  await mongooseTransactionHandler(\n    transactionHandlerMethod,\n    onError,\n    this.connection,\n    session\n  )\n}\n`\n```\nSo, in short:\nYou can use it like this:\n```\n`async create(orderData: CreateOrderServiceDto): Promise {\n  const transactionHandlerMethod = async (session: ClientSession): Promise => {\n    const createOrder = new this.orderModel(orderData);\n    const order = await createOrder.save({ session });\n\n    await this.stripeService.charge(\n      orderData.amount,\n      orderData.paymentMethodId,\n      orderData.stripeCustomerId,\n    );\n\n    return order\n  }\n\n  const onError = (error: any): void => {\n    throw error\n  }\n\n  const order = await mongooseTransactionHandler(\n    transactionHandlerMethod,\n    onError,\n    this.connection\n  )\n\n  return order\n}\n`\n```\nHope it'll help.\nEDIT\nDo not abuse of the `model.save({ session })` of the same model in nested transcations.\nFor some reasons it will throw an error the model is updated too many times.\nTo avoid that, prefer using model embeded methods that update and return a new instance of your model (`model.findOneAndUpdate` for example).",
      "question_score": 7,
      "answer_score": 9,
      "created_at": "2022-04-15T08:46:01",
      "url": "https://stackoverflow.com/questions/71880907/mongodb-transaction-with-nestjs-mongoose-not-working"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 72626159,
      "title": "Why does my NextJS Code run multiple times, and how can this be avoided?",
      "problem": "I've been trying to connect my frontend and backend for a webapp and I have an issue where, when a user creates an account, multiple objects for that user get created in the database. I used `console.log` and found that my NextJS code is repeating. I found this slightly related post: Component functions runs multiple times react\nHowever, it deals with problems with async calls that I already solved with null/undefined checks. This is my current user register code:\n`const handleUser = async (user) => {\n  var bUserExists = false\n\n  await axios.get(GET_USER(user.email))\n    .then(response => {\n      bUserExists = true;\n    })\n    .catch(err => {\n      bUserExists = false\n    })\n\n  if(!bUserExists) {\n    bUserExists = true;\n    await axios.post(CREATE_USER(), {\n      email: user.email,\n      name: user.name,\n    }).then(newUser => {\n      console.log(newUser.data)\n      if(newUser.data) {\n        router.push('/form/p1')\n      }\n    }).catch(err => {\n      console.log(err)\n    })\n  }\n}\n\nconst { user, error, isLoading } = useUser();\n  \nif(isLoading) return Loading...;\nif(error) return {error.message};\nif(user) {\n  // try to get the user from the database\n  // if the user doesn't exist, create one\n  handleUser(user)\n}\n`\nWhen I create a user with this code, my backend outputs this:\n(Sorry, I don't have enough rep to post images yet ;-; )\nhttps://i.gyazo.com/fd83cf9362b0e88977dfb277687ca071.png\nWeird, right? It repeats so quickly that the user isn't even created by the time the second call comes in.\nI've come to the conclusion that NextJS is repeating the code on both the client and server, but my knowledge of Next/React is too little for that to be very accurate. Why is it that my code (that should only run once) is running multiple times?",
      "solution": "-> In react due to Strict mode enable it render every page Two times(only in development environment).\n-> so we can remove Strict mode enable by just removing\nthis Strict mode wrapper from index.js in react.\n\r\n\r\n`\n    \n      \n      \n     \n `\r\n\r\n\r\n\n-> and next.js is based on react so you have to find the way like how to disable strict mode in next.js here is official doc link[disable strict mode next.js][1]\n[1]: https://nextjs.org/docs/api-reference/next.config.js/react-strict-mode",
      "question_score": 7,
      "answer_score": 2,
      "created_at": "2022-06-15T07:08:31",
      "url": "https://stackoverflow.com/questions/72626159/why-does-my-nextjs-code-run-multiple-times-and-how-can-this-be-avoided"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 72282755,
      "title": "Prisma Mongodb can&#39;t create a user model",
      "problem": "So I am using Prisma for the first time and my provider is `mongodb` and when I want to create a model it throws me an error\n```\n`Invalid `prisma.user.create()` invocation:\n\n  Prisma needs to perform transactions, which requires your MongoDB server to be run \nas a replica set. https://pris.ly/d/mongodb-replica-set\n`\n```\nI am using Prisma in a nextjs app and I put the code inside the API pages\nMy DATABASE_URL is  `mongodb://localhost:27017/threadzees`\nCode :\n`await prisma.user.create({\n      data: {\n        username,\n        email,\n        avatar: \"1\",\n        createdAt: new Date(),\n      },\n    });\n`\nHow do I fix this issue?",
      "solution": "I'm Running Mongodb 4+ version I solved it as below\nAs the error describes you need to create a replica. Either you can use cloud based Mongo or locally you can create a replica like below.\n```\n`# Open new terminal execute below command\n mongod --port=27001 --dbpath=. --replSet=rs0\n# Open another terminal window execute below command\nmongo.exe\n# Then below command\nrs.initiate( {    _id : \"rs0\", members: [ { _id: 0, host: \"localhost:27001\" } ] })\n# your new connection String\nmongodb://localhost:27001\n`\n```\n\n--dbpath : Is your data path directory (above command points to current directory)\n--replSet=rs0 : Replica name\nRead more @ https://www.mongodb.com/docs/manual/tutorial/deploy-replica-set/\n\nHappy coding :)",
      "question_score": 7,
      "answer_score": 6,
      "created_at": "2022-05-18T05:02:34",
      "url": "https://stackoverflow.com/questions/72282755/prisma-mongodb-cant-create-a-user-model"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 68849597,
      "title": "Spring Data MongoDB: MergeOperation returns the whole collection. Why?",
      "problem": "I'm running a MongoDB aggregation where the last stage is a `$merge` that writes some field to another collection. Basically, this $merge should make an effect only on a single document (this is why we have `on: \"_id\"`.\nHere's how I do it with Java:\n```\n`var merge = Aggregation.merge().intoCollection(\"items\")\n                .on(\"_id\")\n                .whenMatched(MergeOperation.WhenDocumentsMatch.mergeDocuments())\n                .whenNotMatched(MergeOperation.WhenDocumentsDontMatch.discardDocument())\n                .build();\n\nvar agg = Aggregation.newAggregation(match, group, merge);\nvar aggResult = mongoTemplate.aggregate(agg, \"prices\", Item.class);\n  \n`\n```\nThe aggregation does what it needs and I can see that the requested document has changed, but the problem is that `aggregate()` returns all the documents in the collection.\nThis is a major drawback and can't scale well when the collection is large enough.\nHow can I change my query so it will return only the changed document. Or if not possible, just apply the query without returning anything.",
      "solution": "Found the answer and will share for future readers:\n```\n`mongoTemplate.aggregate(aggregation.withOptions(newAggregationOptions().skipOutput().allowDiskUse(true).build()), \"collectionNme\", EntityClass.class);\n`\n```",
      "question_score": 7,
      "answer_score": 8,
      "created_at": "2021-08-19T16:17:19",
      "url": "https://stackoverflow.com/questions/68849597/spring-data-mongodb-mergeoperation-returns-the-whole-collection-why"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 66565463,
      "title": "Disable Realm Sync only for non-premium users",
      "problem": "I'm creating an iOS application, where I intend to provide data sync across device feature, only to the premium users. I find Realm Sync as a good solution to keep the local on-device database and cloud MongoDB Atlas in sync. However, I don't want to sync the data of the non-premium users to the cloud database.\nI'm enlisting a couple of ways that I can think of to prevent Realm Sync from triggering for non-premium users, but I'm not sure on what is the best way for this problem.\n\nPrevent syncing by leveraging Sync permissions - I can store list of premium user ids and only give sync permissions to those users.\n```\n` {\n   \"%%user.id\": [\n      \"5f4863e4d49bd2191ff1e623\",\n      \"5f48640dd49bd2191ff1e624\",\n      \"5f486417d49bd2191ff1e625\"\n   ]\n } \n`\n```\n\nConfigure Realm objects on client side i.e. only allow all Realm objects / models if the user is premium.\n```\n` // Get a configuration to open the synced realm.\n var configuration = user.configuration(partitionValue: \"user=\\(user.id)\")\n // For non-premium user it would be [User.self]\n configuration.objectTypes = [User.self, Project.self] \n Realm.asyncOpen(configuration: configuration) { [weak self](result) in /*...*/ }\n`\n```\n\nI'm looking for insights / recommended approach to this problem.\nEdit\nI've a few additional questions about handling two use cases differently - non-premium one by opening a local only `Realm()` and the premium one with `Realm.asyncOpen()`.\n\nHow to handle a use case when an existing user switches to a premium subscription? Should calling `Realm.asyncOpen()` suffice or do I need to do any special handling?\nI plan to sync all my `User` (custom document in a collection) records for all users (premium + non-premium). My guess is I should open a normal Realm for all my conent and synced Realm with just `[User.self]` object in the configuration.",
      "solution": "This is super easy to do!\nWhen you only want to work with a local realm, connect to it with no config - like this\n```\n`let realm = try! Realm()\nlet someObject = realm.results(SomeObject.self)\n`\n```\nor a config that maybe contains a local file name. All of the app data will only be read and written locally with no sync'ing.\nWhen you want to use MongoDB Realm Sync, connect to it like this\n```\n`let app = App(id: YOUR_REALM_APP_ID)\n// Log in...\nlet user = app.currentUser\nlet partitionValue = \"some partition value\"\nvar configuration = user!.configuration(partitionValue: partitionValue)\nRealm.asyncOpen(configuration: configuration) { result in\n    switch result {\n    case .failure(let error):\n        print(\"Failed to open realm: \\(error.localizedDescription)\")\n        // handle error\n    case .success(let realm):\n        print(\"Successfully opened realm: \\(realm)\")\n        // Use realm\n    }\n}\n`\n```\nand then later with a config\n```\n`let config = user?.configuration(partitionValue: \"some partition\")\nlet realm = try! Realm(configuration: config)\n`\n```\nEDIT\nAnswering the two followup question:\n\nHow to handle a use case when an existing user switches to a premium\nsubscription? Should calling Realm.asyncOpen() suffice or do I need to\ndo any special handling?\n\nConnecting to MongoDB Realm with the Sync'ding solution will add additional files and start syncing. If this is a new user that's 'premium', theres nothing else to do, other than (initially) ensure your objects are correctly structured with _id and partitionKey properties.\nIf this user is upgrading from a non-premium local only to a premium that's sync'd you will need to copy your realm objects from the local only realm to a sync'd realm.\nThere are several ways to to that; probably the simplist is to include code in your app then when upgrading, connects to a sync realm (using .async), then connects to your existing local realm and finally iterate over the objects to copy to the sync'd realm.\nAnother option is to export the the realm objects as JSON and then write them to the server directly. The next time your app connects with .async, it will force a client reset and download and create the locally sync'd files. There are some tidbits of information that may help with this particular process in the Realm Legacy Migration Guide\n\nI plan to sync all my User (custom document in a collection) records\nfor all users (premium + non-premium). My guess is I should open a\nnormal Realm for all my conent and synced Realm with just [User.self]\nobject in the configuration.\n\nNon-premium users don't sync so they are not really 'users' as such. You wouldn't need to store them or sync them so you really don't need any authentication or store any data on the server - it's just a locally run and used app so there isn't even a 'user' object to worry about. You will need to do that once they upgrade.",
      "question_score": 7,
      "answer_score": 8,
      "created_at": "2021-03-10T14:10:35",
      "url": "https://stackoverflow.com/questions/66565463/disable-realm-sync-only-for-non-premium-users"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 69534451,
      "title": "MongoDB Node.JS insertOne error: &quot;Type &#39;string&#39; is not assignable to type &#39;ObjectId | undefined&quot;",
      "problem": "I try to insert one document into the collection, but I get this error when I specify the `_id` field of the inserted document. So how can I insert a document with `_id` of a type other than `ObjectId`?\nThe following code gives me that error. When I remove the `_id`, everything goes fine but the _id will be generated automatically. I just want it to be a string type. I know this is possible. I can make `_id` string using MongoDB shell.\n`async function foo() {\n  const users = client.db(\"test\").collection(\"users\")\n  users.insertOne({\n    _id: \"a string\",\n    name: \"Tom\",\n    age: 26,\n  })\n}\n`",
      "solution": "If you type your collection with `_id: string` it should work\n`type usertype = {\n  _id: string,\n  name: string,\n  age: number\n}\n\nasync function foo() {\n  await client.connect()\n  const users = client.db(\"test\").collection(\"users\")\n  const result = await users.insertOne({\n    _id: \"custom string\",\n    name: \"Tom\",\n    age: 26,\n  })\n  console.log(result)\n}\n`\nthat should give you the result you're looking for",
      "question_score": 7,
      "answer_score": 7,
      "created_at": "2021-10-12T04:35:21",
      "url": "https://stackoverflow.com/questions/69534451/mongodb-node-js-insertone-error-type-string-is-not-assignable-to-type-objec"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 72085297,
      "title": "&quot;Unsupported OP_QUERY command: insert&quot; in Elixir project when trying to insert into Mongo DB",
      "problem": "I have a Mix project and I am trying to insert some data into my local database.\nI have added the Mongo dependency to my project, then added the lines of code for connecting to the database and for storing data into it.\n`{:ok, pid} =\n  Mongo.start_link(\n    url: \"mongodb://localhost:27017/tweet_processor\")\n    \n{:ok, result} =\n  Mongo.insert_one(pid, \"tweets\", tweet_to_insert)\n`\nBut I keep getting\n`{:error, %Mongo.Error{\n  code: 352,\n  host: nil,\n  message: \"command failed: Unsupported OP_QUERY command: insert\"}}\n`\nWhat could be the problem?",
      "solution": "I wild guess (out of the shape of commands you\u2019ve shared) that you are using `mongodb` package. It explicitly states it supports MongoDB versions 2.6\u00f74.0. I also wild guess you are using MongoDB 5+ backend, which has `OP_QUERY` explicitly deprecated.\nThe driver you use is OSS, and from its source code, one might see that `Mongo.insert_one/4` delegates to low-level call which issues `OP_QUERY`.\nOne possibility to fix the issue would be to downgrade MongoDB to v4.0, another (most appreciated by a community) would be to provide a PR to the library, supporting MongoDB 5+.",
      "question_score": 7,
      "answer_score": 5,
      "created_at": "2022-05-02T12:16:59",
      "url": "https://stackoverflow.com/questions/72085297/unsupported-op-query-command-insert-in-elixir-project-when-trying-to-insert-i"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 69237599,
      "title": "NestJS + Mongoose schema with a custom typescript Type",
      "problem": "I'm Trying to create a Mongo Schema, using `nestjs/mongoose` decorators, from the following class:\n`@Schema()\nexport class Constraint {\n  @Prop()\n  reason: string;\n\n  @Prop()\n  status: Status;\n\n  @Prop()\n  time: number;\n}\n\n`\nThe problem is `Status` is defined as followed:\n`export type Status = boolean | 'pending';\n\n`\nAnd I cannot figure out what to pass to the `status`'s `prop` decorator, since I'm getting the following error:\n```\n`Error: Cannot determine a type for the \"Constraint.status\" field (union/intersection/ambiguous type was used). Make sure your property is decorated with a \"@Prop({ type: TYPE_HERE })\" decorator\n`\n```\nand `{ type: Status }` doesn't work, since `Status` is a `type` and not a `Class`.",
      "solution": "Since `status` can be a boolean or a string it's a mixed type. So you can look into setting your type to Mixed",
      "question_score": 7,
      "answer_score": 1,
      "created_at": "2021-09-18T20:54:46",
      "url": "https://stackoverflow.com/questions/69237599/nestjs-mongoose-schema-with-a-custom-typescript-type"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 69424322,
      "title": "@aws-sdk/lib-storage to Stream JSON from MongoDB to S3 with JSONStream.stringify()",
      "problem": "I'm trying to Stream JSON from MongoDB to S3 with the new version of @aws-sdk/lib-storage:\n```\n`\"@aws-sdk/client-s3\": \"^3.17.0\"\n\"@aws-sdk/lib-storage\": \"^3.34.0\"\n\"JSONStream\": \"^1.3.5\",\n`\n```\nTry #1: It seems that I'm not using JSONStream.stringify() correctly:\n```\n`import { MongoClient } from 'mongodb';\nimport { S3Client } from '@aws-sdk/client-s3';\nimport { Upload } from '@aws-sdk/lib-storage';\nconst s3Client = new S3Client({ region: env.AWS_REGION });\n\nexport const uploadMongoStreamToS3 = async (connectionString, collectionName) => {\n  let client;\n\n  try {\n    client = await MongoClient.connect(connectionString);\n    const db = client.db();\n    const readStream = db.collection(collectionName).find('{}').limit(5).stream();\n    readStream.pipe(JSONStream.stringify());\n \n    const upload = new Upload({\n      client: s3Client,\n      params: {\n        Bucket: 'test-bucket',\n        Key: 'extracted-data/benda_mongo.json',\n        Body: readStream,\n      },\n    });\n    \n    await upload.done(); \n  }\n  catch (err) {\n    log.error(err);\n    throw err.name;\n  }\n  finally {\n    if (client) {\n      client.close();\n    }\n  }\n\n};\n`\n```\nError #1:\n\nTypeError [ERR_INVALID_ARG_TYPE]: The first argument must be one of\ntype string, Buffer, ArrayBuffer, Array, or Array-like Object.\nReceived type object\nat Function.from (buffer.js:305:9)\nat getDataReadable (/.../node_modules/@aws-sdk/lib-storage/src/chunks/getDataReadable.ts:6:18)\nat processTicksAndRejections (internal/process/task_queues.js:94:5)\nat Object.getChunkStream (/.../node_modules/@aws-sdk/lib-storage/src/chunks/getChunkStream.ts:17:20)\nat Upload.__doConcurrentUpload (/.../node_modules/@aws-sdk/lib-storage/src/Upload.ts:121:22)\nat async Promise.all (index 0)\nat Upload.__doMultipartUpload (/.../node_modules/@aws-sdk/lib-storage/src/Upload.ts:196:5)\nat Upload.done (/.../node_modules/@aws-sdk/lib-storage/src/Upload.ts:88:12)\n\nTry #2, using the variable `jsonStream`:\n```\n`  const readStream = db.collection(collectionName).find('{}').limit(5).stream();\n    const jsonStream = readStream.pipe(JSONStream.stringify());\n \n    const upload = new Upload({\n      client: s3Client,\n      params: {\n        Bucket: 'test-bucket',\n        Key: 'extracted-data/benda_mongo.json',\n        Body: jsonStream,\n      },\n    });\n`\n```\nError #2:\n\nReferenceError: ReadableStream is not defined\nat Object.getChunk (/.../node_modules/@aws-sdk/lib-storage/src/chunker.ts:22:30)\nat Upload.__doMultipartUpload (/.../node_modules/@aws-sdk/lib-storage/src/Upload.ts:187:24)\nat Upload.done (/.../node_modules/@aws-sdk/lib-storage/src/Upload.ts:88:37)\n\nTry #3: use `stream.PassThrough`:\n```\n`    client = await MongoClient.connect(connectionString);\n    const db = client.db();\n    const readStream = db.collection(collectionName).find('{}').limit(5).stream();\n    readStream.pipe(JSONStream.stringify()).pipe(uploadStreamFile('benda_mongo.json'));\n\n...\n\nconst stream = require('stream');\nexport const uploadStreamFile = async(fileName) => {\n  try{\n\n    const pass = new stream.PassThrough();\n    const upload = new Upload({\n      client: s3Client,\n      params: {\n        Bucket: 'test-bucket',\n        Key: 'extracted-data/benda_mongo.json',\n        Body: pass,\n      },\n    });\n    const res = await upload.done();\n    \n    log.info('finished uploading file', fileName);\n    return res;\n  }\n  catch(err){\n    return;\n  }\n};\n`\n```\nError #3:\n\n'dest.on is not a function at Stream.pipe (internal/streams/legacy.js:30:8'\n\nTry #4: mongodb.stream({transform: doc => JSON.stringify...}) instead of JSONStream:\n```\n`import { S3Client } from '@aws-sdk/client-s3';\nimport { Upload } from '@aws-sdk/lib-storage';\nimport { env } from '../../../env';\nconst s3Client = new S3Client({ region: env.AWS_REGION });\n\nexport const uploadMongoStreamToS3 = async (connectionString, collectionName) => {\n  let client;\n\n  try {\n    client = await MongoClient.connect(connectionString);\n    const db = client.db();\n    const readStream = db.collection(collectionName)\n      .find('{}')\n      .limit(5)\n      .stream({ transform: doc => JSON.stringify(doc) + '\\n' });\n  \n    const upload = new Upload({\n      client: s3Client,\n      params: {\n        Bucket: 'test-bucket',\n        Key: 'extracted-data/benda_mongo.json',\n        Body: readStream,\n      },\n    });\n  \n    await upload.done(); \n  }\n  catch (err) {\n    log.error('waaaaa', err);\n    throw err.name;\n  }\n  finally {\n    if (client) {\n      client.close();\n    }\n  }\n};\n`\n```\nError: #4:\n\nTypeError [ERR_INVALID_ARG_TYPE]: The first argument must be one of\ntype string, Buffer, ArrayBuffer, Array, or Array-like Object.\nReceived type object\nat Function.from (buffer.js:305:9)\nat getDataReadable (/.../node_modules/@aws-sdk/lib-storage/src/chunks/getDataReadable.ts:6:18)\nat processTicksAndRejections (internal/process/task_queues.js:94:5)\nat Object.getChunkStream (/.../node_modules/@aws-sdk/lib-storage/src/chunks/getChunkStream.ts:17:20)\nat Upload.__doConcurrentUpload (/.../node_modules/@aws-sdk/lib-storage/src/Upload.ts:121:22)\nat async Promise.all (index 0)\nat Upload.__doMultipartUpload (/.../node_modules/@aws-sdk/lib-storage/src/Upload.ts:196:5)\nat Upload.done (/.../node_modules/@aws-sdk/lib-storage/src/Upload.ts:88:12)\n\nTry #5: using `stream.PassThrough()` and return `pass` to `pipe`:\n```\n`export const uploadMongoStreamToS3 = async (connectionString, collectionName) => {\n  let client;\n\n  try {\n    client = await MongoClient.connect(connectionString);\n    const db = client.db();\n    const readStream = db.collection(collectionName).find('{}').limit(5).stream({ transform: doc => JSON.stringify(doc) + '\\n' });\n    readStream.pipe(uploadStreamFile());\n  }\n  catch (err) {\n    log.error('waaaaa', err);\n    throw err.name;\n  }\n  finally {\n    if (client) {\n      client.close();\n    }\n  }\n};\n\nconst stream = require('stream');\n\nexport const uploadStreamFile = async() => {\n  try{\n    const pass = new stream.PassThrough();\n    const upload = new Upload({\n      client: s3Client,\n      params: {\n        Bucket: 'test-bucket',\n        Key: 'extracted-data/benda_mongo.json',\n        Body: pass,\n      },\n    });\n    await upload.done();\n    return pass;\n  }\n  catch(err){\n    log.error('pawoooooo', err);\n    return;\n  }\n};\n`\n```\nError #5:\n\nTypeError: dest.on is not a function\nat Cursor.pipe (_stream_readable.js:680:8)",
      "solution": "After reviewing your error stack traces, probably the problem has to do with the fact that the MongoDB driver provides a cursor in object mode whereas the `Body` parameter of `Upload` requires a traditional stream, suitable for be processed by `Buffer` in this case.\nTaking your original code as reference, you can try providing a `Transform` stream for dealing with both requirements.\nPlease, consider for instance the following code:\n```\n`import { Transform } from 'stream';\nimport { MongoClient } from 'mongodb';\nimport { S3Client } from '@aws-sdk/client-s3';\nimport { Upload } from '@aws-sdk/lib-storage';\nconst s3Client = new S3Client({ region: env.AWS_REGION });\n\nexport const uploadMongoStreamToS3 = async (connectionString, collectionName) => {\n  let client;\n\n  try {\n    client = await MongoClient.connect(connectionString);\n    const db = client.db();\n    const readStream = db.collection(collectionName).find('{}').limit(5).stream();\n    // We are creating here a Transform to adapt both sides\n    const toJSONTransform = new Transform({\n      writableObjectMode: true,\n      transform(chunk, encoding, callback) {\n        this.push(JSON.stringify(chunk) + '\\n');\n        callback();  \n      }  \n    });\n\n    readStream.pipe(toJSONTransform);\n \n    const upload = new Upload({\n      client: s3Client,\n      params: {\n        Bucket: 'test-bucket',\n        Key: 'extracted-data/benda_mongo.json',\n        Body: toJSONTransform,\n      },\n    });\n    \n    await upload.done(); \n  }\n  catch (err) {\n    log.error(err);\n    throw err.name;\n  }\n  finally {\n    if (client) {\n      client.close();\n    }\n  }\n\n};\n`\n```\nIn the code, in `toJSONTransform` we are defining the writable part of the stream as object mode; in contrast, the readable part will be suitable for being read from the S3 `Upload` method... at least, I hope so.\nRegarding the second error you reported, the one related with `dest.on`, I initially thought, and I wrote you about the possibility, that the error was motivated because in `uploadStreamFile` you are returning a `Promise`, not a stream, and you are passing that `Promise` to the `pipe` method, which requires a stream, basically that you returned the wrong variable. But I didn't realize that you are trying passing the `PassThrough` stream as a param to the `Upload` method: please, be aware that this stream doesn't contain any information because you are not passing any information to it, the contents of the readable stream obtained from the MongoDB query are never passed to the callback nor the `Upload` itself.",
      "question_score": 7,
      "answer_score": 2,
      "created_at": "2021-10-03T13:15:28",
      "url": "https://stackoverflow.com/questions/69424322/aws-sdk-lib-storage-to-stream-json-from-mongodb-to-s3-with-jsonstream-stringify"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 71188697,
      "title": "Handling Database reconnections with MongoDB Native Driver in Node.JS",
      "problem": "With mongoose one can simply handle reconnections via connection-options:\n```\n`let dbOptions = {\n    dbName: process.env.MONGO_DATABASE_NAME,\n    autoReconnect: true,\n    reconnectInterval: 1000,\n    reconnectTries: 10,\n};\n\nmongoose.connect(process.env.MONGO_URI, dbOptions);\n// Create connection object.\nconst db = mongoose.connection;\n`\n```\nIn the native MongoDB driver (version 4.4) there are no similar connection options available:\nhttps://mongodb.github.io/node-mongodb-native/4.4/interfaces/MongoClientOptions.html\nWhat is the easiest way to handle database reconnections in case of a major error?",
      "solution": "Wherever you've got the mongoose connection snippet from, it's outdated.\nThe `autoReconnect` and `auto_reconnect` options were a thing in Nodejs native driver before v4.0, and mongoose just proxied these options to the driver.\nThis is the documentation for the driver 3.7 with \"autoReconnect\" still there: http://mongodb.github.io/node-mongodb-native/3.7/api/global.html#MongoClientOptions  and this is the commit where it's been removed: https://github.com/mongodb/node-mongodb-native/commit/e3cd9e684aea99be0430d856d6299e65258bb4c3#diff-f005e84d9066ef889099ec2bd907abf7900f76da67603e4130e1c92fac92533dL90\nThe option was \"True\" by default with a strong note to do not change this value unless you know exactly why you need to disable it.\nv4 introduced many changes to the driver - refactoring to typescript, architectural changes, you can see it from the commit, right. One of the changes affected connection logic and pool management. There is no option to disable reconnection anymore. It's always reconnects regardless how you connect directly or via mongoose.",
      "question_score": 7,
      "answer_score": 2,
      "created_at": "2022-02-19T20:59:25",
      "url": "https://stackoverflow.com/questions/71188697/handling-database-reconnections-with-mongodb-native-driver-in-node-js"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 73450818,
      "title": "MongoDB C# unable to connect to database: Unable to create an authenticator",
      "problem": "While setting up my new environment with a freshly deployed MongoDB container with authentication enabled, I ran into this exception: `\"An unhandled exception has occurred while executing the request. MongoDB.Driver.MongoConnectionException: An exception occurred while opening a connection to the server. ---> System.NotSupportedException: Unable to create an authenticator.\"`\nIn my case I'm using a connection string like this example: `mongodb://USER:PASSWORD@HOST:27017/?authMechanism=DEFAULT`. This string works perfectly fine in MongoDB Compass but not inside my .NET 6.0 application.",
      "solution": "If you take a look at the source code of C#\u00a0MongoDB driver in MongoCredential.cs#L469, you see this exception gets thrown while checking the auth mechanism.\nAfter specifying the exact auth mechanism in the connection string, all exceptions are gone!\nexample: `mongodb://USER:PASSWORD@HOST:27017/?authMechanism=SCRAM-SHA-256`\nHope anyone else googeling around will find my answer helpful!\nhappy coding.",
      "question_score": 6,
      "answer_score": 21,
      "created_at": "2022-08-22T22:42:13",
      "url": "https://stackoverflow.com/questions/73450818/mongodb-c-unable-to-connect-to-database-unable-to-create-an-authenticator"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 68047878,
      "title": "&quot;TypeError: GridFsStorage is not a constructor&quot;",
      "problem": "for some reason I keep getting a \"TypeError: GridFsStorage is not a constructor\" error. I have no idea why its giving me this error since I'm just following the official documentation.\nStorage and upload\n```\n`conn.once('open', ()=> {\n    gfs = Grid(conn.db, mongoose.mongo)\n    gfs.collection('uploads')\n})\n\nconst storage = new GridFsStorage({\n    url: DBURI,\n    file: (req, file)=> {\n        return new Promise((resolve,reject)=> {\n            crypto.randomBytes(16, (err, buf)=> {\n                if(err) {\n                    return reject(err)\n                }\n                const filename = buf.toString('hex') + path.extname(file.originalname);\n                const fileInfor = {\n                    filename: filename,\n                    bucketName: 'uploads'\n                }\n                resolve(fileInfo)\n            })\n        })\n    }\n})\nconst upload = multer({storage})\n`\n```\nRequirement\n```\n`const GridFsStorage = require('multer-gridfs-storage')\n`\n```\nBug\n```\n`C:\\Users\\gabri\\Desktop\\GridFS\\server.js:21\nconst storage = new GridFsStorage({\n                ^\n\nTypeError: GridFsStorage is not a constructor\n    at Object. (C:\\Users\\gabri\\Desktop\\GridFS\\server.js:21:17)      \n    at Module._compile (node:internal/modules/cjs/loader:1092:14)\n    at Object.Module._extensions..js (node:internal/modules/cjs/loader:1121:10)\n    at Module.load (node:internal/modules/cjs/loader:972:32)\n    at Function.Module._load (node:internal/modules/cjs/loader:813:14)\n    at Function.executeUserEntryPoint [as runMain] (node:internal/modules/run_main:76:12)\n    at node:internal/main/run_main_module:17:47\n`\n```",
      "solution": "The doc on NPM shows this:\n```\n`const {GridFsStorage} = require('multer-gridfs-storage');\n`\n```\nwhich is the same as:\n```\n`const GridFsStorage = require('multer-gridfs-storage').GridFsStorage;\n`\n```\nWhereas you are using this which is not the same at all:\n```\n`const GridFsStorage = require('multer-gridfs-storage');\n`\n```",
      "question_score": 6,
      "answer_score": 23,
      "created_at": "2021-06-19T16:46:27",
      "url": "https://stackoverflow.com/questions/68047878/typeerror-gridfsstorage-is-not-a-constructor"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 75913878,
      "title": "Problem with User.remove, it says that it is not a function",
      "problem": "I'm trying to post users on /user with express.Router()\nBut it says that await User.remove is not a function.\nMaybe i need to import some more features?\nHere is code\n```\n`import express from \"express\";\nimport User from \"./Models/UserModel.js\";\nimport users from \"./data/users.js\";\nimport Product from \"./Models/ProductModel.js\"\n\nconst ImportData = express.Router()\n\nImportData.post(\n    \"/user\",\n    async (req, res) => {\n      await User.remove({});\n      const importUser = await User.insertMany(users);\n      res.send({ importUser });\n    }\n  );\n\nImportData.post(\"/products\",async (req,res)=>{\n    await Product.remove({});\n    const importProducts = await Product.insertMany(products);\n    res.send({ importProducts });\n});\n\nexport default ImportData;\n`\n```\nHere is error:\n```\n`await User.remove({});\n                 ^\n\nTypeError: User.remove is not a function\n    at file:///C:/react//frontend/Server/DataImport.js:12:18 \n`\n```\nUserModel.js:\n```\n`import mongoose from \"mongoose\";\nimport bcrypt from \"bcryptjs\";\n\nconst userSchema = mongoose.Schema(\n  {\n    name: {\n      type: String,\n      required: true,\n    },\n    email: {\n      type: String,\n      required: true,\n      unique: true,\n    },\n    password: {\n      type: String,\n      required: true,\n    },\n    isAdmin: {\n      type: Boolean,\n      required: true,\n      default: false,\n    },\n  },\n  {\n    timestamps: true,\n  }\n);\n\nconst User = mongoose.model(\"User\", userSchema);\n\nexport default User;\n\n`\n```\nI tried a lot, maybe here is another solving of this problem?\nThank u in advance",
      "solution": "The object you are calling `remove()` on is a Model. Looking at its documentation, there indeed is no `remove` function; the only function whose name hints at similar behavior I can find is the `deleteOne()` and `deleteMany()` function found in the documentation.\nThe `remove` function is part of a `Schema`, as seen here in the documentation. In your code, a `Schema` is only used in the `UserModel.js` file, to create the Model.",
      "question_score": 6,
      "answer_score": 7,
      "created_at": "2023-04-02T20:25:53",
      "url": "https://stackoverflow.com/questions/75913878/problem-with-user-remove-it-says-that-it-is-not-a-function"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 76293776,
      "title": "Environment variable MONGODB_CONFIG_OVERRIDE_NOFORK == 1, overriding \\&quot;processManagement.fork\\&quot; to false&quot;",
      "problem": "I tried convert standalone MongoDB to replica set with only one replica, it was required for change streams on my backend.\nI found error, which don't descripted in any site in google, so here is error message: 'Environment variable MONGODB_CONFIG_OVERRIDE_NOFORK == 1, overriding \"processManagement.fork\" to false\"'.\nAnybody who knows how solve it?\nI tried search environment variable in linux -- not found, i tried set variable with descripted name to environment in linux -- not works, and searched possible enviroment variables",
      "solution": "Delete `MONGODB_CONFIG_OVERRIDE_NOFORK=1` in `/usr/lib/systemd/system/mongod.service`\nAfter saving file run `sudo systemctl daemon-reload`\nSource: https://www.alibabacloud.com/blog/how-to-set-environment-variables-in-a-systemd-service_598533",
      "question_score": 6,
      "answer_score": 5,
      "created_at": "2023-05-20T08:12:47",
      "url": "https://stackoverflow.com/questions/76293776/environment-variable-mongodb-config-override-nofork-1-overriding-processma"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 69195824,
      "title": "trying to connect mongoDB to my web app but it shows following error",
      "problem": "Error:\n\nC:\\Users\\Bpc\\Desktop\\devcamper_api\\node_modules\\mongodb\\lib\\connection_string.js:281\nthrow new error_1.MongoParseError(`${optionWord} ${Array.from(unsupportedOptions).join(', ')} ${isOrAre} not supported`);\n^\n\nMongoParseError: options usecreateindex, usefindandmodify are not supported\nat Object.parseOptions (C:\\Users\\Bpc\\Desktop\\devcamper_api\\node_modules\\mongodb\\lib\\connection_string.js:281:15)\nat new MongoClient (C:\\Users\\Bpc\\Desktop\\devcamper_api\\node_modules\\mongodb\\lib\\mongo_client.js:62:46)\nat C:\\Users\\Bpc\\Desktop\\devcamper_api\\node_modules\\mongoose\\lib\\connection.js:781:16\nat new Promise ()\nat NativeConnection.Connection.openUri (C:\\Users\\Bpc\\Desktop\\devcamper_api\\node_modules\\mongoose\\lib\\connection.js:778:19)\nat C:\\Users\\Bpc\\Desktop\\devcamper_api\\node_modules\\mongoose\\lib\\index.js:330:10\nat C:\\Users\\Bpc\\Desktop\\devcamper_api\\node_modules\\mongoose\\lib\\helpers\\promiseOrCallback.js:32:5\nat new Promise ()\nat promiseOrCallback (C:\\Users\\Bpc\\Desktop\\devcamper_api\\node_modules\\mongoose\\lib\\helpers\\promiseOrCallback.js:31:10)\nat Mongoose._promiseOrCallback (C:\\Users\\Bpc\\Desktop\\devcamper_api\\node_modules\\mongoose\\lib\\index.js:1151:10)\n\nand this is my code:\n```\n`const mongoose = require('mongoose');\n\nconst connectDB = async () => {\n    const conn = await mongoose.connect(process.env.MONGO_URI,\n        {\n            useNewUrlParser: true,\n            useCreateIndex: true,\n            useFindAndModify: false,\n            useUnifiedTopology: true\n        });\n    console.log(`MongoDB Connected: ${conn.connection.host}`);\n};\n\nmodule.exports = connectDB;\n`\n```",
      "solution": "Seems to be the same as:\nMongoParseError: options useCreateIndex, useFindAndModify are not supported\n\nFrom the Mongoose 6.0 docs:\n\nuseNewUrlParser, useUnifiedTopology, useFindAndModify, and useCreateIndex >>are no longer supported options. Mongoose 6 always behaves as if >>useNewUrlParser, useUnifiedTopology, and useCreateIndex are true, and >>useFindAndModify is false. Please remove these options from your code.\n\nSource: https://stackoverflow.com/a/68962378/7860331",
      "question_score": 6,
      "answer_score": 13,
      "created_at": "2021-09-15T17:20:48",
      "url": "https://stackoverflow.com/questions/69195824/trying-to-connect-mongodb-to-my-web-app-but-it-shows-following-error"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 66388523,
      "title": "Error: Cannot init client | mongo-connect express-session",
      "problem": "I am getting error while trying to save session on mongodb. Here is my code..\n```\n`const express = require(\"express\");\nconst session = require(\"express-session\");\nconst MongoStore = require(\"connect-mongo\").default;\nconst app = express();\n\nlet sessionOptions = session({\n  secret: \"JavaScript is cool\",\n  store: MongoStore.create({ client: require(\"./db\") }),\n  resave: false,\n  saveUninitialized: false,\n  cookie: { maxAge: 1000 * 60 * 60 * 24, httpOnly: true },\n});\n\napp.use(sessionOptions);\nconst router = require(\"./router\");\napp.use(express.urlencoded({ extended: false }));\napp.use(express.json());\napp.use(express.static(\"public\"));\napp.set(\"views\", \"views\");\napp.set(\"view engine\", \"ejs\");\n\napp.use(\"/\", router);\n\nmodule.exports = app;\n`\n```\nand db.js\n```\n`const dotenv = require(\"dotenv\");\ndotenv.config();\nconst mongodb = require(\"mongodb\");\n\nmongodb.connect(\n  process.env.CONNECTIONSTRING,\n  { useNewUrlParser: true, useUnifiedTopology: true },\n  (err, client) => {\n    module.exports = client;\n    const app = require(\"./app\");\n    app.listen(process.env.PORT);\n  }\n);\n`\n```\nAnd the error is here..\n```\n`Assertion failed: You must provide either mongoUr|clientPromise in options\n/home/irfan/Desktop/Brad_Sciff/Complex_App/node_modules/connect-mongo/build/main/lib/MongoStore.js:121\n            throw new Error('Cannot init client');\n            ^\n\n    Error: Cannot init client\n        at new MongoStore (/home/irfan/Desktop/Brad_Sciff/Complex_App/node_modules/connect-mongo/build/main/lib/MongoStore.js:121:19)\n        at Function.create (/home/irfan/Desktop/Brad_Sciff/Complex_App/node_modules/connect-mongo/build/main/lib/MongoStore.js:137:16)\n        at Object. (/home/irfan/Desktop/Brad_Sciff/Complex_App/app.js:9:21)\n        at Module._compile (internal/modules/cjs/loader.js:778:30)\n        at Object.Module._extensions..js (internal/modules/cjs/loader.js:789:10)\n        at Module.load (internal/modules/cjs/loader.js:653:32)\n        at tryModuleLoad (internal/modules/cjs/loader.js:593:12)\n        at Function.Module._load (internal/modules/cjs/loader.js:585:3)\n        at Module.require (internal/modules/cjs/loader.js:692:17)\n        at require (internal/modules/cjs/helpers.js:25:18)\n`\n```\nI tried to change from const MongoStore = require(\"connect-mongo\").default to const MongoStore = require(\"connect-mongo\")(session)\nBut the error is showing..\n```\n`const MongoStore = require(\"connect-mongo\")(session);\n                                           ^\n\nTypeError: require(...) is not a function\n    at Object. (/home/irfan/Desktop/Brad_Sciff/Complex_App/app.js:4:44)\n    at Module._compile (internal/modules/cjs/loader.js:778:30)\n    at Object.Module._extensions..js (internal/modules/cjs/loader.js:789:10)\n    at Module.load (internal/modules/cjs/loader.js:653:32)\n    at tryModuleLoad (internal/modules/cjs/loader.js:593:12)\n    at Function.Module._load (internal/modules/cjs/loader.js:585:3)\n    at Module.require (internal/modules/cjs/loader.js:692:17)\n    at require (internal/modules/cjs/helpers.js:25:18)\n    at mongodb.connect (/home/irfan/Desktop/Brad_Sciff/Complex_App/db.js:10:17)\n    at /home/irfan/Desktop/Brad_Sciff/Complex_App/node_modules/mongodb/lib/utils.js:693:5\n`\n```\nUsing Connect-mongo 4.2, express-session 1.17.1 express 4.17.1 mongodb 3.6.4\nDon't know what I am missing.\nPlease help.\nThanks in Advance.\nIrfan.",
      "solution": "So it looks like `connect-mongo` has been updated recently. I came across this issue today as well and here's how I fixed it.\nHow it used to be:\n```\n`const session = require('express-session');\nconst MongoStore = require('connect-mongo')(session);\n\napp.use(\n  session({\n    ...options\n    store: new MongoStore({ mongooseConnection: mongoose.connection  }),\n  })\n);\n`\n```\nHow it is now:\n```\n`const session = require('express-session');\nconst MongoStore = require('connect-mongo').default;\n\napp.use(\n  session({\n    store: MongoStore.create({ mongoUrl: process.env.MONGO_URI }),\n    ...options\n  })\n);\n`\n```\nTry passing your connection string into `mongoURL` instead of `client` and see if that helps.\nYou can read more about connect-mongo in their docs.",
      "question_score": 6,
      "answer_score": 11,
      "created_at": "2021-02-26T16:27:47",
      "url": "https://stackoverflow.com/questions/66388523/error-cannot-init-client-mongo-connect-express-session"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 66317184,
      "title": "GitHub actions cannot connect to MongoDB service",
      "problem": "I'm having trouble running my automated tests with GitHub actions. I can't figure out why I can't connect with the MongoDB service running my integration tests. I tried different hosts: localhost, 127.0.0.1, 0.0.0.0, but none of them can connect with the database.\nIt works perfectly fine in my docker setup, but for some reason not with GitHub actions.\n`    name: CI master\n    \n    on: [push, pull_request]\n    \n    env:\n      RUST_BACKTRACE: 1\n      CARGO_TERM_COLOR: always\n      APP_ENV: development\n      APP_MONGO_USER: test\n      APP_MONGO_PASS: password\n      APP_MONGO_DB: test\n    \n    jobs:\n      # Run tests\n      test:\n        name: Test\n        runs-on: ubuntu-latest\n        services:\n          mongo:\n            image: mongo\n            env:\n              MONGO_INITDB_ROOT_USERNAME: ${APP_MONGO_USER}\n              MONGO_INITDB_ROOT_PASSWORD: ${APP_MONGO_PASS}\n              MONGO_INITDB_DATABASE: ${APP_MONGO_DB}\n            ports:\n              - 27017:27017\n        steps:\n          - uses: actions/checkout@v2\n          - uses: actions-rs/toolchain@v1\n            with:\n              profile: minimal\n              toolchain: stable\n              override: true\n          - uses: actions-rs/cargo@v1\n            with:\n              command: test\n`\nConfig file (development.toml).\n```\n`[application]\nhost = \"127.0.0.1\"\nport = 8080\n`\n```\nConnecting to the database. The environment variables and config file get merged and I'm accessing them here through config: &Settings.\n`pub async fn init(config: &Settings) -> Result {\n    let client_options = ClientOptions::parse(\n        format!(\n            \"mongodb://{}:{}@{}:27017\",\n            config.mongo.user, config.mongo.pass, config.application.host\n        )\n        .as_str(),\n    )\n    .await?;\n\n    let client = Client::with_options(client_options)?;\n    let database = client.database(\"test\"); // TODO: replace with env var\n\n    database.run_command(doc! {\"ping\": 1}, None).await?;\n    println!(\"Connected successfully.\");\n\n    Ok(database)\n}\n`\nCalling the init function.\n`// Mongo\nlet mongo = mongo::init(&config).await.expect(\"Failed to init mongo\");\n`\nThe error I get.\n```\n`thread 'health_check' panicked at 'Failed to init mongo: Error { kind: ServerSelectionError { message: \"Server selection timeout: No available servers. Topology: { Type: Unknown, Servers: [ { Address: 127.0.0.1:27017, Type: Unknown, Error: Connection refused (os error 111) }, ] }\" }, labels: [] }', tests/health_check.rs:31:44\n`\n```",
      "solution": "I eventually solved it by adding a health check to my service. Seems that my issue had something to do with my database not being up yet before running the tests.\n`services:\n          mongodb:\n              image: mongo\n              env:\n                MONGO_INITDB_ROOT_USERNAME: test\n                MONGO_INITDB_ROOT_PASSWORD: password\n                MONGO_INITDB_DATABASE: test\n              options: >-\n                --health-cmd mongo\n                --health-interval 10s\n                --health-timeout 5s\n                --health-retries 5\n              ports:\n                - 27017:27017\n`",
      "question_score": 6,
      "answer_score": 11,
      "created_at": "2021-02-22T15:04:38",
      "url": "https://stackoverflow.com/questions/66317184/github-actions-cannot-connect-to-mongodb-service"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 66375732,
      "title": "Zsh: Command Not found : mongo After trying to install mongodb 4.2 using brew",
      "problem": "I have tried the following steps to install and setup mongodb in my mac from here https://docs.mongodb.com/manual/tutorial/install-mongodb-on-os-x/ but I got the following error when running the final \"mongo\" command in my terminal:\nError Message -  Zsh: Command Not found : mongo\nThis error msg occurred after trying to install mongodb 4.2 using brew\n```\n`sudo chown -R $(whoami) $(brew --prefix)/*\n\nthen\n\nbrew tap mongodb/brew\n\nthen\n\nbrew install mongodb-community@4.2\n\nand\n\nbrew services start mongodb-community@4.2\n\nor\n\nmongod --config /usr/local/etc/mongod.conf\n\nthen\n\nps aux | grep -v grep | grep mongod\n\nand\n\nmongo\n`\n```\nrunning brew services start mongodb-community@4.2 returns:\n```\n`Successfully started `mongodb-community@4.2` (label: homebrew.mxcl.mongodb-community@4.2)\n`\n```\nrunning ps aux | grep -v grep | grep mongod returns:\n```\n`9081   0.2  0.5  5528024  41856   ??  S     3:01pm   0:01.48 /usr/local/opt/mongodb-community@4.2/bin/mongod --config /usr/local/etc/mongod.conf\n\n7613   0.0  0.1  4298832   5600 s000  T     2:47pm   0:00.08 vim /usr/local/etc/mongod.conf\n`\n```\nrunning mongod --config /usr/local/etc/mongod.conf returns:\n```\n`zsh: command not found: mongod\n`\n```\nThere are also no mongo files in my /usr/local/bin directory after using these commands\nI created a data/db folder in my /usr/local/bin directory using the following commands:\n```\n`sudo mkdir -p  /usr/local/bin/data/db   \nsudo chown -R `id -un`  /usr/local/bin/data/db      \n`\n```\nRunning \"brew update\" returns:\n```\n`brew update                                                                                                                                                    \nUpdated 1 tap (homebrew/cask).\n==> Updated Casks\nbrave-browser\n`\n```",
      "solution": "Solved it by manually installing the mongodb community files and db tools using the website instead. Then copying them into /usr/local/bin. Then ignoring the app permissions whenever calling mongo or related commands in the terminal through System Preferences > Security & Privacy > General.\nAfter googling I found out that mongoimport and the other features have to be installed separately: https://www.mongodb.com/try/download/database-tools\nFollowed by copying those bin files after extracting them into the same /usr/local/bin directory\nNot sure why its' not working through homebrew though",
      "question_score": 6,
      "answer_score": 3,
      "created_at": "2021-02-25T21:06:03",
      "url": "https://stackoverflow.com/questions/66375732/zsh-command-not-found-mongo-after-trying-to-install-mongodb-4-2-using-brew"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 72355901,
      "title": "Property &#39;_id&#39; does not exist on type. Getting type error when trying to access property _id on result of a promise in nestjs application",
      "problem": "In my nest application I am getting type error when calling `_id` on `user` because mongoose defines the `_id` automatically & therefore its not present in my schema which is defined as type for the promise.\nWhen the promise type is changed to any like `Promise` then there is no type error.\n```\n`async create(createUserDto: CreateUserDto): Promise {\n    const createdUser = await new this.userModel(createUserDto).save();\n    return createdUser;\n  }\n`\n```\nbut I want to know is this the correct way or I should be doing something else.\nI do not want to define `_id` in schema to solve this issue.\n```\n` @Prop({ auto: true})\n _id!: mongoose.Types.ObjectId;\n`\n```\nuser.schema.ts\n```\n`// all the imports here....\n\nexport type UserDocument = User & Document;\n\n@Schema({ timestamps: true })\nexport class User {\n\n  @Prop({ required: true, unique: true, lowercase: true })\n  email: string;\n\n  @Prop()\n  password: string;\n\n}\n\nexport const UserSchema = SchemaFactory.createForClass(User);   \n`\n```\nusers.controller.ts\n```\n`@Controller('users')\n@TransformUserResponse(UserResponseDto)\nexport class UsersController {\n  constructor(private readonly usersService: UsersService) {}\n\n  @Post()\n  async create(@Body() createUserDto: CreateUserDto) {\n      const user = await this.usersService.create(createUserDto);\n      return user._id;\n  }\n\n}\n`\n```\nusers.service.ts\n```\n`// all the imports here....  \n   \n@Injectable()\nexport class UsersService {\n  constructor(@InjectModel(User.name) private userModel: Model) {}\n\n  async create(createUserDto: CreateUserDto): Promise {\n    const createdUser = await new this.userModel(createUserDto).save();\n    return createdUser;\n  }\n}\n`\n```",
      "solution": "Your `create` service method should return `Promise` instead of `Promise`. `UserDocument` has the `_id` property specified.\n`async create(createUserDto: CreateUserDto): Promise {\n    const createdUser = await new this.userModel(createUserDto).save();\n    return createdUser;\n  }\n`",
      "question_score": 6,
      "answer_score": 12,
      "created_at": "2022-05-24T01:56:18",
      "url": "https://stackoverflow.com/questions/72355901/property-id-does-not-exist-on-type-getting-type-error-when-trying-to-access"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 66643862,
      "title": "Mongo $push operator do not accept object according to typescript: &quot;No overload matches this call&quot;",
      "problem": "I want to push a message in an array with Mongoose/Mongo. Typescript claims that:\n\nNo overload matches this call.\nOverload 1 of 3, '(id: any, update: UpdateQuery, options: QueryOptions & { rawResult: true; }, callback?: ((err: any, doc: FindAndModifyWriteOpResultObject, res: any) => void) | undefined): any', gave the following error.\nType '{ messages: { message: string; authorId: string; }; }' is not assignable to type 'PushOperator, string | number | symbol>>>'.\nType '{ messages: { message: string; authorId: string; }; }' is not assignable to type 'NotAcceptedFields, string | number | symbol>>, readonly any[]>'.\nProperty 'messages' is incompatible with index signature.\n\nHere is the code snippet:\n\r\n\r\n`    const conversation = await Conversation.findByIdAndUpdate(\n      conversationId,\n      {\n        $push: { messages: { message, authorId } },\n        lastMessage: {\n          authorId,\n          snippet: `${message.substring(0, 47)}...`,\n          read: false,\n        },\n      },\n      { new: true }\n    );`\r\n\r\n\r\n\nThe build crash because of this. How to fix it?",
      "solution": "This is an issue with types not lining up.\nMy understanding is the push operation can only be done on a field that is explicitly defined as an array at some point in your code previously.\nA potential solution is to explicitly set the type of your collection to contain the fields you want (which is good practice anyway).\nSo rather than something along the lines of\n```\n`const Conversations = database.collection('conversations');\n`\n```\nYou would explicitly define the schema for the 'conversation' documents, and then type the collection:\n```\n`import { Collection } from 'mongodb';\n//...\ntype Conversation = {\n  messages: { message: String, authorId: Number }[],\n//...\n}\n\nconst Conversations: Collection = database.collection('conversations');\n`\n```",
      "question_score": 6,
      "answer_score": 11,
      "created_at": "2021-03-15T19:47:13",
      "url": "https://stackoverflow.com/questions/66643862/mongo-push-operator-do-not-accept-object-according-to-typescript-no-overload"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb",
      "question_id": 70764771,
      "title": "mongoose findOne returns first object found even if condition is not true",
      "problem": "I am using the MERN stack, I have a problem where mongoose findOne returns first object found even if condition is not true\nThis is my user model\n```\n`{ local:{\nusername,\nemail,\npassword\n},\ngoogle: {\ngoogleId,\nemail,\nname\n}\n}\n`\n```\nI have this schema statics that find if email exists or not\n```\n`userSchema.statics.findByEmail = async (email) => {\n  const localUser = await User.findOne({ \"local.email\": email });\n  const googleUser = await User.findOne({ \"goolge.email\": email });\n  return { localUser, googleUser };\n};\n`\n```\nIn my api before i add a user i search for it in my database using findByEmail\n```\n`router.post(\"/register\", async (req, res) => {\n  const user = await User.findByEmail(req.user.email);\n})\n`\n```\nif there is no documents in my database, the api works perfectly but if there is at least one document, it always returns first document in my database even if  `req.user.email`\nisn't in my database",
      "solution": "This problem happens in case in case the field is not declared in the schema where its being queried.\nPlease make sure that in your User Schema these fields exists: \"local.email\" and \"goolge.email\"",
      "question_score": 6,
      "answer_score": 11,
      "created_at": "2022-01-19T03:54:04",
      "url": "https://stackoverflow.com/questions/70764771/mongoose-findone-returns-first-object-found-even-if-condition-is-not-true"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongoose",
      "question_id": 76438660,
      "title": "Class constructor ObjectId cannot be invoked without &#39;new&#39; in mongoose",
      "problem": "When I send data from frontend to backend via API request I got this error:\n\nClass constructor ObjectId can not be invoked without 'new' in mongoose,\n\nI tried to convert into string and also into int but it didn't work.....",
      "solution": "I am new to nodejs too, i am trying to post some questions, even answers that i am not able to find.\nHere the problem not from frontend and backend, it is from mongoose schema\n\nwhen we are converting data to objectId we just need to declare new\n\n.\nSolution: new mongoose.Types.ObjectId(Id)\nIf you find helpful then like the answer so others reach fast.",
      "question_score": 14,
      "answer_score": 19,
      "created_at": "2023-06-09T10:47:03",
      "url": "https://stackoverflow.com/questions/76438660/class-constructor-objectid-cannot-be-invoked-without-new-in-mongoose"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongoose",
      "question_id": 67879357,
      "title": "Nestjs: How to use mongoose to start a session for transaction?",
      "problem": "The mongoose documentation to use a transaction is straightforward but when it is followed in nestjs, it returns an error:\n```\n`Connection 0 was disconnected when calling `startSession`\nMongooseError: Connection 0 was disconnected when calling `startSession`\n    at NativeConnection.startSession\n`\n```\nMy code:\n```\n`const transactionSession = await mongoose.startSession();\n    transactionSession.startTransaction();\n\n    try\n    {\n      const newSignupBody: CreateUserDto = {password: hashedPassword, email, username};\n  \n      const user: User = await this.userService.create(newSignupBody);\n\n      //save the profile.\n      const profile: Profile = await this.profileService.create(user['Id'], signupDto);\n\n      const result:AuthResponseDto = this.getAuthUserResponse(user, profile);\n\n      transactionSession.commitTransaction();\n      return result;\n    }\n    catch(err)\n    {\n      transactionSession.abortTransaction();\n    }\n    finally\n    {\n      transactionSession.endSession();\n    }\n`\n```",
      "solution": "I found the solution after studying @nestjs/mongoose. The mongoose here has no connection in it. This is the reason of error being returned.\nThe solution:\n`import {InjectConnection} from '@nestjs/mongoose';\nimport * as mongoose from 'mongoose';\n`\nIn the constructor of the service class, we need add connection parameter that can be used by the service.\n`export class AuthService {\nconstructor(\n  // other dependencies...\n  @InjectConnection() private readonly connection: mongoose.Connection){}\n`\nInstead of\n`const transactionSession = await mongoose.startSession();\ntransactionSession.startTransaction();\n`\nWe will now use:\n`const transactionSession = await this.connection.startSession();\ntransactionSession.startTransaction();\n`\nThis way, the issue of disconnection after startSession() can be resolved.",
      "question_score": 13,
      "answer_score": 29,
      "created_at": "2021-06-08T00:26:37",
      "url": "https://stackoverflow.com/questions/67879357/nestjs-how-to-use-mongoose-to-start-a-session-for-transaction"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongoose",
      "question_id": 65667158,
      "title": "Create mongoose schema methods using TypeScript",
      "problem": "I try to create method hashPassword for user schema.\n```\n`schema.method(\"hashPassword\", function (): void {\n  const salt = bcrypt.genSaltSync(10);\n  const hash = bcrypt.hashSync(this.password, salt);\n\n  this.password = hash;\n});\n`\n```\nAnd get an error `Property 'password' does not exist on type 'Document'.` on password\nHere is my file\n```\n`import mongoose, { Schema, Document } from \"mongoose\";\nimport bcrypt from \"bcryptjs\";\n\n/**\n * This interface should be the same as JWTPayload declared in types/global.d.ts file\n */\nexport interface IUser extends Document {\n  name: string;\n  email: string;\n  username: string;\n  password: string;\n  confirmed: boolean;\n  hashPassword: () => void;\n  checkPassword: (password: string) => boolean;\n}\n\n// User schema\nconst schema = new Schema(\n  {\n    name: {\n      type: String,\n      required: true,\n    },\n    email: {\n      type: String,\n      required: true,\n    },\n    username: {\n      type: String,\n      required: true,\n    },\n    password: {\n      type: String,\n      required: true,\n    },\n    confirmed: {\n      type: Boolean,\n      default: false,\n    },\n  },\n  { timestamps: true }\n);\n\nschema.method(\"hashPassword\", function (): void {\n  const salt = bcrypt.genSaltSync(10);\n  const hash = bcrypt.hashSync(this.password, salt);\n\n  this.password = hash;\n});\n\n// User model\nexport const User = mongoose.model(\"User\", schema, \"users\");\n\n`\n```",
      "solution": "At the point where you define the method, the `schema` object doesn't know that it is the `Schema` for an `IUser` and not just any `Document`.  You need to set the generic type for the `Schema` when you create it: `new Schema( ... )`.",
      "question_score": 8,
      "answer_score": 10,
      "created_at": "2021-01-11T13:46:42",
      "url": "https://stackoverflow.com/questions/65667158/create-mongoose-schema-methods-using-typescript"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongoose",
      "question_id": 73554309,
      "title": "Typescript error when defining property as ObjectId in Mongoose",
      "problem": "I'm writing interface and schema in mongoose, but got some typescript error:\n`import { Schema, Types } from 'mongoose';\n\ninterface Foo {\n  _id: Types.ObjectId,\n  bar: Types.ObjectId,\n}\n\nconst FooSchema = new Schema({\n  _id: { type: Types.ObjectId },\n  bar: { type: Types.ObjectId },\n});\n\n`\nboth property `_id` and `bar` has typescript error in this code:\n```\n`Type '{ type: typeof Types.ObjectId; }' is not assignable to type 'SchemaDefinitionProperty | undefined'.\n  Types of property 'type' are incompatible.\n    Type 'typeof ObjectId' is not assignable to type 'typeof Mixed | ObjectIdSchemaDefinition | undefined'.\n      Type 'typeof ObjectId' is missing the following properties from type 'typeof Mixed': schemaName, cast, checkRequired, set, get\n`\n```\nThe mongoose version is 6.5.4 (which is currently the latest version available)\nI have no idea why. It's not the first time to me to write mongoose schema, but didn't have any problems like this.\nI've confirmed that I can reproduce this error from clean new project. (means that there's no other background reason)",
      "solution": "Here, try changing your schema type definitions like so:\n```\n`const FooSchema = new Schema({\n  _id: { type: mongoose.Schema.Types.ObjectId },\n  bar: { type: mongoose.Schema.Types.ObjectId },\n});\n\n`\n```\nExplanation:\nAccording to the Mongoose documentation, a SchemaType is different from a regular type. In other words, `mongoose.ObjectId !== mongoose.Types.ObjectId`. I.E the SchemaType `mongoose.Schema.Types.ObjectId` is a special definition used by Mongoose. The regular type definition `Types.ObjectId` can be used to create Interfaces, but the SchemaType must be used when creating a schema. It's slightly confusing yes. Just like SchemaType `String` must be used in a Schema instead of `string` in an Interface even though both are technically type definitions. Hope this helps!",
      "question_score": 8,
      "answer_score": 15,
      "created_at": "2022-08-31T12:12:23",
      "url": "https://stackoverflow.com/questions/73554309/typescript-error-when-defining-property-as-objectid-in-mongoose"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongoose",
      "question_id": 65704763,
      "title": "Typescript &amp; Mongoose - &quot;this&quot; not available in instance methods",
      "problem": "I am currently converting my API from JS to TS. However, I'm having some difficulties with mongoose & typescript. Specifically, `this` is not available inside my instance methods.\nMy code:\n```\n`AccountSchema.methods.comparePassword = async function (candidatePassword: string) {\n  const isMatch = await bcrypt.compare(candidatePassword, this.password);\n  return isMatch;\n};\n`\n```\nWhich generates the following error as `this` refers to the function rather than the actual document:\n\nArgument of type 'Function' is not assignable to parameter of type 'string'.\n\nHowever, according to Mongoose's documentation, there shouldn't be an error as `this` should refer to the actual document.\nWhat this actually refers to:\n```\n`this: {\n    [name: string]: Function;\n}\n`\n```\nHow I defined my schema:\n```\n`const AccountSchema = new mongoose.Schema({...})\n`\n```\nMy approach worked just fine with JS, so I know it has something to do with TypeScript. Mongoose documentation confirms that my implementation is the right way to define instance methods, so I'm quite confused why it doesn't work.\nIs there a specific TS way of declaring & using instance methods in mongoose that I don't know of?\nAny help would be greatly appreciated!",
      "solution": "All I needed was a `this` parameter on that function. That's special in TS, and specifies the type of `this` within the function.\nLike so:\n```\n`async function (this: IAccount, candidatePassword: string) { ... }\n`\n```\n`this` parameters are not passed explicitly, so the new parameter doesn't change how the function is called.",
      "question_score": 7,
      "answer_score": 7,
      "created_at": "2021-01-13T16:17:19",
      "url": "https://stackoverflow.com/questions/65704763/typescript-mongoose-this-not-available-in-instance-methods"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongoose",
      "question_id": 72460269,
      "title": "How to read a .env file on MongooseModule in Nestjs?",
      "problem": "So I am trying to add a config to my NestJs project, so far I've been using MongooseModule in order to connect to the Database but I was providing the full URL in MongooseModule.forRoot().\nIt was something like this:\n```\n`//app.module.ts\nimport { Module } from '@nestjs/common';\nimport { MongooseModuele } from '@nestjs/mongoose';\n\n@Module({\n  imports: [MongooseModule.forRoot('mongodb://.....')]\n})\n`\n```\nSo then I added the nestjs config and its looking like this:\n```\n`//app.module.ts\nimport { Module } from '@nestjs/common';\nimport { MongooseModuele } from '@nestjs/mongoose';\nimport { ConfigModule, ConfigService } from '@nestjs/config';\n\n@Module({\n  imports: [\n    ConfigModule.forRoot({\n      isGlobal: true,\n    }),\n    MongooseModule.forRootAsync({\n     imports: [ConfigModule],\n     useFactory: async (config: ConfigService) => ({\n      uri: config.get('DB_HOST'),\n     }),\n     inject: [ConfigService],\n   }),\n  ]\n})\n`\n```\nBut then got this error:\n[Nest] 14098  - 06/01/2022, 7:16:42 AM   ERROR [ExceptionHandler] Invalid scheme, expected connection string to start with \"mongodb://\" or \"mongodb+srv://\"\nI also tried this way:\n```\n`//app.module.ts\nimport { Module } from '@nestjs/common';\nimport { MongooseModuele } from '@nestjs/mongoose';\nimport { ConfigModule, ConfigService } from '@nestjs/config';\n\n@Module({\n  imports: [\n    MongooseModule.forRootAsync({\n     imports: [ConfigModule],\n     useFactory: async (config: ConfigService) => ({\n      uri: config.get('DB_HOST'),\n     }),\n     inject: [ConfigService],\n   }),\n  ]\n})\n`\n```\nnest print this error:\nERROR [ExceptionHandler] The `uri` parameter to `openUri()` must be a string, got \"undefined\". Make sure the first parameter to `mongoose.connect()` or `mongoose.createConnection()` is a string.\nMy .env file looks like this:\n```\n`DB_HOST=\"mongodb://.....\"\n`\n```\nIt seems like that on the app.module MongooseModule is not reading my .env file, does anyone knows how to solve that?\nThanks",
      "solution": "Please try the following code, it works for me.\n```\n`// app.module.ts\nimport { Module } from '@nestjs/common';\nimport { ConfigModule, ConfigService } from '@nestjs/config'\nimport { MongooseModule } from '@nestjs/mongoose';\n\n@Module({\n  import: [\n    MongooseModule.forRootAsync({\n      imports: [ConfigModule],\n      inject: [ConfigService],\n      useFactory: async (config: ConfigService) => ({\n        uri: config.get('MONGODB_URI'), // Loaded from .ENV\n      })\n    })\n  ],\n})\nexport class AppModule {}\n`\n```",
      "question_score": 6,
      "answer_score": 25,
      "created_at": "2022-06-01T12:27:36",
      "url": "https://stackoverflow.com/questions/72460269/how-to-read-a-env-file-on-mongoosemodule-in-nestjs"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongoose",
      "question_id": 65550070,
      "title": "NodeJS TypeScript - Mongoose index.d.ts throwing errors",
      "problem": "Hello so I do not know what is the problem here so when I run my NodeJS server mongoose `index.d.ts` throws more than one error which I do not know of I tried ignoring the node_modules from tsconfig but it seems like I am not winning\nError Received: I am giving pastebin link cause trace back was long and I did not want to cut potential error: pastebin link having all error message\nError Header\n`node_modules/@types/mongoose/index.d.ts:79:1 - error TS6200: Definitions of the following identifiers conflict with those in another file: DocumentDefinition, FilterQuery, UpdateQuery, NativeError, Mongoose, SchemaTypes, STATES, connection, connections, models, mongo, version, CastError, ConnectionOptions, Collection, Connection, disconnected, connected, connecting, disconnecting, uninitialized, Error, QueryCursor, VirtualType, Schema, SchemaTypeOpts, Subdocument, Array, DocumentArray, Buffer, ObjectId, ObjectIdConstructor, Decimal128, Map, mquery, Aggregate, SchemaType, Promise, PromiseProvider, Model, Document, ModelUpdateOptions\n`\nMy tsonfig\n`{\n  \"compilerOptions\": {\n    \"module\": \"commonjs\",\n    \"esModuleInterop\": true,\n    \"allowSyntheticDefaultImports\": true,\n    \"target\": \"es6\",\n    \"noImplicitAny\": true,\n    \"moduleResolution\": \"node\",\n    \"sourceMap\": true,\n    \"outDir\": \"dist\",\n    \"baseUrl\": \".\",\n    \"paths\": {\n      \"*\": [\n        \"node_modules/*\",\n        \"src/types/*\"\n      ]\n    }\n  },\n  \"include\": [\n    \"src/server.ts\"\n  ],\n  \"exclude\": [\n    \"./node_modules/\"\n  ]\n}\n`\nMy base server setup:\n`import express, { Application } from 'express';\nimport { config } from 'dotenv';\nimport mongoose, { CastError, ConnectOptions } from 'mongoose';\nimport expressSession from 'express-session';\nimport MongoStore, { MongoDBStore } from 'connect-mongodb-session';\nimport cors from 'cors';\nconfig();\n\nconst app: Application = express();\nenum BaseUrl {\n  dev = 'http://localhost:3000',\n  prod = ''\n}\nconst corsOptions = {\n  origin: process.env.NODE_ENV === 'production' ? BaseUrl.prod : BaseUrl.dev,\n  credentials: true\n};\n\nconst mongoURI = process.env.mongoURI;\n//======================================================Middleware==============================================================\napp.use(cors(corsOptions));\n\nconst mongoStore = MongoStore(expressSession);\n\nconst store: MongoDBStore = new mongoStore({\n  collection: 'usersession',\n  uri: mongoURI,\n  expires: 10 * 60 * 60 * 24 * 1000\n});\n\nconst isCookieSecure: boolean = process.env.NODE_ENV === 'production' ? true : false;\n\napp.use(\n  expressSession({\n    secret: process.env.session_secret,\n    name: '_sid',\n    resave: false,\n    saveUninitialized: false,\n    store: store,\n    cookie: {\n      httpOnly: true,\n      maxAge: 10 * 60 * 60 * 24 * 1000,\n      secure: isCookieSecure,\n      sameSite: false\n    }\n  })\n);\n\n//================================================MongoDB Connection & Configs==================================================\nconst connectionOptions: ConnectOptions = {\n  useCreateIndex: true,\n  useFindAndModify: false,\n  useNewUrlParser: true,\n  useUnifiedTopology: true\n};\n\nmongoose.connect(mongoURI, connectionOptions, (error: CastError) => {\n  if (error) {\n    return console.error(error.message);\n  }\n  return console.log('Connection to MongoDB was successful');\n});\n\n//===============================================Server Connection & Configd====================================================\nconst PORT = process.env.PORT || 5000;\napp.listen(PORT, () => {\n  console.log(`Server started on PORT ${PORT}`);\n});\n`",
      "solution": "I solved the same problem by adding `skipLibCheck: true` option below to `compilerOptions`.\n\ntsconfig\n\n`{\n  \"compilerOptions\": {\n    \"module\": \"commonjs\",\n    \"esModuleInterop\": true,\n    \"allowSyntheticDefaultImports\": true,\n    \"target\": \"es6\",\n    \"noImplicitAny\": true,\n    \"skipLibCheck\": true,\n    \"moduleResolution\": \"node\",\n    \"sourceMap\": true,\n    \"outDir\": \"dist\",\n    \"baseUrl\": \".\",\n    \"paths\": {\n      \"*\": [\n        \"node_modules/*\",\n        \"src/types/*\"\n      ]\n    }\n  },\n  \"include\": [\n    \"src/server.ts\"\n  ],\n  \"exclude\": [\n    \"./node_modules/\"\n  ]\n}\n`",
      "question_score": 6,
      "answer_score": 18,
      "created_at": "2021-01-03T13:52:06",
      "url": "https://stackoverflow.com/questions/65550070/nodejs-typescript-mongoose-index-d-ts-throwing-errors"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongoose",
      "question_id": 65597011,
      "title": "What should be the type of a field in schema if &quot;any&quot; cannot be used in Nest.js?",
      "problem": "I'm working in Nest.js using Mongoose. While creating schema, I have a field extra_options which can store any type of value (array, object, string etc). Keeping its type as \"any\" does not work. What should be the correct type? Here is a fragment of the code.\n```\n`@Schema({\n    collection: 'xyz',\n    timestamps: {\n        updatedAt: 'updated_at',\n        createdAt: 'created_at'\n    },\n})\nexport class xyz {\n    @Prop({default: true})\n    active: boolean;\n\n    @Prop()\n    extra_options: any;\n\n    @Prop({required: true})\n    created_by: string;\n}\n`\n```",
      "solution": "```\n`import * as mongoose from 'mongoose';\n\n@Prop({type: mongoose.Schema.Types.Mixed})\nextra_options: any;\n`\n```\nThis worked.",
      "question_score": 6,
      "answer_score": 16,
      "created_at": "2021-01-06T14:50:38",
      "url": "https://stackoverflow.com/questions/65597011/what-should-be-the-type-of-a-field-in-schema-if-any-cannot-be-used-in-nest-js"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongoose",
      "question_id": 68970788,
      "title": "Getting no overload matches this call running mongoose with typescript and express",
      "problem": "I am getting the below error for useNewUrlParser, useUnifiedTopology, useCreateIndex:\n```\n`No overload matches this call.\n  Overload 1 of 3, '(uri: string, callback: CallbackWithoutResult): void', gave the following error.\n    Argument of type '{ useNewUrlParser: boolean; useUnifiedTopology: boolean; useCreateIndex: boolean; }' is not assignable to parameter of type 'CallbackWithoutResult'.\n      Object literal may only specify known properties, and 'useNewUrlParser' does not exist in type 'CallbackWithoutResult'.\n  Overload 2 of 3, '(uri: string, options?: ConnectOptions | undefined): Promise', gave the following error.\n    Argument of type '{ useNewUrlParser: boolean; useUnifiedTopology: boolean; useCreateIndex: boolean; }' is not assignable to parameter of type 'ConnectOptions'.\n      Object literal may only specify known properties, and 'useNewUrlParser' does not exist in type 'ConnectOptions'.\n`\n```\nIndex.ts\n```\n`import express from 'express'\nimport { errorHandler } from './middleware/error-handler';\nimport router from './router'\nimport mongoose from 'mongoose'\n\nconst app = express()\n\napp.use(express.json())\napp.use('/api/users', router)\napp.use(errorHandler)\n\nconst start = async () => {\n  try {\n    await mongoose.connect(\"mongodb://auth-mongo-srv:27017/auth\", {\n      useNewUrlParser: true,    // error here\n      useUnifiedTopology: true,\n      useCreateIndex: true,\n    });\n    console.log(\"Connected to MongoDb\");\n  } catch (err) {\n    console.error(err);\n  }\n};\n\nconst PORT = 3000\napp.listen(PORT, () => {\n    console.log(\"working on port \",PORT)\n})\n\nstart()\n`\n```\ntsconfig.json\n```\n`{\n  \"compilerOptions\": {\n\"target\": \"es6\",\n\"module\": \"commonjs\", \n\"esModuleInterop\": true, \n\"forceConsistentCasingInFileNames\": true,\n\"strict\": true, \n\"skipLibCheck\": true\n }\n}\n`\n```\nI have mongoose 6.02 installed in package.json and if it is of any help, I am getting mongo from dockerhub and running in a cluster pod.\nAny help or suggestion is appreciated.",
      "solution": "ConnectOptions of mongoose for version 6.0.2 does not include useNewUrlParser, useUnifiedTopology, useCreateIndex. However, mongoose docs for 6.0.2 still shows those options to be valid and without those options, mongoose is throwing a topology error.\nDowngraded mongoose to version 5.13.8 and now it is working without problems.",
      "question_score": 6,
      "answer_score": 2,
      "created_at": "2021-08-29T09:32:30",
      "url": "https://stackoverflow.com/questions/68970788/getting-no-overload-matches-this-call-running-mongoose-with-typescript-and-expre"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongoose",
      "question_id": 69820475,
      "title": "Model.find Mongoose 6.012 always return all documents even though having filter",
      "problem": "A sample of my schema,\n```\n`const XXXSchema = new mongoose.Schema({\n  name: String\n}\n`\n```\nI am using mongoose for awhile, and just recently I started experience the issues. The following query works as expected,\n```\n`await MyModel.find({_id: ObjectId(SOME NUMBER)}).exec()\n`\n```\nHowever, it always returns all documents if I query using any other fields, regardless value. For example,\n```\n`await MyModel.find({anotherField: \"some value\"}).exec()\n`\n```\nI tried to use callback, but the result is the same. Could someone help?\nThe mongodb version I use is 5.0.2.\nThanks.\n\nUpdate: I reviewed mongoose query debug, and found, mongoose ignored my filter, and only send an empty {} as the filter.",
      "solution": "I just found the due to a syntax error, my schema was not be used to create the corresponding model, and thus mongoose did not recognize the field I used to query.",
      "question_score": 6,
      "answer_score": 5,
      "created_at": "2021-11-03T07:27:55",
      "url": "https://stackoverflow.com/questions/69820475/model-find-mongoose-6-012-always-return-all-documents-even-though-having-filter"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongoose",
      "question_id": 66168628,
      "title": "An outer value of &#39;this&#39; is shadowed by this container with Mongoose Schema Typescript",
      "problem": "I have the following for a schema validator for MongoDB:{\n```\n`UserSchema.path('email').validate(async function (email: string) {\n  const count = await User.count({ email, _id: { $ne: this._id } })\n  return !count\n}, 'Email already exists')\n`\n```\nI'm getting the following error:\n```\n`'this' implicitly has type 'any' because it does not have a type annotation.ts(2683)\nUser.ts(171, 35): An outer value of 'this' is shadowed by this container.\n`\n```\nThis is defined in my `User.ts` file.  Everything works exactly as expected but this Typescript error is blocking CI from continuing.  Is there anyway to get around this (no pun intended).",
      "solution": "Try:\n```\n`UserSchema.path('email').validate(async function (this:any, email: string) {\n  const count = await User.count({ email, _id: { $ne: this._id } })\n  return !count\n}, 'Email already exists')\n`\n```\nYou can use your type instead of \"any\".\nAnd the link to the docu: https://www.typescriptlang.org/docs/handbook/functions.html#this-parameters",
      "question_score": 6,
      "answer_score": 8,
      "created_at": "2021-02-12T09:39:49",
      "url": "https://stackoverflow.com/questions/66168628/an-outer-value-of-this-is-shadowed-by-this-container-with-mongoose-schema-type"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongoose",
      "question_id": 79873598,
      "title": "MongoDB Atlas SRV connection fails with querySrv ECONNREFUSED after switching Node versions (Node 22, Windows)",
      "problem": "I\u2019m running into a MongoDB connection issue that only appeared after switching Node.js versions, and I haven\u2019t been able to resolve it despite extensive troubleshooting.\nEnvironment\n\nOS: Windows 10\n\nCurrent versions:\n```\n`node v22.22.0\nnpm 10.9.4\nmongoose 8.x\nmongodb 6.x\n\n`\n```\n\nThe application code and MongoDB Atlas cluster configuration have not changed.\n\nError\nOn application startup, I consistently get:\n```\n`Error: querySrv ECONNREFUSED _mongodb._tcp..mongodb.net\n    at QueryReqWrap.onresolve [as oncomplete] (node:internal/dns/promises:294:17)\n`\n```\n\nMongoDB connection\nI\u2019m using a standard Atlas SRV connection string, for example:\n```\n`mongodb+srv://:@.mongodb.net/?retryWrites=true&w=majority\n`\n```\nThis same connection string worked previously on Node 22.\n\nDNS verification (Windows)\nSRV resolution works at the OS level:\n```\n`nslookup -type=SRV _mongodb._tcp..mongodb.net\n`\n```\nThis returns the expected shard hosts and ports, so DNS itself appears to be resolving correctly outside of Node.\n\nWhat I\u2019ve already tried\n\nFull dependency reset:\n```\n`rm -rf node_modules package-lock.json\nnpm cache clean --force\nnpm install\nnpm rebuild\n\n`\n```\n\nVerified Node and npm versions\n\nForced IPv4 DNS resolution in Node:\n```\n`import dns from \"dns\";\ndns.setDefaultResultOrder(\"ipv4first\");\n\n`\n```\n\nChanged system DNS to Cloudflare (`1.1.1.1`)\n\nRestarted the system\n\nVerified `.env` loading and `mongoose.connect()` usage\n\nNone of the above resolved the issue.",
      "solution": "After switching Node.js versions on Windows, Node started using a broken or unreachable DNS resolver. Even though SRV records resolved correctly with `nslookup`, Node\u2019s internal DNS resolver failed, causing `querySrv ECONNREFUSED` before any MongoDB connection was attempted.\nFix\nForce DNS servers explicitly in Node before connecting to MongoDB:\n```\n`require(\"node:dns/promises\").setServers([\"1.1.1.1\", \"8.8.8.8\"]);\n`\n```\nWhy this works\nNode.js does not always use the Windows system DNS resolver. Setting DNS servers inside Node bypasses resolver issues introduced by switching Node versions and allows MongoDB Atlas SRV lookup to succeed.\nRef: Error: querySrv ECONNREFUSED MongoDB",
      "question_score": 6,
      "answer_score": 7,
      "created_at": "2026-01-22T11:01:10",
      "url": "https://stackoverflow.com/questions/79873598/mongodb-atlas-srv-connection-fails-with-querysrv-econnrefused-after-switching-no"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongoose",
      "question_id": 72651883,
      "title": "Mongoose is causing 100+ type errors that break the build?",
      "problem": "I am trying to get a server build running for a small API. However, I've been hitting a painful problem. Mongoose fails to compile under TypeScript, due to it's internal build of MongoDB. This problem is similar but not an exact repeat of others - I've found a few of them, but their solutions have not worked.\nThis is the kind of output I've gotten, error wise:\n```\n`> servapi@1.0.0 clean\n> node node_modules/rimraf/bin lib\n\nnode_modules/mongodb/mongodb.d.ts:3571:117 - error TS1005: '?' expected.\n\n3571 export declare type Join = T extends [] ? '' : T extends [string | number] ? `${T[0]}` : T extends [string | number, ...infer R] ? `${T[0]}${D}${Join}` : string;\n                                                                                                                        \n\nnode_modules/mongodb/mongodb.d.ts:3571:127 - error TS1005: ';' expected.\n\n3571 export declare type Join = T extends [] ? '' : T extends [string | number] ? `${T[0]}` : T extends [string | number, ...infer R] ? `${T[0]}${D}${Join}` : string;\n                                                                                                                               \n\nnode_modules/mongodb/mongodb.d.ts:3571:131 - error TS1005: ';' expected.\n\n3571 export declare type Join = T extends [] ? '' : T extends [string | number] ? `${T[0]}` : T extends [string | number, ...infer R] ? `${T[0]}${D}${Join}` : string;\n                                                                                                                                  \n\nnode_modules/mongodb/mongodb.d.ts:3571:166 - error TS1005: ',' expected.\n\n3571 export declare type Join = T extends [] ? '' : T extends [string | number] ? `${T[0]}` : T extends [string | number, ...infer R] ? `${T[0]}${D}${Join}` : string;\n                                                                                                                                                                      \nnode_modules/mongodb/mongodb.d.ts:3571:195 - error TS1005: '(' expected.\n\n3571 export declare type Join = T extends [] ? '' : T extends [string | number] ? `${T[0]}` : T extends [string | number, ...infer R] ? `${T[0]}${D}${Join}` : string;\n                                                                                                                                                                                                   \n\nnode_modules/mongodb/mongodb.d.ts:4021:7 - error TS1005: ',' expected.\n\n4021  * \n~~~~~~~~~~~\n\nFound 114 errors.\n`\n```\nBasically..everything in mongodb fails. I get multiple such  type errors. External types are no longer maintained, so those cannot be installed to help.\nUpdating Mongoose and MongoDB to the latest version at least reduced the errors from 200+.\nThis is my current tsconfig and package.json:\ntsconfig:\n```\n`{\n  \"compilerOptions\": {\n    \"outDir\": \"lib\",\n    \"target\": \"es6\",\n    \"module\": \"commonjs\",\n    \"strict\": false,\n    \"allowJs\": true,\n    \"allowSyntheticDefaultImports\": true,\n    \"strictNullChecks\": false,\n    \"esModuleInterop\": true,\n    \"experimentalDecorators\": true,\n    \"skipLibCheck\": true,\n    \"forceConsistentCasingInFileNames\": true,\n    \"resolveJsonModule\": true\n  },\n  \"include\": [\n    \"src\",\n    \"src/data\"\n  ],\n  \"exclude\": [\n    \"node_modules\"\n  ]\n}\n`\n```\npackage.json:\n```\n`    {\n      \"private\": true,\n      \"name\": \"servapi\",\n      \"version\": \"1.0.0\",\n      \"description\": \"API Server\",\n      \"main\": \"lib/index.js\",\n      \"directories\": {\n        \"data\": \"src/data\"\n      },\n      \"scripts\": {\n        \"start\": \"ts-node-dev --respawn --transpile-only src/index.ts\",\n        \"build\": \"npm run clean && tsc && node node_modules/copyfiles/copyfiles package.json ./lib && node node_modules/copyfiles/copyfiles ./lib\",\n        \"clean\": \"node node_modules/rimraf/bin lib\"\n      },\n      \"author\": \"\",\n      \"license\": \"UNLICENSED\",\n      \"devDependencies\": {\n        \"@types/cors\": \"^2.8.6\",\n        \"@types/express\": \"^4.17.1\",\n        \"@types/mocha\": \"^8.2.3\",\n        \"copyfiles\": \"^2.4.1\",\n        \"mocha\": \"^9.0.2\",\n        \"rimraf\": \"^2.7.1\",\n        \"ts-node\": \"^8.1.0\",\n        \"ts-node-dev\": \"^1.0.0-pre.63\",\n        \"typescript\": \"^3.7.0\"\n      },\n      \"dependencies\": {\n        \"@types/multer\": \"^1.4.7\",\n        \"axios\": \"^0.22.0\",\n        \"cors\": \"^2.8.5\",\n        \"escape-goat\": \"^4.0.0\",\n        \"express\": \"^4.16.4\",\n        \"mongoose\": \"^6.3.8\",\n        \"multer\": \"^1.4.5-lts.1\"\n      }\n    }\n`\n```\nIt does appear to at least partially build, but halts when it hits the index.js. Adding \"skipLibCheck\" doesn't seem to work - my guess would be because MongoDB is being called by Mongoose. How can I fix these type errors?",
      "solution": "The answer actually turned out to be a TypeScript problem.\nFor whatever reason, earlier versions don't like the current typing in Mongo/ose, etc. If you come across this problem - upgrade to the latest TypeScript version if you can do so.\nFor me, that was by forcing TS to the latest version (mine was around 4.7.2) - Remove the ~ in your package.json, change the TypeScript version manually and do an NPM Update.",
      "question_score": 6,
      "answer_score": 5,
      "created_at": "2022-06-16T23:06:13",
      "url": "https://stackoverflow.com/questions/72651883/mongoose-is-causing-100-type-errors-that-break-the-build"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongoose",
      "question_id": 69532987,
      "title": "Mongoose returns &quot;new ObjectId&quot; in _id field of the result",
      "problem": "When I try to query, the result contains the `_id` field with \"new ObjectId()\" in it. How to avoid this \"new ObjectId()\" and only include the hash value as string. Because of this issue, sending the data back as JSON response is failing.\nThe following is a basic demo:\nMy code for querying:\n```\n`book_data = await Book.find({ slug: 'test' }).\n                        where('status').equals('Active').select('title').exec()\n\nconsole.log( book_data )                    \n`\n```\nResponse:\n```\n`[\n  {\n    _id: new ObjectId(\"6164aff742da0eac31a87b9a\"),\n    title: 'Test' \n  }\n]\n`\n```",
      "solution": "You can pass in an options object to the `.find` method. There, you can specify the field you want to ignore by providing a 0 as the value.\nSee here for more info as well.\nSomething like this should do the trick:\n`book_data = await Book.find(\n    { slug: 'test' }, // Query\n    { _id: 0 }        // Options\n).where('status').equals('Active').select('title').exec();\n`\nWhile the above query should do the trick, I believe this means you could also just do:\n`book_data = await Book.find(\n    // Query\n    { slug: 'test', status: 'Active' },\n    // Options\n    { _id: 0, title: 1 }                \n).exec();\n`\nEdit 2\nI found a better way to do this via `.find`..\nMongo Playground\nThis is what your code would look like:\n`book_data = await Book.find({\n  slug: \"test\",\n  status: \"active\"\n},\n{\n  title: 1,\n  _id: {\n    \"$toString\": \"$_id\"\n  }\n}).exec();\n`",
      "question_score": 6,
      "answer_score": 5,
      "created_at": "2021-10-12T00:12:24",
      "url": "https://stackoverflow.com/questions/69532987/mongoose-returns-new-objectid-in-id-field-of-the-result"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongoose",
      "question_id": 67796060,
      "title": "Mongoose + Typescript requires me to extend Document",
      "problem": "I'm trying to follow the instructions given in the Mongoose documentation for their support of TypeScript: https://mongoosejs.com/docs/typescript.html.\nIn the docs, they provide the following example:\n`import { Schema, model, connect } from 'mongoose';\n\n// 1. Create an interface representing a document in MongoDB.\ninterface User {\n  name: string;\n  email: string;\n  avatar?: string;\n}\n\n// 2. Create a Schema corresponding to the document interface.\nconst schema = new Schema({\n  name: { type: String, required: true },\n  email: { type: String, required: true },\n  avatar: String\n});\n\n// 3. Create a Model.\nconst UserModel = model('User', schema);\n`\nThey also provide an alternative example where the interface is extending `Document`:\n`interface User extends Document {\n  name: string;\n  email: string;\n  avatar?: string;\n}\n`\nThey recommend not to use the approach of extending the `Document`. However, when I try their exact code (without `extends`) I get the following errors:\n```\n`Type 'User' does not satisfy the constraint 'Document'.\n  Type 'User' is missing the following properties from type 'Document': $getAllSubdocs, $ignore, $isDefault, $isDeleted, and 47 more.\n`\n```\nI'm using Mongoose v 5.12.7. Is there something I don't understand about this? How can I create the schema without extending the document? I want to test it later on, and I don't want to have to mock 47 or more properties...",
      "solution": "OK, I figured the issue. I had two problems. First, I was using the third-party `@types/mongoose` package, which is no longer needed as Mongoose natively supports TypeScript.\nHowever, the main issue was actually the version. Although in the docs it says that the new support exists since v5.11.0, the difference between v5.12.7 which I had installed and the current version\u20145.12.12 actually was important. Once I updated the version all the errors disappeared.",
      "question_score": 6,
      "answer_score": 2,
      "created_at": "2021-06-01T23:12:43",
      "url": "https://stackoverflow.com/questions/67796060/mongoose-typescript-requires-me-to-extend-document"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongoose",
      "question_id": 65934020,
      "title": "Mongoose: CastError: Cast to embedded failed for value &quot;{ value: &#39;x&#39; }&quot; at path &quot;items&quot;",
      "problem": "After updating to Mongoose 5.11.13 I am getting the following error when trying to add an item to a sub-object inside a document.\n```\n`CastError: Cast to embedded failed for value \"{ value: 'new item' }\" at path \"items\"\n    at model.Query.exec (D:\\repos\\pushbox\\node_modules\\mongoose\\lib\\query.js:4358:21)\n    at model.Query.Query.then (D:\\repos\\pushbox\\node_modules\\mongoose\\lib\\query.js:4452:15)\n    at processTicksAndRejections (internal/process/task_queues.js:97:5) {\n  messageFormat: undefined,\n  stringValue: `\"{ value: 'new item' }\"`,\n  kind: 'embedded',\n  value: \"{ value: 'new item' }\",\n  path: 'items',\n  reason: TypeError: this.ownerDocument(...).isSelected is not a function\n`\n```\nMy main Schma is called `Card`. It holds a sub-object/sub-document called `Property` and it looks like this:\n`export const CardSchema = new mongoose.Schema({\n  title: {\n    type: String,\n    required: true,\n  },\n\n  description: {\n    type: String,\n    default: '',\n  },\n\n  // Checklists in a Card\n  checklists: [{\n    title: {\n      type: String,\n      required: true,\n    },\n    items: [{\n      name: String,\n      select: Boolean,\n    }],\n  }],\n // Properties in a card\n  properties: [{\n    name: {\n      type: String,\n      required: true,\n    },\n    items: [{\n      value: { type: String, default: '' },\n      isSelected: { type: Boolean, default: false },\n    }],\n  }],\n\n  box: {\n    type: ObjectId,\n    ref: 'Box',\n  },\n}, {\n  timestamps: { createdAt: true, updatedAt: true },\n});\n`\nThe query being used to insert a new `item` inside a `property` is:\n`const newPropItem = await Card.findOneAndUpdate(\n        {\n          _id: cardId,\n          'properties._id': propertyId,\n        },\n        {\n          $push: {\n            'properties.$.items': { value: newItem.trim() },\n          },\n        },\n        {\n          new: true,\n        },\n      );\n`\nI have no idea why this is happening as we have a similar query for `Checklist` and it works. I tried this query inside the mongo shell and it worked there. Could you guys help me figure out what exactly am I missing?\nOh and I tried looking into the whole `TypeError: this.ownerDocument(...).isSelected is not a function` part as well, didnt have any luck finding anything that could help me in my case",
      "solution": "you can not using `isSelected` in Schema as a field name because `isSelected()` internally for checking which paths we need to validate in mongoose,so change filed\nname to `isSelect` or ...",
      "question_score": 6,
      "answer_score": 1,
      "created_at": "2021-01-28T10:06:49",
      "url": "https://stackoverflow.com/questions/65934020/mongoose-casterror-cast-to-embedded-failed-for-value-value-x-at-path"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongoose",
      "question_id": 69763130,
      "title": "MongooseServerSelectionError: connect ECONNREFUSED ::1:27017 wont get fixed",
      "problem": "I've been trying for over 2 hours now trying to figure out what's wrong with this database. I've tried everything. From reinstalling the server, restarting the processes, rebooting and so much more. It keeps giving me this error when trying to connect:\n```\n`const serverSelectionError = new ServerSelectionError();\n                               ^\n\nMongooseServerSelectionError: connect ECONNREFUSED ::1:27017\n    at NativeConnection.Connection.openUri (D:\\TheShed\\MX_\\node_modules\\mongoose\\lib\\connection.js:797:32)\n    at D:\\TheShed\\MX_\\node_modules\\mongoose\\lib\\index.js:330:10\n    at D:\\TheShed\\MX_\\node_modules\\mongoose\\lib\\helpers\\promiseOrCallback.js:32:5\n    at new Promise ()\n    at promiseOrCallback (D:\\TheShed\\MX_\\node_modules\\mongoose\\lib\\helpers\\promiseOrCallback.js:31:10)\n    at Mongoose._promiseOrCallback (D:\\TheShed\\MX_\\node_modules\\mongoose\\lib\\index.js:1151:10)\n    at Mongoose.connect (D:\\TheShed\\MX_\\node_modules\\mongoose\\lib\\index.js:329:20)\n    at module.exports (D:\\TheShed\\MX_\\other\\DB\\mong.js:4:20)\n    at D:\\TheShed\\MX_\\app.js:195:37\n    at Object. (D:\\TheShed\\MX_\\app.js:197:3) {\n  reason: TopologyDescription {\n    type: 'Unknown',\n    servers: Map(1) {\n      'localhost:27017' => ServerDescription {\n        _hostAddress: HostAddress { isIPv6: false, host: 'localhost', port: 27017 },\n        address: 'localhost:27017',\n        type: 'Unknown',\n        hosts: [],\n        passives: [],\n        arbiters: [],\n        tags: {},\n        minWireVersion: 0,\n        maxWireVersion: 0,\n        roundTripTime: -1,\n        lastUpdateTime: 1421094,\n        lastWriteDate: 0,\n        error: MongoNetworkError: connect ECONNREFUSED ::1:27017\n            at connectionFailureError (D:\\TheShed\\MX_\\node_modules\\mongodb\\lib\\cmap\\connect.js:293:20)\n            at Socket. (D:\\TheShed\\MX_\\node_modules\\mongodb\\lib\\cmap\\connect.js:267:22)\n            at Object.onceWrapper (node:events:510:26)\n            at Socket.emit (node:events:390:28)\n            at emitErrorNT (node:internal/streams/destroy:164:8)\n            at emitErrorCloseNT (node:internal/streams/destroy:129:3)\n            at processTicksAndRejections (node:internal/process/task_queues:83:21)\n      }\n    },\n    stale: false,\n    compatible: true,\n    heartbeatFrequencyMS: 10000,\n    localThresholdMS: 15,\n    logicalSessionTimeoutMinutes: undefined\n  }\n}\n`\n```\nThis error wont resolve no matter what I do. The MongoDB server is running, I've checked by doing >show dbs and it lists them perfectly fine. Also, C:/data/db exists and its fine too.\nWhat do I do?\nHere's my connection code:\n```\n`(async () => {\n    await require('./other/DB/mong')();\n    console.log(\"Connected to Database.\");\n})();\n`\n```\nand here's ./other/DB/mong:\n```\n`var mongoose = require(\"mongoose\");\n\nmodule.exports = async () => {\n    await mongoose.connect('mongodb://localhost:27017/MX', {\n        keepAlive: true,\n        useNewUrlParser: true,\n        useUnifiedTopology: true\n    });\n    return mongoose;\n}\n`\n```",
      "solution": "If you are using latest nodejs (v17.x) , update the mongodb url from `localhost` to `127.0.0.1`.",
      "question_score": 5,
      "answer_score": 17,
      "created_at": "2021-10-29T05:01:01",
      "url": "https://stackoverflow.com/questions/69763130/mongooseserverselectionerror-connect-econnrefused-127017-wont-get-fixed"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongoose",
      "question_id": 74529747,
      "title": "MongoDB returns wrong rows with $sort, $skip and $limit",
      "problem": "This is the function I'm using\n```\n`MyModel.aggregate([\n  { $match: query },\n  { $sort: { 'createdAt': -1 } },\n  { $skip: skip },\n  { $limit: 10 }\n  ], { allowDiskUse : true });\n`\n```\n`query` is to filter the rows. `skip` is dynamic value based on pagination (i.e 0, 10, 20 ...). The problem is, the rows for each page in wrong. For instance, I can see a specific row in page 1,2,3,4 at the same time! some rows are missing as well.\nWhy is it happening?",
      "solution": "I think the key to this question is the information that you shared in this comment:\n\n`createdAt` for many rows has the same value. is it a probability for `$sort` not to work properly?\n\nIt's not that the sort doesn't \"work properly\" necessarily, it's that it doesn't have enough information to sort deterministically each time. The value of `123` doesn't come before or after another value of `123` on its own. As @Noel points out, you need to provide an additional field(s) to your sort.\nThis is also covered here in the documentation:\n\nMongoDB does not store documents in a collection in a particular order. When sorting on a field which contains duplicate values, documents containing those values may be returned in any order.\nIf consistent sort order is desired, include at least one field in your sort that contains unique values. The easiest way to guarantee this is to include the `_id` field in your sort query.\n\nThis is because the `_id` field is unique. If you take that approach it would change your sort to:\n```\n`{ $sort: { 'createdAt': -1, _id: 1 } },\n`\n```",
      "question_score": 5,
      "answer_score": 13,
      "created_at": "2022-11-22T09:47:10",
      "url": "https://stackoverflow.com/questions/74529747/mongodb-returns-wrong-rows-with-sort-skip-and-limit"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongoose",
      "question_id": 75655652,
      "title": "Model.find() no longer accepts a callback in Mongoose",
      "problem": "I am currently learning Mongoose from Dr. Angela Yu's Course , however since mongoose has changed the syntax of find and other several functions, it is throwing above error\nHere is the JavaScript Code\n```\n`const express = require(\"express\");\nconst bodyParser = require(\"body-parser\");\nconst mongoose = require(\"mongoose\");\nconst app = express();\nvar items = [];\n\napp.set(\"view engine\", \"ejs\");\napp.use(bodyParser.urlencoded({extended: true}));\nmongoose.connect(\"mongodb://localhost:27017/todoList\", {useNewUrlParser: true});\n\nconst ItemSchema = new mongoose.Schema({\n name: String\n});\n\nconst Item = mongoose.model(\"Item\", ItemSchema); // Items = collection name & ItemScema = Schema\n\n// const  = new  ({\n//      : \n//         })\n\n//Some Default Items for the list\nconst Item1 = new Item({\n    name: \"Welcome To Your To-Do-List!\"\n})\n\nconst Item2 = new Item({\n    name: \"Hit the + button to add a new Item\"\n})\n\nconst Item3 = new Item({\n    name: \"The EJS code is below\n```\n`\n\n    \n    \n    \n    To Do List \n    \n\n    \n  \n    Buy Food\n    Cook Food\n    Eat Food\n      -->\n    \n        \n   \n \n  \n\n  \n    \n    Add\n  \n    \n\n`\n```\nThis is a to-do-List app and hence I need to render all the list items to my To-do-list ,  I have been trying to learn the alternatives but since I am a newbie I am unable to find a solution.",
      "solution": "This should work\n```\n`async function getItems(){\n\n  const Items = await Item.find({});\n  return Items;\n\n}\n\napp.get(\"/\", function (req, res) {\n\n  var today = new Date();\n\n  var options = { weekday: \"long\", day: \"numeric\", year: \"numeric\" , \n  month: \"numeric\" };\n\n  var day = today.toLocaleDateString(\"en-GB\", options);\n\n  getItems().then(function(FoundItems){\n    \n    res.render(\"list\", {kindOfDay: day, newItem:FoundItems});\n\n  });\n\n});\n`\n```",
      "question_score": 5,
      "answer_score": 1,
      "created_at": "2023-03-06T21:43:53",
      "url": "https://stackoverflow.com/questions/75655652/model-find-no-longer-accepts-a-callback-in-mongoose"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongoose",
      "question_id": 68929315,
      "title": "Error when trying to use .find() in Mongoose",
      "problem": "I am creating a database with MongoDB and using the Mongoose ODM. I'm using Node.js. I ran the code without the last block several times and it was fine, but when I wrote the last block in order to use the .find() method, it threw me an odd error.\nThis is the app.js file:\n```\n`//require mongoose\nconst mongoose = require('mongoose');\n//connect to mongoDB database\nmongoose.connect('mongodb://localhost:27017/albumDB', {useNewUrlParser: true, useUnifiedTopology: true});\n\n//CREATE\n\n//create schema (blueprint/structure)\n//of data that we save to the database\nconst albumSchema = new mongoose.Schema ({\n  name: String, //the DB has a variable called name with a value of String\n  author: String,\n  year: Number,\n  genre: String,\n  listened: Boolean,\n  liked: Boolean\n});\n\n//creating the model. parameters: object of collection, schema\nconst Album = mongoose.model('Album', albumSchema);\n//creating the album document\nconst album = new Album({\n  name: 'Insurgentes',\n  author: 'Steven Wilson',\n  year: 2008,\n  genre: 'Prog rock',\n  listened: 1,\n  liked: 1\n});\n//save album inside Album inside albumDB\n//album.save().then(() => console.log('meow'));\n\nconst personSchema = new mongoose.Schema ({\n  name: String,\n  age: Number\n});\n\nconst Person = mongoose.model('Person', personSchema);\n\nconst person = new Person({\n  name: \"John\",\n  age: 37\n});\n\n//person.save();\n\nconst SiameseDream = new Album({\n  name: 'Siamese Dream',\n  author: 'The Smashing Pumpkins',\n  year: 1993,\n  genre: 'Alt rock, Grunge',\n  listened: 1,\n  liked: 1\n});\n\nconst MellonCollie = new Album({\n  name: 'Mellon Collie and the Infinite Sadness',\n  author: 'The Smashing Pumpkins',\n  year: 1995,\n  genre: 'Alt rock, Dream pop',\n  listened: 1,\n  liked: 1\n});\n\nconst Adore = new Album({\n  name: 'Adore',\n  author: 'The Smashing Pumpkins',\n  year: 1998,\n  genre: 'Alt rock, Art rock',\n  listened: 1,\n  liked: 1\n});    \n\n//READ\n\nAlbum.find(function (err, albums){ //1. error 2.what it finds back\n  if (err) {\n    console.log(err);\n  } else {\n  console.log(albums);\n  }\n});\n`\n```\nThis is the error that shows up on my terminal, related to the last block of code:\n```\n`$ node app.js\nTypeError: cursor.toArray is not a function\n    at model.Query. (C:\\Users\\user\\Desktop\\Music\\node_modules\\mongoose\\lib\\query.js:2151:19)    \n    at model.Query._wrappedThunk [as _find] (C:\\Users\\user\\Desktop\\Music\\node_modules\\mongoose\\lib\\helpers\\query\\wrapThunk.js:27:8)\n    at C:\\Users\\user\\Desktop\\Music\\node_modules\\kareem\\index.js:370:33\n    at processTicksAndRejections (internal/process/task_queues.js:75:11)\n(node:11536) UnhandledPromiseRejectionWarning: MongoInvalidArgumentError: Method \"collection.find()\" accepts at most two arguments\n    at Collection.find (C:\\Users\\user\\Desktop\\Music\\node_modules\\mongodb\\lib\\collection.js:238:19)\n    at NativeCollection. [as find] (C:\\Users\\user\\Desktop\\Music\\node_modules\\mongoose\\lib\\drivers\\node-mongodb-native\\collection.js:191:33)\n    at NativeCollection.Collection.doQueue (C:\\Users\\user\\Desktop\\Music\\node_modules\\mongoose\\lib\\collection.js:135:23)\n    at C:\\Users\\user\\Desktop\\Music\\node_modules\\mongoose\\lib\\collection.js:82:24\n    at processTicksAndRejections (internal/process/task_queues.js:75:11)\n(Use `node --trace-warnings ...` to show where the warning was created)\n(node:11536) UnhandledPromiseRejectionWarning: Unhandled promise rejection. This error originated either by throwing inside of an async function without a catch block, or by rejecting a promise which was not handled with .catch(). To terminate the node process on unhandled promise rejection, use the CLI flag `--unhandled-rejections=strict` (see https://nodejs.org/api/cli.html#cli_unhandled_rejections_mode). (rejection id: 1)(node:11536) [DEP0018] DeprecationWarning: Unhandled promise rejections are deprecated. In the future, promise rejections that are not handled will terminate the Node.js process with a non-zero exit code.\n`\n```",
      "solution": "This issue is facing mongoose version 6.0 So you just have to downgrade the mongoose version. Just run `npm uninstall mongoose` to uninstall the current mongoose version then run `npm i mongoose@5.13.8`, this will install the version that will fix your problem. Just check this link https://www.zhishibo.com/articles/132795.html",
      "question_score": 5,
      "answer_score": 4,
      "created_at": "2021-08-25T22:14:06",
      "url": "https://stackoverflow.com/questions/68929315/error-when-trying-to-use-find-in-mongoose"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongoose",
      "question_id": 67287143,
      "title": "The $divide accumulator is a unary operator",
      "problem": "I created a request to Mongo DB:\n```\n`{\n    $project:\n      {\n        difference:\n          {\n            $subtract: ['$' + endDate, '$' + startDate]\n          }\n      }\n  },\n  {\n    $match:\n      {\n        difference:\n          {\n            $gte: 0\n          }\n      }\n  },\n  {\n    $group:\n      {\n        _id: null,\n        avgTime:\n          {\n            $divide:\n              [\n                {\n                  $avg: \"$difference\"\n                }, 60000 // in minutes\n              ]\n          }\n      }\n  }\n`\n```\nI try to:\n\nCalculate difference between end date and start date and take it\nwhere this `difference` > 0\nCalculate average value\nDevide it on 60000 (to get minutes)\n\nBut I got the error: The `$divide` accumulator is a unary operator\nHow can I fix this request?",
      "solution": "But I got the error: The $divide accumulator is a unary operator\n\nThe `` operator must be one of the following accumulator operators: accumulator-operator, and `$divide` operator is not one of them,\nThe `$avg` operator must be in root if you want to average,\nCorrect your syntax in `$group` stage and add new stage to get `$divide`\n```\n`  {\n    $group: {\n      _id: null,\n      avgTime: { $avg: \"$difference\" }\n    }\n  },\n  {\n    $project: {\n      avgTime: { $divide: [\"$avgTime\", 60000] }\n    }\n  }\n`\n```",
      "question_score": 5,
      "answer_score": 6,
      "created_at": "2021-04-27T18:35:32",
      "url": "https://stackoverflow.com/questions/67287143/the-divide-accumulator-is-a-unary-operator"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongoose",
      "question_id": 75689772,
      "title": "error TS2339: Property &#39;remove&#39; does not exist on type &#39;Document&lt;unknown,",
      "problem": "I'm getting this error on my application\nruntime Error :\nTSError: \u2a2f Unable to compile TypeScript:\nsrc/controllers/notes.ts:134:20 - error TS2339: Property 'remove' does not exist on type 'Document & Omit'.\n134         await note.remove()\nhighlighted Error :\nany\nProperty 'remove' does not exist on type 'Document & Omit'.ts(2339)\nControllers/notes\n```\n`import NoteModel from \"../models/note\";\nimport {RequestHandler} from 'express';\nimport createHttpErrors from \"http-errors\";\nimport mongoose from \"mongoose\";\n\nexport const deleteNote: RequestHandler = async (req, res, next) => {\n    const noteId = req.params.noteId;\n\n    try{\n\n        if(!mongoose.isValidObjectId(noteId)){\n            throw createHttpErrors(400, \"invalid Note ID\")\n        }\n\n        const note = await NoteModel.findById(noteId).exec();\n\n        if(!note){\n            throw createHttpErrors(404, \"Note not found\")\n        }\n\n        await note.remove();\n\n        res.sendStatus(204)\n    }\n    catch(error){\n       next(error)\n    }\n}\n`\n```\nmodel/notes\n```\n`import { InferSchemaType, model, Schema } from \"mongoose\";\n\nconst noteSchema = new Schema({\n    title: { type: String, required: true},\n    text: { type: String},\n}, {timestamps: true})\n\ntype Note = InferSchemaType\n\nexport default model(\"Note\",noteSchema)\n\n`\n```\nrouter/notes\n```\n`import express from 'express';\nimport * as NoteController from '../controllers/notes'\nconst router = express.Router()\n\nrouter.delete('/:noteId', NoteController.deleteNote);\n`\n```",
      "solution": "Guessing you are following the same tutorial.\nIf you want to keep:\n```\n`if (!note) {\n  throw createHttpErrors(404, \"Note not found\")\n}\n`\n```\nThen use\n```\n`await note.deleteOne()\n`\n```\n`remove()` can no longer be used (src: How do I remove documents using Node.js Mongoose?)",
      "question_score": 5,
      "answer_score": 5,
      "created_at": "2023-03-09T21:35:02",
      "url": "https://stackoverflow.com/questions/75689772/error-ts2339-property-remove-does-not-exist-on-type-documentunknown"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongoose",
      "question_id": 72616813,
      "title": "Merging arrays into one array in same field MongoDB",
      "problem": "I have a question on merging arrays into one array in same field, but they are in different objects. I hope you will understand what I wanted after looking at the code below.\n```\n` db.addresses.find().pretty()\n{\n        \"_id\" : ObjectId(\"62a72be40c7db0af0b79d721\"),\n        \"rollingStockNumber\" : \"922698325786980137200129101715063039706000000\",\n        \"addresses\" : [\n                \"Raximova\",\n                \"Raximova\",\n                \"Raximova\",\n                \"Nazarbek\",\n                \"Oxunboboyeva\",\n        ],\n        \"__v\" : 0\n}\n{\n        \"_id\" : ObjectId(\"62a8727978c18925711d40f2\"),\n        \"rollingStockNumber\" : \"922698012567076507200129101700057022060000000\",\n        \"addresses\" : [\n                \"Toshkent\",\n                \"Chuqursoy\",\n                \"Chuqursoy\",\n                \"Chuqursoy\",\n                \"Chuqursoy\",\n                \"Chuqursoy\",\n                \"Chuqursoy\",\n                \"Toshkent\",\n        ],\n        \"__v\" : 0\n}\n{\n        \"_id\" : ObjectId(\"62a878d778c18925711d40f7\"),\n        \"rollingStockNumber\" : \"922720020326980977200102111555058048630000000\",\n        \"addresses\" : [\n                \"Oxangaron\",\n                \"Oxangaron\",\n                \"Oxangaron\",\n                \"Oxangaron\",\n                \"Oxangaron\",\n                \"Oxangaron\",\n                \"Jaloir\"\n        ],\n        \"__v\" : 0\n}\n`\n```\nI wanted to merge \"addresses\" arrays in one array called \"allAddresses\", used $group, $concat, but failed...\n```\n` \"allAddresses\" : [\n                    \"Raximova\",\n                    \"Raximova\",\n                    \"Raximova\",\n                    \"Nazarbek\",\n                    \"Oxunboboyeva\",\n                    \"Toshkent\",\n                    \"Chuqursoy\",\n                    \"Chuqursoy\",\n                    \"Chuqursoy\",\n                    \"Chuqursoy\",\n                    \"Chuqursoy\",\n                    \"Chuqursoy\",\n                    \"Toshkent\",\n                    \"Oxangaron\",\n                    \"Oxangaron\",\n                    \"Oxangaron\",\n                    \"Oxangaron\",\n                    \"Oxangaron\",\n                    \"Oxangaron\",\n                    \"Jaloir\"\n            ]\n`\n```\nHow can I achieve that? Thanks in advance.",
      "solution": "`$group` - Group by `null` and add `addresses` into `addresses` array. It returns a nested array.\n\n`$project` - With `$reduce` to flatten nested array.\n\n```\n`db.collection.aggregate([\n  {\n    $group: {\n      _id: null,\n      addresses: {\n        $push: \"$addresses\"\n      }\n    }\n  },\n  {\n    $project: {\n      allAddresses: {\n        $reduce: {\n          input: \"$addresses\",\n          initialValue: [],\n          in: {\n            \"$concatArrays\": [\n              \"$$value\",\n              \"$$this\"\n            ]\n          }\n        }\n      }\n    }\n  }\n])\n`\n```\nSample Mongo Playground",
      "question_score": 5,
      "answer_score": 4,
      "created_at": "2022-06-14T14:14:46",
      "url": "https://stackoverflow.com/questions/72616813/merging-arrays-into-one-array-in-same-field-mongodb"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongoose",
      "question_id": 70161241,
      "title": "Mongoose + TypeScript: Conditionally find() documents on model throws error: Union type has signatures, but none are compatible with each other",
      "problem": "I'm trying to conditionally access a mongoose model in my TypeScript code. Anyone knows how to fix this TypeScript error?\n`Each member of the union type has signatures, but none of those signatures are compatible with each other`\n`.find()` on the conditional model throws the error.\n```\n`\ninterface PictureDocument extends Document {\n    paint: string;\n}\ninterface VideoDocument extends Document {\n    duration: number;\n}\n\nconst PictureSchema = new Schema({ paint: String });\nconst VideoSchema = new Schema({ duration: Number });\n\nconst PictureModel = model(\"video\", PictureSchema);\nconst VideoModel = model(\"video\", VideoSchema);\n\nconst MY_CONDITION = \"picture\";\n\nconst getDocuments = async (condition: \"picture\" | \"video\") => {\n    const model = condition === \"picture\" ? PictureModel : VideoModel; //  void) | undefined): DocumentQuery; (conditions: FilterQuery, callback?: ((err: any, res: VideoDocument[]) => void) | undefined): DocumentQuery; (conditions: FilterQuery, projection?: any, callback?: ((err: any, res:...' has signatures, but none of those signatures are compatible with each other.\n`\n```",
      "solution": "Okay actually it was quite simple and it is a common solution to union type problems in TypeScript I think\n```\n`const model: Model = condition === \"picture\" ? PictureModel : VideoModel;\n`\n```\nBy adding the type to the variable declaration, the union type problem is solved.",
      "question_score": 5,
      "answer_score": 5,
      "created_at": "2021-11-29T21:58:03",
      "url": "https://stackoverflow.com/questions/70161241/mongoose-typescript-conditionally-find-documents-on-model-throws-error-uni"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongoose",
      "question_id": 70065794,
      "title": "how to create one-to-one relation in mongoDb",
      "problem": "I read the official documentation but still don't get it https://docs.mongodb.com/manual/tutorial/model-embedded-one-to-one-relationships-between-documents/\nExplain me pls, how to create in my example.\nI have nest.js app with mongoose.\nTwo schemas (2 tables in db):\n1)\n```\n`export type UserModelDocument = UserModel & Document;\n\n@Schema()\nexport class UserModel {\n  @Prop()\n  email: string;\n\n  @Prop({ type: MongooseSchema.Types.ObjectId, ref: 'InviteModel' })\n  invite: InviteModel;\n}\n\nexport const UserSchema = SchemaFactory.createForClass(UserModel);\n`\n```\n\n```\n`@Schema()\nexport class InviteModel {\n\n  @Prop()\n  status: string;\n}\n\nexport const InviteSchema = SchemaFactory.createForClass(InviteModel);\n`\n```\nAnd here, in my service i create a user:\n```\n`async create(createUserDto: CreateUserDto) {\n    const userExists = await this.userModel.findOne({ email: createUserDto.email });\n    if (userExists) {\n      this.logger.error('User already exists');\n      throw new NotAcceptableException('User already exists');\n    }\n    const createdUser = await new this.userModel(createUserDto).save();\n//need some operations to save default invite too in the same user\n    return toCreateUserDto(createdUser);\n  }\n`\n```\nHow to add a relation one-to-one when registering for a new user, so that the object of the created user is also added invite object.\nExample:\n```\n`{userEmail: \"email@email.com\",\n invite: {\n  status: 'not_sent',\n }\n}\n`\n```\n\u0421an i do it in one request? or I need to save in different requests?\n```\n`const createdUser = await new this.userModel(createUserDto).save();\nconst createdInvite = await new this.inviteModel(createInviteDto).save(); ?\n`\n```",
      "solution": "It is well explained in the documentation , if you have less frequently searched data you can offload it to second collection to reduce the size of the main document and improve read performance , in case you need the less frequent data you will make a second request to the second collection.\nIn general if your document in first collection is not so big and no impact on read performance best is to embed the field in the first collection and avoid too much collections and fetch everything in single call.\n\nOne-to-one example:\n\ncollection 1:\n```\n` { _id:\"useremail\" , frequentData: \"the data1\"  }\n`\n```\ncollection 2:\n```\n` { _id:\"useremail\" , lessFrequentData:\"the data2\"  }\n`\n```\n\nEmbedding option:\n\ncollection 1:\n```\n` { _id:\"usermail\" , frequentData: \"the Data1\" , lessFrequentData:\"the Data2\" }\n`\n```\nP.S.\nmy personal observation:\nmongoDB wiredTiger storage engine is splitting and storing data to 32KB blocks on storage so documents below that size are considered relatively small and embedding seems to be the best option if there is no other specific requirements  ...\nDocument size is important , but in the document model design the prefered is the denormalized data  , correct indexes creation and utilisation will help more with the performance improvement.",
      "question_score": 5,
      "answer_score": 5,
      "created_at": "2021-11-22T13:31:42",
      "url": "https://stackoverflow.com/questions/70065794/how-to-create-one-to-one-relation-in-mongodb"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongoose",
      "question_id": 67544458,
      "title": "MongoDB aggregation $lookup to parent array field with pipeline syntax and _id as string",
      "problem": "I have the following collections in my MongoDB (v.4.4):\n`guilds`:\n`{\n _id: String,\n members: [{ _id String, rank: number },{ _id String, rank: number }...]\n}\n`\nand `characters`:\n`{\n  _id: String,\n  many_other: 'fields'\n}\n`\nAnd I'd like to join with the `$lookup` pipeline syntax `guild.members \n\nI don't use Mongo build-in `OjbectIds` as `_id`, I overwrite it with strings in both collections. As I heard, there is a bit different behaviour with `$lookup` with strings as `_id` in pipelines. So make sure, that you knew about it, before answering.\n\nThe expected result that I want is simple, I'd like to save rank from the original document, and also add any other field from `$lookup` it:\n`{\n  _id: String // (guild)\n  members: [\n    {\n      _id: String, // (characters)\n      rank: number,\n      many_other: '...fields from characters'\n    },\n    {\n      _id: String, // (characters)\n      rank: number,\n      many_other: '...fields from characters'\n    }, ...\n]\n`\nMongo Playground example: avaliable\nWhat have I tried:\nVarious queries, like:\n`      {\n        $lookup: {\n          from: \"characters\",\n          pipeline: [\n            {\n              $match: {\n                $expr: { $eq: [ { $toString :\"$members._id\" }, { $toString : \"$$character_id\" } ] }\n              }\n            }\n          ],\n          as: \"guild_members\"\n        },\n      }\n`\nWith converting ID's to string, with `let` stages variables, and using `$map` operator. But I still far away from the required result.\n\nThere is also another problem, which is some relevance to the question, but not related to queries itself\n\nAs you may see, the `characters` collection has `many other fields`. Some of them are pretty heavy, (because guilds can have up to 500 members) which making the result document  >16 MB (MongoDB threshold limit) which gives an error. Since, as far as I know, we could not pick fields from `joined` documents, it will be much better to exclude it right after the `$lookup` stage or something like that. Any advice about it will be pretty welcome.",
      "solution": "Lookup with pipeline is not required, when you pass `members._id` as localField,\n\n`$lookup` with characters and pass `members._id` as localField\n`$map` to iterate loop of `members` array\n`$filter` to iterate loop of `members_guid` and get matching member\n`$arrayElemAt` to get first matching element from above filter\n`$mergeObjects` to merge current fields with above filtered object of member\n\n```\n`db.guilds.aggregate([\n  {\n    $lookup: {\n      from: \"characters\",\n      localField: \"members._id\",\n      foreignField: \"_id\",\n      as: \"members_guid\"\n    }\n  },\n  {\n    $project: {\n      members: {\n        $map: {\n          input: \"$members\",\n          as: \"m\",\n          in: {\n            $mergeObjects: [\n              \"$$m\",\n              {\n                $arrayElemAt: [\n                  {\n                    $filter: {\n                      input: \"$members_guid\",\n                      cond: { $eq: [\"$$this._id\", \"$$m._id\"] }\n                    }\n                  },\n                  0\n                ]\n              }\n            ]\n          }\n        }\n      }\n    }\n  }\n])\n`\n```\nPlayground",
      "question_score": 5,
      "answer_score": 3,
      "created_at": "2021-05-15T09:52:24",
      "url": "https://stackoverflow.com/questions/67544458/mongodb-aggregation-lookup-to-parent-array-field-with-pipeline-syntax-and-id-a"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongoose",
      "question_id": 69067799,
      "title": "TextEncoder is not defined. Connecting to MongoDB on wsl2 with nodejs",
      "problem": "I am trying to get a simple MERN app working on an Ubuntu wsl2 instance. I am following this guide. This is the code that I have in `server.js` (it is slightly different to the code in the guide as body-parser is deprecated. Using the advice from this post I've changed those methods).\n`const express = require(\"express\");\nconst mongoose = require(\"mongoose\");\n\n// Setup express app\nconst app = express();\n\napp.use(\n    express.urlencoded({\n        extended: false\n    })\n);\n\napp.use(express.json());\n\n// Configure Mongo\nconst db = \"mongodb://localhost/313-demo-mern-db\";\n\n// Connect to Mongo with Mongoose\nmongoose\n    .connect(\n        db,\n        { useNewUrlParser: true }\n    )\n    .then(() => console.log(\"Mongo connected\"))\n    .catch(err => console.log(err));\n\n// Specify the Port where the backend server can be accessed and start listening on that port\nconst port = process.env.PORT || 5000;\napp.listen(port, () => console.log(`Server up and running on port ${port}.`));\n`\nI have a MongoDB database running where it is currently \"Waiting for connections on port 27017\". I run the command `node server.js` and get the error.\n```\n`/home/NAME/learnreact/MERN-demo/node_modules/whatwg-url/dist/encoding.js:2\nconst utf8Encoder = new TextEncoder();\n                    ^\n\nReferenceError: TextEncoder is not defined\n    at Object. (/home/NAME/learnreact/MERN-demo/node_modules/whatwg-url/dist/encoding.js:2:21)\n    at Module._compile (internal/modules/cjs/loader.js:778:30)\n    at Object.Module._extensions..js (internal/modules/cjs/loader.js:789:10)\n    at Module.load (internal/modules/cjs/loader.js:653:32)\n    at tryModuleLoad (internal/modules/cjs/loader.js:593:12)\n    at Function.Module._load (internal/modules/cjs/loader.js:585:3)\n    at Module.require (internal/modules/cjs/loader.js:692:17)\n    at require (internal/modules/cjs/helpers.js:25:18)\n    at Object. (/home/NAME/learnreact/MERN-demo/node_modules/whatwg-url/dist/url-state-machine.js:5:34)\n    at Module._compile (internal/modules/cjs/loader.js:778:30)\n`\n```",
      "solution": "I was having the same error today. Updating the node to latest version fixed the issue for me.\n\nYou may refer https://askubuntu.com/a/480642/1267099 for updating node.\n\nedit: update to the latest version (not the current stable)",
      "question_score": 5,
      "answer_score": 4,
      "created_at": "2021-09-06T01:04:25",
      "url": "https://stackoverflow.com/questions/69067799/textencoder-is-not-defined-connecting-to-mongodb-on-wsl2-with-nodejs"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongoose",
      "question_id": 68306685,
      "title": "MongoDB custom sort order for a query with pagination",
      "problem": "I have some documents in a MongoDB collection with this schema:\n```\n`{\n    \"_id\": {\n        \"$oid\": \"60c1e8e318afd80016ce58b1\"\n    },\n    \"searchPriority\": 1,\n    \"isLive\": false,\n    \"vehicleCondition\": \"USED\",\n    \"vehicleDetails\": {\n        \"city\": \"Delhi\"\n    }\n},\n{\n    \"_id\": {\n        \"$oid\": \"60c1f2f418afd80016ce58b5\"\n    },\n    \"searchPriority\": 2,\n    \"isLive\": false,\n    \"vehicleCondition\": \"USED\",\n    \"vehicleDetails\": {\n        \"city\": \"Delhi\"\n    }\n},\n{\n    \"_id\": {\n        \"$oid\": \"60cb429eadd33c00139d2be7\"\n    },\n    \"searchPriority\": 1,\n    \"isLive\": false,\n    \"vehicleCondition\": \"USED\",\n    \"vehicleDetails\": {\n        \"city\": \"Gurugram\"\n    }\n},\n{\n    \"_id\": {\n        \"$oid\": \"60c21be618afd80016ce5905\"\n    },\n    \"searchPriority\": 2,\n    \"isLive\": false,\n    \"vehicleCondition\": \"USED\",\n    \"vehicleDetails\": {\n        \"city\": \"New Delhi\"\n    }\n},\n{\n    \"_id\": {\n        \"$oid\": \"60e306d29e452d00134b978f\"\n    },\n    \"searchPriority\": 3,\n    \"isLive\": false,\n    \"vehicleCondition\": \"USED\",\n    \"vehicleDetails\": {\n        \"city\": \"New Delhi\"\n    }\n}\n`\n```\n`vehicleCondition` can be `NEW` or `USED`, `isLive` can be `true` or `false` and `searchPriority` will be an integer between 1 to 3. (lower number means it should be higher in search result)\nHere, except `_id` none of the other fields are unique. I have created a compound index on  `isLive`, `vehicleDetails.city` and `searchPriority`.\nIn my application I will perform some queries of this form:\n\nfind all cars where `isLive` is `true`, `vehicleDetails.city` is\neither `Delhi` or `New Delhi` or `Gurugram` and `vehicleCondition` is\n`USED` (or `NEW`).\n\nFor this, I can do a find query like this:\n```\n`db.collection.find({\"isLive\": true, \"vehicleDetails.city\": { $in: [ \"Gurugram\", \"Delhi\", \"New Delhi\" ] }, \"vehicleCondition\": \"USED\" }, {})\n`\n```\nI want the results of this query sorted in this order:\n\nAll cars belonging to the 1st city inside `$in` arrray in the find query, having lowest priority\nAll cars belonging to the 1st city inside `$in` arrray in the find query, having  2nd lowest priority\nAll cars belonging to the 1st city inside `$in` arrray in the find\nquery, having 3rd lowest priority\nAll cars belonging to the 2nd city inside `$in` arrray in the find query, having lowest priority\nAll cars belonging to the 2nd city inside `$in` arrray in the find query, having 2nd lowest priority\nAll cars belonging to the 2nd city inside `$in` arrray in the find query, having 3rd lowest priority\nAll cars belonging to the 3rd city inside `$in` arrray in the find query, having lowest priority\nAll cars belonging to the 3rd city inside `$in` arrray in the find query, having 2nd lowest priority\nAll cars belonging to the 3rd city inside `$in` arrray in the find query, having 3rd lowest priority\n\nHow can I do this? Since the number of documents returned by this query could be very large, I will be using pagination to limit the number of returned documents. Will this extra requirement have any effect on the possible solution for this problem?",
      "solution": "So I've read the other answer ( which gives a technical solution ) however based on your comments and request it is not suitable.\nSo firstly using `aggregate` here while technically solves the problem has some issues.\nAs you mentioned the query can have a high amount of documents matching, the aggregation pipeline unlike the `find` method does indeed load ALL of them into memory, this will inventively cause performance issue's, I also saw you mention something about not having an index. this will cause a \"collection\" scan for every single API call.\nWhat I recommend you do is:\n\nFirst you absolutely must build a compound index on `isLive, vehicleCondition, \"vehicleDetails.city\"` in case you don't have one. this is simply a must for high scale usage.\n\nNow that we got that out of the way I recommend you split your call into into several pieces, I'm going to paste some puesdo code that might look a little all over the places but I do believe this is the best approach you can achieve using Mongo as each of these queries is suppose to be efficient by using the previously built index.\n\nI will briefly explain the methodology, We want to be able to query each city independently of the other cities so we can use the \"custom sort\" function without needing to load all the matches into memory.\nTo do so we need to know how much each city needs to \"skip\" and \"limit\", as city #2 (Delhi) limit for example will be ( limit - city#1 (Gurugram) matches ).\nSo here is the pseudo code, I left it simple on purpose so it will be understandable. I will however add some ideas at the end for some basic improvements.\n```\n`let limit = 10; // determined by req?\nconst skip = 0; // determined by req?\nconst cities = ['Gurugram', 'Delhi', 'New Delhi'];\n\n// we need this to resolve the proper skip / limit. the last city is not relevant.\nconst countPromises = [];\nfor (let i = 0; i = countPromises[i]) {\n        continue;\n    }\n    const cityLimit = limit - finalResults.length;\n    if (cityLimit Ok so possible improvements you can make:\n\nIf the database doesn't get updated too often / you don't care about extreme accurateness you can pre-calculate each city match count in advance ( once a day? once a week? depending on your app ). This will speed up the `countDocuments` part which is needed to determine the `skip` and `limit` of each of the cities.\nThe last `for` loop can be a `Promise.all` similar to the count to speed up results. Again if the number of cities if never too too high this could be a good solution.\nLastly it wasn't clear if a single vehicle can be related to more than 1 city, but if it is the case then you need to add an exclude condition on documents that are already matched.",
      "question_score": 5,
      "answer_score": 1,
      "created_at": "2021-07-08T20:05:13",
      "url": "https://stackoverflow.com/questions/68306685/mongodb-custom-sort-order-for-a-query-with-pagination"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongoose",
      "question_id": 65861573,
      "title": "NestJS-Mongoose: cannot access fullDocument on ChangeEvent&lt;any&gt;",
      "problem": "I have the following basic snippet of code and my aim is to get the fullDocument property from the `obj: ChangeEvent` however I am not able to access this property (`Property 'fullDocument' does not exist on type 'ChangeEvent'`). The only properties I can access are _id, clusterTime and operationType. Is there something I am missing or should I just query the fullDocument non-directly (`obj['fullDocument']`)?\n```\n`const changeStream = this.model.watch([], { fullDocument: 'updateLookup' })\n      .on('change', obj => {\n        console.log(obj.fullDocument);\n      });\n`\n```",
      "solution": "The problem you're having (TypeScript complaining about the missing property) is due to unspecialized return type generic `mongodb.ChangeStream` in mongoose's `Model.watch` method (in both `@types/mongoose` and official `mongoose` types, I'm assuming you are using mongoose 5.x branch).\nFor example `@types/mongodb` package declares/specializes the return type on the `watch` method correctly.\nThe real solution would be to create a Pull Request fixing the return type of the `watch` return type in the aforementioned package repositories, and/or report the issue to maintainers. For a quick 'fix' you could resort to type casting like this (for the cleanest `changeStream` shape the `MySchema` should be the class you've annotated with `@Schema`):\n`const changeStream = this.model\n  .watch([], { fullDocument: 'updateLookup' }) as ChangeStream; //  {\n  console.log(obj.fullDocument);\n});\n`\nAlso be aware that MongoDB Change Stream requires Replica Set or Sharded Cluster.",
      "question_score": 5,
      "answer_score": 3,
      "created_at": "2021-01-23T17:26:53",
      "url": "https://stackoverflow.com/questions/65861573/nestjs-mongoose-cannot-access-fulldocument-on-changeeventany"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongoose",
      "question_id": 69124591,
      "title": "MongoDB delete embedded documents through array of Ids",
      "problem": "I am working on a `Node.js` application that is using a `MongoDB` database with Mongoose. I've been stuck in this thing and didn't come up with the right query.\nProblem:\nThere is a collection named `chats` which contain embedded documents `(rooms)` as an array of objects. I want to delete these embedded documents `(rooms)` through `Ids` which are in the array.\n```\n`  {\n    \"_id\": \"ObjectId(6138e2b55c175846ec1e38c5)\",\n    \"type\": \"bot\",\n    \"rooms\": [\n      {\n        \"_id\": \"ObjectId(6138e2b55c145846ec1e38c5)\",\n        \"genre\": \"action\"\n      },\n      {\n        \"_id\": \"ObjectId(6138e2b545c145846ec1e38c5)\",\n        \"genre\": \"adventure\"\n      }\n    ]\n  },\n  {\n    \"_id\": \"ObjectId(6138e2b55c1765846ec1e38c5)\",\n    \"type\": \"person\",\n    \"rooms\": [\n      {\n        \"_id\": \"ObjectId(6138e2565c145846ec1e38c5)\",\n        \"genre\": \"food\"\n      },\n      {\n        \"_id\": \"ObjectId(6138e2b5645c145846ec1e38c5)\",\n        \"genre\": \"sport\"\n      }\n    ]\n  },\n  {\n    \"_id\": \"ObjectId(6138e2b55c1765846ec1e38c5)\",\n    \"type\": \"duo\",\n    \"rooms\": [\n      {\n        \"_id\": \"ObjectId(6138e21c145846ec1e38c5)\",\n        \"genre\": \"travel\"\n      },\n      {\n        \"_id\": \"ObjectId(6138e35645c145846ec1e38c5)\",\n        \"genre\": \"news\"\n      }\n    ]\n  }\n`\n```\nI am converting my array of `ids` into MongoDB `ObjectId` so I can use these ids as match criteria.\n```\n`const idsRoom = [\n  '6138e21c145846ec1e38c5',\n  '6138e2565c145846ec1e38c5',\n  '6138e2b545c145846ec1e38c5',\n];\n\nconst objectIdArray = idsRoom.map((s) => mongoose.Types.ObjectId(s));\n`\n```\nand using this query for the chat collection. But it is deleting the whole document and I only want to delete the `rooms` embedded document because the ids array is only for the embedded documents.\n```\n`Chat.deleteMany({ 'rooms._id': objectIdArray }, function (err) {\n  console.log('Delete successfully')\n})\n`\n```\nI really appreciate your help on this issue.",
      "solution": "You have to use $pull operator in a `update` query like this:\nThis query look for documents where exists the `_id` into `rooms` array and use `$pull` to remove the object from the array.\n```\n`yourModel.updateMany({\n  \"rooms._id\": {\n    \"$in\": [\n      \"6138e21c145846ec1e38c5\",\n      \"6138e2565c145846ec1e38c5\",\n      \"6138e2b545c145846ec1e38c5\"\n    ]\n  }\n},\n{\n  \"$pull\": {\n    \"rooms\": {\n      \"_id\": {\n        \"$in\": [\n          \"6138e21c145846ec1e38c5\",\n          \"6138e2565c145846ec1e38c5\",\n          \"6138e2b545c145846ec1e38c5\"\n        ]\n      }\n    }\n  }\n})\n`\n```\nExample here.\nAlso you can run your query without the `query` parameter (in update queries the first object is the `query`) like this and result is the same. But is better to indicate mongo the documents using this first object.",
      "question_score": 5,
      "answer_score": 2,
      "created_at": "2021-09-09T22:51:46",
      "url": "https://stackoverflow.com/questions/69124591/mongodb-delete-embedded-documents-through-array-of-ids"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongoose",
      "question_id": 67747533,
      "title": "how to connect to mongodb server via ssh tunnel with Proxy Jump (Bastion Host)",
      "problem": "I have an ssh config file like this.\nI have a proxy jump to host1 from test2.\n```\n`Host host1\n  Hostname xxxxxx.us-east-1.elb.amazonaws.com\n  Port 2222\n  User xxxx\n  IdentityFile ~/.ssh/cert\n  StrictHostKeyChecking no\n  UserKnownHostsFile /dev/null\n  KeepAlive yes\n  ServerAliveInterval 30\n  ServerAliveCountMax 30\n\nHost test2\n  Hostname xx.xxx.xx.xxx\n  ProxyCommand  ssh.exe host1  -q -W %h:%p host1\n  User ubuntu\n  IdentityFile ~/.ssh/cert\n  KeepAlive yes\n  StrictHostKeyChecking no\n  UserKnownHostsFile /dev/null\n  ServerAliveInterval 30\n  ServerAliveCountMax 30\n`\n```\nMy mongo db host : xxx-nonprod.cluster-xx.us-east-1.docdb.amazonaws.com\nI have to Use SSH Tunneling to Mongodb using Host test2, But it uses Proxy Jump using ProxyCommand\nI want to connect to mongodb using SSH Tunneling with a Mongo DB Compass and also with node js mongoose.\nHow Can I connect using the Mongo DB Compass?\n\nHere I don't have an option to enter ProxyCommand details.\nHow Can I connect using node js?\nI am using tunnel-ssh,  I have a reference code ,\n```\n`var config = {\n    username:'ubuntu',\n    host:'xx.xxx.xx.xxx',\n    agent : process.env.SSH_AUTH_SOCK,\n    privateKey:require('fs').readFileSync('~/.ssh/cert'),\n    port:22,\n    dstPort:27017\n};\n\nvar server = tunnel(config, function (error, server) {\n  \n});\n`\n```\nHere as well how can I enter ProxyCommand details here? Or please suggest any node js package which solves this problem.",
      "solution": "I am able to connect to the database now.\n\nMake a tunnel through Bastion to the Database from the terminal.\nssh -L 27017:{mongodb-host}:27017 host1\n\nI am able to connect to database via the tunnel from localhost in another terminal.\nmongo --host 127.0.0.1:27017  --username {username} --password {password}\n\nSo I am able to connect through mongoose too using connection string.\nmongodb://dbadmin:{username}:{password}@localhost:27017",
      "question_score": 5,
      "answer_score": 2,
      "created_at": "2021-05-29T05:42:24",
      "url": "https://stackoverflow.com/questions/67747533/how-to-connect-to-mongodb-server-via-ssh-tunnel-with-proxy-jump-bastion-host"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongoose",
      "question_id": 75697312,
      "title": "Import Mongoose lib in api directory in Next js 13.2 app directory gives error",
      "problem": "In hello.js\n```\n`import connectMongo from '../../../util/connectDB';\nimport UserModel from '../../../models/UserModel';\nimport { NextResponse } from 'next/server'\n\nexport async function GET(request) {\n  return NextResponse.json({hello: 'Hello, Next.js!'})\n}\n\nexport async function POST(request) {\n  const data = await request.json();\n  console.log(data);\n  return NextResponse.json(data)\n}\n`\n```\nIn util/connextDB\n```\n`import mongoose from 'mongoose';\n\nconst connectMongo = () => {\n    try {\n        const conn = mongoose.connect(process.env.MONGODB_URI, {\n        useNewUrlParser: true,\n        useUnifiedTopology: true,\n        useCreateIndex: true,\n        });\n    \n        console.log(`MongoDB Connected: ${conn.connection.host}`);\n    } catch (error) {\n        console.error(`Error: ${error.message}`);\n        process.exit(1);\n    }\n};\n\nexport default  connectMongo;\n`\n```\nAnd the error shows\nerror - node_modules/mongoose/lib/types/objectid.js (21:31) @ prototype\nerror - TypeError: Cannot read properties of undefined (reading 'prototype')\nat eval (webpack-internal:///(sc_server)/./node_modules/mongoose/lib/types/objectid.js:15:36)\nat Object.(sc_server)/./node_modules/mongoose/lib/types/objectid.js (/Users/avinashsethu/Desktop/Development/ekali-lms/.next/server/app/api/hello/route.js:2728:1)\nat webpack_require (/Users/avinashsethu/Desktop/Development/ekali-lms/.next/server/webpack-runtime.js:33:43)\nat eval (webpack-internal:///(sc_server)/./node_modules/mongoose/lib/drivers/node-mongodb-native/collection.js:7:18)\nat Object.(sc_server)/./node_modules/mongoose/lib/drivers/node-mongodb-native/collection.js (/Users/avinashsethu/Desktop/Development/ekali-lms/.next/server/app/api/hello/route.js:451:1)\nat webpack_require (/Users/avinashsethu/Desktop/Development/ekali-lms/.next/server/webpack-runtime.js:33:43)\nat eval (webpack-internal:///(sc_server)/./node_modules/mongoose/lib/drivers/node-mongodb-native/index.js:4:22)\nat Object.(sc_server)/./node_modules/mongoose/lib/drivers/node-mongodb-native/index.js (/Users/avinashsethu/Desktop/Development/ekali-lms/.next/server/app/api/hello/route.js:473:1)\nat webpack_require (/Users/avinashsethu/Desktop/Development/ekali-lms/.next/server/webpack-runtime.js:33:43)\nat eval (webpack-internal:///(sc_server)/./node_modules/mongoose/lib/index.js:4:100)\nat Object.(sc_server)/./node_modules/mongoose/lib/index.js (/Users/avinashsethu/Desktop/Development/ekali-lms/.next/server/app/api/hello/route.js:2057:1)\nat webpack_require (/Users/avinashsethu/Desktop/Development/ekali-lms/.next/server/webpack-runtime.js:33:43)\nat eval (webpack-internal:///(sc_server)/./node_modules/mongoose/index.js:5:18)\nat Object.(sc_server)/./node_modules/mongoose/index.js (/Users/avinashsethu/Desktop/Development/ekali-lms/.next/server/app/api/hello/route.js:242:1)\nat webpack_require (/Users/avinashsethu/Desktop/Development/ekali-lms/.next/server/webpack-runtime.js:33:43)\nat eval (webpack-internal:///(sc_server)/./models/UserModel.js:5:66)\nat Module.(sc_server)/./models/UserModel.js (/Users/avinashsethu/Desktop/Development/ekali-lms/.next/server/app/api/hello/route.js:154:1)\nat webpack_require (/Users/avinashsethu/Desktop/Development/ekali-lms/.next/server/webpack-runtime.js:33:43)\nat eval (webpack-internal:///(sc_server)/./app/api/hello/route.js:6:75)\nat Module.(sc_server)/./app/api/hello/route.js (/Users/avinashsethu/Desktop/Development/ekali-lms/.next/server/app/api/hello/route.js:143:1)\nat webpack_require (/Users/avinashsethu/Desktop/Development/ekali-lms/.next/server/webpack-runtime.js:33:43)\nat eval (webpack-internal:///(sc_server)/./node_modules/next/dist/build/webpack/loaders/next-app-loader.js?name=app%2Fapi%2Fhello%2Froute&appPaths=&pagePath=private-next-app-dir%2Fapi%2Fhello%2Froute.js&appDir=%2FUsers%2Favinashsethu%2FDesktop%2FDevelopment%2Fekali-lms%2Fapp&pageExtensions=tsx&pageExtensions=ts&pageExtensions=jsx&pageExtensions=js&rootDir=%2FUsers%2Favinashsethu%2FDesktop%2FDevelopment%2Fekali-lms&isDev=true&tsconfigPath=tsconfig.json&assetPrefix=!:13:130)\nat Module.(sc_server)/./node_modules/next/dist/build/webpack/loaders/next-app-loader.js?name=app%2Fapi%2Fhello%2Froute&appPaths=&pagePath=private-next-app-dir%2Fapi%2Fhello%2Froute.js&appDir=%2FUsers%2Favinashsethu%2FDesktop%2FDevelopment%2Fekali-lms%2Fapp&pageExtensions=tsx&pageExtensions=ts&pageExtensions=jsx&pageExtensions=js&rootDir=%2FUsers%2Favinashsethu%2FDesktop%2FDevelopment%2Fekali-lms&isDev=true&tsconfigPath=tsconfig.json&assetPrefix=!\nat /Users/avinashsethu/Desktop/Development/ekali-lms/node_modules/next/dist/server/load-components.js:49:73\nat async Object.loadComponentsImpl [as loadComponents] (/Users/avinashsethu/Desktop/Development/ekali-lms/node_modules/next/dist/server/load-components.js:49:26)\nat async DevServer.findPageComponentsImpl (/Users/avinashsethu/Desktop/Development/ekali-lms/node_modules/next/dist/server/next-server.js:599:36) {\npage: '/api/hello'\n}\nI need to connect the mongoDB to next js application then only i can able to build an application without this i cant able to create an application",
      "solution": "I am currently facing the same issue, after updating the mongoose dependencies to 7.0.1. Downgrading to mongoose \"6.10.3\" helped me to resolve the issue for the time being. I hope, this can help you as well!",
      "question_score": 4,
      "answer_score": 4,
      "created_at": "2023-03-10T15:23:27",
      "url": "https://stackoverflow.com/questions/75697312/import-mongoose-lib-in-api-directory-in-next-js-13-2-app-directory-gives-error"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongoose",
      "question_id": 69346158,
      "title": "MongooseError: Query was already executed: User.countDocuments({})",
      "problem": "(node:9540) UnhandledPromiseRejectionWarning: MongooseError: Query was already executed: User.countDocuments({})\nat model.Query._wrappedThunk [as _countDocuments] (D:\\Acadamic-LANGUAGE-PROJECTS\\Angular-Projects\\eShop-MEAN STACK\\Back-End\\node_modules\\mongoose\\lib\\helpers\\query\\wrapThunk.js:21:19)\nat D:\\Acadamic-LANGUAGE-PROJECTS\\Angular-Projects\\eShop-MEAN STACK\\Back-End\\node_modules\\kareem\\index.js:370:33\nat processTicksAndRejections (internal/process/task_queues.js:77:11)\n(Use `node --trace-warnings ...` to show where the warning was created)\n(node:9540) UnhandledPromiseRejectionWarning: Unhandled promise rejection. This error originated either by throwing inside of an async function without a catch block, or by rejecting a promise which was not handled with .catch(). To terminate the node\nprocess on unhandled promise rejection, use the CLI flag `--unhandled-rejections=strict` (see https://nodejs.org/api/cli.html#cli_unhandled_rejections_mode). (rejection id: 1)\n(node:9540) [DEP0018] DeprecationWarning: Unhandled promise rejections are deprecated. In the future, promise rejections that are not handled will terminate the Node.js process with a non-zero exit code.\nthis is my Code......\n```\n`router.get(`/get/count`, async (req, res) =>{\nconst userCount = await User.countDocuments((count) => count)\n\nif(!userCount) {\n    res.status(500).json({success: false})\n} \nres.send({\n    userCount: userCount\n});\n`\n```\n})",
      "solution": "This is because countDocuments() is called using its callback (which passes its result to the callback), while on the other hand, it is also asked to pass its result to the userCount variable using the await command.\nThis is exactly what this error message is trying to say:  hey, you're sending the same query to the database twice !  While, since since v6 of Mongoose, you can only get run query once - ie, either by adding the cbk argument, or using async-await block.  Read about it here:  https://mongoosejs.com/docs/migrating_to_6.html#duplicate-query-execution\nNow let's move ahead to fixing the problem:\nI don't completely understand what you're trying to do this in line:\n```\n`    const userCount = await User.countDocuments((count) => count)\n`\n```\nI think what you're trying to do is just get the document count.  If so, simply drop 'count => count'.\n```\n`router.get(`/get/count`, async (req, res) =>{\n\nconst userCount = await User.countDocuments();\n\nif(!userCount) {\n    res.status(500).json({success: false})\n} \nres.send({\n    userCount: userCount\n});\n})\n`\n```\nIf you were to add a filter to the count (which is what the countDocuments gets - a filter; see API here), then you should use the key:value pair form, ie {count: count}.\n```\n`router.get(`/get/count`, async (req, res) =>{\n\n/* let count; etc. */\n\nconst userCount = await User.countDocuments({count: count});\n\nif(!userCount) {\n    res.status(500).json({success: false})\n} \nres.send({\n    userCount: userCount\n});\n})\n`\n```\nOf course you should use a proper try-catch block when using await, to be able to handle the error if thrown.\n(Just encountered this problem myself and made some research into it.)",
      "question_score": 4,
      "answer_score": 7,
      "created_at": "2021-09-27T13:56:21",
      "url": "https://stackoverflow.com/questions/69346158/mongooseerror-query-was-already-executed-user-countdocuments"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongoose",
      "question_id": 66839427,
      "title": "Mongoose middleware schema.pre(&#39;save&#39;, ...)",
      "problem": "I am making a REST API in NodeJS using the Mongoose Driver. I want to hash the passwords before saving them. For the same, I am using Mongoose Schema, where I made a userSchema for my user model. And for hashing I used the following function.\n```\n`userSchema.pre('save', async (next) => {\n    const user = this;\n    console.log(user);\n    console.log(user.isModified);\n    console.log(user.isModified());\n    console.log(user.isModified('password'));\n    if (!user.isModified('password')) return next();\n    console.log('just before saving...');\n    user.password = await bcrypt.hash(user.password, 8);\n    console.log('just before saving...');\n    next();\n});\n`\n```\nBut on creating a user or modifying it I am getting Error 500, and {} is getting returned. My routers are as follows.\n```\n`router.post('/users', async (req, res) => {\n    const user = User(req.body);\n    try {\n        await user.save();\n        res.status(201).send(user);\n    } catch (e) {\n        res.status(400).send(e);\n    }\n});\n\nrouter.patch('/users/:id', async (req, res) => {\n    const updateProperties = Object.keys(req.body);\n    const allowedUpdateProperties = [\n        'name', 'age', 'email', 'password'\n    ]; \n    const isValid = updateProperties.every((property) => allowedUpdateProperties.includes(property));\n    if (!isValid) {\n        return res.status(400).send({error: \"Invalid properties to update.\"})\n    }\n\n    const _id = req.params.id;\n    try { \n        const user = await User.findById(req.params.id);\n        updateProperties.forEach((property) => user[property] = req.body[property]);\n        await user.save();\n        if (!user) {\n            return res.status(404).send();\n        }\n        res.status(200).send(user);\n    } catch (e) {\n        res.status(400).send(e);\n    }\n});\n`\n```\nAnd the following is my console output.\n```\n`Server running on port 3000\n{}\nundefined\n`\n```\nOn commenting out the userSchema.pre('save', ...) everything is working as expected. Please can you help me figure out where am I going wrong.",
      "solution": "Using function definition instead of arrow function for mongoose `pre` save middleware:\n```\n`userSchema.pre('save', async function(next) { // this line\n    const user = this;\n    console.log(user);\n    console.log(user.isModified);\n    console.log(user.isModified());\n    console.log(user.isModified('password'));\n    if (!user.isModified('password')) return next();\n    console.log('just before saving...');\n    user.password = await bcrypt.hash(user.password, 8);\n    console.log('just before saving...');\n    next();\n});\n`\n```\nUpdate:\nThe difference is `this` context, if you use `arrow` function in this line `const user = this;`, this now is your current file (schema file, I guess).\nWhen you use `function` keyword, `this` context will belong to the caller object (user instance).",
      "question_score": 4,
      "answer_score": 16,
      "created_at": "2021-03-28T09:56:00",
      "url": "https://stackoverflow.com/questions/66839427/mongoose-middleware-schema-presave"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongoose",
      "question_id": 67997029,
      "title": "Mongoose - can&#39;t access createdAt",
      "problem": "I'm creating a NestJs application using Mongoose, I'm currently having a problem trying to access createdAt even though I have set timestamps to true, my code is below.\nproduct.schema.ts\n```\n`@Schema({timestamps: true})\nexport class Product extends Document {\n  @Prop({ required: true })\n  name!: string;\n}\n`\n```\nproduct.service.ts\n```\n`public async getProduct(name: string): Promise {\n    const existingProduct = await this.productModel.findOne({ name });\n\n    if (!existingProduct) {\n      throw new NotFoundException();\n    }\n\n    existingProduct.createdAt //Property 'createdAt' does not exist on type 'Product'\n  }\n`\n```",
      "solution": "Just because you set `timestamps: true` doesn't mean that typescript understands that those fields should exist. You need to add them to the class, but without the `@Prop()` decorator, so that typescript knows the fields exist, but Nest doesn't try to re-create the fields for you.",
      "question_score": 4,
      "answer_score": 12,
      "created_at": "2021-06-16T07:50:28",
      "url": "https://stackoverflow.com/questions/67997029/mongoose-cant-access-createdat"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongoose",
      "question_id": 70904341,
      "title": "How can I have class-transform converting properly the _id of a MongoDB class?",
      "problem": "I've the following MongoDB class:\n```\n`@Schema()\nexport class Poker {\n  @Transform(({ value }) => value.toString())\n  _id: ObjectId;\n\n  @Prop()\n  title: string;\n\n  @Prop({ type: mongoose.Schema.Types.ObjectId, ref: User.name })\n  @Type(() => User)\n  author: User;\n}\n`\n```\nwhich I return, transformed by `class-transform` in a NestJS server.\nIt get transformed by an interceptor:\n```\n`  @Get()\n  @UseGuards(JwtAuthenticationGuard)\n  @UseInterceptors(MongooseClassSerializerInterceptor(Poker))\n  async findAll(@Req() req: RequestWithUser) {\n    return this.pokersService.findAll(req.user);\n  }\n`\n```\nI'm not the author of the interceptor, but here is how it is implemented:\n```\n`function MongooseClassSerializerInterceptor(\n  classToIntercept: Type,\n): typeof ClassSerializerInterceptor {\n  return class Interceptor extends ClassSerializerInterceptor {\n    private changePlainObjectToClass(document: PlainLiteralObject) {\n      if (!(document instanceof Document)) {\n        return document;\n      }\n\n      return plainToClass(classToIntercept, document.toJSON());\n    }\n\n    private prepareResponse(\n      response: PlainLiteralObject | PlainLiteralObject[],\n    ) {\n      if (Array.isArray(response)) {\n        return response.map(this.changePlainObjectToClass);\n      }\n\n      return this.changePlainObjectToClass(response);\n    }\n\n    serialize(\n      response: PlainLiteralObject | PlainLiteralObject[],\n      options: ClassTransformOptions,\n    ) {\n      return super.serialize(this.prepareResponse(response), options);\n    }\n  };\n}\n\nexport default MongooseClassSerializerInterceptor;\n`\n```\nThe problem I'm having, is that when I do a console.log of the return of my controller, I get this:\n```\n`[\n  {\n    _id: new ObjectId(\"61f030a9527e209d8cad179b\"),\n    author: {\n      _id: new ObjectId(\"61f03085527e209d8cad1793\"),\n      password: '--------------------------',\n      name: '----------',\n      email: '-------------',\n      __v: 0\n    },\n    title: 'Wonderful first poker2',\n    __v: 0\n  }\n]\n`\n```\nBut I get this returned:\n```\n`[\n    {\n        \"_id\": \"61f5149643092051ba048c6e\",\n        \"author\": {\n            \"_id\": \"61f5149643092051ba048c6f\",\n            \"name\": \"----------\",\n            \"email\": \"-------------\",\n            \"__v\": 0\n        },\n        \"title\": \"Wonderful first poker2\",\n        \"__v\": 0\n    }\n]\n`\n```\nIf you check the id, it's not at all the same. Then the client will ask some data for this ID and receive nothing.\nAny idea what am I missing?\nAlso, every time I make a GET request, I receive a different value back.",
      "solution": "Try using this for _id:\n```\n`@Transform(params => params.obj._id)\n`\n```\nOr this for a more general case:\n```\n`@Transform(({ key, obj }) => obj[key])\n`\n```\nStuck with the same problem, solved it this way. params.obj is an original object. Unfortunately, I don't know, why class-transformer doesn't define _id correctly by default.",
      "question_score": 4,
      "answer_score": 9,
      "created_at": "2022-01-29T11:22:26",
      "url": "https://stackoverflow.com/questions/70904341/how-can-i-have-class-transform-converting-properly-the-id-of-a-mongodb-class"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongoose",
      "question_id": 69055292,
      "title": "Error: grid.mongo.GridStore is not a consstructor ,Using mongoose, Grid-fs-stream and grid multer storage",
      "problem": "I am getting the following error mentioned.\nThe basic configs are as follow:\nI have uploaded the files on the server\nI want to download them but getting these errors\nI called a POST request to /api/files/delete/${fileId}\nWhich should call the route and give back the file to the browser instead getting the error with the Grid related module.\n```\n`MONGODB_CONNECTION_STRING = mongodb://localhost:27017/Cstore\n\nDb structure={Cstore:{uploads.files,uploads.chunks,users}}\n\nOn requesting POST         axios.post(`/api/files/delete/${fileId}`)\n\nGetting error: \n\n     this._store = new grid.mongo.GridStore(grid.db, this.id || new grid.mongo.ObjectID(), this.name, this.mode, options);\n[0]                 ^\n[0] \n[0] TypeError: grid.mongo.GridStore is not a constructor\n[0]     at new GridReadStream (/home/lenovo/Desktop/react/cstore/node_modules/gridfs-stream/lib/readstream.js:68:17)\n\nThis is server.js\n\n      app.get('/image/:filename', (req, res) => {\n    gfs.files.findOne({ \n      _id: mongoose.Types.ObjectId(req.params.filename)\n      // filename: req.params.filename.toString()\n    }, (err, file) => {\n      // Check if file\n      if (!file || file.length === 0) {\n        console.log(\"not found\")\n        return res.status(404).json({\n          \n          err: 'No file exists'\n        });\n      }\n\n      // Check if image\n      if (file.contentType === 'image/jpeg' || file.contentType === 'image/png') {\n        // Read output to browser\n        console.log(file)\n        let id=file._id;\n\n        const readStream = gfs.createReadStream(\n          {\n            _id: mongoose.Types.ObjectId(req.params.filename)\n          }\n          // {\n          // _id: id\n          // }\n        )\n        readStream.on('error', err => {\n            // report stream error\n            console.log(err);\n        });\n        // the response will be the file itself.\n        readStream.pipe(res);\n\n      //   let readstream = gfs.createReadStream(mongoose.Types.ObjectId(file._id))\n      //   readstream.pipe(res)\n      // } else {\n        res.status(404).json({\n          err: 'Not an image'\n        });\n      }\n    });\n  });\n\n  \n\n    mongoose.Promise = global.Promise;\n      mongoose.connect(\n        mongoURI,\n        {\n          useNewUrlParser: true,\n          useUnifiedTopology: true,\n        },\n        (err) => {\n          if (err) throw err;\n          console.log('MongoDB connection established');\n        }\n      )\n      const connection = mongoose.connection;\n    \n      ///  HANDLING FILE\n      let gfs;\n      connection.once('open', () => {\n        // Init stream\n        gfs = Grid(connection.db, mongoose.mongo);\n        gfs.collection('uploads');\n      });\n    \n      const storage = new GridFsStorage({\n        url: mongoURI, \n        file: (req, file) => {\n          return new Promise((resolve, reject) => {\n            crypto.randomBytes(16, (err, buf) => {\n              if (err) {\n                return reject(err);\n              }\n              const filename = file.originalname;\n              const fileInfo = {\n                filename: filename,\n                bucketName: 'uploads'\n              };\n              resolve(fileInfo);\n            });\n          });\n        }\n      })\n    \n      const upload = multer({ storage });\n\nInitial:\n  const express = require('express');\n  const mongoose = require('mongoose');\n  const cookieParser = require('cookie-parser');\n  const path = require('path');\n\n  const multer = require('multer');\n  const {GridFsStorage} = require('multer-gridfs-storage');\n  const Grid = require('gridfs-stream');\n  const methodOverride = require('method-override')\n  const crypto = require('crypto')\n\n  const app = express();\n\n  app.use(express.json());\n  app.use(express.urlencoded({ extended: true }));\n  app.use(methodOverride('_method'));\n  app.use(cookieParser());\n\n  const dotenv=require('dotenv');\n  dotenv.config({path:__dirname+'/.env'});\n\n  const mongoURI = process.env.MONGODB_CONNECTION_STRING\n`\n```",
      "solution": "I also faced similar issues. The resolution for me was with mongoose version. The issue arises in the version 6.0.5 but is working in the version 5.13.7",
      "question_score": 4,
      "answer_score": 3,
      "created_at": "2021-09-04T14:28:14",
      "url": "https://stackoverflow.com/questions/69055292/error-grid-mongo-gridstore-is-not-a-consstructor-using-mongoose-grid-fs-strea"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongoose",
      "question_id": 68915722,
      "title": "Option &quot;useFindAndModify&quot; is not supported",
      "problem": "I'm trying to connect to my database using mongoose and in my console is displaying ' option usefindandmodify is not supproted '. I'm using mongoose 6.0.0\nthis is my code\n```\n`  mongoose.connect(constants.CONNECTION_URL,\n     { useNewUrlParser: true,\n       useUnifiedTopology: true, \n       useFindAndModify: false \n     })\n.then(() => app.listen(constants.PORT, () => console.log(`Server Running on Port ${constants.PORT}`)))\n.catch((error) => console.log(error.message));\n\nmongoose.Promise = global.Promise;\n`\n```\ncan someone suggest me how can I get rid of that? is written with white if matters neither green nor red, white.",
      "solution": "Starting with Mongoose version 6, you should not specify that as an option. It will be handled automatically.\nThis issue is explained here.\n\n`useNewUrlParser`, `useUnifiedTopology`, `useFindAndModify`, and `useCreateIndex` are no longer supported options. Mongoose 6 always behaves as if `useNewUrlParser`, `useUnifiedTopology`, and `useCreateIndex` are true, and `useFindAndModify` is false. Please remove these options from your code.",
      "question_score": 4,
      "answer_score": 10,
      "created_at": "2021-08-25T03:23:20",
      "url": "https://stackoverflow.com/questions/68915722/option-usefindandmodify-is-not-supported"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongoose",
      "question_id": 68167071,
      "title": "Models ref each other error: circular dependencies problem",
      "problem": "admin.model.ts\n```\n`import mongoose, { Schema, Document } from 'mongoose';\nimport UserRole, { IUserRole } from './user-role.model';\n\nexport interface IAdmin extends Document {\n    role: IUserRole;\n}\n\nlet adminSchema = new Schema({\n    role: {\n        type: Schema.Types.ObjectId, ref: UserRole\n    }\n});\n\nexport default mongoose.model('Admin', adminSchema);\n`\n```\nuser-role.model.ts\n```\n`import { Schema, Document, model } from 'mongoose';\n\nexport interface IUserRole extends Document{\n    updated_by: IAdmin|string;\n}\n\nlet userRoleSchema = new Schema({\n    updated_by: {\n        type: Schema.Types.ObjectId, ref: Admin\n    }\n})\n\nexport default model('UserRole', userRoleSchema);\n`\n```\n```\n`MongooseError: Invalid ref at path \"updated_by\". Got undefined\n    at validateRef (/home/ess24/ess-smartlotto/node-rest/node_modules/mongoose/lib/helpers/populate/validateRef.js:17:9)\n    at Schema.path (/home/ess24/ess-smartlotto/node-rest/node_modules/mongoose/lib/schema.js:655:5)\n    at Schema.add (/home/ess24/ess-smartlotto/node-rest/node_modules/mongoose/lib/schema.js:535:14)\n    at require (internal/modules/cjs/helpers.js:88:18)\n[ERROR] 22:25:54 MongooseError: Invalid ref at path \"updated_by\". Got undefined\n`\n```\nHere is my two model how can I solve this type of problem and how to deal with circular dependencies?",
      "solution": "You have to put the `ref` values for `UserRole` and `Admin` in `''` like this:\n```\n`const adminSchema = new Schema({\n    role: {\n        type: Schema.Types.ObjectId, ref: 'UserRole' // Name of the model you are referencing\n    }\n});\nlet userRoleSchema = new Schema({\n    updated_by: {\n        type: Schema.Types.ObjectId, ref: 'Admin' // <- and here\n    }\n})\n`\n```",
      "question_score": 4,
      "answer_score": 5,
      "created_at": "2021-06-28T18:39:03",
      "url": "https://stackoverflow.com/questions/68167071/models-ref-each-other-error-circular-dependencies-problem"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongoose",
      "question_id": 65865612,
      "title": "Mongoose document type declaration",
      "problem": "I know maybe this is far too basic but I can't recall how to do this properly. I want to declare a Mongoose document to use VS Code IntelliSense for retrieve data.\nRight now, `document` is declared as `any` since `findById()` returns `any`:\n`const document = await MyModel.findById(docId);\n`\nSo, whenever I want to call to something like `document.updateOne()` I don't have intelliSense on.\nI have tried using something like:\n`import { Model, Document } from 'mongoose';\n...\nconst document: Model = await MyModel.findById(docId);\n`\nBut this don't give me the ability to refer internal attributes directly like `document.title` or any other.\nSo, what is the proper way to declare `document`?",
      "solution": "Your `MyModel` has some sort of document type that `extends` the Mongoose `Document` type and likely adds some properties of its own.  That's the generic that you want to use.\nInstead of setting the generic (``) when you retrieve the document, you want to set the generic on the `MyModel` object itself so that the Typescript will infer the correct type for `findById` and for any other methods.  So you want to handle this at the place where you create `MyModel`.\n```\n`interface MyDocument extends Document {\n    title: string;\n}\n\nconst MyModel = mongoose.model(name, schema);\n`\n```\nNow the `document` is inferred to be type `MyDocument | null` here:\n```\n`const document = await MyModel.findById(docId);\n`\n```",
      "question_score": 4,
      "answer_score": 3,
      "created_at": "2021-01-24T00:29:24",
      "url": "https://stackoverflow.com/questions/65865612/mongoose-document-type-declaration"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongoose",
      "question_id": 69376637,
      "title": "How to connect to a Mongo Standalone ReplicaSet, running in Docker, with Mongoose 6.0.8?",
      "problem": "Connecting to a MongoDB (with ReplicaSet) running in Docker worked in 5.13.9 but fails in 6.0.8\ndocker-compose.yml for the database:\nThis is the docker-compose.yml file for the Database:\n`version: '3.9'\n\nservices:\n\n  loggerdb:\n    image: mongo\n    container_name: loggerdb\n    command: [\"--replSet\", \"rs0\", \"--bind_ip_all\", ]\n    restart: always\n    ports:\n      - \"27017:27017\"\n    volumes:\n      - ./data:/data/db\n    environment:\n      MONGO_INITDB_DATABASE: logger\n\nnetworks:\n  default:\n    external: true\n    name: logger-network\n`\nAs you see, there is no Authentication and the database is listening to 27017 on localhost.\nThe database is running in the Docker Container:\n`$ docker ps -a\nCONTAINER ID   IMAGE          COMMAND                  CREATED       STATUS       PORTS                                           NAMES\nb42adb1e26ec   0bcbeb494bed   \"docker-entrypoint.s\u2026\"   3 hours ago   Up 3 hours   0.0.0.0:27017->27017/tcp, :::27017->27017/tcp   loggerdb\n`\nCode to connect (5.13.9)\nThe code is in Typescript, and for the version 5.13.9 Mongoose I use the following:\n`mongoose.connect('mongodb://localhost:27017/logger', { useNewUrlParser: true, useUnifiedTopology: true })\n    .then(() => {\n        app.listen(PORT, () => {\n            console.log(`Listening on ${PORT} for Log Messages`);\n        });\n    });\n`\nIt is necessary to specify useNewUrlParser and useUnifiedTopology here, otherwise you'll get an error.\nThe result is that this is working correctly. The program is connecting to the database and I can write stuff to it.\nCode to connect (6.0.8)\nThe code for 6.0.8 is slightly different:\n`mongoose.connect('mongodb://localhost:27017/logger?replicaSet=rs0')\n    .then(() => {\n        app.listen(PORT, () => {\n            console.log(`Listening on ${PORT} for Log Messages`);\n        });\n    });\n`\nThe useNewUrlParser and useUnifiedTopology are obsolete now, and the docs show you need to specify the ReplicaSet as a param.\nThe result of connecting with this code to 6.0.8 Mongoose is as follows:\n`[INFO] 14:33:33 ts-node-dev ver. 1.1.8 (using ts-node ver. 9.1.1, typescript ver. 4.3.5)\nMongooseServerSelectionError: getaddrinfo ENOTFOUND b42adb1e26ec\n   [[ Removed the StackTrace ]]\n[ERROR] 14:34:05 MongooseServerSelectionError: getaddrinfo ENOTFOUND b42adb1e26ec\n`\nQuestion\nWhat is the correct to connect to a Mongo Standalone ReplicaSet, running in Docker, with Mongoose 6.0.8 ?",
      "solution": "I had this exact problem when trying to connect to a single node replica set inside docker from localhost (I was also upgrading mongoose from v5 to v6.)\nI solved the issue by changing my connection string from\n`mongodb://admin:pass@127.0.0.1:27017/testdb`\nto\n`mongodb://admin:pass@127.0.0.1:27017/testdb?directConnection=true`\nIt looks like mongoose is forcefully using the docker hostname which would only work if your code was running inside of docker.\ndirectConnection - Specifies whether to force dispatch all operations to the host specified in the connection URI.",
      "question_score": 4,
      "answer_score": 8,
      "created_at": "2021-09-29T14:44:00",
      "url": "https://stackoverflow.com/questions/69376637/how-to-connect-to-a-mongo-standalone-replicaset-running-in-docker-with-mongoos"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongoose",
      "question_id": 69218445,
      "title": "&#39;ObjectId&#39; only refers to a type, but is being used as a value here.ts(2693)",
      "problem": "I have searched but couldn't find a proper answer. I am new to stackoverflow and typescript. I am getting an error while creating a Mongoose Schema, my code looks like this:\n`import  { Schema, ObjectId } from 'mongoose'\n\ninterface IArticle {\n    title: string\n    userId: ObjectId\n    thumb: string\n    slug: string\n    children: object[]\n    claps: number\n}\n\nconst articleSchema = new Schema({\n    -------\n    userId: {\n        type: ObjectId, // ERROR HERE!!!\n        required: true\n    },\n    -------\n})\n`\nBut it gives error that `'ObjectId' only refers to a type, but is being used as a value here.ts(2693)`\ntsconfig.json has the following content:\n```\n`{\n    \"compilerOptions\": {\n\n        \"incremental\": true,\n\n        \"target\": \"ES6\",\n        \"module\": \"commonjs\",\n        \"rootDir\": \"./src\" ,                       \n        \"baseUrl\": \"./src\",\n        \"outDir\": \"./build\",\n        \"esModuleInterop\": true,\n        \"forceConsistentCasingInFileNames\": true,\n\n        \"strict\": true,\n        \"skipLibCheck\": true\n    },\n    \"include\": [\"src/**/*\"]\n}\n`\n```\nI have also used `mongoose.Types.ObjectId` instead of `ObjectId`, but then get the following error:\n`Type 'typeof ObjectId' is missing the following properties from type 'typeof SchemaType': cast, checkRequired, set, getts(2322)`\nmongoose   => 6.0.5\ntypescript => 4.4.3\nwhat am I doing wrong?",
      "solution": "Tried Using `mongoose.Schema.Types.ObjectId` instead of `mongoose.ObjectId`. and it worked.\nmy not-so-clear understanding of why it gave an error:\n`mongoose.ObjectId` is an alias for `type ObjectId = mongoose.Schema.Types.ObjectId` which can only be used as a type, not as a \"value\" for type property in mongoose schema.\n`mongoose.Schema.Types.ObjectId` refers to the actual class, hence can be used both as a type and value.",
      "question_score": 4,
      "answer_score": 8,
      "created_at": "2021-09-17T08:01:43",
      "url": "https://stackoverflow.com/questions/69218445/objectid-only-refers-to-a-type-but-is-being-used-as-a-value-here-ts2693"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongoose",
      "question_id": 69270281,
      "title": "Mongoose return &quot;new ObjectId(&quot;//id&quot;)&quot; instead of just the id",
      "problem": "I am trying to do my login function (I am using bcrypt and jsonwebtoken) the problem is that console.log (user._id) returns me \"new ObjectId (\" 6148f043ebbaa0ab41ac8499 \")\" instead of just \"6148f043ebbaa0ab41ac8499\" , which would be easier for the creation of the token.\n```\n`  module.exports.login = async (req, res) => {\n     const { email, password } = req.body;\n    \n      // Compare the req.body.password to the hashed password in DB\n      const user = await UserModel.findOne({ email: email });\n      const match = await bcrypt.compare(password, user.password);\n    \n      if (match) {\n        try {\n          const user = await UserModel.findOne({ email: email });\n          console.log(user._id);\n    \n          // Assign a token\n          const token = jwt.sign({ userId: user._id }, process.env.LOGIN_TOKEN, {\n            expiresIn: \"1h\",\n          });\n          console.log(token);\n          res.cookie(\"jwt\", token, { httpOnly: true});\n          res.status(200).json({ user: user._id });\n        } catch (err) {\n          res.status(500).json(err);\n        }\n      } else {\n        res.status(500).json({ message: \"error!!!\" });\n      }\n    };\n`\n```\nHow to fix this please?",
      "solution": "That is a normal behaviour. Since you got an `ObjectId`, you can convert it to a string by calling the `toHexString()` method on it. I have also modified the code to check for an undefined user, and removed the extra call to find a user since you already did in the previous line. Please see updated code:\n```\n`module.exports.login = async (req, res) => {\n  const { email, password } = req.body;\n   const user = await UserModel.findOne({ email: email });\n\n   if (!user) {\n     return res.status(400).json({ message: \"Unauthorised\"});\n   }\n \n   // Compare the req.body.password to the hashed password in DB\n   const match = await bcrypt.compare(password, user.password);\n \n   if (match) {\n     try {\n      //  Convert user id (ObjectId) to a string\n       const userId = user._id.toHexString();\n      //  Now user id is a string\n       console.log(userId);\n \n       // Assign a token\n       const token = jwt.sign({ userId }, process.env.LOGIN_TOKEN, {\n         expiresIn: \"1h\",\n       });\n       console.log(token);\n       res.cookie(\"jwt\", token, { httpOnly: true});\n       res.status(200).json({ user });\n     } catch (err) {\n       res.status(500).json(err);\n     }\n   } else {\n     res.status(400).json({ message: \"Unauthorised\" });\n   }\n };\n`\n```",
      "question_score": 4,
      "answer_score": 5,
      "created_at": "2021-09-21T16:03:35",
      "url": "https://stackoverflow.com/questions/69270281/mongoose-return-new-objectid-id-instead-of-just-the-id"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongoose",
      "question_id": 77749884,
      "title": "( session: options?.session, ^ SyntaxError: Unexpected token &#39;.&#39;)mongoose give a error deploying in instance ubuntu aws",
      "problem": "I have this error while using mongoose:\n```\n`/var/www/node-api/node_modules/mongoose/node_modules/mongodb/lib/admin.js:62\n            session: options?.session,\n                             ^\n\nSyntaxError: Unexpected token '.'\n`\n```\nI reinstalled node_modules but I always get the same problem.\nFor mongoose and my connection in my device it worked correctly.",
      "solution": "Adding an answer for this because it keeps coming up.\nProblem:\nMongoose uses the MongoDB native Node.js driver as one of it's dependencies.\nIn the mongodb V6 native driver they implemented optional chaining in the `async command()` within `admin.js`:\n`async command(command, options) {\n   return (0, execute_operation_1.executeOperation)(this.s.db.client, new run_command_1.RunAdminCommandOperation(command, {\n      ...(0, bson_1.resolveBSONOptions)(options),\n      session: options?.session, //\nThe problem is that optional chaining wasn't supported in Node.js until V14.5.\nSolution:\nYou need to upgrade your Node.js version to a newer version. At the current time of writing, Node V20 is the LTS so I would recommend upgrading to that or see the Release Schedule to plan ahead.",
      "question_score": 4,
      "answer_score": 6,
      "created_at": "2024-01-03T06:27:14",
      "url": "https://stackoverflow.com/questions/77749884/session-options-session-syntaxerror-unexpected-token-mongoose-give-a"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongoose",
      "question_id": 69272891,
      "title": "NestJS Mongoose Schema Inheritence",
      "problem": "I am attempting to inherit Mongoose Schemas or SchemaDefitions within NestJS but I am not having much luck.\nI am doing this so I can share Base and Common Schema Definition Details such as a virtual('id') and a nonce, we have attached to each of the entities. Each schema definition should have its own collection in Mongo, so discriminators will not work.\nI tried to implement this in the following different ways\nFirst, I have the following Base Schema Definition defined:\nbase.schema.ts\n```\n`import { Prop, Schema, SchemaFactory } from '@nestjs/mongoose';\nimport { Document, Types } from 'mongoose';\nimport { TimeStamps } from './timestamps.schema';\n\nexport type BaseDocument = BaseSchemaDefinition & Document;\n\n@Schema({\n  toJSON: {\n    virtuals: true,\n    transform: function (doc: any, ret: any) {\n      delete ret._id;\n      delete ret.__v;\n      return ret;\n    },\n  },\n})\nexport class BaseSchemaDefinition {\n  @Prop({\n    type: Types.ObjectId,\n    required: true,\n    default: Types.ObjectId,\n  })\n  nonce: Types.ObjectId;\n\n  @Prop()\n  timestamps: TimeStamps;\n}\n`\n```\nI then inherit the schema definition and create the schema so it can be used later in my services and controllers by the following:\nperson.schema.ts\n```\n`import { Prop, SchemaFactory } from '@nestjs/mongoose';\nimport * as mongoose from 'mongoose';\nimport { Document } from 'mongoose';\nimport { Address } from './address.schema';\nimport { BaseSchemaDefinition } from './base.schema';\n\nexport type PersonDocument = PersonSchemaDefintion & Document;\n\nexport class PersonSchemaDefintion extends BaseSchemaDefinition {\n  @Prop({ required: true })\n  first_name: string;\n\n  @Prop({ required: true })\n  last_name: string;\n\n  @Prop()\n  middle_name: string;\n\n  @Prop()\n  data_of_birth: Date;\n\n  @Prop({ type: [{ type: mongoose.Schema.Types.ObjectId, ref: 'Address' }] })\n  addresses: [Address];\n}\n\nconst PersonSchema = SchemaFactory.createForClass(PersonSchemaDefintion);\n\nPersonSchema.virtual('id').get(function (this: PersonDocument) {\n  return this._id;\n});\n\nexport { PersonSchema };\n`\n```\nThis results in only allowing me to create and get properties defined in the BaseSchemaDefinition.\n\n{\n\"timestamps\": {\n\"deleted\": null,\n\"updated\": \"2021-09-21T16:55:17.094Z\",\n\"created\": \"2021-09-21T16:55:17.094Z\"\n},\n\"_id\": \"614a0e75eb6cb52aa0ccd026\",\n\"nonce\": \"614a0e75eb6cb52aa0ccd028\",\n\"__v\": 0 }\n\nSecond, I then tried to implement inheritance by using the method described here\nInheriting Mongoose schemas (different MongoDB collections)\nbase.schema.ts\n```\n`import { Prop, Schema, SchemaFactory } from '@nestjs/mongoose';\nimport { Document, Types } from 'mongoose';\nimport { TimeStamps } from './timestamps.schema';\n\nexport type BaseDocument = BaseSchemaDefinition & Document;\n\n@Schema({\n  toJSON: {\n    virtuals: true,\n    transform: function (doc: any, ret: any) {\n      delete ret._id;\n      delete ret.__v;\n      return ret;\n    },\n  },\n})\nexport class BaseSchemaDefinition {\n  @Prop({\n    type: Types.ObjectId,\n    required: true,\n    default: Types.ObjectId,\n  })\n  nonce: Types.ObjectId;\n\n  @Prop()\n  timestamps: TimeStamps;\n}\n\nconst BaseSchema = SchemaFactory.createForClass(BaseSchemaDefinition);\n\nBaseSchema.virtual('id').get(function (this: BaseDocument) {\n  return this._id;\n});\n\nexport { BaseSchema };\n`\n```\nperson.schema.ts\n```\n`import { Prop } from '@nestjs/mongoose';\nimport * as mongoose from 'mongoose';\nimport { Document } from 'mongoose';\nimport { Address } from './address.schema';\nimport { BaseSchema, BaseSchemaDefinition } from './base.schema';\n\nexport type PersonDocument = PersonSchemaDefintion & Document;\n\nexport class PersonSchemaDefintion extends BaseSchemaDefinition {\n  @Prop({ required: true })\n  first_name: string;\n\n  @Prop({ required: true })\n  last_name: string;\n\n  @Prop()\n  middle_name: string;\n\n  @Prop()\n  data_of_birth: Date;\n\n  @Prop({ type: [{ type: mongoose.Schema.Types.ObjectId, ref: 'Address' }] })\n  addresses: [Address];\n}\n\nexport const PersonSchema = Object.assign(\n  {},\n  BaseSchema.obj,\n  PersonSchemaDefintion,\n);\n`\n```\nResults in the same output. Not sure why the inheritance is not taking\nThe following is the service code that uses the schemas and builds the models\nperson.service.ts\n```\n`import { Model } from 'mongoose';\nimport { Injectable } from '@nestjs/common';\nimport { InjectModel } from '@nestjs/mongoose';\nimport {\n  PersonSchemaDefintion,\n  PersonDocument,\n} from 'src/schemas/person.schema';\nimport { TimeStamps } from 'src/schemas/timestamps.schema';\n\n@Injectable()\nexport class PersonService {\n  constructor(\n    @InjectModel(PersonSchemaDefintion.name)\n    private personModel: Model,\n  ) {}\n\n  async create(\n    personModel: PersonSchemaDefintion,\n  ): Promise {\n    personModel.timestamps = new TimeStamps();\n    const createdPerson = new this.personModel(personModel);\n\n    return createdPerson.save();\n  }\n\n  async update(\n    id: string,\n    changes: Partial,\n  ): Promise {\n    const existingPerson = this.personModel\n      .findByIdAndUpdate(id, changes)\n      .exec()\n      .then(() => {\n        return this.personModel.findById(id);\n      });\n    if (!existingPerson) {\n      throw Error('Id does not exist');\n    }\n    return existingPerson;\n  }\n\n  async findAll(): Promise {\n    return this.personModel.find().exec();\n  }\n\n  async findOne(id: string): Promise {\n    return this.personModel.findById(id).exec();\n  }\n\n  async delete(id: string): Promise {\n    return this.personModel.deleteOne({ _id: id }).then(() => {\n      return Promise.resolve(`${id} has been deleted`);\n    });\n  }\n}\n`\n```\nI can provide additional details if it is needed",
      "solution": "After fiddling around with it for a while I found the right combination that appears to work when leveraging these technologies\nHere is the base class\nbase.schema.ts\n```\n`import { Prop, Schema } from '@nestjs/mongoose';\nimport { Document, Types } from 'mongoose';\nimport { TimeStamps } from './timestamps.schema';\n\nexport type BaseDocument = Base & Document;\n\n@Schema()\nexport class Base {\n  @Prop({\n    type: Types.ObjectId,\n    required: true,\n    default: Types.ObjectId,\n  })\n  nonce: Types.ObjectId;\n\n  @Prop()\n  timestamps: TimeStamps;\n}\n`\n```\nHere is the class that inherits the base.schema\nperson.schema.ts\n```\n`import { Prop, Schema, SchemaFactory } from '@nestjs/mongoose';\nimport { Document, Types } from 'mongoose';\nimport { Address } from './address.schema';\nimport { Base } from './base.schema';\n\nexport type PersonDocument = Person & Document;\n\n@Schema({\n  toJSON: {\n    virtuals: true,\n    transform: function (doc: any, ret: any) {\n      delete ret._id;\n      delete ret.__v;\n      return ret;\n    },\n  },\n})\nexport class Person extends Base {\n  @Prop({ required: true })\n  first_name: string;\n\n  @Prop({ required: true })\n  last_name: string;\n\n  @Prop()\n  middle_name: string;\n\n  @Prop()\n  data_of_birth: Date;\n\n  @Prop({ type: [{ type: Types.ObjectId, ref: 'Address' }] })\n  addresses: [Address];\n}\nconst PersonSchema = SchemaFactory.createForClass(Person);\n\nPersonSchema.virtual('id').get(function (this: PersonDocument) {\n  return this._id;\n});\n\nexport { PersonSchema };\n`\n```\nThe only thing I would like to improve on is moving the virtual('id') to the base class. However the schema inheritance does not work. At this point, it will only work with the Schema Definition. This at least gets me in the right direction. If anyone has a way to improve on this please contribute.",
      "question_score": 4,
      "answer_score": 2,
      "created_at": "2021-09-21T19:07:59",
      "url": "https://stackoverflow.com/questions/69272891/nestjs-mongoose-schema-inheritence"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongoose",
      "question_id": 69953709,
      "title": "Error: Collection method aggregate is synchronous",
      "problem": "I'm trying the following code:\n```\n`const Conn = mongoose.createConnection('mongodb://127.0.0.1:27017/db');\n\nconst addresses = Conn.collection('users').aggregate([\n  {\n    $project: {\n      _id: false,\n      ethAddr: true,\n    }\n  }\n]);\n\n`\n```\nI receive the following error:\n```\n`[...]\\backend\\node_modules\\mongoose\\lib\\drivers\\node-mongodb-native\\collection.js:100\n        throw new Error('Collection method ' + i + ' is synchronous');\n              ^\n\nError: Collection method aggregate is synchronous\n    at NativeCollection. [as aggregate] ([...]\\backend\\node_modules\\mongoose\\lib\\drivers\\node-mongodb-native\\collection.js:100:15)\n    at file:///[...]/backend/scripts/dbGetMerkleRoot.js:11:46\n    at file:///[...]/backend/scripts/dbGetMerkleRoot.js:28:3\n    at ModuleJob.run (node:internal/modules/esm/module_job:185:25)\n    at async Promise.all (index 0)\n    at async ESMLoader.import (node:internal/modules/esm/loader:281:24)\n    at async loadESM (node:internal/process/esm_loader:88:5)\n    at async handleMainPromise (node:internal/modules/run_main:65:12)\n`\n```\nWhat in cattle's name I'm doing wrong?",
      "solution": "It seems to be a problem with the way mongoose connects to the database. Creating the connection but not connecting prior to invoking the aggregation method causes that exception to be thrown. I should've used it this way:\n`// create custom connection\nconst Conn = mongoose.createConnection();\n\n// connect to database\nawait Conn.openUri('mongodb://127.0.0.1:27017/db');\n\n// @type {AggregationCursor}\nconst addresses = Conn.collection('users').aggregate([\n  {\n    $project: {\n      _id: false,\n      ethAddr: true,\n    }\n  }\n]);\n\nconsole.log( await addresses.toArray() );\n`\nIt's frustrating the exception itself is poorly documented.",
      "question_score": 4,
      "answer_score": 5,
      "created_at": "2021-11-13T12:14:40",
      "url": "https://stackoverflow.com/questions/69953709/error-collection-method-aggregate-is-synchronous"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongoose",
      "question_id": 68981568,
      "title": "How to check if email or phone already exists in mongodb database",
      "problem": "I am facing a problem regarding checking of email and phone numbers in my MongoDB database. My code only checks if the email is present in the database but does not respond to the phone.\n```\n`const express = require(\"express\");\nconst router = express.Router();\n\nrequire(\"../db/conn\");\nconst User = require(\"../model/userSchema\");\n\nrouter.get(\"/\", (req, res) => {\n  res.send(`Hello World from server lolstar`);\n});\n\nrouter.post(\"/register\", async (req, res) => {\n  const { name, email, phone, work, password, cpassword } = req.body;\n\n  if (!name || !email || !phone || !work || !password || !cpassword) {\n    return res.status(422).json({ error: \"Please fill your details\" });\n    }\n    try {\n        const userExist = await User.findOne({ email: email }, {phone: phone });\n        if (userExist)\n        {\n             return res\n          .status(422)\n          .json({ error: \"Email or Phone number already exists\" });\n        }\n        \n        const user = new User({ name, email, phone, work, password, cpassword });\n        \n        const userRegister = await user.save();\n        if (userRegister)\n        {\n             res.status(201).json({ message: \"User registered successfully\" });\n            }\n      \n    } catch (err) {\n        console.log(err);\n        \n  }\n  \n});\n\nmodule.exports = router;\n`\n```\nI have added my UserSchema file. I don't think it has an error please check if something is wrong here. I want the code to check for both email and phone and then use it for authentication purpose.\n```\n`const mongoose = require('mongoose');\n\nconst userSchema = new mongoose.Schema({\n    name: {\n        type: String,\n        required: true,\n    },\n    email: {\n        type: String,\n        required: true,\n    },\n    phone: {\n        type: Number,\n        required: true,\n    },\n    work: {\n        type: String,\n        required: true,\n    },\n    password: {\n        type: String,\n        required: true,\n    },\n    cpassword: {\n        type: String,\n        required: true,\n    }\n\n})\n\nconst User = mongoose.model('USER', userSchema);\n\nmodule.exports = User;\n`\n```",
      "solution": "You want to check if `email` OR `phone` exists in database. Your current query checks if both of them exist. You should use `$or` operator for your query, like this:\n`await User.findOne({ \"$or\": [ { email: email }, { phone: phone} ] });\n`",
      "question_score": 4,
      "answer_score": 5,
      "created_at": "2021-08-30T11:13:07",
      "url": "https://stackoverflow.com/questions/68981568/how-to-check-if-email-or-phone-already-exists-in-mongodb-database"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongoose",
      "question_id": 78283785,
      "title": "Can&#39;t delete by id from mongoDB in nextjs app",
      "problem": "I'm making an app in `NextJS` to practice, and I am having a hard time getting single data to delete from the database using the `findByIdAndDelete` function.\n\nCastError: Cast to ObjectId failed for value \"undefined\" (type string)\nat path \"_id\" for model \"Appointment\"\n\n./page.jsx:\n`import AddAppointment from \"./addAppointment\";\nimport DeleteAppointment from \"./deleteAppointment\";\nimport UpdateAppointment from \"./updateAppointment\";\n\nasync function getAppointment() {\n  const res = await fetch(\"http://localhost:3000/api/appointment\", {\n    method: \"GET\",\n    cache: \"no-store\",\n  });\n  // const data = await res.json();\n  return res.json();\n}\n\nasync function Appointment() {\n  const { appointment } = await getAppointment();\n\n  return (\n\n    \n      \n        \n      \n      \n        \n          \n            #\n            Nama\n            Tanggal\n            No Telp.\n            Terapis\n            Status\n            Action\n          \n        \n        \n          {appointment.map((row, i) => (\n            \n              {i + 1}\n              {row.name}\n              {row.date}\n              {row.phone}\n              {row.terapist}\n              {row.statust || \"unknown\"}\n              \n                \n                \n              \n            \n          ))}\n        \n      \n    \n  );\n}\n\nexport default Appointment;\n`\n./deleteAppointment.jsx:\n`\"use client\";\nimport { useState } from \"react\";\nimport { useRouter } from \"next/navigation\";\n\nexport default function DeleteAppointment() {\n  const [modal, setModal] = useState(false);\n\n  const router = useRouter();\n\n  async function handleDelete({ id }) {\n    await fetch(`http://localhost:3000/api/appointment?id=${id}`, {\n      method: \"DELETE\",\n    });\n\n    router.refresh();\n    setModal(false);\n  }\n\n  function handleChange() {\n    setModal(!modal);\n  }\n\n  return (\n    \n      \n        Delete\n      \n\n      \n\n      \n        \n          \n            Anda yakin untuk menghapus data \"\"?\n          \n\n          \n            \n              Close\n            \n            \n              Delete\n            \n          \n        \n      \n    \n  );\n}\n\n`\nAPI route:\n`import { connectMongoDB } from \"@/lib/mongodb\";\nimport Appointment from \"@/models/appointment\";\nimport { NextResponse } from \"next/server\";\n\nexport async function DELETE(req) {\n\n  const id = req.nextUrl.searchParams.get(\"id\");\n  await connectMongoDB();\n  await Appointment.findByIdAndDelete(id);\n  return NextResponse.json({ message: \"Appointment deleted\" }, { status: 200 });\n}\n`\nModel:\n`import mongoose, { Schema, models } from \"mongoose\";\n\nconst appointmentSchema = new Schema(\n  {\n    name: {\n      type: String,\n      required: true,\n    },\n    date: {\n      type: String,\n      required: true,\n    },\n    phone: {\n      type: String,\n      required: true,\n    },\n    terapist: {\n      type: String,\n      required: true,\n    },\n    statust: {\n      type: String,\n      required: true,\n    },\n  },\n  { timestamps: true }\n);\n\nconst Appointment =\n  models.Appointment || mongoose.model(\"Appointment\", appointmentSchema);\nexport default Appointment;\n`\n\nCastError: Cast to ObjectId failed for value \"undefined\" (type string)\nat path \"_id\" for model \"Appointment\"\n\nHaving a hard time to figure how to pass `id` and the difference between `id` and `_id`",
      "solution": "There's no parameter sent with the `onClick` function. So, the `id` parameter is `undefined`. Therefore, it sends an API request with an `undefined` id parameter like this: `http://localhost:3000/api/appointment?id=undefined` \nOn the client side, it is necessary to use an `ObjectID` to delete a document by using findByIdAndDelete() but the ID parameter comes from the request as `undefined`, not an `ObjectID`. Therefore, it can't find the related document from DB and returns this error.\nAs a result, the issue is related to the client side. Need to send the correct ID to be deleted with the API parameter. Therefore, need to get the related row or only the `id` value as props in the `DeleteAppointment` component. \nIt would be better to send the whole row object into props if you will need to use other fields as well:\n`// Send related row as prop \n\n`\nOtherwise, it's enough to send only `id`:\n`// Send only the related row's ID field as a prop\n\n`\nThen get the props as a parameter in the `DeleteAppointment` component:\n`export default function DeleteAppointment(props) {\n...\n}\n`\nAfter that, because of reaching the related id into the props, you can use `props.id` or `props.row.id` in the request id instead of the `id` parameter:\n`async function handleDelete() {\n  await fetch(`http://localhost:3000/api/appointment?id=${props.id}`, {\n      method: \"DELETE\",\n  });\n\n  router.refresh();\n  setModal(false);\n}\n`\nIf you send the whole row object as a prop, then you can use `props.row.id`:\n`async function handleDelete() {\n  await fetch(`http://localhost:3000/api/appointment?id=${props.row.id}`, {\n      method: \"DELETE\",\n  });\n\n  router.refresh();\n  setModal(false);\n}\n`\nHope, it's clear and helpful",
      "question_score": 4,
      "answer_score": 3,
      "created_at": "2024-04-06T11:07:53",
      "url": "https://stackoverflow.com/questions/78283785/cant-delete-by-id-from-mongodb-in-nextjs-app"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongoose",
      "question_id": 70440817,
      "title": "Nestjs ClassSerializerInterceptor doesn&#39;t display _id",
      "problem": "I have an issue properly exposing the _id using the Serializer.\nI use:\n`@UseInterceptors(ClassSerializerInterceptor)\n@SerializeOptions({ strategy: 'excludeAll' })\n`\nThe defined Class:\n`export class UpdatedCounts {\n    @Expose()\n    _id: ObjectId;\n    @Expose()\n    aCount: number;\n    @Expose()\n    bCount: number;\n\n    constructor(partial: Partial) {\n        Object.assign(this, partial);\n    }\n}\n`\nThe object in console.log() before it runs through the Serializer\n`{\n  _id: new ObjectId(\"61c2256ee0385774cc85a963\"),\n  bannerImage: 'placeholder2',\n  previewImage: 'placeholder',\n  aCount: 1,\n  bCount: 0,\n}\n`\nThe object being returned:\n`{\n  \"_id\": {},\n  \"aCount\": 1,\n  \"bCount\": 0\n}\n`\nSo what happened to my _id?\nI tried using string type instead of ObjectId but that also does not work\nI do not want to use @Exclude since there are 10 more props which I left out in the example console.log(), and it should be easier to exclude all and just use these 3",
      "solution": "Just use `@Transform`:\n`@Expose()\n@Transform((params) => params.obj._id.toString())\n_id: ObjectId;\n`\nYou can not just send ObjectId with JSON. You must convert it to a string.",
      "question_score": 4,
      "answer_score": 4,
      "created_at": "2021-12-21T20:42:30",
      "url": "https://stackoverflow.com/questions/70440817/nestjs-classserializerinterceptor-doesnt-display-id"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongoose",
      "question_id": 68585036,
      "title": "Issue with Date and Mongoose Typescript",
      "problem": "I'm facing the issue with the Mongoose official document founded here.\n```\n`import { Schema, Model, model } from 'mongoose';\n\nexport interface IUser {\n  name: string;\n  email: string;\n  avatar?: string;\n  created: Date;\n}\n\nconst schema = new Schema, IUser>({\n  name: { type: String, required: true },\n  email: String,\n  avatar: String,\n  created: { type: Date, default: Date.now },\n});\n\nexport const UserModel = model('User', schema);\n\n`\n```\nMy problem is the `created` type in `IUser` is not the same as in `schema` and got the error:\n```\n`Type '{ type: DateConstructor; default: () => number; }' is not assignable to type 'typeof SchemaType | Schema | Schema[] | readonly Schema[] | Function[] | ... 6 more ... | undefined'.\n  Types of property 'type' are incompatible.\n    Type 'DateConstructor' is not assignable to type 'Date | typeof SchemaType | Schema | undefined'.\n      Type 'DateConstructor' is missing the following properties from type 'typeof SchemaType': cast, checkRequired, set, getts(2322)\n(property) created?: typeof SchemaType | Schema | Schema[] | readonly Schema[] | Function[] | ... 6 more ... | undefined\n`\n```\nPlease let me know how to fix this.",
      "solution": "`Date.now()` is a function which returns `number`. Instead of it, try using `new Date()` only. Also need to make changes in the type of `createdAt` to `Number`.\nIn the given doc link, `createdAt` field type is `number` but here you have written `Date`.\n```\n`interface User {\n  name: string;\n  email: string;\n  avatar?: string;\n  createdAt: number;\n}\n`\n```\nOR\n`createdAt` and `updatedAt` are timestamps which can be used directly without specifying in the schema.\n```\n`const schema = new Schema, IUser>({\n  name: { type: String, required: true },\n  email: String,\n  avatar: String\n},{\n    timestamps: true\n});\n`\n```",
      "question_score": 4,
      "answer_score": 4,
      "created_at": "2021-07-30T05:15:24",
      "url": "https://stackoverflow.com/questions/68585036/issue-with-date-and-mongoose-typescript"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongoose",
      "question_id": 67838554,
      "title": "Documentation of @nestjs/mongoose",
      "problem": "I started learning NestJS, by reading documentation. currently, I'm using the @nestjs/mongoose package for MongoDB. unforunatley I can't find any documentation about this package.\nthe only docs I found is this https://docs.nestjs.com/techniques/mongodb\nhowever, it's missing lots of information, such as creating an index for the schema.\nwhere can I find additional information? how people on stack overflow know some of the answers if they are not in the documentation\nedit: I mean documentation of @nestjs/mongoose",
      "solution": "`@nestjs/mongoose` mostly is a simple wrapper around `mongoose` for Nest's DI context. Nest does have additional decorators, like `@Schema()` and `@Prop()` to allow for creating a class based representation of the schema and then the `SchemaFactory.createForClass` method to create the schema object that mongoose will later use. Any properties you can normally set when creating a prop, as described in the mongoose documentation should be passable to the `@Prop()`. The Typescript types are there to help you, and should give you good intellisense on what is and isn't possible.\nOther than that, the Nest team is slowly working on getting some API docs out, but it's a very large initiative that will take a lot of time.",
      "question_score": 4,
      "answer_score": 4,
      "created_at": "2021-06-04T15:58:37",
      "url": "https://stackoverflow.com/questions/67838554/documentation-of-nestjs-mongoose"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongoose",
      "question_id": 66558692,
      "title": "deleteMany showing 0 deleted but actually deleting the documents",
      "problem": "this controller is deleting all the todos of particular user but in response it is showing `{n: 0, deletedCount: 0, ok: 1}`, does anyone knows the solution of it?\n```\n`exports.deleteAll = async (req, res) => {\n\n    const { id } = req.user\n\n    console.log('id', id);\n\n    try {\n        // const deleteAllTodos = await TodoModel.findByIdAndDelete(id)\n\n        const deleteAllTodos = await TodoModel.deleteMany({ createdBy: id}, function (err) { \n\n        console.log('err', err) })\n\n        res.send({ success: true, message: \"All todos deleted successfully\", deleteAllTodos })\n\n    } catch (error) {\n\n        console.log('Error:', error.message);\n\n        res.status(500).json({\n            message: \"Internal server error\",\n            success: false,\n            error: error.message\n        });\n\n    }\n}\n`\n```",
      "solution": "From mongoose Query guides\n\nDon't mix using callbacks and promises with queries, or you may end up\nwith duplicate operations. That's because passing a callback to a\nquery function immediately executes the query, and calling then()\nexecutes the query again.\n\nSo what you did there was calling `deleteMany` twice, first using the callback and then using the `async/await`. The second attempt will try to delete nothing because the first attempt has already deleted them.\nYou could try using either one:\n```\n`...\nconst deleteAllTodos = await TodoModel.deleteMany({ createdBy: id }).exec();\nres.send({ \n    success: true, \n    message: \"All todos deleted successfully\", \n    deleteAllTodos \n})\n`\n```\nDon't worry about error handling using callback as you have already done it with the `try/catch` block which will catch if any error happened on `deleteMany`.",
      "question_score": 4,
      "answer_score": 4,
      "created_at": "2021-03-10T06:05:51",
      "url": "https://stackoverflow.com/questions/66558692/deletemany-showing-0-deleted-but-actually-deleting-the-documents"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongoose",
      "question_id": 79178330,
      "title": "TypeScript can&#39;t compile Mongoose query helper",
      "problem": "I'm working on a TypeScript project that uses Mongoose and is compiled with `tsc`. When I try to use a query helper with `.find()`:\n`import { InferRawDocType, Schema, model } from \"mongoose\";\n\nexport const mySchemaDefinition = {\n  key: \"string\",\n  value: \"string\"\n} as const;\nexport type RawSchema = InferRawDocType;\nexport const mySchema = new Schema(mySchemaDefinition, {\n  query: {\n    byKey: function (key: string) {\n      return this.where({ key: key });\n    }\n  }\n});\nexport const MyModel = model(\"MyModel\", mySchema);\n\nMyModel.find({ key: \"123\" }).exec(); // OK\nMyModel.findOne().byKey(\"123\").exec(); // OK\nMyModel.find().byKey(\"123\").exec(); // Error\n`\nTypeScript throws an error during transpilation:\n```\n`The 'this' context of type 'QueryWithHelpers> & FlatRecord & { _id: ObjectId; } & { __v: number; }, {}, FlatRecord, \"find\", Record>>(this: T, key: string) => T; }, { readonly key: \"string\"; readonly value: \"string\"; }> & { readonly key: \"string\"; readonly value: \"string\"; \n} & { _id: ObjectId; } & { __v: number; })[], Document> & FlatRecord & { _id: ObjectId; } & { __v: number; }, {}, FlatRecord, \"find\", Record>>(this: T, key: string) => T; }, { readonly key: \"string\"; readonly value: \"string\"; }> & { readonly key: \"string\"; readonly value: \"string\"; } & { _id: ObjectId; } & { __v: number; }, { byKey: > & FlatRecord & { _id: ObjectId; } & { __v: number; }, {}, FlatRecord, \"find\", Record>>(this: T, key: string) => T; }, { readonly key: \"string\"; readonly value: \"string\"; }, \"find\", {}>' is not assignable to method's 'this' of type 'Query> & FlatRecord & { _id: ObjectId; } & { __v: number; }, {}, FlatRecord, \"find\", Record>'.\n  The types returned by 'lean(...).exec()' are incompatible between these types.\n    Type 'Promise' is not assignable to type 'Promise'.\n      Type '({ readonly key: \"string\"; readonly value: \"string\"; } & { _id: ObjectId; } & { __v: number; })[]' is not assignable to type '{ readonly key: \"string\"; readonly value: \"string\"; } & \n{ _id: ObjectId; } & { __v: number; }'.\n        Type '({ readonly key: \"string\"; readonly value: \"string\"; } & { _id: ObjectId; } & { __v: number; })[]' is missing the following properties from type '{ readonly key: \"string\"; readonly value: \"string\"; }': key, value\n`\n```\nBut it only happens with `.find`, `.findOne` works fine.\nThe `this` context seems to be the right type.\nIs this a problem with `tsc` configuration or is there a Mongoose type error I can fix?\nBuild command:\n`tsc index.ts --esModuleInterop --outDir ./build --noErrorTruncation\n`\n`tsconfig.json` (mostly targets the React app that is in the same project):\n`{\n  \"compilerOptions\": {\n    \"target\": \"es5\",\n    \"lib\": [\"dom\", \"dom.iterable\", \"esnext\"],\n    \"allowJs\": true,\n    \"skipLibCheck\": true,\n    \"esModuleInterop\": true,\n    \"allowSyntheticDefaultImports\": true,\n    \"strict\": true,\n    \"forceConsistentCasingInFileNames\": true,\n    \"noFallthroughCasesInSwitch\": true,\n    \"module\": \"esnext\",\n    \"moduleResolution\": \"node\",\n    \"resolveJsonModule\": true,\n    \"isolatedModules\": true,\n    \"noEmit\": true,\n    \"jsx\": \"react-jsx\"\n  },\n  \"include\": [\"src\"]\n}\n`",
      "solution": "The error occurs because the byKey query helper is not properly typed to handle both find() and findOne() operations. The current implementation assumes a single document return type, but find() returns an array. I hope it will fix.\n```\n`export const mySchemaDefinition = {\n  key: \"string\",\n  value: \"string\"\n} as const;\nexport const mySchema = new Schema(mySchemaDefinition, {\nquery: {\nbyKey: function>(this: T, key: string) {\n return this.where({ key: key });\n    }\n  }\n});\n`\n```\nThis change makes the byKey query helper generic enough to work with both find() and findOne() operations.\nTo avoid using any in Typescript code you can define a specific type for your schema and use it in the Query type. Here's how you can do it:\n```\n`type MySchemaType = {\n  key: string;\n   value: string;\n};\n\nexport const mySchemaDefinition = {\n key: \"string\",\nvalue: \"string\"\n} as const;\nexport const mySchema = new Schema(mySchemaDefinition, {\n query: {\n byKey: function>(this: T, key: string) {\n return this.where({ key: key });\n  }\n}\n});\n`\n```",
      "question_score": 4,
      "answer_score": 3,
      "created_at": "2024-11-11T17:01:12",
      "url": "https://stackoverflow.com/questions/79178330/typescript-cant-compile-mongoose-query-helper"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongoose",
      "question_id": 74694473,
      "title": "Authentication failure on MongoDB 6.0",
      "problem": "Since I started to use MongoDB 6.0 I can no longer connect to the DB with Mongoose because of \"Authentication failure\". I kept everything the same as on MongoDB 5 but it no longer works with 6.\n```\n`MongoServerError: Authentication failed.\n    at Connection.onMessage (/home/leemorgan/market/market-api/node_modules/mongodb/lib/cmap/connection.js:207:30)\n    at MessageStream. (/home/leemorgan/market/market-api/node_modules/mongodb/lib/cmap/connection.js:60:60)\n    at MessageStream.emit (node:events:513:28)\n    at processIncomingData (/home/leemorgan/market/market-api/node_modules/mongodb/lib/cmap/message_stream.js:132:20)\n    at MessageStream._write (/home/leemorgan/market/market-api/node_modules/mongodb/lib/cmap/message_stream.js:33:9)\n    at writeOrBuffer (node:internal/streams/writable:392:12)\n    at _write (node:internal/streams/writable:333:10)\n    at Writable.write (node:internal/streams/writable:337:10)\n    at Socket.ondata (node:internal/streams/readable:766:22)\n    at Socket.emit (node:events:513:28) {\n  ok: 0,\n  code: 18,\n  codeName: 'AuthenticationFailed',\n  [Symbol(errorLabels)]: Set(1) { 'HandshakeError' }\n}\n`\n```\nCode for connecting with Mongoose:\n```\n`mongoose.connect(\"mongodb://127.0.0.1:27017/myDB\", {\n    useNewUrlParser: true,\n    useUnifiedTopology: true,\n    auth: {authSource: \"admin\"},\n    user: \"user\",\n    pass: process.env.MONGODB_PASS,\n    family: 4\n});\n`\n```\nI checked and re-checked the username/password a million times. I can log in to mongosh with the user, just not through Mongoose. This even happens when authentication is disabled. How do I get past this error?",
      "solution": "I don't see `auth: {authSource` in the documentation? Looks like it should be `authSource: \"admin\",`. Try:\n```\n`mongoose.connect(\"mongodb://127.0.0.1:27017/myDB\", {\n    useNewUrlParser: true,\n    useUnifiedTopology: true,\n    user: \"user\",\n    pass: process.env.MONGODB_PASS,\n    family: 4\n});\n`\n```",
      "question_score": 4,
      "answer_score": 3,
      "created_at": "2022-12-05T22:14:53",
      "url": "https://stackoverflow.com/questions/74694473/authentication-failure-on-mongodb-6-0"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongoose",
      "question_id": 73558930,
      "title": "Mongoose - Model.find({}) is not a function",
      "problem": "I am following a nodejs and mongoose tutorial and the only way i can query the collection is via the `.collection` property. Every tutorial i see, says `Model.find()` or `Model.findOne()` are legal but i keep getting the same error.\nMy model and schema are defined in the same file. i.e\n```\n`const fruitSchema = new mongoose.Schema ({\n    name: { type: String, required: true },\n    rating: { type: Number, required: true },\n    review: { type: String, required: true }\n});\n\nvar collectionName = \"fruit\";\nconst f = mongoose.model(collectionName, fruitSchema);\n\nconst fruit = new f({\n    name: \"Apple\",\n    rating: 7,\n    review: \"Pretty solid as a fruit.\"\n});\n\nfruit.save();\n`\n```\nI have populated above `fruits` collection with 5 documents. But struggling to query the model directly:\n`fruit.Find({}, callback);`\n\nThanks in advance.",
      "solution": "Please note that `find` is a method of the model so you need to your model `f` instead of `fruit`\n`var cursor = f.find();\n`\nDocumentation for reference: https://mongoosejs.com/docs/api.html#model_Model-find",
      "question_score": 4,
      "answer_score": 1,
      "created_at": "2022-08-31T18:15:08",
      "url": "https://stackoverflow.com/questions/73558930/mongoose-model-find-is-not-a-function"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongoose",
      "question_id": 70907062,
      "title": "Starting the instance of mongodb-memory-server failed on Unit Testing with Jest",
      "problem": "I have a micro service running with Express and save data to MongoDB using mongoose 6.x.x version. I am trying to test it with supertest and jest.\nI'm setting up some hooks to create an instance of Mongo Memory Server on version 8.x.x, using the beforeAll hooks as showed in the example:\n```\n`import { MongoMemoryServer } from 'mongodb-memory-server';\nimport mongoose from 'mongoose';\n \nlet mongod: any;\n \nbeforeAll(async () => {\n  mongod = await MongoMemoryServer.create();\n  const uri = mongod.getUri();\n  await mongoose.connect(uri);\n});\n`\n```\nThe mongod variable tries to reach a global scope.\nAt this point I get the error:\n\nStarting the instance failed, enable debug for more information.\n\nTherefore, I cannot use the instance in tests. I have enabled debug mode. And I get this errors about the mongo instance:\n\nMongoMS:MongoInstance Mongo[52463]: start: Starting Processes +8ms\nMongoMS:MongoInstance Mongo[52463]: _launchMongod: Launching Mongod\nProcess +1ms   MongoMS:MongoInstance Mongo[52463]: prepareCommandArgs\n+0ms   MongoMS:MongoInstance Mongo[52463]: prepareCommandArgs: final argument\narray:[\"--port\",\"52463\",\"--dbpath\",\"/var/folders/wt/d0s3lj915wd5pyvc9r7zdfx40000gn/T/mongo-mem--10268-fsntc7R8OJql\",\"--storageEngine\",\"ephemeralForTest\",\"--bind_ip\",\"127.0.0.1\",\"--noauth\"] +0ms\nMongoMS:MongoMemoryServer Mongo[unknown]: stop: Called .stop() method +23ms\nMongoMS:MongoMemoryServer Mongo[unknown]: stop: \"instanceInfo\" is not defined (never ran?) +0ms\n\nThe error is genereted because instance is not defined and I am trying to call mongod.stop in afterAll hooks to stop the instance.\nAnyone has an idea of this?",
      "solution": "The error was caused by an incompatibility in the architecture of my Mac. Normally the software would load, but it would not start the instance because the correct software was not running.\nOn Mac devices with ARM architecture, the use of Rosetta 2 is essential. As of version 7.x.x of MongoDB Memory Server the package downloads the software with correct architecture, automatically.",
      "question_score": 4,
      "answer_score": 3,
      "created_at": "2022-01-29T16:43:08",
      "url": "https://stackoverflow.com/questions/70907062/starting-the-instance-of-mongodb-memory-server-failed-on-unit-testing-with-jest"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongoose",
      "question_id": 68971749,
      "title": "Nest server not connecting to MongoDB cloud UnhandledPromiseRejectionWarning: MongoParseError: URI malformed",
      "problem": "My NestJS backend needs to connect to the mongodb cloud, I followed the docs from here\nThe following error threw up in the terminal:\n```\n`    (node:6920) UnhandledPromiseRejectionWarning: MongoParseError: URI malformed\n    at new ConnectionString (D:\\growth\\quizbackend\\quizbackend\\node_modules\\mongodb-connection-string-url\\src\\index.ts:113:13)\n    at Object.parseOptions (D:\\growth\\quizbackend\\quizbackend\\node_modules\\mongodb\\src\\connection_string.ts:249:15)\n    at new MongoClient (D:\\growth\\quizbackend\\quizbackend\\node_modules\\mongodb\\src\\mongo_client.ts:332:22)\n    at D:\\growth\\quizbackend\\quizbackend\\node_modules\\mongoose\\lib\\connection.js:785:16\n    at new Promise ()\n    at NativeConnection.Connection.openUri (D:\\growth\\quizbackend\\quizbackend\\node_modules\\mongoose\\lib\\connection.js:782:19)\n    at Mongoose.createConnection (D:\\growth\\quizbackend\\quizbackend\\node_modules\\mongoose\\lib\\index.js:275:10)\n    at Function. (D:\\growth\\quizbackend\\quizbackend\\node_modules\\@nestjs\\mongoose\\dist\\mongoose-core.module.js:60:63)\n    at Generator.next ()\n    at D:\\growth\\quizbackend\\quizbackend\\node_modules\\@nestjs\\mongoose\\dist\\mongoose-core.module.js:20:71\n(Use `node --trace-warnings ...` to show where the warning was created)\n(node:6920) UnhandledPromiseRejectionWarning: Unhandled promise rejection. This error originated either by throwing inside of an async function without a catch block, or by rejecting a promise which was not handled with .catch(). To terminate the node process on unhandled promise rejection, use the CLI flag `--unhandled-rejections=strict` (see https://nodejs.org/api/cli.html#cli_unhandled_rejections_mode). (rejection id: 2)\n(node:6920) [DEP0018] DeprecationWarning: Unhandled promise rejections are deprecated. In the future, \npromise rejections that are not handled will terminate the Node.js process with a non-zero exit code.\n`\n```\nMy app module code:\n```\n`import { Module } from '@nestjs/common';\nimport { AppController } from './app.controller';\nimport { AppService } from './app.service';\nimport { UsersModule } from './users/users.module';\nimport {MongooseModule} from '@nestjs/mongoose'\n@Module({\n  imports: [UsersModule,MongooseModule.forRoot('mongodb+srv://icfoajscijwq90j@cluster0.8rxa2.mongodb.net/nest-js-db?retryWrites=true&w=majority')],\n  controllers: [AppController],\n  providers: [AppService],\n})\nexport class AppModule {}\n`\n```\nI doubled check my username and password but they are correct, is there any need for encoding them or why the error is throwing up any explanation would be appreciated.",
      "solution": "Ensure that your username and password in the URI Connection string doesn't have any illegal characters.\n\nIf the username or password includes the following characters:\n```\n`: / ? # [ ] @\n`\n```\nthose characters must be converted using percent encoding.\n\nBased on their documentation: https://docs.mongodb.com/manual/reference/connection-string/",
      "question_score": 4,
      "answer_score": 3,
      "created_at": "2021-08-29T11:53:29",
      "url": "https://stackoverflow.com/questions/68971749/nest-server-not-connecting-to-mongodb-cloud-unhandledpromiserejectionwarning-mo"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongoose",
      "question_id": 72539795,
      "title": "How to connect mongoDB in react",
      "problem": "Such a problem that when I try to simply connect mongoDB in react\n```\n`import React, {Component} from 'react';\n\nexport default class App extends Component{\n\n  componentDidMount(){\n    const {MongoClient} = require('mongodb');\n\n    const client = new MongoClient('mongodb+srv://dafu4k:****@cluster0.jd26aac.mongodb.net/?retryWrites=true&w=majority');\n\n    const start = async () => {\n    try{\n        await client.connect();\n        console.log('Connected')\n    }\n    catch(e){\n      console.log(e)  \n    }\n  }\n  start();\n }\n\n  render(){\n    \n  return (\n\n    <>\n    Home page\n    \n  )\n\n  }\n}\n`\n```\nErrors of the same type appear in the browser itself\n\nModule not found: Error: Can't resolve 'os' in 'C:\\Users\\DaFu4\\OneDrive\\\u0420\u0430\u0431\u043e\u0447\u0438\u0439 >\u0441\u0442\u043e\u043b\\mongotest\\mongo-test\\node_modules\\ip\\lib'\nBREAKING CHANGE: webpack \nIf you want to include a polyfill, you need to:\n\nadd a fallback 'resolve.fallback: { \"os\": require.resolve(\"os-browserify/browser\") }'\ninstall 'os-browserify'\nIf you don't want to include a polyfill, you can use an empty module like this:\nresolve.fallback: { \"os\": false }\n\nThey differ in that the last line has different keys (`timers`,`crypto`,`http`, etc.)\nI watched how to connect mongoDB to react, everywhere they used a certain mongoose along with mongoDB, but I can\u2019t understand, maybe it\u2019s required to connect to react, or am I still doing something wrong? (in normal js everything works fine, the code has not changed)\n`src/index.js`\n```\n`import React from 'react';\nimport ReactDOM from 'react-dom/client';\n\nimport App from './App';\nconst root = ReactDOM.createRoot(document.getElementById('root'));\nroot.render(\n  \n    \n  \n);\n`\n```\n`package.json`\n```\n`{\n  \"name\": \"mongo-test\",\n  \"version\": \"0.1.0\",\n  \"private\": true,\n  \"dependencies\": {\n    \"@testing-library/jest-dom\": \"^5.16.4\",\n    \"@testing-library/react\": \"^13.3.0\",\n    \"@testing-library/user-event\": \"^13.5.0\",\n    \"crypto-browserify\": \"^3.12.0\",\n    \"mongodb\": \"^4.7.0\",\n    \"react\": \"^18.1.0\",\n    \"react-dom\": \"^18.1.0\",\n    \"react-scripts\": \"5.0.1\",\n    \"web-vitals\": \"^2.1.4\"\n  },\n  \"scripts\": {\n    \"start\": \"react-scripts start\",\n    \"build\": \"react-scripts build\",\n    \"test\": \"react-scripts test\",\n    \"eject\": \"react-scripts eject\"\n  },\n  \"eslintConfig\": {\n    \"extends\": [\n      \"react-app\",\n      \"react-app/jest\"\n    ]\n  },\n  \"browserslist\": {\n    \"production\": [\n      \">0.2%\",\n      \"not dead\",\n      \"not op_mini all\"\n    ],\n    \"development\": [\n      \"last 1 chrome version\",\n      \"last 1 firefox version\",\n      \"last 1 safari version\"\n    ]\n  }\n}\n\n`\n```",
      "solution": "We can not connect React JS to MongoDB because things don\u2019t work like this.\nFirst, we create a react app, and then for backend maintenance, we create API in node.js and express.js which is running at a different port and our react app running at a different port. for connecting React to the database (MongoDB) we integrate through API.\nCheck this link:\n[1]: https://www.geeksforgeeks.org/how-to-connect-mongodb-with-reactjs/#:~:text=First%2C%20we%20create%20a%20react,MongoDB)%20we%20integrate%20through%20API.",
      "question_score": 4,
      "answer_score": 3,
      "created_at": "2022-06-08T05:40:05",
      "url": "https://stackoverflow.com/questions/72539795/how-to-connect-mongodb-in-react"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongoose",
      "question_id": 70616348,
      "title": "toJSON method is ignored when populating sub documents",
      "problem": "I need to remove certain fields from the JSON response. I've used the `toJSON()` method for doing this. Here is the code of my modal method for it.\n`    User.methods.toJSON = function () {\n      let obj = this.toObject()\n    \n      delete obj.resetPasswordToken\n      delete obj.resetPasswordExpire\n      delete obj.otp\n      delete obj.otpExpire\n      return obj\n    }\n`\nThe above code is working fine but when I populate the User modal with my Media model it doesn't remove the fields that I deleted from the JSON response.\n`    const allMedia = await Media.find({}).populate({\n      path: 'uploadedBy', // this is the user document \n    })\n`\nThis is the code I wrote for populating the user model with the media model. But the problem is User gets populated with Media but it doesn't ignore the fields that I deleted from `toJSON()` method.\n```\n`    {\n        \"name\": null,\n        \"sizes\": [],\n        \"isPrivate\": false,\n        \"_id\": \"61d6d1a1fcaf7337f6f186de\",\n        \"path\": \"http://192.168.1.7:2121/uploads/2022/1/7b37e2bc-b313-4b08-abd6-101e99c36527.png\",\n        \"uploadedBy\": {\n            \"firstName\": \"abc\",\n            \"lastName\": \"abc\",\n            \"role\": \"ADMIN\",\n            \"resetPasswordToken\": \"77bda3f7794d305d7771fc23d932e1e9922df02c71b02c3564ad46b22ceac27e\",\n            \"resetPasswordExpire\": \"1640954443294\",\n            \"otp\": null,\n            \"otpExpire\": null,\n            \"_id\": \"61ceea9ce989f2d986fa9c5c\",\n            \"userName\": \"abc\",\n            \"email\": \"abc@gmail.com\",\n            \"createdAt\": \"2021-12-31T11:33:48.066Z\",\n            \"updatedAt\": \"2021-12-31T13:08:36.245Z\"\n        },\n        \"createdAt\": \"2022-01-06T11:25:21.839Z\",\n        \"updatedAt\": \"2022-01-06T11:25:21.839Z\"\n    }\n`\n```\nIf anyone can help, it would be appreciated.\nThanks in advance,",
      "solution": "You can define `pre` hook for each query for `Media` schema.\n`const mongoose = require('mongoose');\nconst Schema = mongoose.Schema;\nconst schema = new Schema(...);\n\nschema.pre('find', function () {\n  this.populate('uploadedBy');\n  this.select('-resetPasswordToken -resetPasswordExpire -otp -otpExpire');\n});\n`\nYou can add new `pre` hooks for other queries, just like for the `find`.\nYou can find more details here.",
      "question_score": 4,
      "answer_score": 2,
      "created_at": "2022-01-07T04:41:18",
      "url": "https://stackoverflow.com/questions/70616348/tojson-method-is-ignored-when-populating-sub-documents"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongoose",
      "question_id": 69161712,
      "title": "Unable to delete element from array of objects using mongoose",
      "problem": "I am trying to remove one element from array of objects in MongoDB.\nPlease find the schema structure below.\n\nI just want to remove one object by `status`, `ideaID` and `invitedBy`. Please find the query I am using for it,\n`    await User.findByIdAndUpdate(\n    currentUser,\n    { $pull: { \"invitationStatus.ideaId\": this.req.body.ideaId, \"invitationStatus.status\": \"pending\", \"invitationStatus.invitedBy\": getUserByNotificationId.createdBy._id } })\n`\nbut this query is not removing the specified object.",
      "solution": "You have to specify from which field you want to pull item. Change your query like this:\n`await User.findByIdAndUpdate(currentUser, { \n  $pull: { \n    invitationStatus: {\n      ideaId: this.req.body.ideaId,\n      status: \"pending\", \n      invitedBy: getUserByNotificationId.createdBy._id \n    }\n  } \n})\n`",
      "question_score": 4,
      "answer_score": 2,
      "created_at": "2021-09-13T13:16:16",
      "url": "https://stackoverflow.com/questions/69161712/unable-to-delete-element-from-array-of-objects-using-mongoose"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongoose",
      "question_id": 68110097,
      "title": "Mongoose insertMany - Missing createdAt",
      "problem": "I'm using mongoose in my Node micro service application.\nI have a model with 'createdAt' that created automatically:\n```\n`const mongoose = require('mongoose');\n\nconst recordSchema = new mongoose.Schema({\n    source: {\n        type: String,\n        required: true,\n        trim: true,\n        lowercase: true\n    },\n    coin: {\n        type: String,\n        required: true,\n        trim: true,\n        lowercase: true\n    },\n    rate: {\n        type: Number,\n        required: true\n    },\n    isError: {\n        type: Boolean,\n        required: true\n    },\n}, { timestamps: { createdAt: 'created_at' } });\n\nmodule.exports = mongoose.model('Record', recordSchema);\n`\n```\nAnd I try to insert a bulk of documents into it, like this:\n```\n`// Save the fetched data into the database.\nconst saveCoinsData = (coinsData) => {\n    RecordModel.collection.insertMany(coinsData, (err) => {\n        if (err) { logger.error(err); }\n    });\n};\n`\n```\nFor some reason, if I insert a single document the 'createdAt' is created as expected. But if I use the insertMany function, it's not working and no 'createdAt' (or created_at) is created. \nIs this normal?\nIs this a bug in mongoose?",
      "solution": "You're not using mongoose schema here. You're using default node drivers by using [schema].collection.[method]. Referring to this you can just use\n`RecordModel.insertMany` instead, that should add the timestamps.",
      "question_score": 4,
      "answer_score": 1,
      "created_at": "2021-06-24T07:29:41",
      "url": "https://stackoverflow.com/questions/68110097/mongoose-insertmany-missing-createdat"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongoose",
      "question_id": 66411822,
      "title": "Mongoose query where X is in both arrays and where Y is in only one array",
      "problem": "I am building a filtering system and I am stuck on a problem, please keep in mind that this is demo code, this is not the same project but the concept is the same, I replaced the names to make it as easily understandable as possible.\nI got a filled filter object which gets sent like this (it consists of an array with id's) - keep this one in mind:\n`const filters = {\n  companies: [\"company_a\", \"company_b\"], // this is an example, normally it a mongodb ObjectId(\"XXXXX\") list\n  states: [\"state_1\", \"state_2\"], // this is an example, normally it a mongodb ObjectId(\"XXXXX\") list\n}\n`\nIf you would take a look at both the `Company` and `State` collections, they would look something like this:\n`// MongoDB collection: `companies`\n[\n  {\n    \"id\": \"company_a\",\n    \"nationwide\": false\n  },\n  {\n    \"id\": \"company_b\",\n    \"nationwide\": true\n  }\n]\n// MongoDB collection: `states`\n[\n  {\n    \"id\": \"state_1\",\n    \"name\": \"Arizona\"\n  },\n  {\n    \"id\": \"state_2\",\n    \"name\": \"Texas\"\n  }\n]\n`\nThere is also a global collection which combines both of these, this is the collection I'm going to be using:\n`// MongoDB collection: `country_companies`\n[\n  /* record 1 */\n  {\n    \"_id\": ObjectId(\"XXXXX\"),\n    \"company\": {\n      \"_id\": \"company_a\",\n      \"nationwide\": false\n    },\n    \"state\": {\n      \"_id\": \"state_1\",\n      \"name\": \"Arizona\"\n    }\n  },\n  /* record 2 */\n  {\n    \"_id\": ObjectId(\"XXXXX\"),\n    \"company\": {\n      \"_id\": \"company_b\",\n      \"nationwide\": true\n    },\n    \"state\": {\n      \"_id\": \"state_2\",\n      \"name\": \"Texas\"\n    }\n  }\n]\n`\n\nNow, a company can be nationwide as well as state-oriented (as seen in above collection). So I have a repository like this:\n`export class CompanyRepository {\n  private companies: Company[];\n\n  public async initialize(): Promise {\n    if (this.companies.length > 0) throw new Error(\"Companies have already been initialized!\");\n    this.companies = await CompanyModel.find().exec();\n  }\n\n  public isCompanyNationwide(id: string): boolean {\n    return this.companies.some(company => company.id === id && company.nationwide === true);\n  }\n}\n`\n\nThe problem occurs that once I execute the query like this, with the filters at the top:\n`export class CompanyService {\n  public static async getCompaniesByFilters(filters: CompanyFilters): Promise {\n    const query: Record = {};\n    if (filters.companies.length > 0) query['company._id'] = { $in: filters.companies };\n    if (filters.states.length > 0) query['state._id'] = { $in: filters.states };\n    /* this results in a mongodb query:\n      {\n        \"company._id\": { $in: [\"company_a\", \"company_b\"] },\n        \"state._id\": { $in: [\"state_1\", \"state_2\"] }  \n      }\n    */\n    return await CountryCompanyModel.find(query).exec();\n  }\n}\n`\nWhat the above code basically does, is it adds the items based on if you selected them, in the end you get a query object. The problem there is that it has to be in BOTH arrays. So since `\"company_a\"` is nationwide, it shouldn't be searched in the states array.\n\nTo get a clear view of the point, here are some examples of how the system should work:\n```\n`User A selects `[\"company_a\"]`, without any states ->\n  Receives a list of all company_a records\n\nUser B selects `[\"company_a\"]`, with the state `[\"state_1\"]` ->\n  Receives list of all company_a in state_1 records\n\nUser C selects `[\"company_a\", \"company_b\"]` with the states `[\"state_1\"]` ->\n  Receives a list of all company_a in state_1, together with all company_b (since company B is nation-wide)\n\nUser D selects `[\"company_b\"]` with the states `[\"state_1\", \"state_2\"]` ->\n  Receives a list of all company_b, because company_b is nation wide so states filter should be ignored entirely.\n`\n```\nA solution I can think of is this:\n`import CompanyRepository from \"./company.repository\";\n\nconst stateWideCompanies = filters.companies.filter(companyId => \n  CompanyRepository.isCompanyNationWide(companyId) === false\n);\nconst nationWideCompanies = filters.companies.filter(companyId => \n  CompanyRepository.isCompanyNationWide(companyId) === true\n);\n\nconst countryCompaniesStates = await CountryCompanyModel.find({\"company._id\": { $in: stateWideCompanies }, \"state._id\": { $in: filters.states }).exec(); \nconst countryCompaniesNation = await CountryCompanyModel.find({\"company._id\": { $in: nationWideCompanies }).exec();\n\nconst companyList = [...countryCompaniesStates, ...countryCompaniesNation]\n`\nThis gives me what I want, however I think this should be able to be completed by the database. Because now I have to do two queries and combine them both, this does not look clean at all.\nI hope that I can do this in ONE query to the database. So either the query builder should be fixed or the query itself, I can't seem to get it working properly..",
      "solution": "All you need to do is built a smarter query with boolean logic, In this case all you want to do is allow a nationwide company to be fetched regardless of the selected states.\nHere's how I would do it:\n```\n`const query: Record = {};\n\nif (filters.companies.length > 0) {\n    query['company._id'] =  { $in: filters.companies };   \n}\nif (filters.states.length > 0) {\n    query['$or'] = [\n            {'state._id': { $in: filters.states }},\n            { 'company.nationwide': true}\n        ];\n}\n`\n```\nNow if a state is selected the query is either the `state._id` is in the selected query OR the company is nationwide.",
      "question_score": 4,
      "answer_score": 2,
      "created_at": "2021-02-28T18:19:27",
      "url": "https://stackoverflow.com/questions/66411822/mongoose-query-where-x-is-in-both-arrays-and-where-y-is-in-only-one-array"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongoose",
      "question_id": 77851304,
      "title": "mongoose - how to populate from parent to child document",
      "problem": "I'm just start to learn mongoose, I have 3 collections (User, Post, Comment), with their schema like this:\n`User`\n```\n`{\n  fullName: String,\n  email: String,\n}\n`\n```\n`Post`\n```\n`{\n  author: {type: mongoose.Schema.Types.ObjectId, required: true, ref: 'User'},\n  description: String\n  //other property not really usefull\n}\n`\n```\n`Comment`\n```\n`{\n  postId: {type: mongoose.Schema.Types.ObjectId, required: true, ref: 'Post'},\n  comment: [\n    {\n      user: {\n        type: mongoose.Schema.Types.ObjectId,\n        required: true,\n        ref: 'User',\n      },\n      value: String,\n      created: {type: Date, default: new Date().toISOString()},\n    },\n  ],\n}\n`\n```\nI want to fetch `post` detail `_id` with `comment` and detail of `user` `comment` and the result i'm expected is something like this:\n```\n`{\n  \"author\": \"65a24698a247a52009fb97af\",\n  \"description\": \"I'm the descriptor\",\n  \"comments\": [\n    {\n      \"postId\": \"65abb2cbacb4458e1be4f18b\",\n      \"comment\": [\n        {\n          \"created\": \"2024-01-20T13:28:26.807Z\",\n          \"user\": { \"fullName\": \"Flix\", \"email\": \"flixy121@gmail.com\" },\n          \"value\": \"Comment gw nih 123\"\n        },\n        {\n          \"created\": \"2024-01-20T13:28:26.807Z\",\n          \"user\": { \"fullName\": \"Flix\", \"email\": \"flixy121@gmail.com\" },\n          \"value\": \"Comment gw nih 123\"\n        },\n        {\n          \"user\": { \"fullName\": \"Flix\", \"email\": \"flixy121@gmail.com\" },\n          \"value\": \"Comment gw nih 123\",\n          \"created\": \"2024-01-20T12:04:08.394Z\"\n        }\n      ]\n    }\n  ],\n  \"id\": \"65abb2cbacb4458e1be4f18b\"\n}\n`\n```\nI've tried using `populate` to achieve what i want like this\n```\n`const result = await postModel\n  .findById(req.params.postId)\n  .populate('comments');\n`\n```\nbut i got an error\n\nCannot populate path `comments` because it is not in your schema. Set the `strictPopulate` option to false to override.\",\n\nanyone can help me how to achieve what i want? i have create https://mongoplayground.net/p/VebB03fCqUR but i can't show the expected result since i don't really know how to play with `aggregate`\n\nI don't mind if a suggestion need to restructure my schema, since I'm still learning",
      "solution": "Just to give you a complete example of how to do this with mongoose `populate` you would need to update your `Post` schema like so:\n`{\n   //...\n   author: {\n      type: mongoose.Schema.Types.ObjectId, \n      required: true, \n      ref: 'User'\n  },\n  description: String\n  comments: [{ //\nYou need to push the `ObjectId` of each `Comment` document into the `Post.comments` array but I presume you already know that.\nSince I'm here I would also change your `Comment` schema to this:\n`{\n   postId: {\n      type: mongoose.Schema.Types.ObjectId, \n      required: true,\n      ref: 'Post'\n   },\n   user: {\n      type: mongoose.Schema.Types.ObjectId, \n      required: true,\n      ref: 'User'\n   },\n   value: {\n      type: String, \n      required: true\n   },\n   created: {\n      type: Date, \n      default: Date.now //\nAs you can see, remove the `comments` array from `Comment` schema. If a comment can have a comment then yes you will need an array but your current schema does not suggest that. As an aside, mongoose has timestamps which are better than custom created at implementations.\nNow when you want to populate:\n`const result = await postModel.findById(req.params.postId)\n.populate({\n   path: 'comments', //\nYou can also get the `author`:\n`const result = await postModel.findById(req.params.postId)\n.populate('author')\n.populate({\n   path: 'comments', //",
      "question_score": 4,
      "answer_score": 1,
      "created_at": "2024-01-20T14:59:05",
      "url": "https://stackoverflow.com/questions/77851304/mongoose-how-to-populate-from-parent-to-child-document"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongoose",
      "question_id": 77658896,
      "title": "Get Tree data from MongoDB",
      "problem": "I have a collection in which data is stored in tree structure. Sample:\n```\n`[\n    {\n        \"_id\": \"entry1Id\",\n        \"name\": \"entry1\",\n        \"dependency\": \"entry2\"\n    },\n    {\n        \"_id\": \"entry2Id\",\n        \"name\": \"entry2\",\n        \"dependency\": \"entry3\"\n    },\n    {\n        \"_id\": \"entry3Id\",\n        \"name\": \"entry3\",\n        \"dependency\": \"entry4\"\n    },\n    {\n        \"_id\": \"entry4Id\",\n        \"name\": \"entry4\",\n        \"dependency\": \"entry5\"\n    },\n    {\n        \"_id\": \"entry5Id\",\n        \"name\": \"entry5\",\n        \"dependency\": \"\"\n    }\n]\n`\n```\nThe depth is not defined but wont be circular.\nNow i want a query to pass a name and get that item and all the nodes below that.\nI tried  doing it using multiple DB calls and its working, but i want to improve it.\n```\n`'use strict';\n\nmodule.exports = ({ $db }) => {\n    return async (req, res, next) => {\n\n        try {\n\n            const name = req.query.name;\n            const data = await getEntity($db, name);\n\n            return res.status(200).json({\n                success: true,\n                data\n            });\n\n        } catch (error) {\n            return next(error)\n        }\n    }\n}\n\nconst getEntity = async ($db, id) => {\n    const entity = await $db.models.Entity.findOne({ name }, { name: true, dependency: true });\n    if (entity && entity.dependency) {\n        entity.dependency = await getEntity($db, entity.dependency);\n    }\n    return entity;\n}\n`\n```\nBut i expect the output should be more like tree.\n\r\n\r\n`{\n    \"_id\": \"entry1Id\",\n    \"name\": \"entry1\",\n    \"dependency\": {\n        \"_id\": \"entry2Id\",\n        \"name\": \"entry2\",\n        \"dependency\": {\n            \"_id\": \"entry3Id\",\n            \"name\": \"entry3\",\n            \"dependency\": {\n                \"_id\": \"entry4Id\",\n                \"name\": \"entry4\",\n                \"dependency\": {\n                    \"_id\": \"entry5Id\",\n                    \"name\": \"entry5\",\n                    \"dependency\": \"\"\n                }\n            }\n        }\n    }\n}`\r\n\r\n\r\n\nI also tried $graphLookup but the output is not what i expect\n\r\n\r\n`const getEntityWithDependencies = async ($db, name) => {\n    const aggregationPipeline = [\n        {\n            $match: { name }\n        },\n        {\n            $graphLookup: {\n                from: 'entities', // Use the actual name of your collection\n                startWith: '$dependency',\n                connectFromField: 'dependency',\n                connectToField: 'name',\n                as: 'dependencies',\n            }\n        }\n    ];\n\n    const result = await $db.models.Entity.aggregate(aggregationPipeline).exec();\n    return result.length > 0 ? result[0] : null;\n}`\r\n\r\n\r\n\nThe output i got using above is something like\n\r\n\r\n`{\n    \"_id\": \"entry1Id\",\n    \"children\": [\n      {\n        \"_id\": \"entry2Id\",\n        \"dependency\": \"entry3Id\",\n        \"name\": \"entry2\"\n      },\n      {\n        \"_id\": \"entry3Id\",\n        \"dependency\": \"\",\n        \"name\": \"entry3\"\n      }\n    ],\n    \"dependency\": \"entry2Id\",\n    \"name\": \"entry1\"\n  },`",
      "solution": "Try some loop on top of graphLookup like:\n```\n`'use strict';\n\nconst helper = require('../../../utils/helper');\n\nmodule.exports = ({ $db }) => {\n    return async (req, res, next) => {\n\n        try {\n\n            const entityName = req.query.name;\n            let entity = await getEntityWithDependencies($db, entityName);\n\n            const dependencies = {}\n            if (entity.dependencies) {\n                entity.dependencies.forEach(dependency => {\n                    dependencies[dependency.name] = dependency;\n                });\n                delete entity.dependencies;\n            }\n\n            if (entity.dependency) entity = mapDependency(entity, dependencies)\n\n            return res.status(200).json({\n                success: true,\n                data: entity\n            });\n\n        } catch (error) {\n\n            return next(error)\n\n        }\n    }\n}\n\nconst mapDependency = (entity, dependencies) => {\n    entity['dependency'] = dependencies[entity['dependency']];\n    if (entity['dependency'].dependency)\n        entity['dependency'] = mapDependency(entity['dependency'], dependencies)\n    return entity\n}\n\nconst getEntityWithDependencies = async ($db, name) => {\n    const aggregationPipeline = [\n        {\n            $match: { name }\n        },\n        {\n            $graphLookup: {\n                from: 'entities',\n                startWith: '$dependency',\n                connectFromField: 'dependency',\n                connectToField: 'name',\n                as: 'dependencies',\n            }\n        }\n    ];\n\n    const result = await $db.models.Entity.aggregate(aggregationPipeline).exec();\n    return result.length > 0 ? result[0] : null;\n}\n`\n```",
      "question_score": 4,
      "answer_score": 1,
      "created_at": "2023-12-14T10:03:30",
      "url": "https://stackoverflow.com/questions/77658896/get-tree-data-from-mongodb"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongoose",
      "question_id": 75870721,
      "title": "Error MongoNotConnectedError: Client must be connected before running operations - how to get rid of the error?",
      "problem": "I am creating a project where I am using a seeds file to seed my DB with test data.\nWhen I run the file in node.js, here is the error I get:\n```\n`Database connected\nD:\\OUWork\\Year 6\\TM470\\Project\\node_modules\\mongodb\\lib\\operations\\execute_operation.js:24\n            throw new error_1.MongoNotConnectedError('Client must be connected before running operations');\n                  ^\n\nMongoNotConnectedError: Client must be connected before running operations\n    at executeOperationAsync (D:\\OUWork\\Year 6\\TM470\\Project\\node_modules\\mongodb\\lib\\operations\\execute_operation.js:24:19)\n    at D:\\OUWork\\Year 6\\TM470\\Project\\node_modules\\mongodb\\lib\\operations\\execute_operation.js:12:45\n    at maybeCallback (D:\\OUWork\\Year 6\\TM470\\Project\\node_modules\\mongodb\\lib\\utils.js:338:21)\n    at executeOperation (D:\\OUWork\\Year 6\\TM470\\Project\\node_modules\\mongodb\\lib\\operations\\execute_operation.js:12:38)\n    at Collection.insertOne (D:\\OUWork\\Year 6\\TM470\\Project\\node_modules\\mongodb\\lib\\collection.js:148:57)\n    at NativeCollection. [as insertOne] (D:\\OUWork\\Year 6\\TM470\\Project\\node_modules\\mongoose\\lib\\drivers\\node-mongodb-native\\collection.js:226:33)\n    at Model.$__handleSave (D:\\OUWork\\Year 6\\TM470\\Project\\node_modules\\mongoose\\lib\\model.js:309:33)\n    at Model.$__save (D:\\OUWork\\Year 6\\TM470\\Project\\node_modules\\mongoose\\lib\\model.js:388:8)\n    at D:\\OUWork\\Year 6\\TM470\\Project\\node_modules\\kareem\\index.js:387:18\n    at D:\\OUWork\\Year 6\\TM470\\Project\\node_modules\\kareem\\index.js:113:15 {\n  [Symbol(errorLabels)]: Set(0) {}\n}\n`\n```\nFor reference, here is the Javascript code for the seeds file:\n```\n`const mongoose = require('mongoose');\nconst MusicProduct = require('../database_models/musicproduct');\nconst BookProduct = require('../database_models/bookproduct');\n\nconst musicAlbums = require('./musicseeds');\nconst bookNovels = require('./bookseeds');\n\n// Connnect to MongoDB\nmongoose.connect('mongodb://127.0.0.1/music-bookApp');\nmongoose.set('strictQuery', false);\n\n// Logic to check that the database is connected properly\nmongoose.connection.on('error', console.error.bind(console, 'connection error:'));\nmongoose.connection.once('open', () => {\n    console.log('Database connected');\n});\n\n//Fill the Music products database with 20 random albums taken from the music seeds file\nconst musicSeedDB = async () => {\n    await MusicProduct.deleteMany({});\n    for (let i = 0; i  {\n    await BookProduct.deleteMany({});\n    for (let i = 0; i  {\n    mongoose.connection.close();\n});\n\nbookSeedDB().then(() => {\n    mongoose.connection.close();\n});\n`\n```\nOnce I run the seeds file, the database gets updated with the seeded information, but I would much prefer the error not be there before moving on.\nAny help would be appreciated :)\nThanks",
      "solution": "Replace this code\n```\n`musicSeedDB().then(() => {\n    mongoose.connection.close(); });\n\nbookSeedDB().then(() => {\n    mongoose.connection.close(); });\n`\n```\nto\n```\n`await musicSeedDB();\nawait bookSeedDB();\n`\n```",
      "question_score": 4,
      "answer_score": 1,
      "created_at": "2023-03-28T22:20:07",
      "url": "https://stackoverflow.com/questions/75870721/error-mongonotconnectederror-client-must-be-connected-before-running-operations"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongoose",
      "question_id": 74863962,
      "title": "How to access populated document&#39;s fields in typescript/mongoose?",
      "problem": "I'm populating the Customer model and transform it with helper class. But I cannot access populated field's subfields.\nOrder Model\n```\n`const orderSchema = new Schema({\n  customerID: {\n    type: Schema.Types.ObjectId,\n    ref: \"Customer\",\n    required: true,\n  }\n}\n\nexport const OrderModel = model(\"Order\", orderSchema);\n`\n```\nOrder interfaces\n```\n`export interface IOrder{\n  _id?: Schema.Types.ObjectId;\n  customerID: Schema.Types.ObjectId;\n}\n\nexport interface IOrderModel extends Model {\n}\n`\n```\nOrder Helper\n```\n`export class OrderHelper {\n  private customer: Schema.Types.ObjectId;\n\n  constructor(model: IOrder) {\n    this.customer = model.customerID;\n  }\n}\n`\n```\nOrder Service\n```\n`class OrderService {\n  private readonly orderRepository: OrderRepository;\n\n  constructor() {\n    this.orderRepository = new OrderRepository();\n  }\n\n  public async getProducts(request: Request) {\n    const products = await this.orderRepository.findOrders(request.body);\n    const data: ProductsHelper[] = [];\n    for (let key in products) data.push(new ProductsHelper(products[key]));\n    return data;\n  }\n}\n`\n```\nOrder Repository\n```\n`\nexport class OrderRepository {\n  constructor() {\n  }\n\npublic async findOrders(body: Partial) {\n  const orders = await OrderModel.find(body)\n    .populate({\n      path: \"customerID\",\n      select: [\"customerCode\", \"citizenship\", \"passportSerial\"],\n    }).lean();\n  return orders;\n}\n}\n`\n```\nFor example, I want to access Customer's customerCode field. But it gives me TypeError that there is no field of customerCode. How can I solve this problem? Is it related with interfaces/types or something else?",
      "solution": "You should declare customer's type in order interface and helper. Then you will be able to access its (given interface's) fields.",
      "question_score": 4,
      "answer_score": 1,
      "created_at": "2022-12-20T14:39:54",
      "url": "https://stackoverflow.com/questions/74863962/how-to-access-populated-documents-fields-in-typescript-mongoose"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongoose",
      "question_id": 74521557,
      "title": "How to populate in MongoDB without mongoose?",
      "problem": "I'm working on a university project and I need to populate an array of objects with ObjectIds, but I can't use mongoose in my project. I have two collections - subject and studyProgram.\nExample studyProgram document:\n`{\n  _id: ObjectId('111'),\n  name: \"Study program 1\"\n  description: \"Lorem ipsum dolor sit amet\",\n  language: \"en\",\n  subjects: [\n    {\n      id: ObjectId('222'),\n      optionality: \"selective\",\n      credits: 8,\n    },\n    {\n      id: ObjectId('333'),\n      optionality: \"selective\",\n      credits: 5\n    },\n  ],\n}\n`\nExample subject documents:\n`{\n  _id: ObjectId('222'),\n  name: \"Subject A\",\n  description: \"Subject A description.\",\n},\n{\n  _id: ObjectId('333'),\n  name: \"Subject B\",\n  description: \"Subject B description.\",\n}\n`\nI need to populate objects in `subjects` array with appropriate documents from subject collection based on `id`. Basically what I'm looking for is this result:\n`{\n  _id: ObjectId('111'),\n  name: \"Study program 1\"\n  description: \"Lorem ipsum dolor sit amet\",\n  language: \"en\",\n  subjects: [\n    {\n      \n      _id: ObjectId('222'),\n      name: \"Subject A\",\n      description: \"Subject A description.\",\n      optionality: \"selective\",\n      credits: 8,\n    },\n    {\n      _id: ObjectId('333'),\n      name: \"Subject B\",\n      description: \"Subject B description.\",\n      optionality: \"selective\",\n      credits: 5\n    },\n  ],\n}\n`\nSo far I have tried using the following $lookup:\n`{\n  $lookup: {\n    from: \"subject\",\n    localField: \"subjects.id\",\n    foreignField: \"_id\",\n    as: \"subjects\",\n  }\n}\n`\nbut this removes the `optionality` and `credits` attributes. Is there a way to achieve this without having to use mongoose? Thank you.",
      "solution": "Here's one way to do it.\n`db.studyProgram.aggregate([\n  {\n    \"$lookup\": {\n      \"from\": \"subject\",\n      \"localField\": \"subjects.id\",\n      \"foreignField\": \"_id\",\n      \"as\": \"subjectDocs\"\n    }\n  },\n  {\n    \"$set\": {\n      \"subjects\": {\n        \"$map\": {\n          \"input\": \"$subjects\",\n          \"as\": \"subject\",\n          \"in\": {\n            \"$mergeObjects\": [\n              {\n                \"$first\": {\n                  \"$filter\": {\n                    \"input\": \"$subjectDocs\",\n                    \"as\": \"doc\",\n                    \"cond\": {\"$eq\": [\"$$doc._id\", \"$$subject.id\"]}\n                  }\n                }\n              },\n              {\n                \"optionality\": \"$$subject.optionality\",\n                \"credits\": \"$$subject.credits\"\n              }\n            ]\n          }\n        }\n      },\n      \"subjectDocs\": \"$$REMOVE\"\n    }\n  }\n])\n`\nTry it on mongoplayground.net.\nIf `\"$first\"` is unavailable, tell your SysAdmin to upgrade MongoDB!  Until then, this will probably work.\n`db.studyProgram.aggregate([\n  {\n    \"$lookup\": {\n      \"from\": \"subject\",\n      \"localField\": \"subjects.id\",\n      \"foreignField\": \"_id\",\n      \"as\": \"subjectDocs\"\n    }\n  },\n  {\n    \"$set\": {\n      \"subjects\": {\n        \"$map\": {\n          \"input\": \"$subjects\",\n          \"as\": \"subject\",\n          \"in\": {\n            \"$mergeObjects\": [\n              {\n                \"$arrayElemAt\": [\n                  {\n                    \"$filter\": {\n                      \"input\": \"$subjectDocs\",\n                      \"as\": \"doc\",\n                      \"cond\": {\"$eq\": [\"$$doc._id\", \"$$subject.id\"]}\n                    }\n                  },\n                  0\n                ]\n              },\n              {\n                \"optionality\": \"$$subject.optionality\",\n                \"credits\": \"$$subject.credits\"\n              }\n            ]\n          }\n        }\n      },\n      \"subjectDocs\": \"$$REMOVE\"\n    }\n  }\n])\n`\nTry it on mongoplayground.net.",
      "question_score": 4,
      "answer_score": 1,
      "created_at": "2022-11-21T16:52:00",
      "url": "https://stackoverflow.com/questions/74521557/how-to-populate-in-mongodb-without-mongoose"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongoose",
      "question_id": 71274969,
      "title": "MongoDB: Aggregate query is not passing value inside the function",
      "problem": "I am facing a problem with the Mongoose aggregation query. I have the following schema which is an array of objects and contains the `endDate` value.\n```\n`[\n  {\n    \"id\": 1,\n    \"endDate\": \"2022-02-28T19:00:00.000Z\"\n  },\n  {\n    \"id\": 2,\n    \"endDate\": \"2022-02-24T19:00:00.000Z\"\n  },\n  {\n    \"id\": 3,\n    \"endDate\": \"2022-02-25T19:00:00.000Z\"\n  }\n]\n`\n```\nSo, during the aggregation result, I have to add a new field name `isPast`, It contains the boolean value, and perform the calculation to check if the `endDate` is passed or not. If it is already passed, then `isPast` will be `true` otherwise `false`.\nI am using the `isBefore` function from the moment library which returns the boolean. But inside this function facing a problem regarding passing the `endDate` value. `$endDate` is passing as a string, not a value.\nIs there a way to pass the value of endDate inside the function?\n`const todayDate = moment(new Date()).format(\"YYYY-MM-DD\");\n\ndb.collection.aggregate([\n  {\n    $addFields: {\n      \"isPast\": moment('$endDate', 'YYYY-MM-DD').isBefore(todayDate)\n    },\n\n  },\n\n])\n`",
      "solution": "You can achieve without momentjs. Use `$toDate` to convert date-time string to date\n```\n`db.collection.aggregate([\n  {\n    $addFields: {\n      \"isPast\": {\n        $gt: [\n          new Date(),\n          {\n            $toDate: \"$endDate\"\n          }\n        ]\n      }\n    }\n  }\n])\n\n`\n```\nSample Mongo Playground\n\nIf you just want to compare for date only:\n```\n`db.collection.aggregate([\n  {\n    $set: {\n      \"currentDate\": \"$$NOW\",\n      \"endDate\": {\n        $toDate: \"$endDate\"\n      }\n    }\n  },\n  {\n    $addFields: {\n      \"isPast\": {\n        $gt: [\n          {\n            \"$dateFromParts\": {\n              \"year\": {\n                $year: \"$currentDate\"\n              },\n              \"month\": {\n                $month: \"$currentDate\"\n              },\n              \"day\": {\n                \"$dayOfMonth\": \"$currentDate\"\n              }\n            }\n          },\n          {\n            \"$dateFromParts\": {\n              \"year\": {\n                $year: \"$endDate\"\n              },\n              \"month\": {\n                $month: \"$endDate\"\n              },\n              \"day\": {\n                \"$dayOfMonth\": \"$endDate\"\n              }\n            }\n          }\n        ]\n      }\n    }\n  }\n])\n`\n```\nSample Mongo Playground (Compare date only)",
      "question_score": 4,
      "answer_score": 1,
      "created_at": "2022-02-26T08:53:18",
      "url": "https://stackoverflow.com/questions/71274969/mongodb-aggregate-query-is-not-passing-value-inside-the-function"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongoose",
      "question_id": 68565101,
      "title": "MongoError code 66 immutable field in nodeJS",
      "problem": "I was trying to do an update operation. Now I'm getting an immutable error I knew the cause it was updating the key. I wonder because I'm not passing the _id from the API still I can see the _id if I console. I want to remove the _id from my object in order to fix this error. Code below\n```\n` router.put('/:id', (req, res) => {\n    //var findid= req.params.id;\n    if (!ObjectId.isValid(req.params.id))\n        return res.status(400).send(`No record with given id : ${req.params.id}`);\n\n        var alum = new Alumni({\n            fname: req.body.fname,\n            lname: req.body.lname,\n            contact: req.body.contact,\n            gender: req.body.gender,\n            dob: req.body.dob,\n            message: req.body.message,\n            city: req.body.city,\n            pincode: req.body.pincode,\n            state: req.body.state,\n            district: req.body.district,\n            password: req.body.password,\n            email: req.body.email\n\n    \n        });\n        //delete alum['_id'];\n    Alumni.findByIdAndUpdate(req.params.id, { $set: alum }, { new: true }, (err, doc) => {\n        if (!err) { res.send(doc); }\n        else { \n            console.log(alum);\n            console.log('Error in Alumni Update :' + JSON.stringify(err, undefined, 2)); }\n    });\n});\n`\n```\nHow can I resolve this error?",
      "solution": "Whenever JSON object is assigned using mongoose model like `new Alumni()`, by default it assigns `_id` field which is immutable so you don't need to use model with update object. Simply assign it as given below\n```\n`var alum = {\n    fname: req.body.fname,\n    lname: req.body.lname,\n    contact: req.body.contact,\n    gender: req.body.gender,\n    dob: req.body.dob,\n    message: req.body.message,\n    city: req.body.city,\n    pincode: req.body.pincode,\n    state: req.body.state,\n    district: req.body.district,\n    password: req.body.password,\n    email: req.body.email\n};\n`\n```",
      "question_score": 4,
      "answer_score": 1,
      "created_at": "2021-07-28T19:43:12",
      "url": "https://stackoverflow.com/questions/68565101/mongoerror-code-66-immutable-field-in-nodejs"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongoose",
      "question_id": 67157818,
      "title": "Why is execPopulate method required in mongoose for populating an existing document?",
      "problem": "I know the syntax of it and how it works, but I cannot understand the internal workings, why does a method chaining require another method at one time, but doesn't some other time?\nThis code works fine\n```\n`const cart = await Carts.findById(cartId).populate('product');\n`\n```\nBut this code does not\n```\n`let cart = await Carts.findById(cartId);\ncart = await cart.populate('product');\n`\n```\nAnd to make it work, we use the `execPopulate` method which works like this.\n```\n`let cart = await Carts.findById(cartId);\ncart = await cart.populate('product').execPopulate();\n`\n```\nNow, as far as I have read method chaining in javascript, the code should run fine without the `execPopulate` method too. But I cannot seem to understand why populate does not work on existing mongoose objects.",
      "solution": "`Carts.findById(cartId);` returns query Object.\nWhen you use `await Carts.findById(cartId);` it returns the document as it will resolve the promise and fetch the result.\n\nThe await operator is used to wait for a Promise.\n\n```\n`let cart = await Carts.findById(cartId); // cart document fetched by query\ncart = await cart.populate('product'); // you can't run populate method on document\n`\n```\n\nValid case\n```\n`const cartQuery = Carts.findById(cartId);\nconst cart = await cartQuery.populate('product');\n`\n```\n\n.execPopulate is method on document, while .populate works on query object.",
      "question_score": 3,
      "answer_score": 4,
      "created_at": "2021-04-19T09:40:20",
      "url": "https://stackoverflow.com/questions/67157818/why-is-execpopulate-method-required-in-mongoose-for-populating-an-existing-docum"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongoose",
      "question_id": 69901367,
      "title": "Mongoose NodeJS macOS Big Sur MongooseServerSelectionError: connect ECONNREFUSED ::1:27017",
      "problem": "I'm using brew mongodb-community on macOS Big Sur and can connect to mongo shell with mongosh. But when I try to run NodeJS and Mongoose app I get `MongooseServerSelectionError: connect ECONNREFUSED ::1:27017`. Why can I connect to mongodb with mongo shell but not NodeJS and Mongoose?\nEdit: Sample code from my NodeJS Mongoose code\n```\n`import mongoose from 'mongoose';\nmongoose.connect('mongodb://localhost:27017/mydb', {\n    useNewUrlParser: true,\n    useUnifiedTopology: true\n});\n`\n```",
      "solution": "My solution was to use `127.0.0.1` and NOT `localhost`. For some reason Mongoose can read `mongodb://127.0.0.1:27017/mydb` and NOT `mongodb://localhost:27017/mydb`",
      "question_score": 3,
      "answer_score": 13,
      "created_at": "2021-11-09T17:04:36",
      "url": "https://stackoverflow.com/questions/69901367/mongoose-nodejs-macos-big-sur-mongooseserverselectionerror-connect-econnrefused"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongoose",
      "question_id": 67855499,
      "title": "MongooseModule: Unable to connect to the database. On a dockerized Nestjs app with Mongo",
      "problem": "I'm trying to start a react-nestjs-mongo db application with docker-compose but for some reason it doesnt seems to work. `docker-compose --build` output shows something like this:\n\nserver     | [Nest] 32   - 06/06/2021, 3:10:25 AM   [MongooseModule] Unable to connect to the database. Retrying (7)... +33006ms\ndatabase   | {\"t\":{\"$date\":\"2021-06-06T03:10:27.406+00:00\"},\"s\":\"I\",\n\"c\":\"STORAGE\",  \"id\":22430,\n\"ctx\":\"WTCheckpointThread\",\"msg\":\"WiredTiger\nmessage\",\"attr\":{\"message\":\"[1622949027:406819][1:0x7f9ae2c3b700],\nWT_SESSION.checkpoint: [WT_VERB_CHECKPOINT_PROGRESS] saving checkpoint\nsnapshot min: 7, snapshot max: 7 snapshot count: 0, oldest timestamp:\n(0, 0) , meta checkpoint timestamp: (0, 0)\"}}\n\nHere's my docker-compose:\n```\n`version: \"3.5\"\n\nservices:   client:\n    container_name: client\n    build: ./client\n    ports:\n      - 3000:3000\n    depends_on:\n      - server\n\n  server:\n    container_name: server\n    build: ./server\n    ports:\n      - 8000:8000\n    depends_on:\n      - mongodb\n    links:\n      - mongodb\n\n  mongodb:\n    container_name: database\n    image: mongo\n    restart: always\n    ports:\n      - \"27017:27017\"\n    environment:\n      MONGO_INITDB_ROOT_USERNAME: root\n      MONGO_INITDB_ROOT_PASSWORD: example\n`\n```\nAnd my app.modules.ts file looks like this:\n```\n`import { Module } from '@nestjs/common';\nimport { AppController } from './app.controller';\nimport { AppService } from './app.service';\nimport { MongooseModule } from '@nestjs/mongoose';\n\n@Module({   \nimports: [\n    MongooseModule.forRoot('mongodb://localhost:27017/nestjs', {\n      useNewUrlParser: true,\n    }),   \n],   \ncontrollers: [AppController],   \nproviders: [AppService],\n })  \nclass AppModule {}\n`\n```\nCan someone explain me why this isn't working?",
      "solution": "You are using docker-compose . Thus you are not connecting 'localhost' but it should be address of your 'service' you defined in docker-compose file.\ninstead of 'mongodb://localhost:27017/nestjs' , you should use (in your case service name you defined in docker-compose is mongodb . You should use 'mongodb://mongodb:27017/nestjs'",
      "question_score": 3,
      "answer_score": 9,
      "created_at": "2021-06-06T05:14:10",
      "url": "https://stackoverflow.com/questions/67855499/mongoosemodule-unable-to-connect-to-the-database-on-a-dockerized-nestjs-app-wi"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongoose",
      "question_id": 75595278,
      "title": "Callback is no more accepting in Model.InsertMany() after version 7.0.0 update",
      "problem": "Hii guys I have currently working on a project where I am using mongoose and my I have latest version of 7.0.0.\nThe issue is that when I am trying to give a callback in Model.insertMany() operation in console its showing that\n** MongooseError: Model.insertMany() no longer accepts a callback**\nI added my code below. I want that is there any other alternatives  to solve this error.\nin app.js ----\n```\n`const express = require(\"express\");\nconst mongoose = require(\"mongoose\");// require mongoose\nconst bodyParser = require(\"body-parser\");\nconst date = require(__dirname + \"/date.js\");\n\nconst app = express();\n\napp.set('view engine', 'ejs');\n\napp.use(bodyParser.urlencoded({extended: true}));\napp.use(express.static(\"public\"));\n\n//create a todolistDB database and connect it\nmongoose.connect(\"mongodb://127.0.0.1/todolistDB\", {useNewUrlParser:true});\n\n//create a Schema of only name feild\nconst itemSchema = new mongoose.Schema({\n     name:String\n});\n\nconst Item = mongoose.model(\"Item\", itemSchema); // create a model of Items\n\nconst Item1= new Item({\n  name:\"Welcome to the todolist\"\n});\n\nconst Item2= new Item({\n  name:\"Click + button to add items\"\n});\n\nconst Item3= new Item({\n  name:\"click delete to remove item\"\n});\n\nconst defaultItem= [Item1, Item2, Item3 ]; // create a array of items doc\n\nItem.insertMany(defaultItem,{\n  if(err){\n    console.log(err);\n  }else{\n    console.log(\"Items added succesfully\");\n  }\n});\n`\n```",
      "solution": "The callback function that formally accompanied the insertMany() method has been deprecated (no longer in use).\nBut you can use this instead;\n```\n`Item.insertMany(defultItems).then(function () {\n    console.log(\"Successfully saved defult items to DB\");\n  }).catch(function (err) {\n    console.log(err);\n  });\n`\n```\nNote how the \"then\" and \"catch\" functions are continuous and a part of the \"insertMany\" method, as they are all joined using the dot (.) notation",
      "question_score": 3,
      "answer_score": 3,
      "created_at": "2023-02-28T18:14:56",
      "url": "https://stackoverflow.com/questions/75595278/callback-is-no-more-accepting-in-model-insertmany-after-version-7-0-0-update"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongoose",
      "question_id": 67069682,
      "title": "mongoose @prop() decorator type:Object in schema definition _ NestJS",
      "problem": "I recently moved to NestJs . I have some doubts for defining properties for mongoose schema .\nhow can I define object type inside schema :\nin express I defined such properties like this:\n```\n` foo:{\n        type: Object\n    },\n`\n```\nnow here I cannot use Object type. I did use any keyword too.",
      "solution": "I find my answer, we can define type to any and inside @prop() decorator use:\n{ type: Object }\n```\n`  @Prop({ type: Object })\n  foo: any;\n`\n```",
      "question_score": 3,
      "answer_score": 10,
      "created_at": "2021-04-13T08:22:08",
      "url": "https://stackoverflow.com/questions/67069682/mongoose-prop-decorator-typeobject-in-schema-definition-nestjs"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongoose",
      "question_id": 70692250,
      "title": "MongoDB MongooseError: Cannot call `collection.aggregate()` before initial connection is complete",
      "problem": "My website built with NextJS (and hosted with Vercel) is using Mongoose in my NodeJS API to connect to my MongoDB database.\nI get this weird error for only about 1% of the users:\n```\n`MongooseError: Cannot call `hotels.aggregate()` before initial connection is complete if `bufferCommands = false`. Make sure you `await mongoose.connect()` if you have `bufferCommands = false`.\nat NativeCollection. [as aggregate] (/var/task/node_modules/mongoose/lib/drivers/node-mongodb-native/collection.js:193:15)\nat /var/task/node_modules/mongoose/lib/aggregate.js:998:18\nat /var/task/node_modules/kareem/index.js:23:7\nat processTicksAndRejections (internal/process/task_queues.js:77:11)\n`\n```\n(`hotels` is the collection that is being displayed, and I'm calling the `.aggregate()` function on it in my API)\nI can't reliably reproduce the error, but have had it happen to me too.\nI'm following NextJS recommended way to connect to MonogoDB from their example:\n```\n`import mongoose from 'mongoose'\n\nconst MONGODB_URI = process.env.MONGODB_URI\n\nif (!MONGODB_URI) {\n  throw new Error(\n    'Please define the MONGODB_URI environment variable inside .env.local'\n  )\n}\n\n/**\n * Global is used here to maintain a cached connection across hot reloads\n * in development. This prevents connections growing exponentially\n * during API Route usage.\n */\nlet cached = global.mongoose\n\nif (!cached) {\n  cached = global.mongoose = { conn: null, promise: null }\n}\n\nasync function dbConnect() {\n  if (cached.conn) {\n    return cached.conn\n  }\n\n  if (!cached.promise) {\n    const opts = {\n      bufferCommands: false,\n    }\n\n    cached.promise = mongoose.connect(MONGODB_URI, opts).then((mongoose) => {\n      return mongoose\n    })\n  }\n  cached.conn = await cached.promise\n  return cached.conn\n}\n\nexport default dbConnect\n`\n```\nContrary to the error message, I am awaiting the mongoose.connect(). My API starts like this:\n```\n`import dbConnect from \"utils/dbConnect\";\nimport HotelModel from \"models/HotelModel\";\n\nexport default async function handler(req, res) {\n  await dbConnect();\n\n      try {\n        const pipeline = {}; // this is a big aggregation pipeline\n\n        const hotels = await HotelModel.aggregate(pipeline);\n\n        return res.status(200).json(hotels);\n      } catch{\n        ... \n      }\n}\n`\n```\nDoes anyone have ideas to why this might happen?",
      "solution": "Remove\n```\n`const opts = {\n      bufferCommands: false,\n}\n`\n```\nor set `bufferCommands: true` explicitly.\nVercel is running on AWS Lambda, and sometimes it causes problems with cached connections. One of the Vercel contributors complained about the corresponding PR in mongoose https://github.com/Automattic/mongoose/issues/9239#issuecomment-659000910\nThey recommended to avoid `bufferCommands: false` in the thread and removed it from the official mongoose recommendations a year later: docs/lambda.md",
      "question_score": 3,
      "answer_score": 9,
      "created_at": "2022-01-13T07:23:12",
      "url": "https://stackoverflow.com/questions/70692250/mongodb-mongooseerror-cannot-call-collection-aggregate-before-initial-conne"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongoose",
      "question_id": 72984395,
      "title": "React.Js how to redirect on form submit?",
      "problem": "everyone.\nSo I have a an app, where the user can create posts, if you are the owner of this post, you can click on delete button which will show a modal and you can delete the post.\nI can delete the post, but now I am trying to redirect the user back to the main page or any other page, however It doesn't work.\nI tried using history.push, but it is not available, window.location.replace(\"/\") doesn't work, and I even tried using this\n```\n`const navigate = useNavigate();\n`\n```\nand in the form submit\n```\n`navigate(\"/\");\n`\n```\nThis doesn't work at all, instead what happens is:\n\nThe DELETE request gets send\nThe post gets deleted from the database\nThe page reloads and doesn't redirect the user\nThe console throws fetch error\n\nThe user can see that the post is deleted only after he manualy switches pages, which I don'T want, I want the user to be automaticaly redirected once he pressed the Delete post button.\nThis is my package.json\n```\n`  \"dependencies\": {\n    \"@testing-library/jest-dom\": \"^5.16.4\",\n    \"@testing-library/react\": \"^13.3.0\",\n    \"@testing-library/user-event\": \"^13.5.0\",\n    \"@types/jest\": \"^27.5.2\",\n    \"@types/node\": \"^16.11.41\",\n    \"@types/react\": \"^18.0.14\",\n    \"@types/react-dom\": \"^18.0.5\",\n    \"react\": \"^18.2.0\",\n    \"react-dom\": \"^18.2.0\",\n    \"react-router-dom\": \"^6.3.0\",\n    \"react-scripts\": \"5.0.1\",\n    \"typescript\": \"^4.7.4\",\n    \"web-vitals\": \"^2.1.4\"\n  },\n`\n```\nAnd this is my code for form submit and delete on the Post\n```\n`const Post_page = () => {\n\n  const auth = useContext(context_auth);\n  \n  const { isLoading, error, sendRequest, clearError } = useHttpRequest();\n  const [showModal, setShowModal] = useState(false);\n  const userID = useParams().id;\n\n  const [title, setTitle] = useState();\n  const [postID, setPostId] = useState();\n  const [description, setDescription] = useState();\n  const [creator_name, setCreatorName] = useState();\n  const [creationTime, setCreationTime] = useState('');\n\n  const openModal = (event: any) =>  {\n      setShowModal(true);\n  }\n  const closeModal = (event: any) =>  {\n      setShowModal(false);\n  }\n  const onSubmit = async (event: any) => {\n    event.preventDefault();\n    console.log(\"aaaaaaaaaaaaa\");\n    try {\n      const url: string = `http://localhost:8000/api/posts/${postID}`;\n      await sendRequest(url, 'DELETE');\n      window.location.replace(\"/\");\n      closeModal(event);\n\n    } catch (err: any) {\n      console.log(err.message)\n    }\n    \n   \n    \n  }\n\n  useEffect(() => {\n    const fetchPosts = async () => {\n      try {\n\n        const url: string = `http://localhost:8000/api/posts/${userID}`;\n        const responseData = await sendRequest(url);\n\n       console.log(responseData.post);\n       const data = responseData.post;\n\n       setTitle(data.title);\n       setDescription(data.description);\n       setPostId(data.id);\n       const timeOfCreation = new Date(data.createdAt).toDateString();\n       setCreationTime(timeOfCreation);\n       setCreatorName(data.creator_name);\n\n      } catch (err) { }}\n\n      fetchPosts();\n  }, [sendRequest]);\n\n  return (\n    <>\n    {isLoading &&\n      Loading ...\n    }\n    { !isLoading && showModal &&  auth.isLoggedIn &&   \n      \n        <>\n        \n            Do you want to delete this post ?\n            \n          \n        \n        \n\n      \n    }\n\n    { !isLoading &&\n    \n     \n      \n      \n    }\n    \n  )\n}\n\nexport default Post_page\n`\n```\nEDIT: Posting backend code for post DELETE\nthis is my route\n```\n`route.delete('/:postID', deletePost)\n`\n```\nand this is the deletePost handler\n```\n`export const deletePost = async (req: Request, res: Response, next: NextFunction) => {\n    const postID = req.params.postID;\n    let post: any;\n\n    try {\n        post = await POST.findById(postID).populate('creator_id');\n    } catch (err: any) {\n        console.log(err.message)\n        const error = {\n            message: \"Couldn't delete POST !\",\n            code: 500\n        };\n    \n        return next(error);\n    }\n\n    if(!post){\n        const error = {\n            message: \"Post doesn't exist, so it could not be deleted !\",\n            code: 404\n        };\n    \n        return next(error);\n    }\n\n    try{\n       await post.remove();\n       post.creator.posts.pull(post);\n       await post.creator.save();\n       \n    }catch(err){\n        const error = {\n            message: \"Couldn't delete POST from DATABASE!\",\n            code: 500\n        };\n    \n        return next(error);\n    }\n    res.status(200).json({message: \"Post deleted !\"});\n}\n`\n```\nEDIT: ADDED SCHEMA FOR POST AND USER\nThis is my Schema for Post which is also connected to User Schema\n```\n`import mongoose from \"mongoose\";\n\nconst Schema = mongoose.Schema;\n\nconst post_Schema = new Schema({\n        title: { type: String, required: true, },\n        description: { type: String, required: true, },\n        imageURL: { type: String },\n        creator_id: { type: mongoose.Types.ObjectId, required: true, ref: 'User'  }, //relation bewtean post and user\n        creator_name: { type: String, required: true, ref: 'User'  }, //relation bewtean post and user\n    },\n    { timestamps: true }\n    );\n\nexport const POST: mongoose.Model = mongoose.model(\"Post\", post_Schema);\n`\n```\nand this is my User Schema\n```\n`import mongoose from \"mongoose\";\nimport mongooseUniqueValidator from \"mongoose-unique-validator\";\n\nconst Schema = mongoose.Schema;\nconst validator = mongooseUniqueValidator;\n\nconst user_Schema = new Schema({\n        username: { type: String, required: true, unique: true },\n        email: { type: String, required: true, unique: true },\n        password: { type: String, required: true, minlength: 3 },\n        user_image: { type: String },\n        posts: [{ type: mongoose.Types.ObjectId, required: true, ref: 'Post'  }], //relation bewtean post and user\n\n    },{ timestamps: true }\n    );\n\nuser_Schema.plugin(validator);\n\nexport const USER: mongoose.Model = mongoose.model(\"User\", user_Schema);\n`\n```",
      "solution": "From what you posted, the reason you don't redirect upon post deletion is because the backend prevents it from happening.\n\nto redirect the user after he has deleted the post, if you are using React-router-dom version 6, you need to use useNavigate(), because useHistory and  have been replaced by useNavigate(), so this is how you would redirect the user if he deletes something.\n\n```\n`    const navigate = useNavigate();\n    const onSubmit = async (event: any) => {\n       event.preventDefault();\n       try {\n           const url: string = `http://localhost:8000/api/posts/${postID}`;\n           await sendRequest(url, 'DELETE');\n           closeModal(event);\n           navigate(`/`);\n           } catch (err: any) {}\n    }\n`\n```\n\nThe reason your post doesn'T redirect is because, when you send the DELETE request the backend sends back an error, but before the error happens the Post gets deleted from the database, and its happening in the second try..catch block. The error happens here ->\n\n```\n`    post.creator.posts.pull(post);  //THIS IS WERE THE ERROR HAPPENS\n`\n```\nThe error happens because you are searching for creator in the Post Schema, but Post Schema doesn't have creator so it panics and throws an error, instead it has creator_id and creator_name, so you should replace it like this, if you are searching for user by ID\n```\n`   post.creator_id.posts.pull(post);\n`\n```\nSo your code on the backend should look like this (Its totaly same expect the        post.creator_id.posts.pull(post)\n```\n`export const deletePost = async (req: Request, res: Response, next: NextFunction) => {\n    const postID = req.params.postID;\n    let post: any;\n\n    try {\n        post = await POST.findById(postID).populate('creator_id');\n    } catch (err: any) {\n        console.log(err.message)\n        const error = {\n            message: \"Couldn't delete POST !\",\n            code: 500\n        };\n        return next(error);\n    }\n\n    if(!post){\n        const error = {\n            message: \"Post doesn't exist, so it could not be deleted !\",\n            code: 404\n        };\n        return next(error);\n    }\n    try{\n       await post.remove();\n       post.creator_id.posts.pull(post);\n       await post.creator_id.save();\n       \n    }catch(err){\n        const error = {\n            message: \"Couldn't delete POST from DATABASE!\",\n            code: 500\n        };\n    \n        return next(error);\n    }\n    res.status(200).json({message: \"Post deleted !\"});\n}\n\n`\n```",
      "question_score": 3,
      "answer_score": 4,
      "created_at": "2022-07-14T19:22:07",
      "url": "https://stackoverflow.com/questions/72984395/react-js-how-to-redirect-on-form-submit"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongoose",
      "question_id": 71470050,
      "title": "mongoose model.save() return TypeError: callback is not a function",
      "problem": "In my nestjs project I'm using mongoose and getting `TypeError: callback is not a function` while I'm trying to model.save().\n```\n`\"@nestjs/common\": \"^8.0.0\",\n\"@nestjs/config\": \"^1.2.0\",\n\"@nestjs/core\": \"^8.0.0\",\n\"@nestjs/mongoose\": \"^9.0.2\",\n\"mongoose\": \"^6.2.6\",\n`\n```\nIn my `tags.service.ts` for storing data I have this function:\n```\n`async create(createTagDto: CreateTagDto): Promise {\n   return await new this.tagModel(createTagDto).save();\n}\n`\n```\nAccording to Mongoose documentation by saving a document this should return a Promise.\nIn my `tags.controller.ts` I define endpoint by calling service function above by:\n```\n`@Post()\nasync create(@Body() createTagDto: CreateTagDto) {\n    return await this.tagsService.create(createTagDto);\n}\n`\n```\nTrying to post data to the endpoint, the new document is created in the database but the server return `Internal server error` with status code 500. The only description in the console is above mentioned `TypeError: callback is not a function` somewhere in `node_modules/mongoose/lib/statemachine.js:137:14`.\nDoes anyone experienced such an issue?",
      "solution": "Above mentioned error was caused by using nestjs global interceptor `app.useGlobalInterceptors(new ClassSerializerInterceptor(app.get(Reflector)))` in the main.ts bootstrap function. However the build-in ClassSerializerInterceptor cannot figure out a returned mongo Document, a custom interceptor have to be used to properly serialize returned data.",
      "question_score": 3,
      "answer_score": 5,
      "created_at": "2022-03-14T16:14:19",
      "url": "https://stackoverflow.com/questions/71470050/mongoose-model-save-return-typeerror-callback-is-not-a-function"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongoose",
      "question_id": 69545580,
      "title": "MongoDB and class-validator unique validation - NESTJS",
      "problem": "TL;DR\nI am trying to run mongoose query in my validator\n\nHello, I am trying to make a custom decorator which throws an error if a value for that field already exists. I am trying to use the mongoose model inside the class that validates the route. Unlike in resolver/controller, `@InjectModel()` does not work in validator class. My validator is like this\n```\n`import { getModelToken, InjectModel } from \"@nestjs/mongoose\";\nimport {\n  ValidationArguments,\n  ValidatorConstraint,\n  ValidatorConstraintInterface,\n} from \"class-validator\";\nimport { Model } from \"mongoose\";\nimport { User } from \"../schema/user.schema\";\n\n@ValidatorConstraint({ name: \"IsUniqueUser\", async: true })\nexport class UniqueValidator implements ValidatorConstraintInterface {\n  constructor(\n    @InjectModel(User.name)\n    private readonly userModel: Model,\n  ) {}\n\n  async validate(value: any, args: ValidationArguments) {\n    const filter = {};\n\n    console.log(this.userModel);\n    console.log(getModelToken(User.name));\n    filter[args.property] = value;\n    const count = await this.userModel.count(filter);\n    return !count;\n  }\n\n  defaultMessage(args: ValidationArguments) {\n    return \"$(value) is already taken\";\n  }\n}\n\n`\n```\nand my DTO that uses the above decorator is\n`\n\n@InputType({})\nexport class UserCreateDTO {\n  @IsString()\n  name: string;\n\n  @IsUniqueUser({\n    message: \"Phone number is already taken\",\n  })\n  @Field(() => String)\n  phone: string;\n}\n\n`\nThe console says\n`cannot read value count of undefined` implying that `userModel` is undefined.\nInShort\nI want to run the query in my validator. How can I do so?",
      "solution": "According to this issue (you can't inject a dependency)\nYou should to add in your `main.ts`\n```\n`import { useContainer } from 'class-validator';\nuseContainer(app.select(AppModule), {fallbackOnErrors: true}); \n`\n```\nThen you need to add your `UniqueValidator` to your module like an `@Injectable()` class\nso\n```\n`...\nproviders: [UniqueValidator],  \n...\n`\n```\nThen, in your DTO you can add:\n```\n`@Validate(UniqueValidator, ['email'], {\n    message: 'emailAlreadyExists',\n  })\n`\n```",
      "question_score": 3,
      "answer_score": 8,
      "created_at": "2021-10-12T20:48:00",
      "url": "https://stackoverflow.com/questions/69545580/mongodb-and-class-validator-unique-validation-nestjs"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb-query",
      "question_id": 67486907,
      "title": "Removing all spaces in a string field value in a MongoDB collection",
      "problem": "I have a mongodb collection named \"`users`\" with a few thousand users. Due to lack of validation users were able to create \"`username`\" with spaces in it. I.e, user was able to create username such as \"`I am the best`\" or \"` I am the best`\" or \"`I am the best `\" and so on. Since \"username\" field was not used in any form in the system it was just ok until now.\nFrom now on the client wants to use \"username\" field finally, that is, to make urls such as \"https://example.com/profile/{username}\".\nThe problem is this that the \"username\" field values have spaces at the beginning, middle and at the end as shown above, on random. So I want to remove them using a query.\nI am able to list all users using:\n```\n`db.users.find({username:{ \"$regex\" : \".*[^\\S].*\" , \"$options\" : \"i\"}}).pretty();\n`\n```\nWhat is the best approach to remove all spaces in username field and save them back? I am not sure how to update them in a single query.\nHelp is appreciated!\nPs. I actually need to write a code block to replace these usernames while checking for \"existing\" usernames so that there is no duplicate but I would still want to know how I do it if I need to do it using mongodb query.",
      "solution": "The problem is this that the \"username\" field values have spaces at the beginning, middle and at the end as shown above, on random. So I want to remove them using a query.\n\nMongoDB 4.4 or Above:\nYou can use update with aggregation pipeline starting from MongoDB 4.2,\n\n$replaceAll starting from MongoDB 4.4\nit will find white space and replace with blank\n\n```\n`db.users.update(\n  { username: { $regex: \" \" } },\n  [{\n    $set: {\n      username: {\n        $replaceAll: {\n          input: \"$username\",\n          find: \" \",\n          replacement: \"\"\n        }\n      }\n    }\n  }],\n  { multi: true }\n)\n`\n```\nPlayground\n\nMongoDB 4.2 or Above:\nYou can use update with aggregation pipeline starting from MongoDB 4.2,\n\n`$trim` to remove white space from both left and right\n`$split` to split `username` by space and result array\n`$reduce` to iterate loop of above split result\n`$concat` to concat `username`\n\n```\n`db.users.update(\n  { username: { $regex: \" \" } },\n  [{\n    $set: {\n      username: {\n        $reduce: {\n          input: { $split: [{ $trim: { input: \"$username\" } }, \" \"] },\n          initialValue: \"\",\n          in: { $concat: [\"$$value\", \"$$this\"] }\n        }\n      }\n    }\n  }],\n  { multi: true }\n)\n`\n```\nPlayground\n\nMongoDB 3.6 or Above:\n\n`find` all users and loop through forEach\n`replace` to apply pattern to remove white space, you can update pattern as per your requirement\n`updateOne` to update updated `username`\n\n```\n`db.users.find({ username: { $regex: \" \" } }, { username: 1 }).forEach(function(user) {\n  let username = user.username.replace(/\\s/g, \"\");\n  db.users.updateOne({ _id: user._id }, { $set: { username: username } });\n})\n`\n```",
      "question_score": 6,
      "answer_score": 10,
      "created_at": "2021-05-11T14:37:16",
      "url": "https://stackoverflow.com/questions/67486907/removing-all-spaces-in-a-string-field-value-in-a-mongodb-collection"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb-query",
      "question_id": 66167710,
      "title": "MongoDB decrement until zero",
      "problem": "I would like to achieve an operation in MongoDB that would be analogous to `doc.value = max(doc.value - amount, 0)`. I could do it by fetching document, updating its value then saving it, but is it possible with an atomic operation to avoid problems with synchronisation of parallel decrements?",
      "solution": "It is, in fact, possible to achieve with a single operation. All you need is an aggregation pipeline inside an update operator.\nLet's say you have a doc that looks like this:\n```\n`{\n    \"key\": 1,\n    value: 30\n}\n`\n```\nYou want to subtract x from `value` and if the resulting value is less than zero, set `value` to 0, otherwise set it to whatever `value` - x is. Here is an update aggregator you need. In this example I am subtracting 20 from `value`.\n```\n`db.collection.update({\n  key: 1\n},\n[\n  {\n    $set: {\n      \"value\": {\n        $cond: [\n          {\n            $gt: [\n              {\n                $subtract: [\n                  \"$value\",\n                  20\n                ]\n              },\n              0\n            ]\n          },\n          {\n            $subtract: [\n              \"$value\",\n              20\n            ]\n          },\n          0\n        ]\n      }\n    }\n  }\n])\n`\n```\nThe result will be:\n```\n`  {\n    \"key\": 1,\n    \"value\": 10\n  }\n`\n```\nBut if you change 20 to, say 44, the result is:\n```\n`  {\n    \"key\": 1,\n    \"value\": 0\n  }\n`\n```\nHere is a Playground for you: https://mongoplayground.net/p/Y9yO6v9Oca8",
      "question_score": 6,
      "answer_score": 6,
      "created_at": "2021-02-12T08:18:25",
      "url": "https://stackoverflow.com/questions/66167710/mongodb-decrement-until-zero"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb-query",
      "question_id": 68283988,
      "title": "DocumentDB | Mongoerror: aggregation stage not supported: &#39;$lookup on multiple join conditions and uncorrelated subquery",
      "problem": "I am a newbie to DocumentDB. Trying to join two collections and perform filters like query, sort and projection.\nI have two collections and need to perform filters on both the collections and the result should be a embeded list (Which should contain both items and subitems).\nFirst collection will hold the id's of the second collection in a Subitems field.\nHere is the first collection (Items):\n```\n`{\n    \"_id\": 897979789,\n    \"Country\": \"India\",\n    \"State\": \"XXX\",\n    \"Subitems\": [ ObjectId(\"44497979789\"), ObjectId(\"333897979789\") ]\n},\n{\n    \"_id\": 987979798,\n    \"Country\": \"Australia\",\n    \"State\": \"YYY\",\n    \"Subitems\": [ ObjectId(\"97979789444\"), ObjectId(\"897979789333\") ]\n}\n`\n```\nHere is the second collection:\n```\n`{\n\"_id\": 44497979789,\n\"EmployeeName\": \"Krishna\",\n\"Occupation\": \"Engineer\",\n\"ProjectsHandled\": [\n    {\n        \"ProjectName\": \"Project1\"\n    }\n]\n},\n{\n    \"_id\": 333897979789,\n    \"EmployeeName\": \"krish\",\n    \"Occupation\": \"CTO\",\n    \"ProjectsHandled\": [\n        {\n            \"ProjectName\": \"Project1\"\n        }\n    ]\n},\n{\n    \"_id\": 97979789444,\n    \"EmployeeName\": \"name\",\n    \"Occupation\": \"CEO\",\n    \"ProjectsHandled\": [\n        {\n            \"ProjectName\": \"Project2\"\n        }\n    ]\n},\n{\n    \"_id\": 897979789333,\n    \"EmployeeName\": \"name1\",\n    \"Occupation\": \"manager\",\n    \"ProjectsHandled\": [\n        {\n            \"ProjectName\": \"Project3\"\n        }\n    ]\n}\n`\n```\nHere is my query:\n```\n`let subItemPipeline: any[] = [{ $match: config.subItemsQuery }];\n        if(Object.keys(config.subItemsSort || {}).length)\n            subItemPipeline.push({$sort: config.subItemsSort});\n        if(Object.keys(config.subItemsProjection || {}).length)\n            subItemPipeline.push({$project: config.subItemsProjection});\n            \n        let query: any[] = [\n            { \n                $match: config.itemsQuery\n            },\n            {\n                $lookup: {\n                    from: \"Subitems\",\n                    pipeline: subItemPipeline,\n                    as: \"Subitems\"\n                }\n            }\n        ];\n\n        if(Object.keys(config.overallQuery || {}).length) query.push({$match: config.overallQuery});\n        if(Object.keys(config.itemsSort || {}).length) query.push({$sort: config.itemsSort});\n        if(Object.keys(config.itemsProjection || {}).length) query.push({$project: config.itemsProjection});\n\n        const items = await collection.aggregate(query).toArray();\n`\n```\nIt is working fine in MongoDB, but am facing the following error in DocumentDB:\n```\n`Mongoerror: aggregation stage not supported: '$lookup on multiple join conditions and uncorrelated subquery\n`\n```\nAny assistance will be appreciated",
      "solution": "From the DocumentDB docs:\n\nAmazon DocumentDB supports the ability to do equality matches (for example, left outer join) but does not support uncorrelated subqueries.\n\nThe Mongo docs specifically mention this:\nequality match has the following syntax:\n```\n`{\n   $lookup:\n     {\n       from: ,\n       localField: ,\n       foreignField: ,\n       as: \n     }\n}\n`\n```\nThis is the `$lookup` syntax supported by DocumentDB, the syntax you're trying to use is the \"new\" `$lookup` syntax added in version 3.6 which is what they call \"Uncorrelated Sub-queries\", unfortunately it is not supported by DocumentDB as of right now.\nThe easiest \"quick\" solution would be to add a new field to all `Subitems` documents with a default value. this will allow you to use the `equality match` lookup.\n--- EDIT ---\nI'm suggesting you add to every single document in `Subitems` a new field.\n```\n`Subitems.updateMany(\n    {\n        \n    },\n    {\n        $set: {\n            n: 1\n        }\n    }\n)\n`\n```\nAnd the same to your employee's collection:\n```\n`employee.updateMany(\n    {\n        \n    },\n    {\n        $set: {\n            n: 1\n        }\n    }\n)\n`\n```\nNow you can use the `equality` lookup syntax:\n```\n`{\n    $lookup: {\n        from: 'Subitems',\n        localField: 'n',\n        foreignField: 'n',\n        as: 'Subitems',\n    },\n}\n... continue to filter sub items.\n`\n```\nNote that this is a toy example, it will literally lookup the entire sub item collection before you get to filter it.\nIf you really want a good scaleable solution you have to just split it into 2 different calls.",
      "question_score": 6,
      "answer_score": 3,
      "created_at": "2021-07-07T12:08:34",
      "url": "https://stackoverflow.com/questions/68283988/documentdb-mongoerror-aggregation-stage-not-supported-lookup-on-multiple-j"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb-query",
      "question_id": 73225133,
      "title": "Difference between the UpdateOne() and findOneAndUpdate Methods in Mongo DB",
      "problem": "What is the difference between the `UpdateOne()` and the `findOneAndUpdate()` methods in Mongo DB?\nI can't seem o understand their differences. Would appreciate it if a demonstrative example using `UpdateOne()` and `findOneAndUpdate` could be used.",
      "solution": "Insert a document in an otherwise empty collection using the `mongo-shell` to start:\n```\n`db.users.insertOne({name: \"Jack\", age: 11})\n`\n```\nUpdateOne\n```\n`db.users.updateOne({name: \"Jack\"}, {$set: {name: \"Joe\"}})\n`\n```\nThis operation returns an `UpdateResult`.\n```\n`{ acknowledged: true,\n  insertedId: null,\n  matchedCount: 1,\n  modifiedCount: 1,\n  upsertedCount: 0 }\n`\n```\nFindOneAndUpdate\n```\n`db.users.findOneAndUpdate({name: \"Joe\"}, {$set: {name: \"Jill\"}})\n`\n```\nThis operation returns the document that was updated.\n```\n`{ _id: ObjectId(\"62ecf94510fc668e92f3cecf\"),\n  name: 'Joe',\n  age: 11 }\n`\n```\n`FindOneAndUpdate` is preferred when you have to update a document and fetch it at the same time.",
      "question_score": 5,
      "answer_score": 7,
      "created_at": "2022-08-03T19:00:30",
      "url": "https://stackoverflow.com/questions/73225133/difference-between-the-updateone-and-findoneandupdate-methods-in-mongo-db"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb-query",
      "question_id": 66688549,
      "title": "Mongodb update all the documents with unique id",
      "problem": "I have collection with name `products` with almost 100k documents. I want to introduce a new key called `secondaryKey` with unique value `uuid`  in all the documents.\nI do this using `nodejs`.\nProblem I am facing:-\nWhen I try the below query,\n```\n`db.collection('products').updateMany({},{\"$set\":{secondaryKey: uuid()}});\n`\n```\nHere it updates all the documents with same `uuid` value,\nI try with loop to update document one by one,but here issues is I don't have `filter` value in `updateOne` because I want to update all the documents.\nCan anyone please help me here.\nThanks :)",
      "solution": "If you are using MongoDB version >= 4.4 You can try this:\n`db.products.updateMany(\n    {},\n    [\n        {\n            $set: {\n                secondaryKey: {\n                    $function: {\n                        body: function() {\n                            return UUID().toString().split('\"')[1];\n                        },\n                        args: [],\n                        lang: \"js\"\n                    }\n                }\n            }\n        }\n    ]\n);\n`\nOutput\n`[\n  {\n    \"_id\": ObjectId(\"...\"),\n    \"secondaryKey\": \"f41b15b7-a0c5-43ed-9d15-69dbafc0ed29\"\n  },\n  {\n    \"_id\": ObjectId(\"...\"),\n    \"secondaryKey\": \"50ae7248-a92e-4b10-be7d-126b8083ff64\"\n  },\n  {\n    \"_id\": ObjectId(\"...\"),\n    \"secondaryKey\": \"fa778a1a-371b-422a-b73f-8bcff865ad8e\"\n  }\n]\n`",
      "question_score": 5,
      "answer_score": 6,
      "created_at": "2021-03-18T10:49:25",
      "url": "https://stackoverflow.com/questions/66688549/mongodb-update-all-the-documents-with-unique-id"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb-query",
      "question_id": 70338503,
      "title": "How to perform lead and lag in MongoDB",
      "problem": "I am  using STudio 3T and I have query like this:\n```\n`select [Dashbo],lead([Dashbo]) over(order by [Entered Date])\nfrom ATest_prevback;\n`\n```\nThis is giving me and error. How to perform this in MongoDB? Can someone give me an example?\nThanks,\nAdi",
      "solution": "Starting from MongoDB v5.0+, it can be done by using $shift in $setWindowFields.\n`db.collection.aggregate([\n  {\n    \"$setWindowFields\": {\n      \"partitionBy\": null,\n      \"sortBy\": {\n        \"entered_date\": 1\n      },\n      \"output\": {\n        lag: {\n          $shift: {\n            output: \"$Dashbo\",\n            by: -1,\n            default: \"Not available\"\n          }\n        },\n        lead: {\n          $shift: {\n            output: \"$Dashbo\",\n            by: 1,\n            default: \"Not available\"\n          }\n        }\n      }\n    }\n  }\n])\n`\nHere is the Mongo playground for your reference.",
      "question_score": 5,
      "answer_score": 5,
      "created_at": "2021-12-13T18:17:11",
      "url": "https://stackoverflow.com/questions/70338503/how-to-perform-lead-and-lag-in-mongodb"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb-query",
      "question_id": 68225868,
      "title": "How to upsert with MongoDB array?",
      "problem": "I am trying to insert, update values in the MongoDB array.\nMy MongoDB version is 4.0.5.\nHere is my collection :\n```\n`{\n    'id': 1,\n    'array': [{\n        'code': 'a'\n    }, {\n        'code': 'b'\n    }]\n}\n`\n```\nI am trying to make some upsert queries to insert/update into an array but I don't found a good solution until then.\nMy filters are :\n\n`'id'` (to point the correct document)\n`'array.code'` (to point the correct array cell)\n\nIf the document exists in the collection but there is no cell with `'code': 'c'`\n\n```\n`db.test.update({\n        'id': 1\n    }, {\n        $set: {'array.$[elem].test':'ok'}\n    }, {\n        upsert: true,\n        arrayFilters: [{'elem.code': 'c'}]\n    }\n)\n`\n```\nI have no error but no upsert too.\nI want to insert the element in the array like this :\n```\n`// Desired result\n{\n    'id': 1,\n    'array': [{\n        'code': 'a'\n    }, {\n        'code': 'b'\n    }, {\n        'code': 'c'\n        'test': 'ok'\n    }]\n}\n`\n```\n\nIf the document doesn't exist in the collection\n\n```\n`db.test.update({\n        'id': 3\n    }, {\n        $set: {'array.$[elem].test':'ok'}\n    }, {\n        upsert: true,\n        arrayFilters: [{'elem.code': 'a'}]\n    }\n)\n`\n```\nIn that case, I have this error :\n\n`WriteError: The path 'array' must exist in the document in order to apply array updates., full error: {'index': 0, 'code': 2, 'errmsg': \"The path 'array' must exist in the document in order to apply array updates.\"}`\n\nI want to upsert a new document with elements of the query like this :\n```\n`// Desired result\n{\n    'id': 3,\n    'array': [{\n        'code': 'a'\n        'test': 'ok'\n    }]\n}\n`\n```\n`upsert: true` in the query parameters doesn't seem to work with the array.\nYour help will be highly appreciated.",
      "solution": "The upsert is not effective in the array, If you do update MongoDB version from 4.0.5 to 4.2 then you can able to use update with aggregation pipeline starting from MongoDB 4.2,\nCase 1: If the document exists in the collection but there is no cell with `'code': 'c'`:\n```\n`var id = 2;\nvar item = { code: \"c\", test: \"ok\" };\n`\n```\nPlayground\nCase 2: If the document doesn't exist in the collection:\n```\n`var id = 3;\nvar item = { code: \"a\", test: \"ok\" };\n`\n```\nPlayground\n\n`$ifNull` to check if the field does not exist then return empty\n`$cond` to check if input `code` is in array\n\nyes, then `$mep` to iterate loop of `array` and check condition if match then update other fields otherwise return an empty object\n`$mergeObjects` to merge current object with updated fields\nno, `$concatArrays` to concat current `array` with new item object\n\n```\n`db.collection.update(\n  { \"id\": id },\n  [{\n    $set: {\n      array: {\n        $cond: [\n          {\n            $in: [item.code, { $ifNull: [\"$array.code\", []] }]\n          },\n          {\n            $map: {\n              input: \"$array\",\n              in: {\n                $mergeObjects: [\n                  \"$$this\",\n                  {\n                    $cond: [{ $eq: [\"$$this.code\", \"c\"] }, item, {}]\n                  }\n                ]\n              }\n            }\n          },\n          {\n            $concatArrays: [{ $ifNull: [\"$array\", []] }, [item]]\n          }\n        ]\n      }\n    }\n  }],\n  { upsert: true }\n)\n`\n```",
      "question_score": 5,
      "answer_score": 6,
      "created_at": "2021-07-02T15:13:00",
      "url": "https://stackoverflow.com/questions/68225868/how-to-upsert-with-mongodb-array"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb-query",
      "question_id": 71093789,
      "title": "Cannot perform a non-multi update on a time-series collection",
      "problem": "Hi I am using newly Timeseries mongodb collection. My mongodb version is 5.0.6. I am following this tutorial. I create a collection like this.\n```\n`   db.createCollection(\"ticker\", {\n     timeseries: {\n        timeField: \"time\",\n        metaField: \"metadata\",\n    },\n});\n`\n```\nI inserted the sample document like this.\n```\n`db.ticker.insertOne({\n time: ISODate(\"20210101T01:00:00\"),\n symbol: \"BTC-USD\",\n price: 34114.1145,\n metadata: { a: \"\"}\n});\n`\n```\nWhen I am trying to update metadata field it gives above error. As mentioned here is limitation you can only update metaField but still it is giving above error. Here is the update code\n```\n`db.ticker.update({ \"metadata.a\": \"a\" }, { $set: { \"metadata.d\": \"a\" } })\n\nwrite failed with error: {\n    \"nMatched\" : 0,\n    \"nUpserted\" : 0,\n    \"nModified\" : 0,\n    \"writeError\" : {\n        \"code\" : 72,\n        \"errmsg\" : \"Cannot perform a non-multi update on a time-series collection\"\n    }\n}\n`\n```\nNeed help what I am doing wrong.",
      "solution": "Try adding `{ multi: true }` config.\nIf you check the docs, there are still some limitations for updating time-series collections.\n\nUpdate commands must meet the following requirements:\n\nThe query may only match on metaField field values.\nThe update command may only modify the metaField field value.\nThe update must be performed with an update document that contains only update operator expressions.\nThe update command may not limit the number of documents to be updated. You must use an update command with multi: true or the\nupdateMany() method.\nThe update command may not set upsert: true.",
      "question_score": 5,
      "answer_score": 3,
      "created_at": "2022-02-12T17:38:19",
      "url": "https://stackoverflow.com/questions/71093789/cannot-perform-a-non-multi-update-on-a-time-series-collection"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb-query",
      "question_id": 65679305,
      "title": "How to lookup an array of objects and still retain the object properties?",
      "problem": "Here is the code in mongodb playground.\nI have a query that looks like below:\n```\n`    db.posts.aggregate({\n      $lookup: {\n        from: \"users\",\n        let: {comments: \"$comments\"},\n        pipeline: [\n          {$match: {$expr: {$in: [\"$_id\", \"$$comments.userId\"]} }},\n          {\"$addFields\":{\"text\": \"$$comments.text\"}}\n        ],\n        as: \"comments\"\n      }\n    })\n`\n```\nThe problem with this query is that the `text` value I get is is an array containing the texts from all `comments`. If I try to filter by `userId`, I get another problem. If a user has commented more than once the `text` value will be an array containing all of their `comments`.\nHow can I get the looked up user and their comment as in one object?\nEDIT\nI expect a result that looks like below:\n```\n`[\n  {\n    \"_id\": ObjectId(\"5a934e000102030405000000\"),\n    \"id\": \"1\",\n    \"comments\": [\n      {\n        \"_id\": \"u1\",\n        \"name\": \"James\",\n        \"username\": \"jamo\",\n        \"text\": \"Hi there, who are you?\"\n      },\n      {\n        \"_id\": \"u1\",\n        \"name\": \"James\",\n        \"username\": \"jamo\",\n        \"text\": \"Hi! Are you still there?\"\n      }\n    ],\n  }\n]\n`\n```",
      "solution": "`$unwind` deconstruct `comments` array\n`$lookup` join users collection,\n`$unwind` get object of `user`\n`$group` by `_id` and reconstruct `comments` with required fields\n\n```\n`db.posts.aggregate([\n  { $unwind: \"$comments\" },\n  {\n    $lookup: {\n      from: \"users\",\n      localField: \"comments.userId\",\n      foreignField: \"_id\",\n      as: \"comments.user\"\n    }\n  },\n  { $unwind: \"$comments.user\" },\n  {\n    $group: {\n      _id: \"$_id\",\n      id: { $first: \"$id\" },\n      comments: {\n        $push: {\n          _id: \"$comments._id\",\n          text: \"$comments.text\",\n          username: \"$comments.user.username\",\n          name: \"$comments.user.name\"\n        }\n      }\n    }\n  }\n])\n`\n```\nPlayground\n\nSecond option, without `$unwind`,\n\n`$lookup` with `users` collection\n`$map` to iterate loop of `comments` array\n`$filter` to iterate look of `users` array and find matching user info\n`$arrayElemAt` return first object from filtered user from $filter\n`$mergeObjects` will merge comment object and user info\n\n```\n`db.posts.aggregate([\n  {\n    $lookup: {\n      from: \"users\",\n      localField: \"comments.userId\",\n      foreignField: \"_id\",\n      as: \"users\"\n    }\n  },\n  {\n    $project: {\n      comments: {\n        $map: {\n          input: \"$comments\",\n          as: \"c\",\n          in: {\n            $mergeObjects: [\n              \"$$c\",\n              {\n                $arrayElemAt: [\n                  {\n                    $filter: {\n                      input: \"$users\",\n                      cond: { $eq: [\"$$c.userId\", \"$$this._id\"] }\n                    }\n                  },\n                  0\n                ]\n              }\n            ]\n          }\n        }\n      }\n    }\n  }\n])\n`\n```\nPlayground\n\nI have not tested performance of both queries, Try and use suitable query",
      "question_score": 5,
      "answer_score": 2,
      "created_at": "2021-01-12T07:59:04",
      "url": "https://stackoverflow.com/questions/65679305/how-to-lookup-an-array-of-objects-and-still-retain-the-object-properties"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb-query",
      "question_id": 66894033,
      "title": "Set _id to a new UUID using $addFields",
      "problem": "Well, someone done goofed. Wasn't me, but I get to fix it. What I need to do, is replace the `_id` of each and every document in the collection with a new UUID.\nMy plan was aggregating the entire collection, use `$addFields` to replace the `_id`, then merge it back into itself, replacing the records. Fortunately, there is a secondary (composite) key I can use for merging, so I'm good there \u2014 I think, because I haven't gotten there yet.\nThe problem I face is generating a unique (that's kinda the point of it) UUID during the `$addFields` stage.\n`db.foo.agregate([\n    {\n        $addFields: {\n            // I really just need the UUID string itself, hence the split\n            '_id': UUID().toString().split('\"')[1]\n        }\n    }\n    // here's where the merge would go\n])\n`\nThe problem here is that `UUID()` is called only once, so I get all the same IDs. That's not quite what I want.\nNow the tedious way would be to do this one by one, from a loop. But is there a way to do this from within an aggregation pipeline in one go?\n\nNow I found this question asking basically the same question, but it was solved by not using an UUID, using sequential keys instead, so it really isn't an answer to my question.",
      "solution": "So this answer is just to generate unique `UUID`s by calling `UUID()` from a $function. Try this:\n`db.collectionName.aggregate([\n    {\n        $addFields: {\n            _id: {\n                $function: {\n                    body: function() {\n                        return UUID().toString().split('\"')[1];\n                    },\n                    args: [],\n                    lang: \"js\"\n                }\n            }\n        }\n    }\n])\n`\nbtw, its only works in MongoDB version >= 4.4.\nOutput:\n`/* 1 */\n{\n    \"_id\" : \"5dc9316a-50a8-4013-8090-06fc66cdce9f\",\n    \"dummy\" : \"data\"\n},\n\n/* 2 */\n{\n    \"_id\" : \"062ebc8f-4455-4a81-a9e0-34f6c7132cab\",\n    \"dummy\" : \"data\"\n},\n\n/* 3 */\n{\n    \"_id\" : \"94eef4b8-9c0e-4910-a1cb-58443a63a036\",\n    \"dummy\" : \"data\"\n}\n`",
      "question_score": 4,
      "answer_score": 6,
      "created_at": "2021-03-31T21:21:05",
      "url": "https://stackoverflow.com/questions/66894033/set-id-to-a-new-uuid-using-addfields"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb-query",
      "question_id": 72011703,
      "title": "Mongo updateMany statement with an inner array of objects to manipulate",
      "problem": "I'm struggling to write a Mongo UpdateMany statement that can reference and update an object within an array.\nHere I create 3 documents. Each document has an array called `innerArray` always containing a single object, with a single date field.\n```\n`use test;\ndb.innerArrayExample.insertOne({ _id: 1, \"innerArray\": [ { \"originalDateTime\" : ISODate(\"2022-01-01T01:01:01Z\") } ]});\n\ndb.innerArrayExample.insertOne({ _id: 2, \"innerArray\": [ { \"originalDateTime\" : ISODate(\"2022-01-02T01:01:01Z\") } ]});\n\ndb.innerArrayExample.insertOne({ _id: 3, \"innerArray\": [ { \"originalDateTime\" : ISODate(\"2022-01-03T01:01:01Z\") } ]});\n`\n```\nI want to add a new date field, based on the original date field, to end up with this:\n```\n`{ _id: 1, \"innerArray\": [ { \"originalDateTime\" : ISODate(\"2022-01-01T01:01:01Z\"), \"copiedDateTime\" : ISODate(\"2022-01-01T12:01:01Z\") } ]}\n\n{ _id: 2, \"innerArray\": [ { \"originalDateTime\" : ISODate(\"2022-01-02T01:01:01Z\"), \"copiedDateTime\" : ISODate(\"2022-01-02T12:01:01Z\") } ]}\n\n{ _id: 3, \"innerArray\": [ { \"originalDateTime\" : ISODate(\"2022-01-03T01:01:01Z\"), \"copiedDateTime\" : ISODate(\"2022-01-03T12:01:01Z\") } ]}\n`\n```\nIn pseudo code I am saying take the `originalDateTime`, run it through a function and add a  related `copiedDateTime` value.\nFor my specific use-case, the function I want to run strips the timezone from `originalDateTime`, then overwrites it with a new one, equivalent to the Java `ZonedDateTime` function `withZoneSameLocal`. Aka 9pm UTC becomes 9pm Brussels (therefore effectively 7pm UTC). The technical justification and methodology were answered in another Stack Overflow question here.\nThe part of the query I'm struggling with, is the part that updates/selects data from an element inside an array. In my simplistic example, for example I have crafted this query, but unfortunately it doesn't work:\nThis function puts `copiedDateTime` in the correct  place... but doesn't evaluate the commands to manipulate the date:\n```\n`db.innerArrayExample.updateMany({ \"innerArray.0.originalDateTime\" : { $exists : true }}, { $set: { \"innerArray.0.copiedDateTime\" : { $dateFromString: { dateString: { $dateToString: { \"date\" : \"$innerArray.0.originalDateTime\", format: \"%Y-%m-%dT%H:%M:%S.%L\" }}, format: \"%Y-%m-%dT%H:%M:%S.%L\", timezone: \"Europe/Paris\" }}});\n\n// output\n  {\n    _id: 1,\n    innerArray: [\n      {\n        originalDateTime: ISODate(\"2022-01-01T01:01:01.000Z\"),\n        copiedDateTime: {\n          '$dateFromString': {\n            dateString: { '$dateToString': [Object] },\n            format: '%Y-%m-%dT%H:%M:%S.%L',\n            timezone: 'Europe/Paris'\n          }\n        }\n      }\n    ]\n  }\n`\n```\nThis simplified query, also has the same issue:\n```\n`b.innerArrayExample.updateMany({ \"innerArray.0.originalDateTime\" : { $exists : true }}, { $set: { \"innerArray.0.copiedDateTime\" : \"$innerArray.0.originalDateTime\" }});\n\n//output\n  {\n    _id: 1,\n    innerArray: [\n      {\n        originalDateTime: ISODate(\"2022-01-01T01:01:01.000Z\"),\n        copiedDateTime: '$innerArray.0.originalDateTime'\n      }\n    ]\n  }\n`\n```\nAs you can see this issue looks to be separate from the other stack overflow question. Instead of being able changing timezones, it's about getting things inside arrays to update.\nI plan to take this query, create 70,000 variations of it with different location/timezone combinations and run it against a database with millions of records, so I would prefer something that uses `updateMany` instead of using Javascript to iterate over each row in the database... unless that's the only viable solution.\nI have tried putting `$set` in square brackets. This changes the way it interprets everything, evaluating the right side, but causing other problems:\n```\n`test> db.innerArrayExample.updateMany({ \"_id\" : 1 }, [{ $set: { \"innerArray.0.copiedDateTime\" : \"$innerArray.0.originalDateTime\" }}]);\n\n//output\n  {\n    _id: 1,\n    innerArray: [\n      {\n        '0': { copiedDateTime: [] },\n        originalDateTime: ISODate(\"2022-01-01T01:01:01.000Z\")\n      }\n    ]\n  }\n`\n```\nAbove it seems to interpret .0. as a literal rather than an array element. (For my needs I know the array only has 1 item at all times). I'm at a loss finding an example that meets my needs.\nI have also tried experimenting with the `arrayFilters`, documented on my mongo updateMany documentation but I cannot fathom how it works with objects:\n```\n`test> db.innerArrayExample.updateMany(\n...    { },\n...    { $set: { \"innerArray.$[element].copiedDateTime\" : \"$innerArray.$[element].originalDateTime\" } },\n...    { arrayFilters: [ { \"originalDateTime\": { $exists: true } } ] }\n... );\nMongoServerError: No array filter found for identifier 'element' in path 'innerArray.$[element].copiedDateTime'\ntest> db.innerArrayExample.updateMany(\n...    { },\n...    { $set: { \"innerArray.$[0].copiedDateTime\" : \"$innerArray.$[element].originalDateTime\" } },\n...    { arrayFilters: [ { \"0.originalDateTime\": { $exists: true } } ] }\n... );\nMongoServerError: Error parsing array filter :: caused by :: The top-level field name must be an alphanumeric string beginning with a lowercase letter, found '0'\n`\n```\nIf someone can help me understand the subtleties of the Mongo syntax and help me back on to the right path I'd be very grateful.",
      "solution": "You want to be using pipelined updates, the issue you're having with the syntax you're using is that it does not allow the usage of aggregation operators and document field values.\nHere is a quick example on how to do it:\n```\n`db.collection.updateMany({},\n[\n  {\n    \"$set\": {\n      \"innerArray\": {\n        $map: {\n          input: \"$innerArray\",\n          in: {\n            $mergeObjects: [\n              \"$$this\",\n              {\n                copiedDateTime: \"$$this.originalDateTime\"\n              }\n            ]\n          }\n        }\n      }\n    }\n  }\n])\n`\n```\nMongo Playground",
      "question_score": 4,
      "answer_score": 5,
      "created_at": "2022-04-26T11:39:37",
      "url": "https://stackoverflow.com/questions/72011703/mongo-updatemany-statement-with-an-inner-array-of-objects-to-manipulate"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb-query",
      "question_id": 70214937,
      "title": "mongodump error &quot;Failed: can&#39;t create session: could not connect to server:&quot;",
      "problem": "I can successfully connect with my mongodb locally with self-signed certificates. Security authorization is set to \"disabled\" under mongo config and TLS is enabled. Using the `mongodump` command locally\n```\n`mongodump --ssl --authenticationDatabase admin --host=127.0.0.1 --port=27017 -u=admin -p=821ewuyuiuw3! --sslPEMKeyFile=/etc/ssl/mongodb.pem --sslCAFile=/etc/ssl/rootCA.pem --archive=/home/backups/mongodump.gz --gzip\n`\n```\nThe admin user exists despite the fact I disabled the authorization. I get the same error without the credentials also.\nI always get the error :\n```\n`2021-12-03T14:58:29.420+0200    Failed: can't create session: could not connect to server: server selection error: server selection timeout, current topology: { Type: Single, Servers: [{ Addr: 127.0.0.1:27017, Type: Unknown, Last error: connection() error occured during connection handshake: x509: cannot validate certificate for 127.0.0.1 because it doesn't contain any IP SANs }, ] }\n`\n```\nI tried also using `export GODEBUG=x509ignoreCN=0` without success. Any solution to this?",
      "solution": "I finally managed to make it work with both of the following commands. It seems that it was needed the parameter  --tlsInsecure\n```\n`mongodump --ssl  --host=127.0.0.1 --port=27017  --sslPEMKeyFile=/etc/ssl/mongoCer/mongodb.pem --sslCAFile=/etc/ssl/mongoCer/rootCA.pem --archive=./mongo-daily-backups/mongodump-date_2020-11-16_time_20-37-11.gz --gzip --tlsInsecure\n`\n```\nand\n```\n`mongodump --ssl  --host=127.0.0.1 --port=27017 --archive=./mongo-daily-backups/mongodump-date_2020-11-16_time_20-37-11.gz --gzip --tlsInsecure\n`\n```",
      "question_score": 4,
      "answer_score": 3,
      "created_at": "2021-12-03T14:17:25",
      "url": "https://stackoverflow.com/questions/70214937/mongodump-error-failed-cant-create-session-could-not-connect-to-server"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb-query",
      "question_id": 73624483,
      "title": "Embed operator $exists within the operator $map",
      "problem": "I am extending the solution provided at this example where I am adding multiple conditions within the \"in\" of operator $map. All the conditions need to be true so I'm using $and operator to combine them. The new condition checks whether a particular field exists as a key in the document. I am using operator $exists to check this. The code snippet to check this condition is\n```\n`{\n \"isTypePattern\": {\n    $exists: false\n  }\n}\n`\n```\nwhere the name of the field (to check for existence) is: isTypePattern.\nThis snippet works independently however when embedded under $map operator it throws a compile error:\nThe code looks like following and here is the playground link:\n```\n`db.collection.find({\n  \"$expr\": {\n    \"$anyElementTrue\": {\n      \"$map\": {\n        \"input\": \"$entities\",\n        \"in\": {\n          $and: [\n            {\n              \"$regexMatch\": {\n                \"input\": \"Room1\",\n                \"regex\": \"$$this.id\",\n                \"options\": \"i\"\n              },\n              \n            },\n            {\n              \"isTypePattern\": {\n                $exists: false\n              }\n            }\n          ]\n        }\n      }\n    }\n  }\n})\n`\n```\nSo if someone can suggest how to embed $exists within $map operator that would be great",
      "solution": "Maybe something like this:\n```\n`db.collection.find({\n\"$expr\": {\n\"$anyElementTrue\": {\n  \"$map\": {\n    \"input\": \"$entities\",\n    \"in\": {\n      $and: [\n        {\n          \"$regexMatch\": {\n            \"input\": \"Room1\",\n            \"regex\": \"$$this.id\",\n            \"options\": \"i\"\n          },\n          \n        },\n        {\n          $eq: [\n            {\n              $type: \"$$this.isTypePattern\"\n            },\n            \"missing\"\n          ]\n        }\n      ]\n    }\n   }\n  }\n }\n})\n`\n```\nExplained:\n$expr allow using $type and aggregation comparison operators like $eq,$ne,$lt,$gt that can be used for the case , but do not support $exists.\nplayground",
      "question_score": 4,
      "answer_score": 2,
      "created_at": "2022-09-06T17:26:47",
      "url": "https://stackoverflow.com/questions/73624483/embed-operator-exists-within-the-operator-map"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb-query",
      "question_id": 71349221,
      "title": "Using $exists in MongoDB $cond",
      "problem": "For background, I have to create a field in a collection, but ONLY if another known field (an array) is not empty. I'm trying to make a boolean filter with the $and and some conditions:\n```\n`$set: {\n  clientsAllowed: {\n            $cond: {\n              if: {\n                $and: [\n                  {\n                    businessAllowed: { $exists: true },\n                  },\n                  {\n                    businessAllowed: { $type: \"array\" },\n                  },\n                  {\n                    businessAllowed: { $ne: [] },\n                  },\n                ],\n              },\n              then: [...],\n              else: [...],\n            },\n          }\n}\n`\n```\nBut I get this error:\n\nInvalid $set :: caused by :: Unrecognized expression '$exists'\n\nIs the `$exists` wrongly formatted? Or is the `$and` stage? Any ideas?",
      "solution": "Type cond for businessAllowed is enough. You don't need to check exists or not.\n```\n`db.collection.aggregate([\n  {\n    $set: {\n      clientsAllowed: {\n        $cond: {\n          if: {\n            $and: [\n              {\n                \"$eq\": [\n                  {\n                    $type: \"$businessAllowed\"\n                  },\n                  \"array\"\n                ]\n              },\n              {\n                $ne: [\n                  \"$businessAllowed\",\n                  []\n                ]\n              }\n            ]\n          },\n          then: [],\n          else: []\n        }\n      }\n    }\n  }\n])\n`\n```\nmongoplayground",
      "question_score": 4,
      "answer_score": 2,
      "created_at": "2022-03-04T10:25:31",
      "url": "https://stackoverflow.com/questions/71349221/using-exists-in-mongodb-cond"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb-query",
      "question_id": 70543750,
      "title": "MongoDB v5.0.5 reference existing field in updateMany()",
      "problem": "First time using MongoDB and I'm having an issue that I would appreciate some help with please.\nLet's say I have a collection called \"students\" with documents in the collection structured as followed:\n```\n`{\n\"_id\": ObjectId(\"12345\"),\n\"Name\": \"Joe Bloggs\",\n\"Class_Grade\": \"b\"\n\"Homework_Grade\": \"c\",\n}\n`\n```\nI want to create an embedded document called `\"Grades\"` that contains the class and homework grade fields and applies this to every document in the collection to end up with:\n```\n`{\n\"_id\": ObjectId(\"12345\"),\n\"Name\": \"Joe Bloggs\",\n\"Class_Grade\": \"b\"\n\"Homework_Grade\": \"c\",\n\"Grades\": {\n    \"Class_Grade\": \"b\",\n    \"Homework_Grade\": \"c\",\n    }\n}\n`\n```\nI have been trying to achieve this using updateMany() in MongoShell:\n```\n`db.students.updateMany({}, {$set: {Grades: {\"Class_Grade\": $Class_Grade, \"Homework_Grade\": $Homework_grade\"}}})\n`\n```\nHowever, in doing so, I receive Reference Error: `$Class_Grade` is not defined. I have tried amending the reference to `$students.Class_Grade` and receive the same error.\nYour advice would be greatly appreciated",
      "solution": "There are a few mistakes in your query,\n\nif you want to use the internal existing field's value, you need to use an update with aggregation pipeline starting from MongoDB 4.2, you need to wrap the update part in attay bracket [], as i added query.\n\nuse quotation in field name that you want to use from internal field ex: `\"$Class_Grade\"`\n\nyou have used field `$Homework_grade`, and in your documents it is `G` is capital in `Grade` so try to use exact field name `$Homework_Grade`\n\n```\n`db.students.updateMany({},\n[\n  {\n    $set: {\n      Grades: {\n        \"Class_Grade\": \"$Class_Grade\",\n        \"Homework_Grade\": \"$Homework_Grade\"\n      }\n    }\n  }\n])\n`\n```\nPlayground",
      "question_score": 4,
      "answer_score": 2,
      "created_at": "2021-12-31T16:42:58",
      "url": "https://stackoverflow.com/questions/70543750/mongodb-v5-0-5-reference-existing-field-in-updatemany"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb-query",
      "question_id": 77903991,
      "title": "setOnUpdate in mongo with one query on upsert?",
      "problem": "I have been googling and read the mongo docs, but find it weird no one asked for this so I thought I face the problem the wrong ways, but I am persistent so I am going to ask here, I know there is a `setOnInsert` operator in mongo when using `upsert:true` the fields on `setOnInsert` will be inserted on insert but ignored on update, but what I want is that I want some fields only get saved on update and ignored on insert (upsert) the reverse of that `setOnInsert` in a single query.\nSo here is the case I have field \"createdDate\" and \"updatedDate\" with `upsert:true` and using `setOnInsert` I can have the field createdDate on `setOnInsert` but for the \"updatedDate\" if I put it in `$set` it will get saved on insert (upsert) I want that \"updatedDate\" only get saved on update but ignored on insert (upsert) in one query. I can do this with 2 query and simpler but looking for a single query solution, also find it weird mongo doesn't have the reverse of `setOnInsert` yet , maybe this case is unusual?\nI tried using `$switch` and `$cond` but it got inserted as document doesn't work, I tried in go driver for mongo, but accepting answer as mongo query / shell too (will convert them myself then)\n```\n`coll := mongoCon.Mongo.Collection(\"productModels\")\nproductId, _ := primitive.ObjectIDFromHex(\"65b87bbc571dd2a8d301c9f2\")\ndoc := bson.D{\n        {Key: \"$set\", Value: bson.D{\n            {Key: \"modelName\", Value: \"orange 11\"},\n            {Key: \"updatedDate\", Value: bson.M{\n                \"$switch\": bson.M{\n                    \"branches\": bson.A{\n                        bson.M{\n                            \"case\": bson.M{\n                                \"$eq\": bson.A{\n                                    bson.M{\"_id\": bson.M{\"$exists\": false}}, false,\n                                }},\n                            \"then\": nil,\n                        },\n                    },\n                },\n            },\n            },\n        }},\n        {Key: \"$setOnInsert\", Value: bson.D{{Key: \"createdDate\", Value: time.Now()}}},\n    }\n    res, err := coll.UpdateOne(context.Background(), bson.M{\"product_id\": productId}, doc, options.Update().SetUpsert(true))\n    // throw res here now, might use it later for 2 query solution\n    _ = res\n    if err != nil {\n        log.Fatal(err)\n    }\n`\n```",
      "solution": "In my opinion, the simplest solution is to let `updatedDate` be inserted even for new documents. If you must tell if the document was ever updated, you can compare the `updatedDate` and the `createdDate`, or have a separate `modifiedCount` field maintained.\nNow on to solve your requirement. Your second attempt to use `$switch` doesn't work because that is an aggregation operator, and to use an aggregation pipeline with an update operation, you have to use an array (or slice) document as the update document, this is what triggers interpreting it as an aggregation pipeline. For details, see Golang and MongoDB - I try to update toggle boolean using golang to mongodb but got object instead.\nDoing so will introduce a new issue: you can't use `$setOnInsert` anymore. And you can't use the `$exists` operator in an aggregation pipeline :(\nBut what you want is achievable using a single `$set` stage: you may use `$ifNull` to retrieve a field's value if it exists, or provide a fallback value if it doesn't.\nSo the idea is to use the current value of `createdDate` if it exists, else pass and set the current time.\nSimilarly we can only update `updatedDate` if the `createdDate` already exists, which we can check using the above mentioned `$ifNull` operator (and using a `$cond` to tell if the result is `null`).\nSo you can do it like this:\n```\n`now := time.Now()\nres, err := coll.UpdateOne(ctx,\n    bson.M{\"product_id\": productId},\n    []any{\n        bson.M{\n            \"$set\": bson.M{\n                \"modelName\": \"orange 11\",\n                \"updatedDate\": bson.M{\"$cond\": []any{\n                    bson.M{\"$eq\": []any{\n                        bson.M{\"$ifNull\": []any{\"$createdDate\", nil}}, nil},\n                    },\n                    nil, now,\n                }},\n                \"createdDate\": bson.M{\"$ifNull\": []any{\"$createdDate\", now}},\n            },\n        },\n    },\n    options.Update().SetUpsert(true),\n)\n`\n```\n(Note I used `bson.M` for simplicity, you can translate it to using `bson.D` if field order matters to you, in which case it'll get more verbose.)\nBut again, I would anytime prefer the below solution instead of the above \"ugliness\" (not to mention this is probably faster):\n```\n`now := time.Now()\nres, err = c.UpdateOne(ctx,\n    bson.M{\"product_id\": productId},\n    bson.M{\n        \"$set\": bson.M{\n            \"modelName\":   \"orange 11\",\n            \"updatedDate\": now,\n        },\n        \"$setOnInsert\": bson.M{\n            \"createdDate\": now,\n        },\n    },\n    options.Update().SetUpsert(true),\n)\n`\n```",
      "question_score": 4,
      "answer_score": 1,
      "created_at": "2024-01-30T06:01:32",
      "url": "https://stackoverflow.com/questions/77903991/setonupdate-in-mongo-with-one-query-on-upsert"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb-query",
      "question_id": 76031745,
      "title": "mongo how to load nested document by aggregation",
      "problem": "I have 3 kinds of document listed as below:\n```\n`vilya_be> db.plots.find({})\n[\n  {\n    _id: ObjectId(\"6426b069ca83da130cdb7f70\"),\n    x: 0,\n    y: 0,\n    _class: 'com.vilya.farm.domain.model.Plot'\n  }\n]\n\nvilya_be> db.users_have_plots.find({})\n[\n  {\n    _id: ObjectId(\"6426b073ca83da130cdb7f71\"),\n    userId: '6412c76956d4170a7de34d92',\n    plot: DBRef(\"plots\", ObjectId(\"6426b069ca83da130cdb7f70\")),\n    _class: 'com.vilya.farm.domain.model.UserPlot'\n  }\n]\n\nvilya_be> db.users.find({})\n[\n  {\n    _id: ObjectId(\"6412c76956d4170a7de34d92\"),\n    email: 'abc@abc.com',\n    password: '$2a$10$s9VgOYd.fOKZF66TnAsjWemiCYkA7aG45NJpuSNgbVxpcIGF7fWqu',\n    firstName: 'f',\n    lastName: 'l',\n    plots: [ DBRef(\"users_have_plots\", ObjectId(\"6426b073ca83da130cdb7f71\")) ],\n    _class: 'com.vilya.farm.domain.model.User'\n  },\n  {\n    _id: ObjectId(\"6414667360e4ba4481052627\"),\n    email: 'abc1@abc.com',\n    password: '$2a$10$OP52phZ61l2JX2e2TQOu9ubYFBYcPeqEZ92ox2Nyyp5e.MEZk7GhS',\n    firstName: 'f',\n    lastName: 'l',\n    _class: 'com.vilya.farm.domain.model.User'\n  }\n]\n`\n```\nI want to load all `users` along with their `users_have_plots` (along with their `plots`) on a single command.\nI have tried:\n```\n`db.users.aggregate([\n{\n  \"$lookup\": {\n    \"from\": \"users_have_plots\",\n    \"let\": {\n      \"plots\": \"$plots\"\n    },\n    \"pipeline\": [\n      {\n        \"$match\": {\n          \"$expr\": {\n            \"$in\": [\n              \"$_id\",\n              \"$$plots\"\n            ]\n          }\n        }\n      },\n      {\n        \"$lookup\": {\n          \"from\": \"plots\",\n          \"localField\": \"plot\",\n          \"foreignField\": \"_id\",\n          \"as\": \"plot\"\n        }\n      }\n    ],\n    \"as\": \"plots\"\n  }\n}\n]);\n`\n```\nIt gives me: `PlanExecutor error during aggregation :: caused by :: $in requires an array as a second argument, found: missing`\nAnd this:\n```\n`db.users.aggregate([\n{\n  \"$lookup\": {\n    \"from\": \"users_have_plots\",\n    \"let\": {\n      \"plots\": \"$plots\"\n    },\n    \"pipeline\": [\n      {\n        \"$match\": {\n          \"$expr\": {\n            \"$in\": [\n              \"$_id\",\n              \"$$plots.ObjectId\"\n            ]\n          }\n        }\n      },\n      {\n        \"$lookup\": {\n          \"from\": \"plots\",\n          \"localField\": \"plot\",\n          \"foreignField\": \"_id\",\n          \"as\": \"plot\"\n        }\n      }\n    ],\n    \"as\": \"plots\"\n  }\n}\n]);\n`\n```\nIt also gives me: `PlanExecutor error during aggregation :: caused by :: $in requires an array as a second argument, found: missing`\nI'm new to mongodb and just cannot find anyway to make it works. Any help will be appreciate!\n\nmongo version: mongo:6.0.2-focal running on docker desktop\n\nEDIT:\nTried:\n```\n`db.users.aggregate([{\n  \"$lookup\": {\n    \"from\": \"users_have_plots\",\n    \"let\": {\n      \"plots\": \"$plots\"\n    },\n    \"pipeline\": [\n      {\n        \"$match\": {\n          \"$expr\": {\n            \"$in\": [\n              \"$_id\",\n              {\n                \"$ifNull\": [\n                  \"$$plots.ObjectId\",\n                  []\n                ]\n              }\n            ]\n          }\n        }\n      },\n      {\n        \"$lookup\": {\n          \"from\": \"plots\",\n          \"localField\": \"plot\",\n          \"foreignField\": \"_id\",\n          \"as\": \"plot\"\n        }\n      }\n    ],\n    \"as\": \"plots\"\n  }\n}\n])\n`\n```\nGiving me:\n```\n`[\n  {\n    _id: ObjectId(\"6412c76956d4170a7de34d92\"),\n    email: 'abc@abc.com',\n    password: '$2a$10$s9VgOYd.fOKZF66TnAsjWemiCYkA7aG45NJpuSNgbVxpcIGF7fWqu',\n    firstName: 'f',\n    lastName: 'l',\n    plots: [],\n    _class: 'com.vilya.farm.domain.model.User'\n  },\n  {\n    _id: ObjectId(\"6414667360e4ba4481052627\"),\n    email: 'abc1@abc.com',\n    password: '$2a$10$OP52phZ61l2JX2e2TQOu9ubYFBYcPeqEZ92ox2Nyyp5e.MEZk7GhS',\n    firstName: 'f',\n    lastName: 'l',\n    _class: 'com.vilya.farm.domain.model.User',\n    plots: []\n  }\n]\n`\n```\nNo plots included.",
      "solution": "I think you are using `Dbrefs` as defined here in the documentation. That's why your queries are not working, because `DBRef` store the documents in this format, internally:\n```\n`db={\n  \"plots\": [\n    {\n      _id: ObjectId(\"6426b069ca83da130cdb7f70\"),\n      x: 0,\n      y: 0,\n      _class: \"com.vilya.farm.domain.model.Plot\"\n    }\n  ],\n  \"users_have_plots\": [\n    {\n      _id: ObjectId(\"6426b073ca83da130cdb7f71\"),\n      userId: \"6412c76956d4170a7de34d92\",\n      plot: {\n        \"$ref\": \"plots\",\n        \"$id\": ObjectId(\"6426b069ca83da130cdb7f70\")\n      },\n      _class: \"com.vilya.farm.domain.model.UserPlot\"\n    }\n  ],\n  \"users\": [\n    {\n      _id: ObjectId(\"6412c76956d4170a7de34d92\"),\n      email: \"abc@abc.com\",\n      password: \"$2a$10$s9VgOYd.fOKZF66TnAsjWemiCYkA7aG45NJpuSNgbVxpcIGF7fWqu\",\n      firstName: \"f\",\n      lastName: \"l\",\n      plots: [\n        {\n          \"$ref\": \"users_have_plots\",\n          \"$id\": ObjectId(\"6426b073ca83da130cdb7f71\")\n        }\n      ],\n      _class: \"com.vilya.farm.domain.model.User\"\n    },\n    {\n      _id: ObjectId(\"6414667360e4ba4481052627\"),\n      email: \"abc1@abc.com\",\n      password: \"$2a$10$OP52phZ61l2JX2e2TQOu9ubYFBYcPeqEZ92ox2Nyyp5e.MEZk7GhS\",\n      firstName: \"f\",\n      lastName: \"l\",\n      _class: \"com.vilya.farm.domain.model.User\"\n    }\n  ]\n}\n`\n```\nNote the `plot` field in `user_have_plots` collection and `plots` field in the `user` collection. They are an object. To make it work, try this:\n```\n`db.users.aggregate([\n  {\n    \"$lookup\": {\n      \"from\": \"users_have_plots\",\n      \"let\": {\n        \"plots\": \"$plots\"\n      },\n      \"pipeline\": [\n        {\n          \"$match\": {\n            \"$expr\": {\n              \"$in\": [\n                \"$_id\",\n                {\n                  \"$ifNull\": [\n                    \"$$plots.$id\",\n                    []\n                  ]\n                }\n              ]\n            }\n          }\n        },\n        {\n          \"$lookup\": {\n            \"from\": \"plots\",\n            \"localField\": \"plot.$id\",\n            \"foreignField\": \"_id\",\n            \"as\": \"plot\"\n          }\n        }\n      ],\n      \"as\": \"plots\"\n    }\n  }\n])\n`\n```\nPlayground link.\nFinally, you don't need to use `Dbrefs`, they are suitable for purposes when you have joined a single property with multiple collections, or with collections in multiple databases. All your collections are in the same databases, use simple references.",
      "question_score": 4,
      "answer_score": 1,
      "created_at": "2023-04-17T06:36:04",
      "url": "https://stackoverflow.com/questions/76031745/mongo-how-to-load-nested-document-by-aggregation"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb-query",
      "question_id": 66457611,
      "title": "Comparing objects inside a nested array - mongoDB",
      "problem": "In my db I have a nested array of elements inside each document containing items, in the following form:\n```\n`elements:[\n     {\n      \"elem_id\": 12,\n      items: [ {\"i_id\": 1, \"type\": x}, {\"i_id\": 2, \"type\": y}, {\"i_id\": 3, \"type\": x}]\n     },\n     {\n      \"elem_id\": 13,\n      items: [ {\"i_id\": 4, \"type\": x}, {\"i_id\": 5, \"type\": x}]\n     }\n]\n`\n```\nI am trying to return all elements that have items of different types, meaning I would get back only:\n```\n`     {\n      \"elem_id\": 12,\n      items: [ {\"i_id\": 1, \"type\": x}, {\"i_id\": 2, \"type\": y}, {\"i_id\": 3, \"type\": x}]\n      }\n`\n```\nsince there are items of type x and of type y.\nI think I need to iterate the items array and compare the type of every item in the array to the types of the previous items but I can't figure out how to do this in aggregation.\nJust to note - I am using Redash and so I can't include any JS in the query.\nThank you for the assistance!",
      "solution": "Try this:\n`db.elements.aggregate([\n    { $unwind: \"$elements\" },\n    {\n        $addFields: {\n            \"count\": { $size: \"$elements.items\" },\n            \"uniqueValues\": {\n                $reduce: {\n                    input: \"$elements.items\",\n                    initialValue: [{ $arrayElemAt: [\"$elements.items.type\", 0] }],\n                    in: {\n                        $setUnion: [\"$$value\", [\"$$this.type\"]]\n                    }\n                }\n            }\n        }\n    },\n    {\n        $match: {\n            $expr: {\n                $eq: [\"$count\", { $size: \"$uniqueValues\" }]\n            }\n        }\n    }\n]);\n`\nOutput:\n`{\n    \"_id\" : ObjectId(\"603f8f05bcece4372062bcea\"),\n    \"elements\" : {\n        \"elem_id\" : 12,\n        \"items\" : [\n            {\n                \"i_id\" : 1,\n                \"type\" : 1\n            },\n            {\n                \"i_id\" : 2,\n                \"type\" : 2\n            },\n            {\n                \"i_id\" : 3,\n                \"type\" : 3\n            }\n        ]\n    },\n    \"count\" : 3,\n    \"uniqueValues\" : [1, 2, 3]\n}\n`",
      "question_score": 4,
      "answer_score": 1,
      "created_at": "2021-03-03T14:00:22",
      "url": "https://stackoverflow.com/questions/66457611/comparing-objects-inside-a-nested-array-mongodb"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb-query",
      "question_id": 77656071,
      "title": "Mongo sort by date unless a condition is met?",
      "problem": "Supposed I have the following data:\n\nTime\nSubmitted\n\n1\nTrue\n\n2\nTrue\n\n3\nTrue\n\n4\nFalse\n\n5\nFalse\n\n6\nFalse\n\nMy goal is to get either the the oldest unsubmitted OR the newest submitted. This is for loading user timesheets, so if they have an old unsubmitted timesheet, I want to load that by default. If not I want to show them their most current timesheet.\nAfter the query I'd expect the data back in this order:\n\nTime\nSubmitted\n\n4\nFalse\n\n5\nFalse\n\n6\nFalse\n\n3\nTrue\n\n2\nTrue\n\n1\nTrue\n\nNotice the first 3 records are false so they sort by ascending time while the last 3 records are true so they sort by descending time.\nHere is a mongoplayground sample of the data:\nhttps://mongoplayground.net/p/yEtgMOfZ_ik\nI'm guessing this can be done by projecting some value to use as the sort by, but I can't wrap my head around what that needs to be.",
      "solution": "You have a sort on `submitted` already: `false` sorts before `true` (0 vs 1 in many languages). \ud83d\udc4d So that's the first key to sort on.\nWhen `submitted=false`, you want a ascending order for time (4, 5, 6) and when it's `true`, you want the opposite.\nYou can create an additional key to sort on Time based on whether or not it was submitted and use negative-numbers to change the sort order when it's submitted true/false.\nHere, I'm creating `timeSort` which will negate the time when `submitted=true`. So 3 becomes -3 and in ascending order, -3 \n`db.collection.aggregate([\n  {\n    $set: {\n      timeSort: {\n        $multiply: [\n          \"$time\",\n          { $cond: [ \"$submitted\", -1, 1 ] }\n        ]\n      }\n    }\n  },\n  {\n    $sort: {\n      submitted: 1,\n      timeSort: 1\n    }\n  },\n  { $unset: \"timeSort\" }\n])\n`\nYou can flip the 1 and -1 around in `$cond` and in `timeSort` in `$sort`, gives the same results.\n`[\n  { \"time\": 4, \"submitted\": false },\n  { \"time\": 5, \"submitted\": false },\n  { \"time\": 6, \"submitted\": false },\n  { \"time\": 3, \"submitted\": true },\n  { \"time\": 2, \"submitted\": true },\n  { \"time\": 1, \"submitted\": true }\n]\n`\nMongo Plaground",
      "question_score": 3,
      "answer_score": 4,
      "created_at": "2023-12-13T20:18:07",
      "url": "https://stackoverflow.com/questions/77656071/mongo-sort-by-date-unless-a-condition-is-met"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb-query",
      "question_id": 68981115,
      "title": "Find and replace in array of strings in mongodb",
      "problem": "I'm using MongoDB v4.4. My document structure looks like this:\n```\n`{\n  _id: ObjectId(\"...\"),\n  photos: [\"image1.png\", \"image2.png\"]\n},\n{\n  _id: ObjectId(\"...\"),\n  photos: [\"image3.png\", \"another_image.png, image5.jpg\"]\n},\n{\n  _id: ObjectId(\"...\"),\n  photos: [\"image_name.jpg\", \"image2.jpg\"]\n},\n\n`\n```\nI am trying to change all the strings that contain \".png\" to \".jpg\".\nI have tried the following:\n```\n`db.users.updateMany({\n  photos: { $regex: /.png/ }\n}, [{\n  $set: {\n    \"photos.$[]\": { \n      $replaceOne: { \n        input: \"photos.$[]\", find: \".png\", replacement: \".jpg\" \n      } \n    }\n  }\n}])\n`\n```\nThis returns the error:\n```\n`MongoServerError: Invalid $set :: caused by :: FieldPath field names may not start with '$'.\n`\n```",
      "solution": "You need to iterate the loop of `photos` array and replace the extension,\n\n`$map` to iterate loop of `photos` array\n`$replaceOne` to replace extension\n\n```\n`db.users.updateMany(\n  { photos: { $regex: \".png\" } },\n  [{\n    $set: {\n      photos: {\n        $map: {\n          input: \"$photos\",\n          in: {\n            $replaceOne: {\n              input: \"$$this\",\n              find: \".png\",\n              replacement: \".jpg\"\n            }\n          }\n        }\n      }\n    }\n  }]\n)\n`\n```\nPlayground",
      "question_score": 3,
      "answer_score": 7,
      "created_at": "2021-08-30T10:36:15",
      "url": "https://stackoverflow.com/questions/68981115/find-and-replace-in-array-of-strings-in-mongodb"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb-query",
      "question_id": 69810826,
      "title": "set field in array of objects after lookup. MongoDb",
      "problem": "I have two collections `users` and `posts`.\nusers has documents like this:\n```\n`{\n  _id: ObjectId('611142303c409b5dc826e563'),\n  name: 'foo'\n}\n`\n```\nposts has documents like this:\n```\n`{\n  _id: ObjectId('611142303c409b5dc826e111'),\n  comments:[\n    {\n      owner: ObjectId('611142303c409b5dc826e563'),\n      description: \"my description\"\n    },\n    {\n      owner: ObjectId('611142303c409b5dc826e333'),\n      description: \"my description2\"\n    }\n  ]\n}\n`\n```\nWhen I receive a request server side, I need to return the owner's whole document and not just its id.\nfor example to a get request I have to return:\n```\n`{\n  _id: ObjectId('611142303c409b5dc826e111'),\n  comments:[\n    {\n      owner:{\n        _id: ObjectId('611142303c409b5dc826e563'),\n        name: 'foo'\n    },\n      description: \"my description\"\n    },\n    {\n      owner:     {\n        _id: ObjectId('611142303c409b5dc826e555'),\n        name: 'foo2'\n      },\n      description: \"my description2\"\n    }\n  ]\n}\n`\n```\nTo do that I did the following pipeline:\n```\n`[\n  $lookup:{\n    from: 'owners',\n    localField: 'comments.owner',\n    foreignField: '_id',\n    as: 'owners_comments'\n  }\n]\n`\n```\nDoing this way I get an array of owners that has comments in one specific document.\nMy question is how to get the right owner profile for each comment? I know I can do that server side easyer but I prefer doing it DB side.\nI thought to map each comment and inside filter the `owners_comments`, but I have few problems to that in mongo aggregation.\nDo you have any suggestion?",
      "solution": "You have to `$unwind` the comments array, only then execute the `$lookup` and then you want to `$group` to restore the original structure, like so:\n```\n`db.posts.aggregate([\n  {\n    $unwind: \"$comments\"\n  },\n  {\n    $lookup: {\n      from: \"owners\",\n      localField: \"comments.owner\",\n      foreignField: \"_id\",\n      as: \"owners\"\n    }\n  },\n  {\n    $group: {\n      _id: \"$_id\",\n      comments: {\n        $push: {\n          owner: \"$comments.owner\",\n          description: \"$comments.description\",\n          owner_profile: {\n            \"$arrayElemAt\": [\n              \"$owners\",\n              0\n            ]\n          },\n          \n        }\n      }\n    }\n  }\n])\n`\n```\nMongo Playground",
      "question_score": 3,
      "answer_score": 5,
      "created_at": "2021-11-02T13:53:40",
      "url": "https://stackoverflow.com/questions/69810826/set-field-in-array-of-objects-after-lookup-mongodb"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb-query",
      "question_id": 67252012,
      "title": "How to make mongodb work with countDocuments() and $addFields",
      "problem": "I have a document with the following structure:\n```\n`{\n    '_id': '',\n    'a': '',\n    'b': '',\n    'c': [\n        {\n            '_id': '',\n            'd': '',\n            'f': [\n                {\n                    'orderDate': 12345,\n                    'orderProfit': 12,\n                },\n                {\n                    'orderDate': 67891,\n                    'orderProfit': 12341,\n                },\n                {\n                    'orderDate': 23456,\n                    'orderProfit': 474,\n                },\n            ],\n        },\n        {\n            '_id': '',\n            'd': '',\n            'f': [\n                {\n                    'orderDate': 14232,\n                    'orderProfit': 12222,\n                },\n                {\n                    'orderDate': 643532,\n                    'orderProfit': 4343,\n                },\n                {\n                    'orderDate': 33423,\n                    'orderProfit': 5555,\n                },\n            ],\n        },\n    ],\n}\n`\n```\n`orderDate` is an int64 that represents the date that an order was made\n`orderProfit` is an int64 that represents the profit of an order\nI needed to return the document that had the biggest \"orderDate\" and check if the \"orderProfit\" was the one i was looking for.\nFor that matter I used a query like this (in an aggregate query):\n```\n`[\n    {\n        '$addFields': {\n            'orders': {\n                '$map': {\n                    'input': '$c',\n                    'as': 'c',\n                    'in': {\n                        'profit': {\n                            '$filter': {\n                                'input': '$$c.f',\n                                'cond': {\n                                    '$eq': [\n                                        {\n                                            '$max': '$$c.f.orderDate',\n                                        },\n                                        '$$this.orderDate',\n                                    ],\n                                },\n                            },\n                        },\n                    },\n                },\n            },\n        },\n    },\n    {\n        '$match': {\n            '$or': [\n                { 'orders.profit.orderProfit': 500 },\n            ],\n        },\n    },\n];\n`\n```\nIt is working properly.\nThe issue comes when trying to add this query to a `countDocuments()` query in order to fetch the total number of documents.\nIt is a requirement to use the `countDocuments()`.\nI just can't seem to make it work...\n$addFields throws as an unknown top level operator. if I remove the $addFields then I can't add to the countDocuments() the query that finds the max date.  If I totally remove it $match is an unknown operator.\n```\n`db.getCollection('orders').countDocuments(\n    {\n\"orders.profit.orderProfit\" : {\"Query that was shown previously\"}\n\n})\n`\n```",
      "solution": "The `countDocuments()` can't allow aggregation pipeline query,\nYou can use aggregation operators in match query but it cause the performance issues, you can use this when you don't have any way.\n\n$let Binds variables for use in the specified expression, and returns the result of the expression,\n`vars`, create variable for orders for your `$addFields` operation,\n`in`, `$map` to iterate loop of `$$orders.profit.orderProfit` nested array and check `$in` condition if your profit amount found then it will return true otherwise false\n`$anyElementTrue` will check returned is true then condition will true otherwise fales\n\n```\n`db.getCollection('orders').countDocuments({\n  $expr: {\n    $let: {\n      vars: {\n        orders: {\n          \"$map\": {\n            \"input\": \"$c\",\n            \"as\": \"c\",\n            \"in\": {\n              \"profit\": {\n                \"$filter\": {\n                  \"input\": \"$$c.f\",\n                  \"cond\": {\n                    \"$eq\": [{ \"$max\": \"$$c.f.orderDate\" }, \"$$this.orderDate\"]\n                  }\n                }\n              }\n            }\n          }\n        }\n      },\n      in: {\n        $anyElementTrue: {\n          $map: {\n            input: \"$$orders.profit.orderProfit\",\n            in: { $in: [500, \"$$this\"] }\n          }\n        }\n      }\n    }\n  }\n})\n`\n```\nPlayground\n\nSecond option, other way to handle this condition with less operators,\n\n`$filter` to match your both condition with `orderProfit`\n`$size` to get total result from above `$filter` result\nnow `$map` will return array of size that returned by filter\n`$sum` to sum that return result number, if its greater than 0 then condition will be true otherwise false\n\n```\n`db.getCollection('orders').countDocuments({\n  $expr: {\n    $sum: {\n      \"$map\": {\n        \"input\": \"$c\",\n        \"as\": \"c\",\n        \"in\": {\n          $size: {\n            \"$filter\": {\n              \"input\": \"$$c.f\",\n              \"cond\": {\n                $and: [\n                  { \"$eq\": [{ \"$max\": \"$$c.f.orderDate\" }, \"$$this.orderDate\"] },\n                  { \"$eq\": [\"$$this.orderProfit\", 500] }\n                ]\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n})\n`\n```\nPlayground",
      "question_score": 3,
      "answer_score": 1,
      "created_at": "2021-04-25T11:53:14",
      "url": "https://stackoverflow.com/questions/67252012/how-to-make-mongodb-work-with-countdocuments-and-addfields"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb-query",
      "question_id": 79610460,
      "title": "MongoDB - Document query with the field value dynamically (Condition with dynamic date)",
      "problem": "```\n`const orders = await db1.db.collection(\"order_all\").find({\n    progress_status_kd: { $in: [32, 33] },\n    ps_screen_preparing_n_packaging_permission: true,\n    $expr: {\n        $lte: [\n            \"$progress_status_kd_updated_at\",\n            // \"2025-05-07 13:59:40\" from this condition bellow it should return date format like this, it works if I put hardcoded date but not from this $cond\n            {\n                $cond: [\n                    { $eq : [\"$progress_status_kd\", 33] },\n                    moment().subtract(\"$kdsConfig.ps_screen_preparing_time\", 'hours'),\n                    moment().subtract(\"$kdsConfig.ps_screen_packaging_time\", 'minutes')\n                ]\n            }\n        ],\n    }\n}).toArray();\n`\n```\nI want to subtract a dynamic value based on a condition. If `progress_status_kd === 33`, it should subtract with the `ps_screen_preparing_time` value from `kdsConfig`. How can I achieve this, as currently it is not working correctly as expected?\nHere is my MongoDB data:\n```\n`{\n    \"_id\": ObjectId(\"681a0a555133c90d813dac22\"),\n    \"kdsConfig\": {\n        \"ps_screen_packaging_time\" : 10,\n        \"ps_screen_preparing_time\" : 10\n    },\n    \"progress_status_kd\": 33,\n    \"progress_status_kd_updated_at\": \"2025-05-07 14:00:00\",\n    \"ps_screen_preparing_n_packaging_permission\": true\n}\n`\n```",
      "solution": "You can't use `moment` as it cannot be constructed in the query to obtain the field value for a real-time query.\nInstead, you should use `$dateAdd` operator, `$$NOW` (current date time), and set the negative value to do subtraction.\nFor MongoDB version 5.0 and above:\n```\n`db.collection.find({\n  progress_status_kd: {\n    $in: [\n      32,\n      33\n    ]\n  },\n  ps_screen_preparing_n_packaging_permission: true,\n  $expr: {\n    $lte: [\n      \"$progress_status_kd_updated_at\",\n      {\n        $cond: [\n          {\n            $eq: [\n              \"$progress_status_kd\",\n              33\n            ]\n          },\n          {\n            $dateAdd: {\n              startDate: \"$$NOW\",\n              unit: \"hour\",\n              amount: {\n                $multiply: [\n                  -1,\n                  \"$kdsConfig.ps_screen_preparing_time\"\n                ]\n              }\n            }\n          },\n          {\n            $dateAdd: {\n              startDate: \"$$NOW\",\n              unit: \"minute\",\n              amount: {\n                $multiply: [\n                  -1,\n                  \"$kdsConfig.ps_screen_packaging_time\"\n                ]\n              }\n            }\n          }\n        ]\n      }\n    ]\n  }\n})\n`\n```\nDemo @ MongoPlayground\n\nAlternatively, you can use `$dateSubtract` operator which is more directly.\n```\n`db.collection.find({\n  progress_status_kd: {\n    $in: [\n      32,\n      33\n    ]\n  },\n  ps_screen_preparing_n_packaging_permission: true,\n  $expr: {\n    $lte: [\n      \"$progress_status_kd_updated_at\",\n      // \"2025-05-07 13:59:40\" from this condition bellow it should return date format like this, it works if I put hardcoded date but not from this $cond\n      {\n        $cond: [\n          {\n            $eq: [\n              \"$progress_status_kd\",\n              33\n            ]\n          },\n          {\n            $dateSubtract: {\n              startDate: \"$$NOW\",\n              unit: \"hour\",\n              amount: \"$kdsConfig.ps_screen_preparing_time\"\n            }\n          },\n          {\n            $dateSubtract: {\n              startDate: \"$$NOW\",\n              unit: \"minute\",\n              amount: \"$kdsConfig.ps_screen_packaging_time\"\n            }\n          }\n        ]\n      }\n    ]\n  }\n})\n`\n```\nDemo @ MongoPlayground",
      "question_score": 3,
      "answer_score": 4,
      "created_at": "2025-05-07T13:59:48",
      "url": "https://stackoverflow.com/questions/79610460/mongodb-document-query-with-the-field-value-dynamically-condition-with-dynami"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb-query",
      "question_id": 74174064,
      "title": "MongoDB fill missing dates in aggregation pipeline pagination",
      "problem": "I have this pipeline  :\n```\n`    let pipeline = [\n\n      {\n        $group: {\n          _id: \"$date\",\n          tasks: { $push: \"$$ROOT\" },\n        },\n      },\n      {\n        $sort: { _id: -1 },\n      },\n      { \n        $skip: skip //4,8,12,16...etc\n      },\n      { \n        $limit: 4 \n      }\n    ];\n\n    const aggregationData = await ScheduleTaskModel.aggregate(pipeline);\n`\n```\nwhere i group all \"tasks\" by date and i get that result :\n```\n`[\n    {\n        \"_id\": \"2022-10-21T00:00:00.000Z\",\n        \"tasks\": [...tasks with this date]\n    },\n    {\n        \"_id\": \"2022-10-20T00:00:00.000Z\",\n        \"tasks\": [...tasks with this date]\n    },\n    {\n        \"_id\": \"2022-10-18T00:00:00.000Z\",\n        \"tasks\": [...tasks with this date]\n    },\n    {\n        \"_id\": \"2022-10-16T00:00:00.000Z\",\n        \"tasks\": [...tasks with this date]\n    }\n]\n`\n```\nAs you can see,i have missing dates in between dates,which is fine,i can just manipulate the result with simple javascript,create an array with all dates between high and low date bound with empty tasks,and fill the dates that also appear in the result.\nThe problem lies when i want to \"paginate\" using `$skip`,if for example skip to the next 4 groups,i have no way to tell if the next date has any documents,and if it has'nt,i end up with something like the following :\n```\n`//FIRST RESULT WITH FILLED MISSING DATES\n[\n    {\n        \"_id\": \"2022-10-21T00:00:00.000Z\",\n        \"tasks\": [...tasks with this date]\n    },\n    {\n        \"_id\": \"2022-10-20T00:00:00.000Z\",\n        \"tasks\": [...tasks with this date]\n    },\n    {\n        \"_id\": \"2022-10-19T00:00:00.000Z\",\n        \"tasks\": [] //filled manually\n    },\n    {\n        \"_id\": \"2022-10-18T00:00:00.000Z\",\n        \"tasks\": [...tasks with this date]\n    },\n    {\n        \"_id\": \"2022-10-17T00:00:00.000Z\",\n        \"tasks\": [] //filled manually\n    },\n    {\n        \"_id\": \"2022-10-16T00:00:00.000Z\",\n        \"tasks\": [...tasks with this date]\n    }\n]\n\n//LOST DAYS IN BETWEEN SKIPS\n\n//SECOND RESULT WITH FILLED MISSING DATES\n[\n    {\n        \"_id\": \"2022-10-14T00:00:00.000Z\",\n        \"tasks\": [...tasks with this date]\n    },\n    {\n        \"_id\": \"2022-10-13T00:00:00.000Z\",\n        \"tasks\": [...tasks with this date]\n    },\n    {\n        \"_id\": \"2022-10-12T00:00:00.000Z\",\n        \"tasks\": [] //filled manually\n    },\n    {\n        \"_id\": \"2022-10-11T00:00:00.000Z\",\n        \"tasks\": [...tasks with this date]\n    },\n    {\n        \"_id\": \"2022-10-10T00:00:00.000Z\",\n        \"tasks\": [] //filled manually\n    },\n    {\n        \"_id\": \"2022-10-09T00:00:00.000Z\",\n        \"tasks\": [...tasks with this date]\n    }\n]\n`\n```\ni still bang my head to overcome this,and unfortunately `$densify` is out of the question since i use a mongo version before this was introduced",
      "solution": "If you're using Mongo version 5.1+ you can use the new $densify stage, it does exactly what you want, like so:\n```\n`db.collection.aggregate([\n  {\n    $group: {\n      _id: \"$date\",\n      tasks: {\n        $push: \"$$ROOT\"\n      }\n    }\n  },\n  {\n    $densify: {\n      field: \"_id\",\n      range: {\n        step: 1,\n        unit: \"day\",\n        bounds: \"full\"\n      }\n    }\n  },\n  {\n    $addFields: {\n      tasks: {\n        $ifNull: [\n          \"$tasks\",\n          []\n        ]\n      }\n    }\n  },\n  {\n    $sort: {\n      _id: -1\n    },\n    \n  },\n  {\n     $skip: n\n  },\n  {\n    $limit: 4\n  }\n])\n`\n```\nMongo Playground\nFor a lesser Mongo versions this becomes much much harder, while technically possible I recommend against it, here is a toy example of how I achieved it using Mongo version 4.2 syntax, this is not possible to achieve on earlier versions (unless you're willing not to cast the `_id` field into a date as return the result \"date\" as a number and then you can drop the `$toDate` casting).\nThis pipeline syntax can become much cleaner using date operators like `$dateAdd` and `$dateDiff` but these require version 5.0+\nThe issue is You have to group the entire result set in order to iterate over it and manually \"fill\" it using `$reduce` and `$map`, as you can imagine this is very inefficient:\n```\n`db.collection.aggregate([\n  {\n    $group: {\n      _id: \"$date\",\n      tasks: {\n        $push: \"$$ROOT\"\n      }\n    }\n  },\n  {\n    $sort: {\n      _id: 1\n    }\n  },\n  {\n    $group: {\n      _id: null,\n      roots: {\n        $push: \"$$ROOT\"\n      }\n    }\n  },\n  {\n    $addFields: {\n      roots: {\n        $reduce: {\n          input: \"$roots\",\n          initialValue: {\n            values: [],\n            lastDate: null\n          },\n          in: {\n            lastDate: \"$$this._id\",\n            values: {\n              $concatArrays: [\n                \"$$value.values\",\n                {\n                  $map: {\n                    input: {\n                      $range: [\n                        0,\n                        {\n                          $round: {\n                            $divide: [\n                              {\n                                \"$toDouble\": {\n                                  $subtract: [\n                                    \"$$this._id\",\n                                    {\n                                      $ifNull: [\n                                        \"$$value.lastDate\",\n                                        {\n                                          $subtract: [\n                                            \"$$this._id\",\n                                            86400000\n                                          ]\n                                        }\n                                      ]\n                                    }\n                                  ]\n                                }\n                              },\n                              86400000\n                            ]\n                          }\n                        }\n                      ]\n                    },\n                    as: \"dayDiff\",\n                    in: {\n                      $cond: [\n                        {\n                          $eq: [\n                            \"$$dayDiff\",\n                            0\n                          ]\n                        },\n                        \"$$this\",\n                        {\n                          tasks: [],\n                          _id: {\n                            $toDate: {\n                              $add: [\n                                \"$$this._id\",\n                                {\n                                  $multiply: [\n                                    {\n                                      \"$multiply\": [\n                                        86400000,\n                                        \"$$dayDiff\"\n                                      ]\n                                    },\n                                    -1\n                                  ]\n                                }\n                              ]\n                            }\n                          }\n                        }\n                      ]\n                    }\n                  }\n                }\n              ]\n            }\n          }\n        }\n      }\n    }\n  },\n  {\n    $unwind: \"$roots.values\"\n  },\n  {\n    $replaceRoot: {\n      newRoot: \"$roots.values\"\n    }\n  },\n  {\n    $sort: {\n      _id: -1\n    }\n  }\n])\n`\n```\nMongo Playground",
      "question_score": 3,
      "answer_score": 4,
      "created_at": "2022-10-23T21:14:58",
      "url": "https://stackoverflow.com/questions/74174064/mongodb-fill-missing-dates-in-aggregation-pipeline-pagination"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb-query",
      "question_id": 70035333,
      "title": "filter object from array and select field",
      "problem": "I've got a list of objects with name value pairs in them and I can't figure out how to retrieve a value from an array where name is a certain value.\nFor instance:\n```\n`    {\n            \"ordernumber\" : \"192915\",\n            \"orderparameters\" : {\n                    \"list\" : [\n                            {\n                                    \"composite\" : null,\n                                    \"name\" : \"CURRENCY_CODE\",\n                                    \"value\" : \"NOK\",\n                                    \"type\" : \"String\"\n                            },\n                            {\n                                    \"composite\" : null,\n                                    \"name\" : \"NEED_CUSTOMS_DOCUMENT\",\n                                    \"value\" : \"True\",\n                                    \"type\" : \"Boolean\"\n                            }\n              ]}\n    }\n`\n```\nNow I need to find the value of the item with\n\n\"name\": \"NEED_CUSTOMS_DOCUMENT\"\n\nso in this case\n\n\"value\": \"True\"\n\nI can list all the values with\n```\n` db.orders.aggregate({ $match: { ordernumber:'1234' } }, { $project: { 'orderparameters.list.name': 1 } }).pretty()\n`\n```\nBut then how to retrieve the value field from the index with that name?\nThe index could be different on every document so I cant simply query \"orderparameters.list.1.value\"",
      "solution": "Refine your aggregate pipeline to include a another stage with a `$filter` operator and that allows you to filter the list on a given condition, something like the following:\n```\n`db.orders.aggregate([\n    { $match: { \n        ordernumber: \"1234\", \n        \"orderparameters.list.name\": \"NEED_CUSTOMS_DOCUMENT\" \n    } },\n    { $project: {\n       list: {\n          $filter: {\n              input: \"$orderparameters.list\",\n              cond: { $eq: [\"$$this.name\", \"NEED_CUSTOMS_DOCUMENT\"] }\n          }\n       } \n    } },\n    { $project: { values: \"$list.value\" } }\n])\n`\n```",
      "question_score": 3,
      "answer_score": 3,
      "created_at": "2021-11-19T14:07:09",
      "url": "https://stackoverflow.com/questions/70035333/filter-object-from-array-and-select-field"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb-query",
      "question_id": 69157660,
      "title": "MongoDB Query Error : &#39;$arrayToObject requires an object keys of &#39;k&#39; and &#39;v&#39;. Found incorrect number of keys:1&#39;",
      "problem": "I am creating a mongoDB query that will be executed in Jaspersoft Studio,\nCurrently I am having error which I think the query is finding ObjectID format of \"ctr\" field.\nError Message:\n```\n` '$arrayToObject requires an object keys of 'k' and 'v'. Found incorrect number of keys:1'\n`\n```\nThe dataset that the query is looking\n```\n`{\n    ordernumbers: 1001,\n    items: \"testitems\",\n    orderlogss:\n        [\n            {\n                \"ctr\": {\"$oid\": \"6106d15ca48148060dbbec69\"},\n                \"stat\": \"Ordered\",\n                \"time\": {\"$date\": 1627836764264}\n            },\n            {\n                \"ctr\": {\"$oid\": \"6106d15ca48148060dbbec68\"},\n                \"stat\": \"Ordered\",\n                \"time\": {\"$date\": 1627836764265}\n            },\n            {\n                \"ctr\": {\"$oid\": \"6106db35abcea00b8d901035\"},\n                \"stat\": \"Registered\",\n                \"time\": {\"$date\": 1627839285518}\n            },\n            {\n                \"ctr\": {\"$oid\": \"6106db893a008e1a0112b8d8\"},\n                \"time\": {\"$date\": 1627839369359}\n            },\n            {\n                \"ctr\": {\"$oid\": \"6106db8f3a008e1a0112b922\"},\n                \"stat\": \"Dispensed\",\n                \"time\": {\"$date\": 1627839375597}\n            }\n        ]\n}\n`\n```\nI think the error is caused by automatic conversion of ObjectID to String. How can I make it to Object ID format and also the time is not in string format. ex. ctr: ObjectId(\"6106d15ca48148060dbbec69\"), time: ISODate(\"2025-05-28T18:29:59.999Z\")\nThis is the query I am using\n```\n`[\n    {\n        $project: {\n            itemfinal: \"$items\",\n            ordernumberfinal: \"$ordernumbers\",\n            rows: {\n                $arrayToObject: {\n                    $map: {\n                        input: \"$orderlogss\",\n                        in: {\n                            \"k\": \"$$this.stat\",\n                            \"v\": \"$$this.time\"\n                        }\n                    }\n                }\n            }\n        }\n    },\n    {\n        \"$addFields\": {\n            \"rows.finalitem\": \"$itemfinal\",\n            \"rows.finalorder\": \"$ordernumberfinal\"\n        }\n    },\n    {\n        \"$replaceRoot\": {\n            \"newRoot\": \"$rows\"\n        }\n    }\n]\n`\n```",
      "solution": "Your issue comes from this nested object:\n```\n`{\n \"ctr\": {\"$oid\": \"6106db893a008e1a0112b8d8\"},\n \"time\": {\"$date\": 1627839369359}\n} \n`\n```\nThis object is missing the `stat` field and the result is a missing value. that's why `$arrayToObject` fails.\nYou can just wrap it with $ifNull in your `$map` operation to avoid this, like so:\n```\n`{\n  \"$ifNull\": [\n    \"$$this.stat\",\n    \"replacement-stat\"\n  ]\n}\n`\n```\nMongo Playground",
      "question_score": 3,
      "answer_score": 4,
      "created_at": "2021-09-13T07:26:26",
      "url": "https://stackoverflow.com/questions/69157660/mongodb-query-error-arraytoobject-requires-an-object-keys-of-k-and-v-fo"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb-query",
      "question_id": 65596859,
      "title": "In Mongo C#, when grouping, how do you create a list of your groups with their entire contents?",
      "problem": "I am using Mongo C# v 2.11.5 to group users by their CategoryId. I would like the variable list_of_groups to contain a list of groups, where each group has a list of all the documents in the group, where each document contains all its individual properties.  I don't want to apply any accumulator functions on any fields.\nThe UserGroup class is:\n```\n`public class UserGroup\n{\n    public string gid {get; set;}\n    public List users { get; set; }\n}\n`\n```\nMy grouping query is\n```\n`    var list_of_groups = UserCollection\n        .Aggregate()\n        .Group(usr => usr.CategoryId,\n            g => new UserGroup { gid = g.Key, users = g.ToList() })\n        .ToList();\n`\n```\nI would like to create a \"list of lists\", where each inner list contains all the documents of its grouping i.e. this type of structure:\n```\n`List>(\n- List1 (list of complete documents belonging to CategoryId 1)\n- List2 (list of complete documents belonging to CategoryId 2)\n- List3 (list of complete documents belonging to CategoryId 3)\n)\n`\n```\nThe doesn't work as the following error is generated at runtime:\n```\n`System.NotSupportedException\n  HResult=0x80131515\n  Message=Specified method is not supported.\n  Source=MongoDB.Driver\n`\n```\nCan anyone point out what I'm missing here?  I've been going in circles for a while now.  Should I not being using the Group function, and maybe some other function is more appropriate since I don't want to apply any accumulator functions, and because I want each document in each group to have all its properties available?\nStack Trace:\n```\n`   at MongoDB.Driver.Linq.Processors.AccumulatorBinder.GetAccumulatorArgument(Expression node)\n   at MongoDB.Driver.Linq.Processors.AccumulatorBinder.TryGetAccumulatorTypeAndArgument(PipelineExpression node, AccumulatorType& accumulatorType, Expression& argument)\n   at MongoDB.Driver.Linq.Processors.AccumulatorBinder.VisitPipeline(PipelineExpression node)\n   at MongoDB.Driver.Linq.Expressions.PipelineExpression.Accept(ExtensionExpressionVisitor visitor)\n   at MongoDB.Driver.Linq.Expressions.ExtensionExpression.Accept(ExpressionVisitor visitor)\n   at System.Linq.Expressions.ExpressionVisitor.Visit(Expression node)\n   at MongoDB.Driver.Linq.Processors.AccumulatorBinder.Bind(Expression node, IBindingContext bindingContext)\n   at MongoDB.Driver.Linq.Processors.EmbeddedPipeline.EmbeddedPipelineBinder.Bind(Expression node, IBindingContext parent)\n   at MongoDB.Driver.Linq.Processors.SerializationBinder.BindEmbeddedPipeline(MethodCallExpression node)\n   at MongoDB.Driver.Linq.Processors.SerializationBinder.VisitMethodCall(MethodCallExpression node)\n   at System.Linq.Expressions.MethodCallExpression.Accept(ExpressionVisitor visitor)\n   at System.Linq.Expressions.ExpressionVisitor.Visit(Expression node)\n   at MongoDB.Driver.Linq.Processors.SerializationBinder.Visit(Expression node)\n   at System.Linq.Expressions.ExpressionVisitor.VisitMemberAssignment(MemberAssignment node)\n   at System.Linq.Expressions.ExpressionVisitor.VisitMemberBinding(MemberBinding node)\n   at System.Linq.Expressions.ExpressionVisitor.Visit[T](ReadOnlyCollection`1 nodes, Func`2 elementVisitor)\n   at System.Linq.Expressions.ExpressionVisitor.VisitMemberInit(MemberInitExpression node)\n   at System.Linq.Expressions.MemberInitExpression.Accept(ExpressionVisitor visitor)\n   at System.Linq.Expressions.ExpressionVisitor.Visit(Expression node)\n   at MongoDB.Driver.Linq.Processors.SerializationBinder.Visit(Expression node)\n   at MongoDB.Driver.Linq.Processors.SerializationBinder.Bind(Expression node, IBindingContext context, Boolean isClientSideProjection)\n   at MongoDB.Driver.Linq.Processors.PipelineBindingContext.Bind(Expression node, Boolean isClientSideProjection)\n   at MongoDB.Driver.Linq.Processors.PipelineBindingContext.Bind(Expression node)\n   at MongoDB.Driver.Linq.Translators.AggregateGroupTranslator.BindGroup[TKey,TDocument,TResult](PipelineBindingContext bindingContext, Expression`1 groupProjector, IBsonSerializer`1 parameterSerializer, Expression keySelector)\n   at MongoDB.Driver.Linq.Translators.AggregateGroupTranslator.Translate[TKey,TDocument,TResult](Expression`1 idProjector, Expression`1 groupProjector, IBsonSerializer`1 parameterSerializer, IBsonSerializerRegistry serializerRegistry, ExpressionTranslationOptions translationOptions)\n   at MongoDB.Driver.GroupExpressionProjection`3.Render(IBsonSerializer`1 documentSerializer, IBsonSerializerRegistry serializerRegistry)\n   at MongoDB.Driver.PipelineStageDefinitionBuilder.<>c__DisplayClass19_0`2.b__0(IBsonSerializer`1 s, IBsonSerializerRegistry sr)\n   at MongoDB.Driver.DelegatedPipelineStageDefinition`2.Render(IBsonSerializer`1 inputSerializer, IBsonSerializerRegistry serializerRegistry)\n   at MongoDB.Driver.AppendedStagePipelineDefinition`3.Render(IBsonSerializer`1 inputSerializer, IBsonSerializerRegistry serializerRegistry)\n   at MongoDB.Driver.MongoCollectionImpl`1.Aggregate[TResult](IClientSessionHandle session, PipelineDefinition`2 pipeline, AggregateOptions options, CancellationToken cancellationToken)\n   at MongoDB.Driver.MongoCollectionImpl`1.<>c__DisplayClass19_0`1.b__0(IClientSessionHandle session)\n   at MongoDB.Driver.MongoCollectionImpl`1.UsingImplicitSession[TResult](Func`2 func, CancellationToken cancellationToken)\n   at MongoDB.Driver.MongoCollectionImpl`1.Aggregate[TResult](PipelineDefinition`2 pipeline, AggregateOptions options, CancellationToken cancellationToken)\n   at MongoDB.Driver.CollectionAggregateFluent`2.ToCursor(CancellationToken cancellationToken)\n`\n```",
      "solution": "The problem you're facing here is that everything that's within `.Group()` method should get translated into corresponding MongoDB / Aggregation Framework operator and obviously there's no `.ToList()` counterpart. Therefore, there's no path forward with LINQ if you want to return a list of documents and you have to use Aggregation Framework's syntax directly and take advantage of $$ROOT variable:\n```\n`public class UserGroup\n{\n    [BsonElement(\"_id\")]\n    public string gid { get; set; }\n    public List users { get; set; }\n}\n\nvar groupDef = new BsonDocument()\n{\n    { \"_id\", \"$CategoryId\" },\n    { \"users\", new BsonDocument(){ { \"$push\", \"$$ROOT\" } } }\n};\n\nvar res = collection.Aggregate().Group(groupDef).ToList();\n`\n```",
      "question_score": 3,
      "answer_score": 4,
      "created_at": "2021-01-06T14:40:29",
      "url": "https://stackoverflow.com/questions/65596859/in-mongo-c-when-grouping-how-do-you-create-a-list-of-your-groups-with-their-e"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb-query",
      "question_id": 79808526,
      "title": "Query not printed as expected",
      "problem": "```\n`exports.getAllProducts = async (req, res) => {\ntry {\n    console.log(req.query);\n    const queryObj = { ...req.query };\n    const excludedFilters = [\"page\", \"limit\", \"fields\", \"sort\"];\n\n    excludedFilters.forEach((el) => delete queryObj[el]);\n\n    let queryStr = JSON.stringify(queryObj);\n    queryStr = queryStr.replace(\n        /\\b(gt|gte|lt|lte|ne|eq)\\b/g,\n        (match) => `$${match}`\n    );\n\n    const products = await Product.find(JSON.parse(queryStr)); \n    res.status(200).json({ message: 'success', data: products });\n} catch (error) {\n    res.status(500).json({ message: 'error', error });\n}};\n`\n```\nWhen I tried to use a query to make sure everything is working as expected, the query was `?price[eq]=5` and returned nothing. When printing it to debug, it was printed like this: `{ 'price[eq]': '5' }`\nExpress version is 5.1.0\nis there any method to format it as expected so that it will be `price: { gte: 5 }`\nafter doing some research, I believe that it should have been converted by default",
      "solution": "The issue is not in code. its query-parser behavior change.\nIn Express 4, `qs` was used by default, which converts\n```\n`?price[eq]=5 -> { price: { eq: '5' } }\n`\n```\nIn Express 5, they switched the default query parser to the built-in `querystring` module\n```\n`{ 'price[eq]': '5' } # what you see now\n`\n```\n\nYou can restore the old `qs` parsing behavior by enabling it.\nto Fix globally\n```\n`const express = require(\"express\");\nconst app = express();\n\napp.set(\"query parser\", \"extended\"); // enable advanced query parsing\n`\n```\nfix directly\n```\n`const qs = require(\"qs\");\n\nexports.getAllProducts = async (req, res) => {\n  try {\n    const queryObj = qs.parse(req._parsedUrl.query);\n    ...\n  } catch (error) {\n    res.status(500).json({ message: 'error', error });\n  }\n};\n`\n```",
      "question_score": 3,
      "answer_score": 3,
      "created_at": "2025-11-04T05:35:58",
      "url": "https://stackoverflow.com/questions/79808526/query-not-printed-as-expected"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb-query",
      "question_id": 78756222,
      "title": "How can I filter an ISODate field based only the in the time in MongoDB",
      "problem": "I have a collection with the following structure in MongoDB:\n```\n`    {\n      \"Name\": \"Test\",\n      \"StartDatetime\": ISODate('2024-05-15T15:00:41.180+00:00'),\n      \"EndDatetime\": ISODate('2024-05-15T15:30:12.120+00:00')\n    }\n`\n```\nI have many records in this collection with different datetimes, and I want to filter them based solely on the time, ignoring the date. For example, I want to retrieve all documents where the time is between 02:32:05 and 05:23:12.\nI wrote this query:\n```\n`db.collection.find({\n  $expr: {\n    $and: [\n      {\n        $or: [\n          {\n            $gte: [\n              {\n                $concat: [\n                  { $substr: [{ $hour: \"$StartDateTime\" }, 0, 2] },\n                  \":\",\n                  { $substr: [{ $minute: \"$StartDateTime\" }, 0, 2] },\n                  \":\",\n                  { $substr: [{ $second: \"$StartDateTime\" }, 0, 2] }\n                ]\n              },\n              \"02:32:05\"\n            ]\n          },\n          {\n            $and: [\n              { $eq: [{ $hour: \"$StartDateTime\" }, 2] },\n              { $gte: [{ $concat: [{ $substr: [{ $minute: \"$StartDateTime\" }, 0, 2] }, \":\", { $substr: [{ $second: \"$StartDateTime\" }, 0, 2] }] }, \"32:05\"] }\n            ]\n          }\n        ]\n      },\n      {\n        $or: [\n          {\n            $lte: [\n              {\n                $concat: [\n                  { $substr: [{ $hour: \"$EndDateTime\" }, 0, 2] },\n                  \":\",\n                  { $substr: [{ $minute: \"$EndDateTime\" }, 0, 2] },\n                  \":\",\n                  { $substr: [{ $second: \"$EndDateTime\" }, 0, 2] }\n                ]\n              },\n              \"05:23:12\"\n            ]\n          },\n          {\n            $and: [\n              { $eq: [{ $hour: \"$EndDateTime\" }, 5] },\n              { $lte: [{ $concat: [{ $substr: [{ $minute: \"$EndDateTime\" }, 0, 2] }, \":\", { $substr: [{ $second: \"$EndDateTime\" }, 0, 2] }] }, \"23:12\"] }\n            ]\n          }\n        ]\n      }\n    ]\n  }\n})\n`\n```\nHowever it is not working and I also believe there might be a better way to write this query. I have searched online, but I only found answers about how to create a query based solely on the date.",
      "solution": "Currently, it is unclear about your date comparing logic as provided in your example. However, the idea is to convert the date fields into strings using `$dateToString` that match your input date format (i.e. `%H:%m:%S`). Then, perform strings comparison for the strings.\n`db.collection.aggregate([\n  {\n    \"$match\": {\n      \"$expr\": {\n        \"$and\": [\n          {\n            \"$gte\": [\n              {\n                \"$dateToString\": {\n                  \"date\": \"$StartDatetime\",\n                  \"format\": \"%H:%m:%S\"\n                }\n              },\n              \"02:32:05\"// your start date input here\n              \n            ]\n          },\n          {\n            \"$lt\": [\n              {\n                \"$dateToString\": {\n                  \"date\": \"$EndDatetime\",\n                  \"format\": \"%H:%m:%S\"\n                }\n              },\n              \"05:23:12\"// your end date input here\n              \n            ]\n          }\n        ]\n      }\n    }\n  }\n])\n`\nMongo Playground",
      "question_score": 3,
      "answer_score": 3,
      "created_at": "2024-07-16T20:37:24",
      "url": "https://stackoverflow.com/questions/78756222/how-can-i-filter-an-isodate-field-based-only-the-in-the-time-in-mongodb"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb-query",
      "question_id": 74719710,
      "title": "How to update a property of the last object of a list in mongo",
      "problem": "I would like to update a property of the last objet stored in a list in mongo. For performance reasons, I can not pop the object from the list, then update the property, and then put the objet back. I can not either change the code design as it does not depend on me. In brief am looking for a way to select the last element of a list.\nThe closest I came to get it working was to use `arrayFilters` that I found doing research on the subject (mongodb core ticket: https://jira.mongodb.org/browse/SERVER-27089):\n`db.getCollection(\"myCollection\")\n    .updateOne(\n    {\n        _id: ObjectId('638f5f7fe881c670052a9d08')\n    },\n    {\n       $set: {\"theList.$[i].propertyToUpdate\": 'NewValueToAssign'}\n    },\n    {\n        arrayFilters: [{'i.type': 'MyTypeFilter'}]\n    }\n)\n`\nI use a filter to only update the objets in `theList` that have their property `type` evaluated as `MyTypeFilter`.\nWhat I am looking for is something like:\n`db.getCollection(\"maCollection\")\n    .updateOne(\n    {\n        _id: ObjectId('638f5f7fe881c670052a9d08')\n    },\n    {\n       $set: {\"theList.$[i].propertyToUpdate\": 'NewValueToAssign'}\n    },\n    {\n        arrayFilters: [{'i.index': -1}]\n    }\n)\n`\nI also tried using `\"theList.$last.propertyToUpdate\"` instead of `\"theList.$[i].propertyToUpdate\"` but the path is not recognized (since `$last` is invalid)\nI could not find anything online matching my case.\nThank you for your help, have a great day",
      "solution": "You want to be using Mongo's pipelined updates, this allows us to use aggregation operators within the update body.\nYou do however need to consider edge cases that the previous answer does not. (`null` list, `empty` list, and `list.length == 1`)\nOverall it looks like so:\n```\n`db.collection.update({\n  _id: ObjectId(\"638f5f7fe881c670052a9d08\")\n},\n[\n  {\n    $set: {\n      list: {\n        $concatArrays: [\n          {\n            $cond: [\n              {\n                $gt: [\n                  {\n                    $size: {\n                      $ifNull: [\n                        \"$list\",\n                        []\n                      ]\n                    }\n                  },\n                  1\n                ]\n              },\n              {\n                $slice: [\n                  \"$list\",\n                  0,\n                  {\n                    $subtract: [\n                      {\n                        $size: \"$list\"\n                      },\n                      1\n                    ]\n                  }\n                ]\n              },\n              []\n            ]\n          },\n          [\n            {\n              $mergeObjects: [\n                {\n                  $ifNull: [\n                    {\n                      $last: \"$list\"\n                    },\n                    {}\n                  ]\n                },\n                {\n                  propertyToUpdate: \"NewValueToAssign\"\n                }\n              ]\n            }\n          ]\n        ]\n      }\n    }\n  }\n])\n`\n```\nMongo Playground",
      "question_score": 3,
      "answer_score": 2,
      "created_at": "2022-12-07T17:15:04",
      "url": "https://stackoverflow.com/questions/74719710/how-to-update-a-property-of-the-last-object-of-a-list-in-mongo"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb-query",
      "question_id": 70671477,
      "title": "How to call a Mongodb Realm function from nodejs",
      "problem": "I have a function deployed in Mongodb Realm that I want to call in my local nodejs app. I am able to connect to the Mongodb Atlas Cluster via the `uri` and `MongoClient`.\nBut they are only able to query the database and not call that function. Is there a way I can call that function using this connected instance of `MongoClient`.\nI also discovered two npm packages - `realm` and `realm-web`. But I am unable to get the work done with either of them.\nin `realm-web`, the docs says to use `import` like\n```\n`import * as Realm from \"realm\";\n`\n```\nbut this doesn't work in node.\n\nSyntaxError: Cannot use import statement outside a module\n\nthen in `realm`, the method to open the realm apparantly has two arguments\n```\n`const Realm = require(\"realm\");\n\nconst realm = Realm.open({\n    path: \"myrealm\",\n    schema: [TaskSchema],\n  });\n`\n```\nand I am unable to find what to fill in them.\nAny suggestion or help is appreciated",
      "solution": "You need to have a user object in order to call a function.\nAt a bare minimum, something like:\n```\n`const credentials = Realm.Credentials.anonymous();\nconst user = await app.logIn(credentials);\nconst result = await user.functions.myFunction(arg1, arg2);\n`\n```\nWith this, you'll need to enable anonymous authentication (or a different auth provider for something more secure) and ensure that the function isn't private.\nReferencing the docs:\nGetting a user object: https://docs.mongodb.com/realm/sdk/node/examples/authenticate-users/\nCalling a function: https://docs.mongodb.com/realm/sdk/node/examples/call-a-function/",
      "question_score": 3,
      "answer_score": 3,
      "created_at": "2022-01-11T19:08:23",
      "url": "https://stackoverflow.com/questions/70671477/how-to-call-a-mongodb-realm-function-from-nodejs"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb-query",
      "question_id": 70740926,
      "title": "MongoDB get documents from a collection not in other collection",
      "problem": "I'm trying to get the documents from a collection where aren't in other collection (the common NOT IN clause in SQL).\nIf I run the next query:\n```\n`db.Companies_Movies.aggregate([\n    {\n        $project: \n        {\n            \"CompanyList.Movies.Code\" : 1\n        }\n    },\n    {\n        $match: \n        {\n             \"CompanyList.CodeCompany\": \"23\"\n        }\n    },\n    {   \n        $lookup:    \n        {\n            from: \"Movies\",\n            localField: \"CompanyList.Movies.Code\",\n            foreignField: \"Movie.Code\",\n            as: \"matched_docs\"\n        }\n    }\n]);\n`\n```\nThis query shows the movies includes in CompanyList.Movies.Code and in Movie.Code. Good.\nBut I just have the rest of movies includes in CompanyList.Movies whose codes aren't included in Movie.Code.\nAs Nikos Tsagkas said in Get \"data from collection b not in collection a\" in a MongoDB shell query it should be sufficient to include the following sentence:\n```\n`    {\n        $match: { \"matched_docs\": { $eq: [] } }\n    }\n`\n```\nBut when I run my final code, it doesn't returns anything:\n```\n`    db.Companies_Movies.aggregate([\n    {\n        $project: \n        {\n            \"CompanyList.Movies.Code\" : 1\n        }\n    },\n    {\n        $match: \n        {\n            \"CompanyList.CodeCompany\": \"23\"\n        }\n    },\n    {\n        $lookup:    \n        {\n            from: \"Movies\",\n            localField: \"CompanyList.Movies.Code\",\n            foreignField: \"Movie.Code\",\n            as: \"matched_docs\"\n        }\n    },\n    {\n        $match: { \"matched_docs\": { $eq: [] } }\n    }\n    ]);\n`\n```\nThere are 59 documents that are not returned by this code.\n\nThis is my pipeline I've created in MongoDB Compass after Tom's changes and it still doesn't work:\n```\n`[{\n    $match: \n     {\n        'CompanyList.CodeCompany': '23'\n     }\n     },\n     {\n          $lookup: \n          {\n               from: 'Movies',\n               localField: 'CompanyList.Movies.Code',\n               foreignField: 'Movie.Code',\n               as: 'docs'\n          }\n     }, \n     {\n          $project: \n          {\n               'CompanyList.Movies.Code': 1,\n               'CompanyList.CodeCompany': 1\n          }\n     }, \n     {\n          $match: \n          {\n               docs:{  $eq: [] }\n          }\n    }]\n`\n```\nIf I delete the $project, it not works either.\n\nSample Data (reduced)\nCompanies_Movies collection:\n```\n`{\n_id:ObjectId(\"61bf47b974641866e1244e65\"),\n\"CompanyList\": {\n    \"CodeCompany\": \"23\",\n    \"NameCompany\": \"Company Name Entertainment\",\n    \"Movies\": [{\n        \"Code\": \"123\",\n        \"Name\": \"Title 1\",\n        \"Order\": 1,\n        \"UserDescription\": null\n    }, {\n        \"Code\": \"124\",\n        \"Name\": \"Title 2\",\n        \"Order\": 2,\n        \"UserDescription\": null\n    }, {\n        \"Code\": \"125\",\n        \"Name\": \"Title 3\",\n        \"Order\": 3,\n        \"UserDescription\": null\n    }],\n    \"DateInserted\": {\n        \"$date\": \"2021-12-13T17:30:06.824Z\"\n    }\n  }\n}\n`\n```\nMovies collection:\n```\n`[{\n_id:ObjectId(\"61bf57bc9d1f93b7ae5fa785\"),\n\"Movie\": {\n    \"Code\": \"123\",\n    \"OriginalTitle\": \"Title 1\",\n    \"Year\": 2021\n },\n_id:ObjectId(\"61bf57bc9d1f93b7ae5fa786\"),\n\"Movie\": {\n    \"Code\": \"124\",\n    \"OriginalTitle\": \"Title 2\",\n    \"Year\": 2021\n },\n_id:ObjectId(\"61bf57bc9d1f93b7ae5fa787\"),\n\"Movie\": {\n    \"Code\": \"125\",\n    \"OriginalTitle\": \"Title 3\",\n    \"Year\": 2021\n },\n_id:ObjectId(\"61bf57bc9d1f93b7ae5fa788\"),\n\"Movie\": {\n    \"Code\": \"126\",\n    \"OriginalTitle\": \"Title 4\",\n    \"Year\": 2021\n }\n}]\n`\n```\nAnyone know what might be happening?\nThanks to everyone.",
      "solution": "This is simply caused by your `$project` stage, after you run:\n```\n`{\n  $project: {\n    \"CompanyList.Movies.Code\" : 1\n  }\n},\n`\n```\nYou're data will look like this:\n```\n`{\n   CompanyList: [\n       {\n          Movies: { code: \"123\", ... other fields } \n       }\n   ]\n}\n`\n```\nNow you're trying to match `\"CompanyList.CodeCompany\": \"23\"`, but the field `CodeCompany` simply does not exist anymore as you did not provide it in the project stage.\nSo just change you're projection stage to include fields you will use in later stages:\n```\n`{\n  $project: {\n    \"CompanyList.Movies.Code\" : 1,\n    \"CompanyList.CodeCompany\": 1\n  }\n},\n`\n```",
      "question_score": 3,
      "answer_score": 2,
      "created_at": "2022-01-17T12:54:01",
      "url": "https://stackoverflow.com/questions/70740926/mongodb-get-documents-from-a-collection-not-in-other-collection"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb-query",
      "question_id": 67516405,
      "title": "How to add default criteria to all the queries by default in mongo spring boot",
      "problem": "I have a requirement to soft delete documents in a given MongoDB collection. For that, I use a boolean called deleted. So now when I am retrieving data from the database, I have to always mention taking the data where the `deleted=false`.\nEg:\n```\n`public Organization findOrgById(String id) {\n    Query query = new Query();\n    Criteria criteria = Criteria.where(Constants.ENTITY_ID).is(id)\n            .and(Constants.DELETED).is(false);\n    query.addCriteria(criteria);\n    Organization res = mongoTemplate.findOne(query, Organization.class);\n    return res;\n}\n`\n```\nIs there a way to specify that always all the criteria to add `deleted=false` by default without mentioning it in the code itself?\nIn Hibernate core there is an annotation `@Where` but it is not working with mongo documents.",
      "solution": "I think the best way to do this is extend the `MongoTemplate` class that will add your `deleted=false` condition to all Find queries.\nHere is an example of how to do it with one method used to execute `findOne` queries:\n```\n`public class ExtendedMongoTemplate extends MongoTemplate {\n\n    private static final Document DELETED_CRITERIA_DOC = Criteria.where(Constants.DELETED).is(false)\n        .getCriteriaObject();\n\n    @Override\n    protected  T doFindOne(\n            String collectionName,\n            Document query,\n            Document fields,\n            CursorPreparer preparer,\n            Class entityClass) {\n        query.putAll(DELETED_CRITERIA_DOC);\n        return super.doFindOne(collectionName, query, fields, preparer, entityClass);\n    }\n    ...\n}\n`\n```\nThis method is called in the method `doFindOne(Query query, Class entityClass)` (the last one delegates executing\nOther methods to override are:\n```\n`protected  List doFind(String collectionName, Document query, Document fields,\n    Class entityClass, CursorPreparer preparer);\n\nprotected  T doFindAndRemove(String collectionName, Document query, Document fields,\n    Document sort, @Nullable Collation collation, Class entityClass);\n\nprotected  T doFindAndModify(String collectionName, Document query, Document fields, Document sort,\n    Class entityClass, UpdateDefinition update, @Nullable FindAndModifyOptions options);\n\nprotected  T doFindAndReplace(String collectionName, Document mappedQuery, Document mappedFields,\n    Document mappedSort, com.mongodb.client.model.Collation collation, Class entityType,\n    Document replacement, FindAndReplaceOptions options, Class resultType);\n`\n```\nThese methods execute queries at low-level, so they accept BSON-documents with the query criteria, not Spring's criteria. If you do this, the Find-methods will add an additional criteria to all you queries.\nYou also can override methods `findOne`, `find`, `findAndModify` and so on in a similar manner, but there are a lot of these methods that all use `doFind*` methods. Thus overriding `doFind*` will lead to work with all Find-queries. And don't forget override also `findById` (it also uses `doFindOne` internally).\nBy the way, `@Where` annotation is from Hibernate, but Spring Data Mongo doesn't use them. It requires its own annotations to work with your entities.",
      "question_score": 3,
      "answer_score": 2,
      "created_at": "2021-05-13T10:46:43",
      "url": "https://stackoverflow.com/questions/67516405/how-to-add-default-criteria-to-all-the-queries-by-default-in-mongo-spring-boot"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb-query",
      "question_id": 66853444,
      "title": "mongo convert single object to one element array",
      "problem": "I have the following problem. In this example dataset:\nhttps://mongoplayground.net/p/itNnrDzZ4JQ\n```\n`[\n  {\n    \"name\": \"test\",\n    \"questions\": [\n      {\n        \"headline\": \"headline 1\",\n        \"answer\": {\n          \"@id\": \"001\",\n          \"m_IMG\": {\n            \"type\": \"06\",\n            \"img\": {\n              \"@id\": \"1111\",\n              \"shape\": [\n                {\n                  \"@src\": \"/1_1.jpg\",\n                  \"@type\": \"Z05\"\n                },\n                {\n                  \"@src\": \"/1_2.jpg\",\n                  \"@type\": \"Z08\"\n                }\n              ]\n            }\n          },\n          \"text\": \"text1\"\n        }\n      },\n      {\n        \"headline\": \"Headline 2\",\n        \"answer\": [\n          {\n            \"@id\": \"001\",\n            \"m_IMG\": {\n              \"@type\": \"24\",\n              \"img\": {\n                \"@id\": \"1111\",\n                \"shape\": {\n                  \"@src\": \"/2_1.jpg\",\n                  \"@type\": \"Z05\"\n                }\n              },\n              \"text\": \"Test2\"\n            }\n          },\n          {\n            \"@id\": \"002\",\n            \"m_IMG\": {\n              \"@typeName\": \"\",\n              \"@type\": \"25\",\n              \"img\": {\n                \"@id\": \"2222\",\n                \"shape\": [\n                  {\n                    \"@src\": \"/2_1.jpg\",\n                    \"@type\": \"Z05\"\n                  },\n                  {\n                    \"@src\": \"2_2.jpg\",\n                    \"@type\": \"Z08\"\n                  }\n                ]\n              }\n            },\n            \"text\": \"Test3\"\n          }\n        ]\n      }\n    ]\n  }\n]\n`\n```\nYou will find that if only a single `answer` to a question exists it is represented as an object. If more than one exists then it is an array. This is also true in the `shape` node. I wonder what is the best way to convert those nodes to be always arrays (so if one answer exists it would be an array of one element).",
      "solution": "`$map` to iterate loop of `questions` array\ncheck condition if answer is not array then put into array bracket and input as `$map` to iterate loop of `answer` array\ncheck condition is not `shape` array ten wrap it in to array bracket\n`$mergeObjects` with sub documents `img`, `m_IMG` objects\n`$mergeObjects` with current document and updated `answer` array\n\n```\n`db.collection.aggregate([\n  {\n    $set: {\n      questions: {\n        $map: {\n          input: \"$questions\",\n          as: \"q\",\n          in: {\n            $mergeObjects: [\n              \"$$q\",\n              {\n                answer: {\n                  $map: {\n                    input: {\n                      $cond: [\n                        { $eq: [{ $isArray: \"$$q.answer\" }, true] },\n                        \"$$q.answer\",\n                        [\"$$q.answer\"]\n                      ]\n                    },\n                    as: \"a\",\n                    in: {\n                      $mergeObjects: [\n                        \"$$a\",\n                        {\n                          m_IMG: {\n                            $mergeObjects: [\n                              \"$$a.m_IMG\",\n                              {\n                                img: {\n                                  $mergeObjects: [\n                                    \"$$a.m_IMG.img\",\n                                    {\n                                      shape: {\n                                        $cond: [\n                                          { $eq: [{ $isArray: \"$$a.m_IMG.img.shape\" }, true] },\n                                          \"$$a.m_IMG.img.shape\",\n                                          [\"$$a.m_IMG.img.shape\"]\n                                        ]\n                                      }\n                                    }\n                                  ]\n                                }\n                              }\n                            ]\n                          }\n                        }\n                      ]\n                    }\n                  }\n                }\n              }\n            ]\n          }\n        }\n      }\n    }\n  }\n])\n`\n```\nPlayground",
      "question_score": 3,
      "answer_score": 1,
      "created_at": "2021-03-29T13:30:54",
      "url": "https://stackoverflow.com/questions/66853444/mongo-convert-single-object-to-one-element-array"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb-query",
      "question_id": 76830552,
      "title": "How to lookup in the same document under a collection in MongoDB",
      "problem": "I am having below document in MongoDB [UPDATE : Shortened the document]\n```\n`{\n  \"sections\": [\n    {\n      \"_id\": {\n        \"$oid\": \"64cbeb62b669cd29a5719a8f\"\n      },\n      \"categories\": [\n        {\n          \"measureValues\": [\n            \"64cbeb62b669cd29a5719a95\",\n            \"64cbeb62b669cd29a5719a99\"\n          ]\n        }\n      ]\n    }\n  ],\n  \"measures\": [\n    {\n      \"_id\": {\n        \"$oid\": \"64cbeb62b669cd29a5719a93\"\n      },\n      \"name\": {\n        \"64cbeb62b669cd29a5719a92\": {\n          \"nodeType\": \"nodeName\",\n          \"value\": \"measure-1\"\n        }\n      },\n      \"measureValues\": [\n        {\n          \"_id\": {\n            \"$oid\": \"64cbeb62b669cd29a5719a95\"\n          },\n          \"measureValues\": {\n            \"nodeName\": {\n              \"64cbeb62b669cd29a5719a94\": {\n                \"nodeType\": \"nodeName\",\n                \"value\": \"measureValue\",\n                \"_class\": \"com.myapp.cqrsdemo.model.DataObj\"\n              }\n            },\n            \"comment\": {\n              \"64cbeb62b669cd29a5719a97\": {\n                \"nodeType\": \"comment\",\n                \"value\": \"this is a comment\",\n                \"_class\": \"com.myapp.cqrsdemo.model.DataObj\"\n              }\n            }\n          },\n          \"categoriesIds\": [\n            \"64cbeb62b669cd29a5719a91\"\n          ],\n          \"measureName\": \"measure-1\"\n        },\n        {\n          \"_id\": {\n            \"$oid\": \"64cbeb62b669cd29a5719a99\"\n          },\n          \"measureValues\": {\n            \"nodeName\": {\n              \"64cbeb62b669cd29a5719a98\": {\n                \"nodeType\": \"nodeName\",\n                \"value\": \"measureValue\",\n                \"_class\": \"com.myapp.cqrsdemo.model.DataObj\"\n              }\n            },\n            \"date\": {\n              \"64cbeb62b669cd29a5719a9b\": {\n                \"nodeType\": \"date\",\n                \"value\": \"2023-02-02\",\n                \"_class\": \"com.myapp.cqrsdemo.model.DataObj\"\n              }\n            }\n          },\n          \"categoriesIds\": [\n            \"64cbeb62b669cd29a5719a91\"\n          ],\n          \"measureName\": \"measure-1\"\n        }\n      ]\n    }\n  ],\n  \"_class\": \"com.myapp.cqrsdemo.model.ProjectGroup\"\n}\n`\n```\nI want to fetch the \"measureValues\" under \"measures\" where the id of the \"measureValues\" is matching with the id of the measureValues under categories. I want to include the measureValue object under categories instead of just the Ids.\nHow can I do that? I tried writing aggregate pipeline, but it failed.\nMy Output should look like below.\n```\n`{\n  \"sections\": [\n    {\n      \"_id\": {\n        \"$oid\": \"64cbeb62b669cd29a5719a8f\"\n      },\n      \"categories\": [\n        {\n          \"measureValues\": [\n            {\n              \"_id\": {\n                \"$oid\": \"64cbeb62b669cd29a5719a95\"\n              },\n              \"measureValues\": {\n                \"nodeName\": {\n                  \"64cbeb62b669cd29a5719a94\": {\n                    \"nodeType\": \"nodeName\",\n                    \"value\": \"measureValue\",\n                    \"_class\": \"com.myapp.cqrsdemo.model.DataObj\"\n                  }\n                },\n                \"comment\": {\n                  \"64cbeb62b669cd29a5719a97\": {\n                    \"nodeType\": \"comment\",\n                    \"value\": \"this is a comment\",\n                    \"_class\": \"com.myapp.cqrsdemo.model.DataObj\"\n                  }\n                }\n              },\n              \"categoriesIds\": [\n                \"64cbeb62b669cd29a5719a91\"\n              ],\n              \"measureName\": \"measure-1\"\n            },\n            {\n              \"_id\": {\n                \"$oid\": \"64cbeb62b669cd29a5719a99\"\n              },\n              \"measureValues\": {\n                \"nodeName\": {\n                  \"64cbeb62b669cd29a5719a98\": {\n                    \"nodeType\": \"nodeName\",\n                    \"value\": \"measureValue\",\n                    \"_class\": \"com.myapp.cqrsdemo.model.DataObj\"\n                  }\n                },\n                \"date\": {\n                  \"64cbeb62b669cd29a5719a9b\": {\n                    \"nodeType\": \"date\",\n                    \"value\": \"2023-02-02\",\n                    \"_class\": \"com.myapp.cqrsdemo.model.DataObj\"\n                  }\n                }\n              },\n              \"categoriesIds\": [\n                \"64cbeb62b669cd29a5719a91\"\n              ],\n              \"measureName\": \"measure-1\"\n            }\n          ]\n        }\n      ]\n    }\n  ],\n  \"_class\": \"com.myapp.cqrsdemo.model.ProjectGroup\"\n}\n`\n```",
      "solution": "One option is:\n\nCreate a var `measureValuesArr` and store inside it a single flatten array of all `measureValues` under all `measures`\nSince the `sections.categories.measureValues` is a double nested array (inside an array on arrays) use double `$map` to get into all instances of it.\nUse `$arrayElemAt` with `$indexOfArray` to find the matching `measureValues` item inside `measureValuesArr`\n\n```\n`db.collection.aggregate([\n  {$set: {\n      sections: {$let: {\n          vars: {measureValuesArr: {$reduce: {\n                input: \"$measures\",\n                initialValue: [],\n                in: {$concatArrays: [\"$$value\", \"$$this.measureValues\"]}\n          }}},\n          in: {$map: {\n              input: \"$sections\",\n              as: \"s\",\n              in: {$mergeObjects: [\n                  \"$$s\",\n                  {categories: {$map: {\n                        input: \"$$s.categories\",\n                        as: \"c\",\n                        in: {$mergeObjects: [\n                            \"$$c\",\n                            {measureValues: {$map: {\n                                  input: \"$$c.measureValues\",\n                                  as: \"m\",\n                                  in: {$arrayElemAt: [\n                                      \"$$measureValuesArr\",\n                                      {$indexOfArray: [\n                                          \"$$measureValuesArr._id\",\n                                          {$toObjectId: \"$$m\"}\n                                      ]}\n                                  ]}\n                            }}}\n                        ]}\n                  }}}\n              ]}\n          }}\n      }}\n  }}\n])\n`\n```\nSee how it works on the playground example\n*This solution assumes that a matching object does exists somewhere under `measures` if there is an option that it does not exists, an handling of this case should be added to the query.",
      "question_score": 3,
      "answer_score": 1,
      "created_at": "2023-08-03T19:58:17",
      "url": "https://stackoverflow.com/questions/76830552/how-to-lookup-in-the-same-document-under-a-collection-in-mongodb"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb-query",
      "question_id": 73203579,
      "title": "Given an id of a document with recursive field `children`, find all documents that reference the document or any of its children",
      "problem": "I have a collection of product folders `productfolders` and a collection of products `products`.\n`const ProductFolderSchema = new Schema(\n  {\n    folderName: { type: String, required: true },\n    parent: { type: Schema.Types.ObjectId, ref: 'ProductFolder' },\n    children: [{ type: Schema.Types.ObjectId, ref: 'ProductFolder' }],\n  }\n);\n\nconst ProductSchema = new Schema(\n  {\n    productName: String,\n    folder: { type: Schema.Types.ObjectId, ref: 'ProductFolder' },\n  },\n);\n`\nI have a backend that receives query parameter `folderId` and should return all products whose `folder` property is equal to `folderId` or is a descendant of folder with `_id` of `folderId` (meaning `folder` is one of the children of folder with `_id` of `folderId` - children can be nested deep inside children's children).\nFor example, consider collections `productfolders` and `products` that look like this:\n`const productfolders = [\n  {\n    \"_id\": \"62e74dac78c13b738874e1a9\",\n    \"folderName\": \"Weapons\",\n    \"children\": [\n      {\n        \"_id\": \"62e74dd278c13b738874e1ac\",\n        \"folderName\": \"Bows\",\n        \"parent\": \"62e74dac78c13b738874e1a9\",\n        \"children\": [\n          {\n            \"_id\": \"62e74ddb78c13b738874e1b1\",\n            \"folderName\": \"Long Bows\",\n            \"parent\": \"62e74dd278c13b738874e1ac\",\n            \"children\": [],\n          },\n          {\n            \"_id\": \"62e74de278c13b738874e1b7\",\n            \"folderName\": \"Short Bows\",\n            \"parent\": \"62e74dd278c13b738874e1ac\",\n            \"children\": [],\n          }\n        ],\n      },\n    ]\n  }\n];\n\nconst products = [\n  {\n    \"productName\": \"Curved Bow\",\n    \"folder\": \"62e74de278c13b738874e1b7\",\n    \"_id\": \"62e237368fbde6ed77e3e489\"\n  }\n];\n`\nWhen I pass `folderId` of `62e74dac78c13b738874e1a9` (`\"folderName\": \"Weapons\"`), I want `\"Curved Bow\"` product to be found because its folder is a deep children of `\"Weapons\"` folder.\nI think you can only search something in recursive structures using `$graphLookup` but I couldn't figure out how to pass the variable `folderId` to its `startsWith` operator(sorry if I'm using the wrong naming of things)\nHere's example db: https://mongoplayground.net/p/Yxps44cfG28\nHere's my code that doesn't find anything:\n`const products = await ProductModel.aggregate([\n  {\n    $graphLookup: {\n      from: 'productfolders',\n      startWith: folderId, // can only pass mongo expressions here, not working with variables\n      connectFromField: '_id',\n      connectToField: 'children',\n      as: 'output',\n    },\n  },\n]);\n`\nHow do I find all products whose `folder` property is equal to or is a deep children of folder with `folderId`?",
      "solution": "Your search was quite close. I guess the confusion came from having both parent and children fields in your schema.\nAs I mentioned in the comment, I don't see how you keep children up to date for all parents when you add a new folder to one of the children, but I will leave it with you. For now I will just ignore the children array. `parent` is enough for $graphLookup:\n```\n`db.products.aggregate([\n  {\n    \"$graphLookup\": {\n      \"from\": \"productfolders\",\n      \"startWith\": \"$folder\",\n      \"connectFromField\": \"parent\",\n      \"connectToField\": \"_id\",\n      \"as\": \"path\"\n    }\n  },\n  {\n    \"$match\": {\n      \"path._id\": \"\"\n    }\n  }\n])\n`\n```\nHere $graphLookup builds a flat array of all parents for each product:\n\n`startWith` is the `folder` from `products` document\n`connectToField` is the corresponding field in `productfolders` collection\n`productfolders` is thee field of the `productfolders` document to use in the next recursive call instead of the `startWith`\n\nSo the `path` array for the `Fireworks (\"folder\": \"62e7bead91041bdddf25dd4b\")`  will be:\n```\n`[\n  {\n    \"_id\": \"62e7bead91041bdddf25dd4b\",\n    \"folderName\": \"Short swords\\n\",\n    \"parent\": \"62e79c6191041bdddf25dd1c\"\n  },\n  {\n    \"_id\":    \"62e79c6191041bdddf25dd1c\",\n    \"folderName\": \"Swords\",\n    \"parent\": \"62e74dac78c13b738874e1a9\"\n  },\n  {\n    \"_id\":    \"62e74dac78c13b738874e1a9\",\n    \"folderName\": \"Weapons\"\n  }\n]\n`\n```\nDo you see the chain - `parent` of the document matches `_id` of the next in the chain, right?\nSo after the $graphLookup stage you have full folder path from the root to the product's folder for each product. Now you just $match products that do have the folder in question anywhere in the chain.\nThere is a simplified example on https://mongoplayground.net/p/Cy-_SzzcdNT",
      "question_score": 3,
      "answer_score": 1,
      "created_at": "2022-08-02T09:29:41",
      "url": "https://stackoverflow.com/questions/73203579/given-an-id-of-a-document-with-recursive-field-children-find-all-documents-th"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb-query",
      "question_id": 71596834,
      "title": "MongoError: PlanExecutor error during aggregation",
      "problem": "I have tree records in mongodb but there could be many more, I'm getting shops by an ID coming from frontend\nI need to get 20 records and group them by itemId and colorId, and get counts for every shop. the count of shops can be 1,2,3,....10etc..\nthis is output I need:\n```\n`+--------+----------+-------+-------+-------+\n| itemId | colorId  | shop1 | shop2 | shop3 |\n+========+==========+=======+=======+=======+\n| 1      | colorId1 | 5     | 0     | 3     |\n+--------+----------+-------+-------+-------+\n| 2      | colorId2 | 3     | 0     | 0     |\n+--------+----------+-------+-------+-------+\n| 3      | colorId2 | 0     | 3     | 0     |\n+--------+----------+-------+-------+-------+\n| 2      | colorId1 | 0     | 5     | 0     |\n+--------+----------+-------+-------+-------+\n| 3      | colorId1 | 0     | 0     | 5     |\n+--------+----------+-------+-------+-------+\n`\n```\nhere is my data and query - here shopId is string and it's work good.\nbut when I use this query on my local mashine, I'm getting this error:\nMongoError: PlanExecutor error during aggregation :: caused by :: $arrayToObject requires an object with keys 'k' and 'v', where the value of 'k' must be of type string. Found type: objectId\nbut when I change shopId to the ObjectId I'm getting error.\nObjectId versoin",
      "solution": "Per your request in the comments (if I got it right):\n```\n`    db.collection.aggregate([\n  {\n    \"$match\": {}// After the first `$match`, we `$group` in order to get all shopIds in each document.\nNext we `$unwind` and `$group` by the group you wanted: by colorId and itemId. Then we are adding all the shops with count 0 and removing the ones that do have actual count. Last three steps are just for sorting, summing and formating.\nYou can play with it here.",
      "question_score": 3,
      "answer_score": 1,
      "created_at": "2022-03-24T04:47:04",
      "url": "https://stackoverflow.com/questions/71596834/mongoerror-planexecutor-error-during-aggregation"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb-query",
      "question_id": 70463898,
      "title": "MongoDb operator not working for array of object properties",
      "problem": "I am facing a problem with `MongoDB` query. Our collection named is `products` and the data is placed something like this.\n`    {\n      \"_id\":\"61b823681975ba537915cb0c\",\n      \"salesInfo\":{\n        \"_id\":\"61b823681975ba537915c23c\",\n        \"salesDate\":[\n          {\n            \"_id\":\"61b3aa4a7b04b30cd0a76b06\",\n            \"salesQuantity\":100,\n            \"soldPieces\":36,\n            \"holdPieces\":0\n          },\n          {\n            \"_id\":\"61b3aa4a7b04b30cd0a75506\",\n            \"salesQuantity\":100,\n            \"soldPieces\":36,\n            \"holdPieces\":0\n          }\n        ]\n      }\n    }\n`\nI want to add a new `field` named `percentageSold` inside an array of objects, and the value should be the calculation of the following formula `((soldPieces + holdPieces) / salesQuantity * 100)`.\nMy query is this but it is returning `null` for the `percentageSold` property.\n`    db.products.aggregate( [\n       {\n         $addFields: {\n           \"salesInfo.salesDate.percentageSold\": {$divide: [{$add: [\"$salesDate.soldPieces\", \"$salesDate.holdPieces\"]}, {$multiply: [\"$salesDate.salesQuantity\", 100]}]}\n         }\n       }\n    ])\n`",
      "solution": "As `salesInfo.salesDate` is an array field, you need to to use array operator like `$map` to perform element-wise operation.\n`db.products.aggregate([\n  {\n    $addFields: {\n      \"salesInfo.salesDate\": {\n        \"$map\": {\n          \"input\": \"$salesInfo.salesDate\",\n          \"as\": \"s\",\n          \"in\": {\n            \"_id\": \"$$s._id\",\n            \"salesQuantity\": \"$$s.salesQuantity\",\n            \"soldPieces\": \"$$s.soldPieces\",\n            \"holdPieces\": \"$$s.holdPieces\",\n            \"percentageSold\": {\n              $divide: [\n                {\n                  $add: [\n                    \"$$s.soldPieces\",\n                    \"$$s.holdPieces\"\n                  ]\n                },\n                {\n                  $multiply: [\n                    \"$$s.salesQuantity\",\n                    100\n                  ]\n                }\n              ]\n            }\n          }\n        }\n      }\n    }\n  }\n])\n`\nHere is the Mongo playground for your reference.",
      "question_score": 3,
      "answer_score": 1,
      "created_at": "2021-12-23T16:06:02",
      "url": "https://stackoverflow.com/questions/70463898/mongodb-operator-not-working-for-array-of-object-properties"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb-query",
      "question_id": 66167965,
      "title": "How to find distinct values of fields using facet operation in mongodb",
      "problem": "The filteredAccording part and the categorizedBy is working as expected using the query which I provided in the link but I am facing issues in the findDistinct part.\nIn mongodb I have the following data:\n```\n` {\n        \"_id\": 10001,\n        \"university\": \"SPYU\",\n        \"Courses\": [\n          \"English\",\n          \"French\"\n        ],\n        \"dept\": [\n          \"Literature\"\n        ],\n        \"type\": [\n          \"Autonomous\"\n        ],\n        \"status\": \"ACTIVE\",\n        \"isMarked\": true\n      },\n      {\n        \"_id\": 10002,\n        \"university\": \"SPYU\",\n        \"Courses\": [\n          \"English\",\n          \"French\"\n        ],\n        \"dept\": [\n          \"Literature\"\n        ],\n        \"type\": [\n          \"Autonomous\"\n        ],\n        \"status\": \"NON-ACTIVE\",\n        \"isMarked\": true\n      }\n`\n```\nI wanted the response to be:\n```\n` \"university\": [\n  {\n    \"name\": \"Literature\",\n    \"values\": [\n      {\n        \"_id\": 10001,\n        \"university\": \"SPYU\",\n        \"Courses\": [\n          \"English\",\n          \"French\"\n        ],\n        \"dept\": [\n          \"Literature\"\n        ],\n        \"type\": [\n          \"Autonomous\"\n        ],\n        \"status\": \"ACTIVE\",\n        \"isMarked\": true\n      },\n      {\n        \"_id\": 10002,\n        \"university\": \"SPYU\",\n        \"Courses\": [\n          \"English\",\n          \"French\"\n        ],\n        \"dept\": [\n          \"Literature\"\n        ],\n        \"type\": [\n          \"Autonomous\"\n        ],\n        \"status\": \"NON-ACTIVE\",\n        \"isMarked\": true\n      }\n    ]\n  }\n],\n \"findDistinct\": [\n    {\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\n      \"name\": \"Courses\",\n      \"values\": [\n        \"English\",\n         \"French\"\n      ]\n    }\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b,\n    {\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\n      \"name\": \"Status\",\n      \"values\": [\n        \"ACTIVE\",\n        \"NON-ACTIVE\"\n      ]\n    }\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\n  ]\n\n`\n```\nI tried it using this link but the response is not coming as expected.\nhttps://mongoplayground.net/p/XECZvRMmt3T\nRight now, the response is coming like this\n```\n` \"universities\": [\n  {\n    \"name\": \"Literature\",\n    \"values\": [\n      {\n        \"_id\": 10001,\n        \"university\": \"SPYU\",\n        \"Courses\": [\n          \"English\",\n          \"French\"\n        ],\n        \"dept\": [\n          \"Literature\"\n        ],\n        \"type\": [\n          \"Autonomous\"\n        ],\n        \"status\": \"ACTIVE\",\n        \"isMarked\": true\n      },\n      {\n        \"_id\": 10002,\n        \"university\": \"SPYU\",\n        \"Courses\": [\n          \"English\",\n          \"French\"\n        ],\n        \"dept\": [\n          \"Literature\"\n        ],\n        \"type\": [\n          \"Autonomous\"\n        ],\n        \"status\": \"NON-ACTIVE\",\n        \"isMarked\": true\n      }\n    ]\n  }\n],\n\"findDistinct\": [\n    {\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\n      \"Courses\": [\n        \"English\",\n         \"French\"\n      ]\n    }\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b,\n    {\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\n      \"status\": [\n        \"ACTIVE\",\n        \"NON-ACTIVE\"\n      ]\n    }\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\n  ]\n`\n```\nAny Help will be appreciated!!",
      "solution": "Quick fixes in your query,\nuniversities:\n\n`$addFields`, remove $project and add only one operation for `isMarked`\n`$unwind` deconstruct `dept` array\n`$group` by `dept` and get values array of root\n\nfindDistinct:\n\n`$group` by null and get unique `courses` array and `status`\n`$reduce` to iterate loop of `Courses` nested array and get unique array using `$setUnion`\nMake array of course and `status` in `dest` field\n`$unwind` deconstruct `dest` array\n`$replaceRoot` replace `dest` object to root\n\n```\n`db.collection.aggregate([\n  { $match: { university: \"SPYU\" }\n  },\n  {\n    $facet: {\n      universities: [\n        { $addFields: { isMarked: { $in: [\"French\", \"$Courses\"] } } },\n        { $unwind: \"$dept\" },\n        {\n          $group: {\n            _id: \"$dept\",\n            values: { $push: \"$$ROOT\" }\n          }\n        }\n      ],\n      findDistinct: [\n        {\n          $group: {\n            _id: null,\n            Courses: { $addToSet: \"$Courses\" },\n            Status: { $addToSet: \"$status\" }\n          }\n        },\n        {\n          $project: {\n            _id: 0,\n            dist: [\n              {\n                name: \"Courses\",\n                values: {\n                  $reduce: {\n                    input: \"$Courses\",\n                    initialValue: [],\n                    in: { $setUnion: [\"$$this\", \"$$value\"] }\n                  }\n                }\n              },\n              {\n                name: \"Status\",\n                values: \"$Status\"\n              }\n            ]\n          }\n        },\n        { $unwind: \"$dist\" },\n        { $replaceRoot: { newRoot: \"$dist\" } }\n      ]\n    }\n  }\n])\n`\n```\nPlayground",
      "question_score": 3,
      "answer_score": 1,
      "created_at": "2021-02-12T08:40:24",
      "url": "https://stackoverflow.com/questions/66167965/how-to-find-distinct-values-of-fields-using-facet-operation-in-mongodb"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb-query",
      "question_id": 70654429,
      "title": "MongooseError: Query was already executed:",
      "problem": "I'm trying to update the document but the error says the query has already been executed.\nMongooseError: Query was already executed: footballs.updateOne({ date: 'January 4' }, {})\n\r\n\r\n`app.post('/api/bookslot', async (req, res) => {\n  console.log(req.body);\n  try {\n    const token = req.headers['x-access-token'];\n    const decoded = jwt.verify(token, 'secret123');\n    const email = decoded.email;\n    const user = await UserModel.findOne({ email: email });\n\n    let sportname = req.body.selectedSport.toLowerCase();\n    const time = req.body.slotTime;\n    const seats = req.body.availableSeats - 1;\n\n    if (!sportname.endsWith('s')) {\n      sportname = sportname.concat('s');\n    }\n    const NewSlotModel = mongoose.model(sportname, slotSchema);\n    var update = {};\n    update[time] = seats - 1;\n    console.log(update);\n    const a = await NewSlotModel.updateOne(\n      { date: req.body.slotDate },\n      { $set: update },\n      function (err, success) {\n        if (err) return handleError(err);\n      }\n    );\n\n    return res.json({ status: 'ok' });\n  } catch (e) {\n    console.log(e);\n    res.json({ status: 'error' });\n  }\n});`\r\n\r\n\r\n\nwhere am I going wrong?",
      "solution": "You are using both async/await and callbacks in your code, causing mongoose to throw an error.\nThe actual effect of using them both is exactly the error type that you are receiving:\n\nQuery was already executed\n\nMongoose v6 does not allow duplicate queries.\n\nMongoose no longer allows executing the same query object twice. If\nyou do, you'll get a Query was already executed error. Executing the\nsame query instance twice is typically indicative of mixing callbacks\nand promises, but if you need to execute the same query twice, you can\ncall Query#clone() to clone the query and re-execute it. See gh-7398\n\nDuplicate Query Execution\nTo fix the issue, just remove the third argument from the await\n\nNewSlotModel.updateOne\n\nMaking it:\n\n```\n`const a = await NewSlotModel.updateOne(\n  { date: req.body.slotDate },\n  { $set: update }\n);\n`\n```",
      "question_score": 2,
      "answer_score": 11,
      "created_at": "2022-01-10T16:00:43",
      "url": "https://stackoverflow.com/questions/70654429/mongooseerror-query-was-already-executed"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb-query",
      "question_id": 77932664,
      "title": "MongoDB: How to increment a vector of integers by a delta vector",
      "problem": "I have a very simple document structure like:\n`{\n  \"_id\": \"...\",\n  \"vector\": [100,50,4000],\n  //...\n}\n`\nI want to write an update statement, that changes the `vector` of a single given document id by a given delta (also a vector of integers). `[100,50,4000]` with delta `[-50,0,-2000]` would result in `[50,50,2000]`.\nI also want the statement to fail, if the resulting vector would have any negative values.\nHow can I do this?",
      "solution": "You can achieve this using an update with aggregation pipeline. In order to update only the document with the matching id, you apply a filter for the `_id` field.\nIn order to be able to discern between not found and not modified (due to negative values), you apply an aggregation pipeline:\n\nCalculate the new vector values and store it in the field `vector_updated`.\nCheck whether any of the values of `vector_updated` is less than 0. If yes, keep the `vector`field as it is; otherwise, assign the new values from `vector_updated.\nUnset the `vector_updated` field.\n\nThis way, the result of the update operation returns 1 for `matchedCount` if a document was matched and 1 for `modifiedCount` if the document was updated.\nThis is a sample for the aggregation pipeline:\n```\n`db.collection.update({\n  _id: 1\n},\n[\n  {\n    $set: {\n      vector_updated: {\n        $reduce: {\n          input: \"$vector\",\n          initialValue: [],\n          in: {\n            $concatArrays: [\n              \"$$value\",\n              [\n                {\n                  $add: [\n                    \"$$this\",\n                    {\n                      $arrayElemAt: [\n                        [\n                          -50,\n                          0,\n                          -2000\n                        ],\n                        {\n                          $size: \"$$value\"\n                        }\n                      ]\n                    }\n                  ]\n                }\n              ]\n            ]\n          }\n        }\n      }\n    }\n  },\n  {\n    $set: {\n      vector: {\n        $cond: {\n          if: {\n            $lt: [\n              {\n                $min: \"$vector_updated\"\n              },\n              0\n            ]\n          },\n          then: \"$vector\",\n          else: \"$vector_updated\"\n        }\n      }\n    }\n  },\n  {\n    $unset: \"vector_updated\"\n  }\n])\n`\n```\nSee this mongoplayground to test.",
      "question_score": 2,
      "answer_score": 3,
      "created_at": "2024-02-03T16:25:55",
      "url": "https://stackoverflow.com/questions/77932664/mongodb-how-to-increment-a-vector-of-integers-by-a-delta-vector"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb-query",
      "question_id": 76980190,
      "title": "How do I delete a document in mongoDB using mongoose in Node.js?",
      "problem": "I want to delete a document from a collection in mongoDB. This is my schema:\n```\n`const userSchema = new mongoose.Schema(\n  {\n    _id: {\n      type: String,\n      default: () => uuidv4().replace(/\\-/g, \"\"),\n    },\n    firstName: String,\n    lastName: String,\n    type: String,\n  },\n  {\n    timestamps: true, // timestamps = true will add 2 things to my schema: a createdAt and a updatedAt date value.\n    collection: \"users\",\n  }\n);\n`\n```\nThis is my code so far. How do i delete a row using schema above. This is my code so far\n```\n`userSchema.statics.deleteUserById = async function (id) {\n  try {\n    console.log(typeof(id));\n    const result = await this.remove({ _id: id});\n    \n    return result;\n  } catch (error) {\n    console.log(error)\n    throw error;\n  }\n}\n`\n```\nI'm calling the function through an API. and this is the error it throws\n```\n`TypeError: this.remove is not a function\n    at userSchema.statics.deleteUserById (file:///E:/secure-mm/server/models/User.js:53:31)\n    at onDeleteUserById (file:///E:/secure-mm/server/controllers/user.js:40:42)\n    at Layer.handle [as handle_request] (E:\\secure-mm\\node_modules\\express\\lib\\router\\layer.js:95:5)\n    at next (E:\\secure-mm\\node_modules\\express\\lib\\router\\route.js:144:13)\n    at Route.dispatch (E:\\secure-mm\\node_modules\\express\\lib\\router\\route.js:114:3)\n    at Layer.handle [as handle_request] (E:\\secure-mm\\node_modules\\express\\lib\\router\\layer.js:95:5)\n    at E:\\secure-mm\\node_modules\\express\\lib\\router\\index.js:284:15\n    at param (E:\\secure-mm\\node_modules\\express\\lib\\router\\index.js:365:14)\n    at param (E:\\secure-mm\\node_modules\\express\\lib\\router\\index.js:376:14)\n    at Function.process_params (E:\\secure-mm\\node_modules\\express\\lib\\router\\index.js:421:3)\n`\n```\nWhat should i be using instead to delete a user y their id?",
      "solution": "Mongoose has a straightforward method for deleting a document based on an id using your Model. It's the `Model.findByIdAndDelete()` as per the docs. In your case you can create a model from your schema like this:\n```\n`const User = mongoose.model('User', userSchema);\n`\n```\nthen delete like this:\n```\n`const result = await User.findByIdAndDelete(id);\n`\n```\nThere are other ways of deleting documents in Mongoose. You can research them here as you may find you need a different approach as your application grows.\nAs an aside, you don't need to define `_id` in your Schema.\n\nBy default, Mongoose adds an _id property to your schemas. When you create a new document with the automatically added _id property, Mongoose creates a new _id of type ObjectId to your document.",
      "question_score": 2,
      "answer_score": 5,
      "created_at": "2023-08-25T21:57:09",
      "url": "https://stackoverflow.com/questions/76980190/how-do-i-delete-a-document-in-mongodb-using-mongoose-in-node-js"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb-query",
      "question_id": 69546901,
      "title": "How to use multiple filters in a MongoDb find query using C#",
      "problem": "I have this query which works fine in datagrip\n```\n`test> db.getCollection(\"comments\").find({status: {$ne: \"APPROVED\"},storyID: {$regex: \"bbjfn-*\"}})\n`\n```\nI'm just wondering how to achieve the same thing in C# using the MongoDB Driver 2.13.1\n```\n`IMongoDatabase database = MongoClient.GetDatabase(Program.Settings.MongoDB.Database);\nIMongoCollection collection = database.GetCollection(\"comments\");\n\nvar filter = Builders.Filter.Eq(\"status\", new BsonDocument(\"$ne\", \"APPROVED\")) & \nBuilders.Filter.Eq(\"storyID\", new BsonDocument(\"$regex\", \"bbjfnfn-*\"));\n\nvar results = await collection.FindAsync(filter);\n`\n```\nDoesn't work.. what am I doing wrong?",
      "solution": "You can set `filter` with `BsonDocument` object as below:\n`FilterDefinition filter = new BsonDocument\n{\n    { \"status\", new BsonDocument(\"$ne\", \"APPROVED\") }, \n    { \"storyID\", new BsonDocument(\"$regex\", \"bbjfn-*\") }\n};\n`\nOR\n`var builder = Builders.Filter;\nvar filter = builder.Ne(\"status\", \"APPROVED\") & builder.Regex(\"storyID\", \"bbjfn-*\");\n`\nFYI, you can use MongoDB Compass to export Query to C# Language.",
      "question_score": 2,
      "answer_score": 5,
      "created_at": "2021-10-12T22:53:14",
      "url": "https://stackoverflow.com/questions/69546901/how-to-use-multiple-filters-in-a-mongodb-find-query-using-c"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb-query",
      "question_id": 67937042,
      "title": "How to map array of object and convert string value to number in mongodb?",
      "problem": "Here I have documents with details field as an array:\n```\n` {   \n        \"_id\" : ObjectId(\"60ae0b29ab282518443c7ac5\"),\n        \"details\": [\n              {\n                \"label\": \"Asset title\",\n                \"value\": \"S1\",\n              },\n              {\n                \"label\": \"Total Cost\",\n                \"value\": \"250\",\n              },\n              {\n                \"label\": \"Possession Status\",\n                \"value\": \"Available\",\n              },\n              {\n                \"label\": \"Estimated Monthly Rent\",\n                \"value\": \"15.5\",\n              }\n            ]\n    },\n    {   \n        \"_id\" : ObjectId(\"60ae0b29ab282518443c7ac8\"),\n        \"details\": [\n              {\n                \"label\": \"Asset title\",\n                \"value\": \"S2\",\n              },\n              {\n                \"label\": \"Total Cost\",\n                \"value\": \"455.5\",\n              },\n              {\n                \"label\": \"Possession Status\",\n                \"value\": \"Available\",\n              },\n              {\n                \"label\": \"Estimated Monthly Rent\",\n                \"value\": \"30\",\n              }\n            ]\n    }\n`\n```\nSo I am trying to `$project` this array of objects by Mapping the array of objects and check if the label is \"Total Cost\" or \"Estimated Monthly Rent\", then I want to convert their string values to number. For Ex : if `{label = \"Total Cost\"}` , then :  `{\"$toDouble\" : value}` and if `{label = \"Estimated Monthly Rent\"}` , then :  `{\"$toDouble\" : value}`\nI am trying this but getting error.\n```\n` db.collection.aggregate([\n       {\n             \"$project\": {\n                                   \n                    \"data\": {\n                         \"$map\": {\n                                \"input\": \"$details\", \n                                 \"as\": \"val\", \n                                 \"in\": {\n                                 \"$cond\":{ if: {\"$$val.label\": \"Total Cost\"} , then: { \"$toDouble\" : \"$$val.value\"}},\n                                 \"$cond\":{ if: {\"$$val.label\": \"Estimated Monthly Rent\"} , then: { \"$toDouble\" : \"$$val.value\"}}\n                                  }\n                         }\n                     } \n              }\n        }\n    ])\n`\n```",
      "solution": "check condition with `$in` operator and if match then return converted value otherwise return existing object\n```\n`db.collection.aggregate([\n  {\n    \"$project\": {\n      \"data\": {\n        \"$map\": {\n          \"input\": \"$details\",\n          \"as\": \"val\",\n          \"in\": {\n            $cond: [\n              { $in: [\"$$val.label\", [\"Total Cost\", \"Estimated Monthly Rent\"]] },\n              {\n                label: \"$$val.label\",\n                value: { $toDouble: \"$$val.value\" }\n              },\n              \"$$val\"\n            ]\n          }\n        }\n      }\n    }\n  }\n])\n`\n```\nPlayground",
      "question_score": 2,
      "answer_score": 3,
      "created_at": "2021-06-11T14:27:36",
      "url": "https://stackoverflow.com/questions/67937042/how-to-map-array-of-object-and-convert-string-value-to-number-in-mongodb"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb-query",
      "question_id": 79014505,
      "title": "Group embedded array of documents only",
      "problem": "in MongoDB I want to group an array of documents that is nested in another document without it affecting the parent document.\nDatabase:\n```\n`db={\n      \"users\": [\n        {\n          \"firstName\": \"David\",\n          \"lastName\": \"Mueller\",\n          \"messages\": [\n            {\n              \"text\": \"hello\",\n              \"type\": \"PERSONAL\"\n            },\n            {\n              \"text\": \"test\",\n              \"type\": \"DIRECT\"\n            }\n          ]\n        },\n        {\n          \"firstName\": \"Mia\",\n          \"lastName\": \"Davidson\",\n          \"messages\": [\n            {\n              \"text\": \"hello world\",\n              \"type\": \"DIRECT\"\n            },\n            {\n              \"text\": \":-)\",\n              \"type\": \"PERSONAL\"\n            },\n            {\n              \"text\": \"hi there\",\n              \"type\": \"DIRECT\"\n            }\n          ]\n        }\n      ]\n    }\n`\n```\nDesired result:\n```\n`[\n      {\n        \"firstName\": \"David\",\n        \"lastName\": \"Mueller\",\n        \"messages\": [\n          {\n            \"_id\": \"PERSONAL\",\n            \"count\": 1\n          },\n          {\n            \"_id\": \"DIRECT\",\n            \"count\": 1\n          }\n        ]\n      },\n      {\n        \"firstName\": \"Mia\",\n        \"lastName\": \"Davidson\",\n        \"messages\": [\n          {\n            \"_id\": \"PERSONAL\",\n            \"count\": 1\n          },\n          {\n            \"_id\": \"DIRECT\",\n            \"count\": 2\n          }\n        ]\n      }\n]\n`\n```\nIf I have an array of ids I already know how to do it using the internal pipeline of $lookup, but my question is how can I do that with an array of embedded documents.\n\nThis is an example of a working grouping on an array with ids using lookup. This is not the solution because the question is about an embedded document array and not an array of ids. This example is only provided to show that I can archive the desired result when ids instead of embedded documents are stored in an array.\nDatabase for grouping with lookup:\n```\n`db={\n  \"users\": [\n    {\n      \"firstName\": \"David\",\n      \"lastName\": \"Mueller\",\n      \"messages\": [\n        1,\n        2\n      ]\n    },\n    {\n      \"firstName\": \"Mia\",\n      \"lastName\": \"Davidson\",\n      \"messages\": [\n        3,\n        4,\n        5\n      ]\n    }\n  ],\n  \"messages\": [\n    {\n      \"_id\": 1,\n      \"text\": \"hello\",\n      \"type\": \"PERSONAL\"\n    },\n    {\n      \"_id\": 2,\n      \"text\": \"test\",\n      \"type\": \"DIRECT\"\n    },\n    {\n      \"_id\": 3,\n      \"text\": \"hello world\",\n      \"type\": \"DIRECT\"\n    },\n    {\n      \"_id\": 4,\n      \"text\": \":-)\",\n      \"type\": \"PERSONAL\"\n    },\n    {\n      \"_id\": 5,\n      \"text\": \"hi there\",\n      \"type\": \"DIRECT\"\n    }\n  ]\n}\n`\n```\nAggregation of grouping with lookup:\n```\n`db.users.aggregate([\n  {\n    \"$lookup\": {\n      \"from\": \"messages\",\n      \"localField\": \"messages\",\n      \"foreignField\": \"_id\",\n      \"as\": \"messages\",\n      \"pipeline\": [\n        {\n          \"$group\": {\n            \"_id\": \"$type\",\n            \"count\": {\n              \"$sum\": 1\n            }\n          }\n        }\n      ]\n    }\n  }\n])\n`\n```\nResult of grouping with lookup (which is the desired result):\n```\n`[\n  {\n    \"_id\": ObjectId(\"5a934e000102030405000005\"),\n    \"firstName\": \"David\",\n    \"lastName\": \"Mueller\",\n    \"messages\": [\n      {\n        \"_id\": \"PERSONAL\",\n        \"count\": 1\n      },\n      {\n        \"_id\": \"DIRECT\",\n        \"count\": 1\n      }\n    ]\n  },\n  {\n    \"_id\": ObjectId(\"5a934e000102030405000006\"),\n    \"firstName\": \"Mia\",\n    \"lastName\": \"Davidson\",\n    \"messages\": [\n      {\n        \"_id\": \"PERSONAL\",\n        \"count\": 1\n      },\n      {\n        \"_id\": \"DIRECT\",\n        \"count\": 2\n      }\n    ]\n  }\n]\n`\n```\nThis example in the MongoDB playground\n\nNow back to the issue: I want to archive the same result but with an embedded document array as provided at the top.\nI cannot find out how to do this (I tried AI, lot's of google searches and other forums without success, you are my last resource before giving up), I know I can filter an embedded array using $addField and $fitler but not how I can group just the embedded array.\nPlease note that this is just a simple example my real data structure looks different and might also use other grouping functions like min, sum etc. but I just wanted to know a general way of archieving the same thing as when I use the lookup.\nI appreciate any help with this and thank you \ud83d\ude42",
      "solution": "Unwind `messages`\n\nUse `preserveNullAndEmptyArrays: true` in case the field is missing or empty.\n\nGroup by the `_id` (presumably userID) and message `type`; and set countType with `$count`.\nThen re-group by `_id` only and use the first `doc` (since it's the same for non-message fields)\n\nPush each `{type: ..., count: countType}` into a messages array.\nNote that the documents are back to being one per user/`_id`\n\nSet the `doc.messages` to the array `messages` which was pushed in the previous step.\n\nCheck for the first message not having a `type` - this occurs when the original document had missing or empty `messages`. If it is, then set it to the empty array, otherwise, use as-is.\nUse any default you want, like `[{ _id: null, count: 0 }]`\n(I've added an example doc in the playground which has one message but since it has a type it doesn't meet this criteria and is set correctly.)\n\nReplace the root with the new `doc` which has all the correct info\n(Optionally, sort on `_id` if you need it.)\n\n`db.users.aggregate([\n  {\n    $unwind: {\n      path: \"$messages\",\n      preserveNullAndEmptyArrays: true\n    }\n  },\n  {\n    $group: {\n      _id: {\n        _id: \"$_id\",\n        type: \"$messages.type\"\n      },\n      countType: { $count: {} },\n      doc: { $first: \"$$ROOT\" }\n    }\n  },\n  {\n    $group: {\n      _id: \"$_id._id\",\n      doc: { $first: \"$doc\" },\n      messages: {\n        $push: {\n          _id: \"$_id.type\",\n          count: \"$countType\"\n        }\n      }\n    }\n  },\n  {\n    $set: {\n      \"doc.messages\": {\n        // set it to some default missing message type\n        $cond: {\n          // `messages._id` will be missing\n          if: {\n            $eq: [\n              { $type: { $first: \"$messages._id\" } },\n              \"missing\"\n            ]\n          },\n          // put whichever \"default\" you want\n          // like `[{ _id: null, count: 0 }]`\n          then: [],\n          else: \"$messages\"\n        }\n      }\n    }\n  },\n  { $replaceWith: \"$doc\" }\n])\n`\nMongo Playground\nMongo Playground with `minNumber`\nPrevious demo playground if `messages` is never missing and never empty.",
      "question_score": 2,
      "answer_score": 2,
      "created_at": "2024-09-23T14:01:03",
      "url": "https://stackoverflow.com/questions/79014505/group-embedded-array-of-documents-only"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb-query",
      "question_id": 78237801,
      "title": "Understanding $not in combination with $elemMatch in MongoDB",
      "problem": "I have this document structure:\n```\n`{\n      \"name\": \"ab\",\n      \"grades\": [\n        {\n          \"grade\": \"A\",\n          \"score\": 1\n        },\n        {\n          \"grade\": \"A\",\n          \"score\": 12\n        },\n        {\n          \"grade\": \"A\",\n          \"score\": 7\n        }\n      ],\n      \"borough\": \"Manhattan2\"\n    }\n`\n```\nAssignment is to write a query to find the restaurants that have all grades with a score greater than 5.\nThe solution to the problem is following:\n```\n`db.restaurants.find({\n      \"grades\": {\n        \"$not\": {\n          \"$elemMatch\": {\n            \"score\": {\n              \"$lte\": 5\n            }\n          }\n        }\n      }\n    })\n`\n```\nI have troubles understanding proposed solution.\nSo far I have only used `$elemMatch` to match at least one element of array or elements in array's inner objects (`grades.score`), but how the heck `$not` is \"somehow making\" `$elemMatch` to check for all `grades.score` in this object?\nI do understand general idea, \"don't look at score less the equal to 5, and what remains is what we need\", but I cannot comprehend what does this code snippet returns:\n```\n`\"$not\": {\n      \"$elemMatch\": {\n        \"score\": {\n          \"$lte\": 5\n        }\n      }\n    }\n`\n```\nIf was asked what does this query do before running & testing it, I would say that it find first `score` that is greater then 5 and takes that document, but that is wrong, and I cannot figure why. I see that order of fields and keywords plays some role but don't see the connection.",
      "solution": "In order to understand $elemMatch and $not, it would help if we start with $all.\nAs you we all know, $not is equal to say \u201cNo\u201d, the opposite to \u201cYes\u201d. We shall come to \u201cNo\u201d\nlater. let us first start discussing $all, $elemMatch under the context of \u201cYes\u201d.\nLet us have a sample collection of just one document as below.\nIn Mongo shell:\n```\n`let t = db.test;\nt.find();\n[ a: [ 1, 2 ] } ]\n`\n```\nFirst of $all :\nQuery 1 : found\n```\n`t.find({a: { $all:[1] }});\n[ { a: [ 1, 2 ] } ]\n`\n```\nQuery 2: found\n```\n`t.find({a: { $all:[1,2] }});\n[ {  a: [ 1, 2 ] } ]\n`\n```\nQuery 3: Not found\n```\n`t.find({a: { $all:[1,2,3] }});\n\n`\n```\nWhat does it mean by $all ?\nTh above three queries can be described  in one line as \u201cfind all documents with the array key \u201ca\u201d has all elements in the given array\u201d.\n\nQuery 1 : a has all elements in [1] - found\nQuery 2 : a has all elements in [1,2] - found\nQuery 2 : a does not have all elements in [1,2,3] - not found\n\nNow let us see $elemeMatch\nQuery 4 : found\n```\n`t.find({a : { $elemMatch: {$eq: 1}}});\n[ { a: [ 1, 2 ] } ]\n`\n```\nQuery 5 : found\n```\n`t.find({a : { $elemMatch: {$eq: 2}}});\n[ { a: [ 1, 2 ] } ]\n`\n```\nQuery 6 : Not found\n```\n`t.find({a : { $elemMatch: {$eq: 3}}});\n\n`\n```\nWhat does it mean by $elemMatch ?\nThe above three queries can be described  in one line as \u201cfind all documents with the array key \u201ca\u201d has at least one element matching the given condition\u201d.\n\nQuery 4 : a has one element matching the condition equal to 1 - found\nQuery 5 : a has one element matching the condition equal to 2 - found\nQuery 6 : a has no element matching the condition equal to 3 - not found\n\nThe take-away:\n\nAlthough the two set of queries with $all and $elemMatch yielded the same results, the interpretation of the queries are different - as it is described above.\nNow all these queries are in the context of \u201cYes\u201d.\nIf we are able to understanding the basics of these six queries, we can easily guess what will be the outcome of the same 6 queries in \u201cNo\u201d context. It would be just the opposite results.\n\nPlease see the test results.\n```\n`t.find({a: {$not: { $all:[1] }}}); // no data\nt.find({a: {$not: { $all:[1,2] }}}); // no data\nt.find({a: {$not: { $all:[1,2,3] }}}); // data found\n        1. [ { a: [ 1, 2 ] } ]\nt.find({a : {$not : { $elemMatch: {$eq: 1}}}}); // no data\nt.find({a : {$not : { $elemMatch: {$eq: 2}}}}); // no data\nt.find({a : {$not : { $elemMatch: {$eq: 3}}}}); / data found\n        1. [ { a: [ 1, 2 ] } ]\n`\n```\nNotes:\n$elemMatch can take more than one conditionals, for brevity, the examples in this post include just one condition. When there will be more than one condition, it would be evaluated with AND logical operation which is essentially the same as with just one condition.",
      "question_score": 2,
      "answer_score": 2,
      "created_at": "2024-03-28T12:22:39",
      "url": "https://stackoverflow.com/questions/78237801/understanding-not-in-combination-with-elemmatch-in-mongodb"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb-query",
      "question_id": 76341966,
      "title": "Why is a simple query taking more than 2 seconds with Golang Mongo driver?",
      "problem": "I am coding a golang web service that has a mongo database, I am using the go.mongodb.org/mongo-driver v1.11.6 and a simple query is taking more than 2 seconds to complete. The database has only a few records, is it just for testing, no more than 10 records.\nI've been looking for the code part where the time is being wasted and I found that the problem is with the mongo package. The methods Find, FindOne and even Insert and Update are taking more than 1 or 2 seconds to complete.\nThis is the mongo client instantiation\n```\n`func NewMongoDB() *MongoDB {\n    uri := config.GetEnvConfig().MongoURI\n    database := config.GetEnvConfig().MongoDatabase\n\n    client, err := mongo.Connect(context.TODO(), options.Client().ApplyURI(uri))\n\n    if err != nil {\n    panic(err)\n    }\n\n    return &MongoDB{\n    client:   client,\n    database: database,\n    }\n}\n`\n```\nThis is the function code:\n```\n`func getMessageByIdFromDB(id string) (*Message, error) {\n    conn := database.NewMongoDB()\n    defer conn.Disconnect()\n\n    filter := map[string]string{\n    \"message_id\": id,\n    }\n\n    var message Message\n\n    start := time.Now()\n\n    err := conn.GetCollection(collectionName).FindOne(context.TODO(), filter).Decode(&message)\n\n    elapsed := time.Since(start)\n    log.Printf(\"Querying messages took %s\", elapsed)\n\n    if err != nil {\n    return nil, err\n    }\n\n    return &message, nil\n}\n`\n```\nThis is the result of the time tracking of the function:\n```\n`Querying messages took 2.320409472s\n`\n```\nIt is not about internet connection because I made the query test with python and the request took only 0.003 seconds\nI tried to change the version of the package without achieving it. Also I tried to reinstall all the packages of the project, with the same result.\nI also tried to create search indexes in database with no different results. The query also take more than 2 seconds to complete.\nI think the query shouldn't take more than a few milliseconds to complete.",
      "solution": "`mongo.Connect()` \"only\" initializes the `Client` by starting background monitoring goroutines. It may not necessary connect to the (remote) database.\nWhen you then perform queries, they certainly require to build up connections, and that may take seconds.\nYou may use the `Client.Ping()` method prior to force connecting to the database and verify that the connection was created successfully, so when you perform queries after that, a connection will be ready.\nYou may also try to repeat the same query, as after the first query the connection will not be closed but put into a connection pool and reused when needed again (for the second query).",
      "question_score": 2,
      "answer_score": 4,
      "created_at": "2023-05-26T16:56:17",
      "url": "https://stackoverflow.com/questions/76341966/why-is-a-simple-query-taking-more-than-2-seconds-with-golang-mongo-driver"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb-query",
      "question_id": 75665621,
      "title": "StackOverflow on querying embedded document",
      "problem": "Let me preface this by saying that I'm learning MongoDB still, so this could be a rather easy fix that my SQL brain cannot see.\nFor learning and researching purposes, I'm migrating an internal service for (Mexican) ZipCodes. This service uses MySQL as the database and has one field of the source data provided by the government as the main table, while many other fields were turned into secondary tables for repetitive data, such as `state`.\nThis approach is not necessarily the best for a NoSQL database (AFAIK since it relies heavily on `JOIN`s), so I rethought it and the zip code itself is the main attribute of my document, while other data is embedded in an array. My schema looks like this:\n`'zipcode': {\n    'code': String\n    'state': String\n    'settlements': [\n        'settlement': String\n        'municipality': String\n        'city': String | null\n    ]\n}\n`\nAll the data was imported after the schema was (strictly) defined, and everything but `city` is required.\nI mapped this into a SpringBoot project like\n`public class ZipCode {\n\n    @Id\n    private String id;\n    private String code;\n    private String state;\n    private List settlements;\n\n}\npublic class Settlement {\n\n    @Field(\"settlement\")\n    private String name;\n    private String municipality;\n    private String city;\n\n}\n`\nand created a repository to search by zip code or municipality, which are the most important identifiers.\n`public interface ZipCodesRepository extends MongoRepository {\n\n    @Query(\"{code: ?0}\")\n    ZipCode getByCode(String zc);\n\n    @Query(\"{settlements.municipality: ?0}\")\n    List getByMunicipality(String municipality);\n\n}\n`\nI'm still just testing things out, so these two queries are enough. And yes, I've tried both queries directly through Compass.\nThe problem comes while trying the second query. It throws a `StackOverflowException` and while I can usually get what's wrong by reading it, this time is not a single trace. What I mean is that even if I get `StackOverflowException` every time I hit `getByMunicipality` the trace changes, although It seems to be a limited number of them.\nAt this point, I just don't understand what am I supposed to look for. The traces are the following (asterisks for emphasis by me):\nTrace 1\n```\n`java.lang.StackOverflowError: null\n        at java.base/java.util.Locale.isUnicodeExtensionKey(Locale.java:2283) ~[na:na]\n        at java.base/java.util.Locale.getUnicodeLocaleType(Locale.java:1354) ~[na:na]\n        at java.base/sun.util.locale.provider.CalendarDataUtility.findRegionOverride(CalendarDataUtility.java:136) ~[na:na]\n        at java.base/java.text.DecimalFormatSymbols.initialize(DecimalFormatSymbols.java:799) ~[na:na]\n        at java.base/java.text.DecimalFormatSymbols.(DecimalFormatSymbols.java:115) ~[na:na]\n        at java.base/sun.util.locale.provider.DecimalFormatSymbolsProviderImpl.getInstance(DecimalFormatSymbolsProviderImpl.java:85) ~[na:na]\n        at java.base/java.text.DecimalFormatSymbols.getInstance(DecimalFormatSymbols.java:182) ~[na:na]\n        at java.base/java.util.Formatter.zero(Formatter.java:2450) ~[na:na]\n        at java.base/java.util.Formatter$FormatSpecifier.getZero(Formatter.java:4450) ~[na:na]\n        at java.base/java.util.Formatter$FormatSpecifier.localizedMagnitude(Formatter.java:4466) ~[na:na]\n        at java.base/java.util.Formatter$FormatSpecifier.print(Formatter.java:3276) ~[na:na]\n        at java.base/java.util.Formatter$FormatSpecifier.print(Formatter.java:3261) ~[na:na]\n        at java.base/java.util.Formatter$FormatSpecifier.printInteger(Formatter.java:2957) ~[na:na]\n        at java.base/java.util.Formatter$FormatSpecifier.print(Formatter.java:2918) ~[na:na]\n        at java.base/java.util.Formatter.format(Formatter.java:2689) ~[na:na]\n        at java.base/java.util.Formatter.format(Formatter.java:2625) ~[na:na]\n        at java.base/java.lang.String.format(String.java:4143) ~[na:na]\n        at org.bson.json.JsonParseException.(JsonParseException.java:56) ~[bson-4.6.1.jar:na]\n        **at org.springframework.data.mongodb.util.json.JsonScanner.nextToken(JsonScanner.java:118) ~[spring-data-mongodb-3.4.8.jar:3.4.8]**\n        at org.springframework.data.mongodb.util.json.ParameterBindingJsonReader.popToken(ParameterBindingJsonReader.java:755) ~[spring-data-mongodb-3.4.8.jar:3.4.8]\n        at org.springframework.data.mongodb.util.json.ParameterBindingJsonReader.readBsonType(ParameterBindingJsonReader.java:169) ~[spring-data-mongodb-3.4.8.jar:3.4.8]\n        at org.bson.AbstractBsonReader.verifyBSONType(AbstractBsonReader.java:679) ~[bson-4.6.1.jar:na]\n        at org.bson.AbstractBsonReader.checkPreconditions(AbstractBsonReader.java:721) ~[bson-4.6.1.jar:na]\n        at org.bson.AbstractBsonReader.readStartDocument(AbstractBsonReader.java:449) ~[bson-4.6.1.jar:na]\n        at org.springframework.data.mongodb.util.json.ParameterBindingDocumentCodec.decode(ParameterBindingDocumentCodec.java:235) ~[spring-data-mongodb-3.4.8.jar:3.4.8]\n        at org.springframework.data.mongodb.util.json.ParameterBindingDocumentCodec.decode(ParameterBindingDocumentCodec.java:67) ~[spring-data-mongodb-3.4.8.jar:3.4.8]\n        at org.springframework.data.mongodb.util.json.ParameterBindingDocumentCodec.readValue(ParameterBindingDocumentCodec.java:371) ~[spring-data-mongodb-3.4.8.jar:3.4.8]\n`\n```\nTrace 2\n```\n`java.lang.StackOverflowError: null\n        at java.base/java.lang.CharacterData00.getType(CharacterData00.java:84) ~[na:na]\n        at java.base/java.lang.Character.getType(Character.java:11031) ~[na:na]\n        at java.base/java.text.DecimalFormatSymbols.lambda$findNonFormatChar$0(DecimalFormatSymbols.java:843) ~[na:na]\n        at java.base/java.util.stream.IntPipeline$10$1.accept(IntPipeline.java:392) ~[na:na]\n        at java.base/java.lang.StringUTF16$CharsSpliterator.tryAdvance(StringUTF16.java:1231) ~[na:na]\n        at java.base/java.util.stream.IntPipeline.forEachWithCancel(IntPipeline.java:163) ~[na:na]\n        at java.base/java.util.stream.AbstractPipeline.copyIntoWithCancel(AbstractPipeline.java:527) ~[na:na]\n        at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:513) ~[na:na]\n        at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499) ~[na:na]\n        at java.base/java.util.stream.FindOps$FindOp.evaluateSequential(FindOps.java:150) ~[na:na]\n        at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ~[na:na]\n        at java.base/java.util.stream.IntPipeline.findFirst(IntPipeline.java:552) ~[na:na]\n        at java.base/java.text.DecimalFormatSymbols.findNonFormatChar(DecimalFormatSymbols.java:844) ~[na:na]\n        at java.base/java.text.DecimalFormatSymbols.initialize(DecimalFormatSymbols.java:823) ~[na:na]\n        at java.base/java.text.DecimalFormatSymbols.(DecimalFormatSymbols.java:115) ~[na:na]\n        at java.base/sun.util.locale.provider.DecimalFormatSymbolsProviderImpl.getInstance(DecimalFormatSymbolsProviderImpl.java:85) ~[na:na]\n        at java.base/java.text.DecimalFormatSymbols.getInstance(DecimalFormatSymbols.java:182) ~[na:na]\n        at java.base/java.util.Formatter.zero(Formatter.java:2450) ~[na:na]\n        at java.base/java.util.Formatter$FormatSpecifier.getZero(Formatter.java:4450) ~[na:na]\n        at java.base/java.util.Formatter$FormatSpecifier.localizedMagnitude(Formatter.java:4466) ~[na:na]\n        at java.base/java.util.Formatter$FormatSpecifier.print(Formatter.java:3276) ~[na:na]\n        at java.base/java.util.Formatter$FormatSpecifier.print(Formatter.java:3261) ~[na:na]\n        at java.base/java.util.Formatter$FormatSpecifier.printInteger(Formatter.java:2957) ~[na:na]\n        at java.base/java.util.Formatter$FormatSpecifier.print(Formatter.java:2918) ~[na:na]\n        at java.base/java.util.Formatter.format(Formatter.java:2689) ~[na:na]\n        at java.base/java.util.Formatter.format(Formatter.java:2625) ~[na:na]\n        at java.base/java.lang.String.format(String.java:4143) ~[na:na]\n        at org.bson.json.JsonParseException.(JsonParseException.java:56) ~[bson-4.6.1.jar:na]\n        **at org.springframework.data.mongodb.util.json.JsonScanner.nextToken(JsonScanner.java:118) ~[spring-data-mongodb-3.4.8.jar:3.4.8]**\n        at org.springframework.data.mongodb.util.json.ParameterBindingJsonReader.popToken(ParameterBindingJsonReader.java:755) ~[spring-data-mongodb-3.4.8.jar:3.4.8]\n        at org.springframework.data.mongodb.util.json.ParameterBindingJsonReader.readBsonType(ParameterBindingJsonReader.java:169) ~[spring-data-mongodb-3.4.8.jar:3.4.8]\n        at org.bson.AbstractBsonReader.verifyBSONType(AbstractBsonReader.java:679) ~[bson-4.6.1.jar:na]\n        at org.bson.AbstractBsonReader.checkPreconditions(AbstractBsonReader.java:721) ~[bson-4.6.1.jar:na]\n        at org.bson.AbstractBsonReader.readStartDocument(AbstractBsonReader.java:449) ~[bson-4.6.1.jar:na]\n        at org.springframework.data.mongodb.util.json.ParameterBindingDocumentCodec.decode(ParameterBindingDocumentCodec.java:235) ~[spring-data-mongodb-3.4.8.jar:3.4.8]\n        at org.springframework.data.mongodb.util.json.ParameterBindingDocumentCodec.decode(ParameterBindingDocumentCodec.java:67) ~[spring-data-mongodb-3.4.8.jar:3.4.8]\n        at org.springframework.data.mongodb.util.json.ParameterBindingDocumentCodec.readValue(ParameterBindingDocumentCodec.java:371) ~[spring-data-mongodb-3.4.8.jar:3.4.8]\n        at org.springframework.data.mongodb.util.json.ParameterBindingDocumentCodec.decode(ParameterBindingDocumentCodec.java:248) ~[spring-data-mongodb-3.4.8.jar:3.4.8]\n        at org.springframework.data.mongodb.util.json.ParameterBindingDocumentCodec.decode(ParameterBindingDocumentCodec.java:67) ~[spring-data-mongodb-3.4.8.jar:3.4.8]\n`\n```\nTrace 3\n```\n`java.lang.StackOverflowError: null\n        at java.base/sun.util.locale.provider.CalendarDataUtility.findRegionOverride(CalendarDataUtility.java:136) ~[na:na]\n        at java.base/java.text.DecimalFormatSymbols.initialize(DecimalFormatSymbols.java:799) ~[na:na]\n        at java.base/java.text.DecimalFormatSymbols.(DecimalFormatSymbols.java:115) ~[na:na]\n        at java.base/sun.util.locale.provider.DecimalFormatSymbolsProviderImpl.getInstance(DecimalFormatSymbolsProviderImpl.java:85) ~[na:na]\n        at java.base/java.text.DecimalFormatSymbols.getInstance(DecimalFormatSymbols.java:182) ~[na:na]\n        at java.base/java.util.Formatter.zero(Formatter.java:2450) ~[na:na]\n        at java.base/java.util.Formatter$FormatSpecifier.getZero(Formatter.java:4450) ~[na:na]\n        at java.base/java.util.Formatter$FormatSpecifier.localizedMagnitude(Formatter.java:4466) ~[na:na]\n        at java.base/java.util.Formatter$FormatSpecifier.print(Formatter.java:3276) ~[na:na]\n        at java.base/java.util.Formatter$FormatSpecifier.print(Formatter.java:3261) ~[na:na]\n        at java.base/java.util.Formatter$FormatSpecifier.printInteger(Formatter.java:2957) ~[na:na]\n        at java.base/java.util.Formatter$FormatSpecifier.print(Formatter.java:2918) ~[na:na]\n        at java.base/java.util.Formatter.format(Formatter.java:2689) ~[na:na]\n        at java.base/java.util.Formatter.format(Formatter.java:2625) ~[na:na]\n        at java.base/java.lang.String.format(String.java:4143) ~[na:na]\n        at org.bson.json.JsonParseException.(JsonParseException.java:56) ~[bson-4.6.1.jar:na]\n        **at org.springframework.data.mongodb.util.json.JsonScanner.nextToken(JsonScanner.java:118) ~[spring-data-mongodb-3.4.8.jar:3.4.8]**\n        at org.springframework.data.mongodb.util.json.ParameterBindingJsonReader.popToken(ParameterBindingJsonReader.java:755) ~[spring-data-mongodb-3.4.8.jar:3.4.8]\n        at org.springframework.data.mongodb.util.json.ParameterBindingJsonReader.readBsonType(ParameterBindingJsonReader.java:169) ~[spring-data-mongodb-3.4.8.jar:3.4.8]\n        at org.bson.AbstractBsonReader.verifyBSONType(AbstractBsonReader.java:679) ~[bson-4.6.1.jar:na]\n        at org.bson.AbstractBsonReader.checkPreconditions(AbstractBsonReader.java:721) ~[bson-4.6.1.jar:na]\n        at org.bson.AbstractBsonReader.readStartDocument(AbstractBsonReader.java:449) ~[bson-4.6.1.jar:na]\n        at org.springframework.data.mongodb.util.json.ParameterBindingDocumentCodec.decode(ParameterBindingDocumentCodec.java:235) ~[spring-data-mongodb-3.4.8.jar:3.4.8]\n        at org.springframework.data.mongodb.util.json.ParameterBindingDocumentCodec.decode(ParameterBindingDocumentCodec.java:67) ~[spring-data-mongodb-3.4.8.jar:3.4.8]\n        at org.springframework.data.mongodb.util.json.ParameterBindingDocumentCodec.readValue(ParameterBindingDocumentCodec.java:371) ~[spring-data-mongodb-3.4.8.jar:3.4.8]\n        at org.springframework.data.mongodb.util.json.ParameterBindingDocumentCodec.decode(ParameterBindingDocumentCodec.java:248) ~[spring-data-mongodb-3.4.8.jar:3.4.8]\n        at org.springframework.data.mongodb.util.json.ParameterBindingDocumentCodec.decode(ParameterBindingDocumentCodec.java:67) ~[spring-data-mongodb-3.4.8.jar:3.4.8]\n`\n```\nTrace 4\n```\n`java.lang.StackOverflowError: null\n        at java.base/java.util.stream.AbstractPipeline.wrapSink(AbstractPipeline.java:547) ~[na:na]\n        at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499) ~[na:na]\n        at java.base/java.util.stream.FindOps$FindOp.evaluateSequential(FindOps.java:150) ~[na:na]\n        at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ~[na:na]\n        at java.base/java.util.stream.IntPipeline.findFirst(IntPipeline.java:552) ~[na:na]\n        at java.base/java.text.DecimalFormatSymbols.findNonFormatChar(DecimalFormatSymbols.java:844) ~[na:na]\n        at java.base/java.text.DecimalFormatSymbols.initialize(DecimalFormatSymbols.java:815) ~[na:na]\n        at java.base/java.text.DecimalFormatSymbols.(DecimalFormatSymbols.java:115) ~[na:na]\n        at java.base/sun.util.locale.provider.DecimalFormatSymbolsProviderImpl.getInstance(DecimalFormatSymbolsProviderImpl.java:85) ~[na:na]\n        at java.base/java.text.DecimalFormatSymbols.getInstance(DecimalFormatSymbols.java:182) ~[na:na]\n        at java.base/java.util.Formatter.zero(Formatter.java:2450) ~[na:na]\n        at java.base/java.util.Formatter$FormatSpecifier.getZero(Formatter.java:4450) ~[na:na]\n        at java.base/java.util.Formatter$FormatSpecifier.localizedMagnitude(Formatter.java:4466) ~[na:na]\n        at java.base/java.util.Formatter$FormatSpecifier.print(Formatter.java:3276) ~[na:na]\n        at java.base/java.util.Formatter$FormatSpecifier.print(Formatter.java:3261) ~[na:na]\n        at java.base/java.util.Formatter$FormatSpecifier.printInteger(Formatter.java:2957) ~[na:na]\n        at java.base/java.util.Formatter$FormatSpecifier.print(Formatter.java:2918) ~[na:na]\n        at java.base/java.util.Formatter.format(Formatter.java:2689) ~[na:na]\n        at java.base/java.util.Formatter.format(Formatter.java:2625) ~[na:na]\n        at java.base/java.lang.String.format(String.java:4143) ~[na:na]\n        at org.bson.json.JsonParseException.(JsonParseException.java:56) ~[bson-4.6.1.jar:na]\n        **at org.springframework.data.mongodb.util.json.JsonScanner.nextToken(JsonScanner.java:118) ~[spring-data-mongodb-3.4.8.jar:3.4.8]**\n        at org.springframework.data.mongodb.util.json.ParameterBindingJsonReader.popToken(ParameterBindingJsonReader.java:755) ~[spring-data-mongodb-3.4.8.jar:3.4.8]\n        at org.springframework.data.mongodb.util.json.ParameterBindingJsonReader.readBsonType(ParameterBindingJsonReader.java:169) ~[spring-data-mongodb-3.4.8.jar:3.4.8]\n        at org.bson.AbstractBsonReader.verifyBSONType(AbstractBsonReader.java:679) ~[bson-4.6.1.jar:na]\n        at org.bson.AbstractBsonReader.checkPreconditions(AbstractBsonReader.java:721) ~[bson-4.6.1.jar:na]\n        at org.bson.AbstractBsonReader.readStartDocument(AbstractBsonReader.java:449) ~[bson-4.6.1.jar:na]\n        at org.springframework.data.mongodb.util.json.ParameterBindingDocumentCodec.decode(ParameterBindingDocumentCodec.java:235) ~[spring-data-mongodb-3.4.8.jar:3.4.8]\n        at org.springframework.data.mongodb.util.json.ParameterBindingDocumentCodec.decode(ParameterBindingDocumentCodec.java:67) ~[spring-data-mongodb-3.4.8.jar:3.4.8]\n        at org.springframework.data.mongodb.util.json.ParameterBindingDocumentCodec.readValue(ParameterBindingDocumentCodec.java:371) ~[spring-data-mongodb-3.4.8.jar:3.4.8]\n        at org.springframework.data.mongodb.util.json.ParameterBindingDocumentCodec.decode(ParameterBindingDocumentCodec.java:248) ~[spring-data-mongodb-3.4.8.jar:3.4.8]\n        at org.springframework.data.mongodb.util.json.ParameterBindingDocumentCodec.decode(ParameterBindingDocumentCodec.java:67) ~[spring-data-mongodb-3.4.8.jar:3.4.8]\n`\n```\nTrace 5\n```\n`java.lang.StackOverflowError: null\n        at java.base/java.util.Spliterator.getExactSizeIfKnown(Spliterator.java:414) ~[na:na]\n        at java.base/java.util.stream.AbstractPipeline.copyIntoWithCancel(AbstractPipeline.java:526) ~[na:na]\n        at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:513) ~[na:na]\n        at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499) ~[na:na]\n        at java.base/java.util.stream.FindOps$FindOp.evaluateSequential(FindOps.java:150) ~[na:na]\n        at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ~[na:na]\n        at java.base/java.util.stream.IntPipeline.findFirst(IntPipeline.java:552) ~[na:na]\n        at java.base/java.text.DecimalFormatSymbols.findNonFormatChar(DecimalFormatSymbols.java:844) ~[na:na]\n        at java.base/java.text.DecimalFormatSymbols.initialize(DecimalFormatSymbols.java:815) ~[na:na]\n        at java.base/java.text.DecimalFormatSymbols.(DecimalFormatSymbols.java:115) ~[na:na]\n        at java.base/sun.util.locale.provider.DecimalFormatSymbolsProviderImpl.getInstance(DecimalFormatSymbolsProviderImpl.java:85) ~[na:na]\n        at java.base/java.text.DecimalFormatSymbols.getInstance(DecimalFormatSymbols.java:182) ~[na:na]\n        at java.base/java.util.Formatter.zero(Formatter.java:2450) ~[na:na]\n        at java.base/java.util.Formatter$FormatSpecifier.getZero(Formatter.java:4450) ~[na:na]\n        at java.base/java.util.Formatter$FormatSpecifier.localizedMagnitude(Formatter.java:4466) ~[na:na]\n        at java.base/java.util.Formatter$FormatSpecifier.print(Formatter.java:3276) ~[na:na]\n        at java.base/java.util.Formatter$FormatSpecifier.print(Formatter.java:3261) ~[na:na]\n        at java.base/java.util.Formatter$FormatSpecifier.printInteger(Formatter.java:2957) ~[na:na]\n        at java.base/java.util.Formatter$FormatSpecifier.print(Formatter.java:2918) ~[na:na]\n        at java.base/java.util.Formatter.format(Formatter.java:2689) ~[na:na]\n        at java.base/java.util.Formatter.format(Formatter.java:2625) ~[na:na]\n        at java.base/java.lang.String.format(String.java:4143) ~[na:na]\n        at org.bson.json.JsonParseException.(JsonParseException.java:56) ~[bson-4.6.1.jar:na]\n        **at org.springframework.data.mongodb.util.json.JsonScanner.nextToken(JsonScanner.java:118) ~[spring-data-mongodb-3.4.8.jar:3.4.8]**\n        at org.springframework.data.mongodb.util.json.ParameterBindingJsonReader.popToken(ParameterBindingJsonReader.java:755) ~[spring-data-mongodb-3.4.8.jar:3.4.8]\n        at org.springframework.data.mongodb.util.json.ParameterBindingJsonReader.readBsonType(ParameterBindingJsonReader.java:169) ~[spring-data-mongodb-3.4.8.jar:3.4.8]\n        at org.bson.AbstractBsonReader.verifyBSONType(AbstractBsonReader.java:679) ~[bson-4.6.1.jar:na]\n        at org.bson.AbstractBsonReader.checkPreconditions(AbstractBsonReader.java:721) ~[bson-4.6.1.jar:na]\n        at org.bson.AbstractBsonReader.readStartDocument(AbstractBsonReader.java:449) ~[bson-4.6.1.jar:na]\n        at org.springframework.data.mongodb.util.json.ParameterBindingDocumentCodec.decode(ParameterBindingDocumentCodec.java:235) ~[spring-data-mongodb-3.4.8.jar:3.4.8]\n        at org.springframework.data.mongodb.util.json.ParameterBindingDocumentCodec.decode(ParameterBindingDocumentCodec.java:67) ~[spring-data-mongodb-3.4.8.jar:3.4.8]\n        at org.springframework.data.mongodb.util.json.ParameterBindingDocumentCodec.readValue(ParameterBindingDocumentCodec.java:371) ~[spring-data-mongodb-3.4.8.jar:3.4.8]\n        at org.springframework.data.mongodb.util.json.ParameterBindingDocumentCodec.decode(ParameterBindingDocumentCodec.java:248) ~[spring-data-mongodb-3.4.8.jar:3.4.8]\n        at org.springframework.data.mongodb.util.json.ParameterBindingDocumentCodec.decode(ParameterBindingDocumentCodec.java:67) ~[spring-data-mongodb-3.4.8.jar:3.4.8]\n        at org.springframework.data.mongodb.util.json.ParameterBindingDocumentCodec.readValue(ParameterBindingDocumentCodec.java:371) ~[spring-data-mongodb-3.4.8.jar:3.4.8]\n`\n```\nI see the common problem is on `JsonScanner.nextToken()`, but I have no idea how to handle that, or why this gets such an error. I also tried changing the return type to a simple `List` and even to `String` in case it was returning an array instead of a `Collection`, but those end up in the same exception. Furthermore, if this was a problem with the mapping to `List`, then the `MongoRepository.findAll` method would've failed too, but it doesn't. So... I'm at loss.\nIs there something wrong with what I have so far? Why can't I get the query `getByMinicipality` working? Any pointers will be appreciated!\nMy POM looks like this:\n`\n\n    4.0.0\n    \n        org.springframework.boot\n        spring-boot-starter-parent\n        2.7.9\n         \n    \n    com.example\n    mongoflow\n    0.0.1-SNAPSHOT\n    war\n    mongoflow\n    Demo project for migration to MongoDB\n    \n        17\n    \n    \n        \n            org.springframework.boot\n            spring-boot-starter-data-mongodb\n        \n        \n            org.springframework.boot\n            spring-boot-starter-web\n        \n        \n            org.projectlombok\n            lombok\n            true\n        \n        \n            org.springframework.boot\n            spring-boot-starter-tomcat\n            provided\n        \n    \n\n    \n        \n            \n                org.springframework.boot\n                spring-boot-maven-plugin\n                \n                    \n                        \n                            org.projectlombok\n                            lombok\n                        \n                    \n                \n            \n        \n    \n\n`\nand some sample data\n`[{\n  \"_id\": {\n    \"$oid\": \"64037f46ea9875dd382fa6b2\"\n  },\n  \"state\": \"Michoac\u00e1n de Ocampo\",\n  \"settlements\": [\n    {\n      \"city\": null,\n      \"municipality\": \"Tzintzuntzan\",\n      \"settlement\": \"Tzintzuntzan\"\n    },\n    {\n      \"city\": null,\n      \"municipality\": \"Tzintzuntzan\",\n      \"settlement\": \"Yaguaro\"\n    },\n    {\n      \"city\": null,\n      \"municipality\": \"Tzintzuntzan\",\n      \"settlement\": \"El Pueblito\"\n    },\n    {\n      \"city\": null,\n      \"municipality\": \"Tzintzuntzan\",\n      \"settlement\": \"El Rinc\u00f3n\"\n    }\n  ],\n  \"code\": \"58440\"\n},{\n  \"_id\": {\n    \"$oid\": \"64037f46ea9875dd382fa6b3\"\n  },\n  \"state\": \"Michoac\u00e1n de Ocampo\",\n  \"settlements\": [\n    {\n      \"city\": null,\n      \"municipality\": \"Tzintzuntzan\",\n      \"settlement\": \"Ihuatzio\"\n    }\n  ],\n  \"code\": \"58442\"\n},{\n  \"_id\": {\n    \"$oid\": \"64037f46ea9875dd382fa6b4\"\n  },\n  \"state\": \"Michoac\u00e1n de Ocampo\",\n  \"settlements\": [\n    {\n      \"city\": null,\n      \"municipality\": \"Tzintzuntzan\",\n      \"settlement\": \"El Jag\u00fcey\"\n    },\n    {\n      \"city\": null,\n      \"municipality\": \"Tzintzuntzan\",\n      \"settlement\": \"Las Cuevas\"\n    },\n    {\n      \"city\": null,\n      \"municipality\": \"Tzintzuntzan\",\n      \"settlement\": \"Los Corrales\"\n    },\n    {\n      \"city\": null,\n      \"municipality\": \"Tzintzuntzan\",\n      \"settlement\": \"Nuevo Rodeo\"\n    },\n    {\n      \"city\": null,\n      \"municipality\": \"Tzintzuntzan\",\n      \"settlement\": \"San Isidro (Tziparamuco)\"\n    },\n    {\n      \"city\": null,\n      \"municipality\": \"Tzintzuntzan\",\n      \"settlement\": \"San Rafael\"\n    },\n    {\n      \"city\": null,\n      \"municipality\": \"Tzintzuntzan\",\n      \"settlement\": \"Santa Cruz\"\n    }\n  ],\n  \"code\": \"58443\"\n},{\n  \"_id\": {\n    \"$oid\": \"64037f46ea9875dd382fa6b5\"\n  },\n  \"state\": \"Michoac\u00e1n de Ocampo\",\n  \"settlements\": [\n    {\n      \"city\": null,\n      \"municipality\": \"Tzintzuntzan\",\n      \"settlement\": \"Coenembo\"\n    },\n    {\n      \"city\": null,\n      \"municipality\": \"Tzintzuntzan\",\n      \"settlement\": \"La Noria\"\n    },\n    {\n      \"city\": null,\n      \"municipality\": \"Tzintzuntzan\",\n      \"settlement\": \"El Tigre\"\n    },\n    {\n      \"city\": null,\n      \"municipality\": \"Tzintzuntzan\",\n      \"settlement\": \"El Pozo\"\n    },\n    {\n      \"city\": null,\n      \"municipality\": \"Tzintzuntzan\",\n      \"settlement\": \"Fontezuelas\"\n    }\n  ],\n  \"code\": \"58444\"\n},{\n  \"_id\": {\n    \"$oid\": \"64037f46ea9875dd382fa6b6\"\n  },\n  \"state\": \"Michoac\u00e1n de Ocampo\",\n  \"settlements\": [\n    {\n      \"city\": null,\n      \"municipality\": \"Tzintzuntzan\",\n      \"settlement\": \"Las Camelinas\"\n    },\n    {\n      \"city\": null,\n      \"municipality\": \"Tzintzuntzan\",\n      \"settlement\": \"Los Granjenos (El Puerto de los Granjenos)\"\n    },\n    {\n      \"city\": null,\n      \"municipality\": \"Tzintzuntzan\",\n      \"settlement\": \"Puerta de Coenembo\"\n    },\n    {\n      \"city\": null,\n      \"municipality\": \"Tzintzuntzan\",\n      \"settlement\": \"Tzocurio\"\n    },\n    {\n      \"city\": null,\n      \"municipality\": \"Tzintzuntzan\",\n      \"settlement\": \"Patambicho\"\n    }\n  ],\n  \"code\": \"58445\"\n},{\n  \"_id\": {\n    \"$oid\": \"64037f46ea9875dd382fa6b7\"\n  },\n  \"state\": \"Michoac\u00e1n de Ocampo\",\n  \"settlements\": [\n    {\n      \"city\": null,\n      \"municipality\": \"Tzintzuntzan\",\n      \"settlement\": \"Colonia L\u00e1zaro C\u00e1rdenas\"\n    },\n    {\n      \"city\": null,\n      \"municipality\": \"Tzintzuntzan\",\n      \"settlement\": \"Colonia San Juan\"\n    },\n    {\n      \"city\": null,\n      \"municipality\": \"Tzintzuntzan\",\n      \"settlement\": \"El Catorce\"\n    },\n    {\n      \"city\": null,\n      \"municipality\": \"Tzintzuntzan\",\n      \"settlement\": \"Sanabria\"\n    },\n    {\n      \"city\": null,\n      \"municipality\": \"Tzintzuntzan\",\n      \"settlement\": \"Tzintzuntzita\"\n    },\n    {\n      \"city\": null,\n      \"municipality\": \"Tzintzuntzan\",\n      \"settlement\": \"Tziranga (Tzizanga)\"\n    }\n  ],\n  \"code\": \"58446\"\n},{\n  \"_id\": {\n    \"$oid\": \"64037f46ea9875dd382fa6b8\"\n  },\n  \"state\": \"Michoac\u00e1n de Ocampo\",\n  \"settlements\": [\n    {\n      \"city\": null,\n      \"municipality\": \"Tzintzuntzan\",\n      \"settlement\": \"El Tecolote\"\n    },\n    {\n      \"city\": null,\n      \"municipality\": \"Tzintzuntzan\",\n      \"settlement\": \"Santiago Tzipijo\"\n    },\n    {\n      \"city\": null,\n      \"municipality\": \"Tzintzuntzan\",\n      \"settlement\": \"La Vinata\"\n    },\n    {\n      \"city\": null,\n      \"municipality\": \"Tzintzuntzan\",\n      \"settlement\": \"La Granada (El Mirador)\"\n    },\n    {\n      \"city\": null,\n      \"municipality\": \"Tzintzuntzan\",\n      \"settlement\": \"Cucuchucho (San Pedro)\"\n    }\n  ],\n  \"code\": \"58447\"\n},{\n  \"_id\": {\n    \"$oid\": \"64037f46ea9875dd382fa6b9\"\n  },\n  \"state\": \"Michoac\u00e1n de Ocampo\",\n  \"settlements\": [\n    {\n      \"city\": null,\n      \"municipality\": \"Tzintzuntzan\",\n      \"settlement\": \"Ichupio\"\n    },\n    {\n      \"city\": null,\n      \"municipality\": \"Tzintzuntzan\",\n      \"settlement\": \"Isla de Pacanda (Isla Pacanda)\"\n    },\n    {\n      \"city\": null,\n      \"municipality\": \"Tzintzuntzan\",\n      \"settlement\": \"Tarer\u00edo\"\n    },\n    {\n      \"city\": null,\n      \"municipality\": \"Tzintzuntzan\",\n      \"settlement\": \"Ucasanastacua (El Esp\u00edritu)\"\n    }\n  ],\n  \"code\": \"58448\"\n}]\n`",
      "solution": "From the documentation\n\nTo specify a query condition on fields in an embedded/nested document,\nuse dot notation (\"field.nestedField\"). When querying using dot\nnotation, the field and nested field must be inside quotation marks.\n\nSo, you need quotes for your field.\n```\n`@Query(\"{'settlements.municipality': ?0}\")\n`\n```\nYou can write this without `@Query` annotation also.\n```\n`List findBySettlementsMunicipality(String municipality);\n`\n```\nor\n```\n`List findBySettlements_Municipality(String municipality);\n`\n```",
      "question_score": 2,
      "answer_score": 4,
      "created_at": "2023-03-07T18:57:04",
      "url": "https://stackoverflow.com/questions/75665621/stackoverflow-on-querying-embedded-document"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb-query",
      "question_id": 72116644,
      "title": "How can I get random documents with the field starting letters (A to Z) in MongoDB?",
      "problem": "I'm trying to make an aggregate with mongodb. My goal is that I want to get random names\nstarting with letters A to Z. As a result, each word starts with letter must be only once in the response but I can't figure out how to do it. I used match condition with regex and sample condition to get random documents.\nHere is my collection;\n```\n`[\n  {\n    \"name\": \"ahmet\"\n  },\n  {\n    \"name\": \"bar\u0131\u015f\"\n  },\n  {\n    \"name\": \"ceyhun\"\n  },\n  {\n    \"name\": \"aslan\"\n  },\n  {\n    \"name\": \"deniz\"\n  },\n  ....\n]\n`\n```\nHere is my aggregate function;\n```\n`db.collection.aggregate([\n  {\n    $match: {\n      name: {\n        $regex: \"^a|^b|^c\" // must be A to Z\n      }\n    }\n  },\n  {\n    \"$sample\": {\n      \"size\": 3 // Must be 26\n    }\n  }\n])\n`\n```\nI'm waiting response to be like this;\n```\n`[\n  {\n    \"name\": \"ahmet\"\n  },\n  {\n    \"name\": \"bar\u0131\u015f\"\n  },\n  {\n    \"name\": \"ceyhun\"\n  },\n  .... // other words starting with d, e , f but only one word for each letter\n]\n`\n```\nBut I'm getting;\n```\n`[\n  {\n    \"_id\": ObjectId(\"5a934e000102030405000001\"),\n    \"name\": \"bar\u0131\u015f\"\n  },\n  {\n    \"_id\": ObjectId(\"5a934e000102030405000003\"),\n    \"name\": \"aslan\"\n  },\n  {\n    \"_id\": ObjectId(\"5a934e000102030405000000\"),\n    \"name\": \"ahmet\"\n  },\n  // name => aslan, name => ahmet (Two words starting with same letter)\n]\n`\n```\nI'm newbie at mongodb and if anyone can help me where I'm wrong, I'll be appreciate.\nMongo Playground",
      "solution": "You can do something like this:\nEdit with guard improvement suggestions* (using `$substrCP`, `$ifNull`):\n```\n`db.collection.aggregate([\n  {\n    $group: {\n      _id: {$substrCP: [\"$name\", 0, 1]},\n      name: {$push: \"$name\"}\n    }\n  },\n  {\n    $project: {_id: 0,\n      name: {\n        $arrayElemAt: [\n            \"$name\", \n            {$toInt: {$multiply: [{$rand: {}}, {$size: {$ifNull: [\"$name\",[]] }}]}\n          }\n        ]\n      }\n    }\n  }\n])\n`\n```\nAs you can see on this playground example.\nThe `$group` will keep a list of names per each `firstL`, the `$arryElemAt` with the `$rand` will keep only a random item.\n*Thanks to @Mbay and @Paul for the improvement suggestions",
      "question_score": 2,
      "answer_score": 4,
      "created_at": "2022-05-04T18:47:46",
      "url": "https://stackoverflow.com/questions/72116644/how-can-i-get-random-documents-with-the-field-starting-letters-a-to-z-in-mongo"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb-query",
      "question_id": 70954804,
      "title": "MongoDB aggregation matching ObjectId against string",
      "problem": "I have the following document which is also available in the mongo playground at:\nhttps://mongoplayground.net/p/zhcoi1BF0Ny\n```\n`db={\n  MyCollectionOne: [\n    {\n      \"firstId\": \"10\",\n      \"secondId\": \"123456789012345678901234\"\n    },\n    {\n      \"firstId\": \"11\",\n      \"secondId\": \"999999999999999999999999\"\n    }\n  ],\n  MyCollectionTwo: [\n    {\n      \"_id\": ObjectId(\"123456789012345678901234\"),\n      \"otherFieldOne\": \"Some Data\",\n      \"otherFieldTwo\": [\n        {\n          someNumber: 7\n        }\n      ]\n    },\n    {\n      \"_id\": ObjectId(\"999999999999999999999999\"),\n      \"otherFieldOne\": \"Some Other Data\",\n      \"otherFieldTwo\": [\n        {\n          someNumber: 9\n        },\n        {\n          someNumber: 39\n        }\n      ]\n    }\n  ]\n}\n`\n```\nGiven a firstId, I need to determine the correct MyCollectionTwo entry to return.  So for example, if I was given a firstId of 11, I would use that to lookup its corresponding secondId (which is \"999999999999999999999999\"). I would need to convert the secondId value to an ObjectId and then look through the MyCollectionTwo _id fields until I find the matching one.\nI gave it a try and am very close but cannot figure out how to correctly do the string->objectId conversion.\n```\n`db.MyCollectionTwo.aggregate([\n  {\n    $lookup: {\n      from: \"MyCollectionOne\",\n      localField: \"_id\",\n      foreignField: \"secondId\",\n      as: \"Temp\"\n    }\n  },\n  {\n    $unwind: \"$Temp\"\n  },\n  {\n    $match: {\n      \"Temp.firstId\": \"11\"\n    }\n  },\n  {\n    $project: {\n      _id: 1,\n      otherFieldOne: 1,\n      otherFieldTwo: 1\n    }\n  }\n]).find()\n`\n```\nI am aware there is a let/pipeline which can use a $toObjectId but I can't get that working in the above context.\nAny help would be appreciated. Thanks!",
      "solution": "Your `$lookup` with pipeline should be as below:\n```\n`$lookup: {\n  from: \"MyCollectionOne\",\n  let: {\n    id: \"$_id\"\n  },\n  pipeline: [\n    {\n      $match: {\n        $expr: {\n          $eq: [\n            {\n              $toObjectId: \"$secondId\"\n            },\n            \"$$id\"\n          ]\n        }\n      }\n    }\n  ],\n  as: \"Temp\"\n}\n`\n```\nSample Mongo Playground",
      "question_score": 2,
      "answer_score": 4,
      "created_at": "2022-02-02T12:28:54",
      "url": "https://stackoverflow.com/questions/70954804/mongodb-aggregation-matching-objectid-against-string"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb-query",
      "question_id": 69601591,
      "title": "MongoDB .NET Driver - StartsWith &amp; Contains with loosely typed data",
      "problem": "I can use the following for exact matches on loosely typed data in MongoDB:\n`var mongoClient = new MongoClient(con);\nIMongoDatabase mongoDatabase = mongoClient.GetDatabase(\"mydb\");\nvar profile = mongoDatabase.GetCollection(\"profiles\");\nvar query = profile.AsQueryable();\n\nvar results = query.Where(x => x[\"first_name\"] == \"john\").Take(10);\n`\nBut how do I use the same approach to do `StartsWith` and `Contains`?\nI tried:\n`var results = query.Where(x => x[\"first_name\"].AsString.Contains(\"john\")).Take(10);\n`\nBut I get the error:\n\nMethod 'Boolean Contains(System.String)' declared on type 'System.String' cannot be called with an instance of type 'MongoDB.Bson.BsonValue'\n\nHow do I use these filters?",
      "solution": "If you cast to string instead of using `AsString`, it should work:\n`var results = query.Where(x => ((string) x[\"first_name\"]).Contains(\"john\")).Take(10);\n`",
      "question_score": 2,
      "answer_score": 1,
      "created_at": "2021-10-17T07:32:38",
      "url": "https://stackoverflow.com/questions/69601591/mongodb-net-driver-startswith-contains-with-loosely-typed-data"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "mongodb-query",
      "question_id": 66932544,
      "title": "Find ObjectId _id but Schema has defined _id as String",
      "problem": "Previously I didn't declare the `_id` in my Schema, so each new submission naturally will have MongoDB generated `ObjectId` as it's `_id`. However, the requirements have changed and now `_id` is declared as `String` as below.\n```\n`var mongoose = require(\"mongoose\");\nvar Schema = mongoose.Schema;\n\nvar MySchema = new Schema({\n    _id: {\n        type: String,\n    },\n    schoolID: {\n        type: mongoose.Schema.Types.ObjectId, ref: 'School'\n    },\n    points: {\n        type: Number\n    },\n});\nMySchema.index({ schoolID : 1})\n\nmodule.exports = mongoose.model('Submission', MySchema);\n`\n```\nHowever, now I cannot find my previously inserted documents using `_id` at all. I tried\n```\n`var submissionId = \"60654319a8062f684ac8fde4\"\nSubmission.findOne({ _id: mongoose.mongo.ObjectId(submissionId ) })\nSubmission.findOne({ _id: mongoose.Types.ObjectId(submissionId ) })\nSubmission.findOne({ _id: mongoose.ObjectId(submissionId ) })\n`\n```\nbut it will always return `null`. So when I use `var mongoose = require('mongoose').set('debug', true);` to check, it will display below; that all my queries above will still find using `String`, and not `ObjectId`\n```\n`Mongoose: submission.findOne({ _id: '60654319a8062f684ac8fde4' }, { projection: {} })\n`\n```",
      "solution": "Problem - `_id: { type: String,},` mongoose will cast the value before making a query to the database, so in your case, it will always be a string.\nOption -1\nConvert your old objectId's to String as you're planning to use String `_id` so it's better to keep it consistent.\nRun these commands from the shell, Robomongo\nThis will add records with strings `_id` for existing `ObjectId` records.\n```\n`db.collection.find({}).toArray() // loop all records\n    .forEach(function (c) {\n        if (typeof c._id !== 'string') { // check _id is not string\n            c._id = c._id.str; db.collection.save(c); // create new record with _id as string value\n        }\n    });\n`\n```\nDelete records with `ObjectId`\n```\n`db.collection.remove({ _id: { $type: 'objectId' } })\n`\n```\n\nOption -2\nAdd Mongoose custom types.\nhttps://mongoosejs.com/docs/customschematypes.html\n```\n`class StringOrObjectId extends mongoose.SchemaType {\n  constructor(key, options) {\n    super(key, options, 'StringOrObjectId');\n  }\n\n  convertToObjectId(v) {\n    const checkForHexRegExp = new RegExp(\"^[0-9a-fA-F]{24}$\");\n    let _val;\n    try {\n      if (checkForHexRegExp.test(v)) {\n        _val = mongoose.Types.ObjectId(v);\n        return _val;\n      }\n    } catch (e) {\n      console.log(e);\n    }\n  }\n\n  convertToString(v) {\n    let _val = v;\n    try {\n      if(_.isString(_val)) return _val;\n      if(_.isNumber(_val)) return _.toString(_val);\n    } catch (e) {\n      console.log(e);\n    }\n  }\n\n  cast(val) {\n    const objectIdVal = this.convertToObjectId(val);\n    if (objectIdVal) return objectIdVal;\n\n    const stringVal = this.convertToString(val)\n    if (stringVal) return stringVal;\n\n    throw new Error('StringOrObjectId: ' + val +\n        ' Nor string nor ObjectId');\n  }\n}\n\nmongoose.Schema.Types.StringOrObjectId = StringOrObjectId;\n`\n```\n\n```\n`var MySchema = new Schema({\n    _id: {\n        type: StringOrObjectId, // custom type here\n    },\n    schoolID: {\n        type: mongoose.Schema.Types.ObjectId, ref: 'School'\n    },\n    points: {\n        type: Number\n    },\n});\n`\n```\n\nQuery\n```\n`Submission.findOne({ _id: submissionId }); // it will cast ObjectId or String or throw error\n`\n```\n\nDrawbacks\n\nIf your _id type string is `60516ae1ef682d2804a2fa72` is like valid `ObjectId` it will convert to `ObjectId` which will not match the record.\n\nNote - It's a rough class `StringOrObjectId` add proper checks and test properly.\n\nOption -3\nEasy way - Use `mongoose.Mixed`\nhttps://mongoosejs.com/docs/schematypes.html#mixed\nhttps://mongoosejs.com/docs/api.html#mongoose_Mongoose-Mixed\n```\n`cont MySchema = new Schema({\n    _id: {\n        type: mongoose.Mixed,\n    },\n    schoolID: {\n        type: mongoose.Schema.Types.ObjectId, ref: 'School'\n    },\n    points: {\n        type: Number\n    },\n});\n`\n```",
      "question_score": 2,
      "answer_score": 2,
      "created_at": "2021-04-03T16:58:13",
      "url": "https://stackoverflow.com/questions/66932544/find-objectid-id-but-schema-has-defined-id-as-string"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "pymongo",
      "question_id": 74139174,
      "title": "How to mock mongo with python",
      "problem": "How to create a mocked mongo db object to test my software using python?\nI tried https://pytest-mock-resources.readthedocs.io/en/latest/mongo.html but got error.\nFirst, i tried the code below:\n```\n`def insert_into_customer(mongodb_connection):\n    collection = mongodb_connection['customer']\n    to_insert = {\"name\": \"John\", \"address\": \"Highway 37\"}\n    collection.insert_one(to_insert)\n\nfrom pytest_mock_resources import create_mongo_fixture\nmongo = create_mongo_fixture()\n\ndef test_insert_into_customer(mongo):\n    insert_into_customer(mongo)\n\n    collection = mongo['customer']\n    returned = collection.find_one()\n\n    assert returned == {\"name\": \"John\", \"address\": \"Highway 37\"}\n\ntest_insert_into_customer(mongo)\n`\n```\nI got the error below:\n```\n`Traceback (most recent call last):\n  File \"/home/ehasan-karbasian/Desktop/NationalEliteFoundation/serp_matcher/src/mock_mongo.py\", line 19, in \n    test_insert_into_customer(mongo)\n  File \"/home/ehasan-karbasian/Desktop/NationalEliteFoundation/serp_matcher/src/mock_mongo.py\", line 11, in test_insert_into_customer\n    insert_into_customer(mongo)\n  File \"/home/ehasan-karbasian/Desktop/NationalEliteFoundation/serp_matcher/src/mock_mongo.py\", line 2, in insert_into_customer\n    collection = mongodb_connection['customer']\nTypeError: 'function' object is not subscriptable\n`\n```\nAnd then i tried the code:\n```\n`def insert_into_customer(mongodb_connection):\n    collection = mongodb_connection['customer']\n    to_insert = {\"name\": \"John\", \"address\": \"Highway 37\"}\n    collection.insert_one(to_insert)\n\nfrom pymongo import MongoClient\nfrom pytest_mock_resources import create_mongo_fixture\n\nmongo = create_mongo_fixture()\n\ndef test_create_custom_connection(mongo):\n    client = MongoClient(**mongo.pmr_credentials.as_mongo_kwargs())\n    db = client[mongo.config[\"database\"]]\n\n    collection = db[\"customers\"]\n    to_insert = [\n        {\"name\": \"John\"},\n        {\"name\": \"Viola\"},\n    ]\n    collection.insert_many(to_insert)\n\n    result = collection.find().sort(\"name\")\n    returned = [row for row in result]\n\n    assert returned == to_insert\n\ntest_create_custom_connection(mongo)\n`\n```\nand got the error:\n```\n`Traceback (most recent call last):\n  File \"/home/ehasan-karbasian/Desktop/NationalEliteFoundation/serp_matcher/src/mock_mongo.py\", line 30, in \n    test_create_custom_connection(mongo)\n  File \"/home/ehasan-karbasian/Desktop/NationalEliteFoundation/serp_matcher/src/mock_mongo.py\", line 14, in test_create_custom_connection\n    client = MongoClient(**mongo.pmr_credentials.as_mongo_kwargs())\nAttributeError: 'function' object has no attribute 'pmr_credentials'\n`\n```\nIt looks like `mongo` is a `function` not a `MongoClient`.\nHow can i use the library `pytest_mock_resources` to mock mongo?\nIs there a better library to mock monodb to test with pytest?",
      "solution": "`import mongomock\nclient = mongomock.MongoClient()\ndatabase = client.__getattr__(database_name)\ncollection = database.__getattr__(collection_name)\n`\nYou can define as many as client, database and collections you need at the same time.",
      "question_score": 5,
      "answer_score": 4,
      "created_at": "2022-10-20T13:33:11",
      "url": "https://stackoverflow.com/questions/74139174/how-to-mock-mongo-with-python"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "pymongo",
      "question_id": 73217987,
      "title": "Temporary failure in name resolution,mongo-sec:27017",
      "problem": "I am trying to connect to a remote mongodb server using `pymongo`, but I am getting error. The connection works in `Studio3T`. I am behind VPN, but I am sure the issue is not related with it otherwise the connection via `Studio3T` wouldn't work either. The error I am getting is\n\npymongo.errors.ServerSelectionTimeoutError: mongo-arb:27017: [Errno -3] Temporary failure in name resolution,mongo-sec:27017: [Errno -3] Temporary failure in name resolution,mongo-sec2:27017: [Errno -3] Temporary failure in name resolution,mongo:27017: [Errno -3] Temporary failure in name resolution, Timeout: 30s, Topology Description: , , , ]>\n\nThis is the code I am using\n```\n`from pymongo import MongoClient\nclient = MongoClient(\"mongodb://remote_server_ip:27017/\")\nclient.server_info()\n`\n```",
      "solution": "Turns out this was pymongo version issue.. Version 3.12 worked for me\n```\n`python3 -m pip install pymongo==3.12\n`\n```",
      "question_score": 5,
      "answer_score": 2,
      "created_at": "2022-08-03T09:41:39",
      "url": "https://stackoverflow.com/questions/73217987/temporary-failure-in-name-resolution-mongo-sec27017"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "pymongo",
      "question_id": 71015606,
      "title": "Connect to MongoDB without password using pymongo",
      "problem": "I'm a bit confused with the problem with pymongo and MongoDB.\nI have a MongoDB server that I can connect using Studio 3T without any password (behind VPN):\n\n```\n`mongodb://mongodb-dev.my.server:27017/?serverSelectionTimeoutMS=5000&connectTimeoutMS=10000\n`\n```\nWhen I'm connecting to the server from Python I get error:\n\npymongo.errors.ServerSelectionTimeoutError: Could not reach any servers in [('mongodb-dev', 27017)]. Replica set is configured with internal hostnames or IPs?, Timeout: 5.0s, Topology Description: \n\nPython script:\n`import pymongo\n\nclient = pymongo.MongoClient('mongodb://mongodb-dev.my.server:27017/?serverSelectionTimeoutMS=5000&connectTimeoutMS=10000')\n\ndb = client.get_database('feeeper')\ncol = db.get_collection('my_collection')\n\ncur = col.find({}, {'body': 1})\nlast_task = cur.sort([('created', pymongo.DESCENDING)]).limit(1)\n\nfor t in last_task:\n    print(t)\n`\nUPD:\nPing to the server works fine:\n\nI can connect to the server with mongo shell:",
      "solution": "The problem is with default arguments for the `MongoClient` constructor. It has a `directConnection` parameter with default value of `False` \u2014 connect to replica set:\n\ndirectConnection (optional): if True, forces this client to\nconnect directly to the specified MongoDB host as a standalone. If false, the client connects to the entire replica set of which the given MongoDB host(s) is a part. If this is True and a mongodb+srv:// URI or a URI containing multiple seeds is provided, an exception will be raised.\n\nWhen I passed `directConnection` as `True` the query worked fine.",
      "question_score": 4,
      "answer_score": 6,
      "created_at": "2022-02-07T09:41:51",
      "url": "https://stackoverflow.com/questions/71015606/connect-to-mongodb-without-password-using-pymongo"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "pymongo",
      "question_id": 66655722,
      "title": "AWS DocumentDB Syntax differs from MongoDB for UpdateMany",
      "problem": "I'm attempting to perform an update using a substr after migrating from mongo > documentDB, however I am getting a strange error.\nThe following works on Mongo 4.0 using pymongo:\n```\n`    await db[cls.colname].update_many(\n        {'$or': [{'preview_title': {'$exists': False}}, {'preview_title': None}]},\n        [{'$set': {'preview_title': {'$substr': ['$title', 0, 70]}}}],\n    )\n`\n```\nThe equivalent mongo shell command also works:\n```\n`db.getCollection('pages').updateMany({'$or': [{'preview_title': {'$exists': false}}, {'preview_title': null}]}, [{'$set': {'preview_title': {'$substrBytes': ['$title', 0, 70]}}}])\n`\n```\nHowever, after switching to AWS DocumentDB, it doesn't accept the update as an array or it will return this error: \"MongoError: Wrong type for parameter u\"\nimage\nIf I change the command to the below, it will work (basically just remove the square brackets from the update portion):\n```\n`db.getCollection('pages').updateMany({'$or': [{'preview_title': {'$exists': false}}, {'preview_title': null}]}, {'$set': {'preview_title': {'$substr': ['$title', 0, 70]}}})\n`\n```\nThis would be fine, however, the equivalent pymongo call doesnt seem to work:\n```\n`await db[cls.colname].update_many(\n            {'$or': [{'preview_title': {'$exists': False}}, {'preview_title': None}]},\n            {'$set': {'preview_title': {'$substr': ['$title', 0, 70]}}},\n        )\n`\n```\nIt returns the following error:\npymongo.errors.WriteError: Document can't have $ prefix field names: $substr",
      "solution": "Passing array as the update is called \"updating with an aggregation pipeline\". DocumentDB apparently doesn't implement that (among a bunch of other MongoDB features).\nWhen you remove the array from update you change from using https://docs.mongodb.com/manual/reference/operator/aggregation/set/ to using https://docs.mongodb.com/manual/reference/operator/update/set/. You'll notice that the update $set doesn't accept expressions like aggregation $set does. So your shell update with $set writes a (nested) document `{'$substr': ['$title', 0, 70]}` into the `preview` field. Generally keys starting with dollars are not queryable because the dollar prefix is used by MongoDB operators, so writing such a document into a field is a bad idea. Pymongo tells you this and refuses the operation. The shell allows it because it's also used internally by MongoDB server engineers for testing and so it permits various weird/unusual things to be done.",
      "question_score": 4,
      "answer_score": 4,
      "created_at": "2021-03-16T13:59:15",
      "url": "https://stackoverflow.com/questions/66655722/aws-documentdb-syntax-differs-from-mongodb-for-updatemany"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "pymongo",
      "question_id": 75430161,
      "title": "Cursor.count() gives AttributeError in pymongo 4.3.3",
      "problem": "As the title suggests, I am trying to use `count()` with a `find()` on a collection but it keeps throwing the error `AttributeError: 'Cursor' object has no attribute 'count'`.\nFor reference, I went through this question but `count_documents()` seems to be tehre for colelctions themselves, and not cursors. The other option mentioned was `len(list(cursor))` but I can't use that as it consumes the cursor itself (can't iterate over it after this). I went through a couple more answers, but these seem to be the main ways out.\nMoreover, my pymongo version is `4.3.3` which can't be changed due to some restrictions.\nIs there any operation I can perform directly on `Cursor` which doesn't consume it?\nSample code\n`def temp(col):\n    return col.find().count()\n\nprint(temp(collection))\n`\nThanks!",
      "solution": "`list()` will exhaust the cursor, but save its ouput to a variable and you can access it multiple times, e.g.\n```\n`records = list(col.find())\nnum_records = len(records)\n\nfor record in records:\n    # do stuff\n`\n```",
      "question_score": 4,
      "answer_score": 3,
      "created_at": "2023-02-12T22:02:47",
      "url": "https://stackoverflow.com/questions/75430161/cursor-count-gives-attributeerror-in-pymongo-4-3-3"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "pymongo",
      "question_id": 75500135,
      "title": "how can I add my own _id instead of the already given _id in MongoDB in Python?",
      "problem": "I have a class model using Pydantics. I try to supply my own ID but it gives me two id fields in the MongoDB database. The one I gave it and the one it makes automatically.\nHere is the result of my post method:\n\nhere is my class in models/articleModel.py:\n`class ArticleModel(BaseModel):\n    _id: int\n    title: str\n    body: str\n    tags: Optional[list] = None\n    datetime: Optional[datetime] = None\n    caption: Optional[str] = None\n    link: Optional[str] = None\n    \nclass Config:\n    orm_mode = True\n    allow_population_by_field_name = True\n    arbitrary_types_allowed = True\n`\nhere is my code for the post method in routers/article_router:\n`@router.post(\"/article/\", status_code=status.HTTP_201_CREATED)\ndef add_article(article: articleModel.ArticleModel):\n    article.datetime = datetime.utcnow()\n\n    try:\n        result = Articles.insert_one(article.dict())\n        pipeline = [\n            {'$match': {'_id': result.inserted_id}}\n        ]\n        new_article = articleListEntity(Articles.aggregate(pipeline))[0]\n        return new_article\n    except DuplicateKeyError:\n        raise HTTPException(status_code=status.HTTP_409_CONFLICT,\n                            detail=f\"Article with title: '{article.id}' already exists\")\n`",
      "solution": "I had to change the articleModel to a dictionary and add a new key called _id.\n`@router.post(\"/article/\", status_code=status.HTTP_201_CREATED)\ndef add_article(article: articleModel.ArticleModel):\n    article.datetime = datetime.utcnow()\n    \n    article_new_id = article.dict()\n    article_new_id['_id'] = article_new_id['id']\n    del article_new_id['id']\n\n    try:\n        result = Articles.insert_one(article_new_id)\n        pipeline = [\n            {'$match': {'_id': result.inserted_id}}\n        ]\n        new_article = articleListEntity(Articles.aggregate(pipeline))[0]\n        return new_article\n    except DuplicateKeyError:\n        raise HTTPException(status_code=status.HTTP_409_CONFLICT,\n                            detail=f\"Article with title: '{article.id}' already exists\")\n`",
      "question_score": 3,
      "answer_score": 4,
      "created_at": "2023-02-19T13:26:44",
      "url": "https://stackoverflow.com/questions/75500135/how-can-i-add-my-own-id-instead-of-the-already-given-id-in-mongodb-in-python"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "pymongo",
      "question_id": 69133756,
      "title": "File corrupted when using send_file() from flask, data from pymongo gridfs",
      "problem": "Well my English is not good, and the title may looks weird.\nAnyway, I'm now using flask to build a website that can store files, and mongodb is the database.\nThe file upload, document insert functions have no problems, the weird thing is that the file sent from flask `send_file()` was truncated for no reasons. Here's my code\n`from flask import ..., send_file, ...\nimport pymongo\nimport gridfs\n\n#...\n\n@app.route(\"/record/download/\")\ndef api_softwares_record_download(record_id):\n    try:\n        #...\n        file = files_gridfs.find_one({\"_id\": record_id})\n        file_ext = filetype.guess_extension(file.read(2048))\n        filename = \"{}-{}{}\".format(\n            app[\"name\"],\n            record[\"version\"],\n            \".{}\".format(file_ext) if file_ext else \"\",\n        )\n        response = send_file(file, as_attachment=True, attachment_filename=filename)\n        return response\n    except ...\n`\nThe original image file, for example, is 553KB. But the response body returns 549.61KB, and the image was broken. But if I just directly write the file to my disk\n`#...\nwith open('test.png', 'wb+') as file:\n    file.write(files_gridfs.find_one({\"_id\": record_id}).read())\n`\nThe image file size is 553KB and the image is readable.\nWhen I compare the two files with VS Code's text editor, I found that the correct file starts with `\ufffdPNG`, but the corrupted file starts with `\ufffd\u03df8\ufffd\ufffd\ufffd>\ufffdL\ufffdy`\nsearch the corrupted file head in the correct file\nAnd I tried to use `Blob` object and download it from the browser. No difference.\nIs there any wrong with my code or I misused `send_file()`? Or should I use `flask_pymongo`?",
      "solution": "And it's interesting that I have found what is wrong with my code.\nThis is how I solved it\n```\n`...file.read(2048)\nfile.seek(0)\n...\nfile.read(2048)\nfile.seek(0)\n...\nresponse = send_file(file, ...)\nreturn response\n`\n```\nAnd here's why:\nFor some reasons, I use `filetype` to detect the file's extension name and mime type, so I sent 2048B to `filetype` for detection.\n`file_ext = filetype.guess_extension(file.read(2048))\nfile_mime = filetype.guess_mime(file.read(2048)) #this line wasn't copied in my question. My fault.\n`\nAnd I have just learned from the pymongo API that python (or `pymongo` or `gridfs`, completely unknown to this before) reads file by using a cursor. When I try to find the cursor's position using `file.seek()`, it returns `4096`. So when I call `file.read()` again in `send_file()`, the cursor reads from 4096B away to the file head. 549+4=553, and here's the problem.\nFinally I set the cursor to position 0 after every `read()` operation, and it returns the correct file.\nHope this can help if you made the same mistake just like me.",
      "question_score": 3,
      "answer_score": 4,
      "created_at": "2021-09-10T16:27:58",
      "url": "https://stackoverflow.com/questions/69133756/file-corrupted-when-using-send-file-from-flask-data-from-pymongo-gridfs"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "pymongo",
      "question_id": 73168947,
      "title": "Unable to convert decimal to Decimal128 in python",
      "problem": "My Django project (python==3.10.3, Django==4.0.4) is accessing an MSSQL server via pymssql (pymssql==2.2.5), scraping what I need, and getting a list of dicts.\nI then save to MongoDB (pymongo==4.2.0) but there are decimal values that need to be Decimal128. I found Pymongo: Cannot encode object of type decimal.Decimal? which has a similar goal as mine. So I adapted to handle the list of dict like this:\n```\n`def convert_decimal_iterdict(list_of_dict):\n    if list_of_dict is None:\n        return None\n\n    for i in list_of_dict:\n        for k, v in i.items():\n            if isinstance(v, dict):\n                convert_decimal_iterdict(v)\n            elif isinstance(v, list):\n                for l in v:\n                    convert_decimal_iterdict(l)\n            elif isinstance(v, Decimal):\n                list_of_dict[k] = Decimal128(str(v))\n\n    return list_of_dict\n\n`\n```\nIt errors with\n```\n`   list_of_dict[k] = Decimal128(str(v))\nTypeError: list indices must be integers or slices, not str\n`\n```\nSo I attempted to use Decimal, float, or int...\n```\n`                list_of_dict[k] = Decimal128(Decimal(v))\n`\n```\nbut get\n```\n`#Decimal\n   raise TypeError(\"Cannot convert %r to Decimal128\" % (value,))\nTypeError: Cannot convert 1 to Decimal128\n\n#float\n    raise TypeError(\"Cannot convert %r to Decimal128\" % (value,))\nTypeError: Cannot convert 1.0 to Decimal128\n\n#int\n    raise TypeError(\"Cannot convert %r to Decimal128\" % (value,))\nTypeError: Cannot convert 1 to Decimal128\n\n`\n```\nremoving altogether\n```\n`                list_of_dict[k] = Decimal128(v)\n`\n```\njust gives the original 'not str' TypeError:\nI'm sure it's something easy that i'm just not seeing. I know you can't",
      "solution": "Let's take a closer look at the line at which an error is being raised:\n`                list_of_dict[k] = Decimal128(str(v))\n`\n`list_of_dict` is a list, `k` is a key from one of the `dict`s. From your error message, `k` is evidently a string. If you're indexing a list, you should be using list indexes, not `dict` keys.\nWhat you seem to want to do is to convert a single decimal value in one of the `dict`s to Decimal128. The current `dict` you are iterating through is in the variable `i`, so try changing the line above to this:\n`                i[k] = Decimal128(str(v))\n`\n(You might also want to consider improving the name of the variable `i`, both to make it more descriptive, and also because `i`, `j`, `k` etc. are often used as loop counter variables containing integer values.)\nFinally, I must state that this code hasn't been tested. I don't have a copy of MongoDB installed.",
      "question_score": 3,
      "answer_score": 3,
      "created_at": "2022-07-29T19:19:04",
      "url": "https://stackoverflow.com/questions/73168947/unable-to-convert-decimal-to-decimal128-in-python"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "pymongo",
      "question_id": 77674717,
      "title": "MongoDB MQL, split list in two and get only unique values",
      "problem": "I'm having a books collection in MongoDB.\nEach has a categories list up to 2 entries, example :\n```\n`categories: [\n   'Thriller'\n]\n`\n```\nor\n```\n`categories: [\n   'Adventure',\n   'Action'\n]\n`\n```\nUsing MongoDB MQL I need to get two categories lists filtered to project :\n\nfirst list with uniques first values of categories\nsecond list with uniques second values of categories\n\nI can only use $group, $addToSet and $arrayElemAt (no $unwind).\nHere where I am and I can't find the way to do it :\n```\n`collection.aggregate(\n    [\n        {\"$group\":\n            {\n                \"_id\": \"$_id\",\n                \"categories1\" : { \"$addToSet\": { \"$arrayElemAt\": [ \"$categories\", 0 ] } },\n                \"categories2\" : { \"$addToSet\": { \"$arrayElemAt\": [ \"$categories\", 1 ] } }\n            }\n        }\n    ]\n)\n`\n```\nExample of entry in books collection :\n```\n`{\n    _id: 2,\n    title: 'HARRY POTTER A L'ECOLE DES SORCIERS - ILLUSTRE PAR MINALIMA',\n    isbn: '2075145938',\n    pageCount: 368,\n    publishedDate: ISODate('2020-10-22T08:00:00.000Z'),\n    shortDescription: 'D\u00e9couvrez ou red\u00e9couvrez le texte int\u00e9gral...',\n    status: 'PUBLISH',\n    authors: [\n        'J.K. Rowling',\n        'Minalima'\n    ],\n    categories: [\n        'Youth',\n        'Adventure'\n    ]\n}\n`\n```\nExpected output :\n```\n`{\n    categories1 : [\n        'Youth',\n        'Thriller',\n        'Newspaper'],\n    categories2 : [\n        'Adventure',\n        'Newspaper',\n        'Essai'],\n\n}\n`\n```\ncategories1 include only unique values from categories with 0 index (first value) and categories2 include only unique values from categories but this time in second position (index 1).\nAny idea ?\nThanks!",
      "solution": "Your query is almost correct. You only need to change the `_id` for `$group`. Since you want the unique lists across all books, don't group on `$_id` - that would treat each object/book individually. Use `null` to group across all books in the collection:\n`db.collection.aggregate(\n    [\n        {\"$group\":\n            {\n                \"_id\": null,  // this is the only line I changed\n                \"categories1\" : { \"$addToSet\": { \"$arrayElemAt\": [ \"$categories\", 0 ] } },\n                \"categories2\" : { \"$addToSet\": { \"$arrayElemAt\": [ \"$categories\", 1 ] } }\n            }\n        },\n        { \"$project\": { \"_id\": 0 } }\n    ]\n)\n`\nAnd since you don't want `_id: null` in the result, I've added a second stage `$project` in the pipeline.\nBtw, if you're putting that aggregation pipeline directly in Python, change the `\"_id\": null` to  `\"_id\": None`\nMongo Playground\nexample data:\n`[\n  {\n    title: \"book one title\",\n    categories: [ \"Thriller\" ]\n  },\n  {\n    title: \"book TWO title\",\n    categories: [ \"Adventure\", \"Action\" ]\n  },\n  {\n    title: \"book 3 title\",\n    categories: [ \"Action\", \"Musical\", \"Animated\" ]\n  },\n  {\n    title: \"fourth book\",\n    categories: [ \"Thriller\", \"Musical\" ] // both of these are duplicate categories\n  }\n]\n`\nOutput:\n`[\n  {\n    \"categories1\": [ \"Thriller\", \"Adventure\", \"Action\" ],\n    \"categories2\": [ \"Musical\", \"Action\" ]\n  }\n]\n`",
      "question_score": 3,
      "answer_score": 1,
      "created_at": "2023-12-17T15:14:15",
      "url": "https://stackoverflow.com/questions/77674717/mongodb-mql-split-list-in-two-and-get-only-unique-values"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "pymongo",
      "question_id": 74250999,
      "title": "Pymongo : Ignore null input value when searching documents",
      "problem": "In mongodb i have many document structure like this in root collection\n```\n` _id: ObjectId()\n A : \"a\"\n B : \"b\"\n`\n```\n\n```\n` _id:ObjectId()\n A : \"c\"\n B : \"d\"\n`\n```\nAnd then i want to find document depend on user input, for example\n```\n`data = request.data\nitem_A = data.get('A', None)\nitem_B = data.get('B', None)\nfor item in root.find({\n    'A': item_A,\n    'B': item_B\n}):\n    print(item)\n`\n```\nbut the problem is if the user just want to find document depend on A and dont have input value for item_B then item_B will be None, so that the code don't return anything.\nAny suggestion?",
      "solution": "You just need to built the query properly, if `B` input is None then just ignore it in the query - you can do this in many ways here is one example:\n```\n`data = request.data\nitem_A = data.get('A', None)\nitem_B = data.get('B', None)\nquery = {}\n\nif item_A is not None:\n    query['A'] = item_A\n    \nif item_B is not None:\n    query['B'] = item_B\n    \nfor item in root.find(query):\nprint(item)\n`\n```\n\nEDIT\nAnother way for example is to create a dynamic query replacement.\n```\n`data = request.data\nitem_A = data.get('A', { \"$exists\": True })\nitem_B = data.get('B', { \"$exists\": True })\n\nfor item in root.find({\n    'A': item_A,\n    'B': item_B\n}):\n    print(item)\n`\n```",
      "question_score": 3,
      "answer_score": 1,
      "created_at": "2022-10-30T07:29:59",
      "url": "https://stackoverflow.com/questions/74250999/pymongo-ignore-null-input-value-when-searching-documents"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "pymongo",
      "question_id": 69574095,
      "title": "Mongodb: ignore large documents ( BSON &gt; 16 MB) during collection.aggregate()",
      "problem": "I'm scanning a mongodb collection which has large docs containing bson greater than 16 MB in size.\nEssentially, I'm calling either of the 2 depending on the flag for random sampling:\n```\n`documents = collection.aggregate(\n                [{\"$sample\": {\"size\": sample_size}}], allowDiskUse=True)\n`\n```\nOR\n```\n`documents = collection.aggregate(\n                [{\"$limit\": sample_size}], allowDiskUse=True)\n`\n```\nsample_size is a parameter here.\nThe issue is that this command gets stuck for minutes over large bson and then eventually mongodb aborts the execution and my scan of the entire collection is not completed.\nIs there a way to tell mongodb to skip/ignore documents having size larger than a threshold?\n```\n`For those who think that MongoDB cannot store values larger than 16 MB, here is the error message by a metadata collector (LinkedIn DataHub):\n\nOperationFailure: BSONObj size: 17375986 (0x10922F2) is invalid. \nSize must be between 0 and 16793600(16MB) First element: _id: \"Topic XYZ\",\nfull error: {'operationTime': Timestamp(1634531126, 2), 'ok': 0.0, 'errmsg': 'BSONObj size: 17375986 (0x10922F2) is invalid. Size must be between 0 and 16793600(16MB) \n`\n```",
      "solution": "Document max size is 16 MB see\n(Exception is the GridFS specification)\nIn your collection each document is already \nIf you want to filter lets say \nYou can use the \"$bsonSize\" operator to get the size of a document and filter out the big ones.",
      "question_score": 3,
      "answer_score": 1,
      "created_at": "2021-10-14T18:17:43",
      "url": "https://stackoverflow.com/questions/69574095/mongodb-ignore-large-documents-bson-16-mb-during-collection-aggregate"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "pymongo",
      "question_id": 77018602,
      "title": "MongoDB Atlas ServerSelectionTimeoutError: SSL handshake failed",
      "problem": "I'm having a hard time fetching some data from my mongodb database using pymongo driver. I keep getting the following error:\n```\n`pymongo.errors.ServerSelectionTimeoutError: SSL handshake failed: ac-mtxrap4-shard-00-01.vfg8vpd.mongodb.net:27017: [SSL: TLSV1_ALERT_INTERNAL_ERROR] tlsv1 alert internal error (_ssl.c:1007),SSL handshake failed: ac-mtxrap4-shard-00-00.vfg8vpd.mongodb.net:27017: [SSL: TLSV1_ALERT_INTERNAL_ERROR] tlsv1 alert internal error (_ssl.c:1007),SSL handshake failed: ac-mtxrap4-shard-00-02.vfg8vpd.mongodb.net:27017: [SSL: TLSV1_ALERT_INTERNAL_ERROR] tlsv1 alert internal error (_ssl.c:1007), Timeout: 30s, Topology Description: , , ]>\n`\n```\nI use linux ubuntu 22.04. Python version 3.10.12 and pymongo version 4.5.0\n```\n`from pymongo.mongo_client import MongoClient\nfrom pymongo.server_api import ServerApi\nfrom pymongo import database\nfrom credentials import mongoDB_uri\nimport json\nfrom bson import json_util\n\nclient = MongoClient(mongoDB_uri, server_api=ServerApi('1'))\n\nclass mongoDB:\n    def __init__(self, client=client):\n        self.client = client\n        \n    def getImageMetadata(self):\n        db = database.Database(client=self.client, name=\"productimagedata\")\n        cl = db.get_collection(name=\"images\")\n        imageMetaData = []\n\n        for obj in cl.find():\n            imageMetaData.append(obj)\n\n        return json.loads(json_util.dumps(imageMetaData))\n`\n```\nThere's my code just in case, but I'm almost 100% sure it isn't the issue with my script.\nI also must mention that my code worked completely fine just a few days ago.\nOn google I couldn't find any relevant solution that would work.\nI  tried:\n\nUpdating python to version  3.11.2\nAssigning certifi.where() (perhaps, ssl certificate path) to tlsCAFile property of MongoClient instance as suggested here\nEven setting ssl=False which didn't work (probably because it isn't the certificate issue as occurs to me)\nResearching on TLSV1_ALERT_INTERNAL_ERROR, but couldn't find anything that explains what's going on better than this answer\n\nCurious if anyone had encountered this kind of error before and can help with a solution.",
      "solution": "I have encountered this kind of error before make sure you do not have any VPN running on your computer. Moreover, check mongoDB atlas network settings check that your IP is allowed. Sometimes, this error occurs for apparently no reason as it was the case with me but it does not occur if you try sometime later.",
      "question_score": 2,
      "answer_score": 6,
      "created_at": "2023-08-31T21:02:08",
      "url": "https://stackoverflow.com/questions/77018602/mongodb-atlas-serverselectiontimeouterror-ssl-handshake-failed"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "pymongo",
      "question_id": 69658590,
      "title": "MongoClient to connect to multiple hosts to handle failover?",
      "problem": "I have a pretty simple PyMongo configuration that connects to two hosts:\n```\n`from pymongo import MongoClient\n\nhost = os.getenv('MONGODB_HOST', '127.0.0.1')\nport = int(os.getenv('MONGODB_PORT', 27017))\nhosts = []\nfor h in host.split(','):\n    hosts.append('{}:{}'.format(h, port))\ncls.client = MongoClient(host=hosts, replicaset='replicatOne')\n`\n```\n`MONGODB_HOST` is composed of a list of two ips, such as \"primary_mongo,secondary_mongo\"\nThey are configured as a replica set.\nThe issue I notice is that, in this current configuration, if `secondary_mongo` goes down, my whole code stops working.\nI believe that giving a list of hosts to `MongoClient` would tell it \"use the one that works, starting with the first\", but it appears it's not correct.\nWhat is wrong and how can I ensure that `MongoClient` connects properly to `primary_mongo` first, and in the event it fails, goes to `secondary_mongo` ?",
      "solution": "In order to have a fully running MongoDB ReplicaSet you must have a PRIMARY member. A PRIMARY can be only elected if the majority of all members is available. 1 out of 2 is not the majority, thus your MongoDB ReplicaSet goes down if one node fails.\nWhen you connect to your MongoDB then you can connect either to the ReplicaSet, e.g.\n```\n`mongo \"mongodb://localhost:27037,localhost:27137,localhost:27237/?replicaSet=repSet\"\n`\n```\nor you connect directly, e.g.\n```\n`mongo \"mongodb://localhost:27037\"\n`\n```\nWhen you connect to a ReplicaSet and the PRIMARY goes down (or when it steps down to SECONDARY for whatever reason) and another member becomes the PRIMARY, then usually the client automatically reconnect to the new PRIMARY. However, connecting to a ReplicaSet requires a PRIMARY member, i.e. the majority of members must be available.\nWhen you connect directly, then you can also connect to a SECONDARY member and use the MongoDB in read/only mode. However, you don't have any failover/reconnect function if the connected member goes down.\nYou could create an ARBITER member on one of your nodes. Then, if the other node goes down, the application is still fully available. Bear in mind, by this setup you can lose only the \"second\" host but not either of them. In best case you configure the ARBITER on an independent third location.\nFrom the MongoDB: The Definitive Guide by Shannon Bradshaw, Eoin Brazil, Kristina Chodorow: Part III - Replication, CHAPTER 9 - Setting Up a Replica Set:\n\nHow to Design a Set\nTo plan out your set, there are certain replica set concepts that you must be familiar\nwith. The next chapter goes into more detail about these, but the most important is that\nreplica sets are all about majorities: you need a majority of members to elect a primary,\na primary can only stay primary so long as it can reach a majority, and a write is safe\nwhen it\u2019s been replicated to a majority. This majority is defined to be \u201cmore than half of\nall members in the set,\u201d as shown in Table 9-1.\n\nNumber of members in the set\nMajority of the set\n\n1\n1\n\n2\n2\n\n3\n2\n\n4\n3\n\n5\n3\n\n6\n4\n\n7\n4\n\nNote that it doesn\u2019t matter how many members are down or unavailable, as majority is\nbased on the set\u2019s configuration.\nFor example, suppose that we have a five-member set and three members go down, as\nshown in Figure 9-1. There are still two members up. These two members cannot reach\na majority of the set (at least three members), so they cannot elect a primary. If one of\nthem were primary, it would step down as soon as it noticed that it could not reach a majority. After a few seconds, your set would consist of two secondaries and three unreachable members.\n\nMany users find this frustrating: why can\u2019t the two remaining members elect a primary?\nThe problem is that it\u2019s possible that the other three members didn\u2019t go down, and that\nit was the network that went down, as shown in Figure 9-2. In this case, the three members  on  the  left  will  elect  a  primary,  since  they  can  reach  a  majority  of  the  set  (three\nmembers out of five).\nIn the case of a network partition, we do not want both sides of the partition to elect a\nprimary: otherwise the set would have two primaries. Then both primaries would be\nwriting to the data and the data sets would diverge. Requiring a majority to elect or stay\nprimary is a neat way of avoiding ending up with more than one primary.\n\nIt is important to configure your set in such a way that you\u2019ll usually be able to have one\nprimary. For example, in the five-member set described above, if members 1, 2, and 3\nare in one data center and members 4 and 5 are in another, there should almost always\nbe a majority available in the first data center (it\u2019s more likely to have a network break\nbetween data centers than within them).",
      "question_score": 2,
      "answer_score": 6,
      "created_at": "2021-10-21T10:37:12",
      "url": "https://stackoverflow.com/questions/69658590/mongoclient-to-connect-to-multiple-hosts-to-handle-failover"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "pymongo",
      "question_id": 67571946,
      "title": "How to implement pagination for fastapi with mongo db(Motor)",
      "problem": "I have a simple REST api which is a books store created with FastAPI and mongo db as the backend (I have used `Motor` as the library instead of `Pymongo`). I have a `GET` endpoint to get all the books in the database which also supports query strings (For example : A user can search for books with a single author or with a genre type etc).\nBelow are the corresponding codes for this endpoint :\n`routers.py`\n```\n`\n@router.get(\"/books\", response_model=List[models.AllBooksResponse])\nasync def get_the_list_of_all_books(\n    authors: Optional[str] = None,\n    genres: Optional[str] = None,\n    published_year: Optional[str] = None,\n) -> List[Dict[str, Any]]:\n    if authors is None and genres is None and published_year is None:\n        all_books = [book for book in await mongo.BACKEND.get_all_books()]\n    else:\n        all_books = [\n            book\n            for book in await mongo.BACKEND.get_all_books(\n                authors=authors.strip('\"').split(\",\") if authors is not None else None,\n                genres=genres.strip('\"').split(\",\") if genres is not None else None,\n                published_year=datetime.strptime(published_year, \"%Y\")\n                if published_year is not None\n                else None,\n            )\n        ]\n\n    return all_books\n`\n```\nThe corresponding model :\n```\n`class AllBooksResponse(BaseModel):\n    name: str\n    author: str\n    link: Optional[str] = None\n\n    def __init__(self, name, author, **data):\n        super().__init__(\n            name=name, author=author, link=f\"{base_uri()}book/{data['book_id']}\"\n        )\n`\n```\nAnd the backend function for getting the data:\n```\n`class MongoBackend:\n    def __init__(self, uri: str) -> None:\n        self._client = motor.motor_asyncio.AsyncIOMotorClient(uri)\n\n    async def get_all_books(\n        self,\n        authors: Optional[List[str]] = None,\n        genres: Optional[List[str]] = None,\n        published_year: Optional[datetime] = None,\n    ) -> List[Dict[str, Any]]:\n        find_condition = {}\n        if authors is not None:\n            find_condition[\"author\"] = {\"$in\": authors}\n        if genres is not None:\n            find_condition[\"genres\"] = {\"$in\": genres}\n        if published_year is not None:\n            find_condition[\"published_year\"] = published_year\n        cursor = self._client[DB][BOOKS_COLLECTION].find(find_condition, {\"_id\": 0})\n        return [doc async for doc in cursor]\n`\n```\nNow i want to implement pagination for this endpoint . Here i have a few questions :\n\nIs it good to do pagination on database level or application level ?\nDo we have some out of the box libraries which can help me do that in fastapi ? I checked the documentation for https://pypi.org/project/fastapi-pagination/ , but this seems to be more targeted towards SQL databases\nI also checked out this link : https://www.codementor.io/@arpitbhayani/fast-and-efficient-pagination-in-mongodb-9095flbqr which talks about different ways of doing this in Mongo db but i think only the first option(using `limit` and `skip`) would work for me, because i want to also make it work when i am using other filter parameters (for example for author and genre) and there is no way i can know the ObjectId's unless i make the first query to get the data and then i want to do pagination.\n\nBut the issue is everywhere i see using `limit` and `skip` is discouraged.\nCan someone please let me know what are the best practices here and can something apply to my requirement and use case?\nMany thanks in advance.",
      "solution": "There is no right or wrong answer to such a question. A lot depends on the technology stack that you use, as well as the context that you have, considering also the future directions of both the software you wrote as well as the software you use (mongo).\nAnswering your questions:\n\nIt depends on the load you have to manage and the dev stack you use. Usually it is done at database level, since retrieving the first 110 and dropping the first 100 is quite dumb and resource consuming (the database will do it for you).\n\nTo me is seems pretty simple on how to do it via `fastapi`: just add to your `get` function the parameters `limit: int = 10` and `skip: int = 0` and use them in the filtering function of your database. `Fastapi` will check the data types for you, while you could check that limit is not negative or above, say, 100.\n\nIt says that there is no silver bullet and that since `skip` function of mongo does not perform well. Thus he believes that the second option is better, just for performances. If you have billions and billions of documents (e.g. amazon), well, it may be the case to use something different, though by the time your website has grown that much, I guess you'll have the money to pay an entire team of experts to sort things out and possibly develop your own database.\n\nTL;DR\nConcluding, the `limit` and `skip` approach is the most common one. It is usually done at the database level, in order to reduce the amount of work of the application and bandwidth.\nMongo is not very efficient in skipping and limiting results. If your database has, say a million of documents, then I don't think you'll even notice. You could even use a relational database for such a workload. You can always benchmark the options you have and choose the most appropriate one.\nI don't know much about mongo, but I know that generally, indexes can help limiting and skipping records (docs in this case), but I'm not sure if it's the case for mongo as well.",
      "question_score": 2,
      "answer_score": 4,
      "created_at": "2021-05-17T16:44:55",
      "url": "https://stackoverflow.com/questions/67571946/how-to-implement-pagination-for-fastapi-with-mongo-dbmotor"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "pymongo",
      "question_id": 67284850,
      "title": "MongoDB conditional (sum if exists, else zero)",
      "problem": "I am trying to sum across a field within an aggregate pipeline where the field may not exist. Otherwise, the return should be zero. This is my code so far:\n```\n`admits = [\n    {'$match': {'meta.State': item['state'],'meta.County': item['county'], 'meta.first_seen': date}},\n    {'$group': {'_id': {'item': '$item'}, 'admissions': {'$ifNull': [{'$sum': 1}, 0]}}},\n]\n`\n```\nThis does not work, because calling `$sum` within an `$ifNull` raises a unary operator exception:\n```\n`pymongo.errors.OperationFailure: The $ifNull accumulator is a unary operator\n`\n```",
      "solution": "pymongo.errors.OperationFailure: The $ifNull accumulator is a unary operator\n\nThe `` operator must be one of the following accumulator operators: accumulator-operator, and `$ifNull` operator is not one of them,\nThe `$sum` operator must be in root if you want to sum,\nThe usage of $ifNull is:\n\nEvaluates an expression and returns the value of the expression if the expression evaluates to a non-null value. If the expression evaluates to a null value, including instances of undefined values or missing fields, returns the value of the replacement expression.\n\nSo `$ifNull` will not fulfil your requirement,\nYou can try $cond operator to check if field type is missing then then 0 otherwise 1,\n```\n`{\n  '$group': {\n    '_id': {'item': '$item'}, \n    'admissions': {\n      $sum: {\n        $cond: [{ $eq: [{ $type: \"$admissions\" }, \"missing\"] }, 0, 1]\n      }\n    }\n  }\n},\n`\n```",
      "question_score": 2,
      "answer_score": 5,
      "created_at": "2021-04-27T16:15:30",
      "url": "https://stackoverflow.com/questions/67284850/mongodb-conditional-sum-if-exists-else-zero"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "pymongo",
      "question_id": 70744604,
      "title": "Collection object is not callable. If you meant to call the createCollection method on a Database object it is failing because no such method exists",
      "problem": "I am having issues with mongoDB specifically when trying to create a collection.\nI have imported pymongo and DNS they both work but when I try to create a collection using `db.createCollection(\"verified\")`, it would throw an error saying that `'Database' not an attribute of 'createCollection'`\nHere is the part of my code that involves MongoDB:\n```\n`import pymongo\nimport dns\n\nclient = pymongo.MongoClient(myMongoDBconnectionURL)\ndb = client['MainDB']\ndb.createCollection(\"verified\")\n`\n```\nDo you know what is causing this?\nThanks",
      "solution": "I suspect that you are using the wrong MongoDB Documentation (as in, the documentation of a MongoDB implementation in a different programming language). `createCollection` is not a method of `pymongo.database.Database`, but `create_collection` is:\n`import pymongo\nimport dns\n\nclient = pymongo.MongoClient(myMongoDBconnectionURL)\ndb = client['MainDB']\ndb.create_collection(\"verified\")\n`",
      "question_score": 2,
      "answer_score": 4,
      "created_at": "2022-01-17T17:33:54",
      "url": "https://stackoverflow.com/questions/70744604/collection-object-is-not-callable-if-you-meant-to-call-the-createcollection-met"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "pymongo",
      "question_id": 79414537,
      "title": "mongomock BulkOperationBuilder.add_update() unexpected keyword argument &#39;sort&#39;",
      "problem": "I'm testing a function that performs a bulk upsert using UpdateOne with bulk_write. In production (using the real MongoDB client) everything works fine, but when running tests with `mongomock` I get this error:\n`app.mongodb.exceptions.CatalogException: failure in mongo repository function `batch_upsert_catalog_by_sku`: BulkOperationBuilder.add_update() got an unexpected keyword argument 'sort'\n`\nI don't pass any sort parameter in my code. Here's the relevant function:\n`def batch_upsert_catalog_by_sku(self, items: List[CatalogBySkuWrite]) -> None:\n    operations = []\n    current_time = datetime.datetime.now(datetime.timezone.utc)\n    for item in items:\n        update_fields = item.model_dump()\n        update_fields[\"updated_at\"] = current_time\n\n        operations.append(\n            UpdateOne(\n                {\"sku\": item.sku, \"chain_id\": item.chain_id},\n                {\n                    \"$set\": update_fields,\n                    \"$setOnInsert\": {\"created_at\": current_time},\n                },\n                upsert=True,\n            )\n        )\n\n    if operations:\n        result = self.collection.bulk_write(operations)\n        logger.info(\"Batch upsert completed\", \n                    matched=result.matched_count,\n                    upserted=result.upserted_count,\n                    modified=result.modified_count)\n`\nHas anyone seen this error with mongomock? Is it a version issue or a bug, and what would be a good workaround? I'm using mongomock version `4.3.0` and pymongo version `4.11`.\nThanks!",
      "solution": "This seem to be related to pymongo version 4.11.\nIn 4.10.1, the method `_add_to_bulk` in the class `UpdateOne` here  calls `add_update` this way:\n`    def _add_to_bulk(self, bulkobj: _AgnosticBulk) -> None:\n        \"\"\"Add this operation to the _AsyncBulk/_Bulk instance `bulkobj`.\"\"\"\n        bulkobj.add_update(\n            self._filter,\n            self._doc,\n            False,\n            bool(self._upsert),\n            collation=validate_collation_or_none(self._collation),\n            array_filters=self._array_filters,\n            hint=self._hint,\n        )\n`\nHowever, in the current release (4.11), here, it calls it this way:\n`    def _add_to_bulk(self, bulkobj: _AgnosticBulk) -> None:\n        \"\"\"Add this operation to the _AsyncBulk/_Bulk instance `bulkobj`.\"\"\"\n        bulkobj.add_update(\n            self._filter,\n            self._doc,\n            False,\n            bool(self._upsert),\n            collation=validate_collation_or_none(self._collation),\n            array_filters=self._array_filters,\n            hint=self._hint,\n            sort=self._sort,\n        )\n`\nLatest version of mongomock does not seem to support the `sort` parameter, check here\nForcing pymongo to version \nI have opened an issue in mongomock repository.",
      "question_score": 2,
      "answer_score": 3,
      "created_at": "2025-02-05T12:09:05",
      "url": "https://stackoverflow.com/questions/79414537/mongomock-bulkoperationbuilder-add-update-unexpected-keyword-argument-sort"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "pymongo",
      "question_id": 72081511,
      "title": "bson.errors.InvalidId: &#39;_id&#39; is not a valid ObjectId, it must be a 12-byte input or a 24-character hex string",
      "problem": "```\n`@app.route(\"/product/\", methods=[\"GET\"])\ndef product(id_product):\n    params = request.args    \n    api_key = params[\"APIKEY\"]\n    if check_key(api_key):\n        review = reviews.find_one(ObjectId(id_product), projection= {\"comments\": 0, \"_id\": 0, \"user_image\":0})\n        product = products.find_one(ObjectId(id_product), projection= {\"views\": 0})\n        products.update_one({\"_id\":ObjectId(id_product)}, {\"$inc\":{\"views\": 1}})\n        return jsonify(product=product, reviews=review)\n    else:\n        return jsonify(ERROR=\"AUTHENTICATION PROBLEM USE VALID API KEY\")\n`\n```\nERROR:\nbson.errors.InvalidId: '626bccb9697a12204fb22ea30' is not a valid ObjectId, it must be a 12-byte input or a 24-character hex string\nMy code it gets a product_id from url product_id is _id of certain product in mongodb \nFOR EXAMPLE :\nid_product = 626bccb9697a12204fb22ea30\nin mongodb my code should get document with _id = ObjectId(626bccb9697a12204fb22ea30)\nI tried so many things but nothing worked !!!\nwhat i think solution is that i have to convert string to a bytes instance\nEven i tried to do this\n```\n`\n@app.route(\"/product/\", methods=[\"GET\"])\ndef product(id_product):\n    params = request.args    \n    api_key = params[\"APIKEY\"]\n    pro= bytes(id_product , \"utf-8\"))\n    if check_key(api_key):\n        review = reviews.find_one(ObjectId(pro), projection= {\"comments\": 0, \"_id\": 0, \"user_image\":0})\n        product = products.find_one(ObjectId(pro), projection= {\"views\": 0})\n        products.update_one({\"_id\":ObjectId(pro)}, {\"$inc\":{\"views\": 1}})\n        return jsonify(product=product, reviews=review)\n    else:\n        return jsonify(ERROR=\"AUTHENTICATION PROBLEM USE VALID API KEY\")\n`\n```\nIn this code i tried to convert string to bytes but doesn't work\nERROR:\nTypeError: id must be an instance of (bytes, str, ObjectId), not \nThanks in advance :)",
      "solution": "Your `objectId` has 25 characters; if you make it 24 characters it will work.\n```\n`from pymongo import MongoClient\nfrom bson import ObjectId\n\ndb = MongoClient()['mydatabase']\n\ndb.mycollection.insert_one({\n    \"_id\": ObjectId('626bccb9697a12204fb22ea3'),\n    \"key\": \"value\"\n})\n`\n```",
      "question_score": 2,
      "answer_score": 3,
      "created_at": "2022-05-02T03:09:15",
      "url": "https://stackoverflow.com/questions/72081511/bson-errors-invalidid-id-is-not-a-valid-objectid-it-must-be-a-12-byte-input"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "pymongo",
      "question_id": 67992277,
      "title": "Select multiple values from a document",
      "problem": "Background\nI have data stored in the following format\n```\n`{\n    \"player_id\": \"VU3R5HNTAGMK\",\n    \"markers\": {\n        \"BICF2P964092\": \"GC\",\n        \"BICF2G630653981\": \"CG\",\n        \"BICF2P483996\": \"CT\",\n        \"BICF2S23452916\": \"CG\",\n        \"chr26_19147949\": \"TC\",\n    }\n}\n`\n```\nYou can imagine i have data stored for multiple players and each has a unique `player_id` and they all have varying number of markers with different marker values.\nIn the above case a marker is `BICF2P964092` and it's marker value is `GC`.\nI am trying to query my mongo db in various ways. One obvious way is by using player_id. To do that I do the following `col.find({\"player_id\": \"VU3R5HNTAGMK\"})`\nAnother thing i want to do is maybe I just want to know value of a specific marker for a specific player. So for that I can do the following `col.find({\"player_id\": \"VU3R5HNTAGMK\"}, {'markers.BICF2P964092'})`\nISSUE\nI also want to be able to get values for multiple markers for a specific player and i am not able to do so. I have tried the following with no luck.\n`col.find({\"player_id\": \"VU3R5HNTAGMK\"},{'markers': {'$in': [\"BICF2P964092\", \"chr26_19147949\"]}})`\n`col.find({\"player_id\": \"VU3R5HNTAGMK\"}, {'markers.BICF2P964092'}, {'markers.chr26_19147949'})`\nI would really appreciate it if someone can help me write a query where i can get multiple marker values for specified `marker` and `player_id`",
      "solution": "You can simply do the following\n```\n`col.find({\u201cplayer_id\u201d: \u201cVU3R5HNTAGMK\u201d}, {\u201cmarkers.\u201d + m: 1 for m in [\u201c BICF2P964092\", \u201cBICF2G630653981\u201d]})\n`\n```",
      "question_score": 2,
      "answer_score": 1,
      "created_at": "2021-06-15T21:20:21",
      "url": "https://stackoverflow.com/questions/67992277/select-multiple-values-from-a-document"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "pymongo",
      "question_id": 66732539,
      "title": "Mongodb DeprecationWarning: count is deprecated. Use Collection.count_documents instead",
      "problem": "I want to query with Python mongodb but \"DeprecationWarning: count is deprecated. Use Collection.count_documents instead.\" gives such an error. How do I solve this?",
      "solution": "Just use the new method `count_documents` as `count` is deprecated\n```\n`mycol.count_documents(myquery)\n`\n```\nhttps://pymongo.readthedocs.io/en/stable/api/pymongo/collection.html?highlight=count#pymongo.collection.Collection.count_documents\nhttps://docs.mongodb.com/manual/reference/method/db.collection.countDocuments/",
      "question_score": 2,
      "answer_score": 3,
      "created_at": "2021-03-21T13:55:35",
      "url": "https://stackoverflow.com/questions/66732539/mongodb-deprecationwarning-count-is-deprecated-use-collection-count-documents"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "pymongo",
      "question_id": 66412005,
      "title": "Mongodb 4.2.8: Unable to add session into the cache because the number of active sessions is too high",
      "problem": "Suddenly we started to experience following problem during connection to Mongo: `{u'code': 261, u'ok': 0.0, u'$clusterTime': {u'clusterTime': Timestamp(1614532995, 3141), u'signature': {u'keyId': 0L, u'hash': Binary('\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00', 0)}}, u'codeName': u'TooManyLogicalSessions', u'operationTime': Timestamp(1614532995, 3141), u'errmsg': u'Unable to add session into the cache because the number of active sessions is too high'}`.\nThis happens for any connection driver we use:\n\nMongo shell\npymongo 3.6\npymongo 3.11\n\nAnd happens not for every query, but roughly for 30-40% of all queries.\nMeanwhile, `maxSession` has default value (1000000) and I have following data from db status:\n```\n`\"logicalSessionRecordCache\" : {\n        \"activeSessionsCount\" : 2244,\n        \"sessionsCollectionJobCount\" : 48430,\n        \"lastSessionsCollectionJobDurationMillis\" : 0,\n        \"lastSessionsCollectionJobTimestamp\" : ISODate(\"2021-02-28T16:56:03.438Z\"),\n        \"lastSessionsCollectionJobEntriesRefreshed\" : 0,\n        \"lastSessionsCollectionJobEntriesEnded\" : 0,\n        \"lastSessionsCollectionJobCursorsClosed\" : 0,\n        \"transactionReaperJobCount\" : 49566,\n        \"lastTransactionReaperJobDurationMillis\" : 1,\n        \"lastTransactionReaperJobTimestamp\" : ISODate(\"2021-02-28T17:03:17.631Z\"),\n        \"lastTransactionReaperJobEntriesCleanedUp\" : 0,\n        \"sessionCatalogSize\" : 33\n    },\n`\n```\nOnce the issue has been discovered, I checked periodically a number of sessions in `config.system.sessions`. It varied from 11k to 560k (most of the time it was between 80k and 350k), which seems to be quite high.\nHowever, the problem remained disregard of the amount of sessions.\nAn error is sudden, we have the same load as before (I don't know the number of sessions we used to have before but we didn't add any new clients - we have about 3k connections.\nThere is no sharding, only a replica (one primary and one secondary).\nI would really appreciate any advice on how to overcome this problem.\nUPD: another thing that looks weird for me:\n```\n`> db.system.sessions.count()\n416068\n> db.currentOp(true).inprog.length\n2911\n`\n```\nhow is it possible to have such a difference?",
      "solution": "Most likely you are going to need to do some debugging in your application to figure out where you are leaking sessions.\n\nUpdate driver and server to most recent versions.\n\nIdentify where your application is using explicit sessions. Explicit sessions are those that you start via a start_session call. The driver also uses sessions automatically by itself, those are called implicit sessions.\n\nLacking evidence to the contrary, you have a session leak. Use https://docs.mongodb.com/manual/reference/command/killAllSessions/ to destroy all sessions, then graph the number of active sessions over time to see what your trend looks like.\n\nReview your code and match every start_session call with how that session is ended (if any). If you do not use a scoped API like https://docs.mongodb.com/ruby-driver/master/tutorials/ruby-driver-sessions/#creating-a-session-from-a-mongo-client you need to CAREFULLY consider where each of the sessions is going to get destroyed.\n\nCheck your code for no timeout cursors. Those would probably hold session references (explicit or implicit).\n\nGoing by the information you provided in the question, my guess is your session state inspection isn't done properly so go over that again and make sure you are looking at the right things.",
      "question_score": 2,
      "answer_score": 2,
      "created_at": "2021-02-28T18:36:32",
      "url": "https://stackoverflow.com/questions/66412005/mongodb-4-2-8-unable-to-add-session-into-the-cache-because-the-number-of-active"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "pymongo",
      "question_id": 73314744,
      "title": "MongoDB: Can&#39;t update in nested arrays",
      "problem": "I've been trying to modify a value in multiple arrays for a few arrays and I can't find documentation on how to do this.\nMy collection looks like this\n```\n`\"rates\": [\n    {\n      \"category\": \"Web\",\n      \"seniorityRates\": [\n        {\n          \"seniority\": \"junior\",\n          \"rate\": 100\n        },\n        {\n          \"seniority\": \"intermediate\",\n          \"rate\": 135\n        },\n        {\n          \"seniority\": \"senior\",\n          \"rate\": 165\n        }\n      ]\n    }\n  ]\n`\n```\nI'm just trying to modify \"junior\" to \"beginner\", this should be simple.\nThanks to these answers:\nHow can I update a multi level nested array in MongoDB?\nMongoDB updating fields in nested array\nI've manage to write that python code (pymongo), but it doesn't works...\n```\n`result = my_coll.update_many({},\n        {\n            \"$set\":\n            {\n                \"rates.$[].seniorityRates.$[j].seniority\" : new\n            }\n        },\n        upsert=False,\n        array_filters= [\n                {\n                \"j.seniority\": old\n                }\n            ]\n        )\n`\n```\n\nThe path 'rates' must exist in the document in order to apply array updates.\n\nIt correspond to this command that doesn't work either\n```\n`db.projects.updateMany({},\n  {\n      $set:\n      {\n          \"rates.$[].seniorityRates.$[j].seniority\" : \"debutant\"\n      }\n  },\n  { arrayFilters = [\n          {\n          \"j.seniority\": \"junior\"\n          }\n      ]\n  }\n)\n`\n```\n\nclone(t={}){const r=t.loc||{};return e({loc:new Position(\"line\"in r?r.line:this.loc.line,\"column\"in r?r.column:......)} could not be cloned\n\nWhat am I doing wrong ?\nAny help would be very appreciated",
      "solution": "The other option could be Sample\n```\n`db.collection.update({},\n{\n  $set: {\n    \"rates.$[].seniorityRates.$[j].seniority\": \"debutant\"\n  }\n},\n{\n  arrayFilters: [\n    {\n      \"j.rate\": { //As per your data, you can apply the condition o rate field to modify the level\n        $lte: 100\n      }\n    }\n  ]\n})\n`\n```\nOr\nThe actual query should work Sample\n```\n`db.collection.update({},\n{\n  $set: {\n    \"rates.$[].seniorityRates.$[j].seniority\": \"debutant\"\n  }\n},\n{\n  arrayFilters: [\n    {\n      \"j.seniority\": \"junior\"\n    }\n  ]\n})\n`\n```\nThe same should work in python, a sample question",
      "question_score": 2,
      "answer_score": 2,
      "created_at": "2022-08-11T04:50:47",
      "url": "https://stackoverflow.com/questions/73314744/mongodb-cant-update-in-nested-arrays"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "pymongo",
      "question_id": 73222881,
      "title": "Datetime to string in mongodb where only month and year are in data",
      "problem": "suppose this is my Data:-\n```\n`[\n  {\"Id_Cust\": \"4145\",\"firstName\": \"Albade\",\"lastName\": \"Maazou\", \"gender\": \"female\",\"date\": \"031981\"},\n  {\"Id_Cust\": \"5296\", \"firstName\": \"Rafael\",\"lastName\": \"Oliveira\",\"gender\": \"male\",\"date\": \"061987\"},\n  {\"Id_Cust\": \"6192\",\"firstName\": \"Abdul Rahman\",\"lastName\": \"Budjana\",\"gender\": \"male\",\"date\": \"011990\"}\n]\n`\n```\nI try by `$datetostring` and `$datefromstring`\nI also try format but still got an Error:-\nBut, Nothing happened.\n```\n`OperationFailure: PlanExecutor error during aggregation :: caused by :: an incomplete date/time string has been found, with elements missing: \"031981\", full error: {'ok': 0.0, 'errmsg': 'PlanExecutor error during aggregation :: caused by :: an incomplete date/time string has been found, with elements missing: \"031981\"', 'code': 241, 'codeName': 'ConversionFailure'}\n`\n```",
      "solution": "Mongo requires the \"date\" part to fully exist, this means it expects to get at least a year, a month and a day.\nSo you can either pad your string with `01` and provide the format option:\n```\n`db.collection.aggregate([\n  {\n    $addFields: {\n      date: {\n        \"$dateFromString\": {\n          \"dateString\": {\n            \"$concat\": [\n              \"01\",\n              \"$date\"\n            ]\n          },\n          \"format\": \"%d%m%Y\"\n        }\n      }\n    }\n  }\n])\n`\n```\nMongo Playground\nOr use `$dateFromParts` instead:\n```\n`db.collection.aggregate([\n  {\n    $addFields: {\n      date: {\n        \"$dateFromParts\": {\n          year: {\n            \"$toInt\": {\n              \"$substr\": [\n                \"$date\",\n                2,\n                4\n              ]\n            }\n          },\n          month: {\n            $toInt: {\n              $substr: [\n                \"$date\",\n                0,\n                2\n              ]\n            }\n          }\n        }\n      }\n    }\n  }\n])\n`\n```\nMongo Playground\n\nBonus - This requirement is not stated explicitly in the documentation but can be found in the source code:\n```\n`if (s->time->y != TIMELIB_UNSET && s->time->m != TIMELIB_UNSET &&\n    s->time->d != TIMELIB_UNSET &&\n    !timelib_valid_date( s->time->y, s->time->m, s->time->d)) {\n    add_pbf_warning(s, TIMELIB_WARN_INVALID_DATE, \"The parsed date was invalid\", string, ptr);\n}\n`\n```",
      "question_score": 2,
      "answer_score": 2,
      "created_at": "2022-08-03T15:57:10",
      "url": "https://stackoverflow.com/questions/73222881/datetime-to-string-in-mongodb-where-only-month-and-year-are-in-data"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "pymongo",
      "question_id": 72746090,
      "title": "MongoDB projection - take first element from Array",
      "problem": "My field \"Field1.Field2\" is an array and I want to take only first element of this array, i.e. \"Field1.Field2.0\" and from this I want to take any other given field for example: \"Field1.Field2.0.Field3\". Unfortunately when I use this projection in  PyMongo then it doesn't work (this \"0\" make problems). The projection doesn't throw an error but the field \"0\" doesnt contain any values after projection while it contain the values before projection. What may be the reason?",
      "solution": "Option 1:aggregation/$arrayAt or $first\n```\n`db.collection.aggregate([\n{\n  $addFields: {\n     x: {\n      \"$arrayElemAt\": [\n        \"$x\",\n        0\n       ]\n     }\n   }\n }\n])\n`\n```\nPlayground 1\nOption 2:aggregation/$slice\n```\n`db.collection.aggregate([\n{\n $addFields: {\n  x: {\n    \"$slice\": [\n      \"$x\",\n      1\n    ]\n   }\n  }\n }\n])\n`\n```\nPlayground 2 \nOption 3:find/project/$slice\n```\n`db.collection.find({},\n{\n  x: {\n    $slice: 1\n  }\n})\n`\n```\nPlayground 3",
      "question_score": 2,
      "answer_score": 2,
      "created_at": "2022-06-24T17:10:39",
      "url": "https://stackoverflow.com/questions/72746090/mongodb-projection-take-first-element-from-array"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "pymongo",
      "question_id": 71360852,
      "title": "Pymongo is it possible to insert one with insert_many?",
      "problem": "This may sound like a dumb question but I was working with pymongo and wrote the following function to insert documents and was wondering if the insert_many method would also work for one record inserts, that way I wouldn't need another function in case I was just inserting one record.\nThis is my function:\n```\n`def insert_records(list_of_documents: list, collection):\n    i = collection.insert_many(list_of_documents)\n    print(len(i.inserted_ids), \" documents inserted!\")\n`\n```\nWhen I insert one it throws an error:\n```\n`post1 = {\"_id\":0, \"user_name\":\"Jack\"}\ninsert_records(list(post1), stackoverflow)\n`\n```\nTypeError: document must be an instance of dict, bson.son.SON, bson.raw_bson.RawBSONDocument, or a type that inherits from collections.MutableMapping\nI know I can use insert_one() for this purpose, I was just wondering if it was possible to do everything with insert_many(), as the original insert() method is deprecated. Thanks!",
      "solution": "As your `post1` is a dict when you use `list(post1)` you have a list of keys:\n```\n`>>> list(post1)\n['_id', 'user_name']\n`\n```\nUse instead:\n```\n`>>> [post1]\n[{'_id': 0, 'user_name': 'Jack'}]\n`\n```\nSo:\n```\n`insert_records([post1], stackoverflow)\n`\n```",
      "question_score": 2,
      "answer_score": 2,
      "created_at": "2022-03-05T10:02:40",
      "url": "https://stackoverflow.com/questions/71360852/pymongo-is-it-possible-to-insert-one-with-insert-many"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "pymongo",
      "question_id": 71058086,
      "title": "Pymongo.db.create_collection with validator returns &#39;unknown operator: $jsonSchema&#39;",
      "problem": "I am trying to create a schema for my `mongoDB` database in order to ensure data format in the Database.\nI am using `pymongo` to make sure this happens in my docker container when application starts, if the database is not initialized.\n`self.client = MongoClient(conn_string)\nself.db = self.client[db_name]\n# ...\n# I am looping over in my json and 'stuff' is the collection name\n# and json['stuff'] is the schema, so:\n# collection = 'stuff'\n# schema = json[collection]\n# ... \nself.db.create_collection(collection, validator= { \"validator\": { \"$jsonSchema\": schema} })\n\n`\nThe schema looks like that, and is loaded from a `json` file. (and that work)\n`{\n   \"stuff\": {\n      \"bsonType\": \"object\",\n      \"required\": [ \"sid\", \"name\", \"url\" ],\n      \"properties\": {\n         \"sid\": {\n            \"bsonType\": \"string\",\n            \"pattern\": \"^[a-z_]{3-20}$\",\n            \"description\": \"must be a string 3-20 lowercase chars and is required\"\n         },\n         \"name\": {\n            \"bsonType\": \"string\",\n            \"minLength\": 2,\n            \"maxLength\": 30,\n            \"description\": \"must be a string 2-30 chars and is required\"\n         },\n         \"url\": {\n            // etc.\n         }\n      }\n   }\n}\n`\nBut I  get this error:\n`pymongo.errors.OperationFailure: unknown operator: $jsonSchema, full error: \n  {'ok': 0.0, 'errmsg': 'unknown operator: $jsonSchema', 'code': 2, 'codeName': 'BadValue'}\n`",
      "solution": "So I found the answer, also thanks to @prasad_ comment:\nThe problem was a syntax error here are the bad and good version side to side :\n`# Bad version :\nself.db.create_collection(collection, validator= { \"validator\": { \"$jsonSchema\": schema} })\n# Good version :\nself.db.create_collection(collection, validator= { \"$jsonSchema\": schema})\n`\nSo because the validator is passed as parameter, it shouldn't be named inside itself, only the `$jsonSchema`\nI hope it helps some people, it is hard to find proper documentation about pymongo usage, so let me know.",
      "question_score": 2,
      "answer_score": 2,
      "created_at": "2022-02-10T00:29:15",
      "url": "https://stackoverflow.com/questions/71058086/pymongo-db-create-collection-with-validator-returns-unknown-operator-jsonsche"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "pymongo",
      "question_id": 70536252,
      "title": "Mongodb sorting issue when using facet",
      "problem": "mongodb sorting is not working when using facet and mongo aggregation. results are not sorting after the sort function used. i am converting the utc date fromat to date string.\nalso need to remove the _id for each document\nplease help me on the query\nInput json payload which using\n```\n`{\n    \"_id\": {\n        \"$oid\": \"122434543sdf\"\n    },\n    \"eventName\": \"ev1\",\n    \"channelId\": \"channel1\",\n    \"domain\": \"domain1\",\n    \"lob\": \"lob1\",\n    \"eventCategory\": \"category1\",\n    \"producerCSI\": \"1234\",\n    \"topicName\": \"topic1\",\n    \"dateTime\": \"2021-12-29T20:04:37Z\",\n    \"errorDetailsList\": [{\n        \"errorType\": \"Missing data\",\n        \"count\": {\n            \"$numberLong\": \"1\"\n        }\n    }, {\n        \"errorType\": \"Invalid Data\",\n        \"count\": {\n            \"$numberLong\": \"1\"\n        }\n    }]\n}\n]\n}\n\n`\n```\nMongo query i have written\n```\n`\n db.failureevents.aggregate( [\n{$unwind: { \"path\": \"$errorDetailsList\", \"preserveNullAndEmptyArrays\": true} },\n{$addFields: {errorType: {$arrayElemAt: [{$objectToArray: \"$errorDetailsList\"}, 0]}}},\n{$addFields: {\"errorType\": \"$errorType.v\"}},\n{$lookup : { \"from\": \"errordescription\", \"localField\": \"errorType\", \"foreignField\": \"errorType\", \"as\": \"dataset\" }},\n{$unwind: { \"path\": \"$dataset\", \"preserveNullAndEmptyArrays\": true } },\n {$facet: {\n    \"top\": [\n { \"$group\": {\n        \"_id\": {\n            \"lob\": \"$lob\",\n            \"channel\": \"$channelId\",\n            \"domain\": \"$domain\",\n            \"eventCategory\": \"$eventCategory\",\n            \"prodcuerCSI\": \"$producerCSI\",\n            \"topicName\": \"$topicName\",\n            \"eventName\": \"$eventName\",\n            \"errorType\":\"$dataset.errorType\",\n            \"errorMessage\":\"$dataset.errorMessage\",\n            \"dateTime\":\"$dateTime\",\n            \"date\" : { \"$dateFromString\" : { \"dateString\" : \"$dateTime\"} },\n        },\n        \"errorCount\": {\"$sum\" : \"$errorDetailsList.count\"},\n    }\n}],\n \"rest\": [ {$count: 'count'}]\n}},\n    { \"$project\": { \"_id\" : 0,\"data\": { \"$concatArrays\": [\"$top\", \"$rest\"] }}},\n  { \"$unwind\": \"$data\" },\n  { \"$replaceRoot\": { \"newRoot\": \"$data\" }},\n  {$sort: {\"date\" : -1}},\n   { $skip: 0 },\n { $limit: 100},\n ]);\n`\n```",
      "solution": "A `$group` stage in MongoDB aggregation retains only those fields in the the group specification.\nIn your example pipeline the `$group` stage in the `top` section will return documents that have only 2 top-level fields, `_id` and `errorCount`.\nAt the point `{$sort: {\"date\" : -1}}` is executed, there is no `date` field at the top level, so the sort has not effect.\nSolution: either use `{$sort: {\"_id.date\" : -1}}` or use a projection that moves the fields embedded in `_id` to the top level before sorting.",
      "question_score": 2,
      "answer_score": 2,
      "created_at": "2021-12-30T21:11:04",
      "url": "https://stackoverflow.com/questions/70536252/mongodb-sorting-issue-when-using-facet"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "pymongo",
      "question_id": 68097649,
      "title": "Sort an array by occurances mongodb",
      "problem": "Is it possible to sort an array by occurrences?\nFor Example, given\n```\n`    {\n        \"_id\": {\n            \"$oid\": \"60d20d342c7951852a21s53a\"\n            },\n        \"site\": \"www.xyz.ie\",\n        \"A\": [\"mary\", \"jamie\", \"john\", \"mary\", \"mary\", \"john\"], \n    }\n`\n```\nreturn\n```\n`    {\n        \"_id\": {\n            \"$oid\": \"60d20d342c7951852a21s53a\"\n            },\n        \"site\": \"www.xyz.ie\",\n        \"A\": [\"mary\", \"jamie\", \"john\", \"mary\", \"mary\", \"john\"], \n        \"sorted_A\" : [\"mary\",\"john\",\"jamie\"]\n    }\n`\n```\nI am able to get it most of the way there but I cannot figure out how to join them all back together in an array.\nI have been using an aggregation pipeline\n\nStarting with `$match` to find the site I want\nThen `$unwind` on with path: `\"$A\"`\nNext `$sortByCount` on `\"$A\"`\n???? I can't figure out how to group it all back together.\n\nHere is the pipeline:\n```\n`[\n    {\n        '$match': {\n            'site': 'www.xyz.ie'\n        }\n    }, {\n        '$unwind': {\n            'path': '$A'\n        }\n    }, {\n        '$sortByCount': '$A'\n    }, {\n        ????\n    }\n]\n`\n```",
      "solution": "`$group` nu `_id` and `A`, get first `site` and count total elements\n`$sort` by `count` in descending order\n`$group` by only `_id` and get first `site`, and construct array of `A`\n\n```\n`[\n  { $match: { site: \"www.xyz.ie\" } },\n  { $unwind: \"$A\" },\n  {\n    $group: {\n      _id: { _id: \"$_id\", A: \"$A\" },\n      site: { $first: \"$site\" },\n      count: { $sum: 1 }\n    }\n  },\n  { $sort: { count: -1 } },\n  {\n    $group: {\n      _id: \"$_id._id\",\n      site: { $first: \"$site\" },\n      A: { $push: \"$_id.A\" }\n    }\n  }\n]\n`\n```\nPlayground",
      "question_score": 2,
      "answer_score": 2,
      "created_at": "2021-06-23T12:01:04",
      "url": "https://stackoverflow.com/questions/68097649/sort-an-array-by-occurances-mongodb"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "pymongo",
      "question_id": 77973380,
      "title": "How to validate email uniqueness with pydantic and fastapi",
      "problem": "i'm trying to validate input email from request body and i have build custom field validator at model.\nhere is code of RegisterModel:\n`from pydantic import BaseModel, EmailStr, Field, field_validator\nfrom repository.repository_user import UserRepository\nfrom pprint import pprint \n\nclass RegisterModel(BaseModel):\n   name:str\n   email:EmailStr \n   password:str = Field(..., min_length=7)\n\n   @field_validator('email')\n   @classmethod\n   def email_must_unique(cls, v):\n      repo = UserRepository()\n      result = repo.find({'email': v})\n      pprint(result)\n      return result\n    \n\n`\nthe email_must_unique method will create new instace of UserRepository class and it will call the find method from UserRepository to find specific user based on email. actualy the email_must_unique method not finished yet to validate the email, it just get specific users, but i already facing an error.\nhere is code of UserRepository:\n`from pymongo.database import Database\nfrom fastapi import Depends\nfrom config.db import db_conn\n\nclass UserRepository:\n    def __init__(self, db: Database = Depends(db_conn)):\n        self.repository = db.users   # users is mongo collection\n\n    def find(self, filter: dict):\n        result = self.repository.find_one(filter)\n        return result\n`\nwith this code i'm facing error like this:\n```\n`    self.repository = db.users\n                      ^^^^^^^^\nAttributeError: 'Depends' object has no attribute 'users'\n`\n```\ni have no idea what What caused it.\ncan you give me the solution to solve this error or maybe alternative way to validate email uniqueness ?",
      "solution": "As mentioned in the comments by @Chiheb Nexus, Do not call the `Depends` function directly. Instead, use the `Annotated` dependency which is more graceful.\nFor example, the `config/db.py` file should look like this:\n`# config/db.py\nfrom typing import Annotated\n\nfrom fastapi import Depends\nfrom pymongo import MongoClient\nfrom pymongo.database import Database\n\ndef get_db() -> Database:\n    client = MongoClient(\"mongodb://localhost:27017/\")\n    return client.some_database\n\nDbConn = Annotated[Database, Depends(get_db)]  # \nYou can then use it like this:\n`from config.db import DbConn\n\nclass UserRepository:\n    def __init__(self, db: DbConn):  # \nIn addition, `db` in the `__init__` parameter is annotated via `DbConn` so IDE knows its type.\nValidating the uniqueness of data is a task best left to the database rather than Pydantic. In other words, you should insert data and handle unique errors, `DuplicateKeyError` in pymongo.",
      "question_score": 2,
      "answer_score": 1,
      "created_at": "2024-02-10T15:10:24",
      "url": "https://stackoverflow.com/questions/77973380/how-to-validate-email-uniqueness-with-pydantic-and-fastapi"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "pymongo",
      "question_id": 77518950,
      "title": "How to find the maximum value of a document field with Python and MongoDB?",
      "problem": "I am trying to find a document in a MongoDB instance with the largest value of a particular field.\nHere is the current code I have to do that:\n```\n`document = collection.find().sort('experiment_id', -1).limit(1)\n\nif document is None:\n    # ...\nelse:\n    return document['experiment_id']\n`\n```\nI am using this query format (`find()` followed by `sort()` combined with `limit(1)` because this is supposed to be interpreted by Mongo in a highly optimized way.\nHowever, this does not work because `document` is a cursor type, therefore one cannot call `['experiment_id']` on it.\nWhat I find slightly suprising is that `limit(1)` returns a cursor object and not a single value or `None` type.\nHow should I write a pymongo query which finds the largest value of the field `'experiment_id'` from a collection of documents?\nIt seems obvious that the following will work but it doesn't seem like a particularly good solution.\n```\n`for document in documents:\n    return document['experiment_id']\n`\n```",
      "solution": "Since you want only one result - as a document, rather than a cursor, you could use `find_one()` with a `sort` criteria which behaves like it does as a param to `find()`, and drop the `limit` clause, since it will be ignored anyway.\n\n`find_one(filter=None, *args, **kwargs)`\nReturns a single document, or `None` if no matching document is found.\n\n`document = collection.find_one({}, sort={'experiment_id': -1})\n\nif document is None:\n    # ...\nelse:\n    return document['experiment_id']\n`\nAdditionally, if you want just the 'experiment_id' and none of the rest, you can use a `projection` which would return a document with only the specified fields.\n`document = collection.find_one(\n    {},\n    sort={'experiment_id': -1},\n    projection={'_id': False, 'experiment_id': True}  # only experiment_id\n)\n\nif document:\n    return document['experiment_id']\n# else logic here\n`",
      "question_score": 2,
      "answer_score": 1,
      "created_at": "2023-11-20T22:19:17",
      "url": "https://stackoverflow.com/questions/77518950/how-to-find-the-maximum-value-of-a-document-field-with-python-and-mongodb"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "pymongo",
      "question_id": 76018083,
      "title": "pymongo BSON public APIs not imported in Jupyter Lab",
      "problem": "I'm working on a project that requires me to import a BSON dataset into Pandas. I'm trying to use the `bson.decode_all` method to do so.\nI have a conda environment named \"tf\" with `pymongo` installed. This is my current script in Jupyter that I open using `(tf) PS C:\\Users\\ashka\\Desktop\\spring 23\\RSRC 4033\\cybersecurity tweets\\jupyter> jupyter lab`:\n```\n`import os\nimport pprint\nfrom platform import python_version\n\nimport pandas as pd\nimport bson\nimport tensorflow as tf\n\nprint(python_version())\nprint(bson.__file__)\nprint(bson.__all__)\n`\n```\ngives:\n```\n`3.9.16\nC:\\Users\\ashka\\anaconda3\\envs\\tf\\lib\\site-packages\\bson\\__init__.py\n['loads', 'dumps']\n`\n```\nand\n```\n`# preprocessing\ndata_file = \"../dataset/threat/tweets.bson\"\nwith open(data_file, 'rb') as f:\n    dataset_dict = bson.decode_all(f.read())\npprint.pprint(dataset_dict)\n`\n```\ngives:\n```\n`---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[35], line 4\n      2 data_file = \"../dataset/threat/tweets.bson\"\n      3 with open(data_file, 'rb') as f:\n----> 4     dataset_dict = bson.decode_all(f.read())\n      5 pprint.pprint(dataset_dict)\n\nAttributeError: module 'bson' has no attribute 'decode_all'\n`\n```\nAs you can see, the only attributes of bson that are importing are \"loads\" and \"dumps\". Funnily enough, this doesn't seem to be a problem outside of Jupyter with the SAME environment. I created a new .py file in the same directory and ran it using `(tf) PS C:\\Users\\ashka\\Desktop\\spring 23\\RSRC 4033\\cybersecurity tweets\\jupyter> python .\\bsontest.py`:\n```\n`from platform import python_version\n\nimport bson\n\nprint(python_version())\nprint(bson.__file__)\nprint(bson.__all__)\n`\n```\nand it gave:\n```\n`3.9.16\nC:\\Users\\ashka\\anaconda3\\envs\\tf\\lib\\site-packages\\bson\\__init__.py\n['ALL_UUID_SUBTYPES', 'CSHARP_LEGACY', 'JAVA_LEGACY', 'OLD_UUID_SUBTYPE', 'STANDARD', 'UUID_SUBTYPE', 'Binary', 'UuidRepresentation', 'Code', 'DEFAULT_CODEC_OPTIONS', 'CodecOptions', 'DBRef', 'Decimal128', 'InvalidBSON', 'InvalidDocument', 'InvalidStringData', 'Int64', 'MaxKey', 'MinKey', 'ObjectId', 'Regex', 'RE_TYPE', 'SON', 'Timestamp', 'utc', 'EPOCH_AWARE', 'EPOCH_NAIVE', 'BSONNUM', 'BSONSTR', 'BSONOBJ', 'BSONARR', 'BSONBIN', 'BSONUND', 'BSONOID', 'BSONBOO', 'BSONDAT', 'BSONNUL', 'BSONRGX', 'BSONREF', 'BSONCOD', 'BSONSYM', 'BSONCWS', 'BSONINT', 'BSONTIM', 'BSONLON', 'BSONDEC', 'BSONMIN', 'BSONMAX', 'get_data_and_view', 'gen_list_name', 'encode', 'decode', 'decode_all', 'decode_iter', 'decode_file_iter', 'is_valid', 'BSON', 'has_c', 'DatetimeConversion', 'DatetimeMS']\n`\n```\nwhich contains everything that I need!!!\nWhy does the `bson` module act differently in Jupyter and how can I fix it?",
      "solution": "Although I am not sure what exactly was causing the issue, I managed to fix it by reinstalling all the packages (and making sure to avoid the `bson` package and use `pymongo`) and restarting Jupyter Lab.",
      "question_score": 2,
      "answer_score": 1,
      "created_at": "2023-04-14T20:42:57",
      "url": "https://stackoverflow.com/questions/76018083/pymongo-bson-public-apis-not-imported-in-jupyter-lab"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "pymongo",
      "question_id": 74276820,
      "title": "Is there a function to only extract/return the value of a field with Pymongo?",
      "problem": "I'm using MongoDB and want to work with it in Python which is needed for my project. I wanted to extract only the value of a specific field with Pymongo. In my case, I tried returning the name of a charging station which is saved in a database as a document with the attributes name, standard, location, charging capacity, operator\nI only found a website which solved my problem in Mongosh by just using `db.products.findOne().collectionname`.\nFor a better understanding of my problem, please visit this website which describes my problem pretty good: https://database.guide/how-to-return-just-the-value-in-mongodb/\nSo I naturally tried using this method. But it didn't work for me with Pymongo...\n```\n`chargers = db.chargers \n\nresult = chargers.findOne().name\nprint(result)\n`\n```\nI received this error as a result in the terminal after running the .py file.\n\nSo my question is: Is there a method for Pymongo to return only the value of a field in a document?\nE.g. the name of a product or in my case a charger.",
      "solution": "ok here is something you can give a try:\nchange:\n```\n`result = chargers.findOne().name\n`\n```\nto:\n```\n`result = chargers.findOne({\"name\": \"the value of the document name\"}, {\"name\"})\n`\n```\nchange the \"value of the document name\" to the filter, like you have a document which has a field `{name: \"abc\"}`, so type abc in place of \"the value of the document name\".\nthen do this for clean results:\n```\n`result = str(result[\"name\"])\nprint(result)\n`\n```",
      "question_score": 2,
      "answer_score": 1,
      "created_at": "2022-11-01T14:38:24",
      "url": "https://stackoverflow.com/questions/74276820/is-there-a-function-to-only-extract-return-the-value-of-a-field-with-pymongo"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "pymongo",
      "question_id": 73754453,
      "title": "auth failed - mongodb - pymongo - k8s",
      "problem": "I'm trying to connect into mongodb with pymongo, but i can't import data or create a collection.\nThis is the code to connect into mongo :\n```\n`db_local = pymongo.MongoClient(\"mongodb://mongodb-service.default.svc.cluster.local:27017\", username=\"admin\", password=\"pass\")\n\nmydb_local= db_local[\"test\"]\n`\n```\nAnd then i tried this to create new collection :\n```\n`mydb_local.create_collection(\"HDFS\"]\n`\n```\nWhile trying to add the collection i get this error :\nAuthentification failed., full error: {'ok': 0.0, 'errmsg': Authentification failed.', 'code : 18, 'codeName': AuthentificationFailed'}\nMeanwhile, i deployed Mongodb into Minikube, and this is the mongodb-sercret.yaml file :\n```\n`apiVersion : v1\nkind: Secret \nmetadata: \n    name: mongodb-secret\ntype: Opaque\ndata : \n     mongo-root-username : dXNlcm5hbWU=\n     mongo-root-password : cGFzc3dvcmQ=\n`\n```",
      "solution": "PLease try this :\n```\n`db_local= pymongo.MongoClient(\"mongodb://admin:pass@mongodb-service.default.svc.cluster.local:27017\")\n`\n```",
      "question_score": 2,
      "answer_score": 1,
      "created_at": "2022-09-17T13:26:47",
      "url": "https://stackoverflow.com/questions/73754453/auth-failed-mongodb-pymongo-k8s"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "pymongo",
      "question_id": 73209795,
      "title": "MongoDB Update: push old field value to another array field if new value is different",
      "problem": "I have a simple collection like:\n```\n`{\n    \"_id\": \"62e92e47e0f473e37a491574\",\n    \"id\": 1,\n    \"price\": 999,\n    \"price_changed\": false,\n    \"prices_history\": [] # there cases when this field does not exist\n}\n`\n```\nI want to make an update statement (using pymongo) with `price = 1000`, so the final document looks like\n```\n`{\n    \"_id\": \"62e92e47e0f473e37a491574\",\n    \"id\": 1,\n    \"price\": 1000,\n    \"price_changed\": true,\n    \"prices_history\": [999]\n}\n`\n```\nSo far I can only detect field value change with:\n```\n`ops = [pymongo.UpdateOne(\n        {'id': 1},\n        [\n            {\n                '$set':\n                    {\n                        'id': 1,\n                        'price': 1000,\n                        'price_changed': {'$ne': ['$price', 1000]},\n                    }\n            }\n        ]\n    )]\n    collection.bulk_write(ops)\n`\n```\nBut I cannot understand how to build the pipeline to add the last stage - push old value to the `prices_history` array\nAny suggestions?\nCases:\nInsert happens - should be inserted with prices_history = []\nUpdate happens, values are equal - prices_history = prices_history\nUpdate happens, values are different - prices_history = prices_history + price (old)",
      "solution": "You can use $concatArrays operator to add new element in array, and `$ifNull` operator if the field `prices_history` does not exists,\n```\n`[\n  {\n    '$set': {\n      'price': 1000,\n      'price_changed': { '$ne': ['$price', 1000] },\n      'prices_history': { \n        '$concatArrays': [\n          { \"$ifNull\": [\"$prices_history\", []] }, \n          [\"$price\"]\n        ] \n      }\n    }\n  }\n]\n`\n```\nPlayground",
      "question_score": 2,
      "answer_score": 1,
      "created_at": "2022-08-02T17:05:36",
      "url": "https://stackoverflow.com/questions/73209795/mongodb-update-push-old-field-value-to-another-array-field-if-new-value-is-diff"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "pymongo",
      "question_id": 72705925,
      "title": "ERROR while ingegrating MongoDB with Django on windows",
      "problem": "I am trying to integrate MongoDB and Django. but when i run the code `python manage.py runmigrations app_name` i am getting the error:\n\nFile \"C:\\Users\\Gourav\\Envs\\test\\lib\\site-packages\\django\\db\\utils.py\", line 126, in load_backend\nraise ImproperlyConfigured(\ndjango.core.exceptions.ImproperlyConfigured: 'djongo' isn't an available database backend or couldn't be imported. Check the above exception. To use one of the built-in backends, use 'django.db.backends.XXX', where XXX is one of:\n'mysql', 'oracle', 'postgresql', 'sqlite3'\n\nand `utils.py` line 119 to 136 code looks like;\n```\n`builtin_backends = [\n    name\n    for _, name, ispkg in pkgutil.iter_modules(django.db.backends.__path__)\n    if ispkg and name not in {\"base\", \"dummy\"}\n]\nif backend_name not in [\"django.db.backends.%s\" % b for b in builtin_backends]:\n    backend_reprs = map(repr, sorted(builtin_backends))\n    raise ImproperlyConfigured(\n        \"%r isn't an available database backend or couldn't be \"\n        \"imported. Check the above exception. To use one of the \"\n        \"built-in backends, use 'django.db.backends.XXX', where XXX \"\n        \"is one of:\\n\"\n        \"    %s\" % (backend_name, \", \".join(backend_reprs))\n    ) from e_user\nelse:\n    # If there's some other error, this must be an error in Django\n    raise\n`\n```\nevery required directories is already installed in my virtual environment. `pip list` gives following result;\n```\n`asgiref            3.5.2\nDjango             4.0.5\ndjango-mongoengine 0.5.4\ndjongo             1.3.6\ndnspython          2.2.1\nmongoengine        0.24.1\nPillow             9.1.1\npip                22.1.2\npsycopg2           2.9.3\npymongo            4.1.1\npython-snappy      0.6.1\nsetuptools         62.1.0\nsqlparse           0.2.4\ntzdata             2022.1\nwheel              0.37.1\nwinkerberos        0.9.0\n`\n```\nKindly looking for help to solve this error.",
      "solution": "The problem is with the new version of pymongo (4.0 from 29.11.2021) which is not supported by Djongo 1.3.6. we need to install pymongo 3.12.1.\nfollow these steps;\n```\n`pip uninstall pymongo\npip install pymongo==3.12.3\n`\n```\nIt will save your time. it took me more than 24 hours to figure it out.",
      "question_score": 2,
      "answer_score": 1,
      "created_at": "2022-06-21T21:01:28",
      "url": "https://stackoverflow.com/questions/72705925/error-while-ingegrating-mongodb-with-django-on-windows"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "pymongo",
      "question_id": 72091093,
      "title": "match condition on on foreign collection with lookup in mongoDB",
      "problem": "I have the 3 collections\n```\n`users: [\n{_id: \"aaa\", name: \"John\", department: 1},\n{_id: \"bbb\", name: \"Charles\", department: 1},\n{_id: \"ccc\", name: \"Jessy\", department: 1\"},\n{_id: \"ddd\", name: \"Tim\", department: 2},\n{_id: \"eee\", name: \"Max\", department: 2},\n{_id: \"fff\", name: \"Julia\", department: 2},\n{_id: \"ggg\", name: \"Arnold\", department: 3}\n]\n\ndepartments: [\n{_id: 1, name: \"press\", society: \"times\"},\n{_id: 2, name: \"news\", society: \"times\"},\n{_id: 3, name: \"infos\", society: \"herald\"}\n]\n\nsociety: [\n{name: \"times\", country: \"England\"},\n{name: \"herald\", country: \"USA\"}\n]\n`\n```\nA user work in a department and a department is in a society.\nI wanto to do 2 requests, the first one is to have all users from the society \"times\" and the second, is to have all users from the country \"England\".\nI tried this request for the first one :\n```\n`db.users.aggregate([\n{'$match': {'dept.society': \"times\"}\n{\n            '$lookup': {\n                'from': \"departments\",\n                'localField': \"department\",\n                'foreignField': \"_id\",\n                'as': \"dept\"\n            }\n        }])\n`\n```\nBut because of the condition is on foreign collection (\"departments\"), it seems not working. Only condition on local collection works ('department': 1). How I can do that, and it's the same problem for country request?",
      "solution": "There are many ways to achieve your two queries.  I also see that you want to begin the aggregations with the `users` collection.\nHere's one way to query for:\n\nall users from the society \"times\"\n\n`db.users.aggregate([\n  {\n    \"$lookup\": {\n      \"from\": \"departments\",\n      \"localField\": \"department\",\n      \"foreignField\": \"_id\",\n      \"pipeline\": [\n        {\n          \"$match\": {\n            \"society\": \"times\"  // input society\n          }\n        }\n      ],\n      \"as\": \"deptLookup\"\n    }\n  },\n  { \"$match\": { \"$expr\": { \"$gt\": [ { \"$size\": \"$deptLookup\" }, 0 ] } } },\n  { \"$unset\": \"deptLookup\" }\n])\n`\nTry it on mongoplayground.net.\nHere's one way to query for:\n\nall users from the country \"England\"\n\n`db.users.aggregate([\n  {\n    \"$lookup\": {\n      \"from\": \"departments\",\n      \"localField\": \"department\",\n      \"foreignField\": \"_id\",\n      \"pipeline\": [\n        {\n          \"$lookup\": {\n            \"from\": \"society\",\n            \"localField\": \"society\",\n            \"foreignField\": \"name\",\n            \"pipeline\": [\n              {\n                \"$match\": {\n                  \"country\": \"England\"  // input country\n                }\n              }\n            ],\n            \"as\": \"socLookup\"\n          }\n        },\n        { \"$match\": { \"$expr\": { \"$gt\": [ { \"$size\": \"$socLookup\" }, 0 ] } } }\n      ],\n      \"as\": \"deptSocLookup\"\n    }\n  },\n  { \"$match\": { \"$expr\": { \"$gt\": [ { \"$size\": \"$deptSocLookup\" }, 0 ] } } },\n  { \"$unset\": \"deptSocLookup\" }\n])\n`\nTry it on mongoplayground.net.",
      "question_score": 2,
      "answer_score": 1,
      "created_at": "2022-05-02T20:51:36",
      "url": "https://stackoverflow.com/questions/72091093/match-condition-on-on-foreign-collection-with-lookup-in-mongodb"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "pymongo",
      "question_id": 71137788,
      "title": "PyMongo: &quot;not authorized on db to execute command&quot; but roles and credentials are ok",
      "problem": "I have two users, admin and user.\n```\n`use admin\ndb.getUser('admin')\n{\n  _id: 'admin.admin',\n  userId: UUID(\"xx\"),\n  user: 'admin',\n  db: 'admin',\n  roles: [\n    { role: 'userAdminAnyDatabase', db: 'admin' },\n    { role: 'readWriteAnyDatabase', db: 'admin' }\n  ],\n  mechanisms: [ 'SCRAM-SHA-1', 'SCRAM-SHA-256' ]\n}\nuse mydb\ndb.getUser('user')\n{\n  _id: 'mydb.user',\n  userId: UUID(\"xx\"),\n  user: 'user',\n  db: 'mydb',\n  roles: [ { role: 'readWrite', db: 'mydb' } ],\n  mechanisms: [ 'SCRAM-SHA-1', 'SCRAM-SHA-256' ]\n}\n`\n```\nIf I log in as user using mongosh or MongoDB Compass I can totally do any read or write queries, but if I try anything using a PyMongo cursor:\n```\n`r = list(mongo.db.mycollection.find())\n`\n```\nI get:\n```\n`pymongo.errors.OperationFailure: not authorized on db to execute command { find: \"mycollection\", filter: {}, lsid: { id: UUID(\"xx\") }, $db: \"db\", $readPreference: { mode: \"primaryPreferred\" } }, full error: {'ok': 0.0, 'errmsg': 'not authorized on db to execute command { find: \"config\", filter: {}, lsid: { id: UUID(\"xx\") }, $db: \"db\", $readPreference: { mode: \"primaryPreferred\" } }', 'code': 13, 'codeName': 'Unauthorized'}\n`\n```\nBut if I do the same using admin instead everything is ok.\nMy guesses are that it authenticates but does not get inside the specified database, it probably reaches some other default database instead and that's why it says it has not authorization (despite I specified it in the URI).\nHere is my URI:\n```\n`mongodb://user:passwd@localhost:27017/mydb?authSource=mydb&readPreference=primary&appname=myapp&ssl=false\n`\n```\nAny ideas how to debug my issue?",
      "solution": "I guess python runs some additional queries in background.\nFrom MongoDB documentation:\n\nRoles which are created in other database than `admin` can only include privileges that apply to its database and can only inherit from other roles in its database.\nA role created in the `admin` database can include privileges that apply to any database or to the cluster resource, and can inherit from roles in other databases as well as the `admin` database.\n\nI would suggest to create the user in `admin` database and grant roles accordingly:\n```\n`db.getSiblindDB(\"admin\").createUser( { \n   user: \"user\",\n   pwd: \"\"\n   roles: [ { role: \"readWrite\", db: \"mydb\" }] \n})\n`\n```",
      "question_score": 2,
      "answer_score": 1,
      "created_at": "2022-02-16T08:27:15",
      "url": "https://stackoverflow.com/questions/71137788/pymongo-not-authorized-on-db-to-execute-command-but-roles-and-credentials-are"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "pymongo",
      "question_id": 70829517,
      "title": "storing large data into mongodb collection using python",
      "problem": "I am working on a Django project where I have to create 10 years of data and store them in MongoDB retrieve it and display it on the HTML page. I am trying to divide 10 years of data into 1 year and then store it in MongoDB collection but whenever I try to do so only two documents get stored and\nthis error is shown in pymongo. errors.DocumentTooLarge: BSON document too large (29948865 bytes) - the connected server supports BSON document sizes up to 16793598 bytes.\nmy python code is\n```\n`\nnow=start\nworkdate=now.date()\nnowtime=now.time()\n\nendt=end\nktime=start\n\ntimes=[]\nstates=[]\nlevel=[]\n\n#generating random level of water in the tank \nwhile (now!=endt): # loop for creating data for given time\n    ktime=ktime+relativedelta(months=5)\n    print(current_level)\n    def fill():\n        global df \n        global now\n        global workdate\n        global nowtime\n        global ktime\n        global current_level\n        global flag\n       \n        global times\n        global states \n        global level\n        while x=='on' and current_level50:\n            times.append(now)\n            states.append(x)\n            level.append(current_level)\n            \n            \n           \n            print(current_level)\n            current_level-=emptyrate\n            current_level=round(current_level,4)\n            now=now+timedelta(minutes=1)\n            nowtime=now.time()\n            workdate=now.date()\n            if now==ktime:\n              times.append(now)\n              states.append(x)\n              level.append(current_level)\n                \n              print(\"true\")\n              flag='red'\n              break\n               \n         \n           \n    \n           \n          \n    \n       \n    flag='green'\n    k=True\n    while k:       \n        if  x=='off' and current_level>50:\n            drain()\n            x='on'\n            \n    \n    \n        if flag =='red':\n         break\n    \n    \n    \n    \n        if x=='on' and  current_level<450: \n            fill()\n            x='off'\n            \n       \n            \n          \n            \n        if flag=='red':\n            break\n    \n    \n    \n    data = {'time': times, 'status': states, 'level': level}\n    df = pd.DataFrame(data)\n    \n    \n    df.reset_index(inplace=True)\n    data = df.to_dict('records')\n    colle.insert({\"data\":data}) #transfering data to collection \n    del df\n    data.clear()\n    \n`\n```",
      "solution": "so the problem was with my logic instead of clearing data I should have cleared the already stored data in the times, status and level list after the end of the loop\nso the last part that should be changed is as follows\n`while k:       \n    if  x=='off' and current_level>50:\n        drain()\n        x='on'\n\n    if flag=='red':\n        data = {'time': times, 'status': states, 'level': level}\n        flag='green'\n        break\n\n    if x=='on' and  current_level\nthe outer loop or the main loop that control the period for which the following code runs remain same",
      "question_score": 2,
      "answer_score": 1,
      "created_at": "2022-01-24T06:50:37",
      "url": "https://stackoverflow.com/questions/70829517/storing-large-data-into-mongodb-collection-using-python"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "pymongo",
      "question_id": 69889193,
      "title": "multi-stage aggregation pipeline matching data based on fields retrieved through $lookup",
      "problem": "I'm trying to build a complex, nested aggregation pipeline in MongoDB (4.4.9 Community Edition, using the `pymongo` driver for Python 3.10).\nThere are relevant data points in different collections which I want to aggregate into one, NEW (ideally) view (or, if that doesn't work) collection.\nThe collections, and the relevant fields therein follow a hierarchy. There is `members`, which contains the top-level key on which other data is to be merged,\n`membershipNumber`.\n```\n`> members.find_one()\n{'_id': ObjectId('61153299af6122XXXXXXXXXXXXX'), 'membershipNumber': 'N03XXXXXX'}\n`\n```\nThen, there's a different collection, which contains `membershipNumber`, but also a different, linked field, `an_user_id`. `an_user_id` is used in other collections to denote records/fields in arrays that pertain to that particular user.\nI 'join' `members` and `an_users` like so:\n```\n`result = members.aggregate([\n    {\n        '$lookup': {\n            'from': 'an_users',                   \n            'localField': 'membershipNumber',     \n            'foreignField': 'memref',             \n            'as': 'an_users'                      \n        }\n    },\n    {   '$unwind' : '$an_users' },     \n    {   \n        '$project' : {\n            '_id' : 1,\n            'membershipNumber' : 1,\n            'an_user_id' : '$an_users.user_id'\n        } \n    }\n]);\n`\n```\nSo far so good, this returns the desired, aggregated record:\n```\n`{'_id': ObjectId('61153253aBBBBBBBBBBBB'),\n  'membershipNumber': 'N0XXXXXXXX',\n  'an_user_id': '48XXXXXX'}\n`\n```\nNow, I have a third collection, which contains the `an_user_id` as a string in arrays, denoting wherever that user clicked a given email, whereby a record is an email (and the `an_user_id`s in the `clicks` array are users that clicked a link in that email.\n```\n`{'_id': ObjectId('blah'),\n 'email_id': '407XXX',\n 'actions_count': 17,\n 'administrative_title': 'test',\n 'bounce': ['3440XXXX'],\n 'click': ['38294CCC',\n  '418FFFF',\n  '48XXXXXX',\n  '38eGGGG'}\n`\n```\nI want to count the number occurences of a given `an_user_id` (which I've attained from aggregating) in arrays (e.g. `clicks`, `bounces`, `opens`) in the `emails` collection, and include it in the `.aggregate` call, to retrieve something like this:\n```\n`{'_id': ObjectId('61153253aBBBBBBBBBBBB'),\n  'membershipNumber': 'N0XXXXXXXX',\n  'an_user_id': '48XXXXXX',\n  'n_email_clicks' : 412,\n  'n_email_bounces' : 12\n}\n`\n```\nFurther, I might want to also attach counts of `an_user_id` in other collections in my DB.\nConsider, e.g., this collection called `events`:\n```\n`{\n    \"_id\": \"617ffa96ee11844e143a63dd\",\n    \"id\": \"12345\",\n    \"administrative_title\": \"my_event\",\n    \"created_at\": {\n        \"$date\": \"2020-01-15T16:28:50.000Z\"\n    },\n    \"event_creator_id\": \"123456\",\n    \"event_title\": \"my_event\",\n    \"group_id\": \"123456\",\n    \"permalink\": \"event_id\",\n    \"rsvp_count\": 54,\n    \"rsvps\": [{\n        \"rsvp_id\": \"56789\",\n        \"display_name\": \"John Doe\",\n        \"rsvp_user_id\": \"48XXXXXX\",\n        \"rsvp_created_at\": {\n            \"$date\": \"2020-01-28T15:38:50.000Z\"\n        },\n        \"rsvp_updated_at\": {\n            \"$date\": \"2020-01-28T15:38:50.000Z\"\n        },\n        \"first_name\": \"John\",\n        \"last_name\": \"Doe\",\n    }, {\n        \"rsvp_id\": \"543895\",\n        \"display_name\": \"James Appleslice\",\n        \"rsvp_user_id\": \"N03XXXXXX\",\n        \"rsvp_created_at\": {\n            \"$date\": \"2020-02-05T13:15:14.000Z\"\n        },\n        \"rsvp_updated_at\": {\n            \"$date\": \"2020-02-05T13:15:14.000Z\"\n        },\n        \"first_name\": \"James\",\n        \"last_name\": \"Appleslice\"}\n  ]\n}\n`\n```\nSo, the end-product would look something like this:\n```\n`{'_id': ObjectId('61153253aBBBBBBBBBBBB'),\n  'membershipNumber': 'N0XXXXXXXX',\n  'an_user_id': '48XXXXXX',\n  'n_email_clicks' : 412,\n  'n_email_bounces' : 12,\n  'n_rsvps' : 12\n}\n`\n```\nMy idea was to use the `$lookup` parameter -- however, I only know how to use this for matching on fields that I have in the parent collection that I'm performing the aggregation on, but not on fields that have been generated in the process of the aggregation.\nAny help would be hugely appreciated!",
      "solution": "You could use `$lookup` pipeline. First you would `$lookup` the user id followed by another `$lookup` to verify if the user id exists in email. Lastly few more stages to collect the results and format per your need. Furthermore, you can add `$out` stage if you would like to write the results into another collection.\n```\n`db.members.aggregate([{\n  $lookup: {\n    from: \"an_users\",\n    let: {\n      membershipNumber: \"$membershipNumber\"\n    },\n    pipeline: [\n      {\n        $match: {\n          $expr: {\n            $eq: [\n              \"$memref\",\n              \"$$membershipNumber\"\n            ]\n          },\n          \n        }\n      },\n      {\n        \"$lookup\": {\n          \"from\": \"emails\",\n          \"localField\": \"user_id\",\n          \"foreignField\": \"click\",\n          \"as\": \"clicks\"\n        }\n      },\n      {\n        \"$project\": {\n          \"_id\": 1,\n          \"membershipNumber\": 1,\n          \"an_user_id\": \"$user_id\",\n          \"n_email_clicks\": {\n            $size: \"$clicks\"\n          }\n        }\n      }\n    ],\n    as: \"details\"\n  }\n},\n{\n  $replaceRoot: {\n    newRoot: {\n      $mergeObjects: [\n        {\n          $arrayElemAt: [\n            \"$details\",\n            0\n          ]\n        },\n        \"$$ROOT\"\n      ]\n    }\n  }\n},\n{\n  $project: {\n    details: 0\n  }\n}])\n`\n```\nWorking example - https://mongoplayground.net/p/yrFsNp44hpi",
      "question_score": 2,
      "answer_score": 1,
      "created_at": "2021-11-08T20:53:49",
      "url": "https://stackoverflow.com/questions/69889193/multi-stage-aggregation-pipeline-matching-data-based-on-fields-retrieved-through"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "pymongo",
      "question_id": 69872986,
      "title": "Project a numberDecimal value into a float in Python",
      "problem": "I am new to MongoDB and Pymongo, but have learnt some lessons from their Mongo university. I have a nested document, and I would like to only extract specific values from it. I am currently trying to extract the numeric part of PriceEGC but I wasn't successful. My code to construct the projection and extract specific values is shown below:\n```\n`import os\nimport math\nimport pymongo\nfrom pprint import pprint\nfrom datetime import datetime\nfrom bson.json_util import dumps\nfrom bson.decimal128 import Decimal128\n\n# more code above not shown\nfor collection in all_collections[:1]:\n    first_seen_date = collection.name.split(\"_\")[-1]\n    projection = {\n        (more projections)...,\n        \"RegoExpiryDate\": \"$Vehicle.Registration.Expiry\",\n        \"VIN\": \"$_id\",\n        \"ComplianceDate\": None,\n        \"PriceEGC\": \"$Price.FinalDisplayPrice\",  # A typical document looks like this:\n\nWhat I do with the aggregation is I print out this single document, to see what it looks like first, before loading the entire collection somewhere.\nThe output I don't want is this:\n```\n`[{\n(more key-value pairs)...,\n\"RegoExpiryDate\": \"2021-08-31T00:00:00.000Z\",\n\"VIN\": \"JTMRBREVX0D087618\",\n\"ComplianceDate\": null,\n\"PriceEGC\": {\n  \"$numberDecimal\": \"36268.00\"  # The output I want is this:\n```\n`[{\n(more key-value pairs)...,\n\"RegoExpiryDate\": \"2021-08-31T00:00:00.000Z\",\n\"VIN\": \"JTMRBREVX0D087618\",\n\"ComplianceDate\": null,\n\"PriceEGC\": 36268.00, (or) \"PriceEGC\": \"36268.00\",  # How would I write the projection or the pipeline, so that I get what I want, as shown above? I've tried:\n```\n`projection = {...,\n\"PriceEGC\": \"$Price.FinalDisplayPrice.$numberDecimal\",\n...\n}\n`\n```\nand\n```\n`projection = {...,\n\"PriceEGC\": {\"$toDecimal\": \"$Price.FinalDisplayPrice\"}\n...\n}\n`\n```\nand\n```\n`projection = {...,\n\"PriceEGC\": Decimal128.to_decimal(\"$Price.FinalDisplayPrice\")\n...\n}\n`\n```\nand altering the pipeline as well\n```\n`pipeline = [\n    {\"$match\": {}},\n    {\"$project\": projection},\n    {\"$toDecimal\": \"$Price.FinalDisplayPrice\"},\n    {\"$skip\": batch_size * num},\n    {\"$limit\": batch_size},\n]\n`\n```",
      "solution": "What you are seeing is the `bson.json_util()` representation of a MongoDB Decimal128 object. For more information on this data type, see https://www.mongodb.com/developer/quickstart/bson-data-types-decimal128/\nThe `bson.json_util()` function provides the `$numberDecimal` wrapper so that the data type is preserved should you wish later to load the data back in.\nIf you want a different behaviour then you might want to use the regular `json.dumps()` and override the Decimal128 behaviour, e.g.\n```\n`from pymongo import MongoClient\nfrom bson.decimal128 import Decimal128\nimport json\nimport bson\n\nclass CustomJsonEncoder(json.JSONEncoder):\n    def default(self, obj):\n        if isinstance(obj, bson.decimal128.Decimal128):\n            return float(obj.to_decimal())\n\ndb = MongoClient()['mydatabase']\ncollection = db['mycollection']\n\ncollection.insert_one({'Price': {'FinalDisplayPrice': Decimal128('36268.00')}})\nprint(json.dumps(list(collection.find({}, {'_id': 0})), indent=4, cls=CustomJsonEncoder))\n`\n```\nprints:\n```\n`[\n    {\n        \"Price\": {\n            \"FinalDisplayPrice\": 36268.0\n        }\n    }\n]\n`\n```",
      "question_score": 2,
      "answer_score": 1,
      "created_at": "2021-11-07T14:45:16",
      "url": "https://stackoverflow.com/questions/69872986/project-a-numberdecimal-value-into-a-float-in-python"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "pymongo",
      "question_id": 68895307,
      "title": "How to listen to change in specific field of a nested array in mongodb change streams?",
      "problem": "This is the structure of BSON document I have in mongodb.\n```\n`{\n    \"tournament_id\": \"P1oi12mwj10b1b\",\n    \"matches\": [\n        {\n            \"date_order\": 1,\n            \"matches\": [\n                {\n                    \"match_id\": \"1A4i0sp34\"\n                    \"time_order\": 1,\n                    \"win\": \"team1\",\n                    \"team1\": \"bar\",\n                    \"team2\": \"psg\"\n                },\n                {\n                    \"match_id\": \"3A4j0sp26\"\n                    \"time_order\": 2,\n                    \"win\": \"\",\n                    \"team1\": \"rma\",\n                    \"team2\": \"sev\"\n                }\n            ]\n        },\n        {\n            \"date_order\": 2,\n            \"matches\": [\n                {\n                    \"match_id\": \"2B4k0sp29\"\n                    \"time_order\": 1,\n                    \"win\": \"\",\n                    \"team1\": \"manU\",\n                    \"team2\": \"manC\"\n                },\n                {\n                    \"match_id\": \"4A4i0sp31\"\n                    \"time_order\": 2,\n                    \"win\": \"\",\n                    \"team1\": \"chelsea\",\n                    \"team2\": \"arsenal\"\n                }\n            ]\n        }\n    ]\n}\n`\n```\nI want to make a notification system which sends notification whenever a match completes. In other words, whenever the value of `win` field changes, I want to catch which match was updated. I'm using mongodb change streams.\nFor example, if match with match_id 3A4j0sp26 just completed, I want to print that object.\n```\n`{\n      \"match_id\": \"3A4j0sp26\"\n      \"time_order\": 2,\n      \"win\": \"team2\",\n      \"team1\": \"rma\",\n      \"team2\": \"sev\"\n      # If possible I also want to find these fields,\n      \"tournament_id\": \"P1oi12mwj10b1b\",\n      \"date_order\": 1\n}\n`\n```\nI tried doing this.\n`import pymongo\nfrom bson.json_util import dumps\n\nMONGO_URI = 'mongodb://localhost/mydb'\nclient = pymongo.MongoClient(MONGO_URI)\n\nfilters = []  # How to correctly set this filter ???\n'''\nWhat I already tried but failed\nfilters = [{\n        '$match': {\n            '$and': [\n                {'updateDescription.updatedFields.matches': {'$exists': 'true'}},  # This line needs fixing.\n                {'operationType': {'$in': ['replace', 'update']}}\n            ]\n        }\n    }]\n'''\n\nchange_stream = client.mydb.match.watch(filters)\nfor change in change_streams:\n    print(dumps(change))\n`\nI tried to debug without applying filters. I updated `win` field of `match_id` 3A4j0sp26 to team2.\nI get this as result.\n```\n`{\n  \"_id\": {\n    \"_data\": \"8261252C2F000000012B022C0100296E5A1004D4D1F2A9AF33491089DE8C2A51537EBB46645F6964006461228AE88CF6743D054B8CEF0004\"\n  },\n  \"operationType\": \"replace\",\n  \"clusterTime\": {\n    \"$timestamp\": {\n      \"t\": 1629826095,\n      \"i\": 1\n    }\n  },\n  \"fullDocument\": {\n    \"_id\": {\n      \"$oid\": \"61228ae88cf6743d054b8cef\"\n    },\n    \"tournament_id\": \"P1oi12mwj10b1b\",\n    \"matches\": [\n        {\n            \"date_order\": 1,\n            \"matches\": [\n                {\n                    \"match_id\": \"1A4i0sp34\"\n                    \"time_order\": 1,\n                    \"win\": \"team1\",    # This was updated earlier. I don't want this.\n                    \"team1\": \"bar\",\n                    \"team2\": \"psg\"\n                },\n                {\n                    \"match_id\": \"3A4j0sp26\"\n                    \"time_order\": 2,\n                    \"win\": \"team1\",     # This is the most recently updated.\n                    \"team1\": \"rma\",\n                    \"team2\": \"sev\"\n                }\n            ]\n        }]\n   }\n}\n`\n```\nIt shows all elements from the array and not the one which was just updated.\nEdited\nResult I got after updating \"score\" field only.\n```\n`{\n  \"_id\": {\n    \"_data\": \"8261254598000000022B022C0100296E5A1004D4D1F2A9AF33491089DE8C2A51537EBB46645F6964006461228AE88CF6743D054B8CEF0004\"\n  },\n  \"operationType\": \"update\",\n  \"clusterTime\": {\n    \"$timestamp\": {\n      \"t\": 1629832600,\n      \"i\": 2\n    }\n  },\n  \"ns\": {\n    \"db\": \"mydb\",\n    \"coll\": \"match\"\n  },\n  \"documentKey\": {\n    \"_id\": {\n      \"$oid\": \"61228ae88cf6743d054b8cef\"\n    }\n  },\n  \"updateDescription\": {\n    \"updatedFields\": {\n       \"matches\": [\n        {\n            \"date_order\": 1,\n            \"matches\": [\n                {\n                    \"match_id\": \"1A4i0sp34\"\n                    \"time_order\": 1,\n                    \"win\": \"team1\",\n                    \"team1\": \"bar\",\n                    \"team2\": \"psg\"\n                },\n                {\n                    \"match_id\": \"3A4j0sp26\"\n                    \"time_order\": 2,\n                    \"win\": \"team1\",\n                    \"team1\": \"rma\",\n                    \"team2\": \"sev\"\n                }\n            ]\n        }]\n   },\n     \"removedFields\": []\n    }\n  }\n}\n`\n```",
      "solution": "It depends on how you do the update.\nA short test to demonstrate:\nInsert a document and start a change stream\n```\n`PRIMARY> db.updtest.insert({list:[\n                         {item:\"1\",state:\"running\"},\n                         {item:\"2\",state:\"done\"},\n                         {item:\"3\",state:\"unknown\"}\n                  ]});\n\nWriteResult({ \"nInserted\" : 1 })\n\nPRIMARY> let stream = db.updtest.watch()\n`\n```\nUpdating by setting the list field results in a change event that returns the entire array:\n```\n`PRIMARY> db.updtest.updateOne({},{$set:{list:[\n                         {item:\"1\",state:\"running\"},\n                         {item:\"2\",state:\"done\"},\n                         {item:\"3\",state:\"running\"}\n                  ]}});\n\n{ \"acknowledged\" : true, \"matchedCount\" : 1, \"modifiedCount\" : 1 }\n\nPRIMARY> stream.next();\n\n{\n    \"_id\" : {\n        \"_data\" : \"82612577BE000000012B022C0100296E5A100436BFE3F91AF84C7CB04826F361BCE50346645F696400646125779D98787C286C5443050004\"\n    },\n    \"operationType\" : \"update\",\n    \"clusterTime\" : Timestamp(1629845438, 1),\n    \"ns\" : {\n        \"db\" : \"test\",\n        \"coll\" : \"updtest\"\n    },\n    \"documentKey\" : {\n        \"_id\" : ObjectId(\"6125779d98787c286c544305\")\n    },\n    \"updateDescription\" : {\n        \"updatedFields\" : {\n            \"list\" : [\n                {\n                    \"item\" : \"1\",\n                    \"state\" : \"running\"\n                },\n                {\n                    \"item\" : \"2\",\n                    \"state\" : \"done\"\n                },\n                {\n                    \"item\" : \"3\",\n                    \"state\" : \"running\"\n                }\n            ]\n        },\n        \"removedFields\" : [ ]\n    }\n}\n`\n```\nUpdating just one field in one subdocument results in a change event that include only the modified field:\n```\n`PRIMARY> db.updtest.update({\"list.item\":\"3\"},{$set:{\"list.$.state\":\"done\"}});\n\nWriteResult({ \"nMatched\" : 1, \"nUpserted\" : 0, \"nModified\" : 1 })\n\nPRIMARY> stream.next();\n\n{\n    \"_id\" : {\n        \"_data\" : \"8261257879000000012B022C0100296E5A100436BFE3F91AF84C7CB04826F361BCE50346645F696400646125779D98787C286C5443050004\"\n    },\n    \"operationType\" : \"update\",\n    \"clusterTime\" : Timestamp(1629845625, 1),\n    \"ns\" : {\n        \"db\" : \"test\",\n        \"coll\" : \"updtest\"\n    },\n    \"documentKey\" : {\n        \"_id\" : ObjectId(\"6125779d98787c286c544305\")\n    },\n    \"updateDescription\" : {\n        \"updatedFields\" : {\n            \"list.2.state\" : \"done\"\n        },\n        \"removedFields\" : [ ]\n    }\n}\n`\n```\nIf you also use the change stream option to return the full documents, you will have the context around the field that was changed.",
      "question_score": 2,
      "answer_score": 1,
      "created_at": "2021-08-23T17:32:02",
      "url": "https://stackoverflow.com/questions/68895307/how-to-listen-to-change-in-specific-field-of-a-nested-array-in-mongodb-change-st"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "pymongo",
      "question_id": 67152832,
      "title": "MongoDB return distinct values based on condition",
      "problem": "I am trying to use MongoDb to return distinct values for `details.sub_ap_name` when searching `details.ap_name` is equal to `EVPN` however my search is returning all the unique values for `details.sub_ap_name`\nHere is what my current query looks like. I have also tried some aggregate searches as well but those are also not working for me as well. Can someonle let me know what I might be doing wrong?\n`db['test-data'].distinct(\"details.sub_ap_name\",{\"details.ap_name\": \"EVPN\"})`\nhere is my data set\n```\n`{ \n    \"_id\" : ObjectId(\"101\"), \n    \"details\" : [\n        {\n            \"run_date\" : \"2021-04-18\",\n            \"ap_name\" : \"EVPN\",\n            \"sub_ap_name\" : \"EVPN TOR\"\n            \n           \n        },\n        {\n              \"run_date\" : \"2021-04-18\",\n              \"ap_name\" : \"EVPN\",\n              \"sub_ap_name\" : \"EVPN Mobility\"\n             \n             \n        },\n        {\n            \"run_date\" : \"2021-04-17\",\n            \"ap_name\" : \"Multicast\",\n            \"sub_ap_name\" : \"Multicast Layer 2\"\n\n        }\n    ]\n}\n`\n```",
      "solution": "Demo - https://mongoplayground.net/p/49vHHUNYn8K\nNote- Add index on `details.ap_name` for performance\n$unwind\n\nDeconstructs an array field from the input documents to output a document for each element. Each output document is the input document with the value of the array field replaced by the element.\n\n$addToSet\n\nReturns an array of all unique values that results from applying an expression to each document in a group of documents that share the same group by key. The order of the elements in the output array is unspecified.\n\n$group\n\nGroups input documents by the specified _id expression and for each distinct grouping outputs a document. The _id field of each output document contains the unique group by value. The output documents can also contain computed fields that hold the values of some accumulator expression.\n\n```\n`db.collection.aggregate([\n  { $match: { \"details.ap_name\": \"EVPN\" } }, // filter to reduce load on unwind\n  { $unwind: \"$details\" }, // break into individual documents\n  { $match: { \"details.ap_name\": \"EVPN\" } }, // filter\n  {\n    $group: { _id: \"$details.ap_name\", \"sub_ap_names\": { $addToSet: \"$details.sub_ap_name\" } } // add unique values\n  }\n])\n`\n```",
      "question_score": 2,
      "answer_score": 1,
      "created_at": "2021-04-18T21:55:19",
      "url": "https://stackoverflow.com/questions/67152832/mongodb-return-distinct-values-based-on-condition"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "pymongo",
      "question_id": 67070896,
      "title": "Updating a nested record in mongodb array when you don&#39;t know the document index",
      "problem": "I want to update the record of a nested document, my document looks like this:\n`[\n  {\n    \"_id\": \"60753fd9b249ad0dfa1eeb48\",\n    \"name\": \"Random Name 1\",\n    \"email\": \"randomname1@zmel.kom\",\n    \"likings\": [\n      {\n        \"breakfast\": {\n          \"eat\": \"oats\",\n          \"drink\": \"milk\"\n        }\n      },\n      {\n        \"lunch\": {\n          \"eat\": \"beef\",\n          \"drink\": \"pepsi\"\n        }\n      },\n      {\n        \"dinner\": {\n          \"eat\": \"steak\",\n          \"drink\": \"champagne\"\n        }\n      }\n    ]\n  },\n  {\n    \"_id\": \"60753fd9b249ad0dfa1eeb58\",\n    \"name\": \"Random Name 2\",\n    \"email\": \"randomname2@zmel.kom\",\n    \"likings\": [\n      {\n        \"breakfast\": {\n          \"eat\": \"cereals\",\n          \"drink\": \"coffee\"\n        }\n      },\n      {\n        \"lunch\": {\n          \"eat\": \"salad\",\n          \"drink\": \"hot-water\"\n        }\n      },\n      {\n        \"dinner\": {\n          \"eat\": \"biryani\",\n          \"drink\": \"apple juice\"\n        }\n      }\n    ]\n  }\n]\n`\nNow I want to update the value of `drink` for `dinner` for `Random Name 2` but I don't know the index of dinner, it could be above the `lunch`, it could be just below the `breakfast`.\nHere's what I have tried in Python :\n`oid = data[0]                # fetched from flask form\nto_be_updated = data[1]      # fetched from flask form\nupdate_value = data[2]       # fetched from flask form\ncondition = {\"_id\" : oid}\nupdate_value = {\n    \"$set\" : {\n        f\"likings.{to_be_updated}.drink\" : update_value\n    }\n}\nresponse = mongo.db.food.update(condition, update_value)\n`\nbut the error I am getting is:\n`pymongo.errors.WriteError: Cannot create field 'dinner' in element `\nWhat my other strategy that I am planning on using is, only match ID and then update `likings` by keeping the values that are no needed to be changed as it is and altering that value that i want to change. But this approach seems too obvious and semantically wrong since I am using `not updating but updating` kind of strategy i.e disturbing the collection schema for no actual reason. Is there a way to do that or should I just continue with what I am thinking",
      "solution": "DEMO: MongoDB Playground\nFirst of all, there are errors in your JSON.\nJSON\n```\n`[\n  {\n    \"_id\": \"60753fd9b249ad0dfa1eeb48\",\n    \"name\": \"Random Name 1\",\n    \"email\": \"randomname1@zmel.kom\",\n    \"likings\": [\n      {\n        \"breakfast\": {\n          \"eat\": \"oats\",\n          \"drink\": \"milk\"\n        }\n      },\n      {\n        \"lunch\": {\n          \"eat\": \"beef\",\n          \"drink\": \"pepsi\"\n        }\n      },\n      {\n        \"dinner\": {\n          \"eat\": \"steak\",\n          \"drink\": \"champagne\"\n        }\n      }\n    ]\n  },\n  {\n    \"_id\": \"60753fd9b249ad0dfa1eeb58\",\n    \"name\": \"Random Name 2\",\n    \"email\": \"randomname2@zmel.kom\",\n    \"likings\": [\n      {\n        \"breakfast\": {\n          \"eat\": \"cereals\",\n          \"drink\": \"coffee\"\n        }\n      },\n      {\n        \"lunch\": {\n          \"eat\": \"salad\",\n          \"drink\": \"hot-water\"\n        }\n      },\n      {\n        \"dinner\": {\n          \"eat\": \"biryani\",\n          \"drink\": \"apple juice\"\n        }\n      }\n    ]\n  }\n]\n`\n```\nTry this:\n```\n`db.collection.update({\n  \"name\": \"Random Name 2\",\n  \"likings.dinner\": {\n    \"$exists\": true\n  }\n},\n{\n  \"$set\": {\n    \"likings.$.dinner.drink\": \"PEPSI\"\n  }\n})\n`\n```\nYou can change `dinner` to whatever field you want to update accordingly.",
      "question_score": 2,
      "answer_score": 2,
      "created_at": "2021-04-13T10:01:34",
      "url": "https://stackoverflow.com/questions/67070896/updating-a-nested-record-in-mongodb-array-when-you-dont-know-the-document-index"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "pymongo",
      "question_id": 65823199,
      "title": "Dynamic mongo query with python",
      "problem": "I have a mongo aggregation query that i want to create on the basis of some json data, So the query will be like dynamic and it will grow as per json selection array.\n```\n`{ \n RuleResultSUmmaryByCategoryRequest:{\n      \"topLevelFilterIds\": [],\n      \"category\": \"abc\",\n      \"selection\": [{\n        \"value\": \"value\",\n        \"category\": \"category\"\n      }]\n    }\n}\n`\n```\nmy mongo aggregate query is like\n```\n`db.rule_execution_result.aggregate([{\n   $match: {\n       $and: [{\n               'topLevelFilter.id': {\n                   $in: ['5fd1bd7868d7ac4e211a7642']\n               }\n           },\n           {\n               'ruleCategory': 'test'\n           }\n       ]\n   }\n},\n{\n   $group: {\n       _id: {\n           ruleCategory: '$ruleCategory',\n           topLevelFilter: '$topLevelFilter.id'\n       },\n       name: {\n           $first: '$topLevelFilter.name'\n       },\n       type: {\n           $first: '$topLevelFilter.type'\n       },\n       fail: {\n           $sum: {\n               $cond: [{\n                   $eq: ['$summaryStatus', 0]\n               }, 1, 0]\n           }\n       }\n   }\n},\n{\n   $group: {\n       _id: '$_id.ruleCategory',\n       fail: {\n           $sum: '$fail'\n       },\n       portfolio: {\n           $push: {\n               id: '$_id.topLevelFilter',\n               name: '$name',\n               type: '$type',\n               fail: '$fail'\n           }\n       }\n   }\n},\n{\n   $group: {\n       _id: 0,\n       result: {\n           $push: {\n               category: '$_id',\n               fail: '$fail',\n               portfolio: '$portfolio'\n           }\n       },\n       fail: {\n           $sum: '$fail'\n       }\n   }\n},\n{\n   $project: {\n       _id: 0,\n       data: '$result',\n       total: {\n           fail: '$fail'\n       }\n   }\n}\n]).pretty()\n`\n```\nHere i need to use loop on json selection request object that i am getting from API and genrate dynamic stages for match and group, I am new in python so can any one help me to write it in dynamic way in python.",
      "solution": "I am done with dynamic query and now i am able to create a dynamic mongo aggregate query.\n```\n`pipeline = []\n\nmatches = {}\nmatches[\"$and\"]=[]\nmatches[\"$and\"].append({ \"topLevelFilter.id\": { \"$in\": request_data[\"topLevelFilterIds\"]}})\nfor x in request_data['Filter']:\n  matches[\"$and\"].append({x['category']: x['value']})\npipeline.append({'$match': matches})\n  \nfirst_stage_grouping = {}\nfirst_stage_grouping = {'_id': {'groupByColumn': '$' + request_data['category'],\n                                'topLevelFilter': '$topLevelFilter.id'}}\nfirst_stage_grouping['name'] = {'$first': '$topLevelFilter.name'}\nfirst_stage_grouping['type'] = {'$first': '$topLevelFilter.type'}\nfirst_stage_grouping['fail'] = {\n    '$sum': {'$cond': [{'$eq': ['$summaryStatus', 0]}, 1, 0]}}\nfirst_stage_grouping['pass'] = {\n    '$sum': {'$cond': [{'$eq': ['$summaryStatus', 1]}, 1, 0]}}\nfirst_stage_grouping['warn'] = {\n    '$sum': {'$cond': [{'$eq': ['$summaryStatus', 2]}, 1, 0]}}\npipeline.append({'$group': first_stage_grouping})\n\nsecond_stage_grouping = {}\nsecond_stage_grouping = {'_id': '$_id.groupByColumn'}\nsecond_stage_grouping['fail'] = {'$sum': '$fail'}\nsecond_stage_grouping['pass'] = {'$sum': '$pass'}\nsecond_stage_grouping['warn'] = {'$sum': '$warn'}\nsecond_stage_grouping['portfolio'] = {'$push': {'id': '$_id.topLevelFilter',\n                                                'name': '$name', 'type': '$type', 'fail': '$fail', 'pass': '$pass', 'warn': '$warn'}}\npipeline.append({'$group': second_stage_grouping})\n\nthird_stage_grouping = {}\nthird_stage_grouping = {'_id': 0}\nthird_stage_grouping['result'] = {'$push': {\n    'category': '$_id', 'fail': '$fail', 'pass': '$pass', 'warn': '$warn', 'portfolio': '$portfolio'}}\nthird_stage_grouping['fail'] = {'$sum': '$fail'}\nthird_stage_grouping['pass'] = {'$sum': '$pass'}\nthird_stage_grouping['warn'] = {'$sum': '$warn'}\npipeline.append({'$group': third_stage_grouping})\n\nprojection = {}\nprojection = {'_id': 0, 'data': '$result', 'total': {\n    'fail': '$fail', 'pass': '$pass', 'warn': '$warn'}}\npipeline.append({'$project': projection})\nprint(pipeline)\n`\n```\nIf anyone can help me to optimize or any better way to achive the same, it will be great help.",
      "question_score": 2,
      "answer_score": 1,
      "created_at": "2021-01-21T08:58:20",
      "url": "https://stackoverflow.com/questions/65823199/dynamic-mongo-query-with-python"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "pymongo",
      "question_id": 65970988,
      "title": "Python mongodb/motor &quot;&#39;ObjectId&#39; object is not iterable&quot; error while trying to find item in collection",
      "problem": "I know that there are similar questions, but I've tried everything that was advised and still getting an error. I'm trying to fetch item from mongo collection by id, converting string to an ObjectId, like that:\n```\n`from bson import ObjectId\n\nasync def get_single_template(db, template_id):\n    template = await db.templates.find_one({ '_id': ObjectId(template_id) })\n    return template\n`\n```\nAnd I'm getting an error:\n```\n`ValueError: [TypeError(\"'ObjectId' object is not iterable\"), TypeError('vars() argument must have __dict__ attribute')]\n`\n```\n\"template_id\" is a valid string, like \"601401887ecf2f6153bbaaad\". ObjectId created from it - too. It fails only to work inside `find_one()` method. When I'm using `find()` with that id it works well. I've tried `from bson.objectid import ObjectId` too - no difference. I'm using `motor` library to access mongo. Is there something that I'm missing?\nP.S. Links to the corresponding docs:\nhttps://pymongo.readthedocs.io/en/stable/tutorial.html#querying-by-objectid\nThough I'm using `motor` async library, I can't find direct examples in it's docs. Basically, it wraps `pymongo`. I can only find examples in other's source code.",
      "solution": "Well, I've found out what caused that issue. The problem was not in the way I've tried to query data, but in the way I've tried to return it. I've forgotten to convert ObjectId to string in the entity that I've retrieved from database and tried to return it 'as is'. My bad.",
      "question_score": 1,
      "answer_score": 3,
      "created_at": "2021-01-30T18:24:07",
      "url": "https://stackoverflow.com/questions/65970988/python-mongodb-motor-objectid-object-is-not-iterable-error-while-trying-to-f"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "pymongo",
      "question_id": 77668532,
      "title": "Failed when trying to create, get data from the database (pymongo, mongodb, docker-compose)",
      "problem": "I'm having the following error when I try to execute a get or post in my flask app.\n\nmongo:27017: [Errno -2] Name does not resolve (configured timeouts:\nsocketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms), Timeout:\n30s, Topology Description: ]>\n\nmy app.py\n```\n`from flask import Flask, jsonify\nfrom pymongo import MongoClient\nfrom loguru import logger\nfrom bson.json_util import dumps\n\napp = Flask(__name__)\n\nMONGO_URI = \"mongodb://admin:admin@mongo:27017/\"\n\n@app.route('/', methods=['GET'])\ndef get_data():\n    try:\n        logger.info('Connecting to MongoDB')\n        client = MongoClient(MONGO_URI)\n        logger.info('Connected to MongoDB')\n        db = client.test\n        collection = db.test\n        data = collection.find({})\n        serialized_data = dumps(data)\n        logger.info('Data retrieved successfully')\n        return jsonify({\"data\": serialized_data}), 200\n    except Exception as e:\n        return jsonify({\"error\": f\"{str(e)}\"}), 500\n\n@app.route('/', methods=['POST'])\ndef create_data():\n    try:\n        logger.info('Connecting to MongoDB')\n        client = MongoClient(MONGO_URI)\n        logger.info('Connected to MongoDB')\n        db = client.test\n        collection = db.test\n        data = collection.insert_one({\"name\": \"test\"})\n        logger.info('Data created successfully')\n        return jsonify({\"data\": data}), 200\n    except Exception as e:\n        return jsonify({\"error\": f\"{str(e)}\"}), 500\n\nif __name__ == '__main__':\n    app.run(host='0.0.0.0')\n`\n```\ndocker-compose\n```\n`version: '3.9'\n\nservices:\n  web:\n    build:\n      context: .\n      dockerfile: Dockerfile\n    ports:\n      - \"5000:5000\"\n    depends_on:\n      - mongodb\n      - nginx\n\n  nginx:\n    image: nginx:latest\n    ports:\n      - \"80:80\"\n    volumes:\n      - ./nginx.conf:/etc/nginx/nginx.conf:ro\n\n  mongodb:\n    image: mongo:latest\n    ports:\n      - \"27017:27017\"\n    environment:\n      - MONGO_INITDB_DATABASE=test\n      - MONGO_INITDB_ROOT_USERNAME=admin\n      - MONGO_INITDB_ROOT_PASSWORD=admin\n      - MONGO_URI=mongodb://admin:admin@mongodb:27017/test\n    volumes:\n      - mongodb_data:/data/db\n\nvolumes:\n  mongodb_data:\n`\n```\nThe error saying that there is a connection error doesn't make much sense to me, because the logs that are being made show that the request passes through the connection line.\nDoes anyone know how to solve it?",
      "solution": "Docker containers are referenced by their service name; change your connection string to:\n```\n`MONGO_URI = \"mongodb://admin:admin@mongodb:27017/\"\n`\n```\nNote that creating the `MongoClient` object doesn't actually attempt a database connection, so you can't assume you're connected just because it doesn't fail at that point.",
      "question_score": 1,
      "answer_score": 3,
      "created_at": "2023-12-15T20:48:17",
      "url": "https://stackoverflow.com/questions/77668532/failed-when-trying-to-create-get-data-from-the-database-pymongo-mongodb-dock"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "pymongo",
      "question_id": 70427594,
      "title": "Problem initiating ReplicaSet node using pymongo",
      "problem": "I'm trying to write a python script which goal is to initiate a MongoDB replicaset node in my development environment (note: I have just one single node) without having to use mongo shell. So the script is intended to be manually run once.\nI run MongoDB using docker and this is part of my docker-compose file:\n```\n`mongodb:\n    image: \"mongo:5.0-focal\"\n    ports:\n      - \"27017:27017\"\n    volumes:\n      - mongodb:/data/db    \n    entrypoint:\n      [\n        \"/usr/bin/mongod\",\n        \"--bind_ip_all\",\n        \"--replSet\",\n        \"rs0\" \n      ]\n`\n```\nI run the following python code in another docker container belonging to the same network so that it has visibility on the host name mongodb. My pymongo version is 4.0.1\n```\n`import os\nfrom pymongo import MongoClient\n\ntry:\n    client = MongoClient('mongodb',27017,serverSelectionTimeoutMS=5000)\n    config = { '_id': 'rs0', 'members': [{'_id': 0, 'host': 'mongodb:27017'}]}\n    print(\"REPLICA SET config:\")\n    print(config)\n    out = client.admin.command(\"replSetInitiate\", config)\n    print(out)\nexcept Exception as e:\n    print(\"Error!\")\n    print(e)\n    pass\n\n`\n```\nrunning the script I've got this error:\n```\n`No servers match selector \"Primary()\", Timeout: 5.0s, Topology Description: ]>\n`\n```\nOk, no server matches Primary selector, but how can I have a Primary if I don't initiate a replicaSet first?  Not an expert in pymongo and MongoDB but am I missing something here?",
      "solution": "I was facing similar issue and came across https://jira.mongodb.org/browse/PYTHON-3027. Passing 'directConnection=True' as a parameter to MongoClient seemed to help. Although, am not sure about how this works.\n```\n`MongoClient('mongodb://localhost:27017', directConnection=True)\n`\n```",
      "question_score": 1,
      "answer_score": 4,
      "created_at": "2021-12-20T21:22:24",
      "url": "https://stackoverflow.com/questions/70427594/problem-initiating-replicaset-node-using-pymongo"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "pymongo",
      "question_id": 67579727,
      "title": "Mongo messy DOB string field to Age",
      "problem": "I have a collection with documents like:\n```\n`{'state': 'NY', 'DOB': '2000-01-02'},\n{'state': 'NY', 'DOB': '2002/03/04'},\n{'state': 'NY', 'DOB': '00-00-00'},\n{'state': 'NY', 'DOB': 'male'},\n...\n`\n```\nI want outputs like:\n```\n`{'state': 'NY', 'DOB': '2000-01-02', 'Age': 21},\n{'state': 'NY', 'DOB': '2002/03/04', 'Age': 19},\n{'state': 'NY', 'DOB': '00-00-00', 'Age': None}, # or Mongo None equivalent\n{'state': 'NY', 'DOB': 'male', 'Age': None}, # or Mongo None equivalent\n...\n`\n```\nI'm constructing aggregation queries in PyMongo, and I'm wondering if there's an aggregate way to try to convert a field to Mongo Date object and then extract `Age` from it, else (if a date cannot be extracted), return None. Some condition in the shell below?\n```\n`def map_age(state, city)\n    db.aggregate([\n        {'$match': {\n             'state': state,\n             'DOB': {\"$exists\": True}, \n             'Age': {\"$exists\": False}\n        }},\n        {...}     \n    ])\n`\n```",
      "solution": "You can try,\n\n`$let` to create variable for dob convert and do operation\n`$dateFromString` to convert in to date from string if its in valid then replace with \"None\"\n`$subtract` minus converted date from current date `$$NOW` you can use `new Date()` as well\n`$divide` above subtract date by \"31536000000\" means \"3652460601000\"\n`$round` to round age number\n\n```\n`db.aggregate([\n  {\n    $set: {\n      Age: {\n        $let: {\n          vars: {\n            dob: {\n              $dateFromString: {\n                dateString: \"$DOB\",\n                onError: \"None\"\n              }\n            }\n          },\n          in: {\n            $cond: [\n              { $eq: [\"$$dob\", \"None\"] },\n              \"None\",\n              {\n                $round: {\n                  $divide: [\n                    { $subtract: [\"$$NOW\", \"$$dob\"] },\n                    31536000000 // 365*24*60*60*1000\n                  ]\n                }\n              }\n            ]\n          }\n        }\n      }\n    }\n  }\n])\n`\n```\nPlayground",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2021-05-18T05:51:35",
      "url": "https://stackoverflow.com/questions/67579727/mongo-messy-dob-string-field-to-age"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "pymongo",
      "question_id": 78394629,
      "title": "BSON Timestamp for MongoDB in Python",
      "problem": "I am running a python script which connects to a MongoDB database and a time stream collection. I am trying to insert a new document in the timestream.\n```\n`newdata = {\"metadata\": { \"ser\": \"129031\", \"type\": \"data\" }, \"timestamp\": {\"$date\" : timestamp}, \"val\": value}\n            \nresult = cur_collection.insert_one(newdata)\n`\n```\nThe problem is that the timestamp value is supposed to be in some BSON format. However, no matter what I've tried, I cannnot properly generate this value in python. All I'm trying to do is generate the current UTC timestamp and pass it.\nThis BSON value is supposed to be 64bit, with 32 bit seconds since epoch, and 32 bit with miliseconds. I could probably generate this manually, but It's hard to believe\nSome of the things I've tried include:\n```\n`            #timestamp = bson.datetime_ms.DatetimeMS(time.time())\n            #timestamp = int(time.time()*1000.0)#time.time() \n            #Timestamp.Timestamp(datetime.now(), 1) #dt.now()\n            #timestamp = datetime.now()\n            #timestamp = datetime.datetime.now(pytz.utc)\n            \n            #eastern = dateutil.tz.gettz('US/Eastern')\n            \n            #timestamp = datetime.now(tz=eastern)\n            #timestamp = new Date().getTime()\n            timestamp =  Timestamp(1412180887, 1)\n`\n```\nWe need an integer and the best I get is XXXXXXX.YYYYY. Maybe multiplying the fraction and adding it could work if I shift the integer by 32.\nWhen i try to run the code I get the error\n```\n`\"'timestamp' must be present and contain a valid BSON UTC datetime value\"\n`\n```",
      "solution": "As per your comment:\nThe Python `datetime` class will create a valid datetime for you. I just tried the following, and it worked correctly:\n```\n`timestamp = datetime.datetime.now()\nrecord = {\"title\": \"today\", \"timestamp\" : timestamp }\ncollection.insert_one(record)\n`\n```",
      "question_score": 1,
      "answer_score": 3,
      "created_at": "2024-04-27T12:11:06",
      "url": "https://stackoverflow.com/questions/78394629/bson-timestamp-for-mongodb-in-python"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "pymongo",
      "question_id": 77080741,
      "title": "Insert if it doesn&#39;t exist, update based in ADDITIONAL condition [mongoDB]",
      "problem": "I'm creating a sync system in which I have the following type of data:\n```\n`records = [\n    {'_id': 0, 'name': 'John', 'sync_date': '2022-01-01 00:00:00'}, \n    {'_id': 1, 'name': 'Paul', 'sync_date': '2021-11-11 11:11:11'}, \n    {'_id': 2, 'name': 'Anna', 'sync_date': '2012-12-12 12:12:12'}\n]\n`\n```\nThe idea is to (1) insert if there is no record with an `_id` 0 or 1 (the IDs of the records to insert/update) or update them ONLY IF the `sync_date` of the `records` is greater than the one in the collection. For example, if we have this in our collection:\n```\n`[\n    {'_id': 0, 'name': 'John', 'sync_date': '2000-01-01 00:00:00'}, \n    {'_id': 1, 'name': 'Paul', 'sync_date': '2023-01-01 23:23:23'}\n]\n`\n```\nThen it should (1) update John's instance, since the collection's `sync_date` is lower (older in time) than the one in our records, and ignore Paul's record because the `sync_date` available in the collection's instance is greater (earlier in time) than the one in our records. Additionally, it should insert Anna's instance as it's simply not in the collection.\nThe problem I see is that with `replace_one` and `update_one` methods if you add `upsert=True` then it will insert it not only when it's not in the collection but also when the `sync_date` is greater than the records' one. See:\n```\n`for record in records:\n    collection.replace_one({\n            '_id': record['_id'], \n            'sync_date': {'$lt': record['sync_date']}\n            },\n        {'upsert': True})\n`\n```\nI know there must be a more efficient way using an aggregate query, but currently this is my provisional solution:\n```\n`for record in records:\n    if not collection.find_one({'_id': record['_id']}):\n        collection.insert_one(record)\n    else:\n        collection.update_one(\n            {'_id': record['_id'], 'sync_date': {'$lt': record['sync_date']}},\n            record)\n`\n```\nAny ideas as to how to build the query?",
      "solution": "If you want to do it in one query, one option is to use aggregation with `$merge`:\n```\n`db.collection.aggregate([\n  {$match: {_id: {$in: recordIds}}},\n  {$group: {_id: null, docs: {$push: \"$$ROOT\"}}},\n  {$addFields: {records: records}}, \n  {$unwind: \"$records\"},\n  {$project: {\n      _id: 0,\n      record: \"$records\",\n      docs: {$filter: {\n          input: \"$docs\",\n          cond: {$and: [\n              {$eq: [\"$$this._id\", \"$records._id\"]},\n              {$gt: [\"$$this.sync_date\", \"$records.sync_date\"]}\n          ]}\n      }}\n  }},\n  {$replaceRoot: {newRoot: {$mergeObjects: [\"$record\", {$first: \"$docs\"}]}}},\n  {$merge: {into: \"collection\"}}\n])\n`\n```\nSee how it works on the playground example",
      "question_score": 1,
      "answer_score": 3,
      "created_at": "2023-09-11T12:13:15",
      "url": "https://stackoverflow.com/questions/77080741/insert-if-it-doesnt-exist-update-based-in-additional-condition-mongodb"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "pymongo",
      "question_id": 75591618,
      "title": "Connect mongodb from AWS lambda python using IAM",
      "problem": "I was using until now `pymongo[srv]` to connect my AWS lambda to my MongoDB cluster with:\n```\n`mongodb+srv://username:password@cluster.name/database\n`\n```\nNow I am trying to setup the IAM role connection and in this repo there is a connection string such as :\n```\n`\"mongodb://:@mongodb.example.com/?authMechanism=MONGODB-AWS&authMechanismProperties=AWS_SESSION_TOKEN:\"\n`\n```\nI have tried this one, but I got the following error in my Cloudwatch logs:\n```\n`[ERROR] ValueError: Port must be an integer between 0 and 65535: 'Fizjqbxairn6K19Fsalbucyz'\nTraceback (most recent call last):\n\u00a0\u00a0File \"/var/lang/lib/python3.9/importlib/__init__.py\", line 127, in import_module\n\u00a0\u00a0\u00a0\u00a0return _bootstrap._gcd_import(name[level:], package, level)\n\u00a0\u00a0File \"\", line 1030, in _gcd_import\n\u00a0\u00a0File \"\", line 1007, in _find_and_load\n\u00a0\u00a0File \"\", line 986, in _find_and_load_unlocked\n\u00a0\u00a0File \"\", line 680, in _load_unlocked\n\u00a0\u00a0File \"\", line 850, in exec_module\n\u00a0\u00a0File \"\", line 228, in _call_with_frames_removed\n\u00a0\u00a0File \"/var/task/lambda_function.py\", line 13, in \n\u00a0\u00a0\u00a0\u00a0client = MongoClient(\n\u00a0\u00a0File \"/var/task/pymongo/mongo_client.py\", line 736, in __init__\n\u00a0\u00a0\u00a0\u00a0res = uri_parser.parse_uri(\n\u00a0\u00a0File \"/var/task/pymongo/uri_parser.py\", line 568, in parse_uri\n\u00a0\u00a0\u00a0\u00a0nodes = split_hosts(hosts, default_port=default_port)\n\u00a0\u00a0File \"/var/task/pymongo/uri_parser.py\", line 376, in split_hosts\n\u00a0\u00a0\u00a0\u00a0nodes.append(parse_host(entity, port))\n\u00a0\u00a0File \"/var/task/pymongo/uri_parser.py\", line 137, in parse_host\n\u00a0\u00a0\u00a0\u00a0raise ValueError(\"Port must be an integer between 0 and 65535: %r\" % (port,))\n`\n```\nSo I am guessing that the String syntax is not correct.\nPlease tell me what is the correct connection string to be used in using the `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY ` env variables:\n```\n`client = MongoClient(conn_string)\n`\n```\nEdit\nMy Python string is:\n```\n`client = MongoClient('mongodb://' + os.environ.get(\"AWS_ACCESS_KEY_ID\") + ':' + os.environ.get(\"AWS_SECRET_ACCESS_KEY\") + @clustername/databasename?retryWrites=true&authMechanism=MONGODB-AWS)\n`\n```\nAnd I am installing `pymongo[aws]` as dependency instead of `pymongo[srv]`.\nEdit 2\nI confirm that the `AWS_SECRET_ACCESS_KEY` contains `/` as characters.\nEdit 3\nThe above error is not relevant any more, `pymongo` accepts the encoded connection string but this is not the good way to go with the AWS Lambda according to this documentation.\nI am trying to connect my AWS Lambda to my Atlas cluster database using `pymongo[aws]` but it times out all the time.\nThe URI should look like this according to the above documentation :\n`uri = \"mongodb://example.com/?authMechanism=MONGODB-AWS\"`\nI have tried it of course, and I have no more errors or anything, the lambda simply times out.\nI have double checked the `ARN` role for my lambda and the one setup in MongoDB when i have created my database user.\nI have also granted the `dbAdmin` role to be sure it's not a permission issue but still times out.\nThe MongoDB community support also tries to help on this case.\nAny ideas where it can come from?",
      "solution": "There are 3 moving parts that should be configured to work together:\n\npython code/libraries\nAWS Lambda role and VPC\nAtlas Network and database access\n\npython\nstarts from `pip install \"pymongo[aws]\"`\nat the time of answering it pulled following dependencies:\n```\n`boto3==1.26.121\nbotocore==1.29.121\ndnspython==2.3.0\njmespath==1.0.1\npymongo==4.3.3\npymongo-auth-aws==1.1.0\npython-dateutil==2.8.2\ns3transfer==0.6.0\nsix==1.16.0\nurllib3==1.26.15\n`\n```\nThe minimal lambda code to test:\n```\n`from pymongo.mongo_client import MongoClient\nfrom pymongo.server_api import ServerApi\n\ndef lambda_handler(event, context):\n\n    uri = \"mongodb+srv://..mongodb.net/?authSource=%24external&authMechanism=MONGODB-AWS&retryWrites=true&w=majority\"\n    # Create a new client and connect to the server\n    client = MongoClient(uri, server_api=ServerApi('1'))\n    # Send a ping to confirm a successful connection\n    try:\n        client.admin.command('ping')\n        print(\"Pinged your deployment. You successfully connected to MongoDB!\")\n    except Exception as e:\n        print(e)\n`\n```\nThe cluster name and domain name in the `uri` can be found on Atlas connection settings. On the screenshot it's the python driver, but the domain is the same regardless of client:\n\nNote: pymongo-auth-aws plays well with AWS environment variables, so no need to set them explicitly in the `uri`\nAWS Lambda and Mongodb Atlas\nNetwork access\nThe function should be deployed within VPC to let VPC peering with Atlas, or at least have a fixed egres IP to allow network connections to Atlas from this IP (circled blue on the image below). For the sake of experiment - temporarily allow access to Atlas from everywhere 0.0.0.0/0 to debug authentication in isolation. Once you get it connected on app layer, return back to the network configuration. Don't leave it open even on dev environment\nData base access:\nThe authentication is based on the role name, so you need to copy exact ARN of the lambda execution role, create a user on Atlas side, paste the role name, and assign mongodb level permissions to the user (red path on the image below).\n\nTesting:\nOnce it's configured, you can trigger the function manually (payload doesn't matter, it's not being used), and observe `Pinged your deployment. You successfully connected to MongoDB!` in the log:",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2023-02-28T12:44:13",
      "url": "https://stackoverflow.com/questions/75591618/connect-mongodb-from-aws-lambda-python-using-iam"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "pymongo",
      "question_id": 73873711,
      "title": "Monogdb / pymongo &#39;authenticate&#39; method call fails because no such method exists",
      "problem": "I have problem connecting to MongoDb using pymongo driver (Python 3.10, pymongo 4.2.0, MongoDb 6), specifically, authentication steps fails. Please see below my code:\n```\n`import pymongo\nfrom pymongo import MongoClient\n\nclient=MongoClient('mongodb://10.10.1.8:27017')\nclient.admin.authenticate('dev01','my_password_here')\n`\n```\nI am behind company firewall, that's why you see internal IP address - 10.10.1.8\nWhen I try to test run Python code to store data, I am getting the following error:\n```\n`  File \"C:\\Users\\ABC\\Documents\\python_development\\pymongo_test.py\", line 7, in \n\n    client.admin.authenticate('dev01','my_password_here')\n\n  File \"C:\\Users\\ABC\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pymongo\\collection.py\", line 3194,\n in __call__ raise TypeError(\nTypeError: 'Collection' object is not callable. If you meant to call the 'authenticate' method\n on a 'Database' object it is failing because no such method exists.\n`\n```\nMongoDb sits on virtual Linux Ubuntu server that sits on top of Linux Debian server. My laptop has Windows 10.\nThe code looks correct, any ideas why this is happening?",
      "solution": "Migration guide from pymongo 3.x to 4.x\nhttps://pymongo.readthedocs.io/en/stable/migrate-to-pymongo4.html?highlight=authenticate#database-authenticate-and-database-logout-are-removed reads:\n\nRemoved pymongo.database.Database.authenticate() and pymongo.database.Database.logout(). Authenticating multiple users on the same client conflicts with support for logical sessions in MongoDB 3.6+. To authenticate as multiple users, create multiple instances of MongoClient. Code like this:\n\n```\n`client = MongoClient()\nclient.admin.authenticate('user1', 'pass1')\nclient.admin.authenticate('user2', 'pass2')\n`\n```\ncan be changed to this:\n```\n`client1 = MongoClient(username='user1', password='pass1')\nclient2 = MongoClient(username='user2', password='pass2')\n`\n```",
      "question_score": 1,
      "answer_score": 3,
      "created_at": "2022-09-27T22:55:24",
      "url": "https://stackoverflow.com/questions/73873711/monogdb-pymongo-authenticate-method-call-fails-because-no-such-method-exists"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "pymongo",
      "question_id": 72749316,
      "title": "Pymongo define list as type of ObjectId for $in query",
      "problem": "I have a list of ObjectIds which I want to pass into the following pymongo command to find the documents by their ObjectId\n`ids = ['62b3221c2db07f9388aa61e9', '62b325f65402e5ceea9a3d4c', '62b48ccee6f77605c2783775']`\n`list(dbusers.find({\"_id\" : {\"$in\": ids }}))`\nHowever it expects them to be in the format\n`ids = [ObjectId('62b3221c2db07f9388aa61e9'), ObjectId('62b325f65402e5ceea9a3d4c'), ObjectId('62b48ccee6f77605c2783775')]`\nBut I can't seem to get them in this format as they are in a list?",
      "solution": "You can convert the object id strings to proper `ObjectId` objects by giving the object id as a parameter when creating it (`ObjectId()`).\nTo convert your list to a list of `ObjectId` objects instead of a plain string list, you can use `map` which invokes a callable on each element in a list and returns an iterator - you can then exhaust this iterator and get a list back by giving it to `list`:\n`from bson.objectId import ObjectId\n\nobject_id_list = list(map(ObjectId, ids))\nfound_users = list(dbusers.find({\"_id\": {\"$in\": object_id_list}}))\n`\nSince a class works as a callable to instantiate the class, you can give the class as the first argument to `map` and get objects created as that class back.",
      "question_score": 1,
      "answer_score": 3,
      "created_at": "2022-06-24T22:56:40",
      "url": "https://stackoverflow.com/questions/72749316/pymongo-define-list-as-type-of-objectid-for-in-query"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "pymongo",
      "question_id": 70606894,
      "title": "How to unset fields based on the fields&#39; name with a &quot;less than&quot; condition?",
      "problem": "I am trying to unset certain fields based on the name of the field (the key).\nSay I have something like this:\n```\n`{\n    5: \"cool\",\n    93: \"cool\",\n    30: \"cool\",\n    56: \"cool\"\n}\n`\n```\nHow would I unset all fields with a value below, say, 40.\nSo the result should be:\n```\n`{\n    5: \"cool\",\n    30: \"cool\"\n}\n`\n```\nI tried using the less than command on the whole field or using the positional operator but both of those failed.\n```\n`collection.update_one(\n        {\"_id\": id},\n        {\"$unset\": {\"blacklist.$\": {\"$lt\": 40}}}\n    )\n`\n```\nI couldn't find anything online or on the docs so I am hoping to find some help here,\nThanks!",
      "solution": "You can't really use `$unset` like this, we can still achieve this using pipelined updates - with slightly more complicated syntax.\nOur approach will be to turn our root object to an array using `$objectToArray`, iterate over it and filter all numeric keys under a certain threshold. then finally convert back to an object and update our document, like so:\n```\n`db.collection.update({},\n[\n  {\n    $replaceRoot: {\n      newRoot: {\n        $arrayToObject: {\n          $filter: {\n            input: {\n              $objectToArray: \"$$ROOT\"\n            },\n            cond: {\n              $cond: [\n                {\n                  $regexMatch: {\n                    input: \"$$this.k\",\n                    regex: \"^[0-9]+$\"\n                  }\n                },\n                {\n                  $lt: [\n                    {\n                      $toInt: \"$$this.k\"\n                    },\n                    40\n                  ]\n                },\n                true\n              ]\n            }\n          }\n        }\n      }\n    }\n  }\n])\n`\n```\nMongo Playground",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2022-01-06T13:08:56",
      "url": "https://stackoverflow.com/questions/70606894/how-to-unset-fields-based-on-the-fields-name-with-a-less-than-condition"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "pymongo",
      "question_id": 67596481,
      "title": "How to remove all the \u201c$oid\u201d and &quot;$date&quot; in a .json file?",
      "problem": "I have a .json file saved in my computer that contains things like `$oid` or `$date` which will later cause me trouble in BigQuery. For example:\n```\n`{\n  \"_id\": {\n  \"$oid\": \"5e7511c45cb29ef48b8cfcff\"\n  },\n  \"about\": \"some text\",\n  \"creationDate\": {\n  \"$date\": \"2021-01-05T14:59:58.046Z\"\n  }\n}\n`\n```\nI want it to look like (so it\u2019s not just removing some letters from the string):\n```\n`{\n  \"_id\": \"5e7511c45cb29ef48b8cfcff\",\n  \"about\": \"some text\",\n  \"creationDate\": \"2021-01-05T14:59:58.046Z\"\n}\n`\n```\nWith Pymongo, one can do something like:\n```\n`my_file['id']=my_file['id']['$oid']\nmy_file['creationDate']=my_file['creationDate']['$date']\n`\n```\nHow would this look without using Pymongo, since I want to first find such keys and remove all the problematic `$oid` or `$date`?\nEdit: sorry for the bad wording, what I meant to say was whether it was possible to find the keys that contain these problematic $ without writing down every key in the dictionary. In reality, there are more files with huge tables and many of them can contain this.",
      "solution": "I would try something as shown below.\n```\n`import json\nfile = open('data.json','r')\ndata = json.load(file)\nfor k,v in data.items():\n    #check if key has dict value\n    if type(v) == dict:\n        #find id with $\n        r = list(data[k].keys())[0]\n        #change value if $ occurs\n        if r[0] == '$':\n            data[k] = data[k][r]\nprint(data)\n`\n```\nseems like we get this output.\n```\n`{'_id': '5e7511c45cb29ef48b8cfcff', 'about': 'some text', 'creationDate': '2021-01-05T14:59:58.046Z'}\n`\n```",
      "question_score": 1,
      "answer_score": 1,
      "created_at": "2021-05-19T05:22:38",
      "url": "https://stackoverflow.com/questions/67596481/how-to-remove-all-the-oid-and-date-in-a-json-file"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "pymongo",
      "question_id": 66505990,
      "title": "docker-compose and connection to Mongo container",
      "problem": "I am trying to create 2 containers as per the following docker-compose.yml file.  The issue is that if I start up the mongo database container and then run my code locally (hitting 127.0.0.1) then everything is fine but if I try and run my api container and hit that (see yml file) then I get connection refused i.e.\n\n172.29.0.12:27117: [Errno 111] Connection refused, Timeout: 30s, Topology Description: ]>\n\nPlease note: I have set mongo to use port 27117 rather than 27017\nMy app is a Python Flask app and I am using PyMongo in the following manner:\n```\n`    try:\n        myclient = pymongo.MongoClient('mongodb://%s:%s@%s:%s/%s' % (username, password, hostName, port, database))\n        mydb = myclient[database]\n        cursor = mydb[\"temperatures\"]\n        app.logger.info('Database connected to: ' + database)  \n    except:\n        app.logger.error('Error connecting to database')\n`\n```\nWhat's driving me mad is it runs locally and successfully accesses mongo via the container, but as soon as I try the app in a container it fails.\ndocker-compose.yml as follows:\n```\n`version: '3.7'\nservices:\n  hotbin-db:\n    image: mongo\n    container_name: hotbin-db\n    restart: always\n    ports:\n      #  : \n      - '27117:27017'\n    expose:\n      # Opens port 3306 on the container\n      - '27117'\n    command: [--auth]\n    environment:\n      MONGO_INITDB_ROOT_USERNAME: ***\n      MONGO_INITDB_ROOT_PASSWORD: ***\n      MONGO_INITDB_DATABASE: ***\n      MONGODB_DATA_DIR: /data/db\n      MONDODB_LOG_DIR: /dev/null\n      # Where our data will be persisted\n    volumes:\n      - /home/simon/mongodb/database/hotbin-db/:/data/db\n        #- my-db:/var/lib/mysql\n    # env_file:\n    #   - .env\n    networks:\n      hotbin-net:\n        ipv4_address: 172.29.0.12\n\n  hotbin-api:\n    image: scsherlock/compost-api:latest\n    container_name: hotbin-api\n    environment:\n      MONGODB_DATABASE: ***\n      MONGODB_USERNAME: ***\n      MONGODB_PASSWORD: ***\n      MONGODB_HOSTNAME: 172.29.0.12\n      MONGODB_PORT: '27117'\n    depends_on: \n      - hotbin-db\n    restart: always\n    ports:\n      #  : \n      - '5050:5050'\n    expose:\n      - '5050'\n    networks:\n      hotbin-net:\n        ipv4_address: 172.29.0.13  \n\n# # Names our volume\nvolumes:\n  my-db:  \n\nnetworks:\n  hotbin-net:\n    driver: bridge\n    ipam:\n      driver: default\n      config:\n        - subnet: 172.29.0.0/16\n\n`\n```",
      "solution": "Using the service name of the mongo container and the standard port of\n27017 instead of 27117 (even though that's what is defined in the\ndocker-compose file) works. I'd like to understand why though\n\nYour docker compose file does NOT configure MongoDB to run on port 27117. If you want to get it to run on 27117 you would have to change this line in the docker compose:\n```\n`command: mongod --auth --port 27117\n`\n```\nAs you haven't specified a port, MongoDB will run on the default port 27017.\nYour `expose` section exposes the container port 27117 to the host, but Mongo isn't running on that port, so that line is effectively doing nothing.\nYour `ports` section maps a host port 27117 to a container port 27017. This means if you're connecting from the host, you can connect on port 27117, but that is connecting to port 27017 on the container.\nNow to your python program. As this is running in the container network, to connect services within a docker-compose network, you reference them by their service name.\nPutting this together, your connection string will be: `mongodb://hotbin-db:27017/yourdb?`\nAs others have mentioned, you really don't need to create specific IP addresses unless you have a very good need to. You also don't even to define a network, as docker-compose creates it's own internal network.\nReference: https://docs.docker.com/compose/networking/",
      "question_score": 1,
      "answer_score": 3,
      "created_at": "2021-03-06T14:10:54",
      "url": "https://stackoverflow.com/questions/66505990/docker-compose-and-connection-to-mongo-container"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "pymongo",
      "question_id": 66414955,
      "title": "is PyMongo / MongoDB insert_many transactional?",
      "problem": "For speed of upload, I have a multiprocessing Python program that splits a CSV into many parts, and uploads each in a different process.  Also for speed, I'm putting 3000 inserts together into each insert_many.\nThe trick is that I have some bad data in some rows, and I haven't yet figured out where it is.  So what I did was a Try/Except around the insert_many, then I try to insert again the 3000 documents, but one at a time, inside another Try/Except.  Then I can do a pprint.pprint on just the rows that have errors.\nHowever, I'm wondering if when the update of 3000 documents fails because of an error, in for example the 1000th row, does the entire 3000 fail?  Or do the first 999 rows get stored and the rest fail?  Or do the 2999 rows get stored, and only the one bad-data row fails?",
      "solution": "When you do inserts via a bulk write, you can set the ordered option.\nWhen ordered is true, inserts stop at the first error.\nWhen ordered is false, inserts continue past each document that failed for any reason.\nIn either case, bulk writes are not transactional in the sense that a failure doesn't remove any previously performed writes (inserts or otherwise).\nIf you would like a transaction, MongoDB provides those as an explicit feature.",
      "question_score": 1,
      "answer_score": 3,
      "created_at": "2021-03-01T00:36:04",
      "url": "https://stackoverflow.com/questions/66414955/is-pymongo-mongodb-insert-many-transactional"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "pymongo",
      "question_id": 79686384,
      "title": "Pymongo and Beanie Incompatibility issues",
      "problem": "I'm trying to migrate from motor to pymongo's AsyncMongoClient.\nAfter doing some upgrading/installing on pymongo I am having the below error when running this import `from beanie import Document`\n```\n`ImportError: cannot import name '_QUERY_OPTIONS' from 'pymongo.cursor'\n`\n```\nUsing python 3.11.9\nMy dependencies are below:\n```\n`dependencies = [\n    \"fastapi==0.95.0\",\n    \"uvicorn==0.22.0\",\n    \"gunicorn==20.1.0\",\n    \"elastic-apm==6.15.1\",\n    \"pymongo==4.13.2\",\n    \"pydantic==1.10.18\",\n    \"beanie==1.29.0\",\n    \"dnspython==2.2.1\",\n    \"python-dotenv==1.0.0\",\n    \"psutil==5.9.4\",\n    \"loguru==0.6.0\",\n    \"fastapi-etag==0.4.0\",\n    \"mongoengine==0.29.1\",\n    \"elasticsearch7==7.17.12\",\n    \"elasticsearch-dsl==7.4.1\",\n    \"promise==2.3\",\n    \"requests==2.31.0\",\n    \"pytz==2023.3\",\n    \"singleton-decorator==1.0.0\",\n    \"cachetools==5.3.1\",\n    \"pymysql==1.0.2\",\n    \"requests-custom-header==0.1.1\",\n    \"aiohttp==3.9.1\",\n    \"telempack==1.7.11\",\n    \"polars==1.9.0\",\n    \"jinja2==3.1.3\",\n    \"oracledb==2.5.1\",\n    \"numpy==2.2.3\",\n    \"pika==1.3.2\",\n    \"zstandard==0.23.0\",\n]\n`\n```\nI've tried multiple attempts to upgrade/downgrade beanie/pymongo/mongoengine but it keeps throwing this error. Any ideas?",
      "solution": "This seems to be an issue with using `pymongo >= 4.9` with older versions of libraries that haven't been updated to be compatible with the breaking changes introduced in those pymongo releases.\nThis is because the `_QUERY_OPTIONS` attribute was removed from `pymongo.cursor` in 4.9 and above. Libraries that rely on this attribute will break if they haven't been updated to accommodate this change. Therefore, the solution involves either updating the dependent library or downgrading pymongo to issue.",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2025-07-01T18:53:04",
      "url": "https://stackoverflow.com/questions/79686384/pymongo-and-beanie-incompatibility-issues"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "pymongo",
      "question_id": 79505258,
      "title": "PyMongo - Group by year based on subdocument date",
      "problem": "I have a MongoDB document like:\n```\n`[\n  {\n    _id: ObjectId('67cfd69ba3e561d35ee57f51'),\n    created_at: ISODate('2025-03-11T06:22:19.044Z'),\n    conversation: [\n      {\n        id: '67cfd6c1a3e561d35ee57f53',\n        feedback: {\n          liked: false,\n          disliked: true,\n          copied: true,\n          created_at: ISODate('2025-03-11T06:27:48.634Z')\n        }\n      },\n      {\n        id: '67cfd77fa3e561d35ee57f54',\n        feedback: {\n          liked: true,\n          disliked: false,\n          copied: false,\n          created_at: ISODate('2025-03-11T06:28:25.099Z')\n        }\n      },\n      { id: '67d009f1a3e561d35ee57f5a', feedback: null },\n      { id: '67d009f8a3e561d35ee57f5b', feedback: null }\n    ]\n  },\n  {\n    _id: ObjectId('67d00aeaa3e561d35ee57f5d'),\n    created_at: ISODate('2025-03-11T10:05:30.848Z'),\n    conversation: [\n      { id: '67d00af7a3e561d35ee57f5f', feedback: null },\n      { id: '67d00afaa3e561d35ee57f60', feedback: null }\n    ]\n  }\n]\n`\n```\nWhere the main document has a `conversation` subdocument, I want to know how many `likes`, `dislikes` and `copied` data in each year.\nI tried to get `year` from the `conversation.feedback.created_at` using `$dateToString` operator.\n```\n`pipeline = [\n  { \n    '$match': { 'conversation.feedback.copied': True }\n  },\n  { \n    '$group': {\n      '_id': { \n        '$dateToString': {\n          'format': '%Y',\n          'date': '$conversation.feedback.created_at'\n        }\n      },\n      'total_copied': { '$sum': 1 }\n    }\n  }\n]\n`\n```\nBut it gives an error:\n\nOperationFailure: PlanExecutor error during aggregation :: caused by :: can't convert from BSON type array to Date, full error: {'ok': 0.0, 'errmsg': \"PlanExecutor error during aggregation :: caused by :: can't convert from BSON type array to Date\", 'code': 16006, 'codeName': 'Location16006'}\n\nWhat I am expecting out as:\n```\n`{\n    \"2025\": {\n        \"total_liked\": 1,\n        \"total_disliked\": 1,\n        \"total_copied\": 1\n    }\n}\n`\n```\nHow to convert the DateTime object to year and combine the total counts for 3 parameters?",
      "solution": "You need the `$unwind` stage to deconstruct the `conversation` array before grouping by `conversation.feedback.created_at`.\n\nNote that, in your sample data, there is possibly the `conversation.feedback` is `null`. Hence you should remove those unwinded document with `conversation.feedback` is `null`.\n\nFor calculating the sum based on the boolean value, you can work with `$cond` to add 1 when the value is `true`.\n\nIf you are looking for the generated output with key-value pair, you may look for `$replaceRoot` and `$arrayToObject` to convert list of objects to key-value pair.\n\n`db.collection.aggregate([\n  {\n    \"$match\": {\n      \"conversation.feedback.copied\": true\n    }\n  },\n  {\n    \"$unwind\": \"$conversation\"\n  },\n  {\n    \"$match\": {\n      \"conversation.feedback\": {\n        \"$ne\": null\n      }\n    }\n  },\n  {\n    \"$group\": {\n      \"_id\": {\n        \"$dateToString\": {\n          \"format\": \"%Y\",\n          \"date\": \"$conversation.feedback.created_at\"\n        }\n      },\n      \"total_copied\": {\n        \"$sum\": {\n          $cond: [\n            {\n              $eq: [\n                \"$conversation.feedback.copied\",\n                true\n              ]\n            },\n            1,\n            0\n          ]\n        }\n      },\n      \"total_liked\": {\n        \"$sum\": {\n          \"$cond\": [\n            {\n              \"$eq\": [\n                \"$conversation.feedback.liked\",\n                true\n              ]\n            },\n            1,\n            0\n          ]\n        }\n      },\n      \"total_disliked\": {\n        \"$sum\": {\n          \"$cond\": [\n            {\n              \"$eq\": [\n                \"$conversation.feedback.disliked\",\n                true\n              ]\n            },\n            1,\n            0\n          ]\n        }\n      }\n    }\n  },\n  {\n    \"$replaceRoot\": {\n      \"newRoot\": {\n        \"$arrayToObject\": [\n          [\n            {\n              \"k\": \"$_id\",\n              \"v\": {\n                \"total_copied\": \"$total_copied\",\n                \"total_liked\": \"$total_liked\",\n                \"total_disliked\": \"$total_disliked\"\n              }\n            }\n          ]\n        ]\n      }\n    }\n  }\n])\n`\nDemo @ Mongo Playground",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2025-03-13T03:15:22",
      "url": "https://stackoverflow.com/questions/79505258/pymongo-group-by-year-based-on-subdocument-date"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "pymongo",
      "question_id": 79284760,
      "title": "PyMongo Async client not raising exception when connection fails",
      "problem": "It seems that a pymongo 4.10 async client does not raise an exception when there is a problem with the connection.\nTaken from the doc, a test without any mongo DB running locally yields:\n```\n`>>> import asyncio\n>>> from pymongo import AsyncMongoClient\n>>> client = AsyncMongoClient('mongodb://localhost:27017/')\n>>> asyncio.run(client.aconnect())\n# no errors\n`\n```\nWhen activating debug logs I see the connection being refused but I would expect an exception to be raised.\n```\n`>>> import logging\n>>> logging.basicConfig(level='DEBUG')\n>>> asyncio.run(client.aconnect())\nDEBUG:asyncio:Using selector: KqueueSelector\nDEBUG:pymongo.topology:{\"topologyId\": {\"$oid\": \"676020be62e71d3fe6f27721\"}, \"serverHost\": \"localhost\", \"serverPort\": 27017, \"awaited\": false, \"durationMS\": 2.786167000522255, \"failure\": \"\\\"AutoReconnect('localhost:27017: [Errno 61] Connection refused (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms)')\\\"\", \"message\": \"Server heartbeat failed\"}\n`\n```\nI would expect the DEBUG log error to be an exception. Am I misunderstanding something with the async client ?",
      "solution": "The mongo client uses connection pools etc in the background, even though you tell it to explicitly connect (why?) it doesn't raise an exception for failing to connect until you actually try to read or write from/to the DB.\nBut you can check if/where it's connected:\n```\n`>>> list(client.nodes)\n[('10.0.0.1', 27017)]\n`\n```\nThe result will be an empty list if `aconnect` fails.\nBut if you try any communication such as:\n```\n`>>> await client.server_info()\n`\n```\n... you will get an exception:\n```\n`pymongo.errors.ServerSelectionTimeoutError: localhost:27017: [Errno 111] Connection refused (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms), Timeout: 1.9985503089847043s, Topology Description: ]>\n`\n```\nThe pymongo async driver is just built like this.. why the tutorial tells you to use `aconnect()` ... I have no idea. I didn't even know it existed.\nBtw, you can use `python -m async` to get a REPL where you can run async commands without `asyncio.run()`",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2024-12-16T13:58:33",
      "url": "https://stackoverflow.com/questions/79284760/pymongo-async-client-not-raising-exception-when-connection-fails"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "pymongo",
      "question_id": 78909939,
      "title": "How to use `pymongo` with `flask` inside of Vercel",
      "problem": "I want to add database functionalities to my Flask app hosted on Vercel.\nWhen using `pymongo` the website throws `HTTP 500` saying that my app crashed.\nIt also says that I should check the logs.\nThe logs say this error:\n```\n`[ERROR] Runtime.ImportModuleError: Unable to import module 'vc__handler__python': cannot import name 'MutableMapping' from 'collections' (/var/lang/lib/python3.12/collections/__init__.py)\nTraceback (most recent call last):INIT_REPORT Init Duration: 1348.16 ms Phase: invoke   Status: error   Error Type: Runtime.Unknown\n`\n```\nMaybe `pymongo` needs to make mutable objects and the Python interpreter on Vercel doesn't have it? I don't know.",
      "solution": "From the error message \"cannot import name 'MutableMapping' from 'collections'\", it seems the code you are running targets an older version of Python. The documentation for Python 3.9 states:\n\nDeprecated since version 3.3, will be removed in version 3.10: Moved\nCollections Abstract Base Classes to the collections.abc module. For\nbackwards compatibility, they continue to be visible in this module\nthrough Python 3.9.\n\nAlso, ensure that the PyMongo version is compatible with the Python version that you are using. You are using Python 3.12 so the PyMongo version should be 4.5 or higher. Look up the compatibility matrix at: https://www.mongodb.com/docs/languages/python/pymongo-driver/current/compatibility/#language-compatibility . If you have to upgrade your PyMongo version, you can do it with `python3 -m pip install --upgrade pymongo`",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2024-08-24T22:32:11",
      "url": "https://stackoverflow.com/questions/78909939/how-to-use-pymongo-with-flask-inside-of-vercel"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "pymongo",
      "question_id": 77763138,
      "title": "pymongo insert all data in string format",
      "problem": "I am importing csv using pymongo then inserting it into mongodb but due to some reason all field is in format of string where i was expecting double. Below is python code .\n```\n`def saveToMongo():\nprint(\"inside saveToMongo\")\ncollection = mydb['country']\n\nheader = ['country_id','country_name','zone_id','minLat','maxLat','minLong','maxLong']\ncsvFile = open('country.csv', 'r')\nreader = csv.DictReader(csvFile)\n\nprint(reader)\n\nfor each in reader:\n    row = {}\n    for field in header:\n        row[field] = each[field]\n\n    #print(row)\n    collection.insert(row)\n`\n```\nand here is the csv file\n```\n`country_id,country_name,zone_id,minLat,maxLat,minLong,maxLong\n2,Bangladesh,1,20.6708832870000,26.4465255803000,88.0844222351000,92.6727209818000\n3,\"Sri Lanka\",1,5.9683698592300,9.8240776636100,79.6951668639000,81.7879590189000\n4,Pakistan,1,23.6919650335000,37.1330309108000,60.8742484882000,77.8374507995000\n5,Bhutan,1,26.7194029811000,28.2964385035000,88.8142484883000,92.1037117859000\n`\n```\nI am unable to understand why python is storing data in String format.\nWhen i try to insert data using mongoimport I'm getting belopw error\n```\n`fatal error: unrecognized DWARF version in .debug_info at 6\n\nruntime stack:\npanic during panic\n\nruntime stack:\nstack trace unavailable\n`\n```",
      "solution": "The csv.DictReader just parse any data as String.\nIf you want another format you can create custom function to parse them in the type you want.\nConsidering that what you call `double` is a `float` in python, you could do something as follow :\n```\n`from pymongo import MongoClient \nimport csv\nmyclient = MongoClient(\"mongodb://localhost:27017/\")\nmydb = myclient[\"mydbname\"]\n\ndef csvToMongo():\n  with open('country.csv','r') as myFile :\n    reader = csv.DictReader(myFile, delimiter=\",\")\n    myParsedData = [\n      {\n        'country_id' : int(elem['country_id']),\n        'country_name' : elem['country_name'],\n        'zone_id' : int(elem['zone_id']),\n        'minLat' : float(elem['minLat']),\n        'maxLat' : float(elem['maxLat']),\n        'minLong' : float(elem['minLong']),\n        'maxLong' : float(elem['maxLong']),\n      }\n      for elem in reader\n    ]\n    collection = mydb['country']\n    collection.insert_many(myParsedData)\n\n#execute function\ncsvToMongo()\n`\n```",
      "question_score": 1,
      "answer_score": 1,
      "created_at": "2024-01-05T08:47:38",
      "url": "https://stackoverflow.com/questions/77763138/pymongo-insert-all-data-in-string-format"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "pymongo",
      "question_id": 76699632,
      "title": "Mongo, counting items in arrays in several documents via Lookup",
      "problem": "We have a collection with documents called notes like:\n```\n`{\n  \"_id\": {\n    \"$oid\": \"64b42008f622157dd5aead17\"\n  },\n  \"isbn\": 9789401466097,\n  \"chapter\": 2,\n  \"notes\": [\n    {\n      \"note\": \"a note\",\n      \"dt\": 2374238742345,\n    },\n    {\n      \"note\": \"another note about the same book and chapter\",\n      \"dt\": 2345234234,\n    }\n  ]\n}\n`\n```\nand a second similar document (and more)\n```\n`{\n  \"_id\": {\n    \"$oid\": \"64b42234f622157dd5aead1c\"\n  },\n  \"isbn\": 9789401466097,\n  \"chapter\": 1,\n  \"notes\": [\n    {\n      \"note\": \"A note about the same book but another chapter\",\n      \"dt\": 23742387423\n    },\n  ]\n}\n\n{\n  \"_id\": {\n    \"$oid\": \"64b42234f6223de7dd5aead1c\"\n  },\n  \"isbn\": 9789401488088,\n  \"chapter\": 1,\n  \"notes\": [\n    {\n      \"note\": \"something about another book\",\n      \"dt\": 23742384555\n    },\n    {\n      \"note\": \"something else\",\n      \"dt\": 23452333333\n      }\n    }\n  ]\n}\n`\n```\nOther similar documents may have the same or other isbs or other chapters.\nThe combination of isbn and chapter form the unique key for a document.\nThen there's the collection of books (hence the isbn). So with:\n```\n`collection = 'books' # using pythons pymongo\nmatch = {'$match': {'isbn': isbn}}\nprojection = {'$project':\n        {\n            '_id': '$_id',\n            'isbn': '$isbn',\n            'title': '$title',\n        }\n    }\n\npipeline = [\n        {'$facet':\n            {\n                'the_book':\n                    [\n                        match,\n                        projection,\n                    ],\n            }\n        }\n    ]\n`\n```\nI consult and project the collection of books.\nWhat I try to accomplish is to count all of the notes that are made for all of the chapters for the book with a given isbn.\nWhat I've tried:\n```\n`lookup_notes = {'$lookup':\n        {\n            'from': 'notes',\n            'localField': 'isbn',\n            'foreignField': 'isbn',\n            'as': 'note',\n        },\n    }\nproject_notes = {'$project':\n        {\n            \"teller\": {\n                \"$size\": '$note.notes'\n            }\n        }\n    }\npipeline = [\n        {'$facet':\n            {\n                'the_book':\n                    [\n                        match,\n                        projection,\n                    ],\n                'the_notes':\n                    [\n                        lookup_notes,\n                        match,\n                        project_notes,\n                    ],\n            }\n        }\n    ]\n`\n```\nas well as a lot of other attempts using $group, $size and $count. They result in errors or only counting the amount of relevant documents in de notes collection, but not the combined amount of notes inside all of the relevant documents.\nFor the above example, with isbn=9789401466097 I should get the result 3. Not 2 or 5.\nThanks for the help.",
      "solution": "```\n`isbn = 9789401466097\nmatch = {\"$match\": {\"isbn\": isbn}}\n\n# join 'notes' collection.\nlookup_notes = {\n    \"$lookup\": {\n        \"from\": \"notes\",\n        \"localField\": \"isbn\",\n        \"foreignField\": \"isbn\",\n        \"as\": \"related_notes\",\n    },\n}\n# deconstruct the notes array.\nunwind_notes = {\n    \"$unwind\": \"$related_notes\"\n}\n# deconstruct the notes inside the related_notes.\nunwind_inner_notes = {\n    \"$unwind\": \"$related_notes.notes\"\n}\n# count the notes.\ngroup_notes = {\n    \"$group\": {\n        \"_id\": \"$isbn\",\n        \"count\": {\"$sum\": 1}\n    },\n}\npipeline = [match, lookup_notes, unwind_notes, unwind_inner_notes, group_notes]\nresult = books_collection.aggregate(pipeline)\n`\n```",
      "question_score": 1,
      "answer_score": 1,
      "created_at": "2023-07-16T19:38:15",
      "url": "https://stackoverflow.com/questions/76699632/mongo-counting-items-in-arrays-in-several-documents-via-lookup"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "pymongo",
      "question_id": 76649785,
      "title": "How to find and replace string inside arrays in MongoDB and pymongo",
      "problem": "I have a collection in MongoDB.(Technically it is the cosmosDB API for MongoDB but in theory that shouldn\u2019t matter.) The collection contains nested arrays of strings.  These strings contain brackets inside them, and I need to remove them from the strings inside the arrays.  Sample (fake) data:\n```\n`{\n    \u201cThing_id\u201d: \u201c100\u201d,\n    \u201cThing_name\u201d: \u201cThing100\u201d\n    \u201cComments\u201d: [\n        \u201cGood: [Thing100 is awesome]\u201d,\n        \u201cBad: [I will never buy Thing100 again.]\u201d\n    ]\n},\n{\n    \u201cThing_id\u201d: \u201c101\u201d,\n    \u201cThing_name\u201d: \u201cThing101\u201d\n    \u201cComments\u201d: [\n        \u201cComparative: [Thing101 is so much better than Thing100.]\u201d,\n        \u201cBad: [Who designed such piece of \u2026]\u201d\n    ]\n}\n]\n`\n```\nAny above syntax errors are because I am typing it on my phone\u2026my apologies if it is not well-formed json/bson.  To recap, I want to remove just the brackets in each value in the comments arrays. I am using pymongo to connect to the DB, and also python 3.7.4.\nI have searched for how to do this, but all I can find are how to update the whole value in the array instead of replacing a character in the string inside the array.\nFindAndModify() looked promising until I found out it will only do the first document found.\nIs the only option to query the collection and loop through each document?",
      "solution": "I don't have access to Azure Cosmos DB, so I can't test.  If you are using `pymongo` and want to update all documents in the collection, you probably want to use `update_many`.\nHere are a couple parameters for `update_many` that may work for you.\n`filter = {'$expr': {'$gt': [{'$size': '$Comments'}, 0]}}\nupdate = [\n  {\n    \"$set\": {\n      # rewrite Comments\n      \"Comments\": {\n        \"$map\": {\n          \"input\": \"$Comments\",\n          \"as\": \"comment\",\n          \"in\": {\n            \"$reduce\": {\n              \"input\": {\n                # get all matches without \"[\" or \"]\"\n                # every comment needs to be a string\n                \"$regexFindAll\": {\n                  \"input\": \"$$comment\",\n                  # backslashes \"\\\" are used to \"escape\" brackets\n                  # the number of \"\\\" required may depend on platform, etc.\n                  \"regex\": \"[^\\\\[\\\\]]*\"\n                }\n              },\n              \"initialValue\": \"\",\n              \"in\": {\n                # concat all matches\n                \"$concat\": [\"$$value\", \"$$this.match\"]\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n]\n`\nYou can try this (slightly modified for the language/platform) on mongoplayground.net.",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2023-07-10T00:23:57",
      "url": "https://stackoverflow.com/questions/76649785/how-to-find-and-replace-string-inside-arrays-in-mongodb-and-pymongo"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "pymongo",
      "question_id": 75583269,
      "title": "Update a specific array element if it fulfils at least one of several conditions",
      "problem": "I am trying to understand (and fix) a code that is not mine, which uses PyMongo.\nWe want to look for the document with `_id==_id` that has, inside its list `comments`, a comment with `id==comment_id` or `id==PyObjectId(comment_id)`.\nTo that comment we want to add an answer.\nIn code:\n`await db_collection.update_one(\n        filter=query, update=update, array_filters=array_filters\n)\n`\nwhere\n`query = {\n    \"_id\": _id,\n    \"$or\": [    \n        {\"comments\": {\"$elemMatch\": {\"id\": comment_id}}},\n        {\"comments\": {\"$elemMatch\": {\"id\": PyObjectId(comment_id)}}},\n    ],\n}\n\nupdate = {\n    \"$set\": {\n        \"comments.$[cmt].answer\": jsonable_encoder(reply)\n    }\n}\n\narray_filters = [{\"cmt.id\": comment_id}]\n`\nMy problem is that `array_filters` only check if `id==comment_id` and not also if `id == PyObjectId(comment_id)`, like the query does.\nThis way when I have a `PyObjectId` as id, no item is updated.\nI guess I should modify `array_filters` with something like\n`array_filters = [{\"cmt.id\": comment_id}, {\"cmt.id\": PyObjectId(comment_id)}]\n`\nor\n`array_filters = [{\"$or$\": [{ \"cmt.id\": comment_id}, {\"cmt.id\": PyObjectId(comment_id)}]}\n`\nbut sadly I can just test my code on production and I'm trying to understand how this actually works before breaking things.\nThank you all!",
      "solution": "Let's see what's going on here...\nFrom the original question:\n`query = {\n    \"_id\": _id,\n    \"$or\": [    \n        {\"comments\": {\"$elemMatch\": {\"id\": comment_id}}},\n        {\"comments\": {\"$elemMatch\": {\"id\": PyObjectId(comment_id)}}},\n    ],\n}\n\nupdate = {\n    \"$set\": {\n        \"comments.$[cmt].answer\": jsonable_encoder(reply)\n    }\n}\n\narray_filters = [{\"cmt.id\": comment_id}]\n`\nThe query is using `\"$elemMatch\"`, but since there is only one condition on a single field, this could be simplified to:\n`query = {\n    \"_id\": _id,\n    \"comments.id\": {\n        \"$in\": [comment_id, PyObjectId(comment_id)]\n    },\n}\n`\nIf we assume (seems reasonable here, but we should be careful) that `\"id\"` is unique in the `\"comments\"` array, then `\"arrayFilters\"` isn't necessary either.  So `update` may be simplified to:\n`update = {\n    \"$set\": {\n        \"comments.$.answer\": jsonable_encoder(reply)\n    }\n}\n`\nHere `$` uses the element \"match\" from `query` and replaces the first, and only the first, matching element in `\"comments\"` for the `update`.  If more than one element of the `\"comments\"` array should be updated, this won't work (see below use of `\"arrayFilters\"` if more than one element should be updated because `\"id\"` is not unique).\nSo, putting it all together with the assumptions we've made:\n`query = {\n    \"_id\": _id,\n    \"comments.id\": {\n        \"$in\": [comment_id, PyObjectId(comment_id)]\n    },\n}\n\nupdate = {\n    \"$set\": {\n        \"comments.$.answer\": jsonable_encoder(reply)\n    }\n}\n\n# possibly some other code here from the original app ...\n\nawait db_collection.update_one(\n        filter=query, update=update\n)\n`\nSee how it works with made-up documents on mongoplayground.net.  Only one element is updated, even though there are duplicate `\"id\"` values.\nWith `\"arrayFilters\"`\nIf `\"id\"` is not unique within the `\"comments\"` array and multiple objects within the array should be updated, then `\"arrayFilters\"` can be used to do this.\n`query = {\n    \"_id\": _id,\n    \"comments.id\": {\n        \"$in\": [comment_id, PyObjectId(comment_id)]\n    },\n}\n\nupdate = {\n    \"$set\": {\n        \"comments.$[cmt].answer\": jsonable_encoder(reply)\n    }\n}\n\narray_filters = [\n    {\n        \"cmt.id\": {\n            \"$in\": [comment_id, PyObjectId(comment_id)]\n        }\n    }\n]\n\n# possibly some other code here from the original app ...\n\nawait db_collection.update_one(\n        filter=query, update=update, array_filters=array_filters\n)\n`\nHere, any `\"comments\"` array element that \"matches\" the `\"arrayFilter\"` will be updated.  See how it works on mongoplayground.net.  All elements that \"match\" the `\"arrayFilter\"` are updated.",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2023-02-27T17:45:57",
      "url": "https://stackoverflow.com/questions/75583269/update-a-specific-array-element-if-it-fulfils-at-least-one-of-several-conditions"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "pymongo",
      "question_id": 74405507,
      "title": "pymongo $lookup for two database. MongoDb Atlas",
      "problem": "It is pobbible to do a cross $lookup for different DB using pymongo?\nI found some solution in Atlas documentation, but it seems like it doen't work with pymongo, because from should be string (just a collection name). Can't find any other solutions that will work with pymongo\n```\n`db.getSiblingDb(\"sourceDB1\").orders.aggregate(\n  {\n    $lookup: {\n      from: { db: \"sourceDB2\", coll: \"catalog\" },\n      localField: \"item\",\n      foreignField: \"sku\",\n      as: \"inventory_docs\"\n    }\n  }\n)\n`\n```\nThank everyone for helping.\nFould a lot of solution but it doen't work with pymongo",
      "solution": "Directly, the answer is \"No, this is not possible at the time of writing via `$lookup`\". The `$lookup` stage documentation mentions this directly several times (emphasis theirs):\n\nPerforms a left outer join to a collection in the same database to filter in documents from the \"joined\" collection for processing.\n\nOne possible solution would be to change the schema. This could include keeping the two collections in the same database, for example.\nNow not to confuse things too much, but MongoDB also offers something called Data Federation in Atlas. This is notable because the `$lookup` implementation in that context (documented here) doesn't have the same restrictions (emphasis added):\n\nIn federated database instance, you can use $lookup to join sharded and unsharded collections from the same database or different databases from Atlas, AWS S3, and HTTP or HTTPS data stores.\n\nThere are important considerations here as it relates to ease of management and performance that should all be factored in as you decide on the approach that is most appropriate for your situation.\nI don't believe any of this is specific to PyMongo.",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2022-11-11T17:31:45",
      "url": "https://stackoverflow.com/questions/74405507/pymongo-lookup-for-two-database-mongodb-atlas"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "pymongo",
      "question_id": 73639954,
      "title": "Python MongoDB Motor dynamically create Indexes",
      "problem": "I am attempting to write a function which will async create Indexes for each of my collections dynamically. The struggle I am having is accessing the class instance of each of my collections, and then calling the object to create the index.\nI have tried:\n\ngetattr\nast.literal_eval\n\nNOTE: I am unable to use eval() for security reasons\nExample of how my class instance looks:\n`import ast\nfrom models.database_models import IndexedKeyModels\n\nclass Mongo:  # pylint: disable=too-many-instance-attributes\n    def __init__(self):\n        self.connection = self.get_set_connection() # Function to create MongoDB connection\n        self.db = self.connection.pal_db\n        self.test1 = self.db.test1\n        self.test2 = self.db.test2\n\n    async def set_indexes(self) -> None:\n\n        test_string: str = \"\"\n        if self.dev == \"True\":\n            test_string += \"test_\"\n        for name in IndexedKeyModels():\n            await ast.literal_eval(\n                f\"self.{test_string}{name[0]}.create_index([('{name[1]}', {ASCENDING})], background=True)\"\n            )\n`\nThe IndexedKeyModels looks like this:\n`class IndexedKeyModels(BaseModel):\n    \"\"\"\n    MongoDB Indexed Key names\n    \"\"\"\n\n    test1: str = \"Index1\"\n    test2: str = \"Index2\"\n`\nI would like to be able to dynamically create each Index for each collection without having to define each create_index for each collection we have.",
      "solution": "If you want to create an index on a specific collection and field, you can do this programatically with this construct:\n```\n`collectionname = 'coll1'\nfieldname = 'field1'\ndb[collectionname].create_index([(fieldname , pymongo.ASCENDING)])\n`\n```",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2022-09-07T20:14:47",
      "url": "https://stackoverflow.com/questions/73639954/python-mongodb-motor-dynamically-create-indexes"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "pymongo",
      "question_id": 72746147,
      "title": "mongodb does not use index for time series data",
      "problem": "I am new to MongoDB and NoSQL and I am trying to run queries on a huge data set (around 50 million documents)\nI am running the latest version of MongoDB using Docker on a Windows 10 host with 64 GB RAM.\nI am using `pymongo` to import the data and run queries and I also have `Mongo Express` running as a docker container to view the imported data.\nMy statement to create my time-series collection is:\n```\n`mydb.command('create', 'sensor_data', timeseries={\n    'timeField': 'collection_time', \n    'metaField': 'sensor' \n})\n`\n```\nEach document looks something like this:\n```\n`{\n    \"sensor\": { \"id\": 1, \"location\":\"Somewhere\"},\n    \"collection_time\": datetime.strptime(\"2022/01/01 01:23:45 PM\", '%Y/%m/%d %I:%M:%S %p'),\n    ...\n}\n`\n```\nI have been able to use `Mongo Express` to verify that the data has been loaded into MongoDB correctly.\nI then tried to run the following bit of code:\n```\n`res = mycol.find({\n    \"collection_time\": { \n        \"$gte\": datetime.strptime(\"2021/01/01 12:00:00 AM\", '%Y/%m/%d %I:%M:%S %p'), \n        \"$lte\": datetime.strptime(\"2022/02/01 12:00:00 AM\", '%Y/%m/%d %I:%M:%S %p') \n    }\n})\n`\n```\nBut this query takes a very long time to run.\nAfter running `res.explain()` , I can see that the operation is doing a `COLSCAN` and not using an index. I even tried manually creating an index on 'collection_time' but the query is still doing a COLSCAN.\nWhat am I missing?\nUpdate 1\nThanks to R2D2's solution, I have got this working for `find()`, but I can't get it to work for `aggregate()`\nHere is my code:\n```\n`res = mycol.aggregate([\n    { \n        \"$match\": {\n            \"collection_time\": { \n                \"$gte\": datetime.strptime(\"2021/01/01 12:00:00 AM\", '%Y/%m/%d %I:%M:%S %p'), \n                \"$lte\": datetime.strptime(\"2022/02/01 12:00:00 AM\", '%Y/%m/%d %I:%M:%S %p') \n            }\n        }\n    },\n    {\"$group\":\n        { \n            \"_id\": {\n                 \"year\" : { \"$year\" : \"$collection_time\" },        \n                \"month\" : { \"$month\" : \"$collection_time\" },        \n                \"day\" : { \"$dayOfMonth\" : \"$collection_time\" },\n            }, \n            \"count\":{ \"$sum\": 1}\n        }\n    }\n], {hint: \"collection_time_1\" })\n`\n```\nThis gives the error: `NameError: name 'hint' is not defined`\nPutting `hint` in quotes gives the error: `AttributeError: 'dict' object has no attribute '_txn_read_preference'`",
      "solution": "By default when you create Time Series collection it is effective for storing time series data , but there is no indexes created , you can create secondary indexes for Time Series collection to improve performance for queries and if query planner do not select some of the created indexes you can add hint() to the query with the index name ( you can get the indexes names with db.collection.getIndexes() )\nFor mongodb 3.6+ you can use hint also in aggregation framework as follow:\n```\n` db.collection.aggregate(pipeline, {hint: \"index_name\"})\n`\n```",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2022-06-24T17:15:45",
      "url": "https://stackoverflow.com/questions/72746147/mongodb-does-not-use-index-for-time-series-data"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "pymongo",
      "question_id": 71962069,
      "title": "Django pymongo search, sort, limit on inner array and count them",
      "problem": "I am learning Django with MongoDb and have a collection to search into. Here is a sample document in the collection:\n`{ \n    \"_id\": {    \"$oid\": \"62615907568ddfca4fef3a25\"  }, \n    \"word\": 'entropy, \n    \"count\": 4,\n    \"occurrence\": [\n        {\"book\": \"62615907568ddfca4fef3a23\",  \"year\": 1942, \"sentence\": 0 },\n        {\"book\": \"62615907568ddfca4fef3a23\",  \"year\": 1942, \"sentence\": 5 },\n        {\"book\": \"62615907568ddfca4fef3a75\",  \"year\": 1928, \"sentence\": 0 },\n        {\"book\": \"62615907568ddfca4fef3a90\",  \"year\": 1959, \"sentence\": 8 } \n    ]\n}\n`\nI want to retrieve the array elements of 'occurrence' field of a specific document (word):\n\nSorted by year\nWithin an year range\nLimited with offset\ncount the total results (without limit/offset)\n\nWhat I have done so far is:\n`offset= 0\nlimit= 10\nsearch= {'$and': [{\"_id\": word_id_obj, \"occurrence\": {\"$elemMatch\": {\"year\": {\"$gte\": 1940, \"$lte\": 1960}}}}]}\nword_docs= wordcollec.aggregate([\n{\"$match\": search},\n{\"$project\":\n    {\"occurrence\":\n        {\"$slice\": ['$occurrence', offset, limit]}\n    }\n},\n{\"$sort\": {'occurrence.year': -1}}\n])\n\n# Count total results\nrecnt= wordcollec.aggregate([{\"$match\": search}, {'$group' : {\"_id\": \"$_id\", \"sno\" : {\"$sum\" : {\"$size\": \"$occurrence\"}}}}])\n`\nThe year range, count are not working and I am unable to sort them. How can I rewrite my query for this?\nThanks in advance.",
      "solution": "Use `$unwind` then `$sort`\n```\n`db.collection.aggregate([\n  {\n    $match: {\n      _id: { \"$oid\": \"62615907568ddfca4fef3a25\" },\n      occurrence: { $elemMatch: { year: { $gte: 1940, $lte: 1960 } } }\n    }\n  },\n  {\n    $set: { totalWithoutLimitOrOffset: { $size: \"$occurrence\" } }\n  },\n  {\n    $unwind: \"$occurrence\"\n  },\n  {\n    $match: { \"occurrence.year\": { $gte: 1940, $lte: 1960 } }\n  },\n  {\n    $sort: { \"occurrence.year\": -1 }\n  },\n  {\n    $skip: 1\n  },\n  {\n    $limit: 2\n  },\n  {\n    $group: {\n      _id: \"$_id\",\n      word: { $first: \"$word\" },\n      count: { $first: \"$count\" },\n      totalWithoutLimitOrOffset: { $first: \"$totalWithoutLimitOrOffset\" },\n      occurrence: { $push: \"$occurrence\" }\n    }\n  }\n])\n`\n```\nmongoplayground",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2022-04-22T02:05:32",
      "url": "https://stackoverflow.com/questions/71962069/django-pymongo-search-sort-limit-on-inner-array-and-count-them"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "pymongo",
      "question_id": 71716930,
      "title": "MongoDB: Upsert with array filter",
      "problem": "I have collection like this:\n`mc.db.collection.insert_many([\n    {\"key_array\": [1], \"another_array\": [\"a\"]},\n    {\"key_array\": [2, 3], \"another_array\": [\"b\"]},\n    {\"key_array\": [4], \"another_array\": [\"c\", \"d\"]},    \n])\n`\nAnd I'm using this kind of updates:\n`mc.db.collection.update_one(\n    {\"key_array\": 5},\n    {\"$addToSet\": {\"another_array\": \"f\"}},\n    upsert=True\n)\n`\nIt works good with updates, but I have trouble when trying to upsert:\nIt creates a document with a non-array `key_array` field, like this\n```\n`{\n    \"_id\": ObjectId(...)\n    \"key_array\": 5,\n    \"another_array\": [\"f\"]\n}\n`\n```\nwhile I want to have this one\n```\n`{\n    \"_id\": ObjectId(...)\n    \"key_array\": [5],\n    \"another_array\": [\"f\"]\n}\n`\n```\nAlso, I cannot use the `{\"key_array\": [5]}` style query, because it won't match the existing array with length > 1.\nSo, is there any chance to save such behavior on updates, and receive the correct document structure on inserts?\nAny help will be appreciated",
      "solution": "This should help.\nhttps://www.mongodb.com/docs/manual/reference/operator/update/setOnInsert/\n```\n`mc.db.collection.update_one(\n    {\"key_array\": 5},\n    {\n        \"$addToSet\": {\"another_array\": \"f\"},\n        \"$setOnInsert\": {\"key_array\": [5], ...}\n    },\n    upsert=True\n)\n`\n```",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2022-04-02T12:57:41",
      "url": "https://stackoverflow.com/questions/71716930/mongodb-upsert-with-array-filter"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "pymongo",
      "question_id": 71443625,
      "title": "Retrieving data from mongodb stored by pymongo",
      "problem": "I have uploaded data to `MongoDB` by `pymongo` and I want to retrieve it in my `nodeJs`. I am writing `function` like this but it is not working. my `collection` name is `linux_trace` and my `database` name is `Linux_Trace_db`.\n\nThe error is `linux_trace` is not defined\n\n```\n`const mongoose = require(\"mongoose\")\nrequire('dotenv').config();\nconst URI = process.env.MONGO_URL;\nmongoose.connect(\n  URI,\n \n  (err) => {\n    if (err) throw err;\n    console.log('Connected to mongodb');\n  }\n);\n\nlinux_trace.find(function (err, adminLogins) {\n    if (err) return console.error(err);\n    console.log(adminLogins)})\n`\n```",
      "solution": "The issue with  your code is that you didn't define `linux_trace` as a variable in javascript.\nTo get access to a model in a mongo database that already has a collection, you can run something like this\n```\n`const query = function (err, adminLogins) {\n  if (err) return console.error(err);\n  console.log(adminLogins)};\nmongoose.connection.db.collection('linux_trace', function (err, collection) {\n       collection.find(query).toArray(cb);\n   });\n`\n```\nI got this from this answer: https://stackoverflow.com/a/6721306/3173748",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2022-03-11T20:15:55",
      "url": "https://stackoverflow.com/questions/71443625/retrieving-data-from-mongodb-stored-by-pymongo"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "pymongo",
      "question_id": 71231656,
      "title": "Pymongo update a document with a condition",
      "problem": "I'm trying to update a document under a condition with pymongo, but failed to figure out a way. Use this document as an example:\n```\n`{\"_id\":\"mol001\",\n\"deid\":[\"a001\", \"a003\", \"a005\"],\n\"count\":3}\n`\n```\nFor _id of mol001, I'm trying to append a tag to deid and update count to 4:\n```\n`{\"_id\":\"mol001\",\n\"deid\":[\"a001\", \"a003\", \"a005\", \"b001\"],\n\"count\":4}\n\n`\n```\nOne thing needs to be aware of is the count value. if It's larger than 10, the document will not be updated. Below is what I came up with:\n```\n`    mol = \"mol001\"\n    b001 = \"b001\"\n    try: \n        ## in case mol001 doesn't exist, use upset = True\n        count = coll.find_one({\"_id\": mol}, {\"_id\": False, \"count\": 1})['count']\n    except:\n        count = 0\n    if count This was very inefficient since it needs to do one query and update twice. Is there a way to  use $cond to do the update in one sentence?",
      "solution": "Here's one way to do it.\n`db.collection.update({\n  \"_id\": \"mol001\",\n  \"count\": {\n    \"$lte\": 10\n  }\n},\n{\n  \"$push\": {\n    \"deid\": \"b001\"\n  },\n  \"$inc\": {\n    \"count\": 1\n  }\n},\n{\n  \"upsert\": true\n})\n`\nTry it on mongoplayground.net.",
      "question_score": 1,
      "answer_score": 1,
      "created_at": "2022-02-23T06:11:36",
      "url": "https://stackoverflow.com/questions/71231656/pymongo-update-a-document-with-a-condition"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "pymongo",
      "question_id": 71190714,
      "title": "How to storage an image in MongoDB with pymongo?",
      "problem": "I'm doing a simple CRUD with FastAPI to store an image and its information in MongoDB, so far this is my code:\n```\n`@app.post(\"/postImage\")\nasync def image_post(id: str, author: str, title: str, file: UploadFile = File(...)):\n    \n    file.filename = f\"{uuid.uuid4()}.jpg\"\n    contents = await file.read()\n\n    image = base64.b64decode(contents)\n \n    data = {\"_id\": id, \"author\": author, \"title\": title ,\"data\": image, \"posted\": False,\"uploaded\": datetime.now()}\n\n    try:\n      collection.insert_one(data)\n    except:\n        print('Error')\n\n    return(\"Added successfully\")\n`\n```\nHowever, when I'm trying to convert the bytes to string it returns me an error.\n```\n`image = base64.b64decode(contents)\nprint(image.decode('utf-8'))\n\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xb0 in position 11: invalid start byte\n`\n```\nI've tried many alternatives, but I don't know what's wrong.",
      "solution": "Mongo suggest this to store files bigger than 16mb (GridFS)\nI suggest you do this:\nFirst create directory in your project named \"static\" then do this\n```\n`import uuid\nfrom fastapi import FastAPI\nfrom fastapi.staticfiles import StaticFiles\nfrom fastapi import UploadFile, File, status\nimport aiofiles\n\napp = FastAPI()\n\napp.mount(\"/static\", StaticFiles(directory=\"static\"), name=\"static\")\n\n@app.post(\"/\")\nasync def post_endpoint(in_file: UploadFile = File(...)):\n    random_name = uuid.uuid4()\n    async with aiofiles.open(f\"static/{random_name}.jpg\", \"wb\") as out_file:\n        while content := await in_file.read(1024):  # async read chunk\n            await out_file.write(content)\n\n    return {\"Result\": \"OK\"}\n`\n```\nand then save your file name in db like string.\nfor get image you can use this :\nhttp://127.0.0.1:8000/static/uuiduuiduuiduuiduuiduuiduuid.jpg",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2022-02-20T02:39:42",
      "url": "https://stackoverflow.com/questions/71190714/how-to-storage-an-image-in-mongodb-with-pymongo"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "pymongo",
      "question_id": 70541480,
      "title": "pymongo not being installed in airflow docker container",
      "problem": "when i run airflow UI and refresh it for refreshing the DAG, the error comes which says no module pymongo. So i have tried to put it in requirements.txt and use that by volumes. Also i have installed it in cli. But the error is not going.\nIs there a proper way we can use the etl technique using python on the DAG.\n```\n`pip install -r requirements.txt\npip install pymongo\n\nimport pymongo\nfrom pymongo import MongoClient\n\nclient = pymongo.MongoClient(---)\n`\n```\nKindly need suggestions on it.",
      "solution": "You need to build your own Airlfow image with extra dependencies if the ones installed by default are not enough for you.\nThis is described in detail in the docuemntation - with plenty of examples and explanation on when and how you should build the image\nhttps://airflow.apache.org/docs/docker-stack/build.html",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2021-12-31T11:47:29",
      "url": "https://stackoverflow.com/questions/70541480/pymongo-not-being-installed-in-airflow-docker-container"
    },
    {
      "tech": "mongodb",
      "source": "stackoverflow",
      "tag": "pymongo",
      "question_id": 70165044,
      "title": "Auto increment pymongo",
      "problem": "I am trying to auto increment a field in my mongo collection. The field is an 'id' field and it contains the 'id' of each document. For example. 1, 2, 3 etc.\nWhat I want to happen is insert a new document and take the 'id' from the last document and add 1 to it so that the new document is lastID + 1.\nThe way I have written the code makes it so that it gets the last document and adds 1 to the last document and then updates it. So if the last id is 5, then the new document will have 5 and the document that I was incrementing on now has the new 'id' of 6.\nI am not sure how to get round this so any help would be appreciated.\nCode\n```\n`last_id = pokemons.find_one({}, sort=[( 'id', -1)])\n\nlast_pokemon = pokemons.find_one_and_update({'id' : last_id['id']}, {'$inc': {'id': 1}}, sort=[( 'id', -1)]) \n\nnew_pokemon = {\n              \"name\" : name, \"avg_spawns\" : avg_spawns, \"candy\" : candy, \"img\" : img_link, \"weaknesses\" : [], \"type\" : [], \"candy_count\" : candy_count, \n              \"egg\" : egg, \"height\" : height, \"multipliers\" : [], \"next_evolution\" : [], \"prev_evolution\" : [],\n              \"spawn_chance\" : spawn_chance, \"spawn_time\" : spawn_time, \"weight\" : weight, \"id\" : last_pokemon['id'], \"num\" : last_pokemon['id'],\n}\n\npokemons.insert_one(new_pokemon)\n                 \n`\n```\nThe variables in new_pokemon don't matter as I am just having issues with the last_pokemon part",
      "solution": "The `find_one` command in MongoDB command doesn't support sort functionality. You have to make use of normal find command with limit parameter set to `1`.\n```\n`last_id = pokemons.find({}, {\"id\": 1}, sort=[('id', -1)]).limit(1).next()  # Will error if there are no documents in collection due to the usage of `next()`\n\nlast_id[\"id\"] += 1\n\nnew_pokemon = {\n              \"name\" : name, \"avg_spawns\" : avg_spawns, \"candy\" : candy, \"img\" : img_link, \"weaknesses\" : [], \"type\" : [], \"candy_count\" : candy_count, \n              \"egg\" : egg, \"height\" : height, \"multipliers\" : [], \"next_evolution\" : [], \"prev_evolution\" : [],\n              \"spawn_chance\" : spawn_chance, \"spawn_time\" : spawn_time, \"weight\" : weight, \"id\" : last_id['id'], \"num\" : last_id['id'],\n}\n\npokemons.insert_one(new_pokemon)\n`\n```",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2021-11-30T07:08:26",
      "url": "https://stackoverflow.com/questions/70165044/auto-increment-pymongo"
    }
  ]
}