{
  "tech": "kubernetes",
  "count": 269,
  "examples": [
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubernetes",
      "question_id": 76841889,
      "title": "Kubectl error: memcache.go:265] couldn\u2019t get current server API group list: Get",
      "problem": "Everything was running smoothly with my Kubernetes clusters until today: after performing an update on my Ubuntu system, I'm now unable to establish a connection from my working environment to the kubernetes clusters.\nWhen executing the command `kubectl get pods`, I'm encountering the following error message:\n`E0805 09:59:45.750534  234576 memcache.go:265] couldn\u2019t get current server API group list: Get \"http://localhost:3334/api?timeout=32s\": EOF`\nHere are the details of my cluster setup: Kubernetes 1.27, bare-metal, Host System is Ubuntu 20.04",
      "solution": "Try\n`kubectl get nodes -v=10\n`\nand look for the errors.",
      "question_score": 67,
      "answer_score": 66,
      "created_at": "2023-08-05T15:33:19",
      "url": "https://stackoverflow.com/questions/76841889/kubectl-error-memcache-go265-couldn-t-get-current-server-api-group-list-get"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubernetes",
      "question_id": 72070748,
      "title": "Failed to load module script: Expected a JavaScript module script but the server responded with a MIME type of &quot;text/html&quot;",
      "problem": "I deployed my angular application(Static webpage) on kubernetes and tried launching it from Google Chrome. I see app is loading, however there is nothing displayed on the browser. Upon checking on browser console , I could see this error\n\"Failed to load module script: Expected a JavaScript module script but the server responded with a MIME type of \"text/html\". Strict MIME type checking is enforced for module scripts per HTML spec.\"\nfor (main.js,poylfill.js,runtime.js) files . I research few forums and one possible rootcause could because of `type` attribute in `` tag should be type=text/javascript instead of `type=module` in my index.html file that is produced under dist folder after executing ng build. I don't how to make that change as to these tags as generated during the build process, and my ng-build command is taken care by a docker command.\nURL i'm trying to access will be something like : \"http://xxxx:portnum/issuertcoetools\nnote: The host `xxxx:portnum` will be used by many other apps as well.\nAre there any work-arounds or solutions to this issue?\nindex.html - produced after running ng-build in local, (which is the same i see in kubernetes POD too)\n```\n`\n    \n    Data Generator\n    \n    \n    \n    \n    @font-face{font-family:'Roboto';font-style:normal;font-weight:300;font-display:swap;src:url(https://fonts.gstatic.com/s/roboto/v29/KFOlCnqEu92Fr1MmSU5fCRc4AMP6lbBP.woff2) format('woff2');unicode-range:U+0460-052F, U+1C80-1C88, U+20B4, U+2DE0-2DFF, U+A640-A69F, U+FE2E-FE2F;}@font-face{font-family:'Roboto';font-style:normal;font-weight:300;font-display:swap;src:url(https://fonts.gstatic.com/s/roboto/v29/KFOlCnqEu92Fr1MmSU5fABc4AMP6lbBP.woff2) format('woff2');unicode-range:U+0400-045F, U+0490-0491, U+04B0-04B1, U+2116;}@font-face{font-family:'Roboto';font-style:normal;font-weight:300;font-display:swap;src:url(https://fonts.gstatic.com/s/roboto/v29/KFOlCnqEu92Fr1MmSU5fCBc4AMP6lbBP.woff2) format('woff2');unicode-range:U+1F00-1FFF;}@font-face{font-family:'Roboto';font-style:normal;font-weight:300;font-display:swap;src:url(https://fonts.gstatic.com/s/roboto/v29/KFOlCnqEu92Fr1MmSU5fBxc4AMP6lbBP.woff2) format('woff2');unicode-range:U+0370-03FF;}@font-face{font-family:'Roboto';font-style:normal;font-weight:300;font-display:swap;src:url(https://fonts.gstatic.com/s/roboto/v29/KFOlCnqEu92Fr1MmSU5fCxc4AMP6lbBP.woff2) format('woff2');unicode-range:U+0102-0103, U+0110-0111, U+0128-0129, U+0168-0169, U+01A0-01A1, U+01AF-01B0, U+1EA0-1EF9, U+20AB;}@font-face{font-family:'Roboto';font-style:normal;font-weight:300;font-display:swap;src:url(https://fonts.gstatic.com/s/roboto/v29/KFOlCnqEu92Fr1MmSU5fChc4AMP6lbBP.woff2) format('woff2');unicode-range:U+0100-024F, U+0259, U+1E00-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF;}@font-face{font-family:'Roboto';font-style:normal;font-weight:300;font-display:swap;src:url(https://fonts.gstatic.com/s/roboto/v29/KFOlCnqEu92Fr1MmSU5fBBc4AMP6lQ.woff2) format('woff2');unicode-range:U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;}@font-face{font-family:'Roboto';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/roboto/v29/KFOmCnqEu92Fr1Mu72xKKTU1Kvnz.woff2) format('woff2');unicode-range:U+0460-052F, U+1C80-1C88, U+20B4, U+2DE0-2DFF, U+A640-A69F, U+FE2E-FE2F;}@font-face{font-family:'Roboto';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/roboto/v29/KFOmCnqEu92Fr1Mu5mxKKTU1Kvnz.woff2) format('woff2');unicode-range:U+0400-045F, U+0490-0491, U+04B0-04B1, U+2116;}@font-face{font-family:'Roboto';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/roboto/v29/KFOmCnqEu92Fr1Mu7mxKKTU1Kvnz.woff2) format('woff2');unicode-range:U+1F00-1FFF;}@font-face{font-family:'Roboto';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/roboto/v29/KFOmCnqEu92Fr1Mu4WxKKTU1Kvnz.woff2) format('woff2');unicode-range:U+0370-03FF;}@font-face{font-family:'Roboto';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/roboto/v29/KFOmCnqEu92Fr1Mu7WxKKTU1Kvnz.woff2) format('woff2');unicode-range:U+0102-0103, U+0110-0111, U+0128-0129, U+0168-0169, U+01A0-01A1, U+01AF-01B0, U+1EA0-1EF9, U+20AB;}@font-face{font-family:'Roboto';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/roboto/v29/KFOmCnqEu92Fr1Mu7GxKKTU1Kvnz.woff2) format('woff2');unicode-range:U+0100-024F, U+0259, U+1E00-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF;}@font-face{font-family:'Roboto';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/roboto/v29/KFOmCnqEu92Fr1Mu4mxKKTU1Kg.woff2) format('woff2');unicode-range:U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;}@font-face{font-family:'Roboto';font-style:normal;font-weight:500;font-display:swap;src:url(https://fonts.gstatic.com/s/roboto/v29/KFOlCnqEu92Fr1MmEU9fCRc4AMP6lbBP.woff2) format('woff2');unicode-range:U+0460-052F, U+1C80-1C88, U+20B4, U+2DE0-2DFF, U+A640-A69F, U+FE2E-FE2F;}@font-face{font-family:'Roboto';font-style:normal;font-weight:500;font-display:swap;src:url(https://fonts.gstatic.com/s/roboto/v29/KFOlCnqEu92Fr1MmEU9fABc4AMP6lbBP.woff2) format('woff2');unicode-range:U+0400-045F, U+0490-0491, U+04B0-04B1, U+2116;}@font-face{font-family:'Roboto';font-style:normal;font-weight:500;font-display:swap;src:url(https://fonts.gstatic.com/s/roboto/v29/KFOlCnqEu92Fr1MmEU9fCBc4AMP6lbBP.woff2) format('woff2');unicode-range:U+1F00-1FFF;}@font-face{font-family:'Roboto';font-style:normal;font-weight:500;font-display:swap;src:url(https://fonts.gstatic.com/s/roboto/v29/KFOlCnqEu92Fr1MmEU9fBxc4AMP6lbBP.woff2) format('woff2');unicode-range:U+0370-03FF;}@font-face{font-family:'Roboto';font-style:normal;font-weight:500;font-display:swap;src:url(https://fonts.gstatic.com/s/roboto/v29/KFOlCnqEu92Fr1MmEU9fCxc4AMP6lbBP.woff2) format('woff2');unicode-range:U+0102-0103, U+0110-0111, U+0128-0129, U+0168-0169, U+01A0-01A1, U+01AF-01B0, U+1EA0-1EF9, U+20AB;}@font-face{font-family:'Roboto';font-style:normal;font-weight:500;font-display:swap;src:url(https://fonts.gstatic.com/s/roboto/v29/KFOlCnqEu92Fr1MmEU9fChc4AMP6lbBP.woff2) format('woff2');unicode-range:U+0100-024F, U+0259, U+1E00-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF;}@font-face{font-family:'Roboto';font-style:normal;font-weight:500;font-display:swap;src:url(https://fonts.gstatic.com/s/roboto/v29/KFOlCnqEu92Fr1MmEU9fBBc4AMP6lQ.woff2) format('woff2');unicode-range:U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;}\n    @font-face{font-family:'Material Icons';font-style:normal;font-weight:400;src:url(https://fonts.gstatic.com/s/materialicons/v128/flUhRq6tzZclQEJ-Vdg-IuiaDsNcIhQ8tQ.woff2) format('woff2');}.material-icons{font-family:'Material Icons';font-weight:normal;font-style:normal;font-size:24px;line-height:1;letter-spacing:normal;text-transform:none;display:inline-block;white-space:nowrap;word-wrap:normal;direction:ltr;-webkit-font-feature-settings:'liga';-webkit-font-smoothing:antialiased;}\n.mat-typography{font:400 14px/20px Roboto,Helvetica Neue,sans-serif;letter-spacing:normal}html,body{height:100%}body{margin:0;font-family:Roboto,Helvetica Neue,sans-serif}\n\n    \n \n\n`\n```\nnginx.conf file\n```\n`worker_processes 4;\n\nevents { worker_connections 1024; }\n\nhttp {\n    server {\n        listen 8080;\n        include /etc/nginx/mime.types;\n\n  location /issuertcoetools {\n    root /usr/share/nginx/html;\n    index index.html index.htm;\n    try_files $uri $uri/ /index.html =404;\n  }\n}\n}\n\n`\n```",
      "solution": "This error usually happens because you deployment was into a subfolder, so it seems like Angular is fetching you app directly from your base URL, so your html is found when you go to your `domain.com/mysubfolder/index.html`, but as the Angular fetches your resources from `domain.com/index.html` instead of domain.com/mysubfolder/index.html; I\u2019m pretty sure this is the cause of your issue. You can resolve it building your app with:\n```\n`ng build --prod --base-href mysubfolder\n`\n```",
      "question_score": 60,
      "answer_score": 36,
      "created_at": "2022-04-30T19:39:03",
      "url": "https://stackoverflow.com/questions/72070748/failed-to-load-module-script-expected-a-javascript-module-script-but-the-server"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubernetes",
      "question_id": 75406313,
      "title": "couldn&#39;t get current server API group list: the server has asked for the client to provide credentials error: You must be logged in to the server",
      "problem": "I created the eks cluster trying to connect it with local cli, for that, I installed the aws-cli and also provide the right 'aws configure' credentials. The user which I am using to connect with the aws have the EKS related policy. Still I am getting the following Error ...\n```\n`E0209 21:09:44.893284 2465691 memcache.go:238] couldn't get current server API group list: the server has asked for the client to provide credentials\nE0209 21:09:45.571635 2465691 memcache.go:238] couldn't get current server API group list: the server has asked for the client to provide credentials\nE0209 21:09:46.380542 2465691 memcache.go:238] couldn't get current server API group list: the server has asked for the client to provide credentials\nE0209 21:09:47.105407 2465691 memcache.go:238] couldn't get current server API group list: the server has asked for the client to provide credentials\nE0209 21:09:47.869614 2465691 memcache.go:238] couldn't get current server API group list: the server has asked for the client to provide credentials\nerror: You must be logged in to the server (the server has asked for the client to provide credentials)\n`\n```",
      "solution": "Well in my case, the aws keys with which I created the cluster and with which I configured the kubectl were different. The two of them were different aws identities.",
      "question_score": 55,
      "answer_score": 17,
      "created_at": "2023-02-10T03:13:34",
      "url": "https://stackoverflow.com/questions/75406313/couldnt-get-current-server-api-group-list-the-server-has-asked-for-the-client"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubernetes",
      "question_id": 67624630,
      "title": "Unable to Copy data from POD to local using kubectl cp command",
      "problem": "I need to copy dump data from pod to local. Below the commands I am trying but I am getting error: `unexpected EOF`\n```\n`kubectl cp device-database-79fc964c8-q7ncc:tmp /Users/raja\nerror: unexpected EOF\n\nor\n\nkubectl cp device-database-79fc964c8-q7ncc:tmp/plsql_data/prod.dump /Users/raja/prod.dump\nerror: unexpected EOF\n\n`\n```\nkubectl version\n```\n`kubectl version --client\nClient Version: version.Info{Major:\"1\", Minor:\"21\", GitVersion:\"v1.21.0\", GitCommit:\"cb303e613a121a29364f75cc67d3d580833a7479\", GitTreeState:\"clean\", BuildDate:\"2021-04-08T16:31:21Z\", GoVersion:\"go1.16.1\", Compiler:\"gc\", Platform:\"darwin/amd64\"}\n\n`\n```\nCan anyone help how to fix this issue?\nThanks",
      "solution": "Using a \"cat\" command, rather than \"cp\" worked for me -- but only after 3 attempts.\nThe \"cp\" command failed over and over to get the whole file.\nThis \"cat\" style command did better each time.\nSo try this and see if your odds improve!\n```\n`kubectl exec -i [pod name] -c [container name] -- cat [path to file] > [output file]\n`\n```",
      "question_score": 50,
      "answer_score": 18,
      "created_at": "2021-05-20T18:41:49",
      "url": "https://stackoverflow.com/questions/67624630/unable-to-copy-data-from-pod-to-local-using-kubectl-cp-command"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubernetes",
      "question_id": 74233349,
      "title": "How do I install gke-gcloud-auth-plugin on a Mac M1 with zsh",
      "problem": "I try to install `gke-gcloud-auth-plugin` on a Mac M1 with zsh, following the gcloud docs.\nThe installation ran without issue and trying to re-run `gcloud components install gke-gcloud-auth-plugin` I get the `All components are up to date.` message.\nHowever, `gke-gcloud-auth-plugin --version` returns `zsh: command not found: gke-gcloud-auth-plugin`. `kubectl`, installed the same way, works properly.\nI tried to install `kubectl` using `brew`, with no more success.",
      "solution": "Not sure if it is the same on macOS. Can you try the following:\n`export USE_GKE_GCLOUD_AUTH_PLUGIN=True\n`\nThen reload the Cluster with\n`gcloud container clusters get-credentials clustername\n`\nGuess it is installed but just not used.\nMaybe you just need to add the directory where to find your `gke-gcloud-auth-plugin` file to your `PATH`.\nIs it working when you call it wirh absolute path?\n`path/to/gke-gcloud-auth-plugin --version\n`\nto find the file use the following command:\n```\n`sudo find / -name gke-gcloud-auth-plugin\n`\n```",
      "question_score": 50,
      "answer_score": 12,
      "created_at": "2022-10-28T11:34:15",
      "url": "https://stackoverflow.com/questions/74233349/how-do-i-install-gke-gcloud-auth-plugin-on-a-mac-m1-with-zsh"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubernetes",
      "question_id": 70464815,
      "title": "Cannot install kubernetes helm chart Error: cannot re-use a name that is still in use",
      "problem": "Cannot install the helm chart but when I use raw file generated by helm, I am able to install via kubectl apply.\nFollowing error is displayed when i use `helm install myChart . --debug`\n```\n`Error: cannot re-use a name that is still in use\nhelm.go:88: [debug] cannot re-use a name that is still in use\nhelm.sh/helm/v3/pkg/action.(*Install).availableName\n        helm.sh/helm/v3/pkg/action/install.go:442\nhelm.sh/helm/v3/pkg/action.(*Install).Run\n        helm.sh/helm/v3/pkg/action/install.go:185\nmain.runInstall\n        helm.sh/helm/v3/cmd/helm/install.go:242\nmain.newInstallCmd.func2\n        helm.sh/helm/v3/cmd/helm/install.go:120\ngithub.com/spf13/cobra.(*Command).execute\n        github.com/spf13/cobra@v1.1.3/command.go:852\ngithub.com/spf13/cobra.(*Command).ExecuteC\n        github.com/spf13/cobra@v1.1.3/command.go:960\ngithub.com/spf13/cobra.(*Command).Execute\n        github.com/spf13/cobra@v1.1.3/command.go:897\nmain.main\n        helm.sh/helm/v3/cmd/helm/helm.go:87\nruntime.main\n        runtime/proc.go:225\nruntime.goexit\n        runtime/asm_amd64.s:1371\n`\n```\nInstalling raw file generated by helm with the following command works great but when I run `helm install myChart .` it gives the above error\n```\n`helm install myChart . --dry-run > myChart.yaml\nkubectl apply -f myChart.yaml\n`\n```",
      "solution": "Use upgrade instead install:\n```\n`helm upgrade -i myChart .\n`\n```\nThe `-i` flag install the release if it doesn't exist.",
      "question_score": 47,
      "answer_score": 79,
      "created_at": "2021-12-23T17:23:50",
      "url": "https://stackoverflow.com/questions/70464815/cannot-install-kubernetes-helm-chart-error-cannot-re-use-a-name-that-is-still-i"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubernetes",
      "question_id": 70953842,
      "title": "Error: wsl.exe exited with code 4294967295 on Installing Rancher Desktop",
      "problem": "I am getting the below error when installing the latest stable Rancher Desktop in my Virtual Machine.\nCould someone please help?\nError:\n\nError: wsl.exe exited with code 4294967295\n\nCommand:\n```\n`wsl --distribution rancher-desktop --exec mkdir -p /mnt/wsl/rancher-desktop/run/data\n`\n```\nLogs:\n```\n`2022-02-02T09:58:39.490Z: Running command wsl --distribution rancher-desktop --exec wslpath -a -u C:\\Users\\VIVEK~1.NUN\\AppData\\Local\\Temp\\rd-distro-gGd3SG\\distro.tar...\n2022-02-02T09:58:40.641Z: Running command wsl --distribution rancher-desktop --exec tar -cf /mnt/c/Users/VIVEK~1.NUN/AppData/Local/Temp/rd-distro-gGd3SG/distro.tar -C / /bin/busybox /bin/mount /bin/sh /lib /etc/wsl.conf /etc/passwd /etc/rancher /var/lib...\n2022-02-02T09:58:42.628Z: Running command wsl --import rancher-desktop-data C:\\Users\\Vivek.Nuna\\AppData\\Local\\rancher-desktop\\distro-data C:\\Users\\VIVEK~1.NUN\\AppData\\Local\\Temp\\rd-distro-gGd3SG\\distro.tar --version 2...\n2022-02-02T09:58:44.025Z: Running command wsl --distribution rancher-desktop-data --exec /bin/busybox [ ! -d /etc/rancher ]...\n2022-02-02T09:58:44.025Z: Running command wsl --distribution rancher-desktop-data --exec /bin/busybox [ ! -d /var/lib ]...\n2022-02-02T10:03:54.533Z: Running command wsl --terminate rancher-desktop...\n2022-02-02T10:03:54.534Z: Running command wsl --terminate rancher-desktop-data...\n2022-02-02T10:03:54.971Z: Running command wsl --distribution rancher-desktop --exec mkdir -p /mnt/wsl/rancher-desktop/run/data...\n2022-02-02T10:04:03.418Z: WSL: executing: mkdir -p /mnt/wsl/rancher-desktop/run/data: Error: wsl.exe exited with code 4294967295\n`\n```",
      "solution": "I resolved the issue by following steps 4 and 5 here:\nhttps://learn.microsoft.com/en-us/windows/wsl/install-manual#step-4---download-the-linux-kernel-update-package\nI found this answer by looking in %USERPROFILE%\\AppData\\Local\\rancher-desktop\\logs\\wsl-exec.log which revealed:\n\"WSL 2 requires an update to its kernel component. For information please visit https://aka.ms/wsl2kernel\"",
      "question_score": 42,
      "answer_score": 17,
      "created_at": "2022-02-02T11:18:21",
      "url": "https://stackoverflow.com/questions/70953842/error-wsl-exe-exited-with-code-4294967295-on-installing-rancher-desktop"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubernetes",
      "question_id": 67746885,
      "title": "@prisma/client did not initialize yet. Please run &quot;prisma generate&quot; and try to import it again",
      "problem": "I am using prisma, postgres, docker, kubernets.\nnpx prisma migrate dev working.\nand npx prisma generate produce below output:\n```\n`\u2714 Generated Prisma Client (2.23.0) to ./node_modules/@prisma/client in 68ms\nYou can now start using Prisma Client in your code. Reference: https://pris.ly/d/client\n\nimport { PrismaClient } from '@prisma/client'\nconst prisma = new PrismaClient()\n`\n```\nbut when I tried to use in my route file produce the error:\nnew-route.ts\n```\n`import { PrismaClient } from '@prisma/client';\n\nconst prisma = new PrismaClient();\n`\n```\nmy docker file:\n```\n`FROM node:alpine\n\nWORKDIR /app\nCOPY package.json .\nRUN npm install --only=prod\nCOPY . .\n\nCMD [\"npm\", \"start\"]\n`\n```",
      "solution": "I usually don't use docker for this while developing, but I have this issue every time I change something in my `schema.prisma` and have to use `npx prisma generate`. The solution for me is to restart the node application running `npm start` again. Maybe if you restart your containers it might work.\nif you are inside kubernets pod then access the pod using terminal then give generate command\n```\n`kubectl exec -it pod_name sh\nnpx prisma generate\n`\n```",
      "question_score": 41,
      "answer_score": 14,
      "created_at": "2021-05-29T03:03:26",
      "url": "https://stackoverflow.com/questions/67746885/prisma-client-did-not-initialize-yet-please-run-prisma-generate-and-try-to-i"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubernetes",
      "question_id": 69517855,
      "title": "Microk8s dashboard using nginx-ingress via http not working (Error: `no matches for kind &quot;Ingress&quot; in version &quot;extensions/v1beta1&quot;`)",
      "problem": "I have microk8s v1.22.2 running on Ubuntu 20.04.3 LTS.\nOutput from `/etc/hosts`:\n```\n`127.0.0.1 localhost\n127.0.1.1 main\n`\n```\nExcerpt from `microk8s status`:\n```\n`addons:\n  enabled:\n    dashboard            # The Kubernetes dashboard\n    ha-cluster           # Configure high availability on the current node\n    ingress              # Ingress controller for external access\n    metrics-server       # K8s Metrics Server for API access to service metrics\n`\n```\nI checked for the running dashboard (`kubectl get all --all-namespaces`):\n```\n`NAMESPACE     NAME                                             READY   STATUS    RESTARTS   AGE\nkube-system   pod/calico-node-2jltr                            1/1     Running   0          23m\nkube-system   pod/calico-kube-controllers-f744bf684-d77hv      1/1     Running   0          23m\nkube-system   pod/metrics-server-85df567dd8-jd6gj              1/1     Running   0          22m\nkube-system   pod/kubernetes-dashboard-59699458b-pb5jb         1/1     Running   0          21m\nkube-system   pod/dashboard-metrics-scraper-58d4977855-94nsp   1/1     Running   0          21m\ningress       pod/nginx-ingress-microk8s-controller-qf5pm      1/1     Running   0          21m\n\nNAMESPACE     NAME                                TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE\ndefault       service/kubernetes                  ClusterIP   10.152.183.1             443/TCP    23m\nkube-system   service/metrics-server              ClusterIP   10.152.183.81            443/TCP    22m\nkube-system   service/kubernetes-dashboard        ClusterIP   10.152.183.103           443/TCP    22m\nkube-system   service/dashboard-metrics-scraper   ClusterIP   10.152.183.197           8000/TCP   22m\n\nNAMESPACE     NAME                                               DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE\nkube-system   daemonset.apps/calico-node                         1         1         1       1            1           kubernetes.io/os=linux   23m\ningress       daemonset.apps/nginx-ingress-microk8s-controller   1         1         1       1            1                              22m\n\nNAMESPACE     NAME                                        READY   UP-TO-DATE   AVAILABLE   AGE\nkube-system   deployment.apps/calico-kube-controllers     1/1     1            1           23m\nkube-system   deployment.apps/metrics-server              1/1     1            1           22m\nkube-system   deployment.apps/kubernetes-dashboard        1/1     1            1           22m\nkube-system   deployment.apps/dashboard-metrics-scraper   1/1     1            1           22m\n\nNAMESPACE     NAME                                                   DESIRED   CURRENT   READY   AGE\nkube-system   replicaset.apps/calico-kube-controllers-69d7f794d9     0         0         0       23m\nkube-system   replicaset.apps/calico-kube-controllers-f744bf684      1         1         1       23m\nkube-system   replicaset.apps/metrics-server-85df567dd8              1         1         1       22m\nkube-system   replicaset.apps/kubernetes-dashboard-59699458b         1         1         1       21m\nkube-system   replicaset.apps/dashboard-metrics-scraper-58d4977855   1         1         1       21m\n`\n```\nI want to expose the microk8s dashboard within my local network to access it through `http://main/dashboard/`\nTo do so, I did the following `nano ingress.yaml`:\n```\n`apiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  annotations:\n    kubernetes.io/ingress.class: public\n    nginx.ingress.kubernetes.io/backend-protocol: \"HTTPS\"\n  name: dashboard\n  namespace: kube-system\nspec:\n  rules:\n  - host: main\n    http:\n      paths:\n      - backend:\n          serviceName: kubernetes-dashboard\n          servicePort: 443\n        path: /\n`\n```\nEnabling the ingress-config through `kubectl apply -f ingress.yaml` gave the following error:\n```\n`error: unable to recognize \"ingress.yaml\": no matches for kind \"Ingress\" in version \"extensions/v1beta1\"\n`\n```\nHelp would be much appreciated, thanks!\nUpdate:\n@harsh-manvar pointed out a mismatch in the config version. I have rewritten ingress.yaml to a very stripped down version:\n```\n`apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: dashboard\n  namespace: kube-system\nspec:\n  rules:\n  - http:\n      paths:\n      - path: /dashboard\n        pathType: Prefix\n        backend:\n          service:\n            name: kubernetes-dashboard\n            port:\n              number: 443\n`\n```\nApplying this works. Also, the ingress rule gets created.\n```\n`NAMESPACE     NAME        CLASS    HOSTS   ADDRESS     PORTS   AGE\nkube-system   dashboard   public   *       127.0.0.1   80      11m\n`\n```\nHowever, when I access the dashboard through `http:///dashboard`, I get a `400` error.\nLog from the ingress controller:\n```\n`192.168.0.123 - - [10/Oct/2021:21:38:47 +0000] \"GET /dashboard HTTP/1.1\" 400 54 \"-\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.71 Safari/537.36\" 466 0.002 [kube-system-kubernetes-dashboard-443] [] 10.1.76.3:8443 48 0.000 400 ca0946230759edfbaaf9d94f3d5c959a\n`\n```\nDoes the dashboard also need to be exposed using the `microk8s proxy`? I thought the ingress controller would take care of this, or did I misunderstand this?",
      "solution": "To fix the error `error: unable to recognize \"ingress.yaml\": no matches for kind \"Ingress\" in version \"extensions/v1beta1` you need to set `apiVersion` to the ` networking.k8s.io/v1`. From the Kubernetes v1.16 article about deprecated APIs:\n\nNetworkPolicy in the  extensions/v1beta1  API version is no longer served\n-   Migrate to use the  networking.k8s.io/v1  API version, available since v1.8. Existing persisted data can be retrieved/updated via the new version.\n\nNow moving to the second issue. You need to add a few annotations and make few changes in your Ingress definition to make dashboard properly exposed on the microk8s cluster:\n\nadd `nginx.ingress.kubernetes.io/rewrite-target: /$2` annotation\nadd `nginx.ingress.kubernetes.io/configuration-snippet: | rewrite ^(/dashboard)$ $1/ redirect;` annotation\nchange `path: /dashboard` to `path: /dashboard(/|$)(.*)`\n\nWe need them to properly forward the request to the backend pods - good explanation in this article:\n\nNote: The \"nginx.ingress.kubernetes.io/rewrite-target\" annotation rewrites the URL before forwarding the request to the backend pods. In /dashboard(/|$)(.*) for path, (.*) stores the dynamic URL that's generated while accessing the Kubernetes Dashboard. The \"nginx.ingress.kubernetes.io/rewrite-target\" annotation replaces the captured data in the URL before forwarding the request to the kubernetes-dashboard service. The \"nginx.ingress.kubernetes.io/configuration-snippet\" annotation rewrites the URL to add a trailing slash (\"/\") only if ALB-URL/dashboard is accessed.\n\nAlso we need another two changes:\n\nadd `nginx.ingress.kubernetes.io/backend-protocol: \"HTTPS\"` annotation to tell NGINX Ingress to communicate with Dashboard service using HTTPs\nadd `kubernetes.io/ingress.class: public` annotation to use NGINX Ingress created by microk8s `ingress` plugin\n\nAfter implementing everything above, the final YAML file looks like this:\n`apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  annotations:\n    nginx.ingress.kubernetes.io/rewrite-target: /$2\n    nginx.ingress.kubernetes.io/configuration-snippet: |\n      rewrite ^(/dashboard)$ $1/ redirect;\n    nginx.ingress.kubernetes.io/backend-protocol: \"HTTPS\"\n    kubernetes.io/ingress.class: public\n  name: dashboard\n  namespace: kube-system\nspec:\n  rules:\n  - http:\n      paths:\n      - path: /dashboard(/|$)(.*)\n        pathType: Prefix\n        backend:\n          service:\n            name: kubernetes-dashboard\n            port:\n              number: 443\n`\nIt should work fine. No need to run `microk8s proxy` command.",
      "question_score": 38,
      "answer_score": 42,
      "created_at": "2021-10-10T20:23:07",
      "url": "https://stackoverflow.com/questions/69517855/microk8s-dashboard-using-nginx-ingress-via-http-not-working-error-no-matches"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubernetes",
      "question_id": 70787520,
      "title": "Your current user or role does not have access to Kubernetes objects on this EKS cluster",
      "problem": "Don't know if this is an error from AWS or something. I created an IAM user and gave it full admin policies. I then used this user to create an EKS cluster using the `eksctl` CLI but when I logging to AWS console with the root user I got the below error while trying to access the cluster nodes.\nYour current user or role does not have access to Kubernetes objects on this EKS cluster\nThis may be due to the current user or role not having Kubernetes RBAC permissions to describe cluster resources or not having an entry in the cluster\u2019s auth config map.\nI have these questions\n\nDoes not the root user have full access to view every resource from the console?\nIf the above is true, does it mean when I create a resource from the CLI I must login with the same user to view it?\nOr is there way I could attach policies to the root user? Didn't see anything like in the console.\n\nAWS itself does not recommend creating access keys for root user and using it for programmable access, so I'm so confused right now. Someone help\nAll questions I have seen so far and the link to the doc here are talking about a user or role created in the AWS IAM and not the root user.",
      "solution": "Note: aws-auth configmap is now depreciated. See my current answer below the old one.\nI had this issue today, and solved it by combining answers here. The aws-auth config after it worked looks like this:\n```\n`apiVersion: v1\ndata:\n  mapRoles: |\n    - groups:\n      - system:bootstrappers\n      - system:nodes\n      rolearn: arn:aws:iam::671177010163:role/eksctl-manu-eks-new2-nodegroup-ng-NodeInstanceRole-1NYUHVMYFP2TK\n      username: system:node:{{EC2PrivateDNSName}}\n  mapUsers: \"- groups: \\n  - system:masters\\n  userarn: arn:aws:iam::671177010163:root\\n\"\nkind: ConfigMap\nmetadata:\n  creationTimestamp: \"2022-02-13T11:03:30Z\"\n  name: aws-auth\n  namespace: kube-system\n  resourceVersion: \"11362\"\n  uid: ac36a1d9-76bc-40dc-95f0-b1e7934357\n`\n```\nNew Method:\nThe recommended way for IAM Access to EKS resources is now via Access Entries. These can be created on the AWS console, CLI.\nAWS Doc: https://docs.aws.amazon.com/eks/latest/userguide/access-entries.html\nhttps://github.com/aws/containers-roadmap/issues/185\nIn short:\n\nMake sure your EKS cluster shows EKS API as one of the authentication modes under Access tab.\nCreate an Access Entry in the same tab. Select Standard as type, enter the IAM User ARN. Leave username blank. Set the Access Policy as AmazonEKSClusterAdminPolicy if you want an IAM principal to have administrator access to all resources on your cluster.\n\nThis new approach solves many of the problems with using aws-auth.",
      "question_score": 35,
      "answer_score": 13,
      "created_at": "2022-01-20T14:58:16",
      "url": "https://stackoverflow.com/questions/70787520/your-current-user-or-role-does-not-have-access-to-kubernetes-objects-on-this-eks"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubernetes",
      "question_id": 74050419,
      "title": "Helm Release stuck in Uninstalling state",
      "problem": "I was trying to uninstall a helm release in my AKS cluster using `helm uninstall RELEASE_NAME` but seems like it failed. The failure happened because for some reason all of the nodes in my cluster went in a not ready state when I used `helm uninstall`.\nSince then, I got all the nodes to get back up and running and the cluster is functioning as it should. Now, when I do try `helm list`, I don't get to see this release present anymore, but doing a `helm list -a` shows me that the state of the release is still in `uninstalling` state. I have tried quite a few things, but it has been stuck there now for the last 3-4 days.\nA few things that I tried was to use `helm uninstall RELEASE_NAME`, `helm delete --purge RELEASE_NAME` but these commands throw an error because the release is already in `uninstalling` state. I tried modifying the helm secrets in the cluster for this specific release but that didn't help either. `Helm3` is being used so it is not like I can restart the tiller pod to maybe stabilize this.\nDoes `Helm3` use some kind of a finalizer mechanism which can be modified to rectify this or\nis there no alternate way in which I can perhaps try to delete this release? I want to use the same release name for the specific API in the future too.\nAny help will be really appreciated.",
      "solution": "Based on the discussion, the following steps resolve the issue.\n```\n`helm hist releasename\nhelm rollback releasename versionnumber-with-status-deployed\n`\n```\nif this did not help, then delete secret for each version\n```\n`helm hist releasename\nkubectl get secrets\nk delete secrets sh.helm.release.v1.name.VERSION-N\n`\n```",
      "question_score": 35,
      "answer_score": 54,
      "created_at": "2022-10-13T06:14:00",
      "url": "https://stackoverflow.com/questions/74050419/helm-release-stuck-in-uninstalling-state"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubernetes",
      "question_id": 66016567,
      "title": "how to uninstall minikube from ubuntu, i get an &#39;Unable to load cached images&#39; error",
      "problem": "how to completely uninstall minikube from ubuntu 20.04.\ni'm getting an error from my current minikube when starting :\n`minikube start `\ngets\n`\ud83d\udc33  Preparing Kubernetes v1.20.0 on Docker 20.10.0 ...| \u274c  Unable to load cached images: loading cached images: stat /home/feiz-nouri/.minikube/cache/images/gcr.io/k8s-minikube/storage-provisioner_v4: no such file or directory`",
      "solution": "how to completely uninstall minikube from ubuntu 20.04\n\nFirst, run `minikube delete` to remove minikube VM (or container if run with `docker` driver), virtual network interfaces configured on the host machine and all other traces of minikube cluster.\nOnly then you can safely remove its binary. The way how you should do it depends on how you've installed it, but as you can see here, there are not so many options.\nIf you've installed it by running:\n```\n`sudo install minikube-linux-amd64 /usr/local/bin/minikube\n`\n```\nyou can simply remove the binary from `/usr/local/bin/minikube` directory as what the above command basically does, is copying the binary to the destination directory. If it's installed in a different directory, you can always check it by running:\n```\n`which minikube\n`\n```\nIf it was installed using `dpkg` package manager:\n```\n`sudo dpkg -i minikube_latest_amd64.deb\n`\n```\nyou can search for it with the following command:\n```\n`dpkg -l | grep minikube\n`\n```\nIf it shows you something like:\n```\n`ii    minikube    1.17.1    amd64    Minikube\n`\n```\nyou can completely remove it (with all its configuration files) by running:\n```\n`sudo dpkg --purge minikube\n`\n```",
      "question_score": 34,
      "answer_score": 67,
      "created_at": "2021-02-02T20:42:00",
      "url": "https://stackoverflow.com/questions/66016567/how-to-uninstall-minikube-from-ubuntu-i-get-an-unable-to-load-cached-images-e"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubernetes",
      "question_id": 67520866,
      "title": "no matches for kind &quot;CronJob&quot; in version &quot;batch/v1&quot;",
      "problem": "I use Kubernetes which v1.19.7, when I run the CronJob sample\n`apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: express-learn-cronjob\nspec:\n  schedule: \"*/1 * * * *\"\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n            - name: hello\n              image: busybox\n              command:\n                - /bin/sh\n                - -c\n                - date; echo Hello from the Kubernetes cluster\n          restartPolicy: OnFailure\n`\nget  unable to recognize \"app-cronjob.yml\": no matches for kind \"CronJob\" in version \"batch/v1\"\nI can get the batch info by run kubectl api-versions | grep batch\n`batch/v1\nbatch/v1beta1\n`\nis there anything I missed? how can I fix it?",
      "solution": "For Kubernetes version 1.19.x you need to use `batch/v1beta1` as apiVersion for your CronJob.\nThat is documented in the doc version 1-19:\nhttps://v1-19.docs.kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/\nIt is stable only on k8s version 1.21.",
      "question_score": 30,
      "answer_score": 52,
      "created_at": "2021-05-13T16:20:32",
      "url": "https://stackoverflow.com/questions/67520866/no-matches-for-kind-cronjob-in-version-batch-v1"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubernetes",
      "question_id": 66427129,
      "title": "Terraform: Error: Kubernetes cluster unreachable: invalid configuration",
      "problem": "After deleting kubernetes cluster with \"terraform destroy\" I can't create it again anymore.\n\"terraform apply\" returns the following error message:\n\nError: Kubernetes cluster unreachable: invalid configuration: no\nconfiguration has been provided, try setting KUBERNETES_MASTER\nenvironment variable\n\nHere is the terraform configuration:\n```\n`terraform {\n  backend \"s3\" {\n    bucket = \"skyglass-msur\"\n    key    = \"terraform/backend\"\n    region = \"us-east-1\"\n  }\n}\n\nlocals {\n  env_name         = \"staging\"\n  aws_region       = \"us-east-1\"\n  k8s_cluster_name = \"ms-cluster\"\n}\n\nvariable \"mysql_password\" {\n  type        = string\n  description = \"Expected to be retrieved from environment variable TF_VAR_mysql_password\"\n}\n\nprovider \"aws\" {\n  region = local.aws_region\n}\n\ndata \"aws_eks_cluster\" \"msur\" {\n  name = module.aws-kubernetes-cluster.eks_cluster_id\n}\n\nmodule \"aws-network\" {\n  source = \"github.com/skyglass-microservices/module-aws-network\"\n\n  env_name              = local.env_name\n  vpc_name              = \"msur-VPC\"\n  cluster_name          = local.k8s_cluster_name\n  aws_region            = local.aws_region\n  main_vpc_cidr         = \"10.10.0.0/16\"\n  public_subnet_a_cidr  = \"10.10.0.0/18\"\n  public_subnet_b_cidr  = \"10.10.64.0/18\"\n  private_subnet_a_cidr = \"10.10.128.0/18\"\n  private_subnet_b_cidr = \"10.10.192.0/18\"\n}\n\nmodule \"aws-kubernetes-cluster\" {\n  source = \"github.com/skyglass-microservices/module-aws-kubernetes\"\n\n  ms_namespace       = \"microservices\"\n  env_name           = local.env_name\n  aws_region         = local.aws_region\n  cluster_name       = local.k8s_cluster_name\n  vpc_id             = module.aws-network.vpc_id\n  cluster_subnet_ids = module.aws-network.subnet_ids\n\n  nodegroup_subnet_ids     = module.aws-network.private_subnet_ids\n  nodegroup_disk_size      = \"20\"\n  nodegroup_instance_types = [\"t3.medium\"]\n  nodegroup_desired_size   = 1\n  nodegroup_min_size       = 1\n  nodegroup_max_size       = 5\n}\n\n# Create namespace\n# Use kubernetes provider to work with the kubernetes cluster API\nprovider \"kubernetes\" {\n  # load_config_file       = false\n  cluster_ca_certificate = base64decode(data.aws_eks_cluster.msur.certificate_authority.0.data)\n  host                   = data.aws_eks_cluster.msur.endpoint\n  exec {\n    api_version = \"client.authentication.k8s.io/v1alpha1\"\n    command     = \"aws-iam-authenticator\"\n    args        = [\"token\", \"-i\", \"${data.aws_eks_cluster.msur.name}\"]\n  }\n}\n\n# Create a namespace for microservice pods\nresource \"kubernetes_namespace\" \"ms-namespace\" {\n  metadata {\n    name = \"microservices\"\n  }\n}\n`\n```\nP.S. There seems to be the issue with terraform kubernetes provider for 0.14.7\nI couldn't use \"load_config_file\" = false in this version, so I had to comment it, which seems to be the reason of this issue.\nP.P.S. It could also be the issue with outdated cluster_ca_certificate, which terraform tries to use: deleting this certificate could be enough, although I'm not sure, where it is stored.",
      "solution": "Before doing something radical like manipulating the state directly, try setting the KUBE_CONFIG_PATH variable:\n```\n`export KUBE_CONFIG_PATH=/path/to/.kube/config\n`\n```\nAfter this rerun the `plan` or `apply` command.\nThis has fixed the issue for me.",
      "question_score": 29,
      "answer_score": 58,
      "created_at": "2021-03-01T18:53:50",
      "url": "https://stackoverflow.com/questions/66427129/terraform-error-kubernetes-cluster-unreachable-invalid-configuration"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubernetes",
      "question_id": 67151953,
      "title": "Forbidden resource in API group at the cluster scope",
      "problem": "I am unable to identify what the exact issue with the permissions with my setup as shown below. I've looked into all the similar QAs but still unable to solve the issue. The aim is to deploy Prometheus and let it scrape `/metrics` endpoints that my other applications in the cluster expose fine.\n`Failed to watch *v1.Endpoints: failed to list *v1.Endpoints: endpoints is forbidden: User \\\"system:serviceaccount:default:default\\\" cannot list resource \\\"endpoints\\\" in API group \\\"\\\" at the cluster scope\"\nFailed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User \\\"system:serviceaccount:default:default\\\" cannot list resource \\\"pods\\\" in API group \\\"\\\" at the cluster scope\"\nFailed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User \\\"system:serviceaccount:default:default\\\" cannot list resource \\\"services\\\" in API group \\\"\\\" at the cluster scope\"\n...\n...\n`\nThe command below returns `no` to all services, nodes, pods etc.\n`kubectl auth can-i get services --as=system:serviceaccount:default:default -n default\n`\nMinikube\n`$ minikube start --vm-driver=virtualbox --extra-config=apiserver.Authorization.Mode=RBAC\n\n\ud83d\ude04  minikube v1.14.2 on Darwin 11.2\n\u2728  Using the virtualbox driver based on existing profile\n\ud83d\udc4d  Starting control plane node minikube in cluster minikube\n\ud83d\udd04  Restarting existing virtualbox VM for \"minikube\" ...\n\ud83d\udc33  Preparing Kubernetes v1.19.2 on Docker 19.03.12 ...\n    \u25aa apiserver.Authorization.Mode=RBAC\n\ud83d\udd0e  Verifying Kubernetes components...\n\ud83c\udf1f  Enabled addons: storage-provisioner, default-storageclass, dashboard\n\ud83c\udfc4  Done! kubectl is now configured to use \"minikube\" by default\n`\nRoles\n`apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\n\nmetadata:\n  name: monitoring-cluster-role\n\nrules:\n  - apiGroups: [\"\"]\n    resources: [\"nodes\", \"services\", \"pods\", \"endpoints\"]\n    verbs: [\"get\", \"list\", \"watch\"]\n  - apiGroups: [\"\"]\n    resources: [\"configmaps\"]\n    verbs: [\"get\"]\n  - apiGroups: [\"extensions\"]\n    resources: [\"deployments\"]\n    verbs: [\"get\", \"list\", \"watch\"]\n`\n`apiVersion: v1\nkind: ServiceAccount\n\nmetadata:\n  name: monitoring-service-account\n  namespace: default\n`\n`apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\n\nmetadata:\n  name: monitoring-cluster-role-binding\n\nroleRef:\n  kind: ClusterRole\n  name: monitoring-cluster-role\n  apiGroup: rbac.authorization.k8s.io\n\nsubjects:\n  - kind: ServiceAccount\n    name: monitoring-service-account\n    namespace: default\n`\nPrometheus\n`apiVersion: v1\nkind: ConfigMap\n \nmetadata:\n  name: prometheus-config-map\n  namespace: default\n \ndata:\n  prometheus.yml: |\n    global:\n      scrape_interval: 15s\n    scrape_configs:\n      - job_name: 'kubernetes-service-endpoints'\n        kubernetes_sd_configs:\n        - role: endpoints\n        relabel_configs:\n        - action: labelmap\n          regex: __meta_kubernetes_service_label_(.+)\n        - source_labels: [__meta_kubernetes_namespace]\n          action: replace\n          target_label: kubernetes_namespace\n        - source_labels: [__meta_kubernetes_service_name]\n          action: replace\n          target_label: kubernetes_name \n`\n`apiVersion: apps/v1\nkind: Deployment\n \nmetadata:\n  name: prometheus-deployment\n  namespace: default\n  labels:\n    app: prometheus\n \nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prometheus\n  template:\n    metadata:\n      labels:\n        app: prometheus\n    spec:\n      containers:\n        - name: prometheus\n          image: prom/prometheus:latest\n          ports:\n            - name: http\n              protocol: TCP\n              containerPort: 9090\n          volumeMounts:\n            - name: config\n              mountPath: /etc/prometheus/\n            - name: storage\n              mountPath: /prometheus/\n      volumes:\n        - name: config\n          configMap:\n            name: prometheus-config-map\n        - name: storage\n          emptyDir: {}\n\n`\n`apiVersion: v1\nkind: Service\n \nmetadata:\n  name: prometheus-service\n  namespace: default\n \nspec:\n  type: NodePort\n  selector:\n    app: prometheus\n  ports:\n    - name: http\n      protocol: TCP\n      port: 80\n      targetPort: 9090\n`",
      "solution": "User \"system:serviceaccount:default:default\" cannot list resource \"endpoints\" in API group \"\" at the cluster scope\"\n\nUser \"system:serviceaccount:default:default\" cannot list resource \"pods\" in API group \"\" at the cluster scope\"\n\nUser \"system:serviceaccount:default:default\" cannot list resource \"services\" in API group \"\" at the cluster scope\"\n\nSomething running with ServiceAccount `default` in namespace `default` is doing things it does not have permissions for.\n```\n`apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: monitoring-service-account\n`\n```\nHere you create a specific ServiceAccount. You also give it some Cluster-wide permissions.\n```\n`apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: prometheus-deployment\n  namespace: default\n`\n```\nYou run Prometheus in namespace `default` but do not specify a specific ServiceAccount, so it will run with ServiceAccount `default`.\nI think your problem is that you are supposed to set the ServiceAccount that you create in the Deployment-manifest for Prometheus.",
      "question_score": 29,
      "answer_score": 23,
      "created_at": "2021-04-18T20:16:51",
      "url": "https://stackoverflow.com/questions/67151953/forbidden-resource-in-api-group-at-the-cluster-scope"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubernetes",
      "question_id": 66236346,
      "title": "Kubernetes apiVersion: networking.k8s.io/v1 Issue with &#39;Ingress&#39;",
      "problem": "Wanted your guidance on an issue while executing a Kubernetes YAML file.\nMy kubectl version is as follows:\n```\n`    Client Version: version.Info{Major:\"1\", Minor:\"20\", GitVersion:\"v1.20.0\", GitCommit:\"af46c47ce925f4c4ad5cc8d1fca46c7b77d13b38\", GitTreeState:\"clean\", BuildDate:\"2020-12-08T17:59:43Z\", GoVersion:\"go1.15.5\", Compiler:\"gc\", Platform:\"windows/amd64\"}\n    Server Version: version.Info{Major:\"1\", Minor:\"18\", GitVersion:\"v1.18.14\", GitCommit:\"89182bdd065fbcaffefec691908a739d161efc03\", GitTreeState:\"clean\", BuildDate:\"2020-12-18T12:02:35Z\", GoVersion:\"go1.13.15\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n`\n```\nThis is the latest version downloaded from the Kubernetes site\nhttps://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl-on-windows\nThe YAML has\napiVersion: networking.k8s.io/v1\nkind: Ingress\nand the error on running the YAML is\n```\n`    no matches for kind \"Ingress\" in version \"networking.k8s.io/v1\"\n`\n```\nKubernetes issue https://github.com/kubernetes/kubernetes/issues/90077 mentions that\n```\n`  networking.k8s.io/v1beta1 == 1.14 to 1.18\n  networking.k8s.io/v1 = 1.19+\n`\n```\nSo I guess it should be working right?\nI have changed the API Version to\n```\n`apiVersion: extensions/v1beta1 or\napiVersion: networking.k8s.io/v1beta1\n`\n```\nbut fail in another section of the YAML\n```\n`backend:\n  service:\n    name: {{ template \"fullname\" $ }}-srv\n     port:\n       number: 80\n`\n```\nwith the error\nerror validating data: ValidationError(Ingress.spec.rules[0].http.paths[0].backend): unknown field \"service\" in io.k8s.api.extensions.v1beta1.IngressBackend\nI am informed that the same YAML works on macOS with the same kubectl version (I do not have access to verify that though). But any thoughts on where I could be going wrong?\nThanks,\nPrabal",
      "solution": "For `networking.k8s.io/v1beta1` it should be\n```\n`backend:\n  serviceName: {{ template \"fullname\" $ }}-srv\n  servicePort: 80\n`\n```\nHow to get documentation:\n```\n`kubectl explain --api-version=networking.k8s.io/v1beta1 ingress.spec.rules.http.paths.backend\n`\n```",
      "question_score": 27,
      "answer_score": 14,
      "created_at": "2021-02-17T06:46:12",
      "url": "https://stackoverflow.com/questions/66236346/kubernetes-apiversion-networking-k8s-io-v1-issue-with-ingress"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubernetes",
      "question_id": 65934606,
      "title": "What does &quot;eksctl create iamserviceaccount&quot; do under the hood on an EKS cluster?",
      "problem": "AWS supports IAM Roles for Service Accounts (IRSA) that allows cluster operators to map AWS IAM Roles to Kubernetes Service Accounts.\nTo do so, one has to create an iamserviceaccount in an EKS cluster:\n```\n`eksctl create iamserviceaccount \\\n    --name  \\\n    --namespace kube-system \\\n    --cluster  \\\n    --attach-policy-arn  \\\n    --approve \\\n    --override-existing-serviceaccounts\n`\n```\nThe problem is that I don't want to use the above `eksctl` command because I want to declare my infrastructure using `terraform`.\nDoes eksctl command do anything other than creating a service account? If it only creates a service account, what is the `YAML` representation of it?",
      "solution": "After `Vasili Angapov`'s helps, now I can answer the question:\nYes It does more than just creating a service account. It does three things:\n\nIt Creates an IAM role.\nIt attaches the desired iam-policy (--attach-policy-arn\n) to the created IAM role.\nIt creates a new kubernetes service account annotated with the arn of the created IAM role.\n\nNow It's easy to declare the above steps using kubernetes and aws providers in terraform.",
      "question_score": 27,
      "answer_score": 3,
      "created_at": "2021-01-28T10:48:04",
      "url": "https://stackoverflow.com/questions/65934606/what-does-eksctl-create-iamserviceaccount-do-under-the-hood-on-an-eks-cluster"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubernetes",
      "question_id": 71721372,
      "title": "minkube dashboard command hangs on &#39;verifying proxy health&#39;",
      "problem": "I'm taking a Kubernetes course and part of the course is trying out minikube. I have minikube and kubectl installed on an Ubuntu 20.04 WSL on Windows 11. When I run `minikube dashboard` in the Ubuntu terminal the process hangs on `Verifying proxy health...`. I tried running `kubectl proxy` in another terminal window then running `minikube dashboard`.\nI go to the dashboard URL and use the proxy port number that the kubectl proxy command indicates but I only get this:\n```\n`{\n  \"kind\": \"Status\",\n  \"apiVersion\": \"v1\",\n  \"metadata\": {},\n  \"status\": \"Failure\",\n  \"message\": \"no endpoints available for service \\\"kubernetes-dashboard\\\"\",\n  \"reason\": \"ServiceUnavailable\",\n  \"code\": 503\n}\n`\n```\nI've also tried running minikube by using `minikube start --vm-driver=docker` as mentioned in this GitHub issue but it still hangs. How do get the dashboard to run? I'm completely new to Kubernetes in general.",
      "solution": "For me the same issue was resolved after a restart of the minikube.\n```\n`minikube stop\n - for me ctrl+c in separate terminal, but maybe you'd have to kill it\nminikube start\nminikube dashboard\n`\n```",
      "question_score": 27,
      "answer_score": 15,
      "created_at": "2022-04-02T23:41:15",
      "url": "https://stackoverflow.com/questions/71721372/minkube-dashboard-command-hangs-on-verifying-proxy-health"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubernetes",
      "question_id": 70818543,
      "title": "Mongo DB deployment not working in kubernetes because processor doesn&#39;t have AVX support",
      "problem": "I am trying to deploy a `mongo db` deployment together with service, as follows:\n```\n`apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mongo-deployment\n  labels:\n    app: mongo\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: mongo\n  template:\n    metadata:\n      labels:\n        app: mongo\n    spec:\n      containers:\n      - name: mongo\n        image: mongo:5.0\n        ports:\n        - containerPort: 27017\n        env:\n        - name: MONGO_INITDB_ROOT_USERNAME\n          valueFrom:\n            secretKeyRef: \n              name: mongo-secret\n              key: mongo-user\n        - name: MONGO_INITDB_ROOT_PASSWORD\n          valueFrom:\n            secretKeyRef: \n              name: mongo-secret\n              key: mongo-password\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: mongo-service\nspec:\n  selector:\n    app: mongo\n  ports:\n    - protocol: TCP\n      port: 27017\n      targetPort: 27017\n`\n```\nEven though everything seems to be configured right and deployed, it gets to a `CrashLoopBackOff` state instead of `Running`, using a `kubectl logs ` I get the following error:\n```\n`MongoDB 5.0+ requires a CPU with AVX support, and your current system does not appear to have that!\n`\n```\nDoes anybody know what to do?",
      "solution": "To solve this issue I had to run an older `mongo-db` docker image version (4.4.6), as follows:\n```\n`image: mongo:4.4.6\n`\n```\nReference:\nMongo 5.0.0 crashes but 4.4.6 works #485",
      "question_score": 25,
      "answer_score": 42,
      "created_at": "2022-01-23T02:43:44",
      "url": "https://stackoverflow.com/questions/70818543/mongo-db-deployment-not-working-in-kubernetes-because-processor-doesnt-have-avx"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubernetes",
      "question_id": 66568194,
      "title": "Kustomize how to replace only the host in Ingress configuration",
      "problem": "I've got this ingress.yaml base configuration:\n```\n`apiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  labels:\n    sia: aza\n    app: asap-ingress-internal\n  name: asap-ingress-internal\n  annotations:\n    kubernetes.io/ingress.class: \"nginx-external\"\n    nginx.ingress.kubernetes.io/use-regex: \"true\"\nspec:\n  rules:\n    - host: the-host-value\n      http:\n        paths:\n          - path: /asap-srv-template/(.*)\n            backend:\n              serviceName: asap-srv-template\n              servicePort: 8080\n`\n```\nAnd want to replace the spec.rules.host value only (and keep all http.paths as is.\nSo I create a env-var.yaml like this :\n```\n`apiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: asap-ingress-internal\nspec:\n  rules:\n    - host: the.real.hostname\n`\n```\nBut the result is the following:\n```\n`$ kustomize build\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  annotations:\n    kubernetes.io/ingress.class: nginx-external\n    nginx.ingress.kubernetes.io/use-regex: \"true\"\n  labels:\n    app: asap-ingress-internal\n    env: dev\n    sia: aza\n  name: asap-ingress-internal\n  namespace: aza-72461-dev\nspec:\n  rules:\n  - host: the.real.hostname\n`\n```\nI have lost all http.paths configuration and I can't find out how to do.\nI tried with patches: or patchesStrategicMerge in kustomization.yaml but the result is always the same.\nAny help would be greatly appreciated",
      "solution": "You can use a json patch for this, below is an example.\nHere is an example `kustomization.yaml`. It will call out a patch in the `patches` section:\n```\n`apiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\n\nresources:\n- ../../base/app1\n\npatches:\n- target:\n    kind: Ingress\n    name: my-ingress\n  path: ingress-patch.json  \n`\n```\nHere would be an example `ingress-patch.json`:\n```\n`[\n    { \n        \"op\": \"replace\", \n        \"path\": \"/spec/rules/0/host\", \n        \"value\": \"the.real.hostname\"\n    }\n]\n`\n```",
      "question_score": 25,
      "answer_score": 28,
      "created_at": "2021-03-10T16:46:49",
      "url": "https://stackoverflow.com/questions/66568194/kustomize-how-to-replace-only-the-host-in-ingress-configuration"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubernetes",
      "question_id": 72334044,
      "title": "&quot;The connection to the server localhost:8080 was refused - did you specify the right host or port?&quot;",
      "problem": "I'm on an ec2 instance trying to get my cluster created. I have kubectl already installed and here are my services and workloads yaml files\nservices.yaml\n```\n`apiVersion: v1\nkind: Service\nmetadata:\n  name: stockapi-webapp\n\nspec:\n  selector:\n    app: stockapi\n\n  ports:\n    - name: http\n      port: 80\n\n  type: LoadBalancer\n`\n```\nworkloads.yaml\n```\n`apiVersion: v1\nkind: Deployment\nmetadata:\n  name: stockapi\nspec:\n  selector:\n    matchLabels:\n      app: stockapi\n  replicas: 1\n  template: # template for the pods\n    metadata:\n      labels:\n        app: stockapi\n    spec:\n      containers:\n      - name: stock-api\n        image: public.ecr.aws/u1c1h9j4/stock-api:latest\n`\n```\nWhen I try to run\n```\n`kubectl apply -f workloads.yaml\n`\n```\nI get this as an error\n```\n`The connection to the server localhost:8080 was refused - did you specify the right host or port?\n`\n```\nI also tried changing the port in my services.yaml to 8080 and that didn't fix it either",
      "solution": "This error comes when you don't have `~/.kube/config` file present or configured correctly on the client / where you run the `kubectl` command.\nkubectl reads the clusterinfo and which port to connect to from the  `~/.kube/config` file.\nif you are using eks here's how you can create `config` file\naws eks create kubeconfig file",
      "question_score": 25,
      "answer_score": 31,
      "created_at": "2022-05-22T02:27:44",
      "url": "https://stackoverflow.com/questions/72334044/the-connection-to-the-server-localhost8080-was-refused-did-you-specify-the-r"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubernetes",
      "question_id": 72240224,
      "title": "What is topologyKey in pod affinity?",
      "problem": "I cannot really understand the purpose and usage of topologyKey in pod affinity. The documentations says:\n\ntopologyKey is the key of node labels. If two Nodes are labelled with this key and have identical values for that label, the scheduler treats both Nodes as being in the same topology. The scheduler tries to place a balanced number of Pods into each topology domain.\n\nAnd example usage is as follows:\n```\n`kind: Pod\nmetadata:\n  name: with-pod-affinity\nspec:\n  affinity:\n    podAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n      - labelSelector:\n          matchExpressions:\n          - key: security\n            operator: In\n            values:\n            - S1\n        topologyKey: topology.kubernetes.io/zone\n    podAntiAffinity:\n      preferredDuringSchedulingIgnoredDuringExecution:\n      - weight: 100\n        podAffinityTerm:\n          labelSelector:\n            matchExpressions:\n            - key: security\n              operator: In\n              values:\n              - S2\n          topologyKey: topology.kubernetes.io/zone\n  containers:\n  - name: with-pod-affinity\n    image: k8s.gcr.io/pause:2.0\n`\n```\nSo where does topology.kubernetes.io/zone come from? How can I know what value should I provide for this topologyKey field in my yaml file, and what happens if I just put a random string here? Should I label my node and use the key of this label in topologyKey field?\nThank you.",
      "solution": "A topology Key is effectively just a label that you assign to your nodes or that a cloud provider has already assigned.\nThe intent is to indicate certain topology characteristics, like the availability zone or server rack, for example. But they are actually arbitrary.",
      "question_score": 24,
      "answer_score": 12,
      "created_at": "2022-05-14T14:31:59",
      "url": "https://stackoverflow.com/questions/72240224/what-is-topologykey-in-pod-affinity"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubernetes",
      "question_id": 67061603,
      "title": "How to communicate between containers in same POD in Kubernetes?",
      "problem": "For one POD, three images has been created. The problem here is that there is no communication between containers within same pod. How should my application connected with these three containers?\nMy pod have below containers.\n```\n`[dev-application dev-app-nginx dev-app-redis]\n`\n```\nHere I am able see only rails is running but redis and nginx is not running. Because Redis and nix is running as different containers in same pod.\n```\n`kubectl exec -ti test-deployment-5f59864c8b-mv4kk sh\nkubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead.\nDefaulting container name to dev-application.\nUse 'kubectl describe pod/test-deployment-5f59864c8b-mv4kk -n dev-app' to see all of the containers in this pod.\n# rails -v\nRails 4.2.11.3\n# redis -v\nsh: 2: redis: not found\n# nginx -v\nsh: 3: nginx: not found\n# \n`\n```\nBelow the yam file I am using\n```\n`apiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  namespace: dev-app\n  name: test-deployment\nspec:\n  replicas: 1 \n  template:\n    metadata:\n      labels:\n        app: Dev-app\n    spec:\n      nodeSelector:\n       cloud.io/sec-zone-green: \"true\"\n      containers:\n        - name: dev-application\n          image: hub.docker.net/appautomation/dev.app.1.0:latest\n            command: [\"/bin/sh\"]\n            args: [\"-c\", \"while true; do echo test; sleep 20;done\"]\n          resources:\n            limits:\n              memory: 8Gi\n              cpu: 5\n            requests:\n              memory: 8Gi\n              cpu: 5\n          ports:\n            - containerPort: 3000\n        - name: dev-app-nginx\n          image: hub.docker.net/appautomation/dev.nginx.1.0:latest\n          resources:\n            limits:\n              memory: 4Gi\n              cpu: 4\n            requests:\n              memory: 4Gi\n              cpu: 4\n          ports:\n            - containerPort: 80\n                       \n        - name: dev-app-redis\n          image: hub.docker.net/appautomation/dev.redis.1.0:latest\n          \n          resources:\n            limits:\n              memory: 4Gi\n              cpu: 4\n            requests:\n              memory: 4Gi\n              cpu: 4\n          ports:\n            - containerPort: 6379\n    \n        \n`\n```",
      "solution": "Use localhost to communicate with other containers within the same pod.\nE.g. the addresses to the containers are\n\n127.0.0.1:3000\n127.0.0.1:80\n127.0.0.1:6379",
      "question_score": 23,
      "answer_score": 35,
      "created_at": "2021-04-12T17:54:42",
      "url": "https://stackoverflow.com/questions/67061603/how-to-communicate-between-containers-in-same-pod-in-kubernetes"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubernetes",
      "question_id": 68881023,
      "title": "Docker standard_init_linux.go:228: exec user process caused: no such file or directory",
      "problem": "Whenever I am trying to run the docker images, it is exiting in immediately.\n```\n`CONTAINER ID   IMAGE                      COMMAND                CREATED          STATUS                      PORTS     NAMES\nae327a2bdba3   k8s-for-beginners:v0.0.1   \"/k8s-for-beginners\"   11 seconds ago   Exited (1) 10 seconds ago             focused_booth\n`\n```\nAs per Container Logs\n```\n`standard_init_linux.go:228: exec user process caused: no such file or directory\n`\n```\nI have created all the files in linux itself:\n```\n`FROM alpine:3.10\nCOPY k8s-for-beginners /\nCMD [\"/k8s-for-beginners\"]\n`\n```\nGO Code:\n```\n`package main\n\nimport (\n    \"fmt\"\n    \"log\"\n    \"net/http\"\n)\n\nfunc main() {\n    http.HandleFunc(\"/\", handler)\n    log.Fatal(http.ListenAndServe(\"0.0.0.0:8080\", nil))\n}\n\nfunc handler(w http.ResponseWriter, r *http.Request) {\n    log.Printf(\"Ping from %s\", r.RemoteAddr)\n    fmt.Fprintln(w, \"Hello Kubernetes Beginners!\")\n}\n`\n```\nThis is the first exercise from THE KUBERNETES WORKSHOP book.\nCommands I have used in this Process:\n```\n`CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build -o k8s-for-beginners\nsudo docker build -t k8s-for-beginners:v0.0.1 .\nsudo docker run -p 8080:8080 -d k8s-for-beginners:v0.0.1\n`\n```\nOutput of the command:\n`sudo docker run k8s-for-beginners:v0.0.1 ldd /k8s-for-beginners\n`\n```\n`        /lib64/ld-linux-x86-64.so.2 (0x7f9ab5778000)\n        libc.so.6 => /lib64/ld-linux-x86-64.so.2 (0x7f9ab5778000)\nError loading shared library libgo.so.16: No such file or directory (needed by /k8s-for-beginners)\nError loading shared library libgcc_s.so.1: No such file or directory (needed by /k8s-for-beginners)\nError loading shared library ld-linux-x86-64.so.2: No such file or directory (needed by /k8s-for-beginners)\nError relocating /k8s-for-beginners: crypto..z2frsa..import: symbol not found\nError relocating /k8s-for-beginners: crypto..z2fx509..import: symbol not found\nError relocating /k8s-for-beginners: log..import: symbol not found\nError relocating /k8s-for-beginners: crypto..z2fmd5..import: symbol not found\nError relocating /k8s-for-beginners: crypto..import: symbol not found\nError relocating /k8s-for-beginners: bytes..import: symbol not found\nError relocating /k8s-for-beginners: fmt.Fprintln: symbol not found\nError relocating /k8s-for-beginners: crypto..z2felliptic..import: symbol not found\nError relocating /k8s-for-beginners: crypto..z2fx509..z2fpkix..import: symbol not found\nError relocating /k8s-for-beginners: crypto..z2frand..import: symbol not found\nError relocating /k8s-for-beginners: golang.x2eorg..z2fx..z2fcrypto..z2fchacha20poly1305..import: symbol not found\nError relocating /k8s-for-beginners: golang.x2eorg..z2fx..z2fcrypto..z2fcurve25519..import: symbol not found\nError relocating /k8s-for-beginners: golang.x2eorg..z2fx..z2fnet..z2fidna..import: symbol not found\nError relocating /k8s-for-beginners: internal..z2foserror..import: symbol not found\nError relocating /k8s-for-beginners: crypto..z2fecdsa..import: symbol not found\nError relocating /k8s-for-beginners: net..z2fhttp.HandleFunc: symbol not found\nError relocating /k8s-for-beginners: io..import: symbol not found\nError relocating /k8s-for-beginners: golang.x2eorg..z2fx..z2fnet..z2fhttp2..z2fhpack..import: symbol not found\nError relocating /k8s-for-beginners: crypto..z2fcipher..import: symbol not found\nError relocating /k8s-for-beginners: log.Fatal: symbol not found\nError relocating /k8s-for-beginners: math..z2fbig..import: symbol not found\nError relocating /k8s-for-beginners: runtime..import: symbol not found\nError relocating /k8s-for-beginners: net..z2fhttp..import: symbol not found\nError relocating /k8s-for-beginners: hash..z2fcrc32..import: symbol not found\nError relocating /k8s-for-beginners: net..z2fhttp.ListenAndServe: symbol not found\nError relocating /k8s-for-beginners: context..import: symbol not found\nError relocating /k8s-for-beginners: fmt..import: symbol not found\nError relocating /k8s-for-beginners: crypto..z2ftls..import: symbol not found\nError relocating /k8s-for-beginners: errors..import: symbol not found\nError relocating /k8s-for-beginners: internal..z2ftestlog..import: symbol not found\nError relocating /k8s-for-beginners: runtime.setIsCgo: symbol not found\nError relocating /k8s-for-beginners: runtime_m: symbol not found\nError relocating /k8s-for-beginners: encoding..z2fhex..import: symbol not found\nError relocating /k8s-for-beginners: mime..import: symbol not found\nError relocating /k8s-for-beginners: golang.x2eorg..z2fx..z2ftext..z2funicode..z2fbidi..import: symbol not found\nError relocating /k8s-for-beginners: internal..z2freflectlite..import: symbol not found\nError relocating /k8s-for-beginners: compress..z2fgzip..import: symbol not found\nError relocating /k8s-for-beginners: sync..import: symbol not found\nError relocating /k8s-for-beginners: compress..z2fflate..import: symbol not found\nError relocating /k8s-for-beginners: encoding..z2fbinary..import: symbol not found\nError relocating /k8s-for-beginners: math..z2frand..import: symbol not found\nError relocating /k8s-for-beginners: runtime_cpuinit: symbol not found\nError relocating /k8s-for-beginners: internal..z2fpoll..import: symbol not found\nError relocating /k8s-for-beginners: mime..z2fmultipart..import: symbol not found\nError relocating /k8s-for-beginners: runtime.check: symbol not found\nError relocating /k8s-for-beginners: golang.x2eorg..z2fx..z2fcrypto..z2fcryptobyte..import: symbol not found\nError relocating /k8s-for-beginners: crypto..z2fsha512..import: symbol not found\nError relocating /k8s-for-beginners: runtime.registerTypeDescriptors: symbol not found\nError relocating /k8s-for-beginners: golang.x2eorg..z2fx..z2fcrypto..z2fchacha20..import: symbol not found\nError relocating /k8s-for-beginners: runtime.setmodinfo: symbol not found\nError relocating /k8s-for-beginners: golang.x2eorg..z2fx..z2ftext..z2ftransform..import: symbol not found\nError relocating /k8s-for-beginners: time..import: symbol not found\nError relocating /k8s-for-beginners: encoding..z2fbase64..import: symbol not found\nError relocating /k8s-for-beginners: crypto..z2fsha256..import: symbol not found\nError relocating /k8s-for-beginners: __go_go: symbol not found\nError relocating /k8s-for-beginners: golang.x2eorg..z2fx..z2fnet..z2fhttp..z2fhttpguts..import: symbol not found\nError relocating /k8s-for-beginners: path..z2ffilepath..import: symbol not found\nError relocating /k8s-for-beginners: golang.x2eorg..z2fx..z2ftext..z2fsecure..z2fbidirule..import: symbol not found\nError relocating /k8s-for-beginners: os..import: symbol not found\nError relocating /k8s-for-beginners: golang.x2eorg..z2fx..z2fnet..z2fhttp..z2fhttpproxy..import: symbol not found\nError relocating /k8s-for-beginners: net..z2ftextproto..import: symbol not found\nError relocating /k8s-for-beginners: encoding..z2fasn1..import: symbol not found\nError relocating /k8s-for-beginners: runtime.requireitab: symbol not found\nError relocating /k8s-for-beginners: golang.x2eorg..z2fx..z2fnet..z2fdns..z2fdnsmessage..import: symbol not found\nError relocating /k8s-for-beginners: path..import: symbol not found\nError relocating /k8s-for-beginners: io..z2fioutil..import: symbol not found\nError relocating /k8s-for-beginners: sort..import: symbol not found\nError relocating /k8s-for-beginners: golang.x2eorg..z2fx..z2ftext..z2funicode..z2fnorm..import: symbol not found\nError relocating /k8s-for-beginners: internal..z2fcpu..import: symbol not found\nError relocating /k8s-for-beginners: runtime.ginit: symbol not found\nError relocating /k8s-for-beginners: runtime.osinit: symbol not found\nError relocating /k8s-for-beginners: runtime.schedinit: symbol not found\nError relocating /k8s-for-beginners: bufio..import: symbol not found\nError relocating /k8s-for-beginners: crypto..z2finternal..z2frandutil..import: symbol not found\nError relocating /k8s-for-beginners: runtime_mstart: symbol not found\nError relocating /k8s-for-beginners: net..import: symbol not found\nError relocating /k8s-for-beginners: strconv..import: symbol not found\nError relocating /k8s-for-beginners: runtime.args: symbol not found\nError relocating /k8s-for-beginners: runtime..z2finternal..z2fsys..import: symbol not found\nError relocating /k8s-for-beginners: runtime.newobject: symbol not found\nError relocating /k8s-for-beginners: syscall..import: symbol not found\nError relocating /k8s-for-beginners: unicode..import: symbol not found\nError relocating /k8s-for-beginners: net..z2fhttp..z2finternal..import: symbol not found\nError relocating /k8s-for-beginners: encoding..z2fpem..import: symbol not found\nError relocating /k8s-for-beginners: _Unwind_Resume: symbol not found\nError relocating /k8s-for-beginners: reflect..import: symbol not found\nError relocating /k8s-for-beginners: mime..z2fquotedprintable..import: symbol not found\nError relocating /k8s-for-beginners: log.Printf: symbol not found\nError relocating /k8s-for-beginners: runtime.typedmemmove: symbol not found\nError relocating /k8s-for-beginners: crypto..z2fdsa..import: symbol not found\nError relocating /k8s-for-beginners: crypto..z2fsha1..import: symbol not found\nError relocating /k8s-for-beginners: bufio..types: symbol not found\nError relocating /k8s-for-beginners: bytes..types: symbol not found\nError relocating /k8s-for-beginners: compress..z2fflate..types: symbol not found\nError relocating /k8s-for-beginners: compress..z2fgzip..types: symbol not found\nError relocating /k8s-for-beginners: context..types: symbol not found\nError relocating /k8s-for-beginners: crypto..types: symbol not found\nError relocating /k8s-for-beginners: crypto..z2fcipher..types: symbol not found\nError relocating /k8s-for-beginners: crypto..z2fdsa..types: symbol not found\nError relocating /k8s-for-beginners: crypto..z2fecdsa..types: symbol not found\nError relocating /k8s-for-beginners: crypto..z2felliptic..types: symbol not found\nError relocating /k8s-for-beginners: crypto..z2finternal..z2frandutil..types: symbol not found\nError relocating /k8s-for-beginners: crypto..z2fmd5..types: symbol not found\nError relocating /k8s-for-beginners: crypto..z2frand..types: symbol not found\nError relocating /k8s-for-beginners: crypto..z2frsa..types: symbol not found\nError relocating /k8s-for-beginners: crypto..z2fsha1..types: symbol not found\nError relocating /k8s-for-beginners: crypto..z2fsha256..types: symbol not found\nError relocating /k8s-for-beginners: crypto..z2fsha512..types: symbol not found\nError relocating /k8s-for-beginners: crypto..z2ftls..types: symbol not found\nError relocating /k8s-for-beginners: crypto..z2fx509..types: symbol not found\nError relocating /k8s-for-beginners: crypto..z2fx509..z2fpkix..types: symbol not found\nError relocating /k8s-for-beginners: encoding..z2fasn1..types: symbol not found\nError relocating /k8s-for-beginners: encoding..z2fbase64..types: symbol not found\nError relocating /k8s-for-beginners: encoding..z2fbinary..types: symbol not found\nError relocating /k8s-for-beginners: encoding..z2fhex..types: symbol not found\nError relocating /k8s-for-beginners: encoding..z2fpem..types: symbol not found\nError relocating /k8s-for-beginners: errors..types: symbol not found\nError relocating /k8s-for-beginners: fmt..types: symbol not found\nError relocating /k8s-for-beginners: golang.x2eorg..z2fx..z2fcrypto..z2fchacha20..types: symbol not found\nError relocating /k8s-for-beginners: golang.x2eorg..z2fx..z2fcrypto..z2fchacha20poly1305..types: symbol not found\nError relocating /k8s-for-beginners: golang.x2eorg..z2fx..z2fcrypto..z2fcryptobyte..types: symbol not found\nError relocating /k8s-for-beginners: golang.x2eorg..z2fx..z2fcrypto..z2fcurve25519..types: symbol not found\nError relocating /k8s-for-beginners: golang.x2eorg..z2fx..z2fnet..z2fdns..z2fdnsmessage..types: symbol not found\nError relocating /k8s-for-beginners: golang.x2eorg..z2fx..z2fnet..z2fhttp..z2fhttpguts..types: symbol not found\nError relocating /k8s-for-beginners: golang.x2eorg..z2fx..z2fnet..z2fhttp..z2fhttpproxy..types: symbol not found\nError relocating /k8s-for-beginners: golang.x2eorg..z2fx..z2fnet..z2fhttp2..z2fhpack..types: symbol not found\nError relocating /k8s-for-beginners: golang.x2eorg..z2fx..z2fnet..z2fidna..types: symbol not found\nError relocating /k8s-for-beginners: golang.x2eorg..z2fx..z2ftext..z2fsecure..z2fbidirule..types: symbol not found\nError relocating /k8s-for-beginners: golang.x2eorg..z2fx..z2ftext..z2ftransform..types: symbol not found\nError relocating /k8s-for-beginners: golang.x2eorg..z2fx..z2ftext..z2funicode..z2fbidi..types: symbol not found\nError relocating /k8s-for-beginners: golang.x2eorg..z2fx..z2ftext..z2funicode..z2fnorm..types: symbol not found\nError relocating /k8s-for-beginners: hash..z2fcrc32..types: symbol not found\nError relocating /k8s-for-beginners: internal..z2fcpu..types: symbol not found\nError relocating /k8s-for-beginners: internal..z2foserror..types: symbol not found\nError relocating /k8s-for-beginners: internal..z2fpoll..types: symbol not found\nError relocating /k8s-for-beginners: internal..z2freflectlite..types: symbol not found\nError relocating /k8s-for-beginners: internal..z2ftestlog..types: symbol not found\nError relocating /k8s-for-beginners: io..types: symbol not found\nError relocating /k8s-for-beginners: io..z2fioutil..types: symbol not found\nError relocating /k8s-for-beginners: log..types: symbol not found\nError relocating /k8s-for-beginners: math..z2fbig..types: symbol not found\nError relocating /k8s-for-beginners: math..z2frand..types: symbol not found\nError relocating /k8s-for-beginners: mime..types: symbol not found\nError relocating /k8s-for-beginners: mime..z2fmultipart..types: symbol not found\nError relocating /k8s-for-beginners: mime..z2fquotedprintable..types: symbol not found\nError relocating /k8s-for-beginners: net..types: symbol not found\nError relocating /k8s-for-beginners: net..z2fhttp..types: symbol not found\nError relocating /k8s-for-beginners: net..z2fhttp..z2finternal..types: symbol not found\nError relocating /k8s-for-beginners: net..z2ftextproto..types: symbol not found\nError relocating /k8s-for-beginners: os..types: symbol not found\nError relocating /k8s-for-beginners: path..types: symbol not found\nError relocating /k8s-for-beginners: path..z2ffilepath..types: symbol not found\nError relocating /k8s-for-beginners: reflect..types: symbol not found\nError relocating /k8s-for-beginners: runtime..types: symbol not found\nError relocating /k8s-for-beginners: runtime..z2finternal..z2fsys..types: symbol not found\nError relocating /k8s-for-beginners: sort..types: symbol not found\nError relocating /k8s-for-beginners: strconv..types: symbol not found\nError relocating /k8s-for-beginners: sync..types: symbol not found\nError relocating /k8s-for-beginners: syscall..types: symbol not found\nError relocating /k8s-for-beginners: time..types: symbol not found\nError relocating /k8s-for-beginners: unicode..types: symbol not found\nError relocating /k8s-for-beginners: container..z2flist..types: symbol not found\nError relocating /k8s-for-beginners: crypto..z2faes..types: symbol not found\nError relocating /k8s-for-beginners: crypto..z2fdes..types: symbol not found\nError relocating /k8s-for-beginners: crypto..z2fed25519..types: symbol not found\nError relocating /k8s-for-beginners: crypto..z2fed25519..z2finternal..z2fedwards25519..types: symbol not found\nError relocating /k8s-for-beginners: crypto..z2fhmac..types: symbol not found\nError relocating /k8s-for-beginners: crypto..z2finternal..z2fsubtle..types: symbol not found\nError relocating /k8s-for-beginners: crypto..z2frc4..types: symbol not found\nError relocating /k8s-for-beginners: crypto..z2fsubtle..types: symbol not found\nError relocating /k8s-for-beginners: golang.x2eorg..z2fx..z2fcrypto..z2fcryptobyte..z2fasn1..types: symbol not found\nError relocating /k8s-for-beginners: golang.x2eorg..z2fx..z2fcrypto..z2fhkdf..types: symbol not found\nError relocating /k8s-for-beginners: golang.x2eorg..z2fx..z2fcrypto..z2finternal..z2fsubtle..types: symbol not found\nError relocating /k8s-for-beginners: golang.x2eorg..z2fx..z2fcrypto..z2fpoly1305..types: symbol not found\nError relocating /k8s-for-beginners: hash..types: symbol not found\nError relocating /k8s-for-beginners: internal..z2fbytealg..types: symbol not found\nError relocating /k8s-for-beginners: internal..z2ffmtsort..types: symbol not found\nError relocating /k8s-for-beginners: internal..z2fnettrace..types: symbol not found\nError relocating /k8s-for-beginners: internal..z2frace..types: symbol not found\nError relocating /k8s-for-beginners: internal..z2fsingleflight..types: symbol not found\nError relocating /k8s-for-beginners: internal..z2fsyscall..z2fexecenv..types: symbol not found\nError relocating /k8s-for-beginners: internal..z2fsyscall..z2funix..types: symbol not found\nError relocating /k8s-for-beginners: math..types: symbol not found\nError relocating /k8s-for-beginners: math..z2fbits..types: symbol not found\nError relocating /k8s-for-beginners: net..z2fhttp..z2fhttptrace..types: symbol not found\nError relocating /k8s-for-beginners: net..z2furl..types: symbol not found\nError relocating /k8s-for-beginners: runtime..z2finternal..z2fatomic..types: symbol not found\nError relocating /k8s-for-beginners: runtime..z2finternal..z2fmath..types: symbol not found\nError relocating /k8s-for-beginners: strings..types: symbol not found\nError relocating /k8s-for-beginners: sync..z2fatomic..types: symbol not found\nError relocating /k8s-for-beginners: unicode..z2futf16..types: symbol not found\nError relocating /k8s-for-beginners: unicode..z2futf8..types: symbol not found\nError relocating /k8s-for-beginners: runtime.strequal..f: symbol not found\nError relocating /k8s-for-beginners: runtime.memequal64..f: symbol not found\nError relocating /k8s-for-beginners: type...1reflect.rtype: symbol not found\nError relocating /k8s-for-beginners: reflect.rtype.Align: symbol not found\nError relocating /k8s-for-beginners: reflect.rtype.Align: symbol not found\nError relocating /k8s-for-beginners: reflect.rtype.AssignableTo: symbol not found\nError relocating /k8s-for-beginners: reflect.rtype.AssignableTo: symbol not found\nError relocating /k8s-for-beginners: reflect.rtype.Bits: symbol not found\nError relocating /k8s-for-beginners: reflect.rtype.Bits: symbol not found\nError relocating /k8s-for-beginners: reflect.rtype.ChanDir: symbol not found\nError relocating /k8s-for-beginners: reflect.rtype.ChanDir: symbol not found\nError relocating /k8s-for-beginners: reflect.rtype.Comparable: symbol not found\nError relocating /k8s-for-beginners: reflect.rtype.Comparable: symbol not found\nError relocating /k8s-for-beginners: reflect.rtype.ConvertibleTo: symbol not found\nError relocating /k8s-for-beginners: reflect.rtype.ConvertibleTo: symbol not found\nError relocating /k8s-for-beginners: reflect.rtype.Elem: symbol not found\nError relocating /k8s-for-beginners: reflect.rtype.Elem: symbol not found\nError relocating /k8s-for-beginners: reflect.rtype.Field: symbol not found\nError relocating /k8s-for-beginners: reflect.rtype.Field: symbol not found\nError relocating /k8s-for-beginners: reflect.rtype.FieldAlign: symbol not found\nError relocating /k8s-for-beginners: reflect.rtype.FieldAlign: symbol not found\nError relocating /k8s-for-beginners: reflect.rtype.FieldByIndex: symbol not found\nError relocating /k8s-for-beginners: reflect.rtype.FieldByIndex: symbol not found\nError relocating /k8s-for-beginners: reflect.rtype.FieldByName: symbol not found\nError relocating /k8s-for-beginners: reflect.rtype.FieldByName: symbol not found\nError relocating /k8s-for-beginners: reflect.rtype.FieldByNameFunc: symbol not found\nError relocating /k8s-for-beginners: reflect.rtype.FieldByNameFunc: symbol not found\nError relocating /k8s-for-beginners: reflect.rtype.Implements: symbol not found\nError relocating /k8s-for-beginners: reflect.rtype.Implements: symbol not found\nError relocating /k8s-for-beginners: reflect.rtype.In: symbol not found\nError relocating /k8s-for-beginners: reflect.rtype.In: symbol not found\nError relocating /k8s-for-beginners: reflect.rtype.IsVariadic: symbol not found\nError relocating /k8s-for-beginners: reflect.rtype.IsVariadic: symbol not found\nError relocating /k8s-for-beginners: reflect.rtype.Key: symbol not found\nError relocating /k8s-for-beginners: reflect.rtype.Key: symbol not found\nError relocating /k8s-for-beginners: reflect.rtype.Kind: symbol not found\nError relocating /k8s-for-beginners: reflect.rtype.Kind: symbol not found\nError relocating /k8s-for-beginners: reflect.rtype.Len: symbol not found\nError relocating /k8s-for-beginners: reflect.rtype.Len: symbol not found\nError relocating /k8s-for-beginners: reflect.rtype.Method: symbol not found\nError relocating /k8s-for-beginners: reflect.rtype.Method: symbol not found\nError relocating /k8s-for-beginners: reflect.rtype.MethodByName: symbol not found\nError relocating /k8s-for-beginners: reflect.rtype.MethodByName: symbol not found\nError relocating /k8s-for-beginners: reflect.rtype.Name: symbol not found\nError relocating /k8s-for-beginners: reflect.rtype.Name: symbol not found\nError relocating /k8s-for-beginners: reflect.rtype.NumField: symbol not found\nError relocating /k8s-for-beginners: reflect.rtype.NumField: symbol not found\nError relocating /k8s-for-beginners: reflect.rtype.NumIn: symbol not found\nError relocating /k8s-for-beginners: reflect.rtype.NumIn: symbol not found\nError relocating /k8s-for-beginners: reflect.rtype.NumMethod: symbol not found\nError relocating /k8s-for-beginners: reflect.rtype.NumMethod: symbol not found\nError relocating /k8s-for-beginners: reflect.rtype.NumOut: symbol not found\nError relocating /k8s-for-beginners: reflect.rtype.NumOut: symbol not found\nError relocating /k8s-for-beginners: reflect.rtype.Out: symbol not found\nError relocating /k8s-for-beginners: reflect.rtype.Out: symbol not found\nError relocating /k8s-for-beginners: reflect.rtype.PkgPath: symbol not found\nError relocating /k8s-for-beginners: reflect.rtype.PkgPath: symbol not found\nError relocating /k8s-for-beginners: reflect.rtype.Size: symbol not found\nError relocating /k8s-for-beginners: reflect.rtype.Size: symbol not found\nError relocating /k8s-for-beginners: reflect.rtype.String: symbol not found\nError relocating /k8s-for-beginners: reflect.rtype.String: symbol not found\nError relocating /k8s-for-beginners: reflect.rtype.common: symbol not found\nError relocating /k8s-for-beginners: reflect.rtype.common: symbol not found\nError relocating /k8s-for-beginners: reflect.rtype.rawString: symbol not found\nError relocating /k8s-for-beginners: reflect.rtype.rawString: symbol not found\nError relocating /k8s-for-beginners: reflect.rtype.uncommon..stub: symbol not found\nError relocating /k8s-for-beginners: reflect.rtype.uncommon..stub: symbol not found\nError relocating /k8s-for-beginners: reflect..reflect.rtype..d: symbol not found\nError relocating /k8s-for-beginners: type...1net.IPAddr: symbol not found\nError relocating /k8s-for-beginners: net.IPAddr.Network: symbol not found\nError relocating /k8s-for-beginners: net.IPAddr.Network: symbol not found\nError relocating /k8s-for-beginners: net.IPAddr.String: symbol not found\nError relocating /k8s-for-beginners: net.IPAddr.String: symbol not found\nError relocating /k8s-for-beginners: net.IPAddr.family: symbol not found\nError relocating /k8s-for-beginners: net.IPAddr.family: symbol not found\nError relocating /k8s-for-beginners: net.IPAddr.isWildcard: symbol not found\nError relocating /k8s-for-beginners: net.IPAddr.isWildcard: symbol not found\nError relocating /k8s-for-beginners: net.IPAddr.sockaddr: symbol not found\nError relocating /k8s-for-beginners: net.IPAddr.sockaddr: symbol not found\nError relocating /k8s-for-beginners: net.IPAddr.toLocal: symbol not found\nError relocating /k8s-for-beginners: net.IPAddr.toLocal: symbol not found\nError relocating /k8s-for-beginners: net.IPAddr..d: symbol not found\nError relocating /k8s-for-beginners: runtime.main: symbol not found\nError relocating /k8s-for-beginners: runtime_iscgo: symbol not found\nError relocating /k8s-for-beginners: runtime_isstarted: symbol not found\nError relocating /k8s-for-beginners: runtime_isarchive: symbol not found\nError relocating /k8s-for-beginners: __gcc_personality_v0: symbol not found\nError relocating /k8s-for-beginners: io.Writer..d: symbol not found\nError relocating /k8s-for-beginners: runtime.writeBarrier: symbol not found\n\n`\n```",
      "solution": "Since you're already using `Docker`, I'd suggest using a multi-stage build. Using a standard docker image like `golang` one can build an executable asset which is guaranteed to work with other docker linux images:\n```\n`FROM golang:1.17 as builder\n\n# first (build) stage\n\nWORKDIR /app\nCOPY . .\nRUN go mod download\nRUN CGO_ENABLED=0 go build -o k8s-for-beginners\n\n# final (target) stage\n\nFROM alpine:3.10\nCOPY --from=builder /app/k8s-for-beginners /\nCMD [\"/k8s-for-beginners\"]\n`\n```\nNote: you since you are using `CGO_ENABLED=0` you can run your app in a much smaller `scratch` container. To do so replace `FROM alpine:3.10` with `FROM scratch`\n\nP.S. ensure you have a `go.mod` file in the same directory as the `Go` source and `Dockerfile`:\n```\n`go mod init k8sapp     # creates a `go.mod`\n`\n```\nor you can create `go.mod` manually:\n```\n`module k8sapp\n\ngo 1.17\n`\n```",
      "question_score": 23,
      "answer_score": 7,
      "created_at": "2021-08-22T13:49:08",
      "url": "https://stackoverflow.com/questions/68881023/docker-standard-init-linux-go228-exec-user-process-caused-no-such-file-or-dir"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubernetes",
      "question_id": 66275458,
      "title": "Could not access Kubernetes Ingress in Browser on Windows Home with Minikube?",
      "problem": "I am facing the problem which is that I could not access the Kubernetes Ingress on the Browser using it's IP. I have installed K8s and Minikube on Windows 10 Home.\nI am following this official document - https://kubernetes.io/docs/tasks/access-application-cluster/ingress-minikube/\n\nFirst I created the deployment by running this below command on Minikube.\nkubectl create deployment web --image=gcr.io/google-samples/hello-app:1.0\n\nThe deployment get created which can be seen on the below image:\n\nNext, I exposed the deployment that I created above. For this I ran the below command.\nkubectl expose deployment web --type=NodePort --port=8080\n\nThis created a service which can be seen by running the below command:\n```\n`kubectl get service web\n`\n```\nThe screenshot of the service is shown below:\n\nI can now able to visit the service on the browser by running the below command:\nminikube service web\n\nIn the below screenshot you can see I am able to view it on the browser.\n\nNext, I created an Ingress by running the below command:\nkubectl apply -f https://k8s.io/examples/service/networking/example-ingress.yaml\n\nBy the way the ingress yaml code is:\n```\n`apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: example-ingress\n  annotations:\n    nginx.ingress.kubernetes.io/rewrite-target: /$1\n    spec:\n      rules:\n        - host: hello-world.info\n          http:\n            paths:\n              - path: /\n                pathType: Prefix\n                backend:\n                  service:\n                    name: web\n                    port:\n                      number: 8080\n`\n```\nThe ingress gets created and I can verify it by running the below command:\n```\n`kubectl get ingress\n`\n```\nThe screenshot for this is given below:\n\nThe ingress ip is listed as `192.168.49.2`. So that means if I should open it in the browser then it should open, but unfortunately not. It is showing site can't be reached. See the below screeshot.\n\nWhat is the problem. Please provide me a solution for it?\nI also added the mappings on etc\\hosts file.\n```\n`192.168.49.2 hello-world.info\n`\n```\nThen I also tried opening hello-world.info on the browser but no luck.\nIn the below picture I have done ping to `hello-world.info` which is going to IP address 192.168.49.2. This shows etc\\hosts mapping is correct:\n\nI also did curl to minikube ip and to `hello-world.info` and both get timeout. See below image:\n\nThe `kubectl describe services web` provides the following details:\n```\n`Name:                     web\nNamespace:                default\nLabels:                   app=web\nAnnotations:              \nSelector:                 app=web\nType:                     NodePort\nIP:                       10.100.184.92\nPort:                       8080/TCP\nTargetPort:               8080/TCP\nNodePort:                   31880/TCP\nEndpoints:                172.17.0.4:8080\nSession Affinity:         None\nExternal Traffic Policy:  Cluster\nEvents:                   \n`\n```\nThe `kubectl describe ingress example-ingress` gives the following output:\n```\n`Name:             example-ingress\nNamespace:        default\nAddress:          192.168.49.2\nDefault backend:  default-http-backend:80 ()\nRules:\n  Host              Path  Backends\n  ----              ----  --------\n  hello-world.info\n                    /   web:8080   172.17.0.4:8080)\nAnnotations:        nginx.ingress.kubernetes.io/rewrite-target: /$1\nEvents:             \n`\n```\nKindly help. Thank you.",
      "solution": "Having same issue as OP and things only work in `minikube ssh`, sharing the ingress.yaml below.\n```\n`apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: frontend-ingress\n  annotations:\n    nginx.ingress.kubernetes.io/rewrite-target: /$2\nspec:\n  defaultBackend:\n    service:\n      name: default-http-backend\n      port:\n        number: 80\n  rules:\n    - host: myapp-com # domain (i.e. need to change host table)\n      http:\n        paths: # specified path below, only be working when there is more than 1 path; If only having 1 path, it's always using / as path\n          - path: /\n            pathType: Prefix\n            backend:\n              service: \n                name: frontend-service # internal service\n                port: \n                  number: 8080 # port number that internal service exposes\n          - path: /e($|/)(.*)\n            pathType: Prefix\n            backend:\n              service: \n                name: express-service # internal service\n                port: \n                  number: 3000 # port number that internal service exposes\n\n`\n```",
      "question_score": 23,
      "answer_score": 7,
      "created_at": "2021-02-19T11:04:27",
      "url": "https://stackoverflow.com/questions/66275458/could-not-access-kubernetes-ingress-in-browser-on-windows-home-with-minikube"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubernetes",
      "question_id": 65864009,
      "title": "Running Kubernetes locally on M1 Mac",
      "problem": "I'm looking to see if it's currently possible to run Kubernetes locally on a 2020 M1 MacBook air.\nThe environment I need is relatively simple, just for going through some tutorials. As an example, this operator-sdk guide.\nSo far I've tried `microk8s` and `minikube`, as they're tools I've used before on other machines.\nFor both of these, I've installed them using `brew` after opening the terminal app \"with Rosetta 2\"\n(i.e like this). My progress is then:\nMinikube\nWhen I run `minikube start --driver=docker` (having installed the tech preview of Docker Desktop for M1), an initialization error occurs. It seems to me that this is being tracked here https://github.com/kubernetes/minikube/issues/9224.\nMicrok8s\n`microk8s install` asks to install `multipass`, which then errors with `An error occurred with the instance when trying to start with 'multipass': returned exit code 2. Ensure that 'multipass' is setup correctly and try again.`. Multipass shows a `microk8s-vm` stuck in starting. I think this may relate to this issue https://github.com/canonical/multipass/issues/1857.\nI'm aware I'd probably be better chasing up those issues for help on these particular errors. What would be great is any general advice on if it's currently possible/advisable to setup a basic Kubernetes env for playing with on an M1 mac. I'm not experienced with the underlying technologies here, so any additional context is welcome. :)\nIf anyone has suggestions for practising Kubernetes, alternative to setting up a local cluster, I'd also appreciate them. Thanks!",
      "solution": "First, it is usually good to have Docker when working with containers. Docker now has a Tech Preview of Docker for Apple M1 based macs.\nWhen you have a workin Docker on your machine, it should also work to use Kind - a way to run Kubernetes on Docker containers.",
      "question_score": 23,
      "answer_score": 15,
      "created_at": "2021-01-23T21:22:42",
      "url": "https://stackoverflow.com/questions/65864009/running-kubernetes-locally-on-m1-mac"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubernetes",
      "question_id": 72263445,
      "title": "Kubernetes Pod terminates with Exit Code 143",
      "problem": "I am using a containerized Spring boot application in Kubernetes. But the application automatically exits and restarts with exit code 143 and error message \"Error\".\nI am not sure how to identify the reason for this error.\nMy first idea was that Kubernetes stopped the container due to too high resource usage, as described here, but I can't see the corresponding kubelet logs.\nIs there any way to identify the cause/origin of the `SIGTERM`? Maybe from spring-boot itself, or from the JVM?",
      "solution": "I also encountered similar problems too.\nBecause my program has scheduled task execution, I finally found out that a piece of my code was causing a memory leak by using Jmeter pressure testing locally and combining it with JProfiler.\nAs the program runs, the memory in the Eden area gradually decreases, and the Old area gradually increases until it reaches the maximum JVM heap memory, which finally triggers a container restart!",
      "question_score": 22,
      "answer_score": 1,
      "created_at": "2022-05-16T19:42:50",
      "url": "https://stackoverflow.com/questions/72263445/kubernetes-pod-terminates-with-exit-code-143"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubernetes",
      "question_id": 70742380,
      "title": "How to `kubectl get all` in k9s?",
      "problem": "Instead of navigating a namespace via e.g. `:service`, then `:pod` etc, I would like to see everything that's in the namespace in a single view. As if you would type `kubectl -n argocd get all`.\nCan't find the info in the docs. Is this even possible?",
      "solution": "As per Mikolaj S. answer, there is now a feature called `workloads` that displays something similar to `kubectl get all` (thread on github)\nTo use it, just type `:workloads` to view all namespace's resources or `:workloads your-namespace` to filter all resources from a specific namespace.\n\nThe only drawback is that this view cannot display CRDs, so to view those you'll still need to switch between them.",
      "question_score": 22,
      "answer_score": 16,
      "created_at": "2022-01-17T14:45:19",
      "url": "https://stackoverflow.com/questions/70742380/how-to-kubectl-get-all-in-k9s"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubernetes",
      "question_id": 66645437,
      "title": "How to resolve Heartbeat took longer than &quot;00:00:01&quot; failure?",
      "problem": "I have a .NetCore C# project which performs an HTTP POST. The project is set up in Kubernetes and I've noticed the logs below:\n```\n`Heartbeat took longer than \"00:00:01\" at \"02/22/2020 15:43:45 +00:00\".\nwarn: Microsoft.AspNetCore.Server.Kestrel[22]\n      Heartbeat took longer than \"00:00:01\" at \"02/22/2020 15:43:46 +00:00\".\nwarn: Microsoft.AspNetCore.Server.Kestrel[22]\n      Heartbeat took longer than \"00:00:01\" at \"02/22/2020 15:43:47 +00:00\".\nwarn: Microsoft.AspNetCore.Server.Kestrel[22]\n      Heartbeat took longer than \"00:00:01\" at \"02/22/2020 15:43:48 +00:00\".\nwarn: Microsoft.AspNetCore.Server.Kestrel[22]\n      Heartbeat took longer than \"00:00:01\" at \"02/22/2020 15:43:49 +00:00\".\nwarn: Microsoft.AspNetCore.Server.Kestrel[22]\n      Heartbeat took longer than \"00:00:01\" at \"02/22/2020 15:43:50 +00:00\".\nwarn: Microsoft.AspNetCore.Server.Kestrel[22]\n      Heartbeat took longer than \"00:00:01\" at \"02/22/2020 15:43:51 +00:00\".\nwarn: Microsoft.AspNetCore.Server.Kestrel[22]\n      Heartbeat took longer than \"00:00:01\" at \"02/22/2020 15:43:52 +00:00\".\nwarn: Microsoft.AspNetCore.Server.Kestrel[22]\n      Heartbeat took longer than \"00:00:01\" at \"02/22/2020 15:43:53 +00:00\".\nwarn: Microsoft.AspNetCore.Server.Kestrel[22]\n      Heartbeat took longer than \"00:00:01\" at \"02/22/2020 15:43:54 +00:00\".\nwarn: Microsoft.AspNetCore.Server.Kestrel[22]\n      Heartbeat took longer than \"00:00:01\" at \"02/22/2020 15:43:55 +00:00\".\nwarn: Microsoft.AspNetCore.Server.Kestrel[22]\n      Heartbeat took longer than \"00:00:01\" at \"02/22/2020 15:43:56 +00:00\".\nwarn: Microsoft.AspNetCore.Server.Kestrel[22]\n      Heartbeat took longer than \"00:00:01\" at \"02/22/2020 15:44:33 +00:00\".\nwarn: Microsoft.AspNetCore.Server.Kestrel[22]\n      Heartbeat took longer than \"00:00:01\" at \"02/22/2020 15:44:34 +00:00\".\nwarn: Microsoft.AspNetCore.Server.Kestrel[22]\n      Heartbeat took longer than \"00:00:01\" at \"02/22/2020 15:44:35 +00:00\".\n`\n```\nAfter some initial research, it seems this is a common result of threadpool starvation. Accordingly, in November last year, I made the post asynchronous and also logged the Max threads and Available threads as follows for monitoring purposes:\n```\n`ThreadPool.GetMaxThreads(out int workerThreads, out int completionPortThreads);\nThreadPool.GetAvailableThreads(out int workerThreadAvailable, out int completionPortThreadsAvailable);\n_log.Info(new { message = $\"Max threads = {workerThreads} and Available threads = {workerThreadAvailable}\" });\n`\n```\nConsistently over the past few months, the logging shows: Max threads = 32767 and Available threads = 32766. That seems fine, however, I'm noticing the same Heartbeat error so am wondering if this really is a threadpool starvation issue. Might someone know what else is going on and if this error is actually a result of something else? Any investigation/resolution tips for this would be much appreciated!",
      "solution": "This is a resource issue, as @andy pointed out in his response.\nAccording to OP, the solution to this problem is to either increase the server's CPU capacity (vertically) or the number of instances of your app (horizontally).",
      "question_score": 22,
      "answer_score": 10,
      "created_at": "2021-03-15T21:47:52",
      "url": "https://stackoverflow.com/questions/66645437/how-to-resolve-heartbeat-took-longer-than-000001-failure"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubernetes",
      "question_id": 69828547,
      "title": "PRECONDITION_FAILED: Delivery Acknowledge Timeout on Celery &amp; RabbitMQ with Gevent and concurrency",
      "problem": "I just switched from ForkPool to gevent with concurrency (5) as the pool method for Celery workers running in Kubernetes pods. After the switch I've been getting a non recoverable erro in the worker:\n`amqp.exceptions.PreconditionFailed: (0, 0): (406) PRECONDITION_FAILED - delivery acknowledgement on channel 1 timed out. Timeout value used: 1800000 ms. This timeout value can be configured, see consumers doc guide to learn more`\nThe broker logs gives basically the same message:\n`2021-11-01 22:26:17.251 [warning]  Consumer None4 on channel 1 has timed out waiting for delivery acknowledgement. Timeout used: 1800000 ms. This timeout value can be configured, see consumers doc guide to learn more`\nI have the `CELERY_ACK_LATE` set up, but was not familiar with the necessity to set a timeout for the acknowledgement period. And that never happened before using processes. Tasks can be fairly long (60-120 seconds sometimes), but I can't find a specific setting to allow that.\nI've read in another post in other forum a user who set the timeout on the broker configuration to a huge number (like 24 hours), and was also having the same problem, so that makes me think there may be something else related to the issue.\nAny ideas or suggestions on how to make worker more resilient?",
      "solution": "For future reference, it seems that the new RabbitMQ versions (+3.8) introduced a tight default for `consumer_timeout` (15min I think).\nThe solution I found (that has also been added to Celery docs not long ago here) was to just add a large number for the `consumer_timeout` in RabbitMQ.\nIn this question, someone mentions setting consumer_timeout to false, in a way that using a large number is not needed, but apparently there's some specifics regarding the format of the configuration for that to work.\nI'm running RabbitMQ in k8s and just done something like:\n`rabbitmq.conf: |\n  consumer_timeout = 31622400000\n`",
      "question_score": 21,
      "answer_score": 15,
      "created_at": "2021-11-03T17:36:50",
      "url": "https://stackoverflow.com/questions/69828547/precondition-failed-delivery-acknowledge-timeout-on-celery-rabbitmq-with-geve"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubernetes",
      "question_id": 66431556,
      "title": "What is the relationship between the HPA and ReplicaSet in Kubernetes?",
      "problem": "I can't seem to find an answer to this but what is the relationship between an HPA and ReplicaSet? From what I know we define a Deployment object which defines replicas which creates the RS and the RS is responsible for supervising our pods and scale up and down.\nWhere does the HPA fit into this picture? Does it wrap over the Deployment object? I'm a bit confused as you define the number of replicas in the manifest for the Deployment object.\nThank you!",
      "solution": "When we create a deployment it create a replica set and number of pods (that we gave in `replicas`). Deployment control the RS, and RS controls pods. Now, HPA is another abstraction which give the instructions to deployment and through RS make sure the pods fullfil the respective scaling.\nAs far the k8s doc: The Horizontal Pod Autoscaler automatically scales the number of Pods in a replication controller, deployment, replica set or stateful set based on observed CPU utilization (or, with custom metrics support, on some other application-provided metrics). Note that Horizontal Pod Autoscaling does not apply to objects that can't be scaled, for example, DaemonSets.\nA brief high level overview is: Basically it's all about controller. Every k8s object has a controller, when a deployment object is created then respective controller creates the rs and associated pods, rs controls the pods, deployment controls rs. On the other hand, when hpa controllers sees that at any moment number of pods gets higher/lower than expected then it talks to deployment.\nRead more from k8s doc",
      "question_score": 21,
      "answer_score": 31,
      "created_at": "2021-03-02T01:20:48",
      "url": "https://stackoverflow.com/questions/66431556/what-is-the-relationship-between-the-hpa-and-replicaset-in-kubernetes"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubernetes",
      "question_id": 66539203,
      "title": "What are the practical differences between an IP vs instance based target types for an AWS NLB?",
      "problem": "I'm using an AWS NLB to expose my Kubernetes pods to the internet. The NLB is currently using `instance` target types but I noticed there are also `IP` target types. What are the differences between an `instance` target type vs an `IP` target type from a practical point of view? When would you use one over the other?\nAWS's documentation specifies some restrictions around using `IP` target types, namely they must be within certain CIDRs and constraints around maximums, but I'm trying to understand when you might want to use one over the other.\nI don't know if it has any impact, but we've been having issues with our kubernetes `rollingUpdate` deployments where we're seeing downtime for the pods as they switch over (we have `liveness` and `readiness` checks there).",
      "solution": "The three key use-cases for using IP target type:\n\nyour target does not have to be an instance - anything with private IP address will work, including internal load balance, VPC private service, Fargate containers, databases, on-premise servers through VPN.\nyour target can be in different region, as long as you have cross-region peering between your VPCs\nyou have multiple network interfaces on your instance, so you can load distribute traffic between them, e.g. different applications on a single instance are bind to different interfaces. Each interface can be associated with different target group.\n\nInstance target type is only limited instances. It should be your default choice when load balancing instances. For example, if you have instances in autoscaling group (ASG), the ASG can automatically register your instances with your load balancer. You can't do this for IP target types.",
      "question_score": 21,
      "answer_score": 13,
      "created_at": "2021-03-09T01:37:20",
      "url": "https://stackoverflow.com/questions/66539203/what-are-the-practical-differences-between-an-ip-vs-instance-based-target-types"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubernetes",
      "question_id": 71164538,
      "title": "Argocd application resource stuck at deletion",
      "problem": "I've started experimenting with Argocd as part of my cluster setup and set it up to watch a test repo containing some yaml files for a small application I wanted to use for the experiment. While getting to know the system a bit, I broke the repo connection and instead of fixing it I decided that I had what I wanted, and decided to do a clean install with the intention of configuring it towards my actual project.\nI pressed the button in the web UI for deleting the application, which got stuck. After which I read that adding `spec.syncPolicy.allowEmpty: true ` and removing the `metadata.finalizers` declaration from the application yaml file. This did not allow me to remove the application resource.\nI then ran an uninstall command with the official manifests/install.yaml as an argument, which cleaned up most resources installed, but left the application resource and the namespace. Command: `kubectl delete -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml`\nHave tried to use the kubectl delete application NAME `--force` flag and the `--cascade=orphans` flag on the application resource as well as on the argocd namespace itself. Now I have both of them stuck at terminating without getting any further.\nNow I'm proper stuck as I can't reinstall the argocd in any way I know due to the resources and namespace being marked for deletion, and I'm at my wits end as to what else I can try in order to get rid of the dangling application resource.\nAny and all suggestions as to what to look into is much appreciated.",
      "solution": "If your problem is that the namespace cannot be deleted, the following two solutions may help you\uff1a\n\nCheck what resources are stuck in the deletion process, delete these resources, and then delete ns\nEdit the namespace of argocd, check if there is a finalizer field in the spec, delete that field and the content of the field\n\nHopefully it helped you.",
      "question_score": 20,
      "answer_score": 6,
      "created_at": "2022-02-17T20:50:07",
      "url": "https://stackoverflow.com/questions/71164538/argocd-application-resource-stuck-at-deletion"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubernetes",
      "question_id": 71207905,
      "title": "minkube start..gives error: &quot;Exiting due to RSRC_INSUFFICIENT_CORES&quot;..Is it possible to start minikube on this Mac with 2 CPU cores?",
      "problem": "I want to start minikube to learn Kubernetes but am having trouble because of error `RSRC_INSUFFICIENT_CORES`.\nMy mac has 2 CPU cores and minikube docs say that 2 cores are required.\nHere a the machine specs from \"About this Mac\":\n\nMacBook Pro (15-inch, Late 2008)\nProcessor 2.4 GHz Intel Core 2 Duo\nMemory 8 GB 1067 MHz DDR3\n\nThis machine has VirtualBox Version 5.2.35 r135669 but its not running, and working docker and docker-machine, as shown here:\n```\n`\u2717 docker-machine --version\ndocker-machine version 0.16.1, build \n\n\u2717 docker --version\nDocker version 17.05.0-ce, build 89658be\n`\n```\nI have successfully installed minikube v1.25.1 using an updated version of MacPorts, as shown here:\n```\n`\u2717 which minikube    \n/opt/local/bin/minikube\n\n\u2717 minikube version\n\nminikube version: v1.25.1\n`\n```\nI cannot start minikube and get error: `Exiting due to RSRC_INSUFFICIENT_CORES`.  Here is the output that I see from 2 different `minikube start` attempts:\n```\n`\u2717 minikube start --cpus=2\n\n\ud83d\ude04   minikube v1.25.1 on Darwin 10.11.6\n\u2728   Automatically selected the docker driver. Other choices: virtualbox, ssh\n- Ensure your docker daemon has access to enough CPU/memory resources.\n- Docs https://docs.docker.com/docker-for-mac/#resources\n\n\u26d4   Exiting due to RSRC_INSUFFICIENT_CORES: Requested cpu count 2 is greater than the available cpus of 1\n\n\u2717 minikube start --cpus=1\n\n\ud83d\ude04   minikube v1.25.1 on Darwin 10.11.6\n\u2728   Automatically selected the docker driver. Other choices: virtualbox, ssh\n\n\u26d4   Exiting due to RSRC_INSUFFICIENT_CORES: Requested cpu count 1 is less than the minimum allowed of 2\n`\n```\nPlease excuse newbie-ness--this is my first ever SO question!\nIs it impossible to start minikube on this Mac?",
      "solution": "To enforce operation on a single core, you can use the following options\n\n--extra-config=kubeadm.ignore-preflight-errors=NumCPU --force --cpus=1\n\nPlease note that docker and `minikube` were designed to run on at least two cores. If available, please consider enabling hyperthreading.",
      "question_score": 19,
      "answer_score": 15,
      "created_at": "2022-02-21T15:22:24",
      "url": "https://stackoverflow.com/questions/71207905/minkube-start-gives-error-exiting-due-to-rsrc-insufficient-cores-is-it-poss"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubernetes",
      "question_id": 66353603,
      "title": "Correct way to perform a reconnect with gRPC client",
      "problem": "I have a Go gRPC client connected to a gRPC server running in a different pod in my k8s cluster.\nIt's working well, receiving and processing requests.\nI am now wondering how best to implement resiliency in the event that the gRPC server pod gets recycled.\nAs far as I can ascertain, the clientconn.go code should handle the reconnection automatically, but I just cannot get it to work and I fear my implementation is incorrect in the first instance.\nCalling code from main:\n```\n`go func() {     \n        if err := gRPCClient.ProcessRequests(); err != nil {\n            log.Error(\"Error while processing Requests\")\n            //do something here??\n        }\n    }()\n`\n```\nMy code in the gRPCClient wrapper module:\n```\n`func (grpcclient *gRPCClient) ProcessRequests() error {\n    defer grpcclient.Close()    \n\n    for {\n        request, err := reqclient.stream.Recv()\n        log.Info(\"Request received\")\n        if err == io.EOF {          \n            break\n        }\n        if err != nil {\n            //when pod is recycled, this is what's hit with err:\n            //rpc error: code = Unavailable desc = transport is closing\"\n\n            //what is the correct pattern for recovery here so that we can await connection\n            //and continue processing requests once more?\n            //should I return err here and somehow restart the ProcessRequests() go routine in the \n            //main funcition?\n            break\n            \n        } else {\n            //the happy path\n            //code block to process any requests that are received\n        }\n    }\n\n    return nil\n}\n\nfunc (reqclient *RequestClient) Close() {\n//this is called soon after the conneciton drops\n        reqclient.conn.Close()\n}\n`\n```\nEDIT:\nEmin Laletovic answered my question elegantly below and gets it most of the way there.\nI had to make a few changes to the waitUntilReady function:\n```\n`func (grpcclient *gRPCClient) waitUntilReady() bool {\nctx, cancel := context.WithTimeout(context.Background(), 300*time.Second) //define how long you want to wait for connection to be restored before giving up\ndefer cancel()\n\ncurrentState := grpcclient.conn.GetState()\nstillConnecting := true\n\nfor currentState != connectivity.Ready && stillConnecting {\n    //will return true when state has changed from thisState, false if timeout\n    stillConnecting = grpcclient.conn.WaitForStateChange(ctx, currentState)\n    currentState = grpcclient.conn.GetState()\n    log.WithFields(log.Fields{\"state: \": currentState, \"timeout\": timeoutDuration}).Info(\"Attempting reconnection. State has changed to:\")\n}\n\nif stillConnecting == false {\n    log.Error(\"Connection attempt has timed out.\")\n    return false\n}\n\nreturn true\n}\n`\n```",
      "solution": "The RPC connection is being handled automatically by `clientconn.go`, but that doesn't mean the stream is also automatically handled.\nThe stream, once broken, whether by the RPC connection breaking down or some other reason, cannot reconnect automatically, and you need to get a new stream from the server once the RPC connection is back up.\nThe pseudo-code for waiting the RPC connection to be in the `READY` state and establishing a new stream might look something like this:\n```\n`func (grpcclient *gRPCClient) ProcessRequests() error {\n    defer grpcclient.Close()    \n    \n    go grpcclient.process()\n    for {\n      select {\n        case EDIT:\nRevisiting the code above, a couple of mistakes should be corrected. The `WaitForStateChange` function waits for the connection state to change from the passed state, it doesn't wait for the connection to change into the passed state.\nIt is better to track the current state of the connection and use the `Connect` function to connect if the channel is idle.\n```\n`func (grpcclient *gRPCClient) ProcessRequests() error {\n        defer grpcclient.Close()    \n        \n        go grpcclient.process()\n        for {\n          select {\n            case <- grpcclient.reconnect:\n               if !grpcclient.isReconnected(1*time.Second, 60*time.Second) {\n                 return errors.New(\"failed to establish a connection within the defined timeout\")\n               }\n               go grpcclient.process()\n            case <- grpcclient.done:\n              return nil\n          }\n        }\n}\n\nfunc (grpcclient *gRPCClient) isReconnected(check, timeout time.Duration) bool {\n  ctx, cancel := context.context.WithTimeout(context.Background(), timeout)\n  defer cancel()\n  ticker := time.NewTicker(check)\n\n  for{\n    select {\n      case <- ticker.C:\n        grpcclient.conn.Connect()\n \n        if grpcclient.conn.GetState() == connectivity.Ready {\n          return true\n        }\n      case <- ctx.Done():\n         return false\n    }\n  }\n}\n`\n```",
      "question_score": 19,
      "answer_score": 17,
      "created_at": "2021-02-24T16:08:56",
      "url": "https://stackoverflow.com/questions/66353603/correct-way-to-perform-a-reconnect-with-grpc-client"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubernetes",
      "question_id": 72480039,
      "title": "Kubernetes nginx ingress controller cannot upload size more than 1mb",
      "problem": "I am fairly new to GCP and I have a rest URI to upload large files.\nI have a ngress-nginx-controller service and want to change it to upload files larger than 1mb and set a limit.\n```\n`apiVersion: v1\nkind: Service\nmetadata:\n  annotations:\n    kubectl.kubernetes.io/last-applied-configuration: |\n      {\"apiVersion\":\"v1\",\"kind\":\"Service\",\"metadata\":{\"annotations\":{},\"labels\":{\"app.kubernetes.io/component\":\"controller\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/managed-by\":\"Helm\",\"app.kubernetes.io/name\":\"ingress-nginx\",\"app.kubernetes.io/version\":\"0.35.0\",\"helm.sh/chart\":\"ingress-nginx-2.13.0\"},\"name\":\"ingress-nginx-controller\",\"namespace\":\"ingress-nginx\"},\"spec\":{\"externalTrafficPolicy\":\"Local\",\"ports\":[{\"name\":\"http\",\"port\":80,\"protocol\":\"TCP\",\"targetPort\":\"http\"},{\"name\":\"https\",\"port\":443,\"protocol\":\"TCP\",\"targetPort\":\"https\"}],\"selector\":{\"app.kubernetes.io/component\":\"controller\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"type\":\"LoadBalancer\"}}\n  creationTimestamp: \"2020-09-21T18:37:27Z\"\n  finalizers:\n  - service.kubernetes.io/load-balancer-cleanup\n  labels:\n    app.kubernetes.io/component: controller\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/version: 0.35.0\n    helm.sh/chart: ingress-nginx-2.13.0\n  name: ingress-nginx-controller\n  namespace: ingress-nginx\n`\n```\nThis is the error it throws :\n```\n`\n413 Request Entity Too Large\n\n413 Request Entity Too Large\nnginx/1.19.2\n\n`\n```",
      "solution": "If you need to increase the body size of files you upload via the ingress controller, you need to add an annotation to your ingress resource:\n```\n`nginx.ingress.kubernetes.io/proxy-body-size: 8m\n`\n```\nDocumentation available here: https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/#custom-max-body-size",
      "question_score": 18,
      "answer_score": 38,
      "created_at": "2022-06-02T19:20:46",
      "url": "https://stackoverflow.com/questions/72480039/kubernetes-nginx-ingress-controller-cannot-upload-size-more-than-1mb"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubernetes",
      "question_id": 66341494,
      "title": "kOps 1.19 reports error &quot;Unauthorized&quot; when interfacing with AWS cluster",
      "problem": "I'm following the kOps tutorial to set up a cluster on AWS. I am able to create a cluster with\n```\n`kops create cluster\nkops update cluster --yes\n`\n```\nHowever, when validating whether my cluster is set up correctly with\n```\n`kops validate cluster\n`\n```\nI get stuck with error:\n```\n`unexpected error during validation: error listing nodes: Unauthorized\n`\n```\nThe same error happens in many other kOps operations.\nI checked my kOps/K8s version and it is 1.19:\n```\n`> kops version\nVersion 1.19.1 (git-8589b4d157a9cb05c54e320c77b0724c4dd094b2)\n\n> kubectl version\nClient Version: version.Info{Major:\"1\", Minor:\"20\" ...\nServer Version: version.Info{Major:\"1\", Minor:\"19\" ...\n`\n```\nHow can I fix this?",
      "solution": "As of kOps 1.19 there are two reasons you will suddenly get this error:\n\nIf you delete a cluster and reprovision it, your old admin is not removed from the kubeconfig and kOps/kubectl tries to reuse it.\nNew certificates have a TTL of 18h by default, so you need to reprovision them about once a day.\n\nBoth issues above are fixed by running `kops export kubecfg --admin`.\nNote that using the default TLS credentials is discouraged. Consider things like using an OIDC provider instead.",
      "question_score": 18,
      "answer_score": 42,
      "created_at": "2021-02-23T22:39:17",
      "url": "https://stackoverflow.com/questions/66341494/kops-1-19-reports-error-unauthorized-when-interfacing-with-aws-cluster"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubernetes",
      "question_id": 75994613,
      "title": "Unable to connect to the server: getting credentials: exec: executable kubelogin not found",
      "problem": "I created new `config` file for Kubernetes from `Azure` in `Powershell` by `az aks get-credentials --resource-group  --name `. Got a message that `Merged \"cluster_name\" as current context in C:\\michu\\.kube\\config`. I copied this file into default `.kube\\config` location and now when I try to run any command e.g `kubectl get pods` I am receiving:\n```\n`Unable to connect to the server: getting credentials: exec: executable kubelogin not found\n\nIt looks like you are trying to use a client-go credential plugin that is not installed.\n\nTo learn more about this feature, consult the documentation available at:\n      https://kubernetes.io/docs/reference/access-authn-authz/authentication/#client-go-credential-plugins\n`\n```\nWhat is wrong here?\nI just realized that when I type `kubectl config get-contexts` then I can see my `cluster_name` and I can even switch to this by `kubectl config use-context cluster_name` and message is correct: `Switched to context cluster_name` but then still all other commands ends with `Unable to connect to the server: getting credentilas: exec: executable kubelogin not found`",
      "solution": "The error implies that the `kubelogin` executable could not be located. You need to install `kubelogin` in the azure cli using\n`az aks install-cli`, then it works as expected.\nRefer github for installation process.\nI tried the same requirement in my environment, and it worked for me as follows.\n`az aks get-credentials --resource-group caroline --name sampleaks1\nkubectl get pods\n`\nOutput:\n\nOnce you have the `aks` credentials, running `kubectl get pods` will prompt you for an `Azure kubernetes service authentication with AAD`, as shown.\n\nJust give `kubectl` in the bash to verify whether it is installed successfully.\n\nIf still the issue persists,\n\nDelete all the cache or any unused folders inside the ~/.kube/ and ran the aks credentials command by adding\n`--admin` flag in the end.\nRefer this doc by @Geert Baeke for more related information.\n\nCheck the kube config version and upgrade if required.",
      "question_score": 18,
      "answer_score": 21,
      "created_at": "2023-04-12T12:52:55",
      "url": "https://stackoverflow.com/questions/75994613/unable-to-connect-to-the-server-getting-credentials-exec-executable-kubelogin"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubernetes",
      "question_id": 74741993,
      "title": "0/1 nodes are available: 1 pod has unbound immediate PersistentVolumeClaims",
      "problem": "As the documentation states:\n\nFor each VolumeClaimTemplate entry defined in a StatefulSet, each Pod\nreceives one PersistentVolumeClaim. In the nginx example above, each\nPod receives a single PersistentVolume with a StorageClass of\nmy-storage-class and 1 Gib of provisioned storage. If no StorageClass\nis specified, then the default StorageClass will be used. When a Pod\nis (re)scheduled onto a node, its volumeMounts mount the\nPersistentVolumes associated with its PersistentVolume Claims. Note\nthat, the PersistentVolumes associated with the Pods' PersistentVolume\nClaims are not deleted when the Pods, or StatefulSet are deleted. This\nmust be done manually.\n\nThe part I'm interested in is this: `If no StorageClassis specified, then the default StorageClass will be used`\nI create a StatefulSet like this:\n```\n`apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  namespace: ches\n  name: ches\nspec:\n  serviceName: ches\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ches\n  template:\n    metadata:\n      labels:\n        app: ches\n    spec:\n      serviceAccountName: ches-serviceaccount\n      nodeSelector:\n        ches-worker: \"true\"\n      volumes:\n      - name: data\n        hostPath:\n          path: /data/test\n      containers:\n      - name: ches\n        image: [here I have the repo]\n        imagePullPolicy: Always\n        securityContext:\n            privileged: true\n        args:\n        - server\n        - --console-address\n        - :9011\n        - /data\n        env:\n        - name: MINIO_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: ches-keys\n              key: access-key\n        - name: MINIO_SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: ches-keys\n              key: secret-key\n        ports:\n        - containerPort: 9000\n          hostPort: 9011\n        resources:\n          limits:\n            cpu: 100m\n            memory: 200Mi\n        volumeMounts:\n        - name: data\n          mountPath: /data\n      imagePullSecrets:\n        - name: edge-storage-token\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 1Gi\n`\n```\nOf course I have already created the secrets, imagePullSecrets etc and I have labeled the node as ches-worker.\nWhen I apply the yaml file, the pod is in Pending status and `kubectl describe pod ches-0 -n ches` gives the following error:\n\nWarning  FailedScheduling  6s    default-scheduler  0/1 nodes are\navailable: 1 pod has unbound immediate PersistentVolumeClaims.\npreemption: 0/1 nodes are available: 1 Preemption is not helpful for\nscheduling\n\nAm I missing something here?",
      "solution": "K3s when installed, also downloads a storage class which makes it as default.\nCheck with `kubectl get storageclass`:\n```\n`NAME        PROVISIONER             RECLAIMPOLICY   VOLUMEBINDINGMODE       ALLOWVOLUMEEXPANSION   AGE \nlocal-path  rancher.io/local-path   Delete          WaitForFirstConsumer    false                  8s\n`\n```\nK8s cluster on the other hand, does not download also a default storage class.\nIn order to solve the problem:\n\nDownload rancher.io/local-path storage class:\nkubectl apply -f\nhttps://raw.githubusercontent.com/rancher/local-path-provisioner/master/deploy/local-path-storage.yaml\n\nCheck with `kubectl get storageclass`\n\nMake this storage class (local-path) the default:\nkubectl patch\nstorageclass local-path -p '{\"metadata\":\n{\"annotations\":{\"storageclass.kubernetes.io/is-default-class\":\"true\"}}}'",
      "question_score": 18,
      "answer_score": 10,
      "created_at": "2022-12-09T11:43:22",
      "url": "https://stackoverflow.com/questions/74741993/0-1-nodes-are-available-1-pod-has-unbound-immediate-persistentvolumeclaims"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubernetes",
      "question_id": 75026935,
      "title": "Pod creation in EKS cluster fails with FailedScheduling error",
      "problem": "I have created a new EKS cluster with 1 worker node in a public subnet. I am able to query node, connect to the cluster, and run pod creation command, however, when I am trying to create a pod it fails with the below error got by describing the pod. Please guide.\n```\n`    Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\n    Events:\n      Type     Reason            Age   From               Message\n      ----     ------            ----  ----               -------\n      Warning  FailedScheduling  81s   default-scheduler  0/1 nodes are available: 1 Too many pods. preemption: 0/1 nodes are available: 1 No preemption victims found for incoming pod.\n  Warning  FailedScheduling  16m                 default-scheduler  0/2 nodes are available: 2 Too many pods, 2 node(s) had untolerated taint {node.kubernetes.io/unschedulable: }, 2 node(s) were unschedulable. preemption: 0/2 nodes are available: 2 Preemption is not helpful for scheduling.\n  Warning  FailedScheduling  16m                 default-scheduler  0/3 nodes are available: 2 node(s) had untolerated taint {node.kubernetes.io/unschedulable: }, 2 node(s) were unschedulable, 3 Too many pods. preemption: 0/3 nodes are available: 1 No preemption victims found for incoming pod, 2 Preemption is not helpful for scheduling.\n  Warning  FailedScheduling  14m (x3 over 22m)   default-scheduler  0/2 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/unschedulable: }, 1 node(s) were unschedulable, 2 Too many pods. preemption: 0/2 nodes are available: 1 No preemption victims found for incoming pod, 1 Preemption is not helpful for scheduling.\n  Warning  FailedScheduling  12m                 default-scheduler  0/2 nodes are available: 1 Too many pods, 2 node(s) had untolerated taint {node.kubernetes.io/unschedulable: }, 2 node(s) were unschedulable. preemption: 0/2 nodes are available: 2 Preemption is not helpful for scheduling.\n  Warning  FailedScheduling  7m14s               default-scheduler  no nodes available to schedule pods\n  Warning  FailedScheduling  105s (x5 over 35m)  default-scheduler  0/1 nodes are available: 1 Too many pods. preemption: 0/1 nodes are available: 1 No preemption victims found for incoming pod.\n`\n```\nI am able to get status of the node and it looks ready:\n```\n`kubectl get nodes  \nNAME                         STATUS   ROLES    AGE   VERSION\nip-10-0-12-61.ec2.internal   Ready       15m   v1.24.7-eks-fb459a0\n`\n```\nWhile troubleshooting I tried below options:\n\nrecreate the complete demo cluster - still the same error\ntry recreating pods with different images - still the same error\ntrying to increase to instance type to t3.micro - still the same error\nreviewed security groups and other parameters in a cluster - Couldnt come to RCA",
      "solution": "it's due to the node's POD limit or IP limit on Nodes.\nSo if we see official Amazon doc, t3.micro maximum 2 interface you can use and 2 private IP. Roughly you might be getting around 4 IPs to use and 1st IP get used by Node etc, There will be also default system PODs running as Daemon set and so.\nAdd new instance or upgrade to larger instance who can handle more pods.",
      "question_score": 18,
      "answer_score": 19,
      "created_at": "2023-01-06T05:29:55",
      "url": "https://stackoverflow.com/questions/75026935/pod-creation-in-eks-cluster-fails-with-failedscheduling-error"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubernetes",
      "question_id": 66213199,
      "title": "Config not found: /etc/kubernetes/admin.conf -- After setting up kubeadm worker node",
      "problem": "Following this tutorial, I set up a worker node for my cluster. However, after running the `join` command and attempting `kubectl get node` to verify the node was connected, I am met with the following error\n```\n`W0215 17:58:44.648813 3084402 loader.go:223] Config not found: /etc/kubernetes/admin.conf\nThe connection to the server localhost:8080 was refused - did you specify the right host or port?\n`\n```\nChecking for the existence of `admin.conf` in `/etc/kubernetes/` shows it does not exist. I have ensured that `$HOME/.kube/config` is also clear. Why is the join command not creating an admin.conf file?",
      "solution": "TLDR\n\nRun `join` with sudo\n` mv /etc/kubernetes/kubelet.conf /etc/kubernetes/admin.conf`\n\nAfter some tinkering, I realized it was a combination of a permissions error and the correct file being generated with an incorrect name.\nInstead of running `kubeadm join ...` naked, running with sudo allowed for the command to create files necessary in `/etc/kubernetes`\n```\n`sudo kubeadm join  --token      --discovery-token-ca-cert-hash \n`\n```\nHowever this doesn't generate an `admin.conf`, but does create a `kubelet.conf`. I'm unsure why this happens, and couldn't find any documentation on this behavior, however running `kubectl` with the following parameter solved my issue\n```\n`kubectl get nodes --kubeconfig /etc/kubernetes/kubelet.conf\n`\n```\nRename `kubelet.conf` to `admin.conf` for your convenience at this point.",
      "question_score": 18,
      "answer_score": 18,
      "created_at": "2021-02-15T19:08:25",
      "url": "https://stackoverflow.com/questions/66213199/config-not-found-etc-kubernetes-admin-conf-after-setting-up-kubeadm-worker"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubernetes",
      "question_id": 66450970,
      "title": "Kubernetes ingress nginx redirect to https",
      "problem": "To redirect any HTTP traffic to HTTPS on tls enabled hosts, I have added the below annotation to my ingress resources\n```\n`nignx.ingress.kubernetes.io/force-ssl-redirect: true\n`\n```\nWith this when I curl the host in question, I get redirected as expected\n\nBut when I use a browser, the request to HTTP times out.\nNow, I am not sure if it's something I am doing wrong at Nginx ingress conf as curl works?\nAny pointers please? Thanks!\ncomplete annotaiotns:\n```\n`   annotations:\n    kubernetes.io/ingress.class: nginx-ingress\n    nginx.ingress.kubernetes.io/force-ssl-redirect: \"true\"\n    nginx.ingress.kubernetes.io/proxy-body-size: 100m\n    nginx.ingress.kubernetes.io/proxy-connect-timeout: \"300\"\n    nginx.ingress.kubernetes.io/proxy-read-timeout: \"300\"\n    nginx.ingress.kubernetes.io/proxy-send-timeout: \"300\"\n    nginx.ingress.kubernetes.io/ssl-passthrough: \"false\"\n    nginx.ingress.kubernetes.io/ssl-redirect: \"false\"\n`\n```\nrules\n```\n` rules:\n  - host: hostX\n    http:\n      paths:\n      - backend:\n          serviceName: svcX\n          servicePort: 8080\n        path: /\n  - host: hostY\n    http:\n      paths:\n      - backend:\n          serviceName: svcX\n          servicePort: 8080\n        path: /\n  tls:\n  - hosts:\n    - hostX\n  - hosts:\n    - hostY\n    secretName: hostY-secret-tls\n`\n```\nNote:\n\nThe curl mentioned is to hostY in the rule above.\nHTTPS to hostY via browser works and so cert is valid one.",
      "solution": "As @mdaniel have mentioned your snippet shows `nignx.ingress.kubernetes.io/force-ssl-redirect: true` but annotations should be strings. Notice that in your \"complete\" config, you have both `force-ssl-redirect: \"true\"`  (now correctly a string) and `ssl-redirect: \"false\"` .\nSimply remove annotation  `nginx.ingress.kubernetes.io/ssl-redirect: \"false\"` and leave just `nginx.ingress.kubernetes.io/force-ssl-redirect: \"true\"`\nAlso enable `--enable-ssl-passthrough`. This is required to enable passthrough backends in Ingress objects.\nYour annotation should look like:\n```\n`kubernetes.io/ingress.class: nginx-ingress\nnginx.ingress.kubernetes.io/force-ssl-redirect: \"true\"\nnginx.ingress.kubernetes.io/ssl-passthrough: \"true\"\nnginx.ingress.kubernetes.io/proxy-body-size: 100m\nnginx.ingress.kubernetes.io/proxy-connect-timeout: \"300\"\nnginx.ingress.kubernetes.io/proxy-read-timeout: \"300\"\nnginx.ingress.kubernetes.io/proxy-send-timeout: \"300\"\n`\n```\nIf you defined hosts under TLS section they are going to be accessible only using https. HTTP requests are being redirected to use HTTPS. That is why you cannot access host via HTTP. Also you have to specify secret for host `hostX`, otherwise the default certificate will be used for ingress. Or if you don't want to connect to host `hostX` via HTTPS simply create different ingress without TLS section for it.\nTake a look: .",
      "question_score": 18,
      "answer_score": 14,
      "created_at": "2021-03-03T05:49:04",
      "url": "https://stackoverflow.com/questions/66450970/kubernetes-ingress-nginx-redirect-to-https"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubernetes",
      "question_id": 69320925,
      "title": "Should we set -Xmx (max java heap size) in a kubernetes container",
      "problem": "TLDR;\nWhy bother settings `-Xmx` and `-Xms` in a kubernetes app?\n\nWe've inherited a kubernetes app that is having out of memory errors.  It looks like the initial devs set kubernetes resource limits (.95 CPU core and 4Gb memory) as shown below.  However, they also set the max heap size in `JAVA_OPTS -Xmx` to 800mb.\n\nI've found lots of good material on best settings for `-Xmx` (eg this one), but can't find a straight answer to the following question: do we really need to set `-Xmx` (and less importantly, `-Xms`) in a kubernetes container?  We've already set hard limits on the container\n`resources`, so what is the point to setting these flags?  If we removed them altogether, what would the consequence be?  Would the app GC more often?  Would it scale heap size dynamically or would heap size max be fixed at some default maxium like 256MB?  Is there any rule of thumb of setting `-Xmx` in proportion to kubernets containers?",
      "solution": "If you don't set a maximum heap size the behavior depends on the java version used. Current JREs support determining the container limits and use that to guide their internal heuristics.\nwhen running with an older version the memory to be used for the heap will be determined based on the physical memory visible to the container, which might be excessive.\nNote that hitting the memory limit of the container will lead to killing the process and restarting the container.\nI suggest you use a sane value, as determined from operational testing, for Xmx and Xms to get a stable memory usage.",
      "question_score": 18,
      "answer_score": 4,
      "created_at": "2021-09-24T22:43:55",
      "url": "https://stackoverflow.com/questions/69320925/should-we-set-xmx-max-java-heap-size-in-a-kubernetes-container"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubernetes",
      "question_id": 67061272,
      "title": "Kubernetes Ingress: Internal error occurred: failed calling webhook &quot;validate.nginx.ingress.kubernetes.io&quot;",
      "problem": "Playing around with K8 and ingress in local minikube setup. Creating ingress from yaml file in networking.k8s.io/v1 api version fails. See below output.\nExecuting\n```\n`> kubectl apply -f ingress.yaml\n`\n```\nreturns\n```\n`Error from server (InternalError): error when creating \"ingress.yaml\": Internal error occurred: failed calling webhook \"validate.nginx.ingress.kubernetes.io\": an error on the server (\"\") has prevented the request from succeeding\n`\n```\nin local minikube environment with hyperkit as vm driver.\nHere is the `ingress.yaml` file:\n```\n`apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: mongodb-express-ingress\n  namespace: hello-world\n  annotations:\n    nginx.ingress.kubernetes.io/rewrite-target: /$1\nspec:\n  rules:\n    - host: mongodb-express.local\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: mongodb-express-service-internal\n                port:\n                  number: 8081\n`\n```\nHere is the mongodb-express deployment file:\n```\n`apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mongodb-express\n  namespace: hello-world\n  labels:\n    app: mongodb-express\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: mongodb-express\n  template:\n    metadata:\n      labels:\n        app: mongodb-express\n    spec:\n      containers:\n        - name: mongodb-express\n          image: mongo-express\n          ports:\n            - containerPort: 8081\n          env:\n            - name: ME_CONFIG_MONGODB_ADMINUSERNAME\n              valueFrom:\n                secretKeyRef:\n                  name: mongodb-secret\n                  key: mongodb-root-username\n            - name: ME_CONFIG_MONGODB_ADMINPASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: mongodb-secret\n                  key: mongodb-root-password\n            - name: ME_CONFIG_MONGODB_SERVER\n              valueFrom:\n                configMapKeyRef:\n                  name: mongodb-configmap\n                  key: mongodb_url\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: mongodb-express-service-external\n  namespace: hello-world\nspec:\n  selector:\n    app: mongodb-express\n  type: LoadBalancer\n  ports:\n    - protocol: TCP\n      port: 8081\n      targetPort: 8081\n      nodePort: 30000\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: mongodb-express-service-internal\n  namespace: hello-world\nspec:\n  selector:\n    app: mongodb-express\n  ports:\n    - protocol: TCP\n      port: 8081\n      targetPort: 8081\n`\n```\nSome more information:\n```\n`> kubectl version\nClient Version: version.Info{Major:\"1\", Minor:\"19\", GitVersion:\"v1.19.7\", GitCommit:\"1dd5338295409edcfff11505e7bb246f0d325d15\", GitTreeState:\"clean\", BuildDate:\"2021-01-13T13:23:52Z\", GoVersion:\"go1.15.5\", Compiler:\"gc\", Platform:\"darwin/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"20\", GitVersion:\"v1.20.2\", GitCommit:\"faecb196815e248d3ecfb03c680a4507229c2a56\", GitTreeState:\"clean\", BuildDate:\"2021-01-13T13:20:00Z\", GoVersion:\"go1.15.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n\n> minikube version\nminikube version: v1.19.0\ncommit: 15cede53bdc5fe242228853e737333b09d4336b5\n\n> kubectl get all -n hello-world\nNAME                                   READY   STATUS    RESTARTS   AGE\npod/mongodb-68d675ddd7-p4fh7           1/1     Running   0          3h29m\npod/mongodb-express-6586846c4c-5nfg7   1/1     Running   6          3h29m\n\nNAME                                       TYPE           CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE\nservice/mongodb-express-service-external   LoadBalancer   10.106.185.132        8081:30000/TCP   3h29m\nservice/mongodb-express-service-internal   ClusterIP      10.103.122.120           8081/TCP         3h3m\nservice/mongodb-service                    ClusterIP      10.96.197.136            27017/TCP        3h29m\n\nNAME                              READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/mongodb           1/1     1            1           3h29m\ndeployment.apps/mongodb-express   1/1     1            1           3h29m\n\nNAME                                         DESIRED   CURRENT   READY   AGE\nreplicaset.apps/mongodb-68d675ddd7           1         1         1       3h29m\nreplicaset.apps/mongodb-express-6586846c4c   1         1         1       3h29m\n\n> minikube addons enable ingress\n    \u25aa Using image k8s.gcr.io/ingress-nginx/controller:v0.44.0\n    \u25aa Using image docker.io/jettech/kube-webhook-certgen:v1.5.1\n    \u25aa Using image docker.io/jettech/kube-webhook-certgen:v1.5.1\n\ud83d\udd0e  Verifying ingress addon...\n\ud83c\udf1f  The 'ingress' addon is enabled\n\n> kubectl get all -n ingress-nginx\nNAME                                            READY   STATUS      RESTARTS   AGE\npod/ingress-nginx-admission-create-2bn8h        0/1     Completed   0          4h4m\npod/ingress-nginx-admission-patch-vsdqn         0/1     Completed   0          4h4m\npod/ingress-nginx-controller-5d88495688-n6f67   1/1     Running     0          4h4m\n\nNAME                                         TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                      AGE\nservice/ingress-nginx-controller             NodePort    10.111.176.223           80:32740/TCP,443:30636/TCP   4h4m\nservice/ingress-nginx-controller-admission   ClusterIP   10.97.107.77             443/TCP                      4h4m\n\nNAME                                       READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/ingress-nginx-controller   1/1     1            1           4h4m\n\nNAME                                                  DESIRED   CURRENT   READY   AGE\nreplicaset.apps/ingress-nginx-controller-5d88495688   1         1         1       4h4m\n\nNAME                                       COMPLETIONS   DURATION   AGE\njob.batch/ingress-nginx-admission-create   1/1           7s         4h4m\njob.batch/ingress-nginx-admission-patch    1/1           9s         4h4m\n`\n```\n\nHowever, it works for the beta api version, i.e.\n```\n`apiVersion: networking.k8s.io/v1beta1\nkind: Ingress\nmetadata:\n  name: mongodb-express-ingress-deprecated\n  namespace: hello-world\nspec:\n  rules:\n    - host: mongodb-express.local\n      http:\n        paths:\n          - path: /\n            backend:\n              serviceName: mongodb-express-service-internal\n              servicePort: 8081\n`\n```\nAny help very much appreciated.",
      "solution": "I have the same problem with you, and you can see this issue https://github.com/kubernetes/minikube/issues/11121.\nTwo way you can try:\n\ndownload the new version ,or go back the old version\nDo a strange thing like what balnbibarbi said.\n\n2. The Strange Thing\n```\n`# Run without --addons=ingress\nsudo minikube start --vm-driver=none #--addons=ingress\n\n# install external ingress-nginx\nsudo helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx\nsudo helm repo update\nsudo helm install ingress-nginx ingress-nginx/ingress-nginx\n\n# expose your services\n`\n```\nAnd then you will find your Ingress lacks Endpoints. And then:\n`sudo minikube addons enable ingress\n`\nAfter minitues, the Endpoints appears.\nProblem\nIf you search examples with addons Ingress by Google, you will find what the below lacks is ingress.\n```\n`root@ubuntu:~# kubectl get pods -n kube-system \nNAME                             READY   STATUS    RESTARTS   AGE\ncoredns-74ff55c5b-xnmx2          1/1     Running   1          4h40m\netcd-ubuntu                      1/1     Running   1          4h40m\nkube-apiserver-ubuntu            1/1     Running   1          4h40m\nkube-controller-manager-ubuntu   1/1     Running   1          4h40m\nkube-proxy-k9lnl                 1/1     Running   1          4h40m\nkube-scheduler-ubuntu            1/1     Running   2          4h40m\nstorage-provisioner              1/1     Running   3          4h40m\n`\n```",
      "question_score": 17,
      "answer_score": 3,
      "created_at": "2021-04-12T17:34:18",
      "url": "https://stackoverflow.com/questions/67061272/kubernetes-ingress-internal-error-occurred-failed-calling-webhook-validate-ng"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubectl",
      "question_id": 71318743,
      "title": "kubectl versions Error: exec plugin is configured to use API version client.authentication.k8s.io/v1alpha1",
      "problem": "I was setting up my new Mac for my eks environment.\nAfter the installation of kubectl, aws-iam-authenticator and the kubeconfig file placement in default location. I ran the command kubectl command and got this error mentioned below in command block.\nMy cluster uses v1alpha1 client auth api version so basically i wanted to use the same one in my Mac as well.\nI tried with latest version (1.23.0) of kubectl as well, still the same error. Whereas When i tried to do with aws-iam-authenticator (version 0.5.5) I was not able to download lower version.\nCan someone help me to resolve it?\n```\n`% kubectl version          \nClient Version: version.Info{Major:\"1\", Minor:\"20\", GitVersion:\"v1.20.0\", GitCommit:\"af46c47ce925f4c4ad5cc8d1fca46c7b77d13b38\", GitTreeState:\"clean\", BuildDate:\"2020-12-08T17:59:43Z\", GoVersion:\"go1.15.5\", Compiler:\"gc\", Platform:\"darwin/amd64\"}\nUnable to connect to the server: getting credentials: exec plugin is configured to use API version client.authentication.k8s.io/v1alpha1, plugin returned version client.authentication.k8s.io/v1beta1\n`\n```",
      "solution": "You're using `aws-iam-authenticator` `0.5.5`, AWS changed the way it behaves in `0.5.4` to require `v1beta1`.\nIt depends on your configuration, but you can try to change the K8s context you're using to `v1beta1`\nby checking your kubeconfig file (usually in `~/.kube/config`) from `client.authentication.k8s.io/v1alpha1` to `client.authentication.k8s.io/v1beta1`\nOtherwise switch back to `aws-iam-authenticator` `0.5.3` - you might need to build it from source if you're using the M1 architecture as there's no `darwin-arm64` binary built for it",
      "question_score": 43,
      "answer_score": 59,
      "created_at": "2022-03-02T08:15:01",
      "url": "https://stackoverflow.com/questions/71318743/kubectl-versions-error-exec-plugin-is-configured-to-use-api-version-client-auth"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubectl",
      "question_id": 72274548,
      "title": "How to remove warning in kubectl with gcp auth plugin?",
      "problem": "When I run any kubectl command I get following WARNING:\n```\n`W0517 14:33:54.147340   46871 gcp.go:120] WARNING: the gcp auth plugin is deprecated in v1.22+, unavailable in v1.25+; use gcloud instead.\nTo learn more, consult https://cloud.google.com/blog/products/containers-kubernetes/kubectl-auth-changes-in-gke\n`\n```\nI have followed the instructions in the link several times but the WARNING keeps appearing making kubectl output uncomfortable to read.\nOS:\n```\n`cat /etc/lsb-release \nDISTRIB_ID=Ubuntu\nDISTRIB_RELEASE=22.04\nDISTRIB_CODENAME=jammy\nDISTRIB_DESCRIPTION=\"Ubuntu 22.04 LTS\"\n`\n```\nkubectl version:\n```\n`Client Version: v1.24.0\nKustomize Version: v4.5.4\n`\n```\ngke-gcloud-auth-plugin:\n```\n`Kubernetes v1.23.0-alpha+66064c62c6c23110c7a93faca5fba668018df732\n`\n```\ngcloud version:\n```\n`Google Cloud SDK 385.0.0\nalpha 2022.05.06\nbeta 2022.05.06\nbq 2.0.74\nbundled-python3-unix 3.9.12\ncore 2022.05.06\ngsutil 5.10\n`\n```\nI \"login\" with:\n```\n`gcloud init\n`\n```\nand then:\n```\n`gcloud container clusters get-credentials cluster_name --region my-region\n`\n```\nfinally:\n```\n`myyser@mymachine:/$ k get pods -n madeupns\nW0517 14:50:10.570103   50345 gcp.go:120] WARNING: the gcp auth plugin is deprecated in v1.22+, unavailable in v1.25+; use gcloud instead.\nTo learn more, consult https://cloud.google.com/blog/products/containers-kubernetes/kubectl-auth-changes-in-gke\nNo resources found in madeupns namespace.\n`\n```\nHow can I remove the WARNING or fix the problem?\nRemoving my `.kube/config` and re-running get-credentials didn't work.",
      "solution": "I fixed this problem by adding the correct export in `.bashrc`\n```\n`export USE_GKE_GCLOUD_AUTH_PLUGIN=True\n`\n```\nAfter sourcing `.bashrc` with `. ~/.bashrc` and reloading cluster config with:\n```\n`gcloud container clusters get-credentials clustername\n`\n```\nthe warning dissapeared:\n```\n`user@laptop:/$ k get svc -A\nNAMESPACE     NAME                   TYPE           CLUSTER-IP     EXTERNAL-IP    \nkube-system   default-http-backend   NodePort       10.10.13.157            \nkube-system   kube-dns               ClusterIP      10.10.0.10              \nkube-system   kube-dns-upstream      ClusterIP      10.10.13.92             \nkube-system   metrics-server         ClusterIP      10.10.2.191             \n`\n```",
      "question_score": 39,
      "answer_score": 48,
      "created_at": "2022-05-17T15:01:40",
      "url": "https://stackoverflow.com/questions/72274548/how-to-remove-warning-in-kubectl-with-gcp-auth-plugin"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubectl",
      "question_id": 67604805,
      "title": "How can get READY, STATUS, RESTARTS, AGE,etc in kubectl as custom-columns?",
      "problem": "I  just want to list pods with their `.status.podIP` as an extra column.\nIt seems that as soon as I specify `-o=custom-colums=` the default columns `NAME, READY, STATUS, RESTARTS, AGE` will disappear.\nThe closest I was able to get is\n```\n`kubectl get pod -o wide -o=custom-columns=\"NAME:.metadata.name,STATUS:.status.phase,RESTARTS:.status.containerStatuses[0].restartCount,PODIP:.status.podIP\"\n`\n```\nbut that is not really equivalent to the the default columns in the following way:\n\nREADY: I don't know how to get the default output (which looks like `2/2` or `0/1` by using custom columns\nSTATUS: In the default behaviour STATUS, can be Running, Failed, Evicted, but `.status.phase` will never be `Evicted`. It seems that the default STATUS is a combination of `.status.phase` and `.status.reason`. Is there a way to say show `.status.phase` if it's Running but if not show `.status.reason`?\nRESTARTS: This only shows the restarts of the first container in the pod (I guess the sum of all containers would be the correct one)\nAGE: Again I don't know how to get the age of the pod using custom-columns\n\nDoes anybody know the definitions of the default columns in custom-columns syntax?",
      "solution": "I checked the differences between in API requests between the `kubectl get pods` and `kubectl -o custom columns`:\n\nWith aggregation:\n\n`curl -k -v -XGET -H Accept: application/json;as=Table;v=v1;g=meta.k8s.io,application/json;as=Table;v=v1beta1;g=meta.k8s.io,application/json -H User-Agent: kubectl/v1.18.8 (linux/amd64) kubernetes/9f2892a http://127.0.0.1:8001/api/v1/namespaces/default/pods?limit=500 \n`\n\nWithout aggregation:\n\n`curl -k -v -XGET -H Accept: \napplication/json -H User-Agent: kubectl/v1.18.8 (linux/amd64) kubernetes/9f2892a http://127.0.0.1:8001/api/v1/namespaces/default/pods?limit=500\n`\nSo you will notice that when `-o custom columns` is used, kubectl gets `PodList` instead of `Table` in response body. Podlist does not have that aggregated data, so to my understanding it is not possible to get the same output with kubectl pods using `custom-column`.\nHere's a code snippet responsible for the output that you desire. Possible solution would be to fork the client and customize it to your own needs since as you already might notice this output requires some custom logic. Another possible solution would be to use one of the Kubernetes api client libraries.  Lastly you may want to try extend kubectl functionalities with kubectl plugins.",
      "question_score": 17,
      "answer_score": 7,
      "created_at": "2021-05-19T16:04:00",
      "url": "https://stackoverflow.com/questions/67604805/how-can-get-ready-status-restarts-age-etc-in-kubectl-as-custom-columns"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubectl",
      "question_id": 68013476,
      "title": "Kubernetes: Cannot convert int64 to string. Kubernetes fails to interpret integer value in helmchart values.yaml file",
      "problem": "I have a `values.yaml` file in which I have given `spring_datasource_hikari_maximum_pool_size: \"10\"`\nIn `deployment yaml` I have used this value as\n```\n` - name: SPRING_DATASOURCE_HIKARI_MAXIMUM-POOL-SIZE\n    value: {{ .Values.spring_datasource_hikari_maximum_pool_size }}\n`\n```\nHowever, when used inside the `deployment.yaml `file it fails with the below error.\n```\n`\nDeploy failed: The request is invalid: patch: Invalid value: \"map[metadata:map[annotations:map[kubectl.kubernetes.io/last-applied-configuration:{\"apiVersion\":\"apps/v1\",\"kind\":\"Deployment\",\"metadata\":{\"annotations\":\n{\n(helm values etc)\n`{\"name\":\"SPRING_DATASOURCE_HIKARI_MAXIMUM-POOL-SIZE\",\"value\":10}]` **(this is the incorrect value)** \n}\ncannot convert int64 to string\n\n`\n```\nWhat is the correct format of using an integer value from `values.yaml `file in a `deployment.yaml `file?\nI have also tried multiple combinations with quotes \"\" but nothing seems to be working.\nAny help is appreciated, Thanks in advance.",
      "solution": "I was able to resolve this by using double quotes on the `value` itself in `deployment.yaml` file\n```\n`- name: SPRING_DATASOURCE_HIKARI_MAXIMUM-POOL-SIZE\n  value: \"{{ .Values.spring_datasource_hikari_maximum_pool_size }}\"\n`\n```\nSince this was a production instance I could not check with @David Maze and Vit's solution.\nEdit:\nTried with `quote` option and it worked too.\n```\n` - name: SPRING_DATASOURCE_HIKARI_MAXIMUMPOOLSIZE \n   value: {{ quote .Values.spring_datasource_hikari_maximum_pool_size }}\n`\n```",
      "question_score": 15,
      "answer_score": 28,
      "created_at": "2021-06-17T07:34:47",
      "url": "https://stackoverflow.com/questions/68013476/kubernetes-cannot-convert-int64-to-string-kubernetes-fails-to-interpret-intege"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubectl",
      "question_id": 70628716,
      "title": "Print from jq using a wild card (or coalesce to first non null)",
      "problem": "I have the following command:\n\n`kubectl get pod -A -o=json | jq -r '.items[]|select(any( .status.containerStatuses[]; .state.waiting or .state.terminated))|[.metadata.namespace, .metadata.name]|@csv'`\n\nThis command works great.  It outputs both the namespace and name of my failing pods.\nBut now I want to add one more column to the results.  The column I want is located in one (and only one) of two places:\n\n.status.containerStatuses[].state.waiting.reason\n.status.containerStatuses[].state.terminated.reason\n\nI first tried adding `.status.containerStatuses[].state.*.reason` to the results fields array.  But that gave me an `unexpected '*'` compile error.\nI then got to thinking about how I would do this with SQL or another programming language.  They frequently have a function that will return the first non-null value of its parameters.  (This is usually called coalesce).  However I could not find any such command for `jq`.\nHow can I return the `reason` as a result of my query?",
      "solution": "jq has a counterpart to \"coalesce\" in the form of `//`.\nFor example, `null // 0` evaluates to 0, and chances are that it will suffice in your case, perhaps:\n```\n`.status.containerStatuses[].state | (.waiting // .terminated) | .reason\n`\n```\nor\n```\n`.status.containerStatuses[].state | (.waiting.reason // .terminated.reason )\n`\n```\nor similar.\nHowever, `//` should only be used with some understanding of what it does, as explained in detail on the jq FAQ at https://github.com/stedolan/jq/wiki/FAQ#or-versus-\nIf `//` is inapplicable for some reason, then the obvious alternative would be an `if ... then ... else ... end` statement, which is quite like C's `_ ? _ : _` in that it can be used to produce a value, e.g. something along the lines of:\n```\n`.status.containerStatuses[].state\n| if has(\"waiting\") then .waiting.reason\n  else .terminated.reason\n  end\n`\n```\nHowever, if `containerStatuses` is an array, then some care may be required.",
      "question_score": 12,
      "answer_score": 17,
      "created_at": "2022-01-08T01:42:37",
      "url": "https://stackoverflow.com/questions/70628716/print-from-jq-using-a-wild-card-or-coalesce-to-first-non-null"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubectl",
      "question_id": 66868323,
      "title": "After certificates renewal, an error: &quot;You must be logged in to the server (Unauthorized)&quot;",
      "problem": "My certificates were expired:\n```\n`root@ubuntu:~# kubectl get pods\nUnable to connect to the server: x509: certificate has expired or is not yet valid                                                                                                                                                           \n`\n```\nI verified it by running:\n```\n`root@ubuntu:~# kubeadm alpha certs check-expiration\n[check-expiration] Reading configuration from the cluster...\n[check-expiration] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'\n[check-expiration] Error reading configuration from the Cluster. Falling back to default configuration\n\nW0330 09:18:49.875780   12562 configset.go:202] WARNING: kubeadm cannot validate component configs for API groups [kubelet.config.k8s.io kubepro                                                                                             xy.config.k8s.io]\nCERTIFICATE                EXPIRES                  RESIDUAL TIME   CERTIFICATE AUTHORITY   EXTERNALLY MANAGED\nadmin.conf                 Mar 29, 2021 09:27 UTC                                  no\napiserver                  Mar 29, 2021 09:27 UTC          ca                      no\napiserver-etcd-client      Mar 29, 2021 09:27 UTC          etcd-ca                 no\napiserver-kubelet-client   Mar 29, 2021 09:27 UTC          ca                      no\ncontroller-manager.conf    Mar 29, 2021 09:27 UTC                                  no\netcd-healthcheck-client    Mar 29, 2021 09:27 UTC          etcd-ca                 no\netcd-peer                  Mar 29, 2021 09:27 UTC          etcd-ca                 no\netcd-server                Mar 29, 2021 09:27 UTC          etcd-ca                 no\nfront-proxy-client         Mar 29, 2021 09:27 UTC          front-proxy-ca          no\nscheduler.conf             Mar 29, 2021 09:27 UTC                                  no\n\nCERTIFICATE AUTHORITY   EXPIRES                  RESIDUAL TIME   EXTERNALLY MANAGED\nca                      Mar 27, 2030 09:27 UTC   8y              no\netcd-ca                 Mar 27, 2030 09:27 UTC   8y              no\nfront-proxy-ca          Mar 27, 2030 09:27 UTC   8y              no\n`\n```\nI renew the certificates by running: `kubeadm alpha certs renew all`.\n```\n`W0330 09:20:21.951839   13124 configset.go:202] WARNING: kubeadm cannot validate component configs for API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io]\ncertificate embedded in the kubeconfig file for the admin to use and for kubeadm itself renewed\ncertificate for serving the Kubernetes API renewed\ncertificate the apiserver uses to access etcd renewed\ncertificate for the API server to connect to kubelet renewed\ncertificate embedded in the kubeconfig file for the controller manager to use renewed\ncertificate for liveness probes to healthcheck etcd renewed\ncertificate for etcd nodes to communicate with each other renewed\ncertificate for serving etcd renewed\ncertificate for the front proxy client renewed\ncertificate embedded in the kubeconfig file for the scheduler manager to use renewed\n`\n```\nAll the certificates are now updated to 2022 so it should be okay:\n```\n`CERTIFICATE                EXPIRES                  RESIDUAL TIME   CERTIFICATE AUTHORITY   EXTERNALLY MANAGED\nadmin.conf                 Mar 30, 2022 09:20 UTC   364d                                    no\napiserver                  Mar 30, 2022 09:20 UTC   364d            ca                      no\napiserver-etcd-client      Mar 30, 2022 09:20 UTC   364d            etcd-ca                 no\napiserver-kubelet-client   Mar 30, 2022 09:20 UTC   364d            ca                      no\ncontroller-manager.conf    Mar 30, 2022 09:20 UTC   364d                                    no\netcd-healthcheck-client    Mar 30, 2022 09:20 UTC   364d            etcd-ca                 no\netcd-peer                  Mar 30, 2022 09:20 UTC   364d            etcd-ca                 no\netcd-server                Mar 30, 2022 09:20 UTC   364d            etcd-ca                 no\nfront-proxy-client         Mar 30, 2022 09:20 UTC   364d            front-proxy-ca          no\nscheduler.conf             Mar 30, 2022 09:20 UTC   364d                                    no\n\nCERTIFICATE AUTHORITY   EXPIRES                  RESIDUAL TIME   EXTERNALLY MANAGED\nca                      Mar 27, 2030 09:27 UTC   8y              no\netcd-ca                 Mar 27, 2030 09:27 UTC   8y              no\nfront-proxy-ca          Mar 27, 2030 09:27 UTC   8y              no\n`\n```\nBut when I run `kubectl get pods` I received the error:\n```\n`error: You must be logged in to the server (Unauthorized)\n`\n```\nIt should be a problem with the certificate I think, but I am not sure how to fix it. Should I create new certificate and replace the one that inside the config file?",
      "solution": "The `~/.kube/config` wasn't updated with the changes.\nI ran:\n```\n`mkdir -p $HOME/.kube\nsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\nsudo chown $(id -u):$(id -g) $HOME/.kube/config  \n`\n```\nand it fixed it.",
      "question_score": 11,
      "answer_score": 30,
      "created_at": "2021-03-30T11:34:39",
      "url": "https://stackoverflow.com/questions/66868323/after-certificates-renewal-an-error-you-must-be-logged-in-to-the-server-unau"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubectl",
      "question_id": 67415637,
      "title": "Kubectl port forward reliably in a shell script",
      "problem": "I am using `kubectl port-forward` in a shell script but I find it is not reliable, or doesn't come up in time:\n```\n`kubectl port-forward ${VOLT_NODE} ${VOLT_CLUSTER_ADMIN_PORT}:${VOLT_CLUSTER_ADMIN_PORT} -n ${NAMESPACE} &\nif [ $? -ne 0 ]; then\n    echo \"Unable to start port forwarding to node ${VOLT_NODE} on port ${VOLT_CLUSTER_ADMIN_PORT}\"\n    exit 1\nfi\nPORT_FORWARD_PID=$!\n\nsleep 10\n`\n```\nOften after I sleep for 10 seconds, the port isn't open or forwarding hasn't happened. Is there any way to wait for this to be ready. Something like `kubectl wait` would be ideal, but open to shell options also.",
      "solution": "I took @AkinOzer's comment and turned it into this example where I port-forward a postgresql database's port so I can make a `pg_dump` of the database:\n```\n`#!/bin/bash\n\nset -e\n\nlocalport=54320\ntypename=service/pvm-devel-kcpostgresql\nremoteport=5432\n\n# This would show that the port is closed\n# nmap -sT -p $localport localhost || true\n\nkubectl port-forward $typename $localport:$remoteport > /dev/null 2>&1 &\n\npid=$!\n# echo pid: $pid\n\n# kill the port-forward regardless of how this script exits\ntrap '{\n    # echo killing $pid\n    kill $pid\n}' EXIT\n\n# wait for $localport to become available\nwhile ! nc -vz localhost $localport > /dev/null 2>&1 ; do\n    # echo sleeping\n    sleep 0.1\ndone\n\n# This would show that the port is open\n# nmap -sT -p $localport localhost\n\n# Actually use that port for something useful - here making a backup of the\n# keycloak database\nPGPASSWORD=keycloak pg_dump --host=localhost --port=54320 --username=keycloak -Fc --file keycloak.dump keycloak\n\n# the 'trap ... EXIT' above will take care of kill $pid\n`\n```",
      "question_score": 11,
      "answer_score": 25,
      "created_at": "2021-05-06T11:43:11",
      "url": "https://stackoverflow.com/questions/67415637/kubectl-port-forward-reliably-in-a-shell-script"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubectl",
      "question_id": 72481655,
      "title": "Creating a Kubernetes Dashboard Token",
      "problem": "I'm trying to follow the instructions at https://github.com/kubernetes/dashboard/blob/master/docs/user/access-control/creating-sample-user.md to create a Kubernetes Dashboard token.  However, when I run the specified command, I get an error\n```\n`% kubectl -n kubernetes-dashboard create token admin-user\nError: must specify one of -f and -k\n\nerror: unknown command \"token admin-user\"\nSee 'kubectl create -h' for help and examples\n`\n```\nIf I jump back in the doc history, I see a different, more verbose command that I can run\n```\n`% kubectl -n kubernetes-dashboard get secret $(kubectl -n kubernetes-dashboard get sa/admin-user -o jsonpath=\"{.secrets[0].name}\") -o go-template=\"{{.data.token | base64decode}}\"\n`\n```\nThis seems to work OK and PR for the doc change mentions \"version 1.24\" but doesn't mention what piece of software version 1.24 refers to (`kubectl`? The Dashboard? Kuberenetes itself? `kind`? Something else?)\nSo what's going on with that first command?  Why doesn't it work?",
      "solution": "This is a new feature in kubernetes 1.24, your cluster and `kubectl` must be running \n\nkubectl create token can now be used to request a service account\ntoken, and permission to request service account tokens is added to\nthe edit and admin RBAC roles (#107880, @liggitt)\n\nAnother snippet showing more relevant info:\n\nKubectl changes:\nAdds a command to kubectl to request a bound service account token.\nThis will help ease the transition from scraping generated service\naccount tokens with commands like kubectl get secret \"$(kubectl get\nserviceaccount default -o jsonpath='{.secrets[0].name}')\"\n\nBoth server and client must be running 1.24 or newer, something like below:\n```\n`kubectl version --output=json\n{\n  \"clientVersion\": {\n    \"major\": \"1\",\n    \"minor\": \"24\",\n    \"gitVersion\": \"v1.24.0\",\n    \"gitCommit\": \"4ce5a8954017644c5420bae81d72b09b735c21f0\",\n    \"gitTreeState\": \"clean\",\n    \"buildDate\": \"2022-05-03T13:46:05Z\",\n    \"goVersion\": \"go1.18.1\",\n    \"compiler\": \"gc\",\n    \"platform\": \"linux/amd64\"\n  },\n  \"kustomizeVersion\": \"v4.5.4\",\n  \"serverVersion\": {\n    \"major\": \"1\",\n    \"minor\": \"24\",\n    \"gitVersion\": \"v1.24.2\",\n    \"gitCommit\": \"f66044f4361b9f1f96f0053dd46cb7dce5e990a8\",\n    \"gitTreeState\": \"clean\",\n    \"buildDate\": \"2022-06-15T14:15:38Z\",\n    \"goVersion\": \"go1.18.3\",\n    \"compiler\": \"gc\",\n    \"platform\": \"linux/amd64\"\n  }\n}\n`\n```\ncheck this for more info: https://github.com/kubernetes/kubernetes/pull/107880",
      "question_score": 11,
      "answer_score": 9,
      "created_at": "2022-06-02T21:53:29",
      "url": "https://stackoverflow.com/questions/72481655/creating-a-kubernetes-dashboard-token"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubectl",
      "question_id": 68489985,
      "title": "Does &quot;kubectl rollout restart deploy&quot; cause downtime?",
      "problem": "I'm trying to get all the deployments of a namespace to be restarted for implementation reasons.\nI'm using \"kubectl rollout -n  restart deploy\" and it works perfectly, but I'm not sure it that command causes downtime or if it works as the \"rollout update\", applying the restart one by one, keeping my services up.\nDoes anyone know?\nIn the documentation I can only find this:\n\nOperation\nSyntax\nDescription\n\nrollout\nkubectl rollout SUBCOMMAND [options]\nManage the rollout of a resource. Valid resource types include: deployments, daemonsets and statefulsets.\n\nBut I can't find details about the specific \"rollout restart deploy\".\nI need to make sure it doesn't cause downtime. Right now is very hard to tell, because the restart process is very quick.\nUpdate: I know that for one specific deployment (kubectl rollout restart deployment/name), it works as expected and doesn't cause downtime, but I need to apply it to all the namespace (without specifying the deployment) and that's the case I'm not sure about.",
      "solution": "`kubectl rollout restart deploy -n namespace1` will restart all deployments in specified namespace with zero downtime.\nRestart command will work as follows:\n\nAfter restart it will create new pods for a each deployments\nOnce new pods are up (running and ready) it will terminate old pods\n\nAdd readiness probes to your deployments to configure initial delays.",
      "question_score": 9,
      "answer_score": 9,
      "created_at": "2021-07-22T20:38:19",
      "url": "https://stackoverflow.com/questions/68489985/does-kubectl-rollout-restart-deploy-cause-downtime"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubectl",
      "question_id": 65705619,
      "title": "Substitute environment variable in all files when kubectl apply-ing",
      "problem": "Let's say we have a number of Kubernetes configuration files in a folder `kubernetes` and we want to apply them all:\n```\n`kubectl apply -f kubernetes -n MyNamespace\n`\n```\nSome of these files contain environment variables which need to be substituted first (no templating in Kubernetes). For instance, several of the deployment yamls contain something like:\n```\n`image: myregistry.com/myrepo:$TAG\n`\n```\nFor a single yaml file, this can be done e.g. by using envsubst like this:\n```\n`envsubst What's the best way to do these substitutions for all the yaml files?\n(Looping over the files in the folder and calling `envsubst` as above is one option, but I suspect that it would be preferrable to pass the entire folder to `kubectl` and not individual files)",
      "solution": "This works:\n```\n`for f in *.yaml; do envsubst < $f | kubectl apply -f -; done\n`\n```",
      "question_score": 9,
      "answer_score": 10,
      "created_at": "2021-01-13T17:08:25",
      "url": "https://stackoverflow.com/questions/65705619/substitute-environment-variable-in-all-files-when-kubectl-apply-ing"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubectl",
      "question_id": 69837573,
      "title": "Restart a Kubernetes Job or Pod with a different command",
      "problem": "I'm looking for a way to quickly run/restart a Job/Pod from the command line and override the command to be executed in the created container.\nFor context, I have a Kubernetes Job that gets executed as a part of our deploy process. Sometimes that Job crashes and I need to run certain commands inside the container the Job creates to debug and fix the problem (subsequent Jobs then succeed).\nThe way I have done this so far is:\n\nCopy the YAML of the Job, save into a  file\nClean up the YAML (delete Kubernetes-managed fields)\nChange the `command:` field to `tail -f /dev/null` (so that the container stays alive)\n`kubectl apply -f job.yaml && kubectl get all && kubectl exec -ti pod/foobar bash`\nRun commands inside the container\n`kubectl delete job/foobar` when I am done\n\nThis is very tedious. I am looking for a way to do something like the following\n```\n`kubectl restart job/foobar --command \"tail -f /dev/null\"\n\n# or even better\nkubectl run job/foobar --exec --interactive bash\n`\n```\n\nI cannot use the `run` command to create a Pod:\n`kubectl run --image xxx -ti\n`\nbecause the Job I am trying to restart has certain `volumeMounts` and other configuration I need to reuse. So I would need something like `kubectl run --from-config job/foobar`.\n\nIs there a way to achieve this or am I stuck with juggling the YAML definition file?\n\nEdit: the Job YAML looks approx. like this:\n`apiVersion: batch/v1\nkind: Job\nmetadata:\n    name: database-migrations\n    labels:\n        app: myapp\n        service: myapp-database-migrations\nspec:\n    backoffLimit: 0\n    template:\n        metadata:\n            labels:\n                app: myapp\n                service: myapp-database-migrations\n        spec:\n            restartPolicy: Never\n            containers:\n                - name: migrations\n                  image: registry.example.com/myapp:977b44c9\n                  command:\n                      - \"bash\"\n                      - \"-c\"\n                      - |\n                          set -e -E\n                          echo \"Running database migrations...\"\n                          do-migration-stuff-here\n                          echo \"Migrations finished at $(date)\"\n                  imagePullPolicy: Always\n                  volumeMounts:\n                      -   mountPath: /home/example/myapp/app/config/conf.yml\n                          name: myapp-config-volume\n                          subPath: conf.yml\n                      -   mountPath: /home/example/myapp/.env\n                          name: myapp-config-volume\n                          subPath: .env\n            volumes:\n                - name: myapp-config-volume\n                  configMap:\n                      name: myapp\n            imagePullSecrets:\n                -   name: k8s-pull-project\n`",
      "solution": "The commands you suggested don't exist. Take a look at this reference where you can find all available commands.\nBased on that documentation the task of the Job is to create one or more Pods and continue retrying execution them until the specified number of successfully terminated ones will be achieved. Then the Job tracks the successful completions. You cannot just update the Job because these fields are not updatable. To do what's you want you should delete current job and create one once again.\n\nI recommend you to keep all your configurations in files. If you have a problem with configuring job commands, practice says that you should modify these settings in yaml and apply to the cluster - if your deployment crashes - by storing the configuration in files, you have a backup.\nIf you are interested how to improve this task, you can try those 2 examples describe below:\nFirstly I've created several files:\nexample job  (`job.yaml`):\n`apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: test1\nspec:\n  template:\n    spec:\n      containers:\n      - name: test1\n        image: busybox\n        command: [\"/bin/sh\", \"-c\", \"sleep 300\"]\n        volumeMounts:\n        - name: foo\n          mountPath: \"/script/foo\"\n      volumes:\n      - name: foo\n        configMap:\n          name: my-conf\n          defaultMode: 0755\n      restartPolicy: OnFailure\n`\n`patch-file.yaml`:\n`spec:\n  template:\n    spec:\n      containers:\n      - name: test1\n        image: busybox\n        command: [\"/bin/sh\", \"-c\", \"echo 'patching test' && sleep 500\"]\n`\nand  `configmap.yaml`:\n`apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: my-conf\ndata:\n  test: |\n    #!/bin/sh\n    echo \"skrypt test\"\n`\n\nIf you want to automate this process you can use `plugin`\n\nA plugin is a standalone executable file, whose name begins with `kubectl-`. To install a plugin, move its executable file to anywhere on your `PATH`.\nThere is no plugin installation or pre-loading required. Plugin executables receive the inherited environment from the `kubectl` binary. A plugin determines which command path it wishes to implement based on its name.\n\nHere is the file that can replace your job\n\nA plugin determines the command path that it will implement based on its filename.\n\n`kubectl-job`:\n`#!/bin/bash\nkubectl patch -f job.yaml -p \"$(cat patch-job.yaml)\" --dry-run=client -o yaml | kubectl replace --force -f - && kubectl wait --for=condition=ready pod -l job-name=test1 && kubectl exec -it $(kubectl get pod -l job-name=test1 --no-headers -o custom-columns=\":metadata.name\") -- /bin/sh\n`\nThis command uses an additional file (`patch-job.yaml`, see this link) - within we can put our changes for `job`.\nThen you should change the permissions of this file and move it:\n```\n`sudo chmod +x .kubectl-job\nsudo mv ./kubectl-job /usr/local/bin\n`\n```\nIt's all done. Right now you can use it.\n```\n`$ kubectl job\njob.batch \"test1\" deleted\njob.batch/test1 replaced\npod/test1-bdxtm condition met\npod/test1-nh2pv condition met\n/ #\n`\n```\nAs you can see `Job` has been replaced (deleted and created).\n\nYou can also use single-line command, here is the example:\n\n```\n`kubectl get job test1 -o json | jq \"del(.spec.selector)\" | jq \"del(.spec.template.metadata.labels)\" | kubectl patch -f - --patch '{\"spec\":  {\"template\":  {\"spec\":  {\"containers\": [{\"name\": \"test1\", \"image\": \"busybox\", \"command\": [\"/bin/sh\", \"-c\",  \"sleep 200\"]}]}}}}' --dry-run=client -o yaml | kubectl replace --force -f -\n`\n```\nWith this command you can change your job entering parameters \"by hand\". Here is the output:\n```\n`job.batch \"test1\" deleted\njob.batch/test1 replaced\n`\n```\nAs you can see this solution works as well.",
      "question_score": 9,
      "answer_score": 8,
      "created_at": "2021-11-04T11:14:57",
      "url": "https://stackoverflow.com/questions/69837573/restart-a-kubernetes-job-or-pod-with-a-different-command"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubectl",
      "question_id": 72117876,
      "title": "Unable to connect to AWS EKS cluster",
      "problem": "I am configuring an EKS cluster using terraform in a private subnet and trying to access it using a VPN in a public subnet. When I configured it, it works fine but now when I run kubectl get pods or kubectl get svc, it is throwing an error:\n`error: exec plugin: invalid apiVersion \"client.authentication.k8s.io/v1alpha1\"`\nI don't know why it is happening. Please reply if you the solution.\nThanks",
      "solution": "It's broken with `kubectl` version `1.24`. Downgrade to `1.23.6` will fix the issue for now\n```\n`sudo apt install kubectl=1.23.6-00\n`\n```",
      "question_score": 8,
      "answer_score": 7,
      "created_at": "2022-05-04T20:34:53",
      "url": "https://stackoverflow.com/questions/72117876/unable-to-connect-to-aws-eks-cluster"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubectl",
      "question_id": 67119864,
      "title": "Why can&#39;t copy a file from pod to my local",
      "problem": "I know this question asked before and I checked them but still fails.\nPod name : postgresl-7c8b9-qs67z\nFile in the pod : /home/backup/db\nSo I want to try to copy \"db\" file to my local and I tried to commands below, but all of them giving the same error.\n```\n` kubectl cp default/postgresl-7c8b9-qs67z:/home/backup/db C:\\Users\\myuser\\Desktop\\mydb1.dmp\n\n kubectl cp default/postgresl-7c8b9-qs67z:/home/backup/ C:\\Users\\myuser\\Desktop\\\n\n kubectl cp postgresl-7c8b9-qs67z:/home/backup/db C:\\Users\\myuser\\Desktop\\mydb1.dmp\n`\n```\nAnd the error is :\n```\n`error: one of src or dest must be a local file specification\n`\n```\nHow can I do that? Thanks!",
      "solution": "Currently there seems to be a bug in kubectl where `C:` is treaten as the pod name.\nJust use a relative path for your local file. E.g:\n```\n`kubectl cp default/postgresl-7c8b9-qs67z:/home/backup/db ./Desktop/mydb1.dmp\n`\n```\nadditional hint:\nIf you receive a `tar: Removing leading '/' from member names`, check if the file was downloaded anyhow.",
      "question_score": 7,
      "answer_score": 25,
      "created_at": "2021-04-16T08:02:02",
      "url": "https://stackoverflow.com/questions/67119864/why-cant-copy-a-file-from-pod-to-my-local"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubectl",
      "question_id": 71008589,
      "title": "Kustomize overlays when using a shared ConfigMap",
      "problem": "I have an environment made of pods that address their target environment based on an environment variable called `CONF_ENV` that could be `test`, `stage` or `prod`.\nThe application running inside the Pod has the same source code across environments, the configuration file is picked according to the `CONF_ENV` environment variable.\nI'v encapsulated this `CONF_ENV` in `*.properties` files just because I may have to add more environment variables later, but I make sure that each property file contains the expected `CONF_ENV` e.g.:\n\n`test.properites` has `CONF_ENV=test`,\n`prod.properties` has `CONF_ENV=prod`, and so on...\n\nI struggle to make this work with Kustomize overlays, because I want to define a `ConfigMap` as a shared resource across all the pods within the same overlay e.g. `test` (each pod in their own directory, along other stuff when needed).\nSo the idea is:\n\n`base/` (shared) with the definition of the `Namespace`, the `ConfigMap` (and potentially other shared resources\n`base/pod1/` with the definition of pod1 picking from the shared `ConfigMap` (this defaults to `test`, but in principle it could be different)\n\nThen the overlays:\n\n`overlay/test` that patches the base with `CONF_ENV=test` (e.g. for `overlay/test/pod1/` and so on)\n`overlay/prod/` that patches the base with `CONF_ENV=prod` (e.g. for `overlay/prod/pod1/` and so on)\n\nEach directory with their own `kustomize.yaml`.\nThe above doesn't work because when going into e.g. `overlay/test/pod1/` and I invoke the command `kubectl kustomize .` to check the output YAML, then I get all sorts of errors depending on how I defined the lists for the YAML keys `bases:` or `resources:`.\nI am trying to share the `ConfigMap` across the entire `CONF_ENV` environment in an attempt to minimize the boilerplate YAML by leveraging the patching-pattern with Kustomize.\nThe Kubernetes / Kustomize YAML directory structure works like this:\n`\u251c\u2500\u2500 base\n\u2502   \u251c\u2500\u2500 configuration.yaml # I am trying to share this!\n\u2502   \u251c\u2500\u2500 kustomization.yaml\n\u2502   \u251c\u2500\u2500 my_namespace.yaml # I am trying to share this!\n\u2502   \u251c\u2500\u2500 my-scheduleset-etl-misc\n\u2502   \u2502   \u251c\u2500\u2500 kustomization.yaml\n\u2502   \u2502   \u2514\u2500\u2500 my_scheduleset_etl_misc.yaml\n\u2502   \u251c\u2500\u2500 my-scheduleset-etl-reporting\n\u2502   \u2502   \u251c\u2500\u2500 kustomization.yaml\n\u2502   \u2502   \u2514\u2500\u2500 my_scheduleset_etl_reporting.yaml\n\u2502   \u2514\u2500\u2500 test.properties # I am trying to share this!\n\u2514\u2500\u2500 overlay\n    \u2514\u2500\u2500 test\n        \u251c\u2500\u2500 kustomization.yaml # here I want tell \"go and pick up the shared resources in the base dir\"\n        \u251c\u2500\u2500 my-scheduleset-etl-misc\n        \u2502   \u251c\u2500\u2500 kustomization.yaml\n        \u2502   \u2514\u2500\u2500 test.properties # I've tried to share this one level above, but also to add this inside the \"leaf\" level for a given pod\n        \u2514\u2500\u2500 my-scheduleset-etl-reporting\n            \u2514\u2500\u2500 kustomization.yaml\n`\nThe command `kubectl` with Kustomize:\n\nsometimes complains that the shared namespace does not exist:\n\n```\n`error: merging from generator &{0xc001d99530 {  map[] map[]} {{ my-schedule-set-props merge {[CONF_ENV=test] [] [] } }}}: \nid resid.ResId{Gvk:resid.Gvk{Group:\"\", Version:\"v1\", Kind:\"ConfigMap\", isClusterScoped:false}, Name:\"my-schedule-set-props\", Namespace:\"\"} \ndoes not exist; cannot merge or replace\n`\n```\n\nsometimes doesn't allow to have shared resources inside an overlay:\n\n```\n`error: loading KV pairs: env source files: [../test.properties]: \nsecurity; file '/my/path/to/yaml/overlay/test/test.properties' \nis not in or below '/my/path/to/yaml/overlay/test/my-scheduleset-etl-misc'\n`\n```\n\nsometimes doesn't allow cycles when I am trying to have multiple bases - the shared resources and the original pod definition:\n\n```\n`error: accumulating resources: accumulation err='accumulating resources from '../': \n'/my/path/to/yaml/overlay/test' must resolve to a file': \ncycle detected: candidate root '/my/path/to/yaml/overlay/test' \ncontains visited root '/my/path/to/yaml/overlay/test/my-scheduleset-etl-misc'\n`\n```\nThe overlay `kustomization.yaml` files inside the pod dirs have:\n`bases:\n  - ../ # tried with/without this to share the ConfigMap\n  - ../../../base/my-scheduleset-etl-misc/\n`\nThe `kustomization.yaml` at the root of the overlay has:\n`bases:\n  - ../../base\n`\nThe `kustomization.yaml` at the base dir contains this configuration for the ConfigMap:\n`# https://gist.github.com/hermanbanken/3d0f232ffd86236c9f1f198c9452aad9\nconfigMapGenerator:\n  - name: my-schedule-set-props\n    namespace: my-ss-schedules\n    envs:\n      - test.properties\n\nvars:\n  - name: CONF_ENV\n    objref:\n      kind: ConfigMap\n      name: my-schedule-set-props\n      apiVersion: v1\n    fieldref:\n      fieldpath: data.CONF_ENV\n\nconfigurations:\n  - configuration.yaml\n`\nWith `configuration.yaml` containing:\n`apiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\n\nvarReference:\n- path: spec/confEnv/value\n  kind: Pod\n`\nHow do I do this?\nHow do I make sure that I minimise the amount of YAML by sharing all the `ConfigMap` stuff and the Pods definitions as much as I can?",
      "solution": "If I understand your goal correctly, I think you may be grossly over-complicating things. I think you want a common properties file defined in your base, but you want to override specific properties in your overlays. Here's one way of doing that.\nIn base, I have:\n```\n`$ cd base\n$ tree\n.\n\u251c\u2500\u2500 example.properties\n\u251c\u2500\u2500 kustomization.yaml\n\u2514\u2500\u2500 pod1\n    \u251c\u2500\u2500 kustomization.yaml\n    \u2514\u2500\u2500 pod.yaml\n`\n```\nWhere `example.properties` contains:\n```\n`SOME_OTHER_VAR=somevalue\nCONF_ENV=test\n`\n```\nAnd `kustomization.yaml` contains:\n```\n`resources:\n  - pod1\n\nconfigMapGenerator:\n  - name: example-props\n    envs:\n      - example.properties\n`\n```\nI have two overlays defined, `test` and `prod`:\n```\n`$ cd ../overlays\n$ tree\n.\n\u251c\u2500\u2500 prod\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 example.properties\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 kustomization.yaml\n\u2514\u2500\u2500 test\n    \u2514\u2500\u2500 kustomization.yaml\n`\n```\n`test/kustomization.yaml` looks like this:\n```\n`resources:\n- ../../base\n`\n```\nIt's just importing the `base` without any changes, since the value of `CONF_ENV` from the `base` directory is `test`.\n`prod/kustomization.yaml` looks like this:\n```\n`apiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\nresources:\n- ../../base\n\nconfigMapGenerator:\n  - name: example-props\n    behavior: merge\n    envs:\n      - example.properties\n`\n```\nAnd `prod/example.properties` looks like:\n```\n`CONF_ENV=prod\n`\n```\nIf I run `kustomize build overlays/test`, I get as output:\n```\n`apiVersion: v1\ndata:\n  CONF_ENV: test\n  SOME_OTHER_VAR: somevalue\nkind: ConfigMap\nmetadata:\n  name: example-props-7245222b9b\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: example\nspec:\n  containers:\n  - command:\n    - sleep\n    - 1800\n    envFrom:\n    - configMapRef:\n        name: example-props-7245222b9b\n    image: docker.io/alpine\n    name: alpine\n`\n```\nIf I run `kustomize build overlays/prod`, I get:\n```\n`apiVersion: v1\ndata:\n  CONF_ENV: prod\n  SOME_OTHER_VAR: somevalue\nkind: ConfigMap\nmetadata:\n  name: example-props-h4b5tc869g\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: example\nspec:\n  containers:\n  - command:\n    - sleep\n    - 1800\n    envFrom:\n    - configMapRef:\n        name: example-props-h4b5tc869g\n    image: docker.io/alpine\n    name: alpine\n`\n```\nThat is, everything looks as you would expect given the configuration in `base`, but we have provided a new value for `CONF_ENV`.\nYou can find all these files here.",
      "question_score": 7,
      "answer_score": 17,
      "created_at": "2022-02-06T16:18:37",
      "url": "https://stackoverflow.com/questions/71008589/kustomize-overlays-when-using-a-shared-configmap"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubectl",
      "question_id": 71542604,
      "title": "Minikube : remote error tls: bad certificate",
      "problem": "I'm following a Kubernetes tutorial, and cannot run first command (`minikube start --vm-driver=hyperkit`). I'm using a MacBook Pro Intel on macOs Monterey. I cannot make it work because of TLS error.\n```\n`$ minikube start --vm-driver=hyperkit\n\ud83d\ude04  minikube v1.25.2 on Darwin 12.2.1\n\ud83c\udd95  Kubernetes 1.23.3 is now available. If you would like to upgrade, specify: --kubernetes-version=v1.23.3\n\u2728  Using the hyperkit driver based on existing profile\n\ud83d\udc4d  Starting control plane node minikube in cluster minikube\n\ud83d\udd04  Restarting existing hyperkit VM for \"minikube\" ...\n\ud83d\udc33  Preparing Kubernetes v1.20.2 on Docker 20.10.3 ...\n\u274c  Problems detected in etcd [592b8a58065e]:\n    2022-03-19 22:12:03.193985 I | embed: rejected connection from \"127.0.0.1:38132\" (error \"remote error: tls: bad certificate\", ServerName \"\")\n`\n```\nI tried :\n\nRestarted the computer : https://github.com/kubernetes/minikube/issues/4329\nUsed `--embed-certs` argument\n\n`$ minikube start --vm-driver=hyperkit --embed-certs\n\ud83d\ude04  minikube v1.25.2 on Darwin 12.2.1\n\ud83c\udd95  Kubernetes 1.23.3 is now available. If you would like to upgrade, specify: --kubernetes-version=v1.23.3\n\u2728  Using the hyperkit driver based on existing profile\n\ud83d\udc4d  Starting control plane node minikube in cluster minikube\n\ud83d\udd04  Restarting existing hyperkit VM for \"minikube\" ...\n\ud83d\udc33  Preparing Kubernetes v1.20.2 on Docker 20.10.3 ...\n\u274c  Problems detected in etcd [78d1e36569b8]:\n    2022-03-19 22:20:53.503532 I | embed: rejected connection from \"127.0.0.1:34926\" (error \"remote error: tls: bad certificate\", ServerName \"\")\n`\nI'm new to K8s, what could cause such behaviour ?\n\nI installed minikube and hyperkit with homebrew. When I display the kubectl version I get another connection error :\n`kubectl version\nClient Version: version.Info{Major:\"1\", Minor:\"22\", GitVersion:\"v1.22.5\", GitCommit:\"5c99e2ac2ff9a3c549d9ca665e7bc05a3e18f07e\", GitTreeState:\"clean\", BuildDate:\"2021-12-16T08:38:33Z\", GoVersion:\"go1.16.12\", Compiler:\"gc\", Platform:\"darwin/amd64\"}\nUnable to connect to the server: dial tcp 192.168.64.2:8443: i/o timeout\n`",
      "solution": "The `kubectl version` error helped :\nUnable to connect to the server: dial tcp i/o time out\nIt seems I had already played with k8s :\n`$ kubectl config view\napiVersion: v1\nclusters:\n- cluster:\n    certificate-authority: /Users/xxx/.minikube/ca.crt\n    extensions:\n    - extension:\n        last-update: Sat, 13 Mar 2021 13:40:06 CET\n        provider: minikube.sigs.k8s.io\n        version: v1.18.1\n      name: cluster_info\n    server: https://192.168.64.2:8443\n  name: minikube\ncontexts:\n- context:\n    cluster: minikube\n    extensions:\n    - extension:\n        last-update: Sat, 13 Mar 2021 13:40:06 CET\n        provider: minikube.sigs.k8s.io\n        version: v1.18.1\n      name: context_info\n    namespace: default\n    user: minikube\n  name: minikube\ncurrent-context: minikube\nkind: Config\npreferences: {}\nusers:\n- name: minikube\n  user:\n    client-certificate: /Users/xxx/.minikube/profiles/minikube/client.crt\n    client-key: /Users/xxx/.minikube/profiles/minikube/client.key\n\n`\nFirst I deleted the existing cluster :\n```\n`$ kubectl config delete-cluster minikube\ndeleted cluster minikube from /Users/xxx/.kube/config\n`\n```\nThen run\n`$ minikube delete\n\ud83d\udd25  Deleting \"minikube\" in hyperkit ...\n\ud83d\udc80  Removed all traces of the \"minikube\" cluster.\n`\nFinally :\n`$ minikube start --vm-driver=hyperkit\n\ud83d\ude04  minikube v1.25.2 on Darwin 12.2.1\n\u2728  Using the hyperkit driver based on user configuration\n\ud83d\udc4d  Starting control plane node minikube in cluster minikube\n\ud83d\udcbe  Downloading Kubernetes v1.23.3 preload ...\n    > preloaded-images-k8s-v17-v1...: 505.68 MiB / 505.68 MiB  100.00% 923.34 K\n\ud83d\udd25  Creating hyperkit VM (CPUs=2, Memory=4000MB, Disk=20000MB) ...\n\ud83d\udc33  Preparing Kubernetes v1.23.3 on Docker 20.10.12 ...\n    \u25aa kubelet.housekeeping-interval=5m\n    \u25aa Generating certificates and keys ...\n    \u25aa Booting up control plane ...\n    \u25aa Configuring RBAC rules ...\n\ud83d\udd0e  Verifying Kubernetes components...\n    \u25aa Using image gcr.io/k8s-minikube/storage-provisioner:v5\n\ud83c\udf1f  Enabled addons: default-storageclass, storage-provisioner\n\ud83c\udfc4  Done! kubectl is now configured to use \"minikube\" cluster and \"default\" namespace by default\n`",
      "question_score": 7,
      "answer_score": 12,
      "created_at": "2022-03-19T23:30:24",
      "url": "https://stackoverflow.com/questions/71542604/minikube-remote-error-tls-bad-certificate"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubectl",
      "question_id": 66114851,
      "title": "Kubectl wait for service to get external ip",
      "problem": "I'm trying to use kubectl to wait for a service to get an external ip assigned. I've been trying to use the below just to get started\n```\n`kubectl wait --for='jsonpath={.spec.externalTrafficPolicy==Cluster}' --timeout=30s --namespace cloud-endpoints svc/esp-echo\n`\n```\nBut I keep getting the below error message\n```\n`error: unrecognized condition: \"jsonpath={.spec.externalTrafficPolicy==Cluster}\"\n`\n```",
      "solution": "It is not possible to pass arbitrary `jsonpath` and there is already a request for the feature.\nHowever, you can use a bash script with some sleep and monitor the service using other `kubectl` commands:\n```\n`kubectl get --namespace cloud-endpoints svc/esp-echo --template=\"{{range .status.loadBalancer.ingress}}{{.ip}}{{end}}\"\n`\n```\nThe above command will return the external IP for the LoadBalancer service for example.\nYou can write a simple bash file using the above as:\n```\n`#!/bin/bash\nip=\"\"\nwhile [ -z $ip ]; do\n  echo \"Waiting for external IP\"\n  ip=$(kubectl get svc $1 --namespace cloud-endpoints --template=\"{{range .status.loadBalancer.ingress}}{{.ip}}{{end}}\")\n  [ -z \"$ip\" ] && sleep 10\ndone\necho 'Found external IP: '$ip\n`\n```",
      "question_score": 7,
      "answer_score": 8,
      "created_at": "2021-02-09T08:45:30",
      "url": "https://stackoverflow.com/questions/66114851/kubectl-wait-for-service-to-get-external-ip"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubectl",
      "question_id": 67920799,
      "title": "Create PersistentVolumeClaim imperative way?",
      "problem": "Why can't we create PV or PVC in imperative way?\nTrying using create command, but it doesn't show any of them.\n`kubectl create --help`\n```\n`Available Commands:\n  clusterrole         Create a ClusterRole.\n  clusterrolebinding  Create a ClusterRoleBinding for a particular ClusterRole\n  configmap           Create a configmap from a local file, directory or literal value\n  cronjob             Create a cronjob with the specified name.\n  deployment          Create a deployment with the specified name.\n  ingress             Create an ingress with the specified name.\n  job                 Create a job with the specified name.\n  namespace           Create a namespace with the specified name\n  poddisruptionbudget Create a pod disruption budget with the specified name.\n  priorityclass       Create a priorityclass with the specified name.\n  quota               Create a quota with the specified name.\n  role                Create a role with single rule.\n  rolebinding         Create a RoleBinding for a particular Role or ClusterRole\n  secret              Create a secret using specified subcommand\n  service             Create a service using specified subcommand.\n  serviceaccount      Create a service account with the specified name\n   \n`\n```",
      "solution": "As described in the documentation `kubectl` uses imperative commands built into the kubectl command-line tool in order to help you creating objects quickly.\nAfter some checks it seems like this is not available because it has not been implemented yet. You can see the full list of the create options at kubectl/pkg/cmd/create.\nFor example, #78153 was responsible for `kubectl create ingress` functionality.\nYou would probably get more information and perhaps reasons why this is not implemented by asking the developers and opening a new issue.",
      "question_score": 7,
      "answer_score": 8,
      "created_at": "2021-06-10T14:06:47",
      "url": "https://stackoverflow.com/questions/67920799/create-persistentvolumeclaim-imperative-way"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubectl",
      "question_id": 68139955,
      "title": "Unknown image flag when creating deployment using Minikube kubectl",
      "problem": "I am getting `unknown image flag` when creating a deployment using `minikube` on `windows 10` `cmd`. Why?\n```\n`C:\\WINDOWS\\system32>minikube kubectl create deployment nginxdepl --image=nginx\nError: unknown flag: --image\nSee 'minikube kubectl --help' for usage.\n\nC:\\WINDOWS\\system32>\n`\n```",
      "solution": "When using kubectl bundled with minikube the command is little different.\nFrom the documentation, your command should be:\n```\n`minikube kubectl -- create deployment nginxdepl --image=nginx\n`\n```\nThe difference is the `--` right after `kubectl`",
      "question_score": 6,
      "answer_score": 18,
      "created_at": "2021-06-26T08:51:48",
      "url": "https://stackoverflow.com/questions/68139955/unknown-image-flag-when-creating-deployment-using-minikube-kubectl"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubectl",
      "question_id": 66833650,
      "title": "Using kubectl to wait until a pvc is bound",
      "problem": "I wanna use `kubectl wait` command to wait until a pvc is bound.\nI tried `kubectl wait --for=condition=bound pvc/my-pvc-claim --timeout=2s` with a pvc which is already bound, but it doesn't seem to work. This is the output `error: timed out waiting for the condition on persistentvolumeclaims/my-pvc-claim`.\nI read `kubectl wait` documentation, but still can't understand which condition I should use. How can I accomplish that? Is there a more complete documentation explaining how to do that?",
      "solution": "You can use the following command:\n```\n`while [[ $(kubectl get pvc myclaim -o 'jsonpath={..status.phase}') != \"Bound\" ]]; do echo \"waiting for PVC status\" && sleep 1; done\n`\n```",
      "question_score": 6,
      "answer_score": 6,
      "created_at": "2021-03-27T17:32:23",
      "url": "https://stackoverflow.com/questions/66833650/using-kubectl-to-wait-until-a-pvc-is-bound"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubectl",
      "question_id": 67346166,
      "title": "Why kubectl exec --username=root does not work?",
      "problem": "I deployed istio/bookinfo on kubernetes, and I want to install stress on the microservice container to inject fault. However, When I use\n```\n`kubectl exec -it reviews-v1-f55d74d54-kpxr2 -c reviews --username=root -- /bin/bash\n`\n```\nto log in the container, it show that the user is still default. and the command 'apt-get' got\n```\n`default@reviews-v2-6f4995984d-4752v:/$ apt-get update\nReading package lists... Done\nE: List directory /var/lib/apt/lists/partial is missing. - Acquire (13: Permission denied)\n`\n```\nI tried to use 'su root' but I don't know the answer.\nI searched some answer say that I can use 'docker exec', it works but it is not convenient, so I want to know how to log in the container by use the command kubectl exec.",
      "solution": "This is not supported.\nSource code suggests it's a TODO feature: kubernetes/kubectl/pkg/cmd/exec/exec.go\nThe `--username` flag explained by kubectl:\n```\n`\u279c  ~ kubectl options  | grep user    \n  --user='': The name of the kubeconfig user to use\n  --username='': Username for basic authentication to the API server\n`\n```\nAs you probably see, none of the user flags can change user/UID for exec.\nAll flags supported by exec command:\n```\n`\u279c  ~ kubectl exec --help\n[...]\n\nOptions:\n  -c, --container='': Container name. If omitted, the first container in the pod will be chosen\n  -f, --filename=[]: to use to exec into the resource\n      --pod-running-timeout=1m0s: The length of time (like 5s, 2m, or 3h, higher than zero) to wait until at least one\npod is running\n  -i, --stdin=false: Pass stdin to the container\n  -t, --tty=false: Stdin is a TTY\n`\n```\nAdditionally, apt-get update is best to be run at build time, not at a run time.\nIt is a good practise to keep your containers immutable. For testing purpouses you should stick with docker exec because ther is no other known alternative.\nAlso, If you have a specific problem to solve, explain the problem, not the solution. xyproblem",
      "question_score": 6,
      "answer_score": 11,
      "created_at": "2021-05-01T14:15:54",
      "url": "https://stackoverflow.com/questions/67346166/why-kubectl-exec-username-root-does-not-work"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubectl",
      "question_id": 74687452,
      "title": "EKS: Error syncing load balancer: failed to ensure load balancer: Multiple tagged security groups found for instance",
      "problem": "```\n`Warning  SyncLoadBalancerFailed  54s (x4 over 91s)  service-controller  Error syncing load balancer: failed to ensure load balancer: Multiple tagged security groups found for instance i-05f3a11329a20bb93; ensure only the k8s security group is tagged; the tagged groups were sg-08ca90265d3402e6c(education-eks-ooHfNJwm-node-20221205083117267100000007) sg-04ad04b5d3bb35e66(eks-cluster-sg-education-eks-ooHfNJwm-1857011925)\n  Normal   EnsuringLoadBalancer    14s (x5 over 94s)  service-controller  Ensuring load balancer\n  Warning  SyncLoadBalancerFailed  13s                service-controller  Error syncing load balancer: failed to ensure load balancer: Multiple tagged security groups found for instance i-046c2cc46714af250; ensure only the k8s security group is tagged; the tagged groups were sg-08ca90265d3402e6c(education-eks-ooHfNJwm-node-20221205083117267100000007) sg-04ad04b5d3bb35e66(eks-cluster-sg-education-eks-ooHfNJwm-1857011925)\n`\n```\nI created the cluster with this code: eks using terraform",
      "solution": "Add in eks module will fix issue.\n```\n`node_security_group_tags = {\n    \"kubernetes.io/cluster/${var.cluster_name}\" = null\n  }\n`\n```\nRef:\nhttps://github.com/terraform-aws-modules/terraform-aws-eks/issues/1986\nhttps://github.com/terraform-aws-modules/terraform-aws-eks/issues/1810",
      "question_score": 6,
      "answer_score": 9,
      "created_at": "2022-12-05T12:29:32",
      "url": "https://stackoverflow.com/questions/74687452/eks-error-syncing-load-balancer-failed-to-ensure-load-balancer-multiple-tagge"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubectl",
      "question_id": 75308525,
      "title": "Why does minikube doesn&#39;t have access to k8s registry?",
      "problem": "Running the `minikube start` command, I am getting this message:\nThis container is having trouble accessing https://registry.k8s.io\nand after this the Booting up control plane process takes a long time then gives the following error:\nError starting cluster: wait: /bin/bash -c \"sudo env PATH=\"/var/lib/minikube/binaries/v1.26.1:$PATH\" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables\": Process exited with status 1\nI have the right `minikube`, `kubectl` , `docker` ... versions.\n`$ echo $(minikube docker-env)` this command outputs the following error:\nExiting due to GUEST_STATUS: state: unknown state \"minikube\": docker container inspect minikube --format=: exit status 1\nstderr:\nGot permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get \"http://%2Fvar%2Frun%2Fdocker.sock/v1.24/containers/minikube/json\": dial unix /var/run/docker.sock: connect: permission denied\nBut what i don't understand, if I run the `docker run hello-world` , it works (I have the super user permission)",
      "solution": "Try running the below commands:\nRemove unused data:\n```\n`docker system prune\n`\n```\nClear minikube's local state:\n```\n`minikube delete\n`\n```\nStart the cluster:\n```\n`minikube start --driver=\n`\n```\n(In your case driver name is docker as per minikube profile list info shared by you)\nCheck the cluster status:\n```\n`minikube status\n`\n```\nAlso refer to this Github link.",
      "question_score": 6,
      "answer_score": 8,
      "created_at": "2023-02-01T11:08:41",
      "url": "https://stackoverflow.com/questions/75308525/why-does-minikube-doesnt-have-access-to-k8s-registry"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubectl",
      "question_id": 69076155,
      "title": "Kubernetes - check if resources defined in YAML file exist",
      "problem": "I am creating a bash script to automate certain actions in my cluster. One of the commands is: `kubectl delete -f example.yaml`.\nThe problem is that when the resources defined in the YAML do not exist, the following error is printed:\n```\n`Error from server (NotFound): error when deleting \"example.yaml\": deployments.apps \"my_app\" not found\n`\n```\nI am looking to add an additional step that first checks whether a set of resources defined in a YAML file exist in the cluster. Is there a command that would allow me to do so?\nFrom the documentation, I found:\n\nCompares the current state of the cluster against the state that the cluster would be in if the manifest was applied.\nkubectl diff -f ./my-manifest.yaml\n\nbut I find it difficult to parse the output that it returns. Is there a better alternative?",
      "solution": "To find out if the same object is already present in the cluster as exactly described in the manifest file. you can use the return code of the `kubectl diff` command.\n```\n`Exit status:\n 0 No differences were found.\n 1 Differences were found. \n >1 Kubectl or diff failed with an error.\n`\n```\nExample:\n```\n`kubectl diff -f crazy.yml &>/dev/null\nrc=$?\nif [ $rc -eq 0 ];then\n  echo \"Exact Object is already installed on the cluster\"\nelif [ $rc -eq 1 ];then\n  echo \"Exact object is not installed, either its not installed or different from the manifest file\"\nelse\n  echo \"Unable to determine the difference\"\nfi\n`\n```\nAlternatively, if you want to really parse the output,  you may use the following env variable to print the diff output in desired format:\n\nKUBECTL_EXTERNAL_DIFF environment variable can be used to select your\nown diff command. Users can use external commands with params too,",
      "question_score": 6,
      "answer_score": 6,
      "created_at": "2021-09-06T16:26:53",
      "url": "https://stackoverflow.com/questions/69076155/kubernetes-check-if-resources-defined-in-yaml-file-exist"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubectl",
      "question_id": 68979336,
      "title": "Kubectl port-forwarding not working for IPv6 binding with socat",
      "problem": "I'm trying to understand why this particular `socat` command isn't working in my case where I run it in a IPv6 only Kubernetes cluster.\nCluster is build on top of AWS with Calico CNI & containerd. Provisioned using `kubeadm` and Kubernetes 1.21.\nI have run the following `socat` command which binds to loopback interface `::1`,\n```\n`kubectl --context=$CLUSTER1 run --image=alpine/socat socat -- tcp6-listen:15000,bind=\\[::1\\],fork,reuseaddr /dev/null\n`\n```\nAnd then I try to `port-forward` and `curl` to `15000` port,\n```\n`kubectl --context=$CLUSTER1 port-forward pod/socat 35000:15000 --address=::1\ncurl -ivg http://localhost:35000\n`\n```\nI get the error,\n```\n`Forwarding from [::1]:35000 -> 15000\nHandling connection for 35000\nE0830 17:09:59.604799   79802 portforward.go:400] an error occurred forwarding 35000 -> 15000: error forwarding port 15000 to pod a8ba619774234e73f4c1b4fe4ff47193af835cffc56cb6ad1a8f91e745ac74e9, uid : failed to execute portforward in network namespace \"/var/run/netns/cni-8bade2c1-28c9-6776-5326-f10d55fd0ff9\": failed to dial 15000: dial tcp4 127.0.0.1:15000: connect: connection refused\n`\n```\nIts listening to `15000` as,\n```\n`Active Internet connections (servers and established)\nProto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name\ntcp        0      0 ::1:15000               :::*                    LISTEN      1/socat\n`\n```\nHowever if I run the following it works fine,\n```\n`kubectl --context=$CLUSTER1 run --image=alpine/socat socat -- tcp6-listen:15000,bind=\\[::\\],fork,reuseaddr /dev/null\n`\n```\nNot sure I understand why `port-forward` would fail for the loopback interface binding `::1` but not for catch all `::`. Can someone please shed some light on this ?",
      "solution": "For those of you running into a similar issue with your IPv6 only Kubernetes clusters heres what I have investigated found so far.\nBackground: It seems that this is a generic issue relating to IPv6 and CRI.\nI was running `containerd` in my setup and `containerd` versions `1.5.0`-`1.5.2` added two PRs (don't use socat for port forwarding and use happy-eyeballs for port-forwarding) which fixed a number of issues in IPv6 port-forwarding.\nPotential fix: Further to pulling in `containerd` version `1.5.2` (as part of Ubuntu 20.04 LTS) I was also getting the error `IPv4: dial tcp4 127.0.0.1:15021: connect: connection refused IPv6 dial tcp6: address localhost: no suitable address found` when port-forwarding. This is caused by a DNS issue when resolving `localhost`. Hence I added `localhost` to resolve as `::1` in the host machine with the following command.\n```\n`sed -i 's/::1 ip6-localhost ip6-loopback/::1 localhost ip6-localhost ip6-loopback/' /etc/hosts\n`\n```\nI think the important point here is that check your container runtimes to make sure IPv6 (tcp6 binding) is supported.",
      "question_score": 6,
      "answer_score": 2,
      "created_at": "2021-08-30T07:37:45",
      "url": "https://stackoverflow.com/questions/68979336/kubectl-port-forwarding-not-working-for-ipv6-binding-with-socat"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubectl",
      "question_id": 71105295,
      "title": "Rancher helm chart, cannot find secret bootstrap-secret",
      "problem": "So I am trying to deploy rancher on my K3S cluster.\nI installed it using the documentation and helm: Rancher documentation\nWhile I am getting access using my loadbalancer. I cannot find the secret to insert into the setup.\nThey discribe the following command for getting the token:\n```\n`kubectl get secret --namespace cattle-system bootstrap-secret -o go-template='{{.data.bootstrapPassword|base64decode}}{{ \"\\n\" }}'\n`\n```\nWhen I run this I get the following error\n```\n`Error from server (NotFound): secrets \"bootstrap-secret\" not found\n`\n```\nAnd also I cannot find the bootstrap-secret inside the namespace cattle-system.\nSo can somebody help me out where I need to look?",
      "solution": "I had the same problem. So I figured it out with the following commands:\n\nI installed the helm chart with \"--set bootstrapPassword=Changeme123!\", for example:\nhelm upgrade --install  \n--namespace cattle-system \n--set hostname=rancher.example.com \n--set replicas=3 \n--set bootstrapPassword=Changeme123! \nrancher rancher-stable/rancher\n\nI forced a hard reset, because even if I had setted the bootstrap password in the installation helm chart command, I was not able to login. So, I used the following command to hard reset:\nkubectl -n cattle-system exec $(kubectl -n cattle-system get pods -l app=rancher | grep '1/1' | head -1 | awk '{ print $1 }') -- reset-password\n\nSo, I hope that can help you.",
      "question_score": 5,
      "answer_score": 13,
      "created_at": "2022-02-13T23:11:35",
      "url": "https://stackoverflow.com/questions/71105295/rancher-helm-chart-cannot-find-secret-bootstrap-secret"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubectl",
      "question_id": 69811701,
      "title": "Shell script should wait until kubernetes pod is running",
      "problem": "In a simple bash script I want to run multiple `kubectl` and `helm` commands, like:\n```\n`helm install \\\n  cert-manager jetstack/cert-manager \\\n  --namespace cert-manager \\\n  --create-namespace \\\n  --version v1.5.4 \\\n  --set installCRDs=true\nkubectl apply -f deploy/cert-manager/cluster-issuers.yaml\n`\n```\nMy problem here is, that after the `helm install` command I have to wait until the cert-manager pod is running, then the `kubectl apply` command can be used. Right now the script is calling it too early, so it will fail.",
      "solution": "As stated in the comments `kubectl wait` is the way to go.\nExample from the `kubectl wait --help`\n`Examples:\n  # Wait for the pod \"busybox1\" to contain the status condition of type \"Ready\"\n  kubectl wait --for=condition=Ready pod/busybox1\n`\nThis way your script will pause until specified pod is Running, and `kubectl` will output\n` condition met\n`\nto STDOUT.\n\n`kubectl wait` is still in experimental phase. If you want to avoid experimental features, you can achieve similar result with bash `while` loop.\nBy pod name:\n`while [[ $(kubectl get pods  -o 'jsonpath={..status.conditions[?(@.type==\"Ready\")].status}') != \"True\" ]]; do\n   sleep 1\ndone\n`\nor by label:\n`while [[ $(kubectl get pods -l = -o 'jsonpath={..status.conditions[?(@.type==\"Ready\")].status}') != \"True\" ]]; do\n   sleep 1\ndone\n`",
      "question_score": 5,
      "answer_score": 10,
      "created_at": "2021-11-02T14:57:54",
      "url": "https://stackoverflow.com/questions/69811701/shell-script-should-wait-until-kubernetes-pod-is-running"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubectl",
      "question_id": 68740515,
      "title": "kubectl status.phase=Running return wrong results",
      "problem": "When I run:\n```\n`kubectl get pods --field-selector=status.phase=Running\n`\n```\nI see:\n```\n`NAME          READY   STATUS    RESTARTS   AGE\nk8s-fbd7b     2/2     Running   0          5m5s\ntestm-45gfg   1/2     Error     0          22h\n`\n```\nI don't understand why this command gives me pod that are in Error status?\nAccording to K8S api, there is no such thing `STATUS=Error`.\nHow can I get only the pods that are in this Error status?\nWhen I run:\n```\n`kubectl get pods --field-selector=status.phase=Failed\n`\n```\nIt tells me that there are no pods in that status.",
      "solution": "You can simply grep the Error pods using the\n```\n`kubectl get pods --all-namespces | grep Error\n`\n```\nRemove all error pods from the cluster\n```\n`kubectl delete pod `kubectl get pods --namespace  | awk '$3 == \"Error\" {print $1}'` --namespace \n`\n```\nMostly Pod failures return explicit error states that can be observed in the status field\nError :\nYour pod is crashed, it was able to schedule on node successfully but crashed after that. To debug it more you can use different methods or commands\n```\n`kubectl describe pod  -n \n`\n```\nhttps://kubernetes.io/docs/tasks/debug-application-cluster/debug-pod-replication-controller/#my-pod-is-crashing-or-otherwise-unhealthy",
      "question_score": 5,
      "answer_score": 2,
      "created_at": "2021-08-11T12:45:45",
      "url": "https://stackoverflow.com/questions/68740515/kubectl-status-phase-running-return-wrong-results"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubectl",
      "question_id": 66375380,
      "title": "Kubectl proxy behind NGINX: invalid upgrade response",
      "problem": "I'm trying to add a local Kubernates cluster to my GitLab group for CI/CD deployments. I've started with running the following command:\n`kubectl proxy --address 0.0.0.0 --accept-hosts '.*'`\nI've tested it executing `curl http://localhost:8001/api` and running `curl http://192.168.1.2:8001/api` from another machine in the same network. Proxy was available in my local network.\nThe next step was to make the proxy available on the internet behind `kubernates.example.com`. For that I've configured NGINX as the following:\n```\n`server {\n    server_name kubernates.example.com;\n    listen 443 ssl;\n    listen 80;\n\n    include ssl_standart_conf;\n\n    location / {\n        proxy_pass http://192.168.1.2:8001/;\n        proxy_set_header Host $host;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_redirect off;\n        proxy_http_version 1.1;\n        proxy_set_header Upgrade $http_upgrade;\n        proxy_set_header Connection \"upgrade\";\n    }\n}\n`\n```\nExecuting `curl https://kubernates.example.com/api` returned the following error:\n```\n`invalid upgrade response: status code 200\n`\n```\nKubernates proxy logs\n```\n`E0225 22:33:50.944018 1642369 upgradeaware.go:312] Proxy upgrade error: invalid upgrade response: status code 200\nE0225 22:33:50.944060 1642369 proxy_server.go:144] Error while proxying request: invalid upgrade response: status code 200\n`\n```",
      "solution": "Okay, I've managed to resolve the issue. The following nginx configuration made a trick\n```\n`server {\n    server_name kubernates.example.com;\n    listen 443 ssl;\n    listen 80;\n\n    include ssl_standart_conf;\n\n    location / {\n        proxy_pass http://192.168.1.2:8001/;\n        proxy_set_header Host $host;\n    }\n}\n`\n```",
      "question_score": 5,
      "answer_score": 3,
      "created_at": "2021-02-25T20:39:51",
      "url": "https://stackoverflow.com/questions/66375380/kubectl-proxy-behind-nginx-invalid-upgrade-response"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubectl",
      "question_id": 67364251,
      "title": "kubectl config get-contexts and custom-columns output",
      "problem": "I would like `kubectl config get-contexts` to show all, or any arbitrary subset, of the columns shown in default output.\nCurrently, `kubectl config get-contexts` shows `CURRENT NAME CLUSTER AUTHINFO` and `NAMESPACE`. On my terminal, that's a total of 221 columns, with `NAME`, `CLUSTER`, and `AUTHINFO` being identical for all contexts.\n`kubectl config get-contexts` documentation shows only one output option: `-o=name`. Attempts to override this with `-o=custom-columns=\"CURRENT:.metadata.current,NAME:.metadata.name\"` (for example) result in an error.\nAm I doing something wrong or is the `custom-columns` option that is common to `kubectl get` a missing feature?\nUpdate: maintainers decided that there was no clean way of implementing output options; see https://github.com/kubernetes/kubectl/issues/1052",
      "solution": "As indicated by the error message:\n```\n`error: output must be one of '' or 'name'\n`\n```\nand described in the docs:\n```\n`output  o       Output format. One of: name\n`\n```\nonly the value of `name` can be used with the custom-columns option for `kubectl config get-contexts`.\nThe other option that you have left is to list the current context with:\n```\n`kubectl config current-context \n`\n```",
      "question_score": 5,
      "answer_score": 5,
      "created_at": "2021-05-03T08:13:04",
      "url": "https://stackoverflow.com/questions/67364251/kubectl-config-get-contexts-and-custom-columns-output"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubectl",
      "question_id": 77115095,
      "title": "Using --sort-by with kubectl get pods --all-namespaces to sort by both namespace AND name doesn&#39;t work",
      "problem": "I'm guessing at this point the answer is a big fat NO. I'm reaching the hive mind here for a clear answer of why, or in case I'm wrong (which I really hope I am) for a concrete example.\nI've been able to use `jq` successfully, but I am hoping I'm just missing something in the jsonpath syntax of `--sort-by`.\nWhat I know\nAccoridng to the documentation(also quoted below) `kubectl`'s `--sort-by` argument takes a jsonpath syntax.\n\nSorting list objects\nTo output objects to a sorted list in your terminal window, you can add the `--sort-by` flag to a supported kubectl command. Sort your objects by specifying any numeric or string field with the `--sort-by flag`. To specify a field, use a jsonpath expression.\nSyntax\n`kubectl [command] [TYPE] [NAME] --sort-by=`\n\nI've looked at their jsonpath examples, and while it seems I am able to use the syntax to output what I think would be a sortable object, in effect I get a blank output.\nMaybe if the output could be concated/transformed into a string, instead of an array/list, but that doesn't seem to work for me.\nCan anyone confirm success of using `--sort-by` with multiple fields?\nWhat I've tried\nExample of successful jsonpath output:\n```\n`$ kubectl get --namespace ix-authentik pods -o=jsonpath=\"{range .items[*]}[{.metadata.namespace},{.metadata.name}]{end}\" && echo\n[,][,][,....\n`\n```\nExample of a failed sort-by attempts based on the syntax above:\n```\n`$ kubectl get --all-namespaces pods --sort-by=\"{range .}[{.metadata.namespace},{.metadata.name}]{end}\"\nNo resources found\n\n$ kubectl get --all-namespaces pods --sort-by=\"[{.metadata.namespace},{.metadata.name}]\"\n# Randomly sorted mess\n\n$ kubectl get --all-namespaces pods --sort-by=\"{.metadata.namespace},{.metadata.name}\"\n# Sorted only by namespace\n\n$ kubectl get --all-namespaces pods --sort-by=\".metadata.namespace,.metadata.name\"\nNo resources found\n\n$ kubectl get --all-namespaces pods --sort-by=\"[.metadata.namespace,.metadata.name]\"\nNo resources found\n`\n```\nResources I've reviewed\nThe kubernates GitHub repo has several issues about sort, there is even a closed PR from 2019 where they were about to deprecate the --sort-by featrue because it was terrible.\nBut nothing I could easily find on multiple values. (I might be searching all wrong though)\nI've even tried Bing Chat's AI to resolve this, it suggest syntax similar to above, but it only sorts on the first column. AI can't figure out questions that havn't been answered (properly) yet (surprise, surprise), so I'm reaching out to the human driven hive mind.",
      "solution": "One workaround is to specify custom columns in `kubectl` and use `sort` for the multi-field sort capability.\nExample:\n`kubectl get nodes --no-headers -o custom-columns='\\\n    NAME:.metadata.name, \\\n    ZONE:.metadata.labels.topology\\.kubernetes\\.io/zone, \\\n    CREATED:.metadata.creationTimestamp' \\\n    | sort -k 2b,2 -k 3b,3\n`\nsorts by availability zone first, then sorts by creation timestamp within the same availability zone.",
      "question_score": 5,
      "answer_score": 4,
      "created_at": "2023-09-15T22:07:54",
      "url": "https://stackoverflow.com/questions/77115095/using-sort-by-with-kubectl-get-pods-all-namespaces-to-sort-by-both-namespace"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubectl",
      "question_id": 68349631,
      "title": "How to set load restrictor when using kustomize from kubectl?",
      "problem": "I am trying to use kustomize from within kubectl. Specifically, I want to know the equivalent kubectl command for:\n```\n`kustomize build --load_restrictor LoadRestrictionsNone config/overlays/dev_mutation | kubectl apply -f -\n`\n```\n(kustomize properly runs this command and does what I expect)\nI've tried this command:\n```\n`$ kubectl apply -k config/overlays/dev_mutation --load_restrictor=\"LoadRestrictionsNone\"\n`\n```\nwhich complains that `load_restrictor` is deprecated and I should use `load-restrictor` instead.\n```\n`W0712 07:58:16.811301 2407909 flags.go:39] load_restrictor is DEPRECATED and will be removed in a future version. Use load-restrictor instead.\nError: unknown flag: --load_restrictor\n`\n```\nSo, I tried replacing with the non-deprecated flag:\n```\n`kubectl apply -k config/overlays/dev_mutation --load-restrictor=\"LoadRestrictionsNone\"\n`\n```\nIf I do this, kubectl complains that `--load-restrictor` is unknown:\n```\n`Error: unknown flag: --load-restrictor\n`\n```\nHow do I properly pass the `load_restrictor`/`load-restrictor` flag to `kubectl apply -k`?\nOutput of `kubectl version`:\n```\n`gatekeeper$ kubectl version\nClient Version: version.Info{Major:\"1\", Minor:\"21\", GitVersion:\"v1.21.2\", GitCommit:\"092fbfbf53427de67cac1e9fa54aaa09a28371d7\", GitTreeState:\"clean\", BuildDate:\"2021-06-16T12:59:11Z\", GoVersion:\"go1.16.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n`\n```",
      "solution": "I suggest installing the `kustomize` binary directly, instead of relying on the bundled version in `kubectl` which would be outdated. More info here: Install Kustomize\nI do not think you can pass the `--load-restrictor` option to `kubectl apply -k` command. Instead, I can confirm that this works\n```\n`kubectl kustomize --load-restrictor LoadRestrictionsNone \n`\n```\nYou can use kustomize binary to achieve the same using\n```\n`kustomize build --load-restrictor LoadRestrictionsNone \n`\n```\nApplying generated yaml\nIf you want to apply the generated output using kubectl, you can pipe this output like so\n```\n`kubectl kustomize --load-restrictor LoadRestrictionsNone  | kubectl apply -f -\n`\n```\nor\n```\n`kustomize build --load-restrictor LoadRestrictionsNone  | kubectl apply -f -\n`\n```",
      "question_score": 5,
      "answer_score": 3,
      "created_at": "2021-07-12T17:01:03",
      "url": "https://stackoverflow.com/questions/68349631/how-to-set-load-restrictor-when-using-kustomize-from-kubectl"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubectl",
      "question_id": 66612592,
      "title": "Redeploy statefulset with CrashLoopBackOff status in kubernetes",
      "problem": "That's what I do:\n\nDeploy a stateful set. The pod will always exit with an error to provoke a failing pod in status `CrashLoopBackOff`: `kubectl apply -f error.yaml`\nChange error.yaml (`echo a` => `echo b`) and redeploy stateful set: `kubectl apply -f error.yaml`\nPod keeps the error status and will not immediately redeploy but wait until the pod is restarted after some time.\n\nRequesting pod status:\n```\n`$ kubectl get pod errordemo-0\nNAME          READY   STATUS             RESTARTS   AGE\nerrordemo-0   0/1     CrashLoopBackOff   15         59m\n`\n```\nerror.yaml\n```\n`apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: errordemo\n  labels:\n    app.kubernetes.io/name: errordemo\nspec:\n  serviceName: errordemo\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: errordemo\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: errordemo\n    spec:\n      containers:\n        - name: demox\n          image: busybox:1.28.2\n          command: ['sh', '-c', 'echo a; sleep 5; exit 1']\n      terminationGracePeriodSeconds: 1\n`\n```\nQuestions\nHow can I achieve an immediate redeploy even if the pod has an error status?\nI found out these solutions but I would like to have a single command to achieve that (In real life I'm using helm and I just want to call `helm upgrade` for my deployments):\n\nKill the pod before the redeploy\nScale down before the redeploy\nDelete the statefulset before the redeploy\n\nWhy doesn't kubernetes redeploy the pod at once?\n\nIn my demo example I have to wait until kubernetes tries to restart the pod after waiting some time.\nA pod with no error (e.g. `echo a; sleep 10000;`) will be restarted immediately. That's why I set `terminationGracePeriodSeconds: 1`\nBut in my real deployments (where I use helm) I also encountered the case that the pods are never redeployed. Unfortunately I cannot reproduce this behaviour in a simple example.",
      "solution": "You could set `spec.podManagementPolicy: \"Parallel\"`\n\nParallel pod management tells the StatefulSet controller to launch or terminate all Pods in parallel, and not to wait for Pods to become Running and Ready or completely terminated prior to launching or terminating another Pod.\n\nRemember that the default podManagementPolicy is `OrderedReady`\n\nOrderedReady pod management is the default for StatefulSets. It tells the StatefulSet controller to respect the ordering guarantees demonstrated above\n\nAnd if your application requires ordered update then there is nothing you can do.",
      "question_score": 5,
      "answer_score": 2,
      "created_at": "2021-03-13T11:50:47",
      "url": "https://stackoverflow.com/questions/66612592/redeploy-statefulset-with-crashloopbackoff-status-in-kubernetes"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubectl",
      "question_id": 66679561,
      "title": "How to see Pod logs: a container name must be specified for pod... choose one of: [wait main]",
      "problem": "I am running an Argo workflow and getting the following error in the pod's log:\n```\n`error: a container name must be specified for pod , choose one of: [wait main]\n`\n```\nThis error only happens some of the time and only with some of my templates, but when it does, it is a template that is run later in the workflow (i.e. not the first template run).  I have not yet been able to identify the parameters that will run successfully, so I will be happy with tips for debugging. I have pasted the output of describe below.\nBased on searches, I think the solution is simply that I need to attach \"-c main\" somewhere, but I do not know where and cannot find information in the Argo docs.\nDescribe:\n```\n`Name:         message-passing-1-q8jgn-607612432\nNamespace:    argo\nPriority:     0\nNode:         REDACTED\nStart Time:   Wed, 17 Mar 2021 17:16:37 +0000\nLabels:       workflows.argoproj.io/completed=false\n              workflows.argoproj.io/workflow=message-passing-1-q8jgn\nAnnotations:  cni.projectcalico.org/podIP: 192.168.40.140/32\n              cni.projectcalico.org/podIPs: 192.168.40.140/32\n              workflows.argoproj.io/node-name: message-passing-1-q8jgn.e\n              workflows.argoproj.io/outputs: {\"exitCode\":\"6\"}\n              workflows.argoproj.io/template:\n                {\"name\":\"egress\",\"arguments\":{},\"inputs\":{...\nStatus:       Failed\nIP:           192.168.40.140\nIPs:\n  IP:           192.168.40.140\nControlled By:  Workflow/message-passing-1-q8jgn\nContainers:\n  wait:\n    Container ID:  docker://26d6c30440777add2af7ef3a55474d9ff36b8c562d7aecfb911ce62911e5fda3\n    Image:         argoproj/argoexec:v2.12.10\n    Image ID:      docker-pullable://argoproj/argoexec@sha256:6edb85a84d3e54881404d1113256a70fcc456ad49c6d168ab9dfc35e4d316a60\n    Port:          \n    Host Port:     \n    Command:\n      argoexec\n      wait\n    State:          Terminated\n      Reason:       Completed\n      Exit Code:    0\n      Started:      Wed, 17 Mar 2021 17:16:43 +0000\n      Finished:     Wed, 17 Mar 2021 17:17:03 +0000\n    Ready:          False\n    Restart Count:  0\n    Environment:\n      ARGO_POD_NAME:  message-passing-1-q8jgn-607612432 (v1:metadata.name)\n    Mounts:\n      /argo/podmetadata from podmetadata (rw)\n      /mainctrfs/mnt/logs from log-p1-vol (rw)\n      /mainctrfs/mnt/processed from processed-p1-vol (rw)\n      /var/run/docker.sock from docker-sock (ro)\n      /var/run/secrets/kubernetes.io/serviceaccount from argo-token-v2w56 (ro)\n  main:\n    Container ID:  docker://67e6d6d3717ab1080f14cac6655c90d990f95525edba639a2d2c7b3170a7576e\n    Image:         REDACTED\n    Image ID:      REDACTED\n    Port:          \n    Host Port:     \n    Command:\n      /bin/bash\n      -c\n    Args:\n    State:          Terminated\n      Reason:       Error\n      Exit Code:    6\n      Started:      Wed, 17 Mar 2021 17:16:43 +0000\n      Finished:     Wed, 17 Mar 2021 17:17:03 +0000\n    Ready:          False\n    Restart Count:  0\n    Environment:    \n    Mounts:\n      /mnt/logs/ from log-p1-vol (rw)\n      /mnt/processed/ from processed-p1-vol (rw)\n      /var/run/secrets/kubernetes.io/serviceaccount from argo-token-v2w56 (ro)\nConditions:\n  Type              Status\n  Initialized       True\n  Ready             False\n  ContainersReady   False\n  PodScheduled      True\nVolumes:\n  podmetadata:\n    Type:  DownwardAPI (a volume populated by information about the pod)\n    Items:\n      metadata.annotations -> annotations\n  docker-sock:\n    Type:          HostPath (bare host directory volume)\n    Path:          /var/run/docker.sock\n    HostPathType:  Socket\n  processed-p1-vol:\n    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)\n    ClaimName:  message-passing-1-q8jgn-processed-p1-vol\n    ReadOnly:   false\n  log-p1-vol:\n    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)\n    ClaimName:  message-passing-1-q8jgn-log-p1-vol\n    ReadOnly:   false\n  argo-token-v2w56:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  argo-token-v2w56\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  \nTolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age    From               Message\n  ----    ------     ----   ----               -------\n  Normal  Scheduled  7m35s  default-scheduler  Successfully assigned argo/message-passing-1-q8jgn-607612432 to ack1\n  Normal  Pulled     7m31s  kubelet            Container image \"argoproj/argoexec:v2.12.10\" already present on machine\n  Normal  Created    7m31s  kubelet            Created container wait\n  Normal  Started    7m30s  kubelet            Started container wait\n  Normal  Pulled     7m30s  kubelet            Container image already present on machine\n  Normal  Created    7m30s  kubelet            Created container main\n  Normal  Started    7m30s  kubelet            Started container main\n`\n```",
      "solution": "This happens when you try to see logs for a pod with multiple containers and not specify for what container you want to see the log. Typical command to see logs:\n```\n`kubectl logs \n`\n```\nBut your Pod has two container, one named \"wait\" and one named \"main\". You can see the logs from the container named \"main\" with:\n```\n`kubectl logs  -c main\n`\n```\nor you can see the logs from all containers with\n```\n`kubectl logs  --all-containers\n`\n```",
      "question_score": 4,
      "answer_score": 10,
      "created_at": "2021-03-17T20:00:08",
      "url": "https://stackoverflow.com/questions/66679561/how-to-see-pod-logs-a-container-name-must-be-specified-for-pod-choose-one-of"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubectl",
      "question_id": 76678256,
      "title": "Kustomize: Patch multiple resources that starts with same name",
      "problem": "I want to patch multiple deployments that starts with same namePrefix instead of targeting specific resource.\nFor example, I have 2 deployments of nginx deployment-v1.yaml and deployment-v2.yaml. I want to patch both the deployment using nginx- prefix.\ndeployment-v1.yaml\n```\n`apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-v1\n  labels:\n    app: web\nspec:\n  selector:\n    matchLabels:\n      app: web\n  replicas: 5\n  strategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: web\n    spec:\n      containers:\n       \u2014name: nginx\n          image: nginx\n          ports:\n           \u2014containerPort: 80 \n`\n```\ndeployment-v2.yaml\n```\n`apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\n  labels:\n    app: web\nspec:\n  selector:\n    matchLabels:\n      app: web\n  replicas: 5\n  strategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: web\n    spec:\n      containers:\n       \u2014name: nginx\n          image: nginx\n          resources:\n            limits:\n              memory: 200Mi\n            requests:\n              cpu: 100m\n              memory: 200Mi\n          ports:\n           \u2014containerPort: 80\n`\n```\nNow I want to overlay both the deployments with common overlay-patch. I am trying something like this.\noverlay.yaml\n```\n`apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namePrefix: nginx-\nspec:\n  replicas: 10\n`\n```\nkustomization.yml\n```\n`apiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\nresources:\n  - deployment-v1.yaml\n  - deployment-v2.yaml\npatchesStrategicMerge:\n  - overlay.yaml\n`\n```\nBut it is not working as it is expecting a name to match the target and totally ignoring namePrefix. Any help is appreciated.",
      "solution": "You can apply a patch to multiple resources using the `target` attribute in a patch. Given your examples (after fixing the errors I pointed out in my comment), we can write a `kustomization.yaml` like this:\n```\n`apiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\nresources:\n- deployment-v1.yaml\n- deployment-v2.yaml\n\npatches:\n  - target:\n      kind: Deployment\n    patch: |\n      apiVersion: apps/v1\n      kind: Deployment\n      metadata:\n        name: this_value_is_ignored\n      spec:\n        replicas: 10\n`\n```\nThe `target` attribute controls to what resources this patch will apply. With the above configuration, running `kustomize build` results in:\n```\n`apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: web\n  name: nginx-deployment\nspec:\n  replicas: 10\n.\n.\n.\n`\n```\nand:\n```\n`apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: web\n  name: nginx-v1\nspec:\n  replicas: 10\n.\n.\n.\n`\n```\n\nThe above configuration would apply the patch to all deployments in your kustomization. If you wanted to limit the patching to only deployments matching a specific name prefix, you could write:\n```\n`apiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\nresources:\n- deployment-v1.yaml\n- deployment-v2.yaml\n\npatches:\n  - target:\n      kind: Deployment\n      name: nginx.*\n    patch: |\n      apiVersion: apps/v1\n      kind: Deployment\n      metadata:\n        name: this_value_is_ignored\n      spec:\n        replicas: 10\n`\n```\nNote that the `name` pattern is regular expression, not a shell-style glob.",
      "question_score": 4,
      "answer_score": 9,
      "created_at": "2023-07-13T12:13:29",
      "url": "https://stackoverflow.com/questions/76678256/kustomize-patch-multiple-resources-that-starts-with-same-name"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubectl",
      "question_id": 78193875,
      "title": "Failed to install krew: failed to list the remote URL for index default",
      "problem": "When I try to install krew using  krew.sigs.k8s.io instruction under WSL2:\n```\n`(\n  set -x; cd \"$(mktemp -d)\" &&\n  OS=\"$(uname | tr '[:upper:]' '[:lower:]')\" &&\n  ARCH=\"$(uname -m | sed -e 's/x86_64/amd64/' -e 's/\\(arm\\)\\(64\\)\\?.*/\\1\\2/' -e 's/aarch64$/arm64/')\" &&\n  KREW=\"krew-${OS}_${ARCH}\" &&\n  curl -fsSLO \"https://github.com/kubernetes-sigs/krew/releases/latest/download/${KREW}.tar.gz\" &&\n  tar zxvf \"${KREW}.tar.gz\" &&\n  ./\"${KREW}\" install krew\n)\n`\n```\nAnd I got an error:\n```\n`failed to list indexes: failed to list the remote URL for index default: command execution failure, output=\"\": exit status 1\n`\n```\nIf that matter, my configuraiton is\n\nWindows 11\nWSL2 with Kali Linux\n`kubectl` is installed using multi-step kubernetes.io instruciton\n`kubectl version` shows `Client Version: v1.29.3`\n`git --version` shows `git version 2.43.0.windows.1`\n`which kubectl` shows `/usr/bin/kubectl`\n`zsh`\n\nI've seen https://github.com/kubernetes-sigs/krew/issues/706 but nothing helped from me there.",
      "solution": "The issue was with git permissions and it was sovled with help of How to correct `git` reporting `detected dubious ownership in repository` without adding `safe.directory` when using WSL?\n```\n`cd ~/.krew/index/default \ngit status\n`\n```\nOutput:\n```\n`fatal: detected dubious ownership in repository at '//wsl.localhost/kali-linux/home/lislo/.krew/index/default'\nTo add an exception for this directory, call:\n\n        git config --global --add safe.directory '%(prefix)///wsl.localhost/kali-linux/home/lislo/.krew/index/default'\n`\n```\nI've executed the proposed command and re-run installation of krew:\n```\n`(\n  set -x; cd \"$(mktemp -d)\" &&\n  OS=\"$(uname | tr '[:upper:]' '[:lower:]')\" &&\n  ARCH=\"$(uname -m | sed -e 's/x86_64/amd64/' -e 's/\\(arm\\)\\(64\\)\\?.*/\\1\\2/' -e 's/aarch64$/arm64/')\" &&\n  KREW=\"krew-${OS}_${ARCH}\" &&\n  curl -fsSLO \"https://github.com/kubernetes-sigs/krew/releases/latest/download/${KREW}.tar.gz\" &&\n  tar zxvf \"${KREW}.tar.gz\" &&\n  ./\"${KREW}\" install krew\n)\n`\n```\nNow kubectl can see krew installed (I've modified path to avoid clashes with windows version of `kubectl.exe`):\n```\n`$ PATH=\"/bin:$HOME/.krew/bin\" kubectl plugin list\nThe following compatible plugins are available:\n\n/home/lislo/.krew/bin/kubectl-krew\n`\n```",
      "question_score": 4,
      "answer_score": 8,
      "created_at": "2024-03-20T14:51:49",
      "url": "https://stackoverflow.com/questions/78193875/failed-to-install-krew-failed-to-list-the-remote-url-for-index-default"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubectl",
      "question_id": 67200483,
      "title": "How do I diff a Helm template against an existing deployment/release?",
      "problem": "It looks like Helm 3 is making this more difficult: https://github.com/databus23/helm-diff/issues/176\nBut I'm finding that using the helm-diff plugin OR just doing this: `helm template releaseName chart | kubectl diff -f - | bat -l diff -` I'm seeing ALL resources as new with \"+\" next to them. Why is this?\nI'm running these commands:\n```\n`# upgrade\nhelm upgrade --install --create-namespace \\\n    --namespace derps -f helm/deploy-values.yaml \\\n    --set 'parentChart.param1=sdfsdfsdfdsf' \\\n    --set 'parentChart.param2=sdfsdfsdfdsf' \\\n    --set 'parentChart.param3=sdfsdfsdfdsf' \\\n    --set 'parentChart.param4=sdfsdfsdfdsf' \\\n    --set 'parentChart.param5=sdfsdfsdfdsf' \\\n    myapp helm/mychart\n\n# make no changes and try to diff\nhelm template \\\n    --namespace derps -f helm/deploy-values.yaml \\\n    --set 'parentChart.param1=sdfsdfsdfdsf' \\\n    --set 'parentChart.param2=sdfsdfsdfdsf' \\\n    --set 'parentChart.param3=sdfsdfsdfdsf' \\\n    --set 'parentChart.param4=sdfsdfsdfdsf' \\\n    --set 'parentChart.param5=sdfsdfsdfdsf' \\\n    myapp helm/mychart | kubectl diff -f - | bat -l diff -\n`\n```\nI get output that shows the ENTIRE manifest is new- why is this?",
      "solution": "You probably need `-n derps` on the diff too. If memory serves me, `helm template --namespace` doesn't actually inject the value.",
      "question_score": 4,
      "answer_score": 2,
      "created_at": "2021-04-21T19:10:49",
      "url": "https://stackoverflow.com/questions/67200483/how-do-i-diff-a-helm-template-against-an-existing-deployment-release"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubectl",
      "question_id": 65857310,
      "title": "kubectl : unable to recognize &quot;csr.yaml&quot;: no matches for kind &quot;CertificateSigningRequest&quot; in version &quot;certificates.k8s.io/v1&quot;",
      "problem": "i have this template i try to invoke: looking at the docs example here\n```\n`--- \napiVersion: certificates.k8s.io/v1\nkind: CertificateSigningRequest\nmetadata: \n  name: vault-csr\nspec: \n  groups: \n    - system: authenticated\n  request: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURSBSRVFVRVNULS0tLS0KTUlJRklEQ0NBd2dDQVFBd0lERWVNQndHQTFVRUF3d1ZkbUYxYkhRdWRtRjFiSFF0Y0dWeWMyOHVjM1pqTUlJQwpJakFOQmdrcWhraUc5dzBCQVFFRkFBT0NBZzhBTUlJQ0NnS0NBZ0VBdFJubkFQR2R4bG1xdjhMOW1Gc29YOXJuCk9JcTVGTnJMZmRDelZCVEVnUEV6TDgzSWFsT1cya2lrNWFRM282d2NSTmx1S3NzeUl1c0ZUSTFqR2djWjN0eXkKSDFqMlROMmNHMHp4MGVaYTJqK3JMVkkwSmVTdXFHNkdmY01rRzRudUhZSGJraDZUYmgyalc5S0RTUTVRekNzdwo0Rlg4bDZXVEVILzdSemgwNCt0RkdFamxVVktkakJYcnVqMnhBc0NqemJ2Sy9GaEhLRjJwRVpza1pSNWtCbC80Cm1KL2xHUTRUTysyVW5CbmsvalJJd3g5a0ZGWDhucEhGWxxxLS0K\n  signerName: kubernetes.io/kubelet-serving\n  usages:\n  - digital signature\n  - key encipherment\n  - server auth\n`\n```\nthe version of kubectl:\n```\n`$ kubectl version --short\nClient Version: v1.20.0\nServer Version: v1.18.9-eks-d1db3c\n`\n```\nAnd im working with AWS EKS\ni keep getting :\n```\n`$ kubectl create -f csr.yaml\nerror: unable to recognize \"csr.yaml\": no matches for kind \"CertificateSigningRequest\" in version \"certificates.k8s.io/v1\"\n`\n```\nUPDATE\nafter changing to apiVersion: certificates.k8s.io/v1beta1\n```\n`apiVersion: certificates.k8s.io/v1beta1\nkind: CertificateSigningRequest\nmetadata: \n  name: vault-csr\nspec: \n  groups: \n    - system: authenticated\n  request: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURSB.....\n  usages:\n  - digital signature\n  - key encipherment\n  - server auth\n`\n```\nIm getting now this error:\n```\n`$ kubectl create -f csr.yaml\nerror: error validating \"tmp/csr.yaml\": error validating data: ValidationError(CertificateSigningRequest.spec.groups[0]): invalid type for io.k8s.api.certificates.v1beta1.CertificateSigningRequestSpec.groups: got \"map\", expected \"string\"; if you choose to ignore these errors, turn validation off with --validate=false\n`\n```",
      "solution": "As per the K8s change doc, the `CertificateSigningRequest` API is promoted to `certificates.k8s.io/v1` only as part of the K8s `1.19` release.\nIt was under `certificates.k8s.io/v1beta1` before that.\nI suspect that to be a problem as your server version is `v1.18`.\nSo, try changing your `apiVersion` as below:\n`apiVersion: certificates.k8s.io/v1beta1`",
      "question_score": 4,
      "answer_score": 7,
      "created_at": "2021-01-23T09:47:27",
      "url": "https://stackoverflow.com/questions/65857310/kubectl-unable-to-recognize-csr-yaml-no-matches-for-kind-certificatesignin"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubectl",
      "question_id": 78041351,
      "title": "AWS EKS nodeGroup stuck at &#39;deleting&#39; status",
      "problem": "I am using terraform to deploy the AWS infrastructure. I stuck at\nEKS nodeGroup is deleting status . The health shows that IAM role is not found, I created that role again and tries to update nodeGroup from CLI and it says\n\"An error occurred (ResourceInUseException) when calling the UpdateNodegroupVersion operation: Nodegroup cannot be updated as it is currently not in Active State\"\nI am unable to delete cluster and nodeGroup from the CLI and from the terraform as well. In my terraform script nodeGroup have instance type is (SPOT - t3.medium) but when I checked nodeGroup from CLI I saw that the instance is ( on-demand t3.large). There is no such config in my terraform code.\n```\n`main.tf: \nmodule \"vpc\" {\n  source = \"./modules/vpc\"\n  cluster_name = var.cluster_name\n}\n\nmodule \"eks\" {\n  source = \"./modules/eks\"\n  cluster_name = var.cluster_name\n  private_subnet_ids = module.vpc.private_subnet_ids\n  subnet_ids = module.vpc.subnet_ids\n}\n\nmodule \"karpenter\" {\n  source = \"./modules/karpenter\"\n  eks-nodeName = module.eks.eks-data\n  eks-connect-provider-url = module.eks.aws_iam_openid_connect_provider\n  eks-connect-provider-arn = module.eks.aws_iam_openid_connect_provider-arn\n  cluster_name = module.eks.cluster_name\n  private-nodes = module.eks.aws_eks_node_group-private-nodes\n}\n\nnodes.tf\nresource \"aws_eks_node_group\" \"private-nodes\" {\n  # count            = var.delete_nodegroup ? 1 : 0\n  cluster_name = aws_eks_cluster.cluster.name\n  node_group_name = \"private-nodes\"\n  node_role_arn = aws_iam_role.nodes.arn\n  \n  # subnet_ids = [ \n  #   aws_subnet.private-eu-west-1a.id,\n  #   aws_subnet.private-eu-west-1b.id\n  #  ]\n\n  subnet_ids = var.private_subnet_ids\n\n    capacity_type  = \"SPOT\"\n    instance_types = [\"t3.medium\"]\n\n   scaling_config {\n     desired_size = 1\n     max_size = 6\n     min_size = 1\n   }\n\n   update_config {\n    max_unavailable = 1\n   }\n\n   labels = {\n     role = \"general\"\n   }\n\n  \ndepends_on = [\n    aws_iam_role_policy_attachment.nodes-AmazonEKSWorkerNodePolicy,\n    aws_iam_role_policy_attachment.nodes-AmazonEKS_CNI_Policy,\n    aws_iam_role_policy_attachment.nodes-AmazonEC2ContainerRegistryReadOnly,\n  ]\n\n  # Allow external changes without Terraform plan difference\n  lifecycle {\n    ignore_changes = [scaling_config[0].desired_size]\n  }\n  \n}\n\n`\n```\nI used this command to delete the nodeGroup\n`aws eks delete-nodegroup --cluster-name eks-gd360 --nodegroup-name private-nodes --region eu-west-1`\nI have followed these docs: Deleting a managed node group\ndeleting EKS cluster",
      "solution": "I engaged with the AWS support billing team to address this issue, Initially, they recommended purchasing AWS Developer Support to access developer assistance for the problem at hand. I expressed my belief that the issue is AWS internal, and they agreed to investigate further.\nAfter a period of 4 to 5 days, the support team took action by deleting the NodeGroup, which allowed me to delete the EKS cluster.\nI asked them a technical reason and this is what they replied:\n\nThis is an internal issue at the AWS end and this can occur due to various reasons. However, they've suggested you to wait till the current request is completed before you can submit next action.\nFor example: When you create a Cluster wait for it to be successfully complete before initiating the creation of Node groups. If we avoid overlapping the requests while one is in progress we can avoid this error from repeating again.\n\nI destroyed resources by terraform destroy, maybe issue occurred from terraform.",
      "question_score": 4,
      "answer_score": 2,
      "created_at": "2024-02-22T14:30:39",
      "url": "https://stackoverflow.com/questions/78041351/aws-eks-nodegroup-stuck-at-deleting-status"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubectl",
      "question_id": 70529570,
      "title": "error with dry-run on the server in kubernetes",
      "problem": "With:\n`kubectl apply -f web.yaml --server-dry-run --validate=false -o yaml`\nI get an error:\n```\n`Error: unknown flag: --server-dry-run\nSee 'kubectl apply --help' for usage.\n`\n```\nAnd even with:\n`kubectl apply -f web.yaml --dry-run=server --validate=false -o yaml`\nI get another error:\n```\n`Warning: resource deployments/web is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically.\nError from server (Conflict): error when applying patch:\n{\"metadata\":{\"annotations\":{\"kubectl.kubernetes.io/last-applied-configuration\":\"{\\\"apiVersion\\\":\\\"apps/v1\\\",\\\"kind\\\":\\\"Deployment\\\",\\\"metadata\\\":{\\\"annotations\\\":{},\\\"creationTimestamp\\\":\\\"2021-12-30T08:51:06Z\\\",\\\"generation\\\":1,\\\"labels\\\":{\\\"app\\\":\\\"web\\\"},\\\"name\\\":\\\"web\\\",\\\"namespace\\\":\\\"default\\\",\\\"resourceVersion\\\":\\\"1589\\\",\\\"uid\\\":\\\"c2a4c20e-f55b-4113-b8e6-d2c19bb3e91c\\\"},\\\"spec\\\":{\\\"progressDeadlineSeconds\\\":600,\\\"replicas\\\":1,\\\"revisionHistoryLimit\\\":10,\\\"selector\\\":{\\\"matchLabels\\\":{\\\"app\\\":\\\"web\\\"}},\\\"strategy\\\":{\\\"rollingUpdate\\\":{\\\"maxSurge\\\":\\\"25%\\\",\\\"maxUnavailable\\\":\\\"25%\\\"},\\\"type\\\":\\\"RollingUpdate\\\"},\\\"template\\\":{\\\"metadata\\\":{\\\"creationTimestamp\\\":null,\\\"labels\\\":{\\\"app\\\":\\\"web\\\"}},\\\"spec\\\":{\\\"containers\\\":[{\\\"image\\\":\\\"nginx\\\",\\\"imagePullPolicy\\\":\\\"Always\\\",\\\"name\\\":\\\"nginx\\\",\\\"resources\\\":{},\\\"terminationMessagePath\\\":\\\"/dev/termination-log\\\",\\\"terminationMessagePolicy\\\":\\\"File\\\"}],\\\"dnsPolicy\\\":\\\"ClusterFirst\\\",\\\"restartPolicy\\\":\\\"Always\\\",\\\"schedulerName\\\":\\\"default-scheduler\\\",\\\"securityContext\\\":{},\\\"terminationGracePeriodSeconds\\\":30}}},\\\"status\\\":{}}\\n\"},\"resourceVersion\":\"1589\"}}\nto:\nResource: \"apps/v1, Resource=deployments\", GroupVersionKind: \"apps/v1, Kind=Deployment\"\nName: \"web\", Namespace: \"default\"\nfor: \"web.yaml\": Operation cannot be fulfilled on deployments.apps \"web\": the object has been modified; please apply your changes to the latest version and try again\n`\n```\nWhat should I do?\nI'm using docker-desktop and my kubectl version is:\n```\n`Client Version: version.Info{Major:\"1\", Minor:\"22\", GitVersion:\"v1.22.4\", GitCommit:\"b695d79d4f967c403a96986f1750a35eb75e75f1\", GitTreeState:\"clean\", BuildDate:\"2021-11-17T15:48:33Z\", GoVersion:\"go1.16.10\", Compiler:\"gc\", Platform:\"darwin/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"22\", GitVersion:\"v1.22.4\", GitCommit:\"b695d79d4f967c403a96986f1750a35eb75e75f1\", GitTreeState:\"clean\", BuildDate:\"2021-11-17T15:42:41Z\", GoVersion:\"go1.16.10\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n`\n```\nand my cluster version is `1.22.4`",
      "solution": "I get an error:\n\n`Error: unknown flag: --server-dry-run\nSee 'kubectl apply --help' for usage.\n`\nThat's correct. This flag is deprecated. You need to use `--dry-run=server` flag. For more look at this site.\nAs for the second problem, it seems that this is correct on the part of k8s. You can find the explanation here. If you want to resolve your problem you need to remove fields `creationTimestamp`. It is well explained in this question.",
      "question_score": 4,
      "answer_score": 6,
      "created_at": "2021-12-30T10:13:38",
      "url": "https://stackoverflow.com/questions/70529570/error-with-dry-run-on-the-server-in-kubernetes"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubectl",
      "question_id": 65657672,
      "title": "Unable to run &#39;kubectl&#39; commands after using impersonation to fetch GKE cluster credentials",
      "problem": "My Objective\nI want to use GCP impersonation to fetch my GKE cluster credentials. And then I want to run `kubectl` commands.\nInitial Context\n\nI have a GCP project called `rakib-example-project`\nI have 2 ServiceAccounts in the project called:\n\nowner@rakib-example-project.iam.gserviceaccount.com\n\nit has the project-wide `roles/owner` role - so it can do anything and everything inside the GCP project\n\nexecutor@rakib-example-project.iam.gserviceaccount.com\n\nit only has the project-wide `roles/iam.serviceAccountTokenCreator` role - so it can impersonate the owner ServiceAccount in the GCP project\n\nI have 1 GKE cluster in the project called `my-gke-cluster`\n\nThe Problem\n\u2705 I have authenticated as the executor ServiceAccount:\n`$ gcloud auth activate-service-account --key-file=my_executor_sa_key.json\n\nActivated service account credentials for: [executor@rakib-example-project.iam.gserviceaccount.com]\n`\n\u2705 I have fetched GKE cluster credentials by impersonating the owner:\n`$ gcloud container clusters get-credentials my-gke-cluster \\\n  --zone asia-southeast1-a \\\n  --project rakib-example-project \\\n  --impersonate-service-account=owner@rakib-example-project.iam.gserviceaccount.com\n\nWARNING: This command is using service account impersonation. All API calls will be executed as [owner@rakib-example-project.iam.gserviceaccount.com].\nWARNING: This command is using service account impersonation. All API calls will be executed as [owner@rakib-example-project.iam.gserviceaccount.com].\nFetching cluster endpoint and auth data.\nkubeconfig entry generated for my-gke-cluster.\n`\n\u274c I am failing to list cluster nodes due to missing `container.nodes.list` permission:\n`$ kubectl get nodes\n\nError from server (Forbidden): nodes is forbidden: User \"executor@rakib-example-project.iam.gserviceaccount.com\" cannot list resource \"nodes\" in API group \"\" at the cluster scope: requires one of [\"container.nodes.list\"] permission(s).\n`\nBut I have already impersonated the Owner ServiceAccount. Why would it still have missing permissions? \ud83d\ude27\ud83d\ude27\ud83d\ude27\nMy Limitations\nIt works well if i grant my executor ServiceAccount the `roles/container.admin` role. However, I am not allowed to grant such roles to my executor ServiceAccount due to compliance requirements. I can only impersonate the owner ServiceAccount and THEN do whatever I want through it - not directly.",
      "solution": "If you have a look to your kubeconfig file at this location `~/.kube/config`, you can see the list of authorization and the secrets, such as\n```\n`- name: gke_gdglyon-cloudrun_us-central1-c_knative\n  user:\n    auth-provider:\n      config:\n        access-token: ya29.-9XQmaTQodj4kS39w\n        cmd-args: config config-helper --format=json\n        cmd-path: /usr/lib/google-cloud-sdk/bin/gcloud\n        expiry: \"2020-08-25T17:48:39Z\"\n        expiry-key: '{.credential.token_expiry}'\n        token-key: '{.credential.access_token}'\n      name: gcp\n`\n```\nYou see external references (`expiry-key` and `token-key`) and a `cmd-path`. The command path is interesting because when a new token need to be generated, it will be called.\nHowever, you see any mention of the impersonation. You have to add it in the command path, to be used by default. For this, add it in your config like this:\n```\n`gcloud config set auth/impersonate_service_account owner@rakib-example-project.iam.gserviceaccount.com\n`\n```\nNow, every use of the gcloud CLI will use the impersonate service account, and it's what you want to generate a valid access_token to reach your GKE cluster",
      "question_score": 4,
      "answer_score": 6,
      "created_at": "2021-01-10T20:40:07",
      "url": "https://stackoverflow.com/questions/65657672/unable-to-run-kubectl-commands-after-using-impersonation-to-fetch-gke-cluster"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubectl",
      "question_id": 76385596,
      "title": "Issue with installing apache superset on EKS",
      "problem": "I have created an EKS cluster with ALB setup. I tried installing superset by following the steps provided in https://superset.apache.org/docs/installation/running-on-kubernetes/\nmy-values.yaml\n```\n`ingress:\n  enabled: true\n  ingressClassName: ~\n  annotations:\n    kubernetes.io/ingress.class: alb\n    alb.ingress.kubernetes.io/scheme: internet-facing\n    alb.ingress.kubernetes.io/target-type: instance\n    # kubernetes.io/tls-acme: \"true\"\n    ## Extend timeout to allow long running queries.\n    # nginx.ingress.kubernetes.io/proxy-connect-timeout: \"300\"\n    # nginx.ingress.kubernetes.io/proxy-read-timeout: \"300\"\n    # nginx.ingress.kubernetes.io/proxy-send-timeout: \"300\"\n  path: /\n  pathType: ImplementationSpecific\n  hosts:\n    - chart-example.local\n  tls: []\n  extraHostsRaw: []\n  #  - secretName: chart-example-tls\n  #    hosts:\n  #      - chart-example.local\n`\n```\nWhen I am running `helm upgrade --install --values my-values.yaml superset superset/superset --timeout 10m30s`, it takes a lot of time and returns\n```\n`Error: UPGRADE FAILED: post-upgrade hooks failed: 1 error occurred:\n    * timed out waiting for the condition\n`\n```\nand when I run\n```\n`[ec2-user@ip-1**-**-**-*** ~]$ kubectl get pods\nNAME                               READY   STATUS       RESTARTS        AGE\nsuperset-7866fcc8b4-tcpk4          0/1     Init:0/1     8 (6m53s ago)   33m\nsuperset-init-db-6q9dp             0/1     Init:Error   0               5m24s\nsuperset-init-db-7hqz4             0/1     Init:Error   0               7m48s\nsuperset-init-db-jt87x             0/1     Init:Error   0               12m\nsuperset-init-db-rt85r             0/1     Init:Error   0               10m\nsuperset-init-db-zptz6             0/1     Init:Error   0               2m40s\nsuperset-postgresql-0              0/1     Pending      0               33m\nsuperset-redis-master-0            1/1     Running      0               33m\nsuperset-worker-748db75bf7-9kzfp   0/1     Init:0/1     8 (6m56s ago)   33m\n`\n```\nI am new to kubernetes and this is new to me. Please help!\nEdit:1\nAdded EBS CSI driver and Storage Class and went ahead with superset installation. Ran the following commands. Attaching responses\n```\n`kubectl get pods\n    NAME                               READY   STATUS     RESTARTS       AGE\n    superset-7866fcc8b4-q59nd          0/1     Init:0/1   4 (109s ago)   13m\n    superset-init-db-gq9b9             0/1     Pending    0              13m\n    superset-postgresql-0              0/1     Pending    0              13m\n    superset-redis-master-0            1/1     Running    0              13m\n    superset-worker-748db75bf7-n7t2r   0/1     Init:0/1   5 (91s ago)    13m\n\n[ec2-user@ip-172-31-23-209 ~]$ kubectl logs superset-worker-748db75bf7-n7t2r\nDefaulted container \"superset\" out of: superset, wait-for-postgres-redis (init)\nError from server (BadRequest): container \"superset\" in pod \"superset-worker-748db75bf7-n7t2r\" is waiting to start: PodInitializing\n\n[ec2-user@ip-172-31-23-209 ~]$ kubectl logs superset-7866fcc8b4-q59nd\nDefaulted container \"superset\" out of: superset, wait-for-postgres (init)\nError from server (BadRequest): container \"superset\" in pod \"superset-7866fcc8b4-q59nd\" is waiting to start: PodInitializing\n\nkubectl describe pod superset-postgresql-0\nEvents:\n  Type     Reason            Age                  From               Message\n  ----     ------            ----                 ----               -------\n  Warning  FailedScheduling  4m20s (x4 over 16m)  default-scheduler  0/1 nodes are available: 1 Too many pods. preemption: 0/1 nodes are available: 1 No preemption victims found for incoming pod.\n`\n```",
      "solution": "If you're running PostgreSQL, and you're using EKS 1.23 or higher, you'll need to install a CSI driver, e.g. the EBS CSI driver. Starting with 1.23, EKS no longer ships with a storage driver (the in-tree driver was deprecated). After installing the CSI driver, create a default storage class. Your pods should start shortly thereafter. If you're new to Kubernetes, I'd recommend installing the CSI driver through EKS addons.",
      "question_score": 4,
      "answer_score": 2,
      "created_at": "2023-06-01T22:45:05",
      "url": "https://stackoverflow.com/questions/76385596/issue-with-installing-apache-superset-on-eks"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubectl",
      "question_id": 74588493,
      "title": "Error when trying to create a deployment using YAML: Deployment in version &quot;v1&quot; cannot be handled as a Deployment",
      "problem": "I am new to K8s and trying to create a deployment using the below YAML file.\n```\n`apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: building-apps-deploy\n  labels:\n    app: kubeacademy\nspec:\n  replicas: 2\n  selector:\n    matchlabels:\n      app: kubeacademy\n  template:\n    metadata:\n      labels:\n        app: kubeacademy\n    spec:\n      containers:\n        - name: building-apps-containers\n          image: 'lander2k2/building-apps:0.1'\n\n`\n```\nI had the file validated by 3 online validator websites and all is coming back ok but when I run the command - `kubectl apply -f deployment_yam27112022.yml`, it is throwing the below error:\n\nError from server (BadRequest): error when creating \"deployment_yam27112022.yml\": Deployment in version \"v1\" cannot be handled as a Deployment: strict decoding error: unknown field \"spec.selector.matchlabels\"*\n\nI understand that the spec.selector.matchlables has some issue but cannot pinpoint the same.\nRequest the community help to resolve this situation and let me know if there are any online resources for validating the YAML files or generating ones which are recommended?\nAdditional info:\nThe pod was created successfully using below YAML file:\n```\n`apiVersion: v1              \nkind: Pod\nmetadata:\n    name: building-apps-pod\n    labels:\n        app: kubeacademy\nspec:\n    containers:\n    - name: building-apps-container\n      image: lander2k2/building-apps:0.1\n`\n```\ncommand used for pod creation: `kubectl apply -f CreatePod22112022.yml`\nand the cluster was created using the command `kind create cluster --name demo271122`\nTo conclude my question: was trying to create a deployment post creation of cluster and pod. The deployment command is failing again and again.\nI have also gone through the questions with same / similar error messages on stack overflow but have not been able to resolve the issue - some suggestions to earlier question was to format the YAML file properly but have already tried the same using Notepad++ and online yaml validator websites.",
      "solution": "Your error message says:\n```\n`unknown field \"spec.selector.matchlabels\"\n`\n```\nIn your deployment, we can see the following:\n```\n`selector:\n  matchlabels:\n    app: kubeacademy\n`\n```\nYou meant to do this instead:\n```\n`selector:\n  matchLabels:\n    app: kubeacademy\n`\n```",
      "question_score": 4,
      "answer_score": 3,
      "created_at": "2022-11-27T09:13:56",
      "url": "https://stackoverflow.com/questions/74588493/error-when-trying-to-create-a-deployment-using-yaml-deployment-in-version-v1"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubectl",
      "question_id": 73012913,
      "title": "Kubernetes pull from image private network / fails to respect /etc/hosts of server",
      "problem": "I am running a small 3 node test kubernetes cluster (using kubeadm) running on Ubuntu Server 22.04, with Flannel as the network fabric.  I also have a separate gitlab private server, with container registry set up and working.\nThe problem I am running into is I have a simple test deployment, and when I apply the deployment yaml, it fails to pull the image from the gitlab private server.\n`apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: platform-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: platform-service\n  template:\n    metadata:\n      labels:\n        app: platform-service\n    spec:\n      containers:\n        - name: platform-service\n          image: registry.examle.com/demo/platform-service:latest\n`\nUbuntu Server: /etc/hosts (the relevant line)\n`192.168.1.30 registry.example.com\n`\nThe Error\n`Failed to pull image \"registry.example.com/demo/platform-service:latest\": \nrpc error: code = Unknown desc = failed to pull and unpack image \n\"registry.example.com/deni/platform-service:latest\": failed to resolve reference \n\"registry.example.com/demo/platform-service:latest\": failed to do request: Head \n\"https://registry.example.com/v2/demo/platform-service/manifests/latest\": dial tcp \nxxx.xxx.xxx.xxx:443: i/o timeout\n`\nThe 'xxx.xxx.xxx.xxx' is related to my external network, to which exists a domain name in the DNS, however all of my internal networks are set up to attach to the internal network representation, and 'registry.example.com' is a representation of my own domains.\nAlso to note:\n`docker pull registry.example.com/demo/platform-service:latest\n`\nFrom the command line of the server, works perfectly fine, it is just not working from kubernetes deploy yaml.\nThe problem\nWhile the network on the server, and the host files on the server are configured correctly, the docker image is not resolving because when I apply it is not using the correct  IP (that is configured in hosts), rather a public IP that is a different server. And the reason for the timeout is because the public facing server is not set up the same.\nWhen I run `kubectl apply -f platform-service.yaml` why does it not respect the hosts file of the server, and is there a way configure hosts inside Kubernetes.\n(If this problem is not clear, I apologize, I am quite new, and still learning terminology, maybe why google is not helping me with this problem.)\nThe closest S/O I could find is:\nKubernetes not able pull image from private registry having private domain pointed via /etc/hosts\n(SO Answer #1): hostAliases (this is for the pod itself, not pulling the image), also, installed through apt/package manager rather than snap.  With the rest of the answer suggests changing the distribution, which I would rather go with my current setup than change it.\nUpdate\nAttempts to add hosts to coredns not working either:\n(How to change host name resolve like host file in coredns)\n```\n`kubectl -n kube-system edit configmap/coredns\n`\n```\n```\n`...\n    .:53 {\n        errors\n        health {\n           lameduck 5s\n        }\n        ready\n        hosts custom.hosts registry.example.com {\n            192.168.1.30 registry.example.com\n            fallthrough\n        }\n        kubernetes cluster.local in-addr.arpa ip6.arpa {\n           pods insecure\n           fallthrough in-addr.arpa ip6.arpa\n           ttl 30\n        }\n        prometheus :9153\n        forward . /etc/resolv.conf {\n           max_concurrent 1000\n        }\n        cache 30\n        loop\n        reload\n        loadbalance\n    }\n...\n`\n```\ndeleted the coredns pods (so they are recreated)\nand still the docker pull on the deployment fails with the external ip address instead of the internal address.",
      "solution": "After going through many different solutions and lots of research and testing.  The answer was actually very simple.\nSolution in my case\nThe /etc/hosts file MUST contain the host for the registry (and possibly the entry for the gitlab instance as well) on EVERY node of the cluster including the master node.\n```\n`192.168.1.30 registry.example.com\n192.168.1.30 gitlab.example.com    # Necessary in my case, not sure required\n`\n```\nOnce I included that on each of the 2 slaves, it attempted to pull the image, and failed with credential issues (which I was expecting to see once the hosts issue was resolved).  From there I was able to add the credentials and now the image pulls fine from the private registry rather than the public facing registry.\nBonus: Fix for credentials error connecting to private registry (not part of the original question, but part of the setup process for connecting)\nAfter fixing the /etc/hosts issue, you will probably need to set up 'regcred' credentials to access the private registry, Kubernetes documentation provides the steps on that part:\nhttps://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/",
      "question_score": 4,
      "answer_score": 3,
      "created_at": "2022-07-17T17:10:44",
      "url": "https://stackoverflow.com/questions/73012913/kubernetes-pull-from-image-private-network-fails-to-respect-etc-hosts-of-serv"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubectl",
      "question_id": 73529247,
      "title": "How to copy file from local system to kubernetes pods with new kubectl version",
      "problem": "I'm trying to copy files from my local to kubernetes pods. I've been using the command for a very few months now and everything worked fine:\n```\n`kubectl cp C:/test.jar backend-0:/usr/local/myproject/tomcat/webapps/WEB-INF/lib -c tomcat\n`\n```\nNow I bought a new computer, re-configured the development environment, and now when I try to use the same command, I see an error:\n```\n`error: one of src or dest must be a local file specification\n`\n```\nI'm using Google Cloud SDK as a terminal. The old computer had the Google Cloud SDK version 368.0.0, now it's 396.0.0. Also, kubectl version changed, was 1.16.6 now it's 1.22.12.\nPlease tell me which command in the terminal will be relevant now, how should I use copying? Thanks!",
      "solution": "You cannot use absolute paths like `C:/test.jar`\nYou can try the following:\n\nMake sure you are under `c:/`\nExecute:\n`kubectl cp test.jar backend-0:/usr/local/myproject/tomcat/webapps/WEB-INF/lib -c tomcat`",
      "question_score": 4,
      "answer_score": 2,
      "created_at": "2022-08-29T15:05:24",
      "url": "https://stackoverflow.com/questions/73529247/how-to-copy-file-from-local-system-to-kubernetes-pods-with-new-kubectl-version"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubectl",
      "question_id": 70293779,
      "title": "The command `gcloud container clusters get-credentials ` will not create a kubeconfig",
      "problem": "When I type `gcloud container clusters get-credentials`, I get response `entry generated for ***.`. and it looks like it is generated, but when I hit `kubectl config view`, there is nothing.\nReference of `gcloud container clusters get-credentials` says,\n\ngcloud container clusters get-credentials updates a kubeconfig file with appropriate credentials and endpoint information to point kubectl at a specific cluster in Google Kubernetes Engine.\n\nSo I thought the problem was that `~/.kube/config` did not exist, but creating an empty file did not change it.",
      "solution": "Thanks to @DazWilkin in the comments for suggesting running the command with the `--verbosity=debug` flag, which then outputted where it was writing the file.\nThe reason was that the `PATH` of WSL included the `PATH` of the Windows side by default, so it was calling the `gcloud` on Windows (installed by scoop).\nI solved the problem by excluding the `PATH` of Windows, referring to this gists.",
      "question_score": 4,
      "answer_score": 2,
      "created_at": "2021-12-09T17:57:43",
      "url": "https://stackoverflow.com/questions/70293779/the-command-gcloud-container-clusters-get-credentials-will-not-create-a-kubec"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubectl",
      "question_id": 69093222,
      "title": "Can a deployment be completed even when readiness probe is failing",
      "problem": "I have an application running in Kubernetes as a `StatefulSet` that starts 2 pods. It has configured a liveness probe and a readiness probe.\nThe `liveness probe` call a simple `/health` endpoint that responds when the server is done loading\nThe `readiness probe`, wait for some start-up job to complete. The job can take several minutes in some cases, and only when it finish the api of the application is ready to start accepting requests.\nEven when the api is not available my app also run side jobs that don't depend on it, and I expect them to be done while the startup is happening too.\nIs it possible to force Kubernetes deployment to complete and deploy 2 pods, even when the readiness probe is still not passing?\nFrom the docs I get that the only effect of a readiness probe not passing is that the current pod won't be included as available in the loadbalancer service (which is actually the only effect that I want).\n\nIf the readiness probe fails, the endpoints controller removes the\nPod's IP address from the endpoints of all Services that match the\nPod.\n\nHowever I am also seeing that the deployment never finishes, since pod 1 readiness probe is not passing and pod 2 is never created.\n```\n`kubectl rollout restart statefulset/pod\nkubectl get pods \nNAME                            READY   STATUS    RESTARTS   AGE\npod-0                           1/2     Running   0          28m\n`\n```\nIf the readiness probe failure, always prevent the deployment, Is there other way to selectively expose only ready pods in the loadbalancer, while not marking them as Unready during the deployment?\nThanks in advance!",
      "solution": "StatefulSet deployment\n\nIs it possible to force kubernetes deployment to complete and deploy 2\npods, even when the readiness probe is still not passing?\n\nAssuming it's meant `statefulSet` instead of `deployment` as object, the answer is no, it's not possible by design, most important is second point:\n\nFor a StatefulSet with N replicas, when Pods are being deployed, they are created sequentially, in order from {0..N-1}.\nBefore a scaling operation is applied to a Pod, all of its predecessors must be Running and Ready.\nWhen Pods are being deleted, they are terminated in reverse order, from {N-1..0}.\n\nWhen the nginx example above is created, three Pods will be deployed\nin the order web-0, web-1, web-2. web-1 will not be deployed before\nweb-0 is Running and Ready, and web-2 will not be deployed until web-1\nis Running and Ready\n\nStatefulSets - Deployment and scaling guaranties\nReadyness probe, endpoints and potential workaround\n\nIf the readiness probe failure, always prevent the deployment, Is\nthere other way to selectively expose only ready pods in the load\nbalancer, while not marking them as Unready during the deployment?\n\nThis is by design, pods are added to service endpoints once they are in `ready` state.\nSome kind of potential workaround can be used, at least in simple example it does work, however you should try and evaluate if this approach will suit your case, this is fine to use as initial deployment.\n`statefulSet` can be started without `readyness` probe included, this way `statefulSet` will start pods one by one when previous is `run and ready`, `liveness` may need to set up `initialDelaySeconds` so kubernetes won't restart the pod thinking it's unhealthy. Once `statefulSet` is fully run and ready, you can add `readyness` probe to the `statefulSet`.\nWhen `readyness` probe is added, kubernetes will restart all pods again starting from the last one and your application will need to start again.\nIdea is to start all pods and they will be able to serve requests +- at the same time, while with `readyness` probe applied, only one pod will start in 5 minutes for instance, next pod will take 5 minutes more and so on.\nExample\nSimple example to see what's going on based on `nginx` webserver and `sleep 30` command which makes kubernetes think when `readyness` probe is setup that pod is `not ready`.\n\nApply `headless service`\nComment `readyness` probe in `statefulSet` and apply manifest\nObserve that all pods are created right after previous pod is `running and ready`\nUncomment `readyness` probe and apply manifest\nKubernetes will recreate all pods starting from the last one waiting this time `readyness` probe to complete and flag a pod as `running and ready`.\n\nVery convenient to use this command to watch for progress:\n```\n`watch -n1 kubectl get pods -o wide\n`\n```\n`nginx-headless-svc.yaml`:\n```\n`apiVersion: v1\nkind: Service\nmetadata:\n  name: nginx\n  labels:\n    app: nginx\nspec:\n  ports:\n  - port: 80\n    name: web\n  clusterIP: None\n  selector:\n    app: nginx\n`\n```\n`nginx-statefulset.yaml`:\n```\n`apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: nginx\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  serviceName: \"nginx\"\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\n        ports:\n        - containerPort: 80\n          name: web\n        command: [\"/bin/bash\", \"-c\"]\n        args: [\"sleep 30 ; echo sleep completed ;  nginx -g \\\"daemon off;\\\"\"]\n        readinessProbe:\n          tcpSocket:\n            port: 80\n          initialDelaySeconds: 1\n          periodSeconds: 5\n`\n```\nUpdate\nThanks to @jesantana for this much easier solution.\nIf all pods have to be scheduled at once and it's not necessary to wait for pods readyness, `.spec.podManagementPolicy` can be set to `Parallel`. Pod Management Policies\nUseful links:\n\nKubernetes statefulsets\nkubernetes liveness, readyness and startup probes",
      "question_score": 4,
      "answer_score": 2,
      "created_at": "2021-09-07T20:48:54",
      "url": "https://stackoverflow.com/questions/69093222/can-a-deployment-be-completed-even-when-readiness-probe-is-failing"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubectl",
      "question_id": 73321432,
      "title": "Terraform kubectl provider error: failed to create kubernetes rest client for read of resource",
      "problem": "I have a Terraform config that (among other resources) creates a Google Kubernetes Engine cluster on Google Cloud. I'm using the `kubectl` provider to add YAML manifests for a ManagedCertificate and a FrontendConfig, since these are not part of the kubernetes or google providers.\nThis works as expected when applying the Terraform config from my local machine, but when I try to execute it in our CI pipeline, I get the following error for both of the `kubectl_manifest` resources:\n```\n`Error: failed to create kubernetes rest client for read of resource: Get \"http://localhost/api?timeout=32s\": dial tcp 127.0.0.1:80: connect: connection refused\n`\n```\nSince I'm only facing this issue during CI, my first guess is that the service account is missing the right scopes, but as far as I can tell, all scopes are present. Any suggestions and ideas are greatly appreciated!",
      "solution": "Fixed the issue by adding `load_config_file = false` to the `kubectl` provider config. My provider config now looks like this:\n```\n`data \"google_client_config\" \"default\" {}\n\nprovider \"kubernetes\" {\n  host                   = \"https://${endpoint from GKE}\"\n  token                  = data.google_client_config.default.access_token\n  cluster_ca_certificate = base64decode(CA certificate from GKE)\n}\n\nprovider \"kubectl\" {\n  host                   = \"https://${endpoint from GKE}\"\n  token                  = data.google_client_config.default.access_token\n  cluster_ca_certificate = base64decode(CA certificate from GKE)\n  load_config_file       = false\n}\n`\n```",
      "question_score": 3,
      "answer_score": 13,
      "created_at": "2022-08-11T15:23:08",
      "url": "https://stackoverflow.com/questions/73321432/terraform-kubectl-provider-error-failed-to-create-kubernetes-rest-client-for-re"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubectl",
      "question_id": 68540187,
      "title": "kubectl rollout status - When the command complete?",
      "problem": "Currently I am using this in my pipeline\n```\n`kubectl apply -f deployment.yaml && kubectl rollout status -f deployment.yaml\n`\n```\nWith this in yaml\n```\n`      readinessProbe:\n        tcpSocket:\n          port: 90\n        initialDelaySeconds: 120\n        periodSeconds: 10\n        timeoutSeconds: 10\n        failureThreshold: 1\n        successThreshold: 1\n      livenessProbe:\n        tcpSocket:\n          port: 90\n        initialDelaySeconds: 120\n        periodSeconds: 20\n        timeoutSeconds: 2\n        failureThreshold: 1\n        successThreshold: 1\n`\n```\nFor me, kubectl rollout is running for a very long time, blocking the deployment pipeline. From the documentation\n\nBy default 'rollout status' will watch the status of the latest rollout until it's done\n\nMy question:\n1/ Which actions are the parts that contribute to the deployment \"until it's done\" (resource creation, resource teardown?... )\n2/ Does readinessProbe and livenessProbe contribute to the deployment time",
      "solution": "The criteria for this are in the `kubectl` source.  A deployment is \"complete\" if:\n\nIt hasn't timed out\nIts updated-replica count is at least its desired-replica count (every new pod has been created)\nIts current-replica count is at most its updated-replica count (every old pod has been destroyed)\nIts available-replica count is at least its updated-replica count (every new pod is running)\n\nYou can use `kubectl get deployment -w` or `kubectl get pod -w` to watch a deployment actually happen in real time; the `kubectl get -w` option watches the given resources and prints out a new line whenever they change.  You'll see the following sequence occur (with default Deployment settings, one at a time for \"small\" deployments):\n\nA new pod is created\nThe new pod passes its probes and become ready\nAn old pod is terminated\nThe old pod actually exits and is deleted\n\nSo for `kubectl rollout status deployment/...` to finish, all of these steps must happen \u2013 new pods are created, new pods all pass their health checks, old pods are destroyed \u2013 for every replica in the deployment.",
      "question_score": 3,
      "answer_score": 7,
      "created_at": "2021-07-27T08:51:38",
      "url": "https://stackoverflow.com/questions/68540187/kubectl-rollout-status-when-the-command-complete"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubectl",
      "question_id": 71753537,
      "title": "kubectl returning cannot unmarshal string into Go value of type map[string]interface {}",
      "problem": "I am trying to patch a secret using kubectl\n```\n`kubectl patch secret operator-secrets --namespace kube-system --context=cluster1 --patch \"'{\\\"data\\\": {\\\"FOOBAR\\\": \\\"$FOOBAR\\\"}}'\"\n`\n```\nBut I receive the error\n\nError from server (BadRequest): json: cannot unmarshal string into Go value of type map[string]interface {}\n\nIf I run the command using echo, it seems to be a valid JSON\n```\n`$ echo \"'{\\\"data\\\": {\\\"FOOBAR\\\": \\\"$FOOBAR\\\"}}'\"\n\n'{\"data\": {\"FOOBAR\": \"value that I want\"}}'\n`\n```\nWhat can be?",
      "solution": "If I run the command using echo, it seems to be a valid JSON\n\nIn fact, it does not. Look carefully at the first character of the output:\n```\n`'{\"data\": {\"FOOBAR\": \"value that I want\"}}'\n`\n```\nYour \"JSON\" string starts with a single quote, which is an invalid character. To get valid JSON, you would need to rewrite your command to look like this:\n```\n`echo \"{\\\"data\\\": {\\\"FOOBAR\\\": \\\"$FOOBAR\\\"}}\"\n`\n```\nAnd we can confirm that's valid JSON using something like the `jq`\ncommand:\n```\n`$ echo \"{\\\"data\\\": {\\\"FOOBAR\\\": \\\"$FOOBAR\\\"}}\"   | jq .\n{\n  \"data\": {\n    \"FOOBAR\": \"value that i want\"\n  }\n}\n`\n```\nMaking your patch command look like:\n```\n`kubectl patch secret operator-secrets \\\n  --namespace kube-system \\\n  --context=cluster1 \\\n  --patch \"{\\\"data\\\": {\\\"FOOBAR\\\": \\\"$FOOBAR\\\"}}\"\n`\n```\nBut while that patch is now valid JSON, it's still going to fail with\na new error:\n```\n`The request is invalid: patch: Invalid value: \"map[data:map[FOOBAR:value that i want]]\": error decoding from json: illegal base64 data at input byte 5\n`\n```\nThe value of items in the `data` map must be base64 encoded values.\nYou can either base64 encode the value yourself:\n```\n`kubectl patch secret operator-secrets \\\n  --namespace kube-system \\\n  --context=cluster1 \\\n  --patch \"{\\\"data\\\": {\\\"FOOBAR\\\": \\\"$(base64 Or use `stringData` instead:\n```\n`kubectl patch secret operator-secrets \\\n  --namespace kube-system \\\n  --context=cluster1 \\\n  --patch \"{\\\"stringData\\\": {\\\"FOOBAR\\\": \\\"$FOOBAR\\\"}}\"\n`\n```",
      "question_score": 3,
      "answer_score": 8,
      "created_at": "2022-04-05T16:27:56",
      "url": "https://stackoverflow.com/questions/71753537/kubectl-returning-cannot-unmarshal-string-into-go-value-of-type-mapstringinter"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubectl",
      "question_id": 73780754,
      "title": "How do I delete resources that have been applied with kustomize?",
      "problem": "I have to upgrade cert-manager on GKE cluster and due to the big version gap I have to uninstall and re-install it. Basically, I am wondering how should I uninstall it and since I installed it through kustomization file I thought I will do the same with the uninstallation.\nThe question is: is it possible or not?\nI also want to know if I can delete the manifests all together or there's an order? eg: delete controller before deleting the CRDs.\n\n```\n`apiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\n\nresources:\n  - crds/clusterissuers.crd.yaml\n  - crds/issuers.crd.yaml\n  - crds/challenges.crd.yaml\n  - crds/certificaterequests.crd.yaml\n  - crds/orders.crd.yaml\n  - crds/certificates.crd.yaml\n\n  - operator/cainjector\n  - operator/webhook\n  - operator/controller\n`\n```",
      "solution": "You should be able to uninstall it by running `kustomize build 'folder' | kubectl delete -f -`.\nKustomize will remove the resources in order, so I would remove first the operators and later the CRDs as cleanup.",
      "question_score": 3,
      "answer_score": 6,
      "created_at": "2022-09-20T03:29:46",
      "url": "https://stackoverflow.com/questions/73780754/how-do-i-delete-resources-that-have-been-applied-with-kustomize"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubectl",
      "question_id": 72130298,
      "title": "kubectl: create replicaset without a yml file",
      "problem": "I am trying to create a replicaset with kubernetes. This time, I don't have a yml file and this is why I am trying to create the replicaset using a command line.\nWhy `kubectl create replicaset somename --image=nginx` raise an error, and how to fix this?",
      "solution": "You cannot create `replicaset` using the command line. Only the following resource creation is possible using `kubectl create`:\n```\n`kubectl create  --help |awk '/Available Commands:/,/^$/'\nAvailable Commands:\n  clusterrole         Create a cluster role\n  clusterrolebinding  Create a cluster role binding for a particular cluster role\n  configmap           Create a config map from a local file, directory or literal value\n  cronjob             Create a cron job with the specified name\n  deployment          Create a deployment with the specified name\n  ingress             Create an ingress with the specified name\n  job                 Create a job with the specified name\n  namespace           Create a namespace with the specified name\n  poddisruptionbudget Create a pod disruption budget with the specified name\n  priorityclass       Create a priority class with the specified name\n  quota               Create a quota with the specified name\n  role                Create a role with single rule\n  rolebinding         Create a role binding for a particular role or cluster role\n  secret              Create a secret using specified subcommand\n  service             Create a service using a specified subcommand\n  serviceaccount      Create a service account with the specified name\n`\n```\nAlthough, You may use the following way to create the replica set, in the below example, `kubectl create -f` is fed with stdout(`-`):\n```\n`echo \"apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: frontend\n  labels:\n    app: guestbook\n    tier: frontend\nspec:\n  # modify replicas according to your case\n  replicas: 3\n  selector:\n    matchLabels:\n      tier: frontend\n  template:\n    metadata:\n      labels:\n        tier: frontend\n    spec:\n      containers:\n      - name: php-redis\n        image: gcr.io/google_samples/gb-frontend:v3\n\" |kubectl create  -f -\n`\n```",
      "question_score": 3,
      "answer_score": 3,
      "created_at": "2022-05-05T18:03:11",
      "url": "https://stackoverflow.com/questions/72130298/kubectl-create-replicaset-without-a-yml-file"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubectl",
      "question_id": 68336875,
      "title": "Why Can&#39;t I Access My Kubernetes Cluster Using the minikube IP?",
      "problem": "I have some questions regarding my minikube cluster, specifically why there needs to be a tunnel, what the tunnel means actually, and where the port numbers come from.\nBackground\nI'm obviously a total kubernetes beginner...and don't have a ton of networking experience.\nOk. I have the following docker image which I pushed to docker hub. It's a hello express app that just prints out \"Hello world\" at the `/` route.\nDockerFile:\n```\n`FROM node:lts-slim\nRUN mkdir /code\nCOPY package*.json server.js /code/\nWORKDIR /code\nRUN npm install\nEXPOSE 3000 \nCMD [\"node\", \"server.js\"]\n`\n```\nI have the following pod spec:\nweb-pod.yaml:\n`apiVersion: v1\nkind: Pod\nmetadata:\n  name: web-pod\nspec:\n  containers:\n    - name: web\n      image: kahunacohen/hello-kube:latest\n      ports:\n      - containerPort: 3000\n`\nThe following service:\nweb-service.yaml\n`apiVersion: v1\nkind: Service\nmetadata:\n  name: web-service\nspec:\n  type: NodePort\n  selector:\n    app: web-pod\n  ports:\n    - port: 8080\n      targetPort: 3000\n      protocol: TCP\n      name: http\n`\nAnd the following deployment:\nweb-deployment.yaml\n`apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-deployment\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: web-pod\n      service: web-service\n  template:\n    metadata:\n      labels:\n        app: web-pod\n        service: web-service\n    spec:\n      containers:\n      - name: web\n        image: kahunacohen/hello-kube:latest\n        ports:\n        - containerPort: 3000\n          protocol: TCP\n`\nAll the objects are up and running and look good after I create them with kubectl.\nI do this:\n```\n`$ kubectl get services                                                                            \nNAME          TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE\nkubernetes    ClusterIP   10.96.0.1              443/TCP          7h5m\nweb-service   NodePort    10.104.15.61           8080:32177/TCP   25m\n`\n```\n\nThen, as per a book I'm reading if I do:\n\n```\n`$ curl $(minikube ip):8080 # or :32177, # or :3000\n`\n```\nI get no response.\nI found when I do this, however I can access the app by going to `http://127.0.0.1:52650/`:\n```\n`$ minikube service web-service\n|-----------|-------------|-------------|---------------------------|\n| NAMESPACE |    NAME     | TARGET PORT |            URL            |\n|-----------|-------------|-------------|---------------------------|\n| default   | web-service | http/8080   | http://192.168.49.2:32177 |\n|-----------|-------------|-------------|---------------------------|\n\ud83c\udfc3  Starting tunnel for service web-service.\n|-----------|-------------|-------------|------------------------|\n| NAMESPACE |    NAME     | TARGET PORT |          URL           |\n|-----------|-------------|-------------|------------------------|\n| default   | web-service |             | http://127.0.0.1:52472 |\n|-----------|-------------|-------------|------------------------|\n`\n```\nQuestions\n\nwhat this \"tunnel\" is and why we need it?\nwhat the targetPort is for (8080)?\nWhat this line means when I do `kubectl get services`:\n\n```\n`web-service   NodePort 10.104.15.61           8080:32177/TCP   25m\n`\n```\nSpecifically, what is that port mapping means and where `32177` comes from?\n\nIs there some kind of problem with simply mapping the internal port to the same port number externally, e.g. 3000:3000? If so, do we specifically have to provide this mapping?",
      "solution": "Let me answer on all your questions.\n0 - There's no need to create pods separately (unless it's something to test), this should be done by creating deployments (or statefulsets, depends on the app and needs) which will create a `replicaset` which will be responsible for keeping right amount of pods in operational conditions. (you can get familiar with deployments in kubernetes.\n\n1 - Tunnel is used to expose the service from inside of VM where minikube is running to the host machine's network. Works with `LoadBalancer` service type. Please refer to access applications in minikube.\n1.1 - Reason why the application is not accessible on the `localhost:NodePort` is NodePort is exposed within VM where `minikube` is running, not on your local machine.\nYou can find minikube VM's IP by running `minikube IP` and then `curl %GIVEN_IP:NodePort`. You should get a response from your app.\n\n2 - `targetPort` indicates the service with which port connection should be established. Please refer to define the service.\nIn `minikube` it may be confusing since it's pointed to the `service port`, not to the `targetPort` which is define within the service. I think idea was to indicate on which port `service` is accessible within the cluster.\n\n3 - As for this question, there are headers presented, you can treat them literally. For instance:\n```\n`$ kubectl get svc -o wide\nNAME          TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE    SELECTOR\nweb-service   NodePort    10.106.206.158           80:30001/TCP   21m    app=web-pod\n`\n```\n`NodePort` comes from your `web-service.yaml` for `service` object. `Type` is explicitly specified and therefore `NodePort` is allocated. If you don't specify `type` of service, it will be created as `ClusterIP` type and will be accessible only within kubernetes cluster. Please refer to Publishing Services (ServiceTypes).\nWhen service is created with `ClusterIP` type, there won't be a `NodePort` in output. E.g.\n```\n`$ kubectl get svc\nNAME          TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)   AGE\nweb-service   ClusterIP   10.106.206.158           80/TCP    23m\n`\n```\n`External-IP` will pop up when `LoadBalancer` service type is used. Additionally for `minikube` address will appear once you run `minikube tunnel` in a different shell. After your service will be accessible on your host machine by `External-IP` + `service port`.\n\n4 - There are not issues with such mapping. Moreover this is a default behaviour for kubernetes:\n\nNote: A Service can map any incoming port to a targetPort. By default\nand for convenience, the targetPort is set to the same value as the\nport field.\n\nPlease refer to define a service\n\nEdit:\nDepending on the driver of `minikube` (usually this is a `virtual box` or `docker` - can be checked on linux VM in ` .minikube/profiles/minikube/config.json`), `minikube` can have different port forwarding. E.g. I have a `minikube` based on `docker` driver and I can see some mappings:\n```\n`$ docker ps -a\nCONTAINER ID   IMAGE                                 COMMAND                  CREATED      STATUS      PORTS                                                                                                                                  NAMES\nebcbc898b557   gcr.io/k8s-minikube/kicbase:v0.0.23   \"/usr/local/bin/entr\u2026\"   5 days ago   Up 5 days   127.0.0.1:49157->22/tcp, 127.0.0.1:49156->2376/tcp, 127.0.0.1:49155->5000/tcp, 127.0.0.1:49154->8443/tcp, 127.0.0.1:49153->32443/tcp   minikube\n`\n```\nFor instance 22 for ssh to ssh into `minikube VM`. This may be an answer why you got response from `http://127.0.0.1:52650/`",
      "question_score": 3,
      "answer_score": 5,
      "created_at": "2021-07-11T15:53:04",
      "url": "https://stackoverflow.com/questions/68336875/why-cant-i-access-my-kubernetes-cluster-using-the-minikube-ip"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubectl",
      "question_id": 72110957,
      "title": "Kubernetes error when forwarding a port - Connection refused",
      "problem": "I'm trying to port-forward my kubernetes service (through minikube) using the following command:\n```\n`kubectl port-forward svc/hapi-fhir-server 8080:8080 --address=0.0.0.0\n`\n```\nBut after trying to reach `localhost:8080` I get the following error: `\"....... an error occurred forwarding 8080 -> 8080: error forwarding port 8080 to pod {PodID, uid ....:E connect(5, AF=2 127.0.0.1:8080, 16): Connection refused\"`\nI checked which port the pod is listening to through the following command\n`kubectl get pod hapi-fhir-server-666b846cbf-lhmr4 --template=\"{{(index (index .spec.containers 0).ports 0).containerPort}}\"` resulting in answer `8080`\nFor if this helps, my service & deployment files (having removed unrelated lines)\n```\n`apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  ....\nspec:\n  replicas: 2\n  selector:\n    ....\n  template:\n      ....\n    spec:\n      containers:\n        - image: .........\n          name: hapi-fhir-server\n          ports:\n            - containerPort: 8080\n          resources: {}\n      restartPolicy: Always\nstatus: {}\n`\n```\n```\n`apiVersion: v1\nkind: Service\nmetadata:\n  ....\nspec:\n  type: NodePort\n  ports:\n    - protocol: TCP\n      port: 8080\n      targetPort: 8080\n      nodePort: 32069\n  selector:\n    app: hapi-fhir-server\nstatus:\n  loadBalancer: {}\n`\n```\nThe image being used is a HAPI FHIR server with the following configuration that runs on Apache Tomcat (server.xml):\n```\n` \n`\n```\n`server.port` being 8080.\nI can't seem to find an accurate answer to why this is happening, even after going through documentation or similar questions like: Kubernetes Port Forwarding - Connection refused.\nIs there something I am missing, not doing correctly or that I am not thinking of?\nNote: I am relatively new to Kubernetes.",
      "solution": "Apparently there was no issue with the Kubernetes or server configuration but rather the Dockerfile that didn't expose port 8080, now it does through the following:\n```\n`# Dockerfile\n......\nEXPOSE 8080\n......\n`\n```\nThanks to IvanAracki that pointed this out to me through the comments.",
      "question_score": 3,
      "answer_score": 3,
      "created_at": "2022-05-04T11:49:17",
      "url": "https://stackoverflow.com/questions/72110957/kubernetes-error-when-forwarding-a-port-connection-refused"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubectl",
      "question_id": 70517823,
      "title": "How to print Status fields defined in Kubebuilder to show up when using Kubectl",
      "problem": "How do I have to specify the comments like `+kubebuilder:printcolumn` to add columns to the output of the command `kubectl get my-crd.my-group.my-domain.com`?\nI've a CRD (Custom Resource Definition) with the usual `struct`s for the specs and the status (similar to what's explained in the Kubebuilder tutorial here https://book.kubebuilder.io/cronjob-tutorial/new-api.html#adding-a-new-api).\nI've a Status `struct` like this:\n`type ScheduleSetStatus struct {\n    // When was the last time the Schedule Set\n    // was successfully deployed.\n    LastDeployTime string `json:\"lastDeployTime\"` // metav1.Time\n    // The CronJobs that have been successfully deployed\n    DeployedCronJobs []string `json:\"deployedCronJobs\"`\n    // The CronJobs that had errors when the deployment\n    // has been attempted.\n    ErroredCronJobs map[string]string `json:\"erroredCronJobs\"` // TODO `error` JSON serialisable\n}\n`\nWhich has a few problems:\nThe time field\n\nI've tried that to be of type `metav1.Time` (handy formatting as they state at https://book.kubebuilder.io/cronjob-tutorial/api-design.html?highlight=metav1.Time#designing-an-api), but then this comment `// +kubebuilder:printcolumn:name=\"Last Deploy\",type=\"date\",JSONPath=`.status.lastDeployTime`` shows as empty in the output of `kubectl`.\nSo I changed the type to be `string` (then in the controller doing `oess.Status.LastDeployTime = fmt.Sprintf(\"%s\", metav1.Time{Time: time.Now().UTC()})`), then adding the comment `+kubebuilder:printcolumn:name=\"Last Deploy\",type=string,JSONPath=`.status.lastDeployTime`` but still the field is shown as empty in the output of `kubectl`.\n\nThe slice field `[]string` and the map field `map[string]string`\n\nHow do I configure these? Here there's no mention (when clicking on \"Show Detailed Argument Help\"): https://book.kubebuilder.io/reference/markers/crd.html\nIn case these are not \"simple types\" with formatting issues when using `kubectl`, does that mean the only option I have is to make them `string` with some sort of `fmt.Sprintf(...)`?\nAny other option?",
      "solution": "The solution was to add the code to update the resource status in\nthe reconciler method of the controller - `Reconcile(ctx context.Context, req ctrl.Request)`, like this:\n`    // Update the status for \"last deploy time\" of a ScheduleSet\n    myStruct.Status.LastDeployTime = metav1.Time{Time: time.Now().UTC()} // https://book.kubebuilder.io/cronjob-tutorial/api-design.html?highlight=metav1.Time#designing-an-api\n    if err := r.Status().Update(ctx, &myStruct); err != nil {\n        log.Error(err, \"unable to update status xyz\")\n        return ctrl.Result{}, err\n    }\n`\nThe special Kubebuilder annotation was all right:\n`//+kubebuilder:printcolumn:name=\"Last Deploy\",type=\"date\",JSONPath=`.status.lastDeployTime`\n`\nAlso, Go slices and Go maps work out of the box with comments like:\n`...\n\n    DeployedCronJobs []string `json:\"deployedCronJobs\"`\n\n...\n\n    ErroredCronJobs map[string]string `json:\"erroredCronJobs\"`\n...\n\n//+kubebuilder:printcolumn:name=\"Deployed CJs\",type=string,JSONPath=`.status.deployedCronJobs`\n//+kubebuilder:printcolumn:name=\"Errored CJs\",type=string,JSONPath=`.status.erroredCronJobs`\n`",
      "question_score": 3,
      "answer_score": 3,
      "created_at": "2021-12-29T10:59:54",
      "url": "https://stackoverflow.com/questions/70517823/how-to-print-status-fields-defined-in-kubebuilder-to-show-up-when-using-kubectl"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubectl",
      "question_id": 66818858,
      "title": "Kubernetes: mounted file is a... directory?",
      "problem": "I've created a secret and when I deploy an application intended to read the secret, the application complains that the secret is a directory.\nWhat am I doing wrong? The file is intended to be read as, well, a file.\n```\n`kc logs \n(error) /var/config/my-file.yaml: is a directory.\n`\n```\nThe secret is created like this.\n```\n`kubectl create secret generic my-file.yaml --from-file=my-file.yaml\n`\n```\nAnd here is the deployment.\n```\n`apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: a-name\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: a-name\n  template:\n    metadata:\n      labels:\n        name: a-name\n    spec:\n      volumes:\n        - name: my-secret-volume\n          secret:\n            secretName: my-file.yaml\n      containers:\n        - name: a-name\n          image: test/image:v1.0.0\n          volumeMounts:\n            - name: my-secret-volume\n              mountPath: /var/config/my-file.yaml\n              subPath: my-file.yaml\n              readOnly: true\n          ports:\n            - containerPort: 1234\n            - containerPort: 5678\n          imagePullPolicy: Always\n          args:\n            - run\n            - --config\n            - /var/config/my-file.yaml \n  revisionHistoryLimit: 1\n\n`\n```",
      "solution": "You are using `subPath` in the volume mount section. According to Kubernetes volume doc, when you need same volume for different purpose in the same pod then you should use `subPath`.\nBut here you are using the volume for only single use. But I'll give you both yaml file with subPath and without subPath.\nWith SubPath\n```\n`          volumeMounts:\n            - name: my-secret-volume\n              mountPath: /var/config\n              subPath: config\n              readOnly: true\n`\n```\nWithOut SubPath\n```\n`          volumeMounts:\n            - name: my-secret-volume\n              mountPath: /var/config\n              readOnly: true\n`\n```\nRest of the manifest file will be same in both cases.",
      "question_score": 3,
      "answer_score": 3,
      "created_at": "2021-03-26T15:29:39",
      "url": "https://stackoverflow.com/questions/66818858/kubernetes-mounted-file-is-a-directory"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubectl",
      "question_id": 66516548,
      "title": "How to fix error: User cannot get resource &quot;deployments&quot; in API group &quot;apps&quot; in the namespace &quot;default&quot;?",
      "problem": "I've got a problem with roles and authentication kubernetes. I created a one-node (one maser) cluster, on my baremetal server, and I made this cluster listen on different IP than default (with option \"--apiserver-advertise-address= ip address \"). But now I basically can do nothing in it, because of kubectl does not work. I can't create pods and services I need. When I created the cluster, without this IP changinh, it works. So my question  is how to fix this? It is probably an authorization problem, but I can't even create cluster role or cluster role binding because of errors like this: \" error: failed to create clusterrolebinding: clusterrolebindings.rbac.authorization.k8s.io is forbidden: User \"system:node:e4-1\" cannot create resource \"clusterrolebindings\" in API group \"rbac.authorization.k8s.io\" at the cluster scope\"... Is there any way to \"login\" as admin, or something, or is there a way to change something in configs files to fix this?",
      "solution": "Based on the flag you mention I assume you are using `kubeadm` to create your cluster. Most probable cause is that you are using the wrong `.conf` file. My suspicions are that you are using the `kubelet.conf` instead of `admin.conf`.\nBelow you can find an example of the `kubeadm init` output. It contains steps that you need to follow to start using `kubectl`:\n`Your Kubernetes control-plane has initialized successfully!\n\nTo start using your cluster, you need to run the following as a regular user:\n\n  mkdir -p $HOME/.kube\n  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\n  sudo chown $(id -u):$(id -g) $HOME/.kube/config\n\nYou should now deploy a Pod network to the cluster.\nRun \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at:\n  /docs/concepts/cluster-administration/addons/\n\nYou can now join any number of machines by running the following on each node\nas root:\n\n  kubeadm join : --token  --discovery-token-ca-cert-hash sha256:\n`\nAs you see one of the commands is to copy `admin.conf` file into `/.kube/config` which then `kubectl` uses to manage cluster.",
      "question_score": 3,
      "answer_score": 3,
      "created_at": "2021-03-07T13:31:47",
      "url": "https://stackoverflow.com/questions/66516548/how-to-fix-error-user-cannot-get-resource-deployments-in-api-group-apps-in"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubectl",
      "question_id": 71524002,
      "title": "Cant connect to GKE cluster with kubectl. getting timeout",
      "problem": "I executed followign command\n```\n`gcloud container clusters get-credentials my-noice-cluter --region=asia-south2\n`\n```\nand that command runs successfully. I can see the relevant config with `kubectl config view`\nBut when I try to kubectl, I get timeout\nkubectl config view\n```\n`\u276f kubectl get pods -A -o wide\nUnable to connect to the server: dial tcp :443: i/o timeout\n`\n```\nIf I create a VM in gcp and use kubectl there or use gcp's cloud shell, It works but it does not work on our local laptops and PCs.\nSome network info about our cluster:-\n```\n`Private cluster     Disabled    \nNetwork     default \nSubnet  default \nVPC-native traffic routing  Enabled     \nPod address range   10.122.128.0/17     \nService address range   10.123.0.0/22   \nIntranode visibility    Enabled     \nNodeLocal DNSCache  Enabled     \nHTTP Load Balancing     Enabled     \nSubsetting for L4 Internal Load Balancers   Disabled    \nControl plane authorized networks   \noffice (192.169.1.0/24)\n    \nNetwork policy  Disabled    \nDataplane V2    Disabled\n`\n```\nI also have firewall riles to allow http/s\n```\n`\u276f gcloud compute firewall-rules list\nNAME                                       NETWORK  DIRECTION  PRIORITY  ALLOW                         DENY  DISABLED\ndefault-allow-http                         default  INGRESS    1000      tcp:80                              False\ndefault-allow-https                        default  INGRESS    1000      tcp:443                             False\n....\n`\n```",
      "solution": "If it's work from your VPC and not from outside, it's because you created a private GKE cluster. The master is only reachable through the private IP or through the autorized network.\nSpeaking about the authorized network, you have one authorizer `office (192.169.1.0/24)`. Sadly, you registered a private IP range in your office network and not the public IP used to access the internet.\nTo solve that, go to a site that provide you your public IP. Then update the authorized network for your cluster with that IP/32, and try again.",
      "question_score": 3,
      "answer_score": 2,
      "created_at": "2022-03-18T09:05:01",
      "url": "https://stackoverflow.com/questions/71524002/cant-connect-to-gke-cluster-with-kubectl-getting-timeout"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubectl",
      "question_id": 68287330,
      "title": "Deleted ~/.kube/config",
      "problem": "I accidentally deleted the config file from ~/.kube/config. Every kubectl command fails due to config missing.\nExample:\n```\n`kubectl get nodes\n`\n```\n\nThe connection to the server localhost:8080 was refused - did you\nspecify the right host or port?\n\nI have already install k3s using:\n```\n`export K3S_KUBECONFIG_MODE=\"644\"\ncurl -sfL https://get.k3s.io | sh -s - --docker\n`\n```\nand kubectl using:\n```\n`snap install kubectl --classic\n`\n```\nDoes anyone know how to fix this?",
      "solution": "The master copy is available at /etc/rancher/k3s/k3s.yaml. So, copy it back  to ~/.kube/config\n```\n`cp /etc/rancher/k3s/k3s.yaml ~/.kube/config\n`\n```\nReference: https://rancher.com/docs/k3s/latest/en/cluster-access/",
      "question_score": 3,
      "answer_score": 2,
      "created_at": "2021-07-07T15:49:41",
      "url": "https://stackoverflow.com/questions/68287330/deleted-kube-config"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubectl",
      "question_id": 75331106,
      "title": "Build Kustomize with Helm Fails to Build",
      "problem": "kustomize build --enable-helm .I have the following project structure:\n```\n`project\n  - helm-k8s\n   - values.yml\n   - Chart.yml\n   - templates\n    - base\n      - project-namespace.yml\n      - grafana\n        - grafana-service.yml\n        - grafana-deployment.yml\n        - grafana-datasource-config.yml\n      - prometheus\n        - prometheus-service.yml\n        - prometheus-deployment.yml\n        - prometheus-config.yml\n        - prometheus-roles.yml\n      - kustomization.yml\n    - prod\n      - kustomization.yml\n    - test\n      - kustomization.yml\n`\n```\nI'm trying to build my kustomization file using helm like below:\n```\n`project/helm-k8s/templates/base/$ kubectl kustomize build . --enable-helm -> dummy.yml\n`\n```\nI get an error message like this:\n```\n`project/helm-k8s/templates/base$ kubectl kustomize . --enable-helm\nerror: accumulating resources: accumulation err='accumulating resources from 'project-namespace.yml': missing metadata.name in object {{v1 Namespace} {{ } map[name:] map[]}}': must build at directory: '/home/my-user/project/helm-k8s/templates/base/project-namespace.yml': file is not directory\n`\n```\nIs it not possible for kustomize to use the values.yml which is located directly under helm-k8s folder and create the final manifest for my cluster? What am I doing wrong here?\nEDIT: Here is how my kustomization.yml looks like:\n```\n`apiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\nmetadata:\n  name: open-electrons-monitoring-kustomization\nresources:\n  # 0. Get the namespaces first\n  - project-namespace.yml\n\n  # 1. Set up monitoring services (prometheus)\n  #- monitoring/prometheus/prometheus-roles.yml\n  - prometheus/prometheus-config.yml\n  - prometheus/prometheus-roles.yml\n  - prometheus/prometheus-deployment.yml\n  - prometheus/prometheus-service.yml\n\n  # 2. Set up monitoring services (grafana)\n  - grafana/grafana-datasource-config.yml\n  - grafana/grafana-deployment.yml\n  - grafana/grafana-service.yml\n`\n```",
      "solution": "I think you may have misunderstood the use of the `--enable-helm` parameter. It does not allow kustomize to perform helm-style templating on files, so when you write:\n```\n`apiVersion: v1\nkind: Namespace\nmetadata:\n  name: {{ .Values.app.namespace }}\n  labels:\n    name: {{ .Values.app.namespace }}\n`\n```\nThat doesn't do anything useful. It just generates invalid YAML output.\n\nThe `--enable-helm` option allows you to explode Helm charts using Kustomize; see here for the documentation, but for example it allows you to process a `kustomization.yaml` file like this:\n```\n`apiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\n\nhelmCharts:\n  - name: traefik\n    repo: https://helm.traefik.io/traefik\n    includeCRDs: true\n    releaseName: example\n    version: 20.8.0\n    valuesInline:\n      deployment:\n        replicas: 3\n      logs:\n        access:\n          enabled: true\n`\n```\nRunning `kubectl kustomize --enable-helm` will cause kustomize to fetch the helm chart and run `helm template` on it, producing YAML manifests on stdout.",
      "question_score": 3,
      "answer_score": 1,
      "created_at": "2023-02-03T04:06:38",
      "url": "https://stackoverflow.com/questions/75331106/build-kustomize-with-helm-fails-to-build"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubectl",
      "question_id": 66032986,
      "title": "Error 413 when trying to install the Elastic ECK",
      "problem": "I am trying to install the Elastic Cloud on Kubernetes (ECK) Kubernetes operator with the all-in-one.yaml file, as per the tutorial: https://www.elastic.co/guide/en/cloud-on-k8s/current/k8s-install-all-in-one.html\nBut I am getting an error:\n\nError from server: error when creating \"https://download.elastic.co/downloads/eck/1.3.1/all-in-one.yaml\": the server responded with the status code 413 but did not return more information (post customresourcedefinitions.apiextensions.k8s.io)\n\nI am a bit lost as to how to proceed solving this issue...\nCommand:\n```\n`kubectl apply -f https://download.elastic.co/downloads/eck/1.3.1/all-in-one.yaml  --insecure-skip-tls-verify \n`\n```\ncomplete log:\n```\n`namespace/elastic-system unchanged\nserviceaccount/elastic-operator unchanged\nsecret/elastic-webhook-server-cert unchanged\nconfigmap/elastic-operator unchanged\ncustomresourcedefinition.apiextensions.k8s.io/apmservers.apm.k8s.elastic.co configured\ncustomresourcedefinition.apiextensions.k8s.io/beats.beat.k8s.elastic.co configured\ncustomresourcedefinition.apiextensions.k8s.io/enterprisesearches.enterprisesearch.k8s.elastic.co configured\ncustomresourcedefinition.apiextensions.k8s.io/kibanas.kibana.k8s.elastic.co configured\nclusterrole.rbac.authorization.k8s.io/elastic-operator unchanged\nclusterrole.rbac.authorization.k8s.io/elastic-operator-view unchanged\nclusterrole.rbac.authorization.k8s.io/elastic-operator-edit unchanged\nclusterrolebinding.rbac.authorization.k8s.io/elastic-operator unchanged\nservice/elastic-webhook-server unchanged\nstatefulset.apps/elastic-operator configured\nvalidatingwebhookconfiguration.admissionregistration.k8s.io/elastic-webhook.k8s.elastic.co configured\nError from server: error when creating \"https://download.elastic.co/downloads/eck/1.3.1/all-in-one.yaml\": the server responded with the status code 413 but did not return more information (post customresourcedefinitions.apiextensions.k8s.io)\n`\n```\nUPDATE 1:\nRunning the command (with windows powershell):\n```\n`curl https://download.elastic.co/downloads/eck/1.3.1/all-in-one.yaml | kubectl apply  --insecure-skip-tls-verify -f-\n`\n```\nI get:\n```\n`error: error parsing STDIN: error converting YAML to JSON: yaml: line 7: mapping values are not allowed in this context\n`\n```\nUPDATE 2:\ncurrent versions:\n```\n`Client Version: version.Info{Major:\"1\", Minor:\"19\", GitVersion:\"v1.19.3\", GitCommit:\"1e11e4a2108024935ecfcb2912226cedeafd99df\", GitTreeState:\"clean\", BuildDate:\"2020-10-14T12:50:19Z\", GoVersion:\"go1.15.2\", Compiler:\"gc\", Platform:\"windows/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"18\", GitVersion:\"v1.18.6\", GitCommit:\"dff82dc0de47299ab66c83c626e08b245ab19037\", GitTreeState:\"clean\", BuildDate:\"2020-07-15T16:51:04Z\", GoVersion:\"go1.13.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n`\n```",
      "solution": "I managed to fix the issue by setting the proxy-body-size config map value in the system nginx config map to 8m.\n```\n`proxy-body-size=8m\nNamespace=ingress-nginx\nConfig Map=nginx-configuration\n`\n```\nthank you @juan-carlos-alafita for providing the relevant links!\n413 error with Kubernetes and Nginx ingress controller\nhttps://www.digitalocean.com/community/questions/413-request-entity-too-large-nginx",
      "question_score": 3,
      "answer_score": 1,
      "created_at": "2021-02-03T18:51:45",
      "url": "https://stackoverflow.com/questions/66032986/error-413-when-trying-to-install-the-elastic-eck"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubectl",
      "question_id": 74046643,
      "title": "kubectl plugin tab auto complete not working with zsh",
      "problem": "After installing kubernetes-cli via homebrew in a mac with zsh, the kubernetes auto complete is not working completely or in full. Only the first tab works and not the second tab onwards. Example, if I type `kubectl [TAB]`, it displays the sub commands for `kubectl`, and now if I select a sub command and press [TAB], eg `kubectl get [TAB]`, it freezes for a lot of time and nothing happens there after.\nI tried the following ways\n\nInstalled kubectl-cli via home brew\nenabled zsh plugin\nadded the below code as well\n```\n`source",
      "solution": "Finally this worked after referring to https://kubernetes.io/docs/reference/kubectl/cheatsheet/#zsh\n```\n`echo '[[ $commands[kubectl] ]] && source > ~/.zshrc # add autocomplete permanently to your zsh shell\n`\n```\nThe below did not work\nhttps://kubernetes.io/docs/tasks/tools/included/optional-kubectl-configs-zsh/",
      "question_score": 2,
      "answer_score": 12,
      "created_at": "2022-10-12T20:45:02",
      "url": "https://stackoverflow.com/questions/74046643/kubectl-plugin-tab-auto-complete-not-working-with-zsh"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubectl",
      "question_id": 72492756,
      "title": "Kubernetes patch multiple resources not working",
      "problem": "I'm trying to apply the same job history limits to a number of CronJobs using a patch like the following, named `kubeJobHistoryLimit.yml`:\n`apiVersion: batch/v1beta1\nkind: CronJob\nspec:\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n`\nMy `kustomization.yml` looks like:\n`bases:\n  - ../base\nconfigMapGenerator:\n- name: inductions-config\n  env: config.properties\npatches:\n  - path: kubeJobHistoryLimit.yml\n    target:\n      kind: CronJob\npatchesStrategicMerge:\n  - job_specific_patch_1.yml\n  - job_specific_patch_2.yml\n  ...\nresources:\n  - secrets-uat.yml\n`\nAnd at some point in my CI pipeline I have:\n```\n`kubectl --kubeconfig $kubeconfig apply --force -k ./\n`\n```\nThe `kubectl` version is `1.21.9`.\nThe issue is that the job history limit values don't seem to be getting picked up. Is there something wrong w/ the configuration or the version of K8s I'm using?",
      "solution": "With kustomize 4.5.2, your patch as written doesn't apply; it fails with:\n```\n`Error: trouble configuring builtin PatchTransformer with config: `\npath: kubeJobHistoryLimit.yml\ntarget:\n  kind: CronJob\n`: unable to parse SM or JSON patch from [apiVersion: batch/v1\nkind: CronJob\nspec:\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n]\n`\n```\nThis is because it's missing `metadata.name`, which is required, even if it's ignored when patching multiple objects. If I modify the patch to look like this:\n```\n`apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: ignored\nspec:\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n`\n```\nIt seems to work.\nIf I have `base/cronjob1.yaml` that looks like:\n```\n`apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: cronjob1\nspec:\n  failedJobsHistoryLimit: 2\n  successfulJobsHistoryLimit: 5\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - command:\n            - sleep\n            - 60\n            image: docker.io/alpine:latest\n            name: example\n  schedule: 30 3 * * *\n`\n```\nThen using the above patch and a `overlay/kustomization.yaml` like this:\n```\n`apiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\nresources:\n- ../base\npatches:\n- path: kubeJobHistoryLimit.yml\n  target:\n    kind: CronJob\n`\n```\nI see the following output from `kustomize build overlay`:\n```\n`apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: cronjob2\nspec:\n  failedJobsHistoryLimit: 1\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - command:\n            - sleep\n            - 60\n            image: docker.io/alpine:latest\n            name: example\n  schedule: 30 3 * * *\n  successfulJobsHistoryLimit: 1\n`\n```\nYou can see the two attributes have been updated correctly.",
      "question_score": 2,
      "answer_score": 9,
      "created_at": "2022-06-03T18:44:05",
      "url": "https://stackoverflow.com/questions/72492756/kubernetes-patch-multiple-resources-not-working"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "kubectl",
      "question_id": 69871895,
      "title": "How to map the guids under /var/lib/kubelet/pods to the actual pods",
      "problem": "I was trying to debug some mount problems and the mount logs led me to paths under `/var/lib/kubelet/pods`, i.e\n`/var/lib/kubelet/pods/f6affad1-941d-4df1-a0b7-38e3f2ab99d5/volumes/kubernetes.io~nfs/my-pv-e0dbe341a6fe475c9029fb372e`\nHow can I map the guid of the root directory under `pods` to the actual running pod or container?\n(`f6affad1-941d-4df1-a0b7-38e3f2ab99d5` in the example above)\nI don't see any correlation to the values returned by `kubectl` or `crictl`.",
      "solution": "They're the `.metadata.uid` of the Pod; one can map them back by using your favorite mechanism for querying all pods and filtering on its `.metadata.uid`, and optionally restricting to just those pods scheduled on that Node if you have a so many Pods as to make the `-A` infeasible\n`for d in /var/lib/kubelet/pods/*; do\n  p_u=$(basename \"$d\")\n  kubectl get po -A -o json | \\\n    jq --arg pod_uuid \"$p_u\" -r '.items[] \n      | select(.metadata.uid == $pod_uuid) \n      | \"uuid \\($pod_uuid) is \\(.metadata.name)\"'\ndone\n`\nI'm sure there is a `-o jsonpath=` or `-o gotemplate=` form that removes the need for `jq` but that'd be a lot more work to type out in a textarea\nwith regard to your `crictl` question, I don't this second have access to my containerd cluster, but the docker based one labels the local containers with `io.kubernetes.pod.uid` so I would guess containerd does something similar:\n`            \"Labels\": {\n                \"annotation.io.kubernetes.container.hash\": \"e44bee94\",\n                \"annotation.io.kubernetes.container.restartCount\": \"4\",\n                \"annotation.io.kubernetes.container.terminationMessagePath\": \"/dev/termination-log\",\n                \"annotation.io.kubernetes.container.terminationMessagePolicy\": \"File\",\n                \"annotation.io.kubernetes.pod.terminationGracePeriod\": \"30\",\n                \"io.kubernetes.container.logpath\": \"/var/log/pods/kube-system_storage-provisioner_b4aa3b1c-62c1-4661-a302-4c06b305b7c0/storage-provisioner/4.log\",\n                \"io.kubernetes.container.name\": \"storage-provisioner\",\n                \"io.kubernetes.docker.type\": \"container\",\n                \"io.kubernetes.pod.name\": \"storage-provisioner\",\n                \"io.kubernetes.pod.namespace\": \"kube-system\",\n                \"io.kubernetes.pod.uid\": \"b4aa3b1c-62c1-4661-a302-4c06b305b7c0\",\n                \"io.kubernetes.sandbox.id\": \"3950ec60121fd13116230cad388a4c6c4e417c660b7da475436f9ad5c9cf6738\"\n            }\n`",
      "question_score": 2,
      "answer_score": 7,
      "created_at": "2021-11-07T12:10:34",
      "url": "https://stackoverflow.com/questions/69871895/how-to-map-the-guids-under-var-lib-kubelet-pods-to-the-actual-pods"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "minikube",
      "question_id": 70497809,
      "title": "Terraform fails to create ingress (could not find the requested resource ingresses.extensions)",
      "problem": "I'm using minikube locally.\nThe following is the `.tf` file I use to create my kubernetes cluster:\n```\n`provider \"kubernetes\" {\n  config_path = \"~/.kube/config\"\n}\n\nresource \"kubernetes_namespace\" \"tfs\" {\n  metadata {\n    name = \"tfs\" # terraform-sandbox\n  }\n}\n\nresource \"kubernetes_deployment\" \"golang_webapp\" {\n  metadata {\n    name      = \"golang-webapp\"\n    namespace = \"tfs\"\n    labels = {\n      app = \"webapp\"\n    }\n  }\n  spec {\n    replicas = 3\n    selector {\n      match_labels = {\n        app = \"webapp\"\n      }\n    }\n    template {\n      metadata {\n        labels = {\n          app = \"webapp\"\n        }\n      }\n      spec {\n        container {\n          image             = \"golang-docker-example\"\n          name              = \"golang-webapp\"\n          image_pull_policy = \"Never\" # this is set so that kuberenetes wont try to download the image but use the localy built one\n          liveness_probe {\n            http_get {\n              path = \"/\"\n              port = 8080\n            }\n            initial_delay_seconds = 15\n            period_seconds        = 15\n          }\n\n          readiness_probe {\n            http_get {\n              path = \"/\"\n              port = 8080\n            }\n            initial_delay_seconds = 3\n            period_seconds        = 3\n          }\n        }\n      }\n    }\n  }\n}\n\nresource \"kubernetes_service\" \"golang_webapp\" {\n  metadata {\n    name      = \"golang-webapp\"\n    namespace = \"tfs\"\n    labels = {\n      app = \"webapp_ingress\"\n    }\n  }\n  spec {\n    selector = {\n      app = kubernetes_deployment.golang_webapp.metadata.0.labels.app\n    }\n    port {\n      port        = 8080\n      target_port = 8080\n      protocol    = \"TCP\"\n    }\n    # type = \"ClusterIP\"\n    type = \"NodePort\"\n  }\n}\n\nresource \"kubernetes_ingress\" \"main_ingress\" {\n  metadata {\n    name      = \"main-ingress\"\n    namespace = \"tfs\"\n  }\n\n  spec {\n    rule {\n      http {\n        path {\n          backend {\n            service_name = \"golang-webapp\"\n            service_port = 8080\n          }\n          path = \"/golang-webapp\"\n        }\n      }\n    }\n  }\n}\n\n`\n```\nWhen executing `terraform apply`, I am successfully able to create all of the resources except for the ingress.\nThe error is:\n```\n`Error: Failed to create Ingress 'tfs/main-ingress' because: the server could not find the requested resource (post ingresses.extensions)\n\nwith kubernetes_ingress.main_ingress,\n   on main.tf line 86, in resource \"kubernetes_ingress\" \"main_ingress\":\n   86: resource \"kubernetes_ingress\" \"main_ingress\" {\n`\n```\nWhen I try to create an ingress service with kubectl using the same configuration as the one above (only in `.yaml` and using the `kubectl apply` command) it works, so it seems that kubectl & minikube are able to create this type of ingress, but terraform cant for some reason...\nThanks in advance for any help!\nEdit 1:\nadding the `.yaml` that I'm able to create the ingress with\n```\n`apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: example-ingress\n  namespace: tfs\n  annotations:\n    nginx.ingress.kubernetes.io/rewrite-target: /$1\nspec:\n  rules:\n    - http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: golang-webapp\n                port:\n                  number: 8080\n`\n```",
      "solution": "I think the issue can be related to the ingress classname. May be you need to explicitely provide it in your .tf:\n```\n`metadata {\n    name = \"example\"\n    annotations = {\n      \"kubernetes.io/ingress.class\" = \"nginx or your classname\"\n    }\n`\n```\nOr may be it's ingresses.extensions that does not exist in your cluster. Can you provide the .yaml that executed correctly ?",
      "question_score": 15,
      "answer_score": 2,
      "created_at": "2021-12-27T17:09:03",
      "url": "https://stackoverflow.com/questions/70497809/terraform-fails-to-create-ingress-could-not-find-the-requested-resource-ingress"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "minikube",
      "question_id": 71714919,
      "title": "Unable to access my minikube cluster from the browser (\u2757 Because you are using a Docker driver on windows, the terminal needs to be open to run it.)",
      "problem": "I am trying to access a simple minikube cluster from the browser, but I keep getting the following:\n`\u2757  Because you are using a Docker driver on windows, the terminal needs to be open to run it.`\nI've created an external service for the cluster with the port number of 30384, and I'm running minikube in a docker container.\nI'm follwing \"Hello Minikube\" example to create my deployment.\nStep1: I created the deployment:\n`kubectl create deployment hello-node --image=k8s.gcr.io/echoserver:1.4`\nStep2: I created the external service:\n`kubectl expose deployment hello-node --type=LoadBalancer --port=8080`\nStep3: I ran the service, and that;s where I stuffed up\n\"`minikube service hello-node`\nThe full return message:\n`\u2757  Executing \"docker container inspect minikube --format={{.State.Status}}\" took an unusually long time: 2.3796077s`\n`\ud83d\udca1  Restarting the docker service may improve performance.`\n`\ud83c\udfc3  Starting tunnel for service hello-node.`\n`\ud83c\udf89  Opening service default/hello-node in default browser...`\n`\u2757  Because you are using a Docker driver on windows, the terminal needs to be open to run it.`\nI tried to run the service to make it accessible from the browser, however, I wasn't able to.",
      "solution": "I got the same issue resolved it by changing minikube base driver to hyperv from docker.\n```\n`Enable-WindowsOptionalFeature -Online -FeatureName Microsoft-Hyper-V -All\n`\n```\nYour pc will restart after that you can say\n```\n`minikube config set driver hyperv\n`\n```\nThen `minikube start` will start you with that driver.\nThis worked for me.",
      "question_score": 13,
      "answer_score": 4,
      "created_at": "2022-04-02T06:38:21",
      "url": "https://stackoverflow.com/questions/71714919/unable-to-access-my-minikube-cluster-from-the-browser-because-you-are-using-a"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "minikube",
      "question_id": 72242713,
      "title": "Kubectl error upon applying agones fleet: ensure CRDs are installed first",
      "problem": "I am using minikube (docker driver) with kubectl to test an agones fleet deployment. Upon running `kubectl apply -f lobby-fleet.yml` (and when I try to apply any other agones yaml file)  I receive the following error:\n```\n`error: resource mapping not found for name: \"lobby\" namespace: \"\" from \"lobby-fleet.yml\": no matches for kind \"Fleet\" in version \"agones.dev/v1\"\nensure CRDs are installed first\n`\n```\nlobby-fleet.yml:\n```\n`apiVersion: \"agones.dev/v1\"\nkind: Fleet\nmetadata:\n  name: lobby\nspec:\n  replicas: 2\n  scheduling: Packed\n  template:\n    metadata:\n      labels:\n        mode: lobby\n    spec:\n      ports:\n      - name: default\n        portPolicy: Dynamic\n        containerPort: 7600\n        container: lobby\n      template:\n        spec:\n          containers:\n          - name: lobby\n            image: gcr.io/agones-images/simple-game-server:0.12 # Modify to correct image\n`\n```\nI am running this on WSL2, but receive the same error when using the windows installation of kubectl (through choco). I have minikube installed and running for ubuntu in WSL2 using docker.\nI am still new to using k8s, so apologies if the answer to this question is clear, I just couldn't find it elsewhere.\nThanks in advance!",
      "solution": "In order to create a resource of kind `Fleet`, you have to apply the Custom Resource Definition (CRD) that defines what is a `Fleet` first.\nI've looked into the YAML installation instructions of agones, and the manifest contains the CRDs. you can find it by searching `kind: CustomResourceDefinition`.\nI recommend you to first try to install according to the instructions in the docs.",
      "question_score": 13,
      "answer_score": 7,
      "created_at": "2022-05-14T19:56:49",
      "url": "https://stackoverflow.com/questions/72242713/kubectl-error-upon-applying-agones-fleet-ensure-crds-are-installed-first"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "minikube",
      "question_id": 66088041,
      "title": "How to expose Minikube cluster to internet",
      "problem": "I know minikube should be used for local only, but i'd like to create a test environment for my applications.\nIn order to do that, I wish to expose my applications running inside the minikube cluster to external access (from any device on public internet - like a 4G smartphone).\nnote : I run minikube with `--driver=docker`\nkubectl get services\n```\n`NAME      TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE\nweb8080   NodePort   10.99.39.162           8080:31613/TCP   3d1h\n`\n```\nminikube ip\n```\n`192.168.49.2\n`\n```\nOne way to do it is as follows :\n```\n`firewall-cmd --add-port=8081/tcp\nkubectl port-forward --address 0.0.0.0 services/web8080 8081:8080\n`\n```\nthen I can access it using :\n```\n`curl localhost:8081      (directly from the machine running the cluster inside a VM)\ncurl 192.168.x.xx:8081   (from my Mac in same network - this is the private ip of the machine running the cluster inside a VM)\ncurl 84.xxx.xxx.xxx:8081 (from a phone connected in 4G - this is the public ip exposed by my router)\n`\n```\nI don't want to use this solution because `kubectl port-forward` is weak and need to be run every time the port-forwarding is no longer active.\nHow can I achieve this ?\n(EDITED) - USING LOADBALANCER\nwhen using `LoadBalancer` type and `minikube tunnel`, I can expose the service only inside the machine running the cluster.\nkubectl get services\n```\n`NAME         TYPE           CLUSTER-IP      EXTERNAL-IP     PORT(S)          AGE\nmy-service   LoadBalancer   10.111.61.218   10.111.61.218   8080:31831/TCP   3d3h\n`\n```\n`curl 10.111.61.218:8080` (inside the machine running the cluster) is working\nbut `curl 192.168.x.xx:8080` (from my Mac on same LAN) is not working\nThanks",
      "solution": "`Minikube` as a development tool for a single node Kubernetes cluster provides inherent isolation layer between Kubernetes and the external devices (being specific the inbound traffic to your cluster from `LAN`/`WAN`).\nDifferent --drivers are allowing for flexibility when it comes to the place where your Kubernetes cluster will be spawned and how it will behave network wise.\n\nA side note (workaround)!\nAs your `minikube` already resides in a `VM` and uses `--driver=docker` you could try to use `--driver=none` (you will be able to `curl VM_IP:NodePort` from the `LAN`). It will spawn your Kubernetes cluster directly on the `VM`.\nConsider checking it's documentation as there are some certain limitations/disadvantages:\n\nMinikube.sigs.k8s.io: Docs: Drivers: None\n\nAs this setup is already basing on the `VM` (with unknown hypervisor) and the cluster is intended to be exposed outside of your LAN, I suggest you going with the production-ready setup. This will inherently eliminate the connectivity issues you are facing. Kubernetes cluster will be provisioned directly on a `VM` and not in the `Docker` container.\nExplaining the `--driver=docker` used: It will spawn a container on a host system with Kubernetes inside of it. Inside of this container, `Docker` will be used once again to spawn the necessary `Pods` to run the Kubernetes cluster.\nAs for the tools to provision your Kubernetes cluster you will need to chose the option that suits your needs the most. Some of them are the following:\n\nKubeadm\nKubespray\nMicroK8S\n\nAfter you created your Kubernetes cluster on a `VM` you could forward the traffic from your router directly to your `VM`.\n\nAdditional resources that you might find useful:\n\nStackoverflow.com: Questions Expose Kubernetes cluster to the Internet (Virtualbox with minikube)",
      "question_score": 12,
      "answer_score": 4,
      "created_at": "2021-02-07T13:41:23",
      "url": "https://stackoverflow.com/questions/66088041/how-to-expose-minikube-cluster-to-internet"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "minikube",
      "question_id": 71580786,
      "title": "getting error unable to recognize &quot;ingress.yaml&quot;: no matches for kind &quot;Ingress&quot; in version &quot;extensions/v1beta1",
      "problem": "I am taking a helm chart class and the 1st lab creates a pod, service and ingress.  I am relatively new to k8s and I am running on minikube.  The pod and service get created without issue; however the ingress.yaml file gives the following error:\nunable to recognize \"ingress.yaml\": no matches for kind \"Ingress\" in version \"extensions/v1beta1\nI am guessing something is obsolete in the ingress.yaml file but have no idea how to fix it.  here's the class repo:\n```\n`https://github.com/phcollignon/helm3\n`\n```\nhere's the pod frontend.yaml :\n```\n`apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: frontend \n  template:\n    metadata:\n      labels:\n        app: frontend\n    spec:\n      containers:\n      - image: phico/frontend:1.0\n        imagePullPolicy: Always\n        name: frontend\n        ports:\n        - name: frontend\n          containerPort: 4200\n`\n```\nhere's the frontend_service.yaml :\n```\n`apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    name: frontend\n  name: frontend\nspec:\n  ports:\n    - protocol: \"TCP\"\n      port: 80\n      targetPort: 4200\n  selector:\n    app: frontend\n`\n```\nHere's the problem file ingress.yaml :\n```\n`apiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: guestbook-ingress\nspec:\n  rules:\n  - host: frontend.minikube.local\n    http:\n      paths:\n      - path: /\n        backend:\n          serviceName: frontend\n          servicePort: 80\n  - host: backend.minikube.local\n    http:\n      paths:\n      - path: /\n        backend:\n          serviceName: backend\n          servicePort: 80%        \n`\n```\nHere's minikube version (kubectrl version) :\n```\n`Client Version: version.Info{Major:\"1\", Minor:\"22\", GitVersion:\"v1.22.5\", GitCommit:\"5c99e2ac2ff9a3c549d9ca665e7bc05a3e18f07e\", GitTreeState:\"clean\", BuildDate:\"2021-12-16T08:38:33Z\", GoVersion:\"go1.16.12\", Compiler:\"gc\", Platform:\"darwin/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"23\", GitVersion:\"v1.23.1\", GitCommit:\"86ec240af8cbd1b60bcc4c03c20da9b98005b92e\", GitTreeState:\"clean\", BuildDate:\"2021-12-16T11:34:54Z\", GoVersion:\"go1.17.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n`\n```\nAny help is very much appreciated.\nI changed the ingress.yaml file to use\n`apiVersion: networking.k8s.io/v1:`\n```\n`apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: guestbook-ingress\nspec:\n  rules:\n  - host: frontend.minikube.local\n    http:\n      paths:\n      - path: /\n        backend:\n          service:\n            name: frontend\n            port:\n              number: 80\n  - host: backend.minikube.local\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: backend\n            port:\n              number: 80\n`\n```\nnow I am getting an error:\nerror: error parsing ingress.yaml: error converting YAML to JSON: yaml: line 17: mapping values are not allowed in this context\nline 17 is the second \"paths:\" line.\nAgain, any help appreciated.",
      "solution": "Ingress spec `apiVersion: extensions/v1beta1` has deprecated. You can update it to `apiVersion: networking.k8s.io/v1`\nSecond question:\n```\n`kind: Ingress\nmetadata:\n  name: guestbook-ingress\nspec:\n  rules:\n  - host: frontend.minikube.local\n    http:\n      paths:\n      - path: /\n        pathType: Prefix  \n        backend:\n          service:\n            name: frontend\n            port:\n              number: 80\n  - host: backend.minikube.local\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: backend\n            port:\n              number: 80\n`\n```",
      "question_score": 10,
      "answer_score": 16,
      "created_at": "2022-03-23T02:24:21",
      "url": "https://stackoverflow.com/questions/71580786/getting-error-unable-to-recognize-ingress-yaml-no-matches-for-kind-ingress"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "minikube",
      "question_id": 71654675,
      "title": "Homebrew installs wrong minikube (amd64) instead of &#39;arm64&#39; on m1 Mac",
      "problem": "Homebrew (`brew install minikube`) is installing amd64 'minikube' on Macbook Air with M1. while running any minikube command, it prints following message:\n```\n`\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                                                          \u2502\n\u2502    You are trying to run the amd64 binary on an M1 system.                                               \u2502\n\u2502    Please consider running the darwin/arm64 binary instead.                                              \u2502\n\u2502    Download at https://github.com/kubernetes/minikube/releases/download/v1.25.2/minikube-darwin-arm64    \u2502\n\u2502                                                                                                          \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\nminikube version: v1.25.2\n`\n```\nI think when I installed Homebrew, it wasn't ported to M1, but later I upgraded it, uninstalled and re-installed it, but still same problem.\nAny Idea how can I install correct version with Homebrew?\nI know, I can install specific arm64 version with `sudo install`, but I prefer to manage packages with Homebrew.",
      "solution": "Found the issue and posting solution here for someone who might get the same issue. Rare, but could happen... :-)\nThe problem was that the Terminal application was running under Rosetta. The Homebrew installation script runs command `/usr/bin/uname -m` to check system architecture. Running this command in a Terminal which runs in Rosetta, returns amd64 architecture, hence homebrew assumes it to be Intel Mac.\nTo remove Terminal from running under Rosetta,\nGo To: Finder -> Applications -> Utilities -> Terminal.\nRight click on Terminal and select Get Info.\nUncheck checkbox: 'Open in Rosetta'. Quit Terminal Application.\nRestart Terminal Application and test with command `/usr/bin/uname -m`. It should print `arm64`\nNote: before doing all this, remove Homebrew and all its files/folders.",
      "question_score": 9,
      "answer_score": 8,
      "created_at": "2022-03-29T01:12:03",
      "url": "https://stackoverflow.com/questions/71654675/homebrew-installs-wrong-minikube-amd64-instead-of-arm64-on-m1-mac"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "minikube",
      "question_id": 68855754,
      "title": "How can I remove an image from minikube?",
      "problem": "```\n`$ minikube image ls\n...\ndocker.io/library/crasher:latest\n...\n\n$ minikube image rm crasher crasher:latest docker.io/library/crasher:latest\n\n$ minikube image ls\n...\ndocker.io/library/crasher:latest\n...\n`\n```\nIt looks like minikube rm doesn't remove the image from minikubes internal cache. I would like to be able to remove one of these images so that I can be sure I when I `minikube image load` that it picks up the new image.",
      "solution": "I figured it out, the problem was that I still had services running that were using the image.\nYou either can't delete an image in use, or minikube is adding the in use image back into the list faster than I can run commands.\nSo if you want to do a local hotswap of your image on minikube, you need to:\n```\n`1. kubectl delete\n2. minikube image rm\n3. minikube image load\n4. kubectl apply\n`\n```",
      "question_score": 8,
      "answer_score": 16,
      "created_at": "2021-08-20T02:27:29",
      "url": "https://stackoverflow.com/questions/68855754/how-can-i-remove-an-image-from-minikube"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "minikube",
      "question_id": 71384252,
      "title": "Cannot access deployed services when Minikube cluster is installed in WSL2",
      "problem": "I have a Minikube cluster setup in WSL 2 of Windows 10 pro, where the docker-for-windows is used with WSL2 integration. Minikube was started with default docker driver.\n```\n`$ minikube version\nminikube version: v1.25.2\ncommit: 362d5fdc0a3dbee389b3d3f1034e8023e72bd3a7\n`\n```\nIf I follow the getting started guide,after creating the `hellow-minikube` service, I should be able to connect to the service either via `:nodeport` or via `minikube service` command.\nBut the first method didn't worked. Because it was impossible to even ping the minikube ip from WSL 2:\n(This works in Minikube setup on a pure Ubuntu installation. The problem is in WSL2 - Windows subsystem for linux).\n```\n`$ minikube ip\n192.168.49.2\n\n$ ping 192.168.49.2\nPING 192.168.49.2 (192.168.49.2) 56(84) bytes of data.\n^C\n--- 192.168.49.2 ping statistics ---\n293 packets transmitted, 0 received, 100% packet loss, time 303708ms\n`\n```\nThe second method `minikube service hello-minikube` also didn't worked because it was again giving the access url with minikube IP.\n```\n`$ minikube service hello-minikube\n\n\ud83c\udfc3  Starting tunnel for service hello-minikube.\n\ud83c\udf89  Opening service default/hello-minikube in default browser...\n\ud83d\udc49  **http://192.168.49.2:30080**\n\u2757  Because you are using a Docker driver on linux, the terminal needs to be open to run it.\n`\n```\nBut this was actually working in previous Minikube versions, as it was actually exposing a host port to the service, and we could connect to the host port to access the service. It needed a manual intervention as the hostport access was available only until the `minikube service` command keeps running.\n\nIs there any way that I can pre-configure a port to access the service (nodePort), and can access the service even if it is deployed in Minikube in WSL2?\nNote:\nI tried using other drivers from WSL like `--driver=none`. But that setup would be much more complicated because it has `systemd`, `conntrack` and other packages as dependencies, which WSL2 doesn't have currently.\nAlso tried to setup a Virtualbox+vagrant Ubuntu box in Windows 10 and installed docker and started minikube with docker driver there. Everything works inside that VM. But cannot access the services from windows host as minikube ip is a host-only ip address available inside that VM only.",
      "solution": "Minikube in WSL2 with docker driver, creates a docker container with name `minikube` when `minikube start` command is executed. That container has some port mappings that helps kubectl and clients to connect to the server.\nNotice that `kubectl cluster-info` connects to one of those ports as server.\n(Normally, the control plane would be running at port 8443, here it is a random high port, which is a mapped one).\n```\n`$ kubectl cluster-info\n\nKubernetes control plane is running at https://127.0.0.1:55757\nCoreDNS is running at https://127.0.0.1:55757/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\n\n$ docker ps\n\nCONTAINER ID   IMAGE                                 COMMAND                  CREATED         STATUS         PORTS\n                                                                                                                    NAMES\n9cc01654bd2f   gcr.io/k8s-minikube/kicbase:v0.0.30   \"/usr/local/bin/entr\u2026\"   7 minutes ago   Up 7 minutes   127.0.0.1:55758->22/tcp, 127.0.0.1:55759->2376/tcp, 127.0.0.1:55756->5000/tcp, 127.0.0.1:55757->8443/tcp, 127.0.0.1:55760->32443/tcp   minikube\n`\n```\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\nIf you can provide a fixed nodePort to your app's service, then you can add a custom port mapping on minikube from that nodePort (of minikube host/VM) to a hostPort (of WSL2). And then you can acccess the service with `localhost:hostPort`.\nFor example,\nYou want to create a service with nodePort `30080`.\nIn that case, make sure you start the minikube with a custom port mapping that includes this node port:\n```\n`$ minikube start --ports=127.0.0.1:30080:30080\n`\n```\n\nNow if you deploy the service with `nodePort=30080` you will be able to access it via http://localhost:30080/.\nThere were issues like this in Minikube installation on MacOS.\nHere are some details about the workaround: https://github.com/kubernetes/minikube/issues/11193",
      "question_score": 7,
      "answer_score": 14,
      "created_at": "2022-03-07T17:30:51",
      "url": "https://stackoverflow.com/questions/71384252/cannot-access-deployed-services-when-minikube-cluster-is-installed-in-wsl2"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "minikube",
      "question_id": 67417306,
      "title": "The Ingress Controller is not created when running the &quot;minikube addons enable ingress&quot;",
      "problem": "I have minikube installed on Windows10, and I'm trying to work with Ingress Controller\nI'm doing:\n\n$ minikube addons enable ingress\n\n```\n`* After the addon is enabled, please run \"minikube tunnel\" and your ingress resources would be available at \"127.0.0.1\"\n  - Using image docker.io/jettech/kube-webhook-certgen:v1.5.1\n  - Using image k8s.gcr.io/ingress-nginx/controller:v0.44.0\n  - Using image docker.io/jettech/kube-webhook-certgen:v1.5.1\n* Verifying ingress addon...\n* The 'ingress' addon is enabled\n`\n```\n\nminikube addons list\n\n```\n` minikube addons list\n|-----------------------------|----------|--------------|\n|         ADDON NAME          | PROFILE  |    STATUS    |\n|-----------------------------|----------|--------------|\n| ambassador                  | minikube | disabled     |\n| auto-pause                  | minikube | disabled     |\n| csi-hostpath-driver         | minikube | disabled     |\n| dashboard                   | minikube | disabled     |\n| default-storageclass        | minikube | enabled \u2705   |\n| efk                         | minikube | disabled     |\n| freshpod                    | minikube | disabled     |\n| gcp-auth                    | minikube | disabled     |\n| gvisor                      | minikube | disabled     |\n| helm-tiller                 | minikube | disabled     |\n| ingress                     | minikube | enabled \u2705   |\n| ingress-dns                 | minikube | disabled     |\n| istio                       | minikube | disabled     |\n| istio-provisioner           | minikube | disabled     |\n| kubevirt                    | minikube | disabled     |\n| logviewer                   | minikube | disabled     |\n| metallb                     | minikube | disabled     |\n| metrics-server              | minikube | disabled     |\n| nvidia-driver-installer     | minikube | disabled     |\n| nvidia-gpu-device-plugin    | minikube | disabled     |\n| olm                         | minikube | disabled     |\n| pod-security-policy         | minikube | disabled     |\n| registry                    | minikube | disabled     |\n| registry-aliases            | minikube | disabled     |\n| registry-creds              | minikube | disabled     |\n| storage-provisioner         | minikube | enabled \u2705   |\n| storage-provisioner-gluster | minikube | disabled     |\n| volumesnapshots             | minikube | disabled     |\n|-----------------------------|----------|--------------|\n`\n```\nNote:\nI ran `minikube tunnel` after the addon was enabled\nBut can't see the nginx controller anywhere:\n\n$ kubectl get pods -n kube-system\n\n```\n`NAME                               READY   STATUS    RESTARTS   AGE\ncoredns-74ff55c5b-8gkwj            1/1     Running   0          2m35s\netcd-minikube                      1/1     Running   0          2m48s\nkube-apiserver-minikube            1/1     Running   0          2m48s\nkube-controller-manager-minikube   1/1     Running   0          2m48s\nkube-proxy-jq4wm                   1/1     Running   0          2m35s\nkube-scheduler-minikube            1/1     Running   0          2m48s\nstorage-provisioner                1/1     Running   2          2m47s\n`\n```\n\n$ kubectl get pods\n\n```\n`No resources found in default namespace.\n`\n```",
      "solution": "As already discussed in the comments the Ingress Controller will be created in the `ingress-nginx` namespace instead of the `kube-system` namespace. Other than that the rest of the tutorial should work as expected.",
      "question_score": 7,
      "answer_score": 11,
      "created_at": "2021-05-06T13:35:12",
      "url": "https://stackoverflow.com/questions/67417306/the-ingress-controller-is-not-created-when-running-the-minikube-addons-enable-i"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "minikube",
      "question_id": 70323248,
      "title": "Minikube: unable to pull image from DockerHub (public repo)",
      "problem": "I'm sporting a fresh Minikube install on an ArchLinux box, using Docker as the Minikube driver.\nI started the minikube \"cluster\" using the `minikube start` command. `docker container ls` tells us it's up and running:\n```\n`CONTAINER ID   IMAGE                                 COMMAND                  CREATED          STATUS          PORTS                                                                                                                                  NAMES\nd86070af0c21   gcr.io/k8s-minikube/kicbase:v0.0.28   \"/usr/local/bin/entr\u2026\"   50 minutes ago   Up 50 minutes   127.0.0.1:49162->22/tcp, 127.0.0.1:49161->2376/tcp, 127.0.0.1:49160->5000/tcp, 127.0.0.1:49159->8443/tcp, 127.0.0.1:49158->32443/tcp   minikube\n`\n```\nI'm trying to run a simple nginx pod, using this command: `kubectl run my-nginx --image nginx`\nSince I'm pulling a public image from a public repo, I would expect I don't need any authentication. But the `describe pod` sub-command shows:\n```\n`Events:\n  Type     Reason     Age                From               Message\n  ----     ------     ----               ----               -------\n  Normal   Scheduled  47s                default-scheduler  Successfully assigned default/my-nginx to minikube\n  Normal   BackOff    31s                kubelet            Back-off pulling image \"nginx\"\n  Warning  Failed     31s                kubelet            Error: ImagePullBackOff\n  Normal   Pulling    19s (x2 over 46s)  kubelet            Pulling image \"nginx\"\n  Warning  Failed     4s (x2 over 31s)   kubelet            Failed to pull image \"nginx\": rpc error: code = Unknown desc = Error response from daemon: Get \"https://registry-1.docker.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)\n  Warning  Failed     4s (x2 over 31s)   kubelet            Error: ErrImagePull\n`\n```\nWhen I try to curl the URL found in the error message from inside the minikube container, it shows that authentication is needed:\n```\n`patres@arch:~$ minikube ssh                                                                        \n\ndocker@minikube:~$ curl https://registry-1.docker.io/v2/\n{\"errors\":[{\"code\":\"UNAUTHORIZED\",\"message\":\"authentication required\",\"detail\":null}]}\n`\n```\nWhen I try to pull that very image from host using `docker pull nginx` command, the image gets pulled, no auth required.\nI also tried to create a kubernetes secret this way, then launching the pod using YAML with that secret, but it was to no avail.\n```\n`kubectl create secret docker-registry regcred --docker-server=https://registry-1.docker.io/v2/ --docker-username=myusername --docker-password=mypass --docker-email=my@email.com\n`\n```\nFinally, it seems like the issue might not be unique to DockerHub, since if I follow\nthe official minikubes documentation and launch the default `hello-minikube` deployment:\n```\n`kubectl create deployment hello-minikube --image=k8s.gcr.io/echoserver:1.4\n`\n```\nI get the same `ImagePullBackOff` error:\n```\n`$ kubectl get pod hello-minikube-6ddfcc9757-zdzz2                                           \nNAME                              READY   STATUS             RESTARTS   AGE\nhello-minikube-6ddfcc9757-zdzz2   0/1     ImagePullBackOff   0          6m11s\n`\n```",
      "solution": "The problem got resolved by one of these actions (not sure by which exactly):\n\nterminating my VPN connection\ndeleting the minikube container and image\nrebooting my computer\nstarting anew with `minikube start`",
      "question_score": 7,
      "answer_score": 1,
      "created_at": "2021-12-12T12:48:51",
      "url": "https://stackoverflow.com/questions/70323248/minikube-unable-to-pull-image-from-dockerhub-public-repo"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "minikube",
      "question_id": 70961901,
      "title": "ingress with minikube working differently on Mac vs Ubuntu. When to set etc/hosts to 127.0.0.1 vs &quot;minikube ip&quot;?",
      "problem": "I have a simple ingress file that does work on multiple environments, but to access it, it behaves differently depending on if I'm running my minikube cluster on my Mac or my Ubuntu machines.\nSpecifically, for my mac, I have to add the the entry:\n`127.0.0.1   my.kube` to /etc/hosts, and also run `minikube tunnel`\nfor me to be able to access my application in the browser at `http://my.kube`.\nBut, for my Ubuntu system, I have to get the `minikube ip` and place the resulting ip in `/etc/hosts` like `192.168.49.2   my.kube` for example. And then I do NOT need to run `minikube tunnel`.\nWhile I can see the pros and cons of each:\n\nusing a `127.0.0.1` and a tunnel removes a dependency on the minikube ip, which could change if the cluster is deleted and recreated (although some work has gone on to make it persistent).\nusing the minikube ip and not needing a tunnel is nice too.\n\nBut, my question is why are things behaving differently at all?\nI have enabled the ingress addon on both environments with `minikube addons enable ingress`.\nWhen I check the hostname of the loadBalancer created by the ingress with `kubectl get ingress my-ingress -o yaml` I get the same result for both. I expected to see an IP in the case where the minikube IP is used (Ubuntu).\n```\n`....\nstatus:\n  loadBalancer:\n    ingress:\n    - hostname: localhost\n`\n```\nAgain, all services are up and running fine, it's just a question of what goes in `/etc/hosts` and whether `minikube tunnel` is or is not running.\nAlso, to be clear, my Ubuntu system will not work with `127.0.0.1` and `minikube tunnel`. I can't find a common approach for the two environments.\nFor reference, here is my ingress file:\n```\n`apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: my-ingress\nspec:\n  rules:\n    - host: my.kube\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: my-ui\n                port:\n                  number: 3000\n          - path: /api\n            pathType: Prefix\n            backend:\n              service:\n                name: my-api\n                port:\n                  number: 8080\n`\n```\nSome names have been changed to protect the innocent.\nJust for completeness, the services are just simply:\n```\n`apiVersion: v1\nkind: Service\nmetadata:\n  name: my-ui\nspec:\n  selector:\n    app: my-ui\n  ports:\n    - protocol: TCP\n      port: 3000\n      targetPort: 3000\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: my-api\nspec:\n  selector:\n    app: my-api\n  ports:\n    - protocol: TCP\n      port: 8080\n      targetPort: 8080\n`\n```",
      "solution": "Minikube supports ingress differently on the Mac and Linux.\nOn Linux the ingress is fully supported and therefore does not need the use of `minikube tunnel`.\nOn Mac there is an open issue due to a network issue. The documentation states that the minikube ingress addon is not supported, but I argue that that is highly misleading if not incorrect. It's just supported differently (and not as well).\nOn both Mac and Linux `minikube addons enable ingress` is required. Enabling the ingress addon on Mac shows that the ingress will be available on 127.0.0.1 as shown in the screenshot, whereas Linux will make it available by the `minikube ip`. Then, when we start `minikube tunnel` on the Mac it will connect to the ingress like any other exposed service.\n\nThanks to Gabriel for pointing me in the right direction.",
      "question_score": 6,
      "answer_score": 8,
      "created_at": "2022-02-02T20:45:08",
      "url": "https://stackoverflow.com/questions/70961901/ingress-with-minikube-working-differently-on-mac-vs-ubuntu-when-to-set-etc-host"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "minikube",
      "question_id": 72038418,
      "title": "Cannot Connect to $(minikube ip):$NODE_PORT on mac M1",
      "problem": "I am learning kubernetes on minikube. I studied the kubernetes official documentation and followed their interactive tutorial in a sandboxed environment. Everything worked fine in the sandbox but I tried the same thing on my system it failed.\nMy Setup :\n\nI am using macOS Big Sur version 11.6.2(20G314) on Apple M1.\nI have used docker instead of virtual machine environment for minikube.\n\nSteps to reproduce :\nFirst I created a deployment, then I created a `NodePort` type service to expose it to external traffic.\nThe pod is running fine and no issues are seen in the service description.\nTo test if the app is exposed outside of the cluster I used `curl` to send a request to the node :\n`curl $(minikube ip):$NODE_PORT\n`\nBut I get no response from the server :\n\ncurl: (7) Failed to connect to 192.168.XX.X port 32048: Operation timed out.\n\nI have copied everything that was done in the tutorial. Same deployment name, same image, same service-name, literally EVERYTHING.\nI tried `LoadBalancer` type, but found out that minikube doesn't support it. To access the `LoadBalancer` deployment, I used the command `minikube tunnel` but this did not help.\nWhat could be the possible reasons? Is it my system?",
      "solution": "I also had this problem on my m1 mac. I was able to access the service by using this command :\n`kubectl port-forward svc/kubernetes-bootcamp 8080:8080\n`\nYou can see this article and this answer for more info and ways to go about it.",
      "question_score": 6,
      "answer_score": 6,
      "created_at": "2022-04-28T07:27:33",
      "url": "https://stackoverflow.com/questions/72038418/cannot-connect-to-minikube-ipnode-port-on-mac-m1"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "minikube",
      "question_id": 70728223,
      "title": "Can not pull any image in minikube",
      "problem": "Im on macOS and im using `minikube` with `hyperkit` driver: `minikube start --driver=hyperkit`\nand everything seems ok...\nwith `minikube status`:\n```\n`minikube\ntype: Control Plane\nhost: Running\nkubelet: Running\napiserver: Running\nkubeconfig: Configured\n`\n```\nwith `minikube version`:\n```\n`minikube version: v1.24.0\n`\n```\nwith `kubectl version`:\n```\n`Client Version: version.Info{Major:\"1\", Minor:\"22\", GitVersion:\"v1.22.4\", GitCommit:\"b695d79d4f967c403a96986f1750a35eb75e75f1\", GitTreeState:\"clean\", BuildDate:\"2021-11-17T15:48:33Z\", GoVersion:\"go1.16.10\", Compiler:\"gc\", Platform:\"darwin/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"22\", GitVersion:\"v1.22.3\", GitCommit:\"c92036820499fedefec0f847e2054d824aea6cd1\", GitTreeState:\"clean\", BuildDate:\"2021-10-27T18:35:25Z\", GoVersion:\"go1.16.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n`\n```\nand with `kubectl get no`:\n```\n`NAME       STATUS   ROLES                  AGE   VERSION\nminikube   Ready    control-plane,master   13m   v1.22.3\n`\n```\nmy problem is when i deploy anything, it wont pull any image...\nfor instance:\n```\n`kubectl create deployment hello-minikube --image=k8s.gcr.io/echoserver:1.4\n`\n```\nthen `kubectl get pods`:\n```\n`NAME                              READY   STATUS             RESTARTS   AGE\nhello-minikube-6ddfcc9757-nfc64   0/1     ImagePullBackOff   0          13m\n`\n```\nthen i tried to figure out what is the problem?\n```\n`k describe pod/hello-minikube-6ddfcc9757-nfc64\n`\n```\nhere is the result:\n```\n`Name:         hello-minikube-6ddfcc9757-nfc64\nNamespace:    default\nPriority:     0\nNode:         minikube/192.168.64.8\nStart Time:   Sun, 16 Jan 2022 10:49:27 +0330\nLabels:       app=hello-minikube\n              pod-template-hash=6ddfcc9757\nAnnotations:  \nStatus:       Pending\nIP:           172.17.0.5\nIPs:\n  IP:           172.17.0.5\nControlled By:  ReplicaSet/hello-minikube-6ddfcc9757\nContainers:\n  echoserver:\n    Container ID:   \n    Image:          k8s.gcr.io/echoserver:1.4\n    Image ID:       \n    Port:           \n    Host Port:      \n    State:          Waiting\n      Reason:       ImagePullBackOff\n    Ready:          False\n    Restart Count:  0\n    Environment:    \n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-k5qql (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             False \n  ContainersReady   False \n  PodScheduled      True \nVolumes:\n  kube-api-access-k5qql:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       \n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              \nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type     Reason     Age                   From               Message\n  ----     ------     ----                  ----               -------\n  Normal   Scheduled  18m                   default-scheduler  Successfully assigned default/hello-minikube-6ddfcc9757-nfc64 to minikube\n  Normal   Pulling    16m (x4 over 18m)     kubelet            Pulling image \"k8s.gcr.io/echoserver:1.4\"\n  Warning  Failed     16m (x4 over 18m)     kubelet            Failed to pull image \"k8s.gcr.io/echoserver:1.4\": rpc error: code = Unknown desc = Error response from daemon: Get \"https://k8s.gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)\n  Warning  Failed     16m (x4 over 18m)     kubelet            Error: ErrImagePull\n  Warning  Failed     15m (x6 over 18m)     kubelet            Error: ImagePullBackOff\n  Normal   BackOff    3m34s (x59 over 18m)  kubelet            Back-off pulling image \"k8s.gcr.io/echoserver:1.4\"\n`\n```\nthen tried to get some logs!:\n`k logs pod/hello-minikube-6ddfcc9757-nfc64` and `k logs deploy/hello-minikube`\nboth returns the same result:\n```\n`Error from server (BadRequest): container \"echoserver\" in pod \"hello-minikube-6ddfcc9757-nfc64\" is waiting to start: trying and failing to pull image\n`\n```\nthis deployment was an example from minikube documentation\nbut i have no idea why it doesnt pull any image...",
      "solution": "I had exactly same problem.\nI found out that my internet connection was slow,\nthe timout to pull an image is `120` seconds, so  kubectl could not pull the  image in under `120` seconds.\nfirst use minikube to pull the image you need\nfor example:\n```\n`minikube image load k8s.gcr.io/echoserver:1.4\n`\n```\nand then everything will work because now kubectl will use the image that is stored locally.",
      "question_score": 6,
      "answer_score": 8,
      "created_at": "2022-01-16T08:49:08",
      "url": "https://stackoverflow.com/questions/70728223/can-not-pull-any-image-in-minikube"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "minikube",
      "question_id": 70895376,
      "title": "Minikube always reset to initial state when restart it",
      "problem": "I faced this problem since yesterday, no problems before.\nMy environment is\n\nWindows 11\nDocker Desktop 4.4.4\nminikube 1.25.1\nkubernetes-cli 1.23.3\n\nReproduce\n1. Start minikube and create cluster\n```\n`minikube start\n`\n```\n2. Check pods\n```\n`kubectl get po -A\n\nNAMESPACE     NAME                               READY   STATUS    RESTARTS   AGE\nkube-system   coredns-64897985d-z7rpf            1/1     Running   0          22s\nkube-system   etcd-minikube                      1/1     Running   1          34s\nkube-system   kube-apiserver-minikube            1/1     Running   1          34s\nkube-system   kube-controller-manager-minikube   1/1     Running   1          33s\nkube-system   kube-proxy-zdr9n                   1/1     Running   0          22s\nkube-system   kube-scheduler-minikube            1/1     Running   1          34s\nkube-system   storage-provisioner                1/1     Running   0          29s\n`\n```\n3. Add new pod (in this case, use istio)\n```\n`istioctl manifest apply -y\n`\n```\n4. Check pods\n```\n`kubectl get po -A\n\nNAMESPACE      NAME                                  READY   STATUS    RESTARTS      AGE\nistio-system   istio-ingressgateway-c6d9f449-nhbvg   1/1     Running   0             13s\nistio-system   istiod-5ffcccb477-5hzgs               1/1     Running   0             19s\nkube-system    coredns-64897985d-nxhxm               1/1     Running   0             67s\nkube-system    etcd-minikube                         1/1     Running   2             79s\nkube-system    kube-apiserver-minikube               1/1     Running   2             82s\nkube-system    kube-controller-manager-minikube      1/1     Running   2             83s\nkube-system    kube-proxy-8jfz7                      1/1     Running   0             67s\nkube-system    kube-scheduler-minikube               1/1     Running   2             83s\nkube-system    storage-provisioner                   1/1     Running   1 (45s ago)   77s\n`\n```\n5. Restart minikube\n```\n`minikube stop\n`\n```\nthen, back to 1 and and check pod, `kubectl get po -A` returns same pods as #2.\n(In this case, istio-system is lost.)\nCreated pods etc. was retained till yesterday even restart minikube or PC.\nDoes anyone face same problem or have any solution?",
      "solution": "This seems to be a bug introduced with 1.25.0 version of minikube: https://github.com/kubernetes/minikube/issues/13503 .\nA PR to revert the changes introducing the bug is already open: https://github.com/kubernetes/minikube/pull/13506\nThe fix is scheduled for minikube v1.26.",
      "question_score": 6,
      "answer_score": 5,
      "created_at": "2022-01-28T15:30:24",
      "url": "https://stackoverflow.com/questions/70895376/minikube-always-reset-to-initial-state-when-restart-it"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "minikube",
      "question_id": 66378335,
      "title": "Starting Minikube with Docker driver and bind it to host network",
      "problem": "I was wondering if it was possible to bind my minikube network to my `host` network.\nI tried:\n```\n`minikube start --memory=10000 --cpus=4 --vm-driver=docker --kubernetes-version=v1.19.6 --mount --mount-string=\"/usr/local/citizennet/db:/usr/local/citizennet/db\" --network=\"host\"\n`\n```\nBut I'm getting the following error:\n```\n`\u2757  Unable to create dedicated network, this might result in cluster IP change after restart: un-retryable: create network host 192.168.49.0/24: docker network create --driver=bridge --subnet=192.168.49.0/24 --gateway=192.168.49.1 -o --ip-masq -o --icc -o com.docker.network.driver.mtu=1500 --label=created_by.minikube.sigs.k8s.io=true host: exit status 1\nstdout:\n\nstderr:\nError response from daemon: operation is not permitted on predefined host network\n`\n```\nI was able to do that by using `haproxy` but I would like to know if there is a cleaner way of doing that.\nMy minikube is hosted on an EC2 instance and I would like to forward everything to my minikube directly. Or at least the HTTP/HTTPS requests.\nThanks!",
      "solution": "I haven't found a way to expose the `minikube` instance with `--driver=docker` to the host network (apart from `$ kubectl port-forward svc/svc-name --address=0.0.0.0 local_port:pod_port` ran on the host).\nIt produces the same error as original poster is experiencing:\n```\n`Error response from daemon: operation is not permitted on predefined host network\n`\n```\nAcknowledging following comment:\n\nthe problem is that I want to use the `ingress` addon and this addon is not compatible anymore with `--driver=none`.\n\nInstead of using `--driver=docker` which will place all of the resources in the Docker container, you can opt for a `--driver=none` which will provision all of your resources directly on the `VM`. You will be able to directly query the resources from other network devices.\nFor now `minikube` version `v1.17.1` does not allow to use the `ingress` addon with `--driver=none` but I found a way it could be provisioned. I've included this example on the end of this answer. Please treat this as a workaround.\nThis issue (inability to use `ingress` addon on `--driver=none`) is already addressed on github:\n\nGithub.com: Kubernetes: Minikube: Issues: Ingress addon stopped to work with 'none' VM driver starting from v1.12.x\n\nTalking from the perspective of exposing `minikube`:\nAs it's intended for accessing from external sources, I do recommend trying out other solutions that will subjectively speaking have easier time exposing your workloads to the external sources. There are many available tools that spawn Kubernetes clusters and you can look which suits your needs the most. Some of them are:\n\nKubeadm\nKubespray\nMicroK8S\n\nDeploying `nginx-ingress` with `minikube --driver=none`\nAs stated previously, please treat it as a workaround.\n\nA side note!\nTake a look on how your `NGINX Ingress` controller is configured with `minikube addons enable ingress` as it will be pretty much mimicked in this example.\n\nSteps:\n\n`Download` the `nginx-ingress` `YAML` manifest:\n\nModify the `Deployment` in the manifest\nDelete the `Service` from manifest\n\nApply and check\n\n`Download` the `nginx-ingress` `YAML` manifest\nYou can use following manifest:\n\nKubernetes.github.io: Ingress Nginx: Deploy (for example `GKE` manifest could be downloaded)\n\nModify the `Deployment` in the manifest\nAs I said previously, what is happening when you run `minikube addons enable ingress` could prove useful. The resources deployed have some clues on how you need to modify it.\n\nAdd the `hostPort` for `HTTP` and `HTTPS` communication:\n\n`          ports:\n            - name: http\n              hostPort: 80 # \n\nDelete the `--publish-service=$(POD_NAMESPACE)/ingress-nginx-controller`:\n\n`          args:\n            - /nginx-ingress-controller\n            - --publish-service=$(POD_NAMESPACE)/ingress-nginx-controller # \nDelete the `Service` from manifest\nYou will need to entirely delete the `Service` of type `LoadBalancer` named:  `ingress-nginx` from the manifest as you will already be using `hostPort`.\nAfter this steps you should be able to use `Ingress` resources and communicate with them on `VM_IP`:`80`/`443`.\n\nAdditional resources:\n\nKubernetes.io: Docs: Concepts: Services networking: Ingress\nMinikube.sigs.k8s.io: Docs: Drivers: None",
      "question_score": 6,
      "answer_score": 6,
      "created_at": "2021-02-26T01:12:51",
      "url": "https://stackoverflow.com/questions/66378335/starting-minikube-with-docker-driver-and-bind-it-to-host-network"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "minikube",
      "question_id": 71493306,
      "title": "Enable use of images from the local library on Kubernetes",
      "problem": "I'm following a tutorial https://docs.openfaas.com/tutorials/first-python-function/,\ncurrently, I have the right image\n`$ docker images | grep hello-openfaas\nwm/hello-openfaas                                     latest                          bd08d01ce09b   34 minutes ago      65.2MB\n$ faas-cli deploy -f ./hello-openfaas.yml \nDeploying: hello-openfaas.\nWARNING! You are not using an encrypted connection to the gateway, consider using HTTPS.\n\nDeployed. 202 Accepted.\nURL: http://IP:8099/function/hello-openfaas\n`\nthere is a step that forewarns me to do some setup(My case is I'm using `Kubernetes` and `minikube` and don't want to push to a remote container registry, I should enable the use of images from the local library on Kubernetes.), I see the hints\n```\n`see the helm chart for how to set the ImagePullPolicy\n`\n```\nI'm not sure how to configure it correctly. the final result indicates I failed.\nUnsurprisingly, I couldn't access the function service, I find some clues in  https://docs.openfaas.com/deployment/troubleshooting/#openfaas-didnt-start which might help to diagnose the problem.\n```\n`$ kubectl logs -n openfaas-fn deploy/hello-openfaas\nError from server (BadRequest): container \"hello-openfaas\" in pod \"hello-openfaas-558f99477f-wd697\" is waiting to start: trying and failing to pull image\n\n$ kubectl describe -n openfaas-fn deploy/hello-openfaas\nName:                   hello-openfaas\nNamespace:              openfaas-fn\nCreationTimestamp:      Wed, 16 Mar 2022 14:59:49 +0800\nLabels:                 faas_function=hello-openfaas\nAnnotations:            deployment.kubernetes.io/revision: 1\n                        prometheus.io.scrape: false\nSelector:               faas_function=hello-openfaas\nReplicas:               1 desired | 1 updated | 1 total | 0 available | 1 unavailable\nStrategyType:           RollingUpdate\nMinReadySeconds:        0\nRollingUpdateStrategy:  0 max unavailable, 1 max surge\nPod Template:\n  Labels:       faas_function=hello-openfaas\n  Annotations:  prometheus.io.scrape: false\n  Containers:\n   hello-openfaas:\n    Image:      wm/hello-openfaas:latest\n    Port:       8080/TCP\n    Host Port:  0/TCP\n    Liveness:   http-get http://:8080/_/health delay=2s timeout=1s period=2s #success=1 #failure=3\n    Readiness:  http-get http://:8080/_/health delay=2s timeout=1s period=2s #success=1 #failure=3\n    Environment:\n      fprocess:  python3 index.py\n    Mounts:      \n  Volumes:       \nConditions:\n  Type           Status  Reason\n  ----           ------  ------\n  Available      False   MinimumReplicasUnavailable\n  Progressing    False   ProgressDeadlineExceeded\nOldReplicaSets:  \nNewReplicaSet:   hello-openfaas-558f99477f (1/1 replicas created)\nEvents:\n  Type    Reason             Age   From                   Message\n  ----    ------             ----  ----                   -------\n  Normal  ScalingReplicaSet  29m   deployment-controller  Scaled up replica set hello-openfaas-558f99477f to 1\n`\n```\n`hello-openfaas.yml`\n`version: 1.0\nprovider:\n  name: openfaas\n  gateway: http://IP:8099\nfunctions:\n  hello-openfaas:\n    lang: python3\n    handler: ./hello-openfaas\n    image: wm/hello-openfaas:latest\n    imagePullPolicy: Never\n`\n\nI create a new project `hello-openfaas2` to reproduce this error\n`$ faas-cli new --lang python3 hello-openfaas2 --prefix=\"wm\"\nFolder: hello-openfaas2 created.\n# I add `imagePullPolicy: Never` to `hello-openfaas2.yml`\n$ faas-cli build -f ./hello-openfaas2.yml \n$ faas-cli deploy -f ./hello-openfaas2.yml \nDeploying: hello-openfaas2.\nWARNING! You are not using an encrypted connection to the gateway, consider using HTTPS.\n\nDeployed. 202 Accepted.\nURL: http://192.168.1.3:8099/function/hello-openfaas2\n\n$ kubectl logs -n openfaas-fn deploy/hello-openfaas2\nError from server (BadRequest): container \"hello-openfaas2\" in pod \"hello-openfaas2-7c67488865-7d7vm\" is waiting to start: image can't be pulled\n\n$ kubectl get pods --all-namespaces\nNAMESPACE              NAME                                        READY   STATUS             RESTARTS         AGE\nkube-system            coredns-64897985d-kp7vf                     1/1     Running            0                47h\n...\nopenfaas-fn            env-6c79f7b946-bzbtm                        1/1     Running            0                4h28m\nopenfaas-fn            figlet-54db496f88-957xl                     1/1     Running            0                18h\nopenfaas-fn            hello-openfaas-547857b9d6-z277c             0/1     ImagePullBackOff   0                127m\nopenfaas-fn            hello-openfaas-7b6946b4f9-hcvq4             0/1     ImagePullBackOff   0                165m\nopenfaas-fn            hello-openfaas2-7c67488865-qmrkl            0/1     ImagePullBackOff   0                13m\nopenfaas-fn            hello-openfaas3-65847b8b67-b94kd            0/1     ImagePullBackOff   0                97m\nopenfaas-fn            hello-python-554b464498-zxcdv               0/1     ErrImagePull       0                3h23m\nopenfaas-fn            hello-python-8698bc68bd-62gh9               0/1     ImagePullBackOff   0                3h25m\n`\n\nfrom https://docs.openfaas.com/reference/yaml/, I know I put the `imagePullPolicy` in the wrong place, there is no such keyword in its schema.\nI also tried `eval $(minikube docker-env` and still get the same error.\n\nI've a feeling that `faas-cli deploy` can be replace by `helm`, they all mean to run the image(whether from remote or local) in Kubernetes cluster, then I can use `helm chart` to setup the `pullPolicy` there. Even though the detail is not still clear to me, This discovery inspires me.\n\nSo far, after `eval $(minikube docker-env)`\n`$ docker images\nREPOSITORY                                TAG        IMAGE ID       CREATED             SIZE\nwm/hello-openfaas2                        0.1        03c21bd96d5e   About an hour ago   65.2MB\npython                                    3-alpine   69fba17b9bae   12 days ago         48.6MB\nghcr.io/openfaas/figlet                   latest     ca5eef0de441   2 weeks ago         14.8MB\nghcr.io/openfaas/alpine                   latest     35f3d4be6bb8   2 weeks ago         14.2MB\nghcr.io/openfaas/faas-netes               0.14.2     524b510505ec   3 weeks ago         77.3MB\nk8s.gcr.io/kube-apiserver                 v1.23.3    f40be0088a83   7 weeks ago         135MB\nk8s.gcr.io/kube-controller-manager        v1.23.3    b07520cd7ab7   7 weeks ago         125MB\nk8s.gcr.io/kube-scheduler                 v1.23.3    99a3486be4f2   7 weeks ago         53.5MB\nk8s.gcr.io/kube-proxy                     v1.23.3    9b7cc9982109   7 weeks ago         112MB\nghcr.io/openfaas/gateway                  0.21.3     ab4851262cd1   7 weeks ago         30.6MB\nghcr.io/openfaas/basic-auth               0.21.3     16e7168a17a3   7 weeks ago         14.3MB\nk8s.gcr.io/etcd                           3.5.1-0    25f8c7f3da61   4 months ago        293MB\nghcr.io/openfaas/classic-watchdog         0.2.0      6f97aa96da81   4 months ago        8.18MB\nk8s.gcr.io/coredns/coredns                v1.8.6     a4ca41631cc7   5 months ago        46.8MB\nk8s.gcr.io/pause                          3.6        6270bb605e12   6 months ago        683kB\nghcr.io/openfaas/queue-worker             0.12.2     56e7216201bc   7 months ago        7.97MB\nkubernetesui/dashboard                    v2.3.1     e1482a24335a   9 months ago        220MB\nkubernetesui/metrics-scraper              v1.0.7     7801cfc6d5c0   9 months ago        34.4MB\nnats-streaming                            0.22.0     12f2d32e0c9a   9 months ago        19.8MB\ngcr.io/k8s-minikube/storage-provisioner   v5         6e38f40d628d   11 months ago       31.5MB\nfunctions/markdown-render                 latest     93b5da182216   2 years ago         24.6MB\nfunctions/hubstats                        latest     01affa91e9e4   2 years ago         29.3MB\nfunctions/nodeinfo                        latest     2fe8a87bf79c   2 years ago         71.4MB\nfunctions/alpine                          latest     46c6f6d74471   2 years ago         21.5MB\nprom/prometheus                           v2.11.0    b97ed892eb23   2 years ago         126MB\nprom/alertmanager                         v0.18.0    ce3c87f17369   2 years ago         51.9MB\nalexellis2/openfaas-colorization          0.4.1      d36b67b1b5c1   2 years ago         1.84GB\nrorpage/text-to-speech                    latest     5dc20810eb54   2 years ago         86.9MB\nstefanprodan/faas-grafana                 4.6.3      2a4bd9caea50   4 years ago         284MB\n\n$ kubectl get pods --all-namespaces\nNAMESPACE              NAME                                        READY   STATUS             RESTARTS        AGE\nkube-system            coredns-64897985d-kp7vf                     1/1     Running            0               6d\nkube-system            etcd-minikube                               1/1     Running            0               6d\nkube-system            kube-apiserver-minikube                     1/1     Running            0               6d\nkube-system            kube-controller-manager-minikube            1/1     Running            0               6d\nkube-system            kube-proxy-5m8lr                            1/1     Running            0               6d\nkube-system            kube-scheduler-minikube                     1/1     Running            0               6d\nkube-system            storage-provisioner                         1/1     Running            1 (6d ago)      6d\nkubernetes-dashboard   dashboard-metrics-scraper-58549894f-97tsv   1/1     Running            0               5d7h\nkubernetes-dashboard   kubernetes-dashboard-ccd587f44-lkwcx        1/1     Running            0               5d7h\nopenfaas-fn            base64-6bdbcdb64c-djz8f                     1/1     Running            0               5d1h\nopenfaas-fn            colorise-85c74c686b-2fz66                   1/1     Running            0               4d5h\nopenfaas-fn            echoit-5d7df6684c-k6ljn                     1/1     Running            0               5d1h\nopenfaas-fn            env-6c79f7b946-bzbtm                        1/1     Running            0               4d5h\nopenfaas-fn            figlet-54db496f88-957xl                     1/1     Running            0               4d19h\nopenfaas-fn            hello-openfaas-547857b9d6-z277c             0/1     ImagePullBackOff   0               4d3h\nopenfaas-fn            hello-openfaas-7b6946b4f9-hcvq4             0/1     ImagePullBackOff   0               4d3h\nopenfaas-fn            hello-openfaas2-5c6f6cb5d9-24hkz            0/1     ImagePullBackOff   0               9m22s\nopenfaas-fn            hello-openfaas2-8957bb47b-7cgjg             0/1     ImagePullBackOff   0               2d22h\nopenfaas-fn            hello-openfaas3-65847b8b67-b94kd            0/1     ImagePullBackOff   0               4d2h\nopenfaas-fn            hello-python-6d6976845f-cwsln               0/1     ImagePullBackOff   0               3d19h\nopenfaas-fn            hello-python-b577cb8dc-64wf5                0/1     ImagePullBackOff   0               3d9h\nopenfaas-fn            hubstats-b6cd4dccc-z8tvl                    1/1     Running            0               5d1h\nopenfaas-fn            markdown-68f69f47c8-w5m47                   1/1     Running            0               5d1h\nopenfaas-fn            nodeinfo-d48cbbfcc-hfj79                    1/1     Running            0               5d1h\nopenfaas-fn            openfaas2-fun                               1/1     Running            0               15s\nopenfaas-fn            text-to-speech-74ffcdfd7-997t4              0/1     CrashLoopBackOff   2235 (3s ago)   4d5h\nopenfaas-fn            wordcount-6489865566-cvfzr                  1/1     Running            0               5d1h\nopenfaas               alertmanager-88449c789-fq2rg                1/1     Running            0               3d1h\nopenfaas               basic-auth-plugin-75fd7d69c5-zw4jh          1/1     Running            0               3d2h\nopenfaas               gateway-5c4bb7c5d7-n8h27                    2/2     Running            0               3d2h\nopenfaas               grafana                                     1/1     Running            0               4d8h\nopenfaas               nats-647b476664-hkr7p                       1/1     Running            0               3d2h\nopenfaas               prometheus-687648749f-tl8jp                 1/1     Running            0               3d1h\nopenfaas               queue-worker-7777ffd7f6-htx6t               1/1     Running            0               3d2h\n\n$ kubectl get -o yaml -n openfaas-fn deploy/hello-openfaas2\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  annotations:\n    deployment.kubernetes.io/revision: \"6\"\n    prometheus.io.scrape: \"false\"\n  creationTimestamp: \"2022-03-17T12:47:35Z\"\n  generation: 6\n  labels:\n    faas_function: hello-openfaas2\n  name: hello-openfaas2\n  namespace: openfaas-fn\n  resourceVersion: \"400833\"\n  uid: 9c4e9d26-23af-4f93-8538-4e2d96f0d7e0\nspec:\n  progressDeadlineSeconds: 600\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      faas_function: hello-openfaas2\n  strategy:\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        prometheus.io.scrape: \"false\"\n      creationTimestamp: null\n      labels:\n        faas_function: hello-openfaas2\n        uid: \"969512830\"\n      name: hello-openfaas2\n    spec:\n      containers:\n      - env:\n        - name: fprocess\n          value: python3 index.py\n        image: wm/hello-openfaas2:0.1\n        imagePullPolicy: Always\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /_/health\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 2\n          periodSeconds: 2\n          successThreshold: 1\n          timeoutSeconds: 1\n        name: hello-openfaas2\n        ports:\n        - containerPort: 8080\n          name: http\n          protocol: TCP\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /_/health\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 2\n          periodSeconds: 2\n          successThreshold: 1\n          timeoutSeconds: 1\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: false\n        terminationMessagePath: /dev/termination-log\n        terminationMessagePolicy: File\n      dnsPolicy: ClusterFirst\n      enableServiceLinks: false\n      restartPolicy: Always\n      schedulerName: default-scheduler\n      securityContext: {}\n      terminationGracePeriodSeconds: 30\nstatus:\n  conditions:\n  - lastTransitionTime: \"2022-03-17T12:47:35Z\"\n    lastUpdateTime: \"2022-03-17T12:47:35Z\"\n    message: Deployment does not have minimum availability.\n    reason: MinimumReplicasUnavailable\n    status: \"False\"\n    type: Available\n  - lastTransitionTime: \"2022-03-20T12:16:56Z\"\n    lastUpdateTime: \"2022-03-20T12:16:56Z\"\n    message: ReplicaSet \"hello-openfaas2-5d6c7c7fb4\" has timed out progressing.\n    reason: ProgressDeadlineExceeded\n    status: \"False\"\n    type: Progressing\n  observedGeneration: 6\n  replicas: 2\n  unavailableReplicas: 2\n  updatedReplicas: 1\n`\n\nIn one shell,\n`docker@minikube:~$ docker run  --name wm -ti wm/hello-openfaas2:0.1\n2022/03/20 13:04:52 Version: 0.2.0  SHA: 56bf6aac54deb3863a690f5fc03a2a38e7d9e6ef\n2022/03/20 13:04:52 Timeouts: read: 5s write: 5s hard: 0s health: 5s.\n2022/03/20 13:04:52 Listening on port: 8080\n...\n\n`\nand another shell\n```\n`docker@minikube:~$ docker ps | grep wm\nd7796286641c   wm/hello-openfaas2:0.1             \"fwatchdog\"              3 minutes ago       Up 3 minutes (healthy)   8080/tcp   wm\n`\n```",
      "solution": "When you specify an image to pull from without a url, this defaults to DockerHub. When you use `:latest` tag, it will always pull the latest image regardless of what pull policy is defined.\nSo to use local built images - don't use the latest tag.\nTo make minikube pull images from your local machine, you need to do few things:\n\nPoint your docker client to the VM's docker daemon: `eval $(minikube docker-env)`\nConfigure image pull policy: `imagePullPolicy: Never`\nThere is a flag to pass in to use insecure registries in minikube VM. This must be specified when you create the machine: `minikube start --insecure-registry`\n\nNote you have to run eval `eval $(minikube docker-env)` on each terminal you want to use, since it only sets the environment variables for the current shell session.\nThis flow works:\n`# Start minikube and set docker env\nminikube start\neval $(minikube docker-env)\n\n# Build image\ndocker build -t foo:1.0 .\n\n# Run in minikube\nkubectl run hello-foo --image=foo:1.0 --image-pull-policy=Never\n`\nYou can read more at the minikube docs.",
      "question_score": 6,
      "answer_score": 2,
      "created_at": "2022-03-16T08:44:36",
      "url": "https://stackoverflow.com/questions/71493306/enable-use-of-images-from-the-local-library-on-kubernetes"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "minikube",
      "question_id": 67194692,
      "title": "Problem with minikube and nginx ingress when reinstalled minikube",
      "problem": "When I'm running following code:\n`minikube addons enable ingress\n`\nI'm getting following error:\n`\u25aa Using image k8s.gcr.io/ingress-nginx/controller:v0.44.0\n    \u25aa Using image docker.io/jettech/kube-webhook-certgen:v1.5.1\n    \u25aa Using image docker.io/jettech/kube-webhook-certgen:v1.5.1\n\ud83d\udd0e  Verifying ingress addon...\n\n\u274c  Exiting due to MK_ENABLE: run callbacks: running callbacks: [sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.20.2/kubectl apply -f /etc/kubernetes/addons/ingress-configmap.yaml -f /etc/kubernetes/addons/ingress-rbac.yaml -f /etc/kubernetes/addons/ingress-dp.yaml: Process exited with status 1\nstdout:\nnamespace/ingress-nginx unchanged\nconfigmap/ingress-nginx-controller unchanged\nconfigmap/tcp-services unchanged\nconfigmap/udp-services unchanged\nserviceaccount/ingress-nginx unchanged\nclusterrole.rbac.authorization.k8s.io/ingress-nginx unchanged\nclusterrolebinding.rbac.authorization.k8s.io/ingress-nginx unchanged\nrole.rbac.authorization.k8s.io/ingress-nginx unchanged\nrolebinding.rbac.authorization.k8s.io/ingress-nginx unchanged\nserviceaccount/ingress-nginx-admission unchanged\nclusterrole.rbac.authorization.k8s.io/ingress-nginx-admission unchanged\nclusterrolebinding.rbac.authorization.k8s.io/ingress-nginx-admission unchanged\nrole.rbac.authorization.k8s.io/ingress-nginx-admission unchanged\nrolebinding.rbac.authorization.k8s.io/ingress-nginx-admission unchanged\nservice/ingress-nginx-controller-admission unchanged\nservice/ingress-nginx-controller unchanged\nvalidatingwebhookconfiguration.admissionregistration.k8s.io/ingress-nginx-admission configured\n\nstderr:\nError from server (Invalid): error when applying patch:\n{\"metadata\":{\"annotations\":{\"kubectl.kubernetes.io/last-applied-configuration\":\"{\\\"apiVersion\\\":\\\"apps/v1\\\",\\\"kind\\\":\\\"Deployment\\\",\\\"metadata\\\":{\\\"annotations\\\":{},\\\"labels\\\":{\\\"addonmanager.kubernetes.io/mode\\\":\\\"Reconcile\\\",\\\"app.kubernetes.io/component\\\":\\\"controller\\\",\\\"app.kubernetes.io/instance\\\":\\\"ingress-nginx\\\",\\\"app.kubernetes.io/name\\\":\\\"ingress-nginx\\\"},\\\"name\\\":\\\"ingress-nginx-controller\\\",\\\"namespace\\\":\\\"ingress-nginx\\\"},\\\"spec\\\":{\\\"minReadySeconds\\\":0,\\\"revisionHistoryLimit\\\":10,\\\"selector\\\":{\\\"matchLabels\\\":{\\\"addonmanager.kubernetes.io/mode\\\":\\\"Reconcile\\\",\\\"app.kubernetes.io/component\\\":\\\"controller\\\",\\\"app.kubernetes.io/instance\\\":\\\"ingress-nginx\\\",\\\"app.kubernetes.io/name\\\":\\\"ingress-nginx\\\"}},\\\"strategy\\\":{\\\"rollingUpdate\\\":{\\\"maxUnavailable\\\":1},\\\"type\\\":\\\"RollingUpdate\\\"},\\\"template\\\":{\\\"metadata\\\":{\\\"labels\\\":{\\\"addonmanager.kubernetes.io/mode\\\":\\\"Reconcile\\\",\\\"app.kubernetes.io/component\\\":\\\"controller\\\",\\\"app.kubernetes.io/instance\\\":\\\"ingress-nginx\\\",\\\"app.kubernetes.io/name\\\":\\\"ingress-nginx\\\",\\\"gcp-auth-skip-secret\\\":\\\"true\\\"}},\\\"spec\\\":{\\\"containers\\\":[{\\\"args\\\":[\\\"/nginx-ingress-controller\\\",\\\"--ingress-class=nginx\\\",\\\"--configmap=$(POD_NAMESPACE)/ingress-nginx-controller\\\",\\\"--report-node-internal-ip-address\\\",\\\"--tcp-services-configmap=$(POD_NAMESPACE)/tcp-services\\\",\\\"--udp-services-configmap=$(POD_NAMESPACE)/udp-services\\\",\\\"--validating-webhook=:8443\\\",\\\"--validating-webhook-certificate=/usr/local/certificates/cert\\\",\\\"--validating-webhook-key=/usr/local/certificates/key\\\"],\\\"env\\\":[{\\\"name\\\":\\\"POD_NAME\\\",\\\"valueFrom\\\":{\\\"fieldRef\\\":{\\\"fieldPath\\\":\\\"metadata.name\\\"}}},{\\\"name\\\":\\\"POD_NAMESPACE\\\",\\\"valueFrom\\\":{\\\"fieldRef\\\":{\\\"fieldPath\\\":\\\"metadata.namespace\\\"}}},{\\\"name\\\":\\\"LD_PRELOAD\\\",\\\"value\\\":\\\"/usr/local/lib/libmimalloc.so\\\"}],\\\"image\\\":\\\"k8s.gcr.io/ingress-nginx/controller:v0.44.0@sha256:3dd0fac48073beaca2d67a78c746c7593f9c575168a17139a9955a82c63c4b9a\\\",\\\"imagePullPolicy\\\":\\\"IfNotPresent\\\",\\\"lifecycle\\\":{\\\"preStop\\\":{\\\"exec\\\":{\\\"command\\\":[\\\"/wait-shutdown\\\"]}}},\\\"livenessProbe\\\":{\\\"failureThreshold\\\":5,\\\"httpGet\\\":{\\\"path\\\":\\\"/healthz\\\",\\\"port\\\":10254,\\\"scheme\\\":\\\"HTTP\\\"},\\\"initialDelaySeconds\\\":10,\\\"periodSeconds\\\":10,\\\"successThreshold\\\":1,\\\"timeoutSeconds\\\":1},\\\"name\\\":\\\"controller\\\",\\\"ports\\\":[{\\\"containerPort\\\":80,\\\"hostPort\\\":80,\\\"name\\\":\\\"http\\\",\\\"protocol\\\":\\\"TCP\\\"},{\\\"containerPort\\\":443,\\\"hostPort\\\":443,\\\"name\\\":\\\"https\\\",\\\"protocol\\\":\\\"TCP\\\"},{\\\"containerPort\\\":8443,\\\"name\\\":\\\"webhook\\\",\\\"protocol\\\":\\\"TCP\\\"}],\\\"readinessProbe\\\":{\\\"failureThreshold\\\":3,\\\"httpGet\\\":{\\\"path\\\":\\\"/healthz\\\",\\\"port\\\":10254,\\\"scheme\\\":\\\"HTTP\\\"},\\\"initialDelaySeconds\\\":10,\\\"periodSeconds\\\":10,\\\"successThreshold\\\":1,\\\"timeoutSeconds\\\":1},\\\"resources\\\":{\\\"requests\\\":{\\\"cpu\\\":\\\"100m\\\",\\\"memory\\\":\\\"90Mi\\\"}},\\\"securityContext\\\":{\\\"allowPrivilegeEscalation\\\":true,\\\"capabilities\\\":{\\\"add\\\":[\\\"NET_BIND_SERVICE\\\"],\\\"drop\\\":[\\\"ALL\\\"]},\\\"runAsUser\\\":101},\\\"volumeMounts\\\":[{\\\"mountPath\\\":\\\"/usr/local/certificates/\\\",\\\"name\\\":\\\"webhook-cert\\\",\\\"readOnly\\\":true}]}],\\\"dnsPolicy\\\":\\\"ClusterFirst\\\",\\\"serviceAccountName\\\":\\\"ingress-nginx\\\",\\\"volumes\\\":[{\\\"name\\\":\\\"webhook-cert\\\",\\\"secret\\\":{\\\"secretName\\\":\\\"ingress-nginx-admission\\\"}}]}}}}\\n\"},\"labels\":{\"addonmanager.kubernetes.io/mode\":\"Reconcile\",\"app.kubernetes.io/managed-by\":null,\"app.kubernetes.io/version\":null,\"helm.sh/chart\":null}},\"spec\":{\"minReadySeconds\":0,\"selector\":{\"matchLabels\":{\"addonmanager.kubernetes.io/mode\":\"Reconcile\"}},\"strategy\":{\"$retainKeys\":[\"rollingUpdate\",\"type\"],\"rollingUpdate\":{\"maxUnavailable\":1}},\"template\":{\"metadata\":{\"labels\":{\"addonmanager.kubernetes.io/mode\":\"Reconcile\",\"gcp-auth-skip-secret\":\"true\"}},\"spec\":{\"$setElementOrder/containers\":[{\"name\":\"controller\"}],\"containers\":[{\"$setElementOrder/ports\":[{\"containerPort\":80},{\"containerPort\":443},{\"containerPort\":8443}],\"args\":[\"/nginx-ingress-controller\",\"--ingress-class=nginx\",\"--configmap=$(POD_NAMESPACE)/ingress-nginx-controller\",\"--report-node-internal-ip-address\",\"--tcp-services-configmap=$(POD_NAMESPACE)/tcp-services\",\"--udp-services-configmap=$(POD_NAMESPACE)/udp-services\",\"--validating-webhook=:8443\",\"--validating-webhook-certificate=/usr/local/certificates/cert\",\"--validating-webhook-key=/usr/local/certificates/key\"],\"image\":\"k8s.gcr.io/ingress-nginx/controller:v0.44.0@sha256:3dd0fac48073beaca2d67a78c746c7593f9c575168a17139a9955a82c63c4b9a\",\"name\":\"controller\",\"ports\":[{\"containerPort\":80,\"hostPort\":80},{\"containerPort\":443,\"hostPort\":443}]}],\"nodeSelector\":null,\"terminationGracePeriodSeconds\":null}}}}\nto:\nResource: \"apps/v1, Resource=deployments\", GroupVersionKind: \"apps/v1, Kind=Deployment\"\nName: \"ingress-nginx-controller\", Namespace: \"ingress-nginx\"\nfor: \"/etc/kubernetes/addons/ingress-dp.yaml\": Deployment.apps \"ingress-nginx-controller\" is invalid: spec.selector: Invalid value: v1.LabelSelector{MatchLabels:map[string]string{\"addonmanager.kubernetes.io/mode\":\"Reconcile\", \"app.kubernetes.io/component\":\"controller\", \"app.kubernetes.io/instance\":\"ingress-nginx\", \"app.kubernetes.io/name\":\"ingress-nginx\"}, MatchExpressions:[]v1.LabelSelectorRequirement(nil)}: field is immutable\nError from server (Invalid): error when applying patch:\n{\"metadata\":{\"annotations\":{\"helm.sh/hook\":null,\"helm.sh/hook-delete-policy\":null,\"kubectl.kubernetes.io/last-applied-configuration\":\"{\\\"apiVersion\\\":\\\"batch/v1\\\",\\\"kind\\\":\\\"Job\\\",\\\"metadata\\\":{\\\"annotations\\\":{},\\\"labels\\\":{\\\"addonmanager.kubernetes.io/mode\\\":\\\"Reconcile\\\",\\\"app.kubernetes.io/component\\\":\\\"admission-webhook\\\",\\\"app.kubernetes.io/instance\\\":\\\"ingress-nginx\\\",\\\"app.kubernetes.io/name\\\":\\\"ingress-nginx\\\"},\\\"name\\\":\\\"ingress-nginx-admission-create\\\",\\\"namespace\\\":\\\"ingress-nginx\\\"},\\\"spec\\\":{\\\"template\\\":{\\\"metadata\\\":{\\\"labels\\\":{\\\"addonmanager.kubernetes.io/mode\\\":\\\"Reconcile\\\",\\\"app.kubernetes.io/component\\\":\\\"admission-webhook\\\",\\\"app.kubernetes.io/instance\\\":\\\"ingress-nginx\\\",\\\"app.kubernetes.io/name\\\":\\\"ingress-nginx\\\"},\\\"name\\\":\\\"ingress-nginx-admission-create\\\"},\\\"spec\\\":{\\\"containers\\\":[{\\\"args\\\":[\\\"create\\\",\\\"--host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc\\\",\\\"--namespace=$(POD_NAMESPACE)\\\",\\\"--secret-name=ingress-nginx-admission\\\"],\\\"env\\\":[{\\\"name\\\":\\\"POD_NAMESPACE\\\",\\\"valueFrom\\\":{\\\"fieldRef\\\":{\\\"fieldPath\\\":\\\"metadata.namespace\\\"}}}],\\\"image\\\":\\\"docker.io/jettech/kube-webhook-certgen:v1.5.1@sha256:950833e19ade18cd389d647efb88992a7cc077abedef343fa59e012d376d79b7\\\",\\\"imagePullPolicy\\\":\\\"IfNotPresent\\\",\\\"name\\\":\\\"create\\\"}],\\\"restartPolicy\\\":\\\"OnFailure\\\",\\\"securityContext\\\":{\\\"runAsNonRoot\\\":true,\\\"runAsUser\\\":2000},\\\"serviceAccountName\\\":\\\"ingress-nginx-admission\\\"}}}}\\n\"},\"labels\":{\"addonmanager.kubernetes.io/mode\":\"Reconcile\",\"app.kubernetes.io/managed-by\":null,\"app.kubernetes.io/version\":null,\"helm.sh/chart\":null}},\"spec\":{\"template\":{\"metadata\":{\"labels\":{\"addonmanager.kubernetes.io/mode\":\"Reconcile\",\"app.kubernetes.io/managed-by\":null,\"app.kubernetes.io/version\":null,\"helm.sh/chart\":null}},\"spec\":{\"$setElementOrder/containers\":[{\"name\":\"create\"}],\"containers\":[{\"image\":\"docker.io/jettech/kube-webhook-certgen:v1.5.1@sha256:950833e19ade18cd389d647efb88992a7cc077abedef343fa59e012d376d79b7\",\"name\":\"create\"}]}}}}\nto:\nResource: \"batch/v1, Resource=jobs\", GroupVersionKind: \"batch/v1, Kind=Job\"\nName: \"ingress-nginx-admission-create\", Namespace: \"ingress-nginx\"\nfor: \"/etc/kubernetes/addons/ingress-dp.yaml\": Job.batch \"ingress-nginx-admission-create\" is invalid: spec.template: Invalid value: core.PodTemplateSpec{ObjectMeta:v1.ObjectMeta{Name:\"ingress-nginx-admission-create\", GenerateName:\"\", Namespace:\"\", SelfLink:\"\", UID:\"\", ResourceVersion:\"\", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{\"addonmanager.kubernetes.io/mode\":\"Reconcile\", \"app.kubernetes.io/component\":\"admission-webhook\", \"app.kubernetes.io/instance\":\"ingress-nginx\", \"app.kubernetes.io/name\":\"ingress-nginx\", \"controller-uid\":\"d33a74a3-101c-4e82-a2b7-45b46068f189\", \"job-name\":\"ingress-nginx-admission-create\"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:\"\", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:core.PodSpec{Volumes:[]core.Volume(nil), InitContainers:[]core.Container(nil), Containers:[]core.Container{core.Container{Name:\"create\", Image:\"docker.io/jettech/kube-webhook-certgen:v1.5.1@sha256:950833e19ade18cd389d647efb88992a7cc077abedef343fa59e012d376d79b7\", Command:[]string(nil), Args:[]string{\"create\", \"--host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc\", \"--namespace=$(POD_NAMESPACE)\", \"--secret-name=ingress-nginx-admission\"}, WorkingDir:\"\", Ports:[]core.ContainerPort(nil), EnvFrom:[]core.EnvFromSource(nil), Env:[]core.EnvVar{core.EnvVar{Name:\"POD_NAMESPACE\", Value:\"\", ValueFrom:(*core.EnvVarSource)(0xc00a79dea0)}}, Resources:core.ResourceRequirements{Limits:core.ResourceList(nil), Requests:core.ResourceList(nil)}, VolumeMounts:[]core.VolumeMount(nil), VolumeDevices:[]core.VolumeDevice(nil), LivenessProbe:(*core.Probe)(nil), ReadinessProbe:(*core.Probe)(nil), StartupProbe:(*core.Probe)(nil), Lifecycle:(*core.Lifecycle)(nil), TerminationMessagePath:\"/dev/termination-log\", TerminationMessagePolicy:\"File\", ImagePullPolicy:\"IfNotPresent\", SecurityContext:(*core.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]core.EphemeralContainer(nil), RestartPolicy:\"OnFailure\", TerminationGracePeriodSeconds:(*int64)(0xc003184dc0), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:\"ClusterFirst\", NodeSelector:map[string]string(nil), ServiceAccountName:\"ingress-nginx-admission\", AutomountServiceAccountToken:(*bool)(nil), NodeName:\"\", SecurityContext:(*core.PodSecurityContext)(0xc010b3d980), ImagePullSecrets:[]core.LocalObjectReference(nil), Hostname:\"\", Subdomain:\"\", SetHostnameAsFQDN:(*bool)(nil), Affinity:(*core.Affinity)(nil), SchedulerName:\"default-scheduler\", Tolerations:[]core.Toleration(nil), HostAliases:[]core.HostAlias(nil), PriorityClassName:\"\", Priority:(*int32)(nil), PreemptionPolicy:(*core.PreemptionPolicy)(nil), DNSConfig:(*core.PodDNSConfig)(nil), ReadinessGates:[]core.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), Overhead:core.ResourceList(nil), EnableServiceLinks:(*bool)(nil), TopologySpreadConstraints:[]core.TopologySpreadConstraint(nil)}}: field is immutable\nError from server (Invalid): error when applying patch:\n{\"metadata\":{\"annotations\":{\"helm.sh/hook\":null,\"helm.sh/hook-delete-policy\":null,\"kubectl.kubernetes.io/last-applied-configuration\":\"{\\\"apiVersion\\\":\\\"batch/v1\\\",\\\"kind\\\":\\\"Job\\\",\\\"metadata\\\":{\\\"annotations\\\":{},\\\"labels\\\":{\\\"addonmanager.kubernetes.io/mode\\\":\\\"Reconcile\\\",\\\"app.kubernetes.io/component\\\":\\\"admission-webhook\\\",\\\"app.kubernetes.io/instance\\\":\\\"ingress-nginx\\\",\\\"app.kubernetes.io/name\\\":\\\"ingress-nginx\\\"},\\\"name\\\":\\\"ingress-nginx-admission-patch\\\",\\\"namespace\\\":\\\"ingress-nginx\\\"},\\\"spec\\\":{\\\"template\\\":{\\\"metadata\\\":{\\\"labels\\\":{\\\"addonmanager.kubernetes.io/mode\\\":\\\"Reconcile\\\",\\\"app.kubernetes.io/component\\\":\\\"admission-webhook\\\",\\\"app.kubernetes.io/instance\\\":\\\"ingress-nginx\\\",\\\"app.kubernetes.io/name\\\":\\\"ingress-nginx\\\"},\\\"name\\\":\\\"ingress-nginx-admission-patch\\\"},\\\"spec\\\":{\\\"containers\\\":[{\\\"args\\\":[\\\"patch\\\",\\\"--webhook-name=ingress-nginx-admission\\\",\\\"--namespace=$(POD_NAMESPACE)\\\",\\\"--patch-mutating=false\\\",\\\"--secret-name=ingress-nginx-admission\\\",\\\"--patch-failure-policy=Fail\\\"],\\\"env\\\":[{\\\"name\\\":\\\"POD_NAMESPACE\\\",\\\"valueFrom\\\":{\\\"fieldRef\\\":{\\\"fieldPath\\\":\\\"metadata.namespace\\\"}}}],\\\"image\\\":\\\"docker.io/jettech/kube-webhook-certgen:v1.5.1@sha256:950833e19ade18cd389d647efb88992a7cc077abedef343fa59e012d376d79b7\\\",\\\"imagePullPolicy\\\":\\\"IfNotPresent\\\",\\\"name\\\":\\\"patch\\\"}],\\\"restartPolicy\\\":\\\"OnFailure\\\",\\\"securityContext\\\":{\\\"runAsNonRoot\\\":true,\\\"runAsUser\\\":2000},\\\"serviceAccountName\\\":\\\"ingress-nginx-admission\\\"}}}}\\n\"},\"labels\":{\"addonmanager.kubernetes.io/mode\":\"Reconcile\",\"app.kubernetes.io/managed-by\":null,\"app.kubernetes.io/version\":null,\"helm.sh/chart\":null}},\"spec\":{\"template\":{\"metadata\":{\"labels\":{\"addonmanager.kubernetes.io/mode\":\"Reconcile\",\"app.kubernetes.io/managed-by\":null,\"app.kubernetes.io/version\":null,\"helm.sh/chart\":null}},\"spec\":{\"$setElementOrder/containers\":[{\"name\":\"patch\"}],\"containers\":[{\"image\":\"docker.io/jettech/kube-webhook-certgen:v1.5.1@sha256:950833e19ade18cd389d647efb88992a7cc077abedef343fa59e012d376d79b7\",\"name\":\"patch\"}]}}}}\nto:\nResource: \"batch/v1, Resource=jobs\", GroupVersionKind: \"batch/v1, Kind=Job\"\nName: \"ingress-nginx-admission-patch\", Namespace: \"ingress-nginx\"\nfor: \"/etc/kubernetes/addons/ingress-dp.yaml\": Job.batch \"ingress-nginx-admission-patch\" is invalid: spec.template: Invalid value: core.PodTemplateSpec{ObjectMeta:v1.ObjectMeta{Name:\"ingress-nginx-admission-patch\", GenerateName:\"\", Namespace:\"\", SelfLink:\"\", UID:\"\", ResourceVersion:\"\", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{\"addonmanager.kubernetes.io/mode\":\"Reconcile\", \"app.kubernetes.io/component\":\"admission-webhook\", \"app.kubernetes.io/instance\":\"ingress-nginx\", \"app.kubernetes.io/name\":\"ingress-nginx\", \"controller-uid\":\"ef303f40-b52d-49c5-ab80-8330379fed36\", \"job-name\":\"ingress-nginx-admission-patch\"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:\"\", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:core.PodSpec{Volumes:[]core.Volume(nil), InitContainers:[]core.Container(nil), Containers:[]core.Container{core.Container{Name:\"patch\", Image:\"docker.io/jettech/kube-webhook-certgen:v1.5.1@sha256:950833e19ade18cd389d647efb88992a7cc077abedef343fa59e012d376d79b7\", Command:[]string(nil), Args:[]string{\"patch\", \"--webhook-name=ingress-nginx-admission\", \"--namespace=$(POD_NAMESPACE)\", \"--patch-mutating=false\", \"--secret-name=ingress-nginx-admission\", \"--patch-failure-policy=Fail\"}, WorkingDir:\"\", Ports:[]core.ContainerPort(nil), EnvFrom:[]core.EnvFromSource(nil), Env:[]core.EnvVar{core.EnvVar{Name:\"POD_NAMESPACE\", Value:\"\", ValueFrom:(*core.EnvVarSource)(0xc00fd798a0)}}, Resources:core.ResourceRequirements{Limits:core.ResourceList(nil), Requests:core.ResourceList(nil)}, VolumeMounts:[]core.VolumeMount(nil), VolumeDevices:[]core.VolumeDevice(nil), LivenessProbe:(*core.Probe)(nil), ReadinessProbe:(*core.Probe)(nil), StartupProbe:(*core.Probe)(nil), Lifecycle:(*core.Lifecycle)(nil), TerminationMessagePath:\"/dev/termination-log\", TerminationMessagePolicy:\"File\", ImagePullPolicy:\"IfNotPresent\", SecurityContext:(*core.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]core.EphemeralContainer(nil), RestartPolicy:\"OnFailure\", TerminationGracePeriodSeconds:(*int64)(0xc00573d190), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:\"ClusterFirst\", NodeSelector:map[string]string(nil), ServiceAccountName:\"ingress-nginx-admission\", AutomountServiceAccountToken:(*bool)(nil), NodeName:\"\", SecurityContext:(*core.PodSecurityContext)(0xc00d7d9100), ImagePullSecrets:[]core.LocalObjectReference(nil), Hostname:\"\", Subdomain:\"\", SetHostnameAsFQDN:(*bool)(nil), Affinity:(*core.Affinity)(nil), SchedulerName:\"default-scheduler\", Tolerations:[]core.Toleration(nil), HostAliases:[]core.HostAlias(nil), PriorityClassName:\"\", Priority:(*int32)(nil), PreemptionPolicy:(*core.PreemptionPolicy)(nil), DNSConfig:(*core.PodDNSConfig)(nil), ReadinessGates:[]core.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), Overhead:core.ResourceList(nil), EnableServiceLinks:(*bool)(nil), TopologySpreadConstraints:[]core.TopologySpreadConstraint(nil)}}: field is immutable\n]\n\n\ud83d\ude3f  If the above advice does not help, please let us know: \n\ud83d\udc49  https://github.com/kubernetes/minikube/issues/new/choose\n`\nSo I had some bug issue in my PC. So, i reinstall minikube. After this when I use `minikube start` and all want fine. But when i enable ingress then the above error was showing.\nAnd when i run `skaffold dev` the following error was showing:\n`Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress\n - Error from server (InternalError): error when creating \"STDIN\": Internal error occurred: failed calling webhook \"validate.nginx.ingress.kubernetes.io\": an error on the server (\"\") has prevented the request from succeeding\nexiting dev mode because first deploy failed: kubectl apply: exit status 1\n\n`",
      "solution": "As @Brian de Alwis pointed out in the comments section, this PR #11189 should resolve the above issue.\nYou can try the v1.20.0-beta.0 release with this fix. Additionally, a stable v1.20.0 version is now available.",
      "question_score": 6,
      "answer_score": 2,
      "created_at": "2021-04-21T13:10:40",
      "url": "https://stackoverflow.com/questions/67194692/problem-with-minikube-and-nginx-ingress-when-reinstalled-minikube"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "minikube",
      "question_id": 74552547,
      "title": "Defaulted container &quot;container-1&quot; out of: container-1, container-2",
      "problem": "In K8s i'm practising the example 6.1. A pod with two containers sharing the same volume: fortune-pod.yaml from the book kubernetes in Action. In volumes concept where my pod contain 2 containers, one of the containers is not running, Please guide me where i'm doing wrong. to run the pod successfully.\non checking the logs of the container i'm getting the below error:\n```\n`Defaulted container \"fortune-cont\" out of: fortune-cont, web-server \n`\n```\nBut where as in pod description events it looks like this.\n```\n`Events:\n  Type     Reason     Age                  From               Message\n  ----     ------     ----                 ----               -------\n  Normal   Scheduled  40m                  default-scheduler  Successfully assigned book/vol-1-fd556f5dc-8ggj6 to minikube\n  Normal   Pulled     40m                  kubelet            Container image \"nginx:alpine\" already present on machine\n  Normal   Created    40m                  kubelet            Created container web-server\n  Normal   Started    40m                  kubelet            Started container web-server\n  Normal   Created    39m (x4 over 40m)    kubelet            Created container fortune-cont\n  Normal   Started    39m (x4 over 40m)    kubelet            Started container fortune-cont\n  Normal   Pulled     38m (x5 over 40m)    kubelet            Container image \"xxxx/fortune:v1\" already present on machine\n  Warning  BackOff    25s (x188 over 40m)  kubelet            Back-off restarting failed container\n`\n```\nhere is my deployment file\n```\n`apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vol-1\n  namespace: book\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: fortune-vol-1\n      type: volume\n  template:\n    metadata:\n      labels:\n        name: fortune-vol-1\n        type: volume\n    spec:\n      containers:\n      - image: ****/fortune:v1\n        name: fortune-cont\n        volumeMounts:\n        - name: html \n          mountPath: /var/htdocs\n      - image: nginx:alpine\n        name: web-server\n        volumeMounts:\n        - name: html\n          mountPath: /usr/share/nginx/html\n          readOnly: true\n        ports:\n        - containerPort: 80\n          protocol: TCP\n      volumes:\n        - name: html\n          emptyDir: {}\n`\n```\nHere is my pod description for containers.\n```\n`Containers:\n  fortune-cont:\n    Container ID:   docker://3959e47a761b670ee826b2824efed09d8f5d6dfd6451c4c9840eebff018a3586\n    Image:          prav33n/fortune:v1\n    Image ID:       docker-pullable://prav33n/fortune@sha256:671257f6387a1ef81a293f8aef27ad7217e4281e30b777a7124b1f6017a330f8\n    Port:           \n    Host Port:      \n    State:          Waiting\n      Reason:       CrashLoopBackOff\n    Last State:     Terminated\n      Reason:       Completed\n      Exit Code:    0\n      Started:      Thu, 24 Nov 2022 02:05:26 +0530\n      Finished:     Thu, 24 Nov 2022 02:05:26 +0530\n    Ready:          False\n    Restart Count:  17\n    Environment:    \n    Mounts:\n      /var/htdocs from html (rw)\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-spdq4 (ro)\n  web-server:\n    Container ID:   docker://37d831a2f7e97abadb548a21ecb20b5c784b5b3d6102cf8f939f2c13cdfd08c0\n    Image:          nginx:alpine\n    Image ID:       docker-pullable://nginx@sha256:455c39afebd4d98ef26dd70284aa86e6810b0485af5f4f222b19b89758cabf1e\n    Port:           80/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 24 Nov 2022 01:02:55 +0530\n    Ready:          True\n    Restart Count:  0\n    Environment:    \n    Mounts:\n      /usr/share/nginx/html from html (ro)\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-spdq4 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             False \n  ContainersReady   False \n  PodScheduled      True \nVolumes:\n  html:\n    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)\n    Medium:     \n    SizeLimit:  \n  kube-api-access-spdq4:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       \n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              \nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type     Reason   Age                    From     Message\n  ----     ------   ----                   ----     -------\n  Warning  BackOff  4m20s (x281 over 64m)  kubelet  Back-off restarting failed container\n`\n```",
      "solution": "Your Pod named `vol-1` has two containers:\n\nOne named `fortune-cont`\nOne named `web-server`\n\nIf you run `kubectl logs vol-1`, Kubernetes doesn't know which container you're asking about, so it has to pick one, and tells you:\n\nDefaulted container \"fortune-cont\" out of: fortune-cont, web-server\n\nYou can select a container explicitly with the `-c` option:\n```\n`kubectl logs vol-1 -c web-server\n`\n```",
      "question_score": 5,
      "answer_score": 17,
      "created_at": "2022-11-23T21:18:36",
      "url": "https://stackoverflow.com/questions/74552547/defaulted-container-container-1-out-of-container-1-container-2"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "minikube",
      "question_id": 65669818,
      "title": "DNS error with MySQL integration with kubernetes cluster",
      "problem": "What happened:\nI am trying to create a service endpoint using the `externalName` spec to allow my microservices running inside the pods to access a local MySQL server on my local host.\nThis is the relevant section for the yaml file:\n```\n`apiVersion: v1 \nkind: Service \nmetadata: \n  name: mysql \n  namespace: default \nspec: \n  type: ExternalName \n  externalName: host.minikube.internal\n`\n```\nWhat you expected to happen:\nI expect to be able to connect but my SpringBoot containers are showing that the mysql connection is not working. I have tested the microservices and it is working in Docker with the same MySQL database.\nHow to reproduce it (as minimally and precisely as possible):\nNormal installation of minikube and kubernetes, running the `dnsutils` image from https://k8s.io/examples/admin/dns/dnsutils.yaml with the mysql service given above.\nAnything else we need to know?:\nI have tested out the troubleshooting detailed here (https://kubernetes.io/docs/tasks/administer-cluster/dns-debugging-resolution/) but it did not resolve the problem. When running:\n```\n`kubectl exec -i -t dnsutils -- nslookup mysql.default\n`\n```\nI get the following message:\n```\n`Server: 10.96.0.10\nAddress: 10.96.0.10#53\n\nmysql.default.svc.cluster.local canonical name = host.minikube.internal.\n** server can't find host.minikube.internal: SERVFAIL\n\ncommand terminated with exit code 1\n`\n```\nI have verified that `CoreDNS` is installed and running:\n```\n`NAME READY STATUS RESTARTS AGE\ncoredns-f9fd979d6-z58cr 1/1 Running 0 31m\n`\n```\nEndpoints are exposed:\n```\n`NAME ENDPOINTS AGE\nkube-dns 172.17.0.2:53,172.17.0.2:53,172.17.0.2:9153 32m\n`\n```\nMy `/etc/resolv.conf` only has one entry:\n```\n`nameserver 192.168.53.145\n`\n```\nEnvironment:\nKubernetes version (use kubectl version):\n```\n`Client Version: version.Info{Major:\"1\", Minor:\"20\", GitVersion:\"v1.20.0\", GitCommit:\"af46c47ce925f4c4ad5cc8d1fca46c7b77d13b38\", GitTreeState:\"clean\", BuildDate:\"2020-12-08T17:59:43Z\", GoVersion:\"go1.15.5\", Compiler:\"gc\", Platform:\"windows/amd64\"}\n\nServer Version: version.Info{Major:\"1\", Minor:\"19\", GitVersion:\"v1.19.4\", GitCommit:\"d360454c9bcd1634cf4cc52d1867af5491dc9c5f\", GitTreeState:\"clean\", BuildDate:\"2020-11-11T13:09:17Z\", GoVersion:\"go1.15.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n`\n```\nCloud provider or hardware configuration:\nLocal Windows 10 Pro x64 running Kubernetes with Minikube\nOS (e.g: cat /etc/os-release):\n`NAME=Buildroot VERSION=2020.02.7 ID=buildroot VERSION_ID=2020.02.7 PRETTY_NAME=\"Buildroot 2020.02.7`\"\nKernel (e.g. uname -a): Linux minikube `4.19.150 #1 SMP Fri Nov 6 15:58:07 PST 2020 x86_64 GNU/Linux`\nInstall tools: Installed using the relevant kubectl and minikube .exe files\nNetwork plugin and version (if this is a network-related bug):\nOthers:",
      "solution": "This problem seems to be close related to `Minikube` issue described on github.\nYou can see, that in your `Pod` in the `/etc/hosts` file - there isn't any `host.minikube.internal` entry:\n```\n`$ kubectl exec -it dnsutils -- cat /etc/hosts | grep \"host.minikube.internal\"\n$\n`\n```\nOn the `Minikube` host you are able to reach `host.minikube.internal` because `Minikube` (version v1.10+) adds this `hostname` entry to `/etc/hosts` file. You can find more information in Host access | minikube.\nThis is example from my `Minikube` (I'm using docker driver):\n```\n`user@minikube:~$ kubectl exec -it dnsutils -- cat /etc/hosts | grep \"host.minikube.internal\"\nuser@minikube:~$\nuser@minikube:~$ minikube ssh\ndocker@minikube:~$ cat /etc/hosts | grep host.minikube.internal \n192.168.49.1    host.minikube.internal\ndocker@minikube:~$ ping host.minikube.internal\nPING host.minikube.internal (192.168.49.1) 56(84) bytes of data.\n64 bytes from host.minikube.internal (192.168.49.1): icmp_seq=1 ttl=64 time=0.075 ms\n64 bytes from host.minikube.internal (192.168.49.1): icmp_seq=2 ttl=64 time=0.067 ms\n`\n```\n`host.minikube.internal` is only the entry in the `/etc/hosts` file, therefore `nslookup` can't correctly resolve it (`nslookup` queries name servers ONLY.).\n```\n`docker@minikube:~$ nslookup host.minikube.internal\nServer:         192.168.49.1\nAddress:        192.168.49.1#53\n\n** server can't find host.minikube.internal: NXDOMAIN\n`\n```\nThe only workaround I think may help in some cases is adding `hostAliases` to `Deployment`/`Pod` manifest files:\n```\n`...\nspec:\n  hostAliases:\n  - ip: \"192.168.49.1\" # minikube IP\n    hostnames:\n    - \"host.minikube.internal\" # one or more hostnames that should resolve to the above address\n  containers:\n  - name: dnsutils\n    image: gcr.io/kubernetes-e2e-test-images/dnsutils:1.3\n...\n`\n```",
      "question_score": 5,
      "answer_score": 4,
      "created_at": "2021-01-11T16:32:52",
      "url": "https://stackoverflow.com/questions/65669818/dns-error-with-mysql-integration-with-kubernetes-cluster"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "minikube",
      "question_id": 71774813,
      "title": "Minikube fails to start on Windows 11 Home and Docker Desktop",
      "problem": "I have Windows 11 Home (which does not allow Hyper-V, only Pro edition does).\nInstalled WSL2 and Docker Desktop.\nInstalled Minikube using Chocolatey but it refused to start.\nSearching on SO, I found this advice in several posts, but it failed to work.\n```\n`PS C:\\WINDOWS\\system32> docker system prune\nWARNING! This will remove:\n  - all stopped containers\n  - all networks not used by at least one container\n  - all dangling images\n  - all dangling build cache\n\nAre you sure you want to continue? [y/N] y\nerror during connect: In the default daemon configuration on Windows, the docker client must be run with elevated privileges to connect.: Post \"http://%2F%2F.%2Fpipe%2Fdocker_engine/v1.24/containers/prune\": open //./pipe/docker_engine: The system cannot find the file specified.\n\nPS C:\\WINDOWS\\system32> minikube delete\n* Removed all traces of the \"minikube\" cluster.\n\nPS C:\\WINDOWS\\system32> minikube start --driver=docker\n* minikube v1.25.2 on Microsoft Windows 11 Home 10.0.22000 Build 22000\n* Using the docker driver based on user configuration\n\nX Exiting due to PROVIDER_DOCKER_VERSION_EXIT_1: \"docker version --format -\" exit status 1: error during connect: In the default daemon configuration on Windows, the docker client must be run with elevated privileges to connect.: Get \"http://%2F%2F.%2Fpipe%2Fdocker_engine/v1.24/version\": open //./pipe/docker_engine: The system cannot find the file specified.\n* Documentation: https://minikube.sigs.k8s.io/docs/drivers/docker/\n`\n```",
      "solution": "I thought of trying to have Docker Desktop already running before I start minikube.\nFrom the Windows Start menu, I ran Docker Desktop in Administrator mode.\nNow I ran the command again\nto remove old stuff,\n```\n`PS C:\\WINDOWS\\system32> minikube delete\n* Removed all traces of the \"minikube\" cluster.\n`\n```\nand now specify the docker driver\n```\n`PS C:\\WINDOWS\\system32> minikube start --driver=docker\n* minikube v1.25.2 on Microsoft Windows 11 Home 10.0.22000 Build 22000\n* Using the docker driver based on user configuration\n* Starting control plane node minikube in cluster minikube\n* Pulling base image ...\n    > gcr.io/k8s-minikube/kicbase: 379.06 MiB / 379.06 MiB  100.00% 10.23 MiB p\n* Creating docker container (CPUs=2, Memory=3000MB) ...\n* Preparing Kubernetes v1.23.3 on Docker 20.10.12 ...\n  - kubelet.housekeeping-interval=5m\n  - Generating certificates and keys ...\n  - Booting up control plane ...\n  - Configuring RBAC rules ...\n* Verifying Kubernetes components...\n  - Using image gcr.io/k8s-minikube/storage-provisioner:v5\n* Enabled addons: storage-provisioner, default-storageclass\n* Done! kubectl is now configured to use \"minikube\" cluster and \"default\" namespace by default\n`\n```\nNow verify minikube status\n`PS C:\\WINDOWS\\system32> minikube status\nminikube\ntype: Control Plane\nhost: Running\nkubelet: Running\napiserver: Running\nkubeconfig: Configured\n`\nI don't know kubernetes as I am learning it, but it appears to have worked. I hope this will be useful to someone so they do not have to go off and spend $99 to upgrade to Windows Pro - as I was going to do if this did not work.\nUpdate: Here is a link with more details How to run Kubernetes on Windows 11",
      "question_score": 4,
      "answer_score": 16,
      "created_at": "2022-04-07T01:52:49",
      "url": "https://stackoverflow.com/questions/71774813/minikube-fails-to-start-on-windows-11-home-and-docker-desktop"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "minikube",
      "question_id": 66901072,
      "title": "Could not access Minikube(v1.18.1) Ingress on Docker-Driver Windows 10",
      "problem": "My issue is exactly the same as this. But replication the question again for your reference::\nI am facing the problem which is that I could not access the Minikube Ingress on the Browser using it's IP. I have installed Minikube on Windows 10 Home, and starting the minikube with docker driver(`minikube start --driver=docker`).\n\nSystem info:\nWindows 10 Home\nMinikube(v1.18.1)\nDocker(driver for minikube) - Docker\nengine version 20.10.5\n\nI am following this official document - https://kubernetes.io/docs/tasks/access-application-cluster/ingress-minikube/\nFirst I created the deployment by running this below command on Minikube.\n`kubectl create deployment web --image=gcr.io/google-samples/hello-app:1.0`\nThe deployment get created which can be seen on the below image: enter image description here\n\nNext, I exposed the deployment that I created above. For this I ran the below command.\n`kubectl expose deployment web --type=NodePort --port=8080`\nThis created a service which can be seen by running the below command:\n`kubectl get service web`\nThe screenshot of the service is shown below:\n\nI can now able to visit the service on the browser by running the below command:\n\n`minikube service web`\nIn the below screenshot you can see I am able to view it on the browser.\n\nNext, I created an Ingress by running the below command:\n\n`kubectl apply -f https://k8s.io/examples/service/networking/example-ingress.yaml`\nThe ingress gets created and I can verify it by running the below command:\n`kubectl get ingress`\nThe screenshot for this is given below:\n\nThe ingress ip is listed as 192.168.49.2. So that means if I should open it in the browser then it should open, but unfortunately not. It is showing site can't be reached. See the below screenshot.\n\nWhat is the problem. Please provide me a solution for it?\nI also added the mappings on etc\\hosts file.\n`192.168.49.2 hello-world.info`\nThen I also tried opening hello-world.info on the browser but no luck.\nIn the below picture I have done ping to hello-world.info which is going to IP address 192.168.49.2. This shows etc\\hosts mapping is correct:\n\nI also did curl to minikube ip and to hello-world.info and both get timeout. See below image:\n\nThe `kubectl describe services web` provides the following details:\n```\n`Name:                     web\nNamespace:                default\nLabels:                   app=web\nAnnotations:              \nSelector:                 app=web\nType:                     NodePort\nIP:                       10.100.184.92\nPort:                       8080/TCP\nTargetPort:               8080/TCP\nNodePort:                   31880/TCP\nEndpoints:                172.17.0.4:8080\nSession Affinity:         None\nExternal Traffic Policy:  Cluster\nEvents:                   \n`\n```\nThe `kubectl describe ingress example-ingress` gives the following output:\n```\n`Name:             example-ingress\nNamespace:        default\nAddress:          192.168.49.2\nDefault backend:  default-http-backend:80 ()\nRules:\n  Host              Path  Backends\n  ----              ----  --------\n  hello-world.info\n                    /   web:8080   172.17.0.4:8080)\nAnnotations:        nginx.ingress.kubernetes.io/rewrite-target: /$1\nEvents:             \n`\n```\nThe issue seems to have been resolved there, by following the below instructions(as posted in the comments):\n\nOnce you setup the ingress with necessary change, i guess you are in\nthe powershell of windows with minikube running right? Make sure you\n\u2018enable addons ingress\u2019 and have a separate console running \u2018minikube\ntunnel\u2019 as well. Also, add the hostname and ip address to windows\u2019\nhost table. Then type \u2018minikue ssh\u2019 in powershell, it gives you\ncommand line. Then you can \u2018curl myapp.com\u2019 then you should get\nresponse as expected.\n\nNevertheless, in my case, the minikube tunnel is not responding upon giving the `minikube tunnel` command. :\n\nI am not able to `curl hello-world.info` even through `minikube ssh`. Kindly help!",
      "solution": "On Windows\nAfter some decent amount of time, I came to the conclusion that ingress has some conflicts to work with Docker on Windows10-Home. Things are working fine if we want to expose a service of NodePort type but Ingress is troublesome.\nFurther, I tried to set up WSL 2 with Ubuntu in Windows 10 but no luck.\nFinally, the following worked for Ingress of Minikube on Windows10 Home:\n\nInstall VirtualBox\nUncheck the Virtual Machine Platform and Windows Hypervisor Platform options from `Control Panel -> Programs -> Turn Windows Features on and off (under Programs and Features)` and then click ok. Restart your computer if prompted to.\nNow, execute the following commands in a new cmd\n\n```\n`minikube delete\nminikube start --driver=virtualbox\n`\n```\nif `minikube start --driver=virtualbox` doesn't work, then use `minikube start --driver=virtualbox --no-vtx-check`.\nThis process solved my problem and ingress is working fine on my Windows 10 Home Minikube.\nOn Ubuntu\nFinally, Docker on Ubuntu is inherently supporting Minikube Ingress seamlessly without any glitch.",
      "question_score": 4,
      "answer_score": 8,
      "created_at": "2021-04-01T10:33:25",
      "url": "https://stackoverflow.com/questions/66901072/could-not-access-minikubev1-18-1-ingress-on-docker-driver-windows-10"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "minikube",
      "question_id": 70686945,
      "title": "This computer doesn&#39;t have VT-X/AMD-v enabled at minikube - windows 10",
      "problem": "! StartHost failed, but will try again: creating host: create: precreate: This computer doesn't have VT-X/AMD-v enabled. Enabling it in the BIOS is mandatory\nI am getting this issue when I am executing the [minikube start --driver=virtualbox] command in my windows machine. I have already enable the VT-X/AMD-v in my machine.",
      "solution": "minikube start --no-vtx-check\nThis command create the kubertenes cluster with out any error.",
      "question_score": 4,
      "answer_score": 6,
      "created_at": "2022-01-12T19:54:57",
      "url": "https://stackoverflow.com/questions/70686945/this-computer-doesnt-have-vt-x-amd-v-enabled-at-minikube-windows-10"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "minikube",
      "question_id": 67488003,
      "title": "How to resolve DNS lookup error when trying to run example microservice application using minikube",
      "problem": "Dear StackOverflow community!\nI am trying to run the https://github.com/GoogleCloudPlatform/microservices-demo locally on minikube, so I am following their development guide: https://github.com/GoogleCloudPlatform/microservices-demo/blob/master/docs/development-guide.md\nAfter I successfully set up minikube (using virtualbox driver, but I tried also hyperkit, however the results were the same) and execute `skaffold run`, after some time it will end up with following error:\n```\n`Building [shippingservice]...\nSending build context to Docker daemon    127kB\nStep 1/14 : FROM golang:1.15-alpine as builder\n ---> 6466dd056dc2\nStep 2/14 : RUN apk add --no-cache ca-certificates git\n ---> Running in 0e6d2ab2a615\nfetch https://dl-cdn.alpinelinux.org/alpine/v3.13/main/x86_64/APKINDEX.tar.gz\nWARNING: Ignoring https://dl-cdn.alpinelinux.org/alpine/v3.13/main: DNS lookup error\nfetch https://dl-cdn.alpinelinux.org/alpine/v3.13/community/x86_64/APKINDEX.tar.gz\nWARNING: Ignoring https://dl-cdn.alpinelinux.org/alpine/v3.13/community: DNS lookup error\nERROR: unable to select packages:\n  git (no such package):\n    required by: world[git]\nBuilding [recommendationservice]...\nBuilding [cartservice]...\nBuilding [emailservice]...\nBuilding [productcatalogservice]...\nBuilding [loadgenerator]...\nBuilding [checkoutservice]...\nBuilding [currencyservice]...\nBuilding [frontend]...\nBuilding [adservice]...\nunable to stream build output: The command '/bin/sh -c apk add --no-cache ca-certificates git' returned a non-zero code: 1. Please fix the Dockerfile and try again..\n`\n```\nThe error message suggest that DNS does not work. I tried to add `8.8.8.8` to `/etc/resolv.conf` on a minikube VM, but it did not help. I've noticed that after I re-run `skaffold run` and it fails again, the content `/etc/resolv.conf` returns to its original state containing `10.0.2.3` as the only DNS entry. Reaching the outside internet and pinging `8.8.8.8` form within the minikube VM works.\nCould you point me to a direction how can possible I fix the problem and learn on how the DNS inside minikube/kubernetes works? I've heard that problems with DNS inside Kubernetes cluster are frequent problems you run into.\nThanks for your answers!\nBest regards,\nRichard",
      "solution": "Tried it with docker driver, i.e. `minikube start --driver=docker`, and it works. Thanks Brian!",
      "question_score": 4,
      "answer_score": 3,
      "created_at": "2021-05-11T15:41:00",
      "url": "https://stackoverflow.com/questions/67488003/how-to-resolve-dns-lookup-error-when-trying-to-run-example-microservice-applicat"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "minikube",
      "question_id": 66481653,
      "title": "Failed to pull image repository does not exist or may require &#39;docker login&#39;:",
      "problem": "I am receiving this error when starting a Pod\n```\n`Failed to pull image \"docker-username/docker-private-repository:latest\": rpc error: code = Unknown desc = Error response from daemon: pull access denied for docker-username/docker-private-repository, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\n`\n```\nMy setup is as follow:\nCeated a Secret service using command line\n```\n`kubectl create secret docker-registry docker-keys --docker-username=docker-username --docker-password=password --docker-email=docker-email@gmail.com --docker-server=https://index.docker.io/v1\n`\n```\nwhich generates the following data inside secrets\n```\n`kubectl get secret docker-keys -o json | jq '.data | map_values(@base64d)'\n{\n  \".dockerconfigjson\": \"{\\\"auths\\\":{\\\"https://index.docker.io/v1\\\":{\\\"username\\\":\\\"docker-username\\\",\\\"password\\\":\\\"password\\\",\\\"email\\\":\\\"docker-email@gmail.com\\\",\\\"auth\\\":\\\"base64encodedtoken\\\"}}}\"\n}\n\n`\n```\nThen in deployment I am using `docker-keys` secrets\n```\n`apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: docker-private-repository\n  labels:\n    app: docker-private-repository\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: docker-private-repository\n  template:\n    metadata:\n      labels:\n        app: docker-private-repository\n    spec:\n      imagePullSecrets:\n        - name: docker-keys\n      containers:\n        - name: docker-private-repository\n          image: docker-username/docker-private-repository:latest\n          imagePullPolicy: Always\n          ports:\n            - containerPort: 3000\n`\n```\nI did tried to search for a solution, but always ended up with the above setup.\nUpdate1:\nSecrets service and pod are running in the same namespace `default`.\nPulling manually from docker-hub works\n```\n`docker pull docker-username/docker-private-repository:latest\nlatest: Pulling from docker-username/docker-private-repository\n0ecb575e629c: Already exists \n...\n7467d1831b69: Already exists \nDigest: sha256:153643ecb19c2ce54635839ce9393b2e256ce6c34a2fe75b91c7a41525a6a535\nStatus: Downloaded newer image for docker-username/docker-private-repository:latest\ndocker.io/docker-username/docker-private-repository:latest\n\n`\n```\nUpdate2 `kubectl describe pod`\nI have 2 sercrets services, one for dockerhub credentials and another is `token-rzlx6` for whatever reason.\nThing is when I run describe pod, I don't see the secrets for dockerhub to be mounted as `token-rzlx6`, could this be the reason? And why it is not mounted?\n```\n`...\nVolumes:\n  default-token-rzlx6:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-rzlx6\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  \n...\n`\n```",
      "solution": "So the problem was the docker-server value. Based on tutorials I've watched I was using api V1, while my image was pushed/created to dockerhub with V2, may be it's not relevant and V1 was deprecated. When I create a secret service I need to use V2 server `https://index.docker.io/v2/`, like:\n```\n`kubectl create secret docker-registry docker-keys \\\n  --docker-username=yyyyyy \\\n  --docker-password=xxxxx \\\n  --docker-email=my@mail.com \\\n  --docker-server=https://index.docker.io/v2/\n`\n```\nA simple thing, that took days to discover, as many articles have V1 or it's not showing it at all or using private docker registry. Docs are here. https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/\nUpdate:\nOne more thing why I wasn't paying attention to API version is `docker config` file, that has V1 as API and from here I took the endpoint to create secret service, while it worked with V2 only.\n```\n`cat ~/.docker/config.json                                                 \n{\n    \"auths\": {\n        \"https://index.docker.io/v1/\": {}\n    },\n    \"credsStore\": \"osxkeychain\"\n}%                              \n`\n```\nUpdate 2:\nHowever, when I pulled image locally with `docker pull` command, image was pulled successfully using v1 url. Assumption is, api V1 works within docker, but not in kubernetes.",
      "question_score": 4,
      "answer_score": 5,
      "created_at": "2021-03-04T20:20:20",
      "url": "https://stackoverflow.com/questions/66481653/failed-to-pull-image-repository-does-not-exist-or-may-require-docker-login"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "minikube",
      "question_id": 69194583,
      "title": "Kubernetes Dashboard error when running minikube start guide",
      "problem": "I am trying to learn how to use Kubernetes and tried following the guide here to create a local Kubernetes cluster with Docker driver.\nHowever, I'm stuck at step 3: Interact with your cluster. I tried to run `minikube dashboard` and I keep getting this error:\n```\n`Unknown error (404)\nthe server could not find the requested resource (get ingresses.extensions)\n`\n```\nMy Kubernetes version:\n```\n`Client Version: version.Info{Major:\"1\", Minor:\"21\", GitVersion:\"v1.21.2\", GitCommit:\"092fbfbf53427de67cac1e9fa54aaa09a28371d7\", GitTreeState:\"clean\", BuildDate:\"2021-06-16T12:59:11Z\", GoVersion:\"go1.16.5\", Compiler:\"gc\", Platform:\"darwin/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"22\", GitVersion:\"v1.22.1\", GitCommit:\"632ed300f2c34f6d6d15ca4cef3d3c7073412212\", GitTreeState:\"clean\", BuildDate:\"2021-08-19T15:39:34Z\", GoVersion:\"go1.16.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n`\n```\nCan someone point out where the problem lies please?",
      "solution": "I have installed the minikube according to the same guide, both locally and with the help of cloud providor and ... it works for me :)\nIf you are just learning and starting your adventure with Kubernetes, try to install everything from scratch.\nThe error you are getting is related to different versions of Client and Server. When I have installed according to the same guide, following the instructions, my versions look like this:\n```\n`Client Version: version.Info{Major:\"1\", Minor:\"22\", GitVersion:\"v1.22.1\", GitCommit:\"632ed300f2c34f6d6d15ca4cef3d3c7073412212\", GitTreeState:\"clean\", BuildDate:\"2021-08-19T15:45:37Z\", GoVersion:\"go1.16.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"22\", GitVersion:\"v1.22.1\", GitCommit:\"632ed300f2c34f6d6d15ca4cef3d3c7073412212\", GitTreeState:\"clean\", BuildDate:\"2021-08-19T15:39:34Z\", GoVersion:\"go1.16.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n`\n```\nYou have different versions. Client `v1.21.2` and Server `v1.22.1`. Additionally your `Platforms` are also not the same. If you want to solve this you need to upgrade your client version or downgrade the server version. For more see this question.",
      "question_score": 4,
      "answer_score": 1,
      "created_at": "2021-09-15T15:57:23",
      "url": "https://stackoverflow.com/questions/69194583/kubernetes-dashboard-error-when-running-minikube-start-guide"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "minikube",
      "question_id": 67896392,
      "title": "How to add host mapping to /etc/host of the minikube?",
      "problem": "I have the `minikube` environment as the following: -\n\nHost OS: `CentOS Linux release 7.7.1908 (Core)`\nDocker:  `Docker Engine - Community 20.10.7`\nminikube: `minikube version: v1.20.0`\n\nI would like to add some additional host mapping (5+ IP and name) to the `/etc/hosts` inside the `minikube` container. Then I use the `minikube ssh` to enter to the shell and try to `echo \"172.17.x.x my.some.host\" >> /etc/hosts`. There is an error as `-bash: /etc/hosts: Permission denied` since the user who login to this shell is a `docker`, not a `root`.\nI also found that at the host machine there is a docker container named `minikube` running, by using the `docker container ls`. Even I can go to this container with `root` by using `docker exec -it -u root minikube /bin/bash`. I understand that it is a kind of tweak and may be a bad practice. Especially it is too much tasks.\nRegarding to the `docker` and `docker-compose` which provides the `--add-host` and `extra_hosts` respectively to add hostname mappings, Does the `minikube` provide it? Is there any good practice to achieve this within the `minikube` and/or system administrator point-of-view good practice?\nEdit 1\nAfter `echo 172.17.x.x my.some.host > ~/.minikube/files/etc/hosts` and start the `minikube`, there are some error as the following: -\n`[kubelet-check] It seems like the kubelet isn't running or healthy.\n[kubelet-check] The HTTP call equal to 'curl -sSL http://localhost:10248/healthz' failed with error: Get \"http://localhost:10248/healthz\": dial tcp: lookup localhost on 8.8.8.8:53: no such host.\n\n        Unfortunately, an error has occurred:\n                timed out waiting for the condition\n\n        This error is likely caused by:\n                - The kubelet is not running\n                - The kubelet is unhealthy due to a misconfiguration of the node in some way (required cgroups disabled)\n\n        If you are on a systemd-powered system, you can try to troubleshoot the error with the following commands:\n                - 'systemctl status kubelet'\n                - 'journalctl -xeu kubelet'\n\n        Additionally, a control plane component may have crashed or exited when started by the container runtime.\n        To troubleshoot, list all containers using your preferred container runtimes CLI.\n\n        Here is one example how you may list all Kubernetes containers running in docker:\n                - 'docker ps -a | grep kube | grep -v pause'\n                Once you have found the failing container, you can inspect its logs with:\n                - 'docker logs CONTAINERID'\n`\nThen I use the `vi` to create a whole `hosts` file at `~/.minikube/files/etc/hosts` as the following: -\n```\n`127.0.0.1       localhost\n::1     localhost ip6-localhost ip6-loopback\nfe00::0 ip6-localnet\nff00::0 ip6-mcastprefix\nff02::1 ip6-allnodes\nff02::2 ip6-allrouters\n\n172.17.x.x my.some.host1\n172.17.x.y my.some.host2\n...\n`\n```\nAt this time the `minikube` is started properly.",
      "solution": "Minikube has a built-in sync mechanism that could deploy a desired /etc/hosts with the following example:\n```\n`mkdir -p ~/.minikube/files/etc\necho 127.0.0.1 localhost > ~/.minikube/files/etc/hosts\nminikube start\n`\n```\nThen go and check if it's working:\n```\n`minikube ssh\n`\n```\nOnce you are inside the container, you can view the contents of the /etc/hosts file using:\n```\n`cat /etc/hosts\n`\n```",
      "question_score": 3,
      "answer_score": 9,
      "created_at": "2021-06-09T03:37:47",
      "url": "https://stackoverflow.com/questions/67896392/how-to-add-host-mapping-to-etc-host-of-the-minikube"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "minikube",
      "question_id": 75636068,
      "title": "Unable to start minikube on MacOS",
      "problem": "I'm getting below error when I try to run the minikube after downloading its binary:-\n```\n`\u274c  Exiting due to GUEST_START: wait: /bin/bash -c \"sudo env PATH=\"/var/lib/minikube/binaries/v1.22.3:$PATH\" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables\": Process exited with status 1\nstdout:\n[init] Using Kubernetes version: v1.22.3\n[preflight] Running pre-flight checks\n[preflight] Pulling images required for setting up a Kubernetes cluster\n[preflight] This might take a minute or two, depending on the speed of your internet connection\n[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'\n[certs] Using certificateDir folder \"/var/lib/minikube/certs\"\n[certs] Using existing ca certificate authority\n[certs] Using existing apiserver certificate and key on disk\n\nstderr:\n    [WARNING Swap]: running with swap on is not supported. Please disable swap\n    [WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'\nW0304 05:40:42.096000    3744 certs.go:489] WARNING: could not validate bounds for certificate apiserver-kubelet-client: the certificate has expired: NotBefore: 2020-06-29 07:35:45 +0000 UTC, NotAfter: 2022-12-10 12:46:24 +0000 UTC\nerror execution phase certs/apiserver-kubelet-client: [certs] certificate apiserver-kubelet-client not signed by CA certificate ca: x509: certificate has expired or is not yet valid: current time 2023-03-04T05:40:42Z is after 2022-12-10T12:46:24Z\nTo see the stack trace of this error execute with --v=5 or higher\n`\n```\nI'm referring its official documentation here\nIt gives a warning that kubelet service is not enabled and suggested to use 'systemctl enable kubelet.service'. I tried below commands but no idea how run the minikube on MacOS:-\n```\n`(base) -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n~ \u00bb systemctl enable kubelet.service                                                                                                                                                                                                         80 \u21b5 vinod827@Vinods-MacBook-Pro\nzsh: command not found: systemctl\n(base) -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n~ \u00bb launchctl enable kubelet.service                                                                                                                                                                                                        127 \u21b5 vinod827@Vinods-MacBook-Pro\nUnrecognized target specifier.\nUsage: launchctl enable \n takes a form of /.\nPlease refer to `man launchctl` for explanation of the  specifiers.\n(base)\n`\n```\nAny idea what could be the problem here?",
      "solution": "Executing both `minikube delete` and `minikube start` has solved the problem for me",
      "question_score": 3,
      "answer_score": 9,
      "created_at": "2023-03-04T14:50:26",
      "url": "https://stackoverflow.com/questions/75636068/unable-to-start-minikube-on-macos"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "minikube",
      "question_id": 70427486,
      "title": "Cannot pull a private package/image from GitHub Container Registry into Okteto Kubernetes",
      "problem": "I hope it's ok to ask for your advice.\nThe problem in a nutshell: my pipeline cannot pull private images from GHCR.IO into Okteto Kubernetes, but public images from the same private repo work.\nI'm on Windows 10 and use WSL2-Ubuntu 20.04 LTS with kinD for development and tried minikube too.\nI get an error in Okteto which says that the image pull is \u201cunauthorized\u201d -> \u201cimagePullBackOff\u201d.\nThings I did:browsed Stack Overflow, RTFM, Okteto FAQ, download the Okteto kubeconfig, pulled my hair out and spent more hours than I would like to admit \u2013 still no success yet.\nFor whatever reason I cannot create a \u201ckubectl secret\u201d that works. When logged-in to ghcr.io via \u201cdocker login --username\u201d I can pull private images locally.\nNo matter what I\u2019ve tried I still get the error \u201cunauthorized\u201d when trying to pull a private image in Okteto.\nMy Setup with latest updates:\n\nWindows 10 Pro\nJetBrains Rider IDE\nWSL2-Ubuntu 20.04 LTS\nASP.NET Core MVC app\n.NET 6 SDK\nDocker\nkinD\nminikube\nChocolatey\nHomebrew\n\nSetup kinD\n```\n`kind create cluster --name my-name\n\nkubectl create my-namespace\n\n// create a secret to pull images from ghcr.io       \nkubectl create secret docker-registry my-secret -n my-namespace --docker-username=\"my-username\" --docker-password=\"my-password\" --docker-email=\"my-email\" --docker-server=\"https://ghcr.io\"\n\n// patch local service account\nkubectl patch serviceaccount default -p '{\"imagePullSecrets\": [{\"name\": \"my-secret\"}]}'\n`\n```\nkubernetes.yaml\n```\n`apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: okteto-repo\n  namespace: my-namespace\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: okteto-repo\n  template:\n    metadata:\n      labels:\n        app: okteto-repo\n    spec:\n      containers:\n        - name: okteto-repo\n          image: ghcr.io/user/okteto-repo:latest\n          ports:\n            - containerPort: 80\n      imagePullSecrets:\n        - name: my-secret\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: okteto-repo\n  annotations:\n    dev.okteto.com/auto-ingress: \"true\"\nspec:\n  type: ClusterIP\n  selector:\n    app: okteto-repo\n  ports:\n    - protocol: TCP\n      port: 8080\n      targetPort: 80\n`\n```\nDo you have an idea why it doesn't work and what I could do?\nThanks a lot my dear friends, every input is highly appreciated!\nHope you guys have great holidays.\nCheers,\nMichael",
      "solution": "I was able to pull a private image by doing the following:\n\nCreate a personal token in GitHub with `repo` access.\nBuild and push the image to GitHub's Container registry (I used `okteto build -t ghcr.io/rberrelleza/go-getting-started:0.0.1`)\nDownload my kubeconfig credentials from Okteto Cloud by running `okteto context update-kubeconfig`.\nCreate a secret with my credentials: `kubectl create secret docker-registry gh-regcred --docker-server=ghcr.io --docker-username=rberrelleza --docker-password=ghp_XXXXXX`\nPatched the default account to include the secret as an image pull secret: `kubectl patch serviceaccount default -p '{\"imagePullSecrets\": [{\"name\": \"gh-regcred\"}]}'`\nUpdated the image name in the kubernetes manifest\nCreated the deployment (`kubectl apply -f k8s.yaml`)\n\nThese is what my kubernetes resources looks like, in case it helps:\n```\n`# k8s.yaml\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hello-world\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: hello-world\n  template:\n    metadata:\n      labels:\n        app: hello-world\n    spec:\n      containers:\n      - image: ghcr.io/rberrelleza/go-getting-started:0.0.1\n        name: hello-world\n\n---\n\napiVersion: v1\nkind: Service\nmetadata:\n  name: hello-world\n  annotations:\n    dev.okteto.com/auto-ingress: \"true\"\nspec:\n  type: ClusterIP  \n  ports:\n  - name: \"hello-world\"\n    port: 8080\n  selector:\n    app: hello-world\n`\n```\n```\n`# default SA\napiVersion: v1\nimagePullSecrets:\n- name: gh-regcred\n- name: okteto-regcred\nkind: ServiceAccount\nmetadata:\n  creationTimestamp: \"2021-05-21T22:26:38Z\"\n  name: default\n  namespace: rberrelleza\n  resourceVersion: \"405042662\"\n  uid: 2b6a6eef-2ce7-40d3-841a-c0a5497279f7\nsecrets:\n- name: default-token-7tm42\n`\n```",
      "question_score": 3,
      "answer_score": 7,
      "created_at": "2021-12-20T21:15:04",
      "url": "https://stackoverflow.com/questions/70427486/cannot-pull-a-private-package-image-from-github-container-registry-into-okteto-k"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "minikube",
      "question_id": 71968562,
      "title": "How to stop gracefully a Pulumi up command",
      "problem": "I am trying to develop a couple of Helm Charts on Minikube. \nTo do that I am running pulumi up on a Minikube environment.\n\n`import pulumi\nfrom pulumi_kubernetes.helm.v3 import Chart, ChartOpts, FetchOpts, RepositoryOptsArgs\nimport pulumi_kubernetes as k8s\n\nconfig = pulumi.Config()\nis_minikube = config.require_bool(\"isMinikube\")\n\ndatahub_prerequisites = Chart(\n    \"prerequisites\",\n    ChartOpts(\n        chart=\"datahub-prerequisites\",\n        fetch_opts=FetchOpts(\n            repo=\"https://helm.datahubproject.io/\",\n        ),\n        values = {\n            'elasticsearch': {\n                'replicas': 1,\n                'minimumMasterNodes': 1,\n                'clusterHealthCheckParams': 'wait_for_status=yellow&timeout=1s',\n                'antiAffinity': \"soft\"\n            }, \n            'neo4j-community': {\n                'enabled': 'true' \n            }   \n        } \n    )\n)\n\ndatahub_prerequisites = Chart(\n    \"datahub\",\n    ChartOpts(\n        chart=\"datahub\",\n        fetch_opts=FetchOpts(\n            repo=\"https://helm.datahubproject.io/\",\n        ),\n    ),\n)\n`\nI made a mistake since I should have used the depends on property so that the datahaub helmchart is developed after the prerequisites. \nNow some of the resources failed to create and pulumi up is not terminating. \nIt is not a problem of minikube resources: I checked with minikube top.\nI tried to launch the pulumi destroy in another terminal window but this error occurs:\n\n```\n`error: the stack is currently locked by 1 lock(s). Either wait for the other process(es) to end or manually delete the lock file(s).\n`\n```\n\nI am quite a beginner and I would like to understand the best practices in such cases.",
      "solution": "When you run a Pulumi program, Pulumi creates a lock file to ensure nobody else can attempt to run operations on that Pulumi program.\nYou can cancel a `pulumi up` operation in the same way as more other Go programs or other Unix like tools, by sending a `SIGINT` via `ctrl+c`.\nThe first `SIGINT` will attempt to gracefully stop the Pulumi program execution, a second `SIGINT` will attempt to forcefully stop the program execution.\nThe final mechanism to stop a Pulumi program in this situation is to completely terminate Pulumi with a `SIGKILL`. This may or may not leave a lock file in place, which you can clean up using `pulumi cancel`.\nHowever, there is something to consider in this situation.\nIf you cancel a running pulumi program, Pulumi will no longer be able to confirm the state of the operation in your cloud provider API (in this case Kubernetes) and reconcile that status with your Pulumi state. You'll need to run `pulumi refresh` so that Pulumi can reconcile your cloud provider resources with the pulumi state. It's usually safe to run a `pulumi destroy` in this scenario, as Pulumi will simply destroy all resources it knows about",
      "question_score": 3,
      "answer_score": 4,
      "created_at": "2022-04-22T14:12:52",
      "url": "https://stackoverflow.com/questions/71968562/how-to-stop-gracefully-a-pulumi-up-command"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "minikube",
      "question_id": 66873437,
      "title": "access minikube cluster on the same network",
      "problem": "I've set up a `minikube` cluster for development purpose inside a VM.\nI've deployed few services and an ingress controller (the minikube one) to be able to access it without using NodePorts.\nInside my VM, I can access my services as usual with `curl http://hello-world.info` or another one. Everything works fine.\nBut when I'm outside of my VM, I can't access it even if I'm on the same network. I tried from my hosting server, from my laptop and outside with a VPN.\nThe cluster IP is well listed in my addresses inside the VM (`ip a`), but not accessible outside of it (e.g: `ping xxx`).\nHow can I access my cluster services on another machine inside the same network ?\nMy VM's IP is set to static (`ubuntu-server 20.XX.XX`) with netplan at `192.168.1.128` and my cluster IP is `192.168.49.2`. My DHCP server is allowed to distribute IPs only between `192.168.1.101-254`.\nThanks :).",
      "solution": "Personally I haven't found a way to expose `minikube` instance with `--driver=docker` on `LAN`.\nAs a workaround to expose your `minikube` instance on `LAN` you can either `--driver`:\n\n`--driver=virtualbox`\n`--driver=none`\n\nBeing specific to the exposing your Kubernetes cluster to `LAN`, I would also consider checking out other Kubernetes solutions like (you could run them on bare-metal or run them in a vbox vm with `bridged` networking:\n\nkubeadm\nkubespray\nmicrok8s\n\n`--driver=virtualbox`:\nCiting part of my own answer some time ago:\n\nAs I previously mentioned: When you create your `minikube` instance with Virtualbox you will create below network interfaces:\n\n`NAT`-  interface which will allow your VM to access the Internet. This connection cannot be used to expose your services\n`Host-only-network-adapter` - interface created by your host which allows to communicate within the interface. It means that your host and other vm's with this particular adapter could connect with each other. It's designed for internal usage.\n\nYou can read more about Virtualbox networking here:\n\nVirtualbox.org: Virtual Networking\n\nI've managed to find a workaround to allow connections outside your laptop/pc to your `minikube` instance. You will need to change network interface in settings of your `minikube` instance from  `Host-only-network-adapter` to `Bridged Adapter` (2nd adapter). This will work as another device was connected to your physical network. Please make sure that this bridged adapter is used with Ethernet NIC. `Minikube` should change IP address to match the one used in your physical one.\n\nYou will also need to change your `.kube/config` as it will have the old/wrong IP address!\n\nAfter that you should be able to connect to your `Ingress` resource by IP accessible in your physical network.\n-- Stackoverflow.com: Answers:: Expose Kubernetes cluster to Internet\n\n`--driver=none`\nYou can also run `minikube` with `--driver=none` but there are some caveats to this method which you can read more about by following this docs (tl;dr you are running your `minikube` directly on your host):\n\nMinikube.sigs.k8s.io: Docs: Drivers: None\n\nAdditional resources:\n\nMinikube.sigs.k8s.io: Docs: Drivers",
      "question_score": 3,
      "answer_score": 4,
      "created_at": "2021-03-30T16:58:16",
      "url": "https://stackoverflow.com/questions/66873437/access-minikube-cluster-on-the-same-network"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "minikube",
      "question_id": 68693295,
      "title": "Unable to access kubernetes NodePort service from browser",
      "problem": "I am doing my practice on Kubernetes through Minikube on my AWS EC2. As part of that, I have created deployment and exposed that through NodePort service, then checked with:\n```\n`curl http://:\n`\n```\nin EC2 Machine that worked fine. But when I hit the same URL on browser gave me:\n```\n`This site can't be reached\n`\n```\nCan anyone help me what is the problem and how can I access this?\nThank you.\nThis is my Deployment YAML file:\n```\n\n`\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myfirstdeployment\n  labels:\n    app: web\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: web\n  template:\n    metadata:\n      name: myfirstpod\n      labels:\n        app: web\n    spec:\n      containers:\n      - name: myfirstpod-1\n        image: nginx\n        command: [\"sleep\",\"3600\"]\n        ports:\n        - containerPort: 80\n`\n\n```\nThis is my Service YAML file\n```\n\n`\napiVersion: v1\nkind: Service\nmetadata:\n  name: myfirstservice\nspec:\n  selector:\n    app: web\n  ports:\n  - targetPort: 80 #target container's port\n    port: 80 #service port\n    nodePort: 30030 #node port that we access to\n  type: NodePort\n`\n\n```",
      "solution": "I strongly recommend going through the official tutorial showing the Accessing apps options in Minikube:\n\nHow to access applications running within minikube There are two major\ncategories of services in Kubernetes:\n\nNodePort\nLoadBalancer\n\nminikube supports either. Read on!\n\nThere you will find how to use both, the NodePort access:\n\nA `NodePort` service is the most basic way to get external traffic\ndirectly to your service. `NodePort`, as the name implies, opens a\nspecific port, and any traffic that is sent to this port is forwarded\nto the service.\n\nNotice that you have to use `minikube ip` here.\nAnd also the LoadBalancer access:\n\nA `LoadBalancer` service is the standard way to expose a service to the\ninternet. With this method, each service gets its own IP address.\n\nThis method uses the `minikube tunnel` command.\nYou can also use these docs as a supplement.",
      "question_score": 3,
      "answer_score": 5,
      "created_at": "2021-08-07T16:32:18",
      "url": "https://stackoverflow.com/questions/68693295/unable-to-access-kubernetes-nodeport-service-from-browser"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "minikube",
      "question_id": 67985163,
      "title": "Kubernetes Dashboard not accessible when providing path in ingress",
      "problem": "I have deployed minikube on Windows VM and the minikube VM is created on Virtualbox with the host-only IP.\nI have deployed the Kubernetes dashboard with NodePort IP so I can access it from outside the cluster. The svc is as follows:\n```\n`PS C:\\Users\\XXX\\Desktop\\ingress> kubectl get svc -n kubernetes-dashboard\nNAME                        TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)         AGE\ndashboard-metrics-scraper   ClusterIP   10.111.167.61           8000/TCP        5d20h\nkubernetes-dashboard        NodePort    10.111.220.57           443:30613/TCP   5d20h\n`\n```\nWith the help of the minikube ingress addon, I installed the Ingress controller which is of Nginx. Its svc details are as follows:\n```\n`PS C:\\Users\\XXX\\Desktop\\ingress> kubectl get svc -n ingress-nginx\nNAME                                 TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)                      AGE\ningress-nginx-controller             NodePort    10.98.29.41           80:32628/TCP,443:31194/TCP   5d20h\ningress-nginx-controller-admission   ClusterIP   10.96.35.36           443/TCP                      5d20h\n`\n```\nThen I have created an Ingress Rule for my dashboard application as follows:\n```\n`apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: dashboard-ingress\n  namespace: kubernetes-dashboard\n  annotations:\n    nginx.ingress.kubernetes.io/rewrite-target: /\n    kubernetes.io/ingress.class: \"nginx\"\n    nginx.ingress.kubernetes.io/add-base-url: \"true\"\n    nginx.ingress.kubernetes.io/backend-protocol: \"HTTPS\"\n    nginx.ingress.kubernetes.io/secure-backends: \"true\"\n    nginx.ingress.kubernetes.io/force-ssl-redirect: \"true\"\n    ingress.kubernetes.io/configuration-snippet: |\n      rewrite ^(/dashboard)$ $1/ permanent;\nspec:\n  rules:\n  - host: k8s.dashboard.com\n    http:\n      paths:\n      - path: /dashboard\n        pathType: Prefix\n        backend:\n          service:\n            name: kubernetes-dashboard\n            port:\n              number: 443\n`\n```\nBut now when I am trying to access the dashboard with the following URL `https://k8s.dashboard.com/dashboard` then I am facing the error of 404 Not Found. I also tried multiple URL to access the dashboard such as :\n```\n`https://k8s.dashboard.com:30613/dashboard\nhttp://k8s.dashboard.com:30613/dashboard\nhttps://k8s.dashboard.com/dashboard\n`\n```\nBut this URL is working for me: `https://k8s.dashboard.com:30613`\nI have added the minikube IP to hosts files in the Windows machine.\nIngress rule describe the output is as follows:\n```\n`PS C:\\Users\\XXX\\Desktop\\ingress> kubectl describe ingress -n kubernetes-dashboard\nName:             dashboard-ingress\nNamespace:        kubernetes-dashboard\nAddress:          192.168.56.100\nDefault backend:  default-http-backend:80 ()\nRules:\n  Host               Path  Backends\n  ----               ----  --------\n  k8s.dashboard.com\n                     /dashboard   kubernetes-dashboard:443 (172.17.0.4:8443)\nAnnotations:         ingress.kubernetes.io/configuration-snippet: rewrite ^(/dashboard)$ $1/ permanent;\n                     kubernetes.io/ingress.class: nginx\n                     nginx.ingress.kubernetes.io/add-base-url: true\n                     nginx.ingress.kubernetes.io/backend-protocol: HTTPS\n                     nginx.ingress.kubernetes.io/force-ssl-redirect: false\n                     nginx.ingress.kubernetes.io/rewrite-target: /\n                     nginx.ingress.kubernetes.io/secure-backends: true\nEvents:\n  Type    Reason  Age                   From                      Message\n  ----    ------  ----                  ----                      -------\n  Normal  Sync    26m (x16 over 5d20h)  nginx-ingress-controller  Scheduled for sync\n`\n```\nAny help regarding this really helps. Thanks\n`EDITED`\nMy ingress controller logs are as follows:\n```\n`192.168.56.1 - - [16/Jun/2021:06:57:00 +0000] \"GET /dashboard HTTP/2.0\" 200 746 \"-\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.77 Safari/537.36\" 418 0.019 [kubernetes-dashboard-kubernetes-dashboard-443] [] 172.17.0.4:8443 746 0.018 200 1a2793052f70031c6c9fa59b0d4374d1\n192.168.56.1 - - [16/Jun/2021:06:57:00 +0000] \"GET /styles.aa1f928b22a88c391404.css HTTP/2.0\" 404 548 \"https://k8s.dashboard.com/dashboard\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.77 Safari/537.36\" 101 0.002 [upstream-default-backend] [] 127.0.0.1:8181 548 0.002 404 1974258442f8b4c46d8badd1dda3e3f5\n192.168.56.1 - - [16/Jun/2021:06:57:00 +0000] \"GET /runtime.2a456dd93bf6c4890676.js HTTP/2.0\" 404 548 \"https://k8s.dashboard.com/dashboard\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.77 Safari/537.36\" 49 0.008 [upstream-default-backend] [] 127.0.0.1:8181 548 0.007 404 96c17c52e6337f29dd8b2b2b68b088ac\n192.168.56.1 - - [16/Jun/2021:06:57:00 +0000] \"GET /polyfills.f4f05ad675be9638106e.js HTTP/2.0\" 404 548 \"https://k8s.dashboard.com/dashboard\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.77 Safari/537.36\" 40 0.008 [upstream-default-backend] [] 127.0.0.1:8181 548 0.007 404 096ae29cb168523aa9191f27a967e47a\n192.168.56.1 - - [16/Jun/2021:06:57:00 +0000] \"GET /scripts.128068f897fc721c4673.js HTTP/2.0\" 404 548 \"https://k8s.dashboard.com/dashboard\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.77 Safari/537.36\" 38 0.008 [upstream-default-backend] [] 127.0.0.1:8181 548 0.007 404 728f73f75276167b387dc87a69b65a72\n192.168.56.1 - - [16/Jun/2021:06:57:00 +0000] \"GET /en.main.09bf52db2dbc808e7279.js HTTP/2.0\" 404 548 \"https://k8s.dashboard.com/dashboard\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.77 Safari/537.36\" 38 0.014 [upstream-default-backend] [] 127.0.0.1:8181 548 0.014 404 b11e5ae324a828508d488816306399c2\n`\n```\nand this is dashboard logs\n```\n`172.17.0.1 - - [16/Jun/2021:06:59:46 +0000] \"GET / HTTP/1.1\" 200 6 \"\" \"kube-probe/1.20\"\n172.17.0.1 - - [16/Jun/2021:06:59:56 +0000] \"GET / HTTP/1.1\" 200 6 \"\" \"kube-probe/1.20\"\n172.17.0.1 - - [16/Jun/2021:07:00:00 +0000] \"GET /healthz HTTP/1.1\" 200 13 \"\" \"dashboard/v2.2.0\"\n172.17.0.1 - - [16/Jun/2021:07:00:06 +0000] \"GET / HTTP/1.1\" 200 6 \"\" \"kube-probe/1.20\"\n172.17.0.1 - - [16/Jun/2021:07:00:16 +0000] \"GET / HTTP/1.1\" 200 6 \"\" \"kube-probe/1.20\"\n172.17.0.1 - - [16/Jun/2021:07:00:26 +0000] \"GET / HTTP/1.1\" 200 6 \"\" \"kube-probe/1.20\"\n172.17.0.1 - - [16/Jun/2021:07:00:30 +0000] \"GET /healthz HTTP/1.1\" 200 13 \"\" \"dashboard/v2.2.0\"\n172.17.0.1 - - [16/Jun/2021:07:00:36 +0000] \"GET / HTTP/1.1\" 200 6 \"\" \"kube-probe/1.20\"\n{\"level\":\"error\",\"msg\":\"Error scraping node metrics: the server could not find the requested resource (get nodes.metrics.k8s.io)\",\"time\":\"2021-06-16T07:00:41Z\"}\n`\n```",
      "solution": "According to this issue this is a limitation/bug of the kubernetes dashboard.\nThey suggest using this config as a workaround:\n```\n`apiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: kubernetes-dashboard\n  labels:\n    app.kubernetes.io/name: kubernetes-dashboard  \n  annotations:\n    kubernetes.io/ingress.class: nginx\n    # Add https backend protocol support for ingress-nginx\n    nginx.ingress.kubernetes.io/backend-protocol: \"HTTPS\"\n    nginx.ingress.kubernetes.io/configuration-snippet: |\n      proxy_set_header Accept-Encoding \"\";\n      sub_filter '' '';\n      sub_filter_once on;\n    nginx.ingress.kubernetes.io/rewrite-target: /$2\nspec:\n  rules:\n    - host: my.example.com\n      http:\n        paths:\n          - path: /dashboard(/|$)(.*)\n            backend:\n              serviceName: kubernetes-dashboard\n              servicePort: 443\n`\n```",
      "question_score": 3,
      "answer_score": 5,
      "created_at": "2021-06-15T13:11:30",
      "url": "https://stackoverflow.com/questions/67985163/kubernetes-dashboard-not-accessible-when-providing-path-in-ingress"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "minikube",
      "question_id": 69771548,
      "title": "How to run minikube inside a docker container?",
      "problem": "I intend to test a non-trivial Kubernetes setup as part of CI and wish to run the full system before CD. I cannot run `--privileged` containers and am running the docker container as a sibling to the host using `docker run -v /var/run/docker.sock:/var/run/docker.sock`\nThe basic docker setup seems to be working on the container:\n```\n`linuxbrew@03091f71a10b:~$ docker run hello-world\n\nHello from Docker!\nThis message shows that your installation appears to be working correctly.\n`\n```\nHowever, minikube fails to start inside the docker container, reporting connection issues:\n```\n`linuxbrew@03091f71a10b:~$ minikube start --alsologtostderr -v=7\nI1029 15:07:41.274378    2183 out.go:298] Setting OutFile to fd 1 ...\nI1029 15:07:41.274538    2183 out.go:345] TERM=xterm,COLORTERM=, which probably does not support color\n...\n...\n...\nI1029 15:20:27.040213     197 main.go:130] libmachine: Using SSH client type: native\nI1029 15:20:27.040541     197 main.go:130] libmachine: &{{{ 0 [] [] []} docker [0x7a1e20] 0x7a4f00   [] 0s} 127.0.0.1 49350  }\nI1029 15:20:27.040593     197 main.go:130] libmachine: About to run SSH command:\nsudo hostname minikube && echo \"minikube\" | sudo tee /etc/hostname\nI1029 15:20:27.040992     197 main.go:130] libmachine: Error dialing TCP: dial tcp 127.0.0.1:49350: connect: connection refused                                                  \n`\n```\nThis is despite the network being linked and the port being properly forwarded:\n```\n`linuxbrew@51fbce78731e:~$ docker container ls\nCONTAINER ID   IMAGE                                 COMMAND                  CREATED         STATUS         PORTS                                                                                                                                  NAMES\n93c35cec7e6f   gcr.io/k8s-minikube/kicbase:v0.0.27   \"/usr/local/bin/entr\u2026\"   2 minutes ago   Up 2 minutes   127.0.0.1:49350->22/tcp, 127.0.0.1:49351->2376/tcp, 127.0.0.1:49348->5000/tcp, 127.0.0.1:49349->8443/tcp, 127.0.0.1:49347->32443/tcp   minikube\n51fbce78731e   7f7ba6fd30dd                          \"/bin/bash\"              8 minutes ago   Up 8 minutes                                                                                                                                          bpt-ci\nlinuxbrew@51fbce78731e:~$ docker network ls\nNETWORK ID     NAME       DRIVER    SCOPE\n1e800987d562   bridge     bridge    local\naa6b2909aa87   host       host      local\nd4db150f928b   kind       bridge    local\na781cb9345f4   minikube   bridge    local\n0a8c35a505fb   none       null      local\nlinuxbrew@51fbce78731e:~$ docker network connect a781cb9345f4 93c35cec7e6f\nError response from daemon: endpoint with name minikube already exists in network minikube\n`\n```\nThe minikube container seems to be alive and well when trying to `curl` from the host and even `ssh`is responding:\n```\n`mastercook@linuxkitchen:~$ curl https://127.0.0.1:49350\ncurl: (35) OpenSSL SSL_connect: SSL_ERROR_SYSCALL in connection to 127.0.0.1:49350 \n\nmastercook@linuxkitchen:~$ ssh root@127.0.0.1 -p 49350\nThe authenticity of host '[127.0.0.1]:49350 ([127.0.0.1]:49350)' can't be established.\nED25519 key fingerprint is SHA256:0E41lExrrezFK1QXULaGHgk9gMM7uCQpLbNPVQcR2Ec.\nThis key is not known by any other names\n`\n```\nWhat am I missing and how can I make minikube properly discover the correctly working minikube container?",
      "solution": "You can run `minikube` in docker in docker container. It will use `docker` driver.\n```\n`docker run --name dind -d --privileged docker:20.10.17-dind \ndocker exec -it dind sh\n/ # wget https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64\n/ # mv minikube-linux-amd64 minikube\n/ # chmod +x minikube \n/ # ./minikube start --force\n...\n* Done! kubectl is now configured to use \"minikube\" cluster and \"default\" namespace by default\n/ # ./minikube kubectl -- run --image=hello-world\n/ # ./minikube kubectl -- logs pod/hello\n\nHello from Docker!\n...\n`\n```\nAlso, note that `--force` is for running `minikube` using `docker` driver as `root` which we shouldn't do according `minikube` instructions.",
      "question_score": 3,
      "answer_score": 2,
      "created_at": "2021-10-29T17:39:31",
      "url": "https://stackoverflow.com/questions/69771548/how-to-run-minikube-inside-a-docker-container"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "minikube",
      "question_id": 69094846,
      "title": "Cannot connect to PostgreSQL in Helm as Dependency in Minikube",
      "problem": "Working Setup\nI can run the following without any issues:\n\nInstall PostgreSQL using: `helm install postgresql bitnami/postgresql`\nGet the appropriate password: `export POSTGRES_PASSWORD=$(kubectl get secret --namespace default postgresql -o jsonpath=\"{.data.postgresql-password}\" | base64 --decode)`\nConnect using the test command: `kubectl run postgresql-client --rm --tty -i --restart='Never' --namespace default --image docker.io/bitnami/postgresql:11.13.0-debian-10-r25 --env=\"PGPASSWORD=$POSTGRES_PASSWORD\" --command -- psql --host postgresql -U postgres -d postgres -p 5432`\nList all tables at the prompt using `\\l`\nExit and remove the deployment: `helm delete postgresql`\n\nWhat Is Not Working\nI created a new Helm chart of my own and have this in my Chart.yaml file with a completely blank values.yaml file:\n`apiVersion: v2\nname: test\ndescription: testing\ntype: application\nversion: 0.0.1\nappVersion: 0.0.1\n\ndependencies:\n- name: postgresql\n  version: 10.9.5\n  repository: https://charts.bitnami.com/bitnami\n`\nHelm install works fine using: `helm install test .`\nOnce that is set up I can get the password almost the same way:\n`export POSTGRES_PASSWORD=$(kubectl get secret test-postgresql -o jsonpath=\"{.data.postgresql-password}\" | base64 --decode)\necho ${POSTGRES_PASSWORD}\n`\nWhen I run the launch command I get an error:\n`kubectl run test-postgresql-client --rm --tty -i --restart='Never' \\\n      --image docker.io/bitnami/postgresql:11.13.0-debian-10-r25 \\\n      --env=\"PGPASSWORD=$POSTGRES_PASSWORD\" \\\n      --command \\\n      -- psql --host test-postgresql -U postgres -d postgres -p 5432\n`\nThe error is:\n\nsql: FATAL:  password authentication failed for user \"postgres\"\npod \"test-postgresql-client\" deleted\npod default/test-postgresql-client terminated (Error)\n\nI updated my values.yaml to include:\n`global:\n  # Dependency: PostgreSQL\n  postgresql:\n    postgresqlDatabase: my-test\n    postgresqlUsername: my-test\n    postgresqlPassword: my-test\n`\nIf I go into the pod that launches and check the secrets, I see it picks up the values properly, but I get a similar error when I try to connect using those credentials.\nWhat am I missing in the chart setup to deploy this as a dependency and be able to connect?",
      "solution": "I spent hours trying to debug the issue.\nIf you are running the helm chart for the `second` time, make sure you have deleted any existing PV, PVCs from previous installations.\n\nWARNING: The configured password will be ignored on new installation in case when previous Posgresql release was deleted through the helm command. In that case, old PVC will have an old password, and setting it through helm won't take effect. Deleting persistent volumes (PVs) will solve the issue.\n\nDelete the PV and PVC if you are getting the error: \"psql: error: could not connect to server: FATAL: password authentication failed for user \"username\"\n\nI guess, this should be highlighted more clearly in the documentation\nYou can try the following to test\n`$ export POSTGRES_USER_PASSWORD=''   # Admin user : postgres\n$ export KEYCLOAK_USER_PASSWORD=''    # Custom user : keycloak\n\n$ helm install keycloak-db postgresql \\\n--namespace keycloak \\\n--create-namespace \\\n--set global.postgresql.auth.postgresPassword=\"$POSTGRES_USER_PASSWORD\" \\\n--set global.postgresql.auth.username=\"keycloak\" \\\n--set global.postgresql.auth.password=\"$KEYCLOAK_USER_PASSWORD\" \\\n--set global.postgresql.auth.database=\"keycloak_db\" \n\n$ export POSTGRES_PASSWORD=$(kubectl get secret --namespace keycloak keycloak-db-postgresql -o jsonpath=\"{.data.password}\" | base64 -d)\n\n$ kubectl run keycloak-db-postgresql-client --rm --tty -i --restart='Never' --namespace keycloak --image docker.io/bitnami/postgresql:15.3.0-debian-11-r17 --env=\"PGPASSWORD=$POSTGRES_PASSWORD\" \\\n      --command -- psql --host keycloak-db-postgresql -U keycloak -d keycloak_db -p 5432\nIf you don't see a command prompt, try pressing enter.\n\nkeycloak_db=> \n`",
      "question_score": 3,
      "answer_score": 4,
      "created_at": "2021-09-07T23:44:32",
      "url": "https://stackoverflow.com/questions/69094846/cannot-connect-to-postgresql-in-helm-as-dependency-in-minikube"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "minikube",
      "question_id": 70237546,
      "title": "Minikube dashboard ingress",
      "problem": "Im trying to make an ingress for the minikube dashboard using the embedded dashboard internal service.\nI enabled both `ingress` and `dashboard` minikube addons.\nI also wrote this ingress YAML file :\n```\n`apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: dashboard-ingress\n  namespace: kubernetes-dashboard\nspec:\n  rules:\n  - host: dashboard.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: kubernetes-dashboard\n            port: \n              number: 80\n`\n```\nMy Ingress is being created well as u can see :\n```\n`NAME                CLASS   HOSTS           ADDRESS     PORTS   AGE\ndashboard-ingress   nginx   dashboard.com   localhost   80      15s\n`\n```\nI edited my `/etc/hosts` to add this line : `127.0.0.1    dashboard.com`.\nNow im trying to access the dashboard trough `dashboard.com`. But it's not working.\n`kubectl describe ingress dashboard-ingress -n kubernetes-dashboard` gives me this :\n```\n`Name:             dashboard-ingress\nNamespace:        kubernetes-dashboard\nAddress:          localhost\nDefault backend:  default-http-backend:80 ()\nRules:\n  Host           Path  Backends\n  ----           ----  --------\n  dashboard.com  \n                 /   kubernetes-dashboard:80 (172.17.0.4:9090)\nAnnotations:     \nEvents:\n  Type    Reason  Age                From                      Message\n  ----    ------  ----               ----                      -------\n  Normal  Sync    14m (x2 over 14m)  nginx-ingress-controller  Scheduled for sync\n`\n```\nI don't really understand what `` means, but maybe my issue comes from this.\n`kubectl get pods -n ingress-nginx` result :\n```\n`NAME                                        READY   STATUS      RESTARTS   AGE\ningress-nginx-admission-create--1-8krc7     0/1     Completed   0          100m\ningress-nginx-admission-patch--1-qblch      0/1     Completed   1          100m\ningress-nginx-controller-5f66978484-hvk9j   1/1     Running     0          100m\n`\n```\nLogs for nginx-controller pod :\n```\n`-------------------------------------------------------------------------------\nNGINX Ingress controller\n  Release:       v1.0.4\n  Build:         9b78b6c197b48116243922170875af4aa752ee59\n  Repository:    https://github.com/kubernetes/ingress-nginx\n  nginx version: nginx/1.19.9\n\n-------------------------------------------------------------------------------\n\nW1205 19:33:42.303136       7 client_config.go:615] Neither --kubeconfig nor --master was specified.  Using the inClusterConfig.  This might not work.\nI1205 19:33:42.303261       7 main.go:221] \"Creating API client\" host=\"https://10.96.0.1:443\"\nI1205 19:33:42.319750       7 main.go:265] \"Running in Kubernetes cluster\" major=\"1\" minor=\"22\" git=\"v1.22.3\" state=\"clean\" commit=\"c92036820499fedefec0f847e2054d824aea6cd1\" platform=\"linux/amd64\"\nI1205 19:33:42.402223       7 main.go:104] \"SSL fake certificate created\" file=\"/etc/ingress-controller/ssl/default-fake-certificate.pem\"\nI1205 19:33:42.413477       7 ssl.go:531] \"loading tls certificate\" path=\"/usr/local/certificates/cert\" key=\"/usr/local/certificates/key\"\nI1205 19:33:42.420838       7 nginx.go:253] \"Starting NGINX Ingress controller\"\nI1205 19:33:42.424731       7 event.go:282] Event(v1.ObjectReference{Kind:\"ConfigMap\", Namespace:\"ingress-nginx\", Name:\"ingress-nginx-controller\", UID:\"f2d27cc7-b103-490f-807f-18ccaa614e6b\", APIVersion:\"v1\", ResourceVersion:\"664\", FieldPath:\"\"}): type: 'Normal' reason: 'CREATE' ConfigMap ingress-nginx/ingress-nginx-controller\nI1205 19:33:42.427171       7 event.go:282] Event(v1.ObjectReference{Kind:\"ConfigMap\", Namespace:\"ingress-nginx\", Name:\"tcp-services\", UID:\"e174971d-df1c-4826-85d4-194598ab1912\", APIVersion:\"v1\", ResourceVersion:\"665\", FieldPath:\"\"}): type: 'Normal' reason: 'CREATE' ConfigMap ingress-nginx/tcp-services\nI1205 19:33:42.427195       7 event.go:282] Event(v1.ObjectReference{Kind:\"ConfigMap\", Namespace:\"ingress-nginx\", Name:\"udp-services\", UID:\"0ffc7ee9-2435-4005-983d-ed41aac1c9aa\", APIVersion:\"v1\", ResourceVersion:\"666\", FieldPath:\"\"}): type: 'Normal' reason: 'CREATE' ConfigMap ingress-nginx/udp-services\nI1205 19:33:43.622661       7 nginx.go:295] \"Starting NGINX process\"\nI1205 19:33:43.622746       7 leaderelection.go:243] attempting to acquire leader lease ingress-nginx/ingress-controller-leader...\nI1205 19:33:43.623402       7 nginx.go:315] \"Starting validation webhook\" address=\":8443\" certPath=\"/usr/local/certificates/cert\" keyPath=\"/usr/local/certificates/key\"\nI1205 19:33:43.623683       7 controller.go:152] \"Configuration changes detected, backend reload required\"\nI1205 19:33:43.643547       7 leaderelection.go:253] successfully acquired lease ingress-nginx/ingress-controller-leader\nI1205 19:33:43.643635       7 status.go:84] \"New leader elected\" identity=\"ingress-nginx-controller-5f66978484-hvk9j\"\nI1205 19:33:43.691342       7 controller.go:169] \"Backend successfully reloaded\"\nI1205 19:33:43.691395       7 controller.go:180] \"Initial sync, sleeping for 1 second\"\nI1205 19:33:43.691435       7 event.go:282] Event(v1.ObjectReference{Kind:\"Pod\", Namespace:\"ingress-nginx\", Name:\"ingress-nginx-controller-5f66978484-hvk9j\", UID:\"55d45c26-eda7-4b37-9b04-5491cde39fd4\", APIVersion:\"v1\", ResourceVersion:\"697\", FieldPath:\"\"}): type: 'Normal' reason: 'RELOAD' NGINX reload triggered due to a change in configuration\nI1205 21:06:47.402756       7 main.go:101] \"successfully validated configuration, accepting\" ingress=\"dashboard-ingress/kubernetes-dashboard\"\nI1205 21:06:47.408929       7 store.go:371] \"Found valid IngressClass\" ingress=\"kubernetes-dashboard/dashboard-ingress\" ingressclass=\"nginx\"\nI1205 21:06:47.409343       7 controller.go:152] \"Configuration changes detected, backend reload required\"\nI1205 21:06:47.409352       7 event.go:282] Event(v1.ObjectReference{Kind:\"Ingress\", Namespace:\"kubernetes-dashboard\", Name:\"dashboard-ingress\", UID:\"be1ebfe9-fdb3-4d0c-925b-0c206cd0ece3\", APIVersion:\"networking.k8s.io/v1\", ResourceVersion:\"5529\", FieldPath:\"\"}): type: 'Normal' reason: 'Sync' Scheduled for sync\nI1205 21:06:47.458273       7 controller.go:169] \"Backend successfully reloaded\"\nI1205 21:06:47.458445       7 event.go:282] Event(v1.ObjectReference{Kind:\"Pod\", Namespace:\"ingress-nginx\", Name:\"ingress-nginx-controller-5f66978484-hvk9j\", UID:\"55d45c26-eda7-4b37-9b04-5491cde39fd4\", APIVersion:\"v1\", ResourceVersion:\"697\", FieldPath:\"\"}): type: 'Normal' reason: 'RELOAD' NGINX reload triggered due to a change in configuration\nI1205 21:07:43.654037       7 status.go:300] \"updating Ingress status\" namespace=\"kubernetes-dashboard\" ingress=\"dashboard-ingress\" currentValue=[] newValue=[{IP: Hostname:localhost Ports:[]}]\nI1205 21:07:43.660598       7 event.go:282] Event(v1.ObjectReference{Kind:\"Ingress\", Namespace:\"kubernetes-dashboard\", Name:\"dashboard-ingress\", UID:\"be1ebfe9-fdb3-4d0c-925b-0c206cd0ece3\", APIVersion:\"networking.k8s.io/v1\", ResourceVersion:\"5576\", FieldPath:\"\"}): type: 'Normal' reason: 'Sync' Scheduled for sync\n`\n```\nAnyone has a clue on how i can solve my problem ?\n(Im using minikube v1.24.0)\nRegards,",
      "solution": "I have also faced the same issue with minikube(v1.25.1) running in my local.\n`kubectl get ingress -n kubernetes-dashboard`\n```\n`NAME                CLASS   HOSTS           ADDRESS     PORTS   AGE\ndashboard-ingress   nginx   dashboard.com   localhost   80      34m\n`\n```\nAfter  debug I have found this.\n\"If you are running Minikube locally, use minikube ip to get the external IP. The IP address displayed within the ingress list will be the internal IP\".\nRun this command\n```\n`minikube ip\nXXX.XXX.64.2\n`\n```\nadd this ip in the host file,after that I am able to access dashboard.com",
      "question_score": 3,
      "answer_score": 3,
      "created_at": "2021-12-05T19:56:37",
      "url": "https://stackoverflow.com/questions/70237546/minikube-dashboard-ingress"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "minikube",
      "question_id": 78663883,
      "title": "Kubectl error : [memcache.go:265] couldn&#39;t get current server API group list",
      "problem": "I'm new to Kubernetes and I'm trying to set up a local cluster using k3d. Here are the steps I've taken:\n\nInstalled k3d and kubectl.\nCreated a cluster using k3d cluster create -a 2.\nSwitched to the cluster context using kubectl config use-context k3d-k3s-default.\nHowever, when I run kubectl cluster-info to check the connection to the cluster, I encounter the following error:\n\n```\n`couldn't get current server API group list: Get \"https://host.docker.internal:55345/api?timeout=32s\": dial tcp 192.168.1.24:55345: connectex: A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond.\n`\n```\nI'm unsure how to resolve this issue. Can anyone help me understand what might be causing this error and how to fix it? Thank you in advance!",
      "solution": "This happens usually because of dns or networking. The issue is with the `host.docker.internal`. you can change `host.docker.internal` to `127.0.0.1` in `$HOME/.kube/config`, you can reach it. Or by creating a new cluster like\n```\n`k3d cluster create mycluster --api-port localhost:5443\n`\n```",
      "question_score": 3,
      "answer_score": 2,
      "created_at": "2024-06-24T19:33:53",
      "url": "https://stackoverflow.com/questions/78663883/kubectl-error-memcache-go265-couldnt-get-current-server-api-group-list"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "minikube",
      "question_id": 76502195,
      "title": "HorizontalPodAutoscaler not found on minikube when installing kubeflow",
      "problem": "I installed the latest minikube version according to its website (https://minikube.sigs.k8s.io/docs/start/):\n```\n`curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64\nsudo install minikube-linux-amd64 /usr/local/bin/minikube\n`\n```\nThe version which was installed is:\n```\n`chris@laptop1:~/tmp$ minikube version\nminikube version: v1.30.1\ncommit: 08896fd1dc362c097c925146c4a0d0dac715ace0\n`\n```\nThe versions for kubernetes are as follows:\n```\n`chris@laptop1:~/tmp$ kubectl version --short\nClient Version: v1.27.3\nKustomize Version: v5.0.1\nServer Version: v1.26.3\n`\n```\nI installed kustomize by myself to the mentioned versions, as this is required according to the kubeflow documentation (https://github.com/kubeflow/manifests#installation):\n```\n`chris@laptop1:~/tmp$ kustomize version\nv5.0.0\n`\n```\nHowever, when I installed `kubeflow`, the following error was shown:\n```\n`chris@laptop1:~/tmp/kubeflow/manifests$ while ! kustomize build example | awk '!/well-defined/' | kubectl apply -f -; do echo \"Retrying to apply resources\"; sleep 10; done\n# Warning: 'patchesStrategicMerge' is deprecated. Please use 'patches' instead. Run 'kustomize edit fix' to update your Kustomization automatically.\n\n... lots of deprecated warnings\n\nvalidatingwebhookconfiguration.admissionregistration.k8s.io/validation.webhook.serving.knative.dev configured\nerror: resource mapping not found for name: \"webhook\" namespace: \"knative-serving\" from \"STDIN\": no matches for kind \"HorizontalPodAutoscaler\" in version \"autoscaling/v2beta2\"\nensure CRDs are installed first\nRetrying to apply resources\n`\n```\nIn `common/knative/knative-serving/base/upstream/serving-core.yaml` there were two api versions for `HorizontalPodAutoscaler`: `autoscaling/v2beta2` and `autoscaling/v2`. I changed `v2beta`to `v2`and then the installation of kubeflow completed.\nI am wondering why there are two api versions mentioned in kubeflow, or its dependency knative. Is this simply a bug or am I missing something here?\nFurthermore I could not figure out how to fix the deprecated warnings.",
      "solution": "Looking through the Kubernetes release notes I noticed this:\n```\n`The autoscaling/v2beta2 API version of HorizontalPodAutoscaler is no longer served as of v1.26.\n\nMigrate manifests and API clients to use the autoscaling/v2 API version, available since v1.23.\nAll existing persisted objects are accessible via the new API\nv1.25\n`\n```\nAnd the Kubeflow release notes are saying v1.7 (the current latest) is validated only on v1.24 and v1.25.\nSo it looks like you might want to try downgrading to minikkube v1.25 if you want to get Kubeflow to work as-is, otherwise you'll need to hack around with the manifests until a new Kubeflow comes out like you did.\nThis is in fact a Kubeflow bug that will need to be fixed as they roll forward and support newer K8s.",
      "question_score": 3,
      "answer_score": 2,
      "created_at": "2023-06-18T21:22:08",
      "url": "https://stackoverflow.com/questions/76502195/horizontalpodautoscaler-not-found-on-minikube-when-installing-kubeflow"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "minikube",
      "question_id": 68833218,
      "title": "Accessing minikube dashboard - (Ubuntu20.04 server)VM running on Windows 10 host with VirtualBox 6.1",
      "problem": "I'm trying to access minikube dashboard from host OS (Windows 10).\nMinikube is running on my virtual machine Ubuntu 20.04 server.\nThe host is Windows 10 and I use VirtualBox to run my VM.\nThese are the commands I ran on Ubuntu:\n```\n`tomas@ubuntu20:~$ minikube start\n* minikube v1.22.0 on Ubuntu 20.04 (vbox/amd64)\n* Using the docker driver based on existing profile\n* Starting control plane node minikube in cluster minikube\n* Pulling base image ...\n* Updating the running docker \"minikube\" container ...\n* Preparing Kubernetes v1.21.2 on Docker 20.10.7 ...\n* Verifying Kubernetes components...\n  - Using image gcr.io/k8s-minikube/storage-provisioner:v5\n  - Using image kubernetesui/dashboard:v2.1.0\n  - Using image kubernetesui/metrics-scraper:v1.0.4\n* Enabled addons: storage-provisioner, default-storageclass, dashboard\n* kubectl not found. If you need it, try: 'minikube kubectl -- get pods -A'\n* Done! kubectl is now configured to use \"minikube\" cluster and \"default\" namespace by default\n\ntomas@ubuntu20:~$ kubectl get po -A\n    \nCommand 'kubectl' not found, but can be installed with:\n    \nsudo snap install kubectl\n\ntomas@ubuntu20:~$ minikube kubectl -- get po -A\nNAMESPACE              NAME                                         READY   STATUS    RESTARTS   AGE\nkube-system            coredns-558bd4d5db-9p9ck                     1/1     Running   2          72m\nkube-system            etcd-minikube                                1/1     Running   2          72m\nkube-system            kube-apiserver-minikube                      1/1     Running   2          72m\nkube-system            kube-controller-manager-minikube             1/1     Running   2          72m\nkube-system            kube-proxy-xw766                             1/1     Running   2          72m\nkube-system            kube-scheduler-minikube                      1/1     Running   2          72m\nkube-system            storage-provisioner                          1/1     Running   4          72m\nkubernetes-dashboard   dashboard-metrics-scraper-7976b667d4-r9k7t   1/1     Running   2          54m\nkubernetes-dashboard   kubernetes-dashboard-6fcdf4f6d-c7kwf         1/1     Running   2          54m\n`\n```\nAnd then I open another terminal window and I run:\n```\n`tomas@ubuntu20:~$ minikube dashboard\n* Verifying dashboard health ...\n* Launching proxy ...\n* Verifying proxy health ...\n* Opening http://127.0.0.1:36337/api/v1/namespaces/kubernetes-dashboard/services/http:kubernetes-dashboard:/proxy/ in your default browser...\n  http://127.0.0.1:36337/api/v1/namespaces/kubernetes-dashboard/services/http:kubernetes-dashboard:/proxy/\n`\n```\nNow on my Windows 10 host machine I go to web browser type in:\n```\n`http://127.0.0.1:36337/api/v1/namespaces/kubernetes-dashboard/services/http:kubernetes-dashboard:/proxy/\n`\n```\nBut I get error:\n```\n`This site can\u2019t be reached 127.0.0.1 refused to connect.\n`\n```\nHow can I access minikube dashboard from my host OS web browser?",
      "solution": "Reproduction\nI reproduced this behaviour on Windows 10 and ubuntu 18.04 LTS virtual machine running using `VirtualBox`.\nI have tried both `minikube drivers`: docker and none (last one means that all kubernetes components will be run on localhost) and behaviour is the same.\nWhat happens\nMinikube is designed to be used on localhost machine. When `minikube dashboard` command is run, minikube downloads images (metrics scraper and dashboard itsefl), launches them, test if they are healthy and then create proxy which is run on `localhost`. It can't accept connections outside of the virtual machine (in this case it's Windows host to ubuntu VM).\nThis can be checked by running `netstat` command (cut off some not useful output):\n```\n`$ minikube dashboard\n\ud83d\udd0c  Enabling dashboard ...\n\ud83d\ude80  Launching proxy ...\n\ud83e\udd14  Verifying proxy health ...\n\ud83d\udc49  http://127.0.0.1:36317/api/v1/namespaces/kubernetes-dashboard/services/http:kubernetes-dashboard:/proxy/\n\n$ sudo netstat -tlpn\nActive Internet connections (only servers)\nProto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name     \ntcp        0      0 127.0.0.1:36317         0.0.0.0:*               LISTEN      461195/kubectl\n`\n```\nHow to resolve it\nOnce `minikube dashboard` command has been run, kubernetes dashboard will remain running in `kubernetes-dashboard` namespace.\nProxy to it should be open manually with following command:\n```\n`kubectl proxy --address='0.0.0.0' &\n`\n```\nOr if you don't have `kubectl` installed on your machine:\n```\n`minikube kubectl proxy -- --address='0.0.0.0' &\n`\n```\nIt will start a proxy to kubernetes api server on port `8001` and will serve on all addresses (it can be changed to default Virtual box NAT address `10.2.0.15`).\nNext step is to add `port-forwarding` in VirtualBox.\nGo to your virtual machine -> settings -> network -> NAT -> advanced -> port-forwarding\nAdd a new rule:\n\nhost IP = 127.0.0.1\nhost port = any free one, e.g. I used 8000\nguest IP = can be left empty\nguest port = 8001 (where proxy is listening to)\n\nNow you can go to your browser on Windows host, paste the URL, correct the port which was assigned in `host port` and it will work:\n```\n`http://127.0.0.1:8000/api/v1/namespaces/kubernetes-dashboard/services/http:kubernetes-dashboard:/proxy/\n`\n```\nUseful links:\n\nkubectl proxy command\nKubernetes dashboard",
      "question_score": 3,
      "answer_score": 2,
      "created_at": "2021-08-18T15:20:14",
      "url": "https://stackoverflow.com/questions/68833218/accessing-minikube-dashboard-ubuntu20-04-servervm-running-on-windows-10-host"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "minikube",
      "question_id": 66355331,
      "title": "Deployment cannot find PVC on minikube",
      "problem": "I am practicing making PV and PVC with Minikube. But I encountered an error that my InfluxDB deployment couldn't find `influxdb-pvc` and I can't solve it.\nI check the message at the top of the event, I can see that my PVC cannot be found. Therefore, I checked the status of PersistentVolumeClaim.\nAs far as I know, if the STATUS of `influxdb-pv` and `influxdb-pvc` is Bound, it is normally created and Deployment should be able to find `influxdb-pvc`. I don't know what's going on... Please help me \ud83d\ude22\n\nThe following is a description of Pod:\n```\n`> kubectl describe pod influxdb-5b769454b8-pksss\n\nName:         influxdb-5b769454b8-pksss\nNamespace:    ft-services\nPriority:     0\nNode:         minikube/192.168.49.2\nStart Time:   Thu, 25 Feb 2021 01:14:25 +0900\nLabels:       app=influxdb\n              pod-template-hash=5b769454b8\nAnnotations:  \nStatus:       Running\nIP:           172.17.0.5\nIPs:\n  IP:           172.17.0.5\nControlled By:  ReplicaSet/influxdb-5b769454b8\nContainers:\n  influxdb:\n    Container ID:   docker://be2eec32cca22ea84f4a0034f42668c971fefe62e361f2a4d1a74d92bfbf4d78\n    Image:          service_influxdb\n    Image ID:       docker://sha256:50693dcc4dda172f82c0dcd5ff1db01d6d90268ad2b0bd424e616cb84da64c6b\n    Port:           8086/TCP\n    Host Port:      0/TCP\n    State:          Waiting\n      Reason:       CrashLoopBackOff\n    Last State:     Terminated\n      Reason:       Completed\n      Exit Code:    0\n      Started:      Thu, 25 Feb 2021 01:30:40 +0900\n      Finished:     Thu, 25 Feb 2021 01:30:40 +0900\n    Ready:          False\n    Restart Count:  8\n    Environment Variables from:\n      influxdb-secret  Secret  Optional: false\n    Environment:       \n    Mounts:\n      /var/lib/influxdb from var-lib-influxdb (rw)\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-lfzz9 (ro)\nConditions:\n  Type              Status\n  Initialized       True\n  Ready             False\n  ContainersReady   False\n  PodScheduled      True\nVolumes:\n  var-lib-influxdb:\n    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)\n    ClaimName:  influxdb-pvc\n    ReadOnly:   false\n  default-token-lfzz9:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-lfzz9\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  \nTolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type     Reason            Age                 From               Message\n  ----     ------            ----                ----               -------\n  Warning  FailedScheduling  20m (x2 over 20m)   default-scheduler  0/1 nodes are available: 1 persistentvolumeclaim \"influxdb-pvc\" not found.\n  Normal   Scheduled         20m                 default-scheduler  Successfully assigned ft-services/influxdb-5b769454b8-pksss to minikube\n  Normal   Pulled            19m (x5 over 20m)   kubelet            Container image \"service_influxdb\" already present on machine\n  Normal   Created           19m (x5 over 20m)   kubelet            Created container influxdb\n  Normal   Started           19m (x5 over 20m)   kubelet            Started container influxdb\n  Warning  BackOff           43s (x93 over 20m)  kubelet            Back-off restarting failed container\n`\n```\nThe following is status information for PV and PVC:\n```\n`> kubectl get pv,pvc\nNAME                           CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM                      STORAGECLASS   REASON   AGE\npersistentvolume/influxdb-pv   10Gi       RWO            Recycle          Bound       ft-services/influxdb-pvc   influxdb                104m\n\nNAME                                 STATUS   VOLUME        CAPACITY   ACCESS MODES   STORAGECLASS   AGE\npersistentvolumeclaim/influxdb-pvc   Bound    influxdb-pv   10Gi       RWO            influxdb       13m\n`\n```\n\nI proceeded with the setting in the following order.\n\nCreate a namespace.\n\n```\n`kubectl create namespace ft-services\nkubectl config set-context --current --namespace=ft-services\n`\n```\n\nApply my config files: `influxdb-deployment.yaml`, `influxdb-secret.yaml`, `influxdb-service.yaml`, `influxdb-volume.yaml`\n\ninfluxdb-deployment.yaml:\n```\n`apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: influxdb\n  labels:\n    app: influxdb\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: influxdb\n  template:\n    metadata:\n      labels:\n        app: influxdb\n    spec:\n      containers:\n      - name: influxdb\n        image: service_influxdb\n        imagePullPolicy: Never\n        ports:\n        - containerPort: 8086\n        envFrom:\n        - secretRef:\n            name: influxdb-secret\n        volumeMounts:\n        - mountPath: /var/lib/influxdb\n          name: var-lib-influxdb\n      volumes:\n      - name: var-lib-influxdb\n        persistentVolumeClaim:\n          claimName: influxdb-pvc\n`\n```\ninfluxdb-volume.yaml\n```\n`apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: influxdb-pv\n  labels:\n    app: influxdb\nspec:\n  storageClassName: influxdb\n  claimRef:\n    namespace: ft-services\n    name: influxdb-pvc\n  capacity:\n    storage: 10Gi\n  accessModes:\n  - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Recycle\n  hostPath:\n    path: \"/mnt/influxdb\"\n    type: DirectoryOrCreate\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: influxdb-pvc\n  labels:\n    app: influxdb\nspec:\n  storageClassName: influxdb\n  accessModes:\n  - ReadWriteOnce\n  resources:\n    requests:\n      storage: 1Gi\n`\n```\n\nBuild my docker image: `service_influxdb`\n\nDockerfile:\n```\n`FROM alpine:3.13.1\n\nRUN apk update && apk upgrade --ignore busybox && \\\n    apk add \\\n        influxdb && \\\n    sed -i \"247s/  #/  /\" /etc/influxdb.conf && \\\n    sed -i \"256s/  #/  /\" /etc/influxdb.conf\n\nEXPOSE 8086\n\nENTRYPOINT influxd & /bin/sh\n`\n```\n\nCheck my minikube with dashboard\n\n```\n`> minikube dashboard\n\n0/1 nodes are available: 1 persistentvolumeclaim \"influxdb-pvc\" not found.\nBack-off restarting failed container\n\n`\n```",
      "solution": "I've tested your YAMLs on my Minikube cluster.\nYour configuration is correct, however you missed one small detail. Container based on `alpine` needs to \"do something\" inside, otherwise container exits when its main process exits. Once container did all what was expected/configured, `pod` will be in `Completed` status.\nYour pod is crashing because it starts up then immediately exits, thus Kubernetes restarts and the cycle continues. For more details please check Pod Lifecycle Documentation.\nExamples\nAlpine example:\n```\n`$ kubectl get po alipne-test -w\nNAME          READY   STATUS      RESTARTS   AGE\nalipne-test   0/1     Completed   2          36s\nalipne-test   0/1     CrashLoopBackOff   2          36s\nalipne-test   0/1     Completed          3          54s\nalipne-test   0/1     CrashLoopBackOff   3          55s\nalipne-test   0/1     Completed          4          101s\nalipne-test   0/1     CrashLoopBackOff   4          113s\n`\n```\nNginx example:\n```\n`$ kubectl get po nginx\nNAME    READY   STATUS    RESTARTS   AGE\nnginx   1/1     Running   0          5m23s\n`\n```\nNginx is a webserver based container so it does not need additional `sleep` command.\nYour Current Configuration\nYour pod with influx is created, has nothing to do and exits.\n```\n`$ kubectl get po -w\nNAME                       READY   STATUS             RESTARTS   AGE\ninfluxdb-96bfd697d-wbkt7   0/1     CrashLoopBackOff   4          2m28s\ninfluxdb-96bfd697d-wbkt7   0/1     Completed          5          3m8s\ninfluxdb-96bfd697d-wbkt7   0/1     CrashLoopBackOff   5          3m19s\n`\n```\nSolution\nYou just need add for example `sleep command` to keep container alive. For test I've used `sleep 60` to keep container alive for 60 seconds using below configuration:\n```\n`    spec:\n      containers:\n      - name: influxdb\n        image: service_influxdb\n        imagePullPolicy: Never\n        ports:\n        - containerPort: 8086\n        envFrom:\n        - secretRef:\n            name: influxdb-secret\n        volumeMounts:\n        - mountPath: /var/lib/influxdb\n          name: var-lib-influxdb\n        command: [\"/bin/sh\"]               # additional command\n        args: [\"-c\", \"sleep 60\"]           # args to use sleep 60 command\n`\n```\nAnd output below:\n```\n`$ kubectl get po -w\nNAME                        READY   STATUS    RESTARTS   AGE\ninfluxdb-65dc56f8df-9v76p   1/1     Running   0          7s\ninfluxdb-65dc56f8df-9v76p   0/1     Completed   0          62s\ninfluxdb-65dc56f8df-9v76p   1/1     Running     1          63s\n`\n```\nIt was running for 60 seconds, as `sleep` command was set to `60`. As container fulfill all configured commands inside, it exit and status changed to `Completed`. If you will use commands to keep this container alive, you don't need to use `sleep`.\nPV issues\nAs last part you mention about issue in `Minikube Dashboard`. I was not able to replicate it, but it might be some leftovers from your previous test.\nPlease let me know if you still have issue.",
      "question_score": 3,
      "answer_score": 2,
      "created_at": "2021-02-24T17:53:05",
      "url": "https://stackoverflow.com/questions/66355331/deployment-cannot-find-pvc-on-minikube"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "minikube",
      "question_id": 70965971,
      "title": "How to Connect to kafka on localhost (host machine) from app inside kubernetes (minikube)",
      "problem": "I am trying to connect my springboot app (running inside minikube) to kafka on my localhost (ie, laptop).\nI have tried many things, including headless services, services without selectors, updating minikube \\etc\\hosts, but nothing works yet.\nI get error from spring boot saying `No resolvable bootstrap urls given in bootstrap.servers`\nCan someone please point me to what I am doing wrong?\nMy Headless Service\n`apiVersion: v1\nkind: Service\nmetadata:\n  name: es-local-kafka\n  namespace: demo\nspec:\n  clusterIP: None\n---\napiVersion: v1\nkind: Endpoints\nmetadata:\n  name: es-local-kafka\nsubsets:\n  - addresses:\n      - ip: \"10.0.2.2\"\n    ports:\n      - name: \"kafkabroker1\"\n        port: 9191\n      - name: \"kafkabroker2\"\n        port: 9192\n      - name: \"kafkabroker3\"\n        port: 9193\n`\nMy application properties for kafka:\n`kafka.bootstrap-servers=${LOCALHOST}:9191,${LOCALHOST}:9192,${LOCALHOST}:9193\n`\nMy Config Map:\n`apiVersion: v1\nkind: ConfigMap\nmetadata:\n  creationTimestamp: null\n  name: rr-config\n  namespace: demo\ndata:\n  LOCALHOST: es-local-kafka.demo.svc\n`",
      "solution": "I eventually got a fix, and it's simpler than I thought:\n\nYou need to make sure your kafka broker is bound to 0.0.0.0 instead of 127.0.0.0 (localhost) . By default, in the single node kafka broker setup, this is what is used. I went with this, due to both time constraint, and the fact that this was just for a POC in my local (prod will have a specific dns-able kafka URL anyway, and no such localhost shenanigans needed)\n\nIn the kafka URL in your application properties file, instead of localhost, you need to give ip as as the minikube ip. This is the same ip that you will get if you do the command `minikube ip` :)\n\nRead more about how this works here: https://minikube.sigs.k8s.io/docs/handbook/host-access/",
      "question_score": 3,
      "answer_score": 1,
      "created_at": "2022-02-03T05:32:54",
      "url": "https://stackoverflow.com/questions/70965971/how-to-connect-to-kafka-on-localhost-host-machine-from-app-inside-kubernetes"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "minikube",
      "question_id": 72568590,
      "title": "Error when running the command minikube start",
      "problem": "The following error when I run the command `minikube start` :\n\ud83d\udc4e  Unable to pick a default driver. Here is what was considered, in preference order:\n\u25aa docker: Not healthy: \"docker version --format {{.Server.Os}}-{{.Server.Version}}\" exit status 1: Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get \"http://%2Fvar%2Frun%2Fdocker.sock/v1.24/version\": dial unix /var/run/docker.sock: connect: permission denied\n\u25aa docker: Suggestion: Add your user to the 'docker' group: 'sudo usermod -aG docker $USER && newgrp docker' https://docs.docker.com/engine/install/linux-postinstall/\n\ud83d\udca1  Alternatively you could install one of these drivers:\n\u25aa kvm2: Not installed: exec: \"virsh\": executable file not found in $PATH\n\u25aa vmware: Not installed: exec: \"docker-machine-driver-vmware\": executable file not found in $PATH\n\u25aa podman: Not installed: exec: \"podman\": executable file not found in $PATH\n\u25aa virtualbox: Not installed: unable to find VBoxManage in $PATH\n\u274c  Exiting due to DRV_NOT_HEALTHY: Found driver(s) but none were healthy. See above for suggestions how to fix installed drivers.",
      "solution": "Maybe you need root access to run the command, try adding your user to the docker group with this command:\n```\n`sudo usermod -aG docker $USER && newgrp docker\n`\n```\nafter that restart the terminal and run the command again to check if it works",
      "question_score": 2,
      "answer_score": 10,
      "created_at": "2022-06-10T04:47:32",
      "url": "https://stackoverflow.com/questions/72568590/error-when-running-the-command-minikube-start"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "minikube",
      "question_id": 71717543,
      "title": "K8s Containers dont start ImagePullBackOff, ErrImagePull",
      "problem": "I am trying to start up a couple of containers locally using k8s but container creation is stopped cause of ImagePullBackOff, ErrImagePull.\nThe yaml is fine, tested it on another workstation. And i can pull images using regular docker. But it fails in k8s/minikube environment\nError container logs is\n```\n`Error from server (BadRequest): container \"mongo-express\" in pod \"mongoexpress-deployment-bd7cf697b-nc4h5\" is waiting to start: trying and failing to pull image\n`\n```\nError in minikube dashboard is\n```\n`Failed to pull image \"docker.io/mongo\": rpc error: code = Unknown desc = Error response from daemon: Get \"https://registry-1.docker.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers) \n`\n```\nI tried pulling the image to my local docker cache and running\n```\n`eval $(minikube docker-env)\n`\n```\nBut i keep getting this error. It doesnt see local image repository and it doesnt dowload the image by itself.\nI am 100% sure it has something to do with user access on Fedora. But dont have any idea what to do, and i've been trying to fix this for a couple of days :(.\nPlease help, thank you\nDont know if this helps: I tried using k3s. Image pull is successful, but minikube isnt compatible with it on Fedora.\nAlso... If i try using docker without sudo it doesnt pull images. With sudo it pulls.\nOS is Fedora, and i am using docker, kubernetes, minikube, podman as driver.\n```\n`- linux version\nNAME=\"Fedora Linux\"\nVERSION=\"35 (Workstation Edition)\"\n- kubectl version\nClient Version: version.Info{Major:\"1\", Minor:\"23\", GitVersion:\"v1.23.5\", \n- docker version\nVersion:          20.10.12\n- minikube version\nminikube version: v1.25.2\n\n`\n```\nI am trying to start up locally this yaml file\n```\n`apiVersion: v1\nkind: Secret\nmetadata:\n  name: mongodb-secret\ntype: Opaque\ndata:\n  mongo-root-username: dXNlcm5hbWU=\n  mongo-root-password: cGFzc3dvcmQ=\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: mongodb-configmap\ndata:\n  database_url: mongodb-service\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mongoexpress-deployment\n  labels:\n    app: mongoexpress\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: mongoexpress\n  template:\n    metadata:\n      labels:\n        app: mongoexpress\n    spec:\n      containers:\n        - name: mongo-express\n          image: mongo-express\n          ports:\n            - containerPort: 8081\n          env:\n            - name: ME_CONFIG_MONGODB_ADMINUSERNAME\n              valueFrom:\n                secretKeyRef:\n                  name: mongodb-secret\n                  key: mongo-root-username\n            - name: ME_CONFIG_MONGODB_ADMINPASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: mongodb-secret\n                  key: mongo-root-password\n            - name: ME_CONFIG_MONGODB_SERVER\n              valueFrom:\n                configMapKeyRef:\n                  name: mongodb-configmap\n                  key: database_url\n            - name: WHATEVER\n              value: Someconfig\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mongodb-deployment\n  labels:\n    app: mongodb\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: mongodb\n  template:\n    metadata:\n      labels:\n        app: mongodb\n    spec:\n      containers:\n        - name: mongodb\n          image: mongo\n          ports:\n            - containerPort: 27017\n          env:\n            - name: MONGO_INITDB_ROOT_USERNAME\n              valueFrom:\n                  secretKeyRef:\n                    name: mongodb-secret\n                    key: mongo-root-username\n            - name: MONGO_INITDB_ROOT_PASSWORD\n              valueFrom:\n                  secretKeyRef:\n                    name: mongodb-secret\n                    key: mongo-root-password\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: mongodb-service\nspec:\n  selector:\n    app: mongodb\n  ports:\n    - protocol: TCP\n      port: 27017\n      targetPort: 27017\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: mongoexpress-service\nspec:\n  selector:\n    app: mongoexpress-deployment\n  type: LoadBalancer\n  ports:\n    - protocol: TCP\n      port: 8081\n      targetPort: 8081\n      nodePort: 30000\n`\n```",
      "solution": "Based on the comments, my suggestion is to use the docker driver, since Docker has been installed in the system and is the preferred stable driver.\n`minikube start --driver=docker\n`\nYou can also set this as the default driver.\n`minikube config set driver docker\nminikube start\n`\nThat doesn't explain why it doesn't work with podman, though.",
      "question_score": 2,
      "answer_score": 4,
      "created_at": "2022-04-02T14:30:56",
      "url": "https://stackoverflow.com/questions/71717543/k8s-containers-dont-start-imagepullbackoff-errimagepull"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "minikube",
      "question_id": 77208814,
      "title": "Can&#39;t log into minicube! message launching miniKube has me concerned: &quot;Unable to resolve the Docker CLI context&quot; &quot;The system cannot find the path&quot;",
      "problem": "This command was run on a Windows 11 machine through Visual Studio Code Terminal window and the minikube sart command was on the command line of that Terminal Window.  The \"unable to..\" command has be concerned.  Since Docker is widely used I imagine people here can offer insight.\n```\n`PS C:\\Users\\wmmth\\source\\docker-nodejs-basic-demo-master> `minikube start --driver=docker`\n\n**W0930 18:48:02.301808   16048 main.go:291] Unable to resolve the current Docker CLI context \"default\": context \"default\": context not found: open C:\\Users\\wmmth\\.docker\\contexts\\meta\\37a8eec1ce19687d132fe29051dca629d164e2c4958ba141d5f4133a33f0688f\\meta.json: The system cannot find the path specified.**\n\n\ud83d\ude04  minikube v1.31.2 on Microsoft Windows 11 Home 10.0.22621.2283 Build 22621.2283\n\n\u2728  Using the docker driver based on existing profile\n\n\ud83d\udc4d  Starting control plane node minikube in cluster minikube\n\n\ud83d\ude9c  Pulling base image ...\n\n\ud83c\udfc3  Updating the running docker \"minikube\" container ...\n\n\ud83d\udc33  Preparing Kubernetes v1.27.4 on Docker 24.0.4 ...\n\n\ud83d\udd0e  Verifying Kubernetes components...\n\n    \u25aa Using image gcr.io/k8s-minikube/storage-provisioner:v5\n\n\ud83c\udf1f  Enabled addons: storage-provisioner, default-storageclass\n\n    \ud83c\udfc4  Done! kubectl is now configured to use \"minikube\" cluster and \"default\" namespace by default\n`\n```\nby the way,\n```\n`minikube status\n`\n```\nhas similar results:\n```\n`minikube status \n\n          \n\n W0930 19:11:44.872755    5636 main.go:291] Unable to resolve the \ncurrent Docker CLI context \"default\": context \"default\": context not found: open C:\\Users\\wmmth\\.docker\\contexts\\meta\\37a8eec1ce19687d132fe29051dca629d164e2c4958ba141d5f4133a33f0688f\\meta.json: The system cannot find the \npath specified.\nminikube\ntype: Control Plane\nhost: Running\nkubelet: Running\napiserver: Running\nkubeconfig: Configured\n`\n```",
      "solution": "If you get the following error, your installation of Minikube and Docker Desktop likely has some problems.\n```\n`Unable to resolve the current Docker CLI context \"default\": context \"default\": context not found: open \nC:\\\\Users\\\\wmmth.docker\\\\contexts\\\\meta\\\\37a8eec1ce19687d132fe29051dca629d164e2c4958ba141d5f4133a33f0688f\\\\meta.json: The system cannot find the path specified \n`\n```\nThe below content taken from this doc by Serdal kepil Follow below steps to resolve the above error\n\nThere are four ways we can fix your problem. I hope one of them can\noffer assistance with your issue.\nOption 1: You can use this command to update the default context for Docker.\n```\n`docker context use default \n`\n```\nOption 2: Please try this command if the last one didn't work.\n```\n`kubectl config set-context minikube \n`\n```\nOption 3: You have docker installation  and it has been set to be deployed to Kubernetes. Minikube is not a part of this integration.\nYou can disable this previously enabled Kubernetes from Docker Desktop\nsettings/preferences, uncheck Enable Kubernetes\nOption 4: You can remove docker local meta file. This is the last\noption\nPlease follow these steps below:\n1.Stop Docker Desktop and all containers.\n2.Find the path under your local user:\n```\n`~/.docker/contexts/meta/(some sha256)/meta.json\n`\n```\nUsing Windows it should be on:\n```\n`C:\\Users\\YOUR_USER\\.docker\\contexts\\meta\\(some sha256)\\meta.json\n`\n```\n3. Then directly delete the hashed folder which is some sha256 and    includes a meta.json file.\n\n4. Restart Docker and check again.",
      "question_score": 2,
      "answer_score": 5,
      "created_at": "2023-10-01T01:59:59",
      "url": "https://stackoverflow.com/questions/77208814/cant-log-into-minicube-message-launching-minikube-has-me-concerned-unable-to"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "minikube",
      "question_id": 70366074,
      "title": "Ingress not working from official kubernetes tutorial",
      "problem": "I am following this official k8 ingress tutorial. However I am not able to `curl` the minikube IP address and access the \"web\" application.\n```\n`minikube addons enable ingress\nkubectl create deployment web --image=gcr.io/google-samples/hello-app:1.0\nkubectl expose deployment web --type=NodePort --port=8080\nkubectl apply -f https://k8s.io/examples/service/networking/example-ingress.yaml\n`\n```\nI'm able to curl the result of `minikube service web --url`\n```\n`    curl http://127.0.0.1:64671 \n    Hello, world!\n    Version: 1.0.0\n    Hostname: web-79d88c97d6-8z8tc \n`\n```\nBut not though ingress, with `kubectl apply -f https://k8s.io/examples/service/networking/example-ingress.yaml`\n(I don't have an external IP - just \"localhost\". )\n```\n`NGG282 kubernetes-ingress % kubectl get ingress\nNAME              CLASS   HOSTS   ADDRESS     PORTS   AGE\nexample-ingress   nginx   *       localhost   80      66m\n`\n```\nThis seems to be normal with minikube. Trying to curl the minikube IP:\n```\n`curl $(minikube ip)\ncurl: (7) Failed to connect to 192.168.49.2 port 80: Operation timed out\n`\n```\nAny help?\n----------EDIT :\n```\n`kubectl get deploy -n ingress-nginx -o yaml\n\n          ports:\n          - containerPort: 80\n            hostPort: 80\n            name: http\n            protocol: TCP\n          - containerPort: 443\n            hostPort: 443\n            name: https\n            protocol: TCP\n          - containerPort: 8443\n            name: webhook\n            protocol: TCP\n\nkubectl get svc -n ingress-nginx -o yaml\napiVersion: v1\nitems:\n- apiVersion: v1\n  kind: Service\n  metadata:\n    annotations:\n      kubectl.kubernetes.io/last-applied-configuration: |\n        {\"apiVersion\":\"v1\",\"kind\":\"Service\",\"metadata\":{\"annotations\":{},\"labels\":{\"app.kubernetes.io/component\":\"controller\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-controller\",\"namespace\":\"ingress-nginx\"},\"spec\":{\"ipFamilies\":[\"IPv4\"],\"ipFamilyPolicy\":\"SingleStack\",\"ports\":[{\"appProtocol\":\"http\",\"name\":\"http\",\"port\":80,\"protocol\":\"TCP\",\"targetPort\":\"http\"},{\"appProtocol\":\"https\",\"name\":\"https\",\"port\":443,\"protocol\":\"TCP\",\"targetPort\":\"https\"}],\"selector\":{\"app.kubernetes.io/component\":\"controller\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"type\":\"NodePort\"}}\n    creationTimestamp: \"2021-12-16T11:41:35Z\"\n    labels:\n      app.kubernetes.io/component: controller\n      app.kubernetes.io/instance: ingress-nginx\n      app.kubernetes.io/name: ingress-nginx\n    name: ingress-nginx-controller\n    namespace: ingress-nginx\n    resourceVersion: \"489\"\n    uid: 63826bc2-5d90-42f1-861f-f7f082ccf0fb\n  spec:\n    clusterIP: 10.104.208.171\n    clusterIPs:\n    - 10.104.208.171\n    externalTrafficPolicy: Cluster\n    internalTrafficPolicy: Cluster\n    ipFamilies:\n    - IPv4\n    ipFamilyPolicy: SingleStack\n    ports:\n    - appProtocol: http\n      name: http\n      nodePort: 30783\n      port: 80\n      protocol: TCP\n      targetPort: http\n    - appProtocol: https\n      name: https\n      nodePort: 30860\n      port: 443\n      protocol: TCP\n      targetPort: https\n    selector:\n      app.kubernetes.io/component: controller\n      app.kubernetes.io/instance: ingress-nginx\n      app.kubernetes.io/name: ingress-nginx\n    sessionAffinity: None\n    type: NodePort\n  status:\n    loadBalancer: {}\n- apiVersion: v1\n  kind: Service\n  metadata:\n    annotations:\n      kubectl.kubernetes.io/last-applied-configuration: |\n        {\"apiVersion\":\"v1\",\"kind\":\"Service\",\"metadata\":{\"annotations\":{},\"labels\":{\"app.kubernetes.io/component\":\"controller\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-controller-admission\",\"namespace\":\"ingress-nginx\"},\"spec\":{\"ports\":[{\"appProtocol\":\"https\",\"name\":\"https-webhook\",\"port\":443,\"targetPort\":\"webhook\"}],\"selector\":{\"app.kubernetes.io/component\":\"controller\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"type\":\"ClusterIP\"}}\n    creationTimestamp: \"2021-12-16T11:41:35Z\"\n    labels:\n      app.kubernetes.io/component: controller\n      app.kubernetes.io/instance: ingress-nginx\n      app.kubernetes.io/name: ingress-nginx\n    name: ingress-nginx-controller-admission\n    namespace: ingress-nginx\n    resourceVersion: \"483\"\n    uid: fe797532-27c9-4dd1-a1bc-0662a3d2a4da\n  spec:\n    clusterIP: 10.106.175.35\n    clusterIPs:\n    - 10.106.175.35\n    internalTrafficPolicy: Cluster\n    ipFamilies:\n    - IPv4\n    ipFamilyPolicy: SingleStack\n    ports:\n    - appProtocol: https\n      name: https-webhook\n      port: 443\n      protocol: TCP\n      targetPort: webhook\n    selector:\n      app.kubernetes.io/component: controller\n      app.kubernetes.io/instance: ingress-nginx\n      app.kubernetes.io/name: ingress-nginx\n    sessionAffinity: None\n    type: ClusterIP\n  status:\n    loadBalancer: {}\nkind: List\nmetadata:\n  resourceVersion: \"\"\n  selfLink: \"\"\n`\n```",
      "solution": "OK so apparently this is a known issue with minikube, Ingress works properly on linux only.\n\nThe ingress, and ingress-dns addons are currently only supported on\nLinux. See #7332\n\nyou need to `minikube tunnel` on windows/macOS before being able to `curl`, but still there are differences:\nOn Windows, both `127.0.0.1` and `localhost` redirect to the application.\nOn macOS, `127.0.0.1` and `localhost` show an \"nginX not found\" message, but `curl hello-world.info` works only after changing `etc/hosts`.",
      "question_score": 2,
      "answer_score": 4,
      "created_at": "2021-12-15T16:14:59",
      "url": "https://stackoverflow.com/questions/70366074/ingress-not-working-from-official-kubernetes-tutorial"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "minikube",
      "question_id": 69132849,
      "title": "Airflow KubernetesExecutor and minikube: Scheduler can&#39;t connect to Minikube",
      "problem": "I have a MiniKube that is running and I deploy Airflow via docker-compose this way:\n```\n`---\nversion: '3'\nx-airflow-common:\n  &airflow-common\n  # In order to add custom dependencies or upgrade provider packages you can use your extended image.\n  # Comment the image line, place your Dockerfile in the directory where you placed the docker-compose.yaml\n  # and uncomment the \"build\" line below, Then run `docker-compose build` to build the images.\n  image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.1.3}\n  # build: .\n  environment:\n    &airflow-common-env\n    AIRFLOW__CORE__EXECUTOR: KubernetesExecutor\n    AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow\n    AIRFLOW__CORE__FERNET_KEY: ''\n    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'\n    # AIRFLOW__CORE__LOAD_EXAMPLES: 'true'\n    AIRFLOW__API__AUTH_BACKEND: 'airflow.api.auth.backend.basic_auth'\n    _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS:-}\n  volumes:\n    - ~/.kube:/home/airflow/.kube\n    - ./dags/:/opt/airflow/dags\n    - ./logs:/opt/airflow/logs\n    - ./plugins:/opt/airflow/plugins\n  user: \"${AIRFLOW_UID:-50000}:${AIRFLOW_GID:-0}\"\n  depends_on:\n    redis:\n      condition: service_healthy\n    postgres:\n      condition: service_healthy\n\nservices:\n  postgres:\n    image: postgres:13\n    environment:\n      POSTGRES_USER: airflow\n      POSTGRES_PASSWORD: airflow\n      POSTGRES_DB: airflow\n    volumes:\n      - postgres-db-volume:/var/lib/postgresql/data\n    healthcheck:\n      test: [\"CMD\", \"pg_isready\", \"-U\", \"airflow\"]\n      interval: 5s\n      retries: 5\n    restart: always\n\n  redis:\n    image: redis:latest\n    ports:\n      - 6379:6379\n    healthcheck:\n      test: [\"CMD\", \"redis-cli\", \"ping\"]\n      interval: 5s\n      timeout: 30s\n      retries: 50\n    restart: always\n\n  airflow-webserver:\n    But the connection between Airflow and Kubernetes seems to fail (removing the AIRFLOW__CORE__EXECUTOR varenv allows the creation):\n```\n`airflow-scheduler_1  | Traceback (most recent call last):\nairflow-scheduler_1  |   File \"/home/airflow/.local/bin/airflow\", line 8, in \nairflow-scheduler_1  |     sys.exit(main())\nairflow-scheduler_1  |   File \"/home/airflow/.local/lib/python3.8/site-packages/airflow/__main__.py\", line 40, in main\nairflow-scheduler_1  |     args.func(args)\nairflow-scheduler_1  |   File \"/home/airflow/.local/lib/python3.8/site-packages/airflow/cli/cli_parser.py\", line 48, in command\nairflow-scheduler_1  |     return func(*args, **kwargs)\nairflow-scheduler_1  |   File \"/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/cli.py\", line 91, in wrapper\nairflow-scheduler_1  |     return f(*args, **kwargs)\nairflow-scheduler_1  |   File \"/home/airflow/.local/lib/python3.8/site-packages/airflow/cli/commands/scheduler_command.py\", line 70, in scheduler\nairflow-scheduler_1  |     job.run()\nairflow-scheduler_1  |   File \"/home/airflow/.local/lib/python3.8/site-packages/airflow/jobs/base_job.py\", line 245, in run\nairflow-scheduler_1  |     self._execute()\nairflow-scheduler_1  |   File \"/home/airflow/.local/lib/python3.8/site-packages/airflow/jobs/scheduler_job.py\", line 686, in _execute\nairflow-scheduler_1  |     self.executor.start()\nairflow-scheduler_1  |   File \"/home/airflow/.local/lib/python3.8/site-packages/airflow/executors/kubernetes_executor.py\", line 485, in start\nairflow-scheduler_1  |     self.kube_client = get_kube_client()\nairflow-scheduler_1  |   File \"/home/airflow/.local/lib/python3.8/site-packages/airflow/kubernetes/kube_client.py\", line 145, in get_kube_client\nairflow-scheduler_1  |     client_conf = _get_kube_config(in_cluster, cluster_context, config_file)\nairflow-scheduler_1  |   File \"/home/airflow/.local/lib/python3.8/site-packages/airflow/kubernetes/kube_client.py\", line 40, in _get_kube_config\nairflow-scheduler_1  |     config.load_incluster_config()\nairflow-scheduler_1  |   File \"/home/airflow/.local/lib/python3.8/site-packages/kubernetes/config/incluster_config.py\", line 93, in load_incluster_config\nairflow-scheduler_1  |     InClusterConfigLoader(token_filename=SERVICE_TOKEN_FILENAME,\nairflow-scheduler_1  |   File \"/home/airflow/.local/lib/python3.8/site-packages/kubernetes/config/incluster_config.py\", line 45, in load_and_set\nairflow-scheduler_1  |     self._load_config()\nairflow-scheduler_1  |   File \"/home/airflow/.local/lib/python3.8/site-packages/kubernetes/config/incluster_config.py\", line 51, in _load_config\nairflow-scheduler_1  |     raise ConfigException(\"Service host/port is not set.\")\nairflow-scheduler_1  | kubernetes.config.config_exception.ConfigException: Service host/port is not set.\n`\n```\nMy Idea is that the kube config file is not correctly found by the Airflow Scheduler. I mounted the volume `~/.kube:/home/airflow/.kube` but can't find a way to make it work.",
      "solution": "Using Docker Compose to run KubernetesExecutor seems like a bad idea.\nWhy would you want to do it?\nIt makes a lot more sense to use the official Helm Chart - it's easier to manage and configure, you can easily deploy it to your minikube and it will work out-of-the-box with KubernetesExecutor.\nhttps://airflow.apache.org/docs/helm-chart/stable/index.html",
      "question_score": 2,
      "answer_score": 3,
      "created_at": "2021-09-10T15:19:15",
      "url": "https://stackoverflow.com/questions/69132849/airflow-kubernetesexecutor-and-minikube-scheduler-cant-connect-to-minikube"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "minikube",
      "question_id": 73530799,
      "title": "minikube does not download a public docker image",
      "problem": "Good day!\nI am facing a strange problem. I have a standard deployment that uses a public image. But when I create it, I get the error ImagePullBackOff\n```\n`$ kubectl get pods\n`\n```\nresult\n```\n`api-gateway-deployment-74968fbf5c-cvqwj   0/1     ImagePullBackOff   0          6h23m\napi-gateway-gateway-deployment-74968fbf5c-hpdxb   0/1     ImagePullBackOff   0          6h23m\napi-gateway-gateway-deployment-74968fbf5c-rctv6   0/1     ImagePullBackOff   0          6h23m\n`\n```\nmy deployment\n`apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: api-gateway-deployment\n  labels:\n    app: api-gateway-deployment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: api-gateway-deployment\n  template:\n    metadata:\n      labels:\n        app: api-gateway-deployment\n    spec:\n      containers:\n      - name: api-gateway-node\n        image: creatorsprodhouse/api-gateway:latest\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 80\n`\nI am using the docker driver, is there anything I can do wrong?\n```\n`minikube start --driver=docker\n`\n```",
      "solution": "I think your internet connection is slow. The timeout to pull an image is `120` seconds, so kubectl could not pull the image in under `120` seconds.\nFirst, pull the image via `Docker`\n`docker image pull creatorsprodhouse/api-gateway:latest\n`\nThen load the downloaded image to `minikube`\n`minikube image load creatorsprodhouse/api-gateway:latest\n`\nAnd then everything will work because now kubectl will use the image that is stored locally.",
      "question_score": 2,
      "answer_score": 4,
      "created_at": "2022-08-29T17:02:35",
      "url": "https://stackoverflow.com/questions/73530799/minikube-does-not-download-a-public-docker-image"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "minikube",
      "question_id": 73266915,
      "title": "Error from server (BadRequest): container &quot;spark-kubernetes-driver&quot; in pod &quot;test-run-spark&quot; is waiting to start: trying and failing to pull image",
      "problem": "minikube in mac os is not able to pull docker images from docker repository.\nTrying to run spark on k8s\n```\n`spark-submit --master k8s://https://ip:port --deploy-mode cluster --name test-run-spark --conf spark.kubernetes.container.image=Docker-image --conf spark.kubernetes.driver.container.image=docker4tg/Docker-image --conf spark.kubernetes.executor.container.image=docker4tg/Docker-image --conf spark.kubernetes.driver.pod.name=test-run-spark --class [class] --num-executors 1 --executor-memory 512m --driver-memory 512m --driver-cores 2 --executor-cores 2 --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark local:///[jar].jar\n`\n```\nUsing the same docker image and I'm able to pull the docker image to my local. But the k8s pods are not able to , but here's the cache. Only few tags of the same image, I moved the mkdir command up or down to change the hash, worked. I did not logical changes, But it worked fine for 3 to 4 tags,and the spplication ran successfully. I could not understand this.\nPlease help me to figure out the issue.\nDockerfile\n```\n`\nFROM ubuntu:18.04\nARG SPARKVERSION=tmpsVersion\nARG HADOOPVERSION=tmpHVersion\nENV SPARK_VERSION=$SPARKVERSION\nENV HADOOP_VERSION=$HADOOPVERSION\nRUN sed -i s/http/ftp/ /etc/apt/sources.list && apt-get update -y\nRUN apt-get install wget -y\nRUN apt-get install openjdk-8-jdk -y\nRUN sed -i s/http/ftp/ /etc/apt/sources.list && apt-get update -y\nRUN mkdir -p /opt/spark/work-dir\nWORKDIR /opt/spark/work-dir\nRUN wget -O spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz  https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz\nRUN tar -xzvf spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz -C /opt/spark/\nRUN rm -rf spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz\nRUN mv -f /opt/spark/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}/* /opt/spark/\nRUN rm -rf /opt/spark/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}\nENV SPARK_HOME=/opt/spark\nENV PATH=\"${SPARK_HOME}/bin:${PATH}\"\nRUN mkdir -p /opt/spark/data-jars/\nCOPY [jar.jar] /opt/spark/data-jars/\nENTRYPOINT [ \"/opt/spark/kubernetes/dockerfiles/spark/entrypoint.sh\" ]\n\n`\n```\nERROR:\n```\n`kubectl get pods; kubectl logs test-run-spark\nNAME             READY   STATUS             RESTARTS   AGE\ntest-run-spark   0/1     ImagePullBackOff   0          2m36s\nError from server (BadRequest): container \"spark-kubernetes-driver\" in pod \"test-run-spark\" is waiting to start: trying and failing to pull image\n\n`\n```\nKindly help me with this guys",
      "solution": "Your `minikube` environment is isolated from your host, so if you already have the image on your host or you can pull it, it doesn't mean you can do the same thing in `minikube`.\nIf you want to build the image in `minikube` context:\n`# export minikube docker config\neval $(minikube docker-env)\n# build your image directly in minikube\ndocker build \n`\nIf you have the image locally, you can load it to `minikube` by:\n`minikube image load IMAGE_NAME\n`\nAnd if you want to let `minikube` pull the images from a private remote registry (ex: dockerhub), you can follow these instructions to add the registry creds to your `minikube`.",
      "question_score": 2,
      "answer_score": 4,
      "created_at": "2022-08-07T13:13:24",
      "url": "https://stackoverflow.com/questions/73266915/error-from-server-badrequest-container-spark-kubernetes-driver-in-pod-test"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "minikube",
      "question_id": 66722627,
      "title": "Kubernetes statefulset : other than &#39;replicas&#39;, &#39;template&#39;, and &#39;updateStrategy&#39; are forbidden",
      "problem": "Note: nfs server and permission are fine, I have checked PV and PVC is creating fine only statefulSet is giving me this error.\nError Message: The StatefulSet \"auth-mongo-ss\" is invalid: spec: Forbidden: updates to statefulset spec for fields other than 'replicas', 'template', and 'updateStrategy' are forbidden\n(err msg is straightforward but didn't help to solve it! what am I missing here ?)\nKubernetes(minkube) version:\nClient Version: v1.20.2\nServer Version: v1.20.2\nOS:\nLinux mint - 20\n```\n`apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: auth-pv\nspec:\n  capacity:\n    storage: 250Mi\n  accessModes:\n    - ReadWriteMany\n  persistentVolumeReclaimPolicy: Retain\n  storageClassName: manual\n  nfs:\n    path: /nfs/auth\n    server: 192.168.10.104\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: auth-mongo-serv\n  labels:\n    app: auth-mongo-serv\nspec:\n  ports:\n    - name: db\n      protocol: TCP\n      port: 27017\n      targetPort: 27017\n---\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: auth-mongo-ss\nspec:\n  selector:\n    matchLabels:\n      app: auth-mongo-serv # has to match .spec.template.metadata.labels\n  serviceName: auth-mongo-ss\n  replicas: 1 # by default is 1\n  template:\n    metadata:\n      labels:\n        app: auth-mongo-serv # has to match .spec.selector.matchLabels\n    spec:\n      terminationGracePeriodSeconds: 10\n      containers:\n        - name: auth-mongo-docker\n          image: mongo\n          ports:\n            - containerPort: 27017\n          resources:\n            limits:\n              memory: \"250Mi\"\n              cpu: \"250m\"\n          volumeMounts:\n            - name: auth-mongo-data\n              mountPath: /data/db\n  volumeClaimTemplates:\n    - metadata:\n        name: auth-mongo-data\n      spec:\n        storageClassName: manual\n        accessModes: [\"ReadWriteMany\"]\n        resources:\n          requests:\n            storage: 250Mi\n    ```\n`\n```",
      "solution": "The error `spec: Forbidden: updates to statefulset spec for fields other than 'replicas', 'template', and 'updateStrategy' are forbidden` saying it all.\nIn `StatefultSet` only mutable (you can change/update) is `replicas`, `template`, and `updateStrategy`. Other than these fields in `Spec` you cannot change others fields during updates.\nUpdate\nYou have multiple issues:\n\nin the `StatefuleSet` Spec you used `serviceName: auth-mongo-ss`, do you have this headless service?\n\nIn this service spec you did not give `selector`\n\n```\n`apiVersion: v1\nkind: Service\nmetadata:\n  name: auth-mongo-serv\n  labels:\n    app: auth-mongo-serv\nspec:\n  ports:\n    - name: db\n      protocol: TCP\n      port: 27017\n      targetPort: 27017\n`\n```\nAn example of StatefulSet from k8s doc is given below, for statefulset you need one headless service.\n```\n`apiVersion: v1\nkind: Service\nmetadata:\n  name: nginx\n  labels:\n    app: nginx\nspec:\n  ports:\n  - port: 80\n    name: web\n  clusterIP: None\n  selector:\n    app: nginx\n---\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: web\nspec:\n  selector:\n    matchLabels:\n      app: nginx # has to match .spec.template.metadata.labels\n  serviceName: \"nginx\"\n  replicas: 3 # by default is 1\n  template:\n    metadata:\n      labels:\n        app: nginx # has to match .spec.selector.matchLabels\n    spec:\n      terminationGracePeriodSeconds: 10\n      containers:\n      - name: nginx\n        image: k8s.gcr.io/nginx-slim:0.8\n        ports:\n        - containerPort: 80\n          name: web\n        volumeMounts:\n        - name: www\n          mountPath: /usr/share/nginx/html\n  volumeClaimTemplates:\n  - metadata:\n      name: www\n    spec:\n      accessModes: [ \"ReadWriteOnce\" ]\n      storageClassName: \"my-storage-class\"\n      resources:\n        requests:\n          storage: 1Gi\n`\n```",
      "question_score": 2,
      "answer_score": 4,
      "created_at": "2021-03-20T15:19:40",
      "url": "https://stackoverflow.com/questions/66722627/kubernetes-statefulset-other-than-replicas-template-and-updatestrategy"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "minikube",
      "question_id": 73533031,
      "title": "MongoDb ImagePullBackOff error in Kubernetes despite trying every SOF solution",
      "problem": "I'm using minikube on a Fedora based machine to run a simple mongo-db deployment on my local machine but I'm constantly getting `ImagePullBackOff` error. Here is the yaml file:\n```\n`apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mongodb-deployment\n  labels:\n    app: mongodb\nspec:\n replicas: 1\n selector:\n   matchLabels:\n     app: mongodb\n template: \n  metadata:\n    labels:\n      app: mongodb\n  spec:\n    containers:\n      - name: mongodb\n        image: mongo\n        ports:\n            - containerPort: 27017\n        env:\n           - name: MONGO_INITDB_ROOT_USERNAME\n             valueFrom:\n              secretKeyRef:\n                 name: mongodb-secret\n                 key: mongo-root-username\n           - name: MONGO_INITDB_ROOT_PASSWORD\n             valueFrom:\n              secretKeyRef:\n                 name: mongodb-secret\n                 key: mongo-root-password\n \n \napiVersion: v1\nkind: Service\nmetadata:\n   name: mongodb-service\nspec:\n  selector:\n      app: mongodb\n  ports:\n     - protocol: TCP\n       port: 27017\n       targetPort: 27017\n`\n```\nI tried to pull the image locally by using `docker pull mongo`, `minikube image pull mongo` & `minikube image pull mongo-express` several times while restarting docker and minikube several times.\nLogining into dockerhub (both in broweser and through terminal didn't work)\nI also tried to login into docker using `docker login` command and then modified my `/etc/resolv.conf` and adding `nameserver 8.8.8.8` and then restartied docker using `sudo systemctl restart docker` but even that failed to work.\nOn running `kubectl describe pod` command I get this output:\n```\n`Name:         mongodb-deployment-6bf8f4c466-85b2h\nNamespace:    default\nPriority:     0\nNode:         minikube/192.168.49.2\nStart Time:   Mon, 29 Aug 2022 23:04:12 +0530\nLabels:       app=mongodb\n          pod-template-hash=6bf8f4c466\nAnnotations:  \nStatus:       Pending\nIP:           172.17.0.2\nIPs:\n  IP:           172.17.0.2\nControlled By:  ReplicaSet/mongodb-deployment-6bf8f4c466\nContainers:\n mongodb:\n   Container ID:   \n   Image:          mongo\n   Image ID:       \n   Port:           27017/TCP\n   Host Port:      0/TCP\n   State:          Waiting\n     Reason:       ImagePullBackOff\n   Ready:          False\n   Restart Count:  0\n   Environment:\n     MONGO_INITDB_ROOT_USERNAME:    \n     Optional: false\n     MONGO_INITDB_ROOT_PASSWORD:    \n     Optional: false\n   Mounts:\n      \n     /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-vlcxl (ro)\n  Conditions:\n       Type              Status\n       Initialized       True \n       Ready             False \n       ContainersReady   False \n       PodScheduled      True \n  Volumes:\n     kube-api-access-vlcxl:\n     Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       \n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              \nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                         node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type     Reason          Age                  From               Message\n`\n```\n\n```\n`  Normal   Scheduled       22m                  default-scheduler  Successfully assigned default/mongodb-deployment-6bf8f4c466-85b2h to minikube\n  Warning  Failed          18m (x2 over 20m)    kubelet            Failed to pull image \"mongo:latest\": rpc error: code = Unknown desc = context deadline exceeded\n  Warning  Failed          18m (x2 over 20m)    kubelet            Error: ErrImagePull\n  Normal   BackOff         17m (x2 over 20m)    kubelet            Back-off pulling image \"mongo:latest\"\n  Warning  Failed          17m (x2 over 20m)    kubelet            Error: ImagePullBackOff\n  Normal   Pulling         17m (x3 over 22m)    kubelet            Pulling image \"mongo:latest\"\n  Normal   SandboxChanged  11m                  kubelet            Pod sandbox changed, it will be killed and re-created.\n  Normal   Pulling         3m59s (x4 over 11m)  kubelet            Pulling image \"mongo:latest\"\n  Warning  Failed          2m (x4 over 9m16s)   kubelet            Failed to pull image \"mongo:latest\": rpc error: code = Unknown desc = context deadline exceeded\n  Warning  Failed          2m (x4 over 9m16s)   kubelet            Error: ErrImagePull\n  Normal   BackOff         83s (x7 over 9m15s)  kubelet            Back-off pulling image \"mongo:latest\"\n  Warning  Failed          83s (x7 over 9m15s)  kubelet            Error: ImagePullBackOff\n`\n```\nPS: Ignore any any spacing errors",
      "solution": "I think your internet connection is slow. The timeout to pull an image is `120` seconds, so kubectl could not pull the image in under `120` seconds.\nFirst, pull the image via `Docker`\n`docker image pull mongo\n`\nThen load the downloaded image to `minikube`\n`minikube image load mongo\n`\nAnd then everything will work because now kubectl will use the image that is stored locally.",
      "question_score": 2,
      "answer_score": 3,
      "created_at": "2022-08-29T20:12:45",
      "url": "https://stackoverflow.com/questions/73533031/mongodb-imagepullbackoff-error-in-kubernetes-despite-trying-every-sof-solution"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "minikube",
      "question_id": 73368691,
      "title": "cannot connect to minikube ip and NodePort service port - windows",
      "problem": "I am trying to run an application locally on k8s but I am not able to reach it.\nhere is my deloyment:\n```\n`apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: listings\n  labels:\n    app: listings\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: listings\n  template:\n    metadata:\n      labels:\n        app: listings\n    spec:\n      containers:\n        - image: mydockerhub/listings:latest\n          name: listings\n          envFrom:\n            - secretRef:\n                name: listings-secret\n            - configMapRef:\n                name: listings-config\n          ports:\n            - containerPort: 8000\n              name: django-port\n\n`\n```\nand it is my service\n```\n`apiVersion: v1\nkind: Service\nmetadata:\n  name: listings\n  labels:\n    app: listings\nspec:\n  type: NodePort\n  selector:\n    app: listings\n  ports:\n    - name: http\n      port: 8000\n      targetPort: 8000\n      nodePort: 30036\n      protocol: TCP\n`\n```\nAt this stage, I don't want to use other methods like ingress or ClusterIP, or load balancer. I want to make nodePort work because I am trying to learn.\nWhen I run `kubectl get svc -o wide` I see\n```\n`NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE   SELECTOR\nlistings     NodePort    10.107.77.231           8000:30036/TCP   28s   app=listings\n`\n```\nWhen I run `kubectl get node -o wide` I see\n```\n`NAME       STATUS   ROLES                  AGE   VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION                      CONTAINER-RUNTIME\nminikube   Ready    control-plane,master   85d   v1.23.3   192.168.49.2           Ubuntu 20.04.2 LTS   5.10.16.3-microsoft-standard-WSL2   docker://20.10.12\n`\n```\nand when I run `minikube ip` it shows `192.168.49.2`\nI try to open `http://192.168.49.2:30036/health` it is not opening `This site can\u2019t be reached`\nHow should expose my application externally?\nnote that I have created the required configmap and secret objects. also note that this is a simple django restful application that if you hit the /health endpoint, it returns success. and that's it. so there is no problem with the application",
      "solution": "That is because your local and minikube are not in the same network segment,\nyou must do something more to access minikube service on windows.\nFirst\n```\n`$ minikube service list\n`\n```\nThat will show your service detail which include name, url, nodePort, targetPort.\nThen\n```\n`$ minikube service --url listings\n`\n```\nIt will open a port to listen on your windows machine that can forward the traffic to minikube node port.\nOr you can use command `kubectl port-forward` to expose service on host port, like:\n```\n`kubectl port-forward --address 0.0.0.0 -n default service/listings 30036:8000\n`\n```\nThen try with `http://localhost:30036/health`",
      "question_score": 2,
      "answer_score": 3,
      "created_at": "2022-08-16T06:06:19",
      "url": "https://stackoverflow.com/questions/73368691/cannot-connect-to-minikube-ip-and-nodeport-service-port-windows"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "minikube",
      "question_id": 69795804,
      "title": "MariaDB Galera on Minikube: mkdir: cannot create directory &#39;/bitnami/mariadb/data&#39;: Permission denied",
      "problem": "I want to deploy a MariaDB Galera instance onto a local Minikube cluster with 3 nodes via Helm.\nI used the following command for that:\n```\n`helm install my-release bitnami/mariadb-galera --set rootUser.password=test --set db.name=test\n`\n```\nThe problem is, if I do that I get the following error in the log:\n```\n`mariadb 10:27:41.60 \nmariadb 10:27:41.60 Welcome to the Bitnami mariadb-galera container\nmariadb 10:27:41.60 Subscribe to project updates by watching https://github.com/bitnami/bitnami-docker-mariadb-galera\nmariadb 10:27:41.60 Submit issues and feature requests at https://github.com/bitnami/bitnami-docker-mariadb-galera/issues\nmariadb 10:27:41.61 \nmariadb 10:27:41.61 INFO  ==> ** Starting MariaDB setup **\nmariadb 10:27:41.64 INFO  ==> Validating settings in MYSQL_*/MARIADB_* env vars\nmariadb 10:27:41.67 INFO  ==> Initializing mariadb database\nmkdir: cannot create directory '/bitnami/mariadb/data': Permission denied\n`\n```\nThe site of the image lists the possibility to use an extra init container to fix that (Link).\nSo I came up with the following configuration:\nmariadb-galera-init-config.yaml\n```\n`extraInitContainers:\n- name: initcontainer\n  image: bitnami/minideb\n  command: [\"chown -R 1001:1001 /bitnami/mariadb/\"]\n`\n```\nThe problem is that when I run the command with this configuration:\n```\n`helm install my-release bitnami/mariadb-galera --set rootUser.password=test --set db.name=test -f mariadb-galera-init-config.yaml\n`\n```\nI get the following error on the Minikube dashboard:\n```\n`Error: failed to start container \"initcontainer\": Error response from daemon: OCI runtime create failed: container_linux.go:380: starting container process caused: exec: \"chown -R 1001:1001 /bitnami/mariadb/\": stat chown -R 1001:1001 /bitnami/mariadb/: no such file or directory: unknown\n`\n```\nI don't know how to fix this configuration file, or if there is some other better way to get this working...",
      "solution": "In any case someone has issues with this, may I suggest running a initContainer before.\n```\n` initContainers:\n      - name: mariadb-create-directory-structure\n        image: busybox\n        command:\n          [\n            \"sh\",\n            \"-c\",\n            \"mkdir -p /bitnami/mariadb/data && chown -R 1001:1001 /bitnami\",\n          ]\n        volumeMounts:\n        - name: data\n          mountPath: /bitnami\n`\n```",
      "question_score": 2,
      "answer_score": 3,
      "created_at": "2021-11-01T11:46:05",
      "url": "https://stackoverflow.com/questions/69795804/mariadb-galera-on-minikube-mkdir-cannot-create-directory-bitnami-mariadb-dat"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "minikube",
      "question_id": 69114227,
      "title": "Vanilla JS ERR_NAME_NOT_RESOLVED on minikube cluster",
      "problem": "I have a minimal web application running on local minikube cluster.\nThe backend is exposing an API at `/be/test/hi` and service name is also be.\nWhen I send a GET request from frontend to backend i get:\n```\n`main.js:15 GET http://be/be/test/hi net::ERR_NAME_NOT_RESOLVED\n`\n```\nIf I run `nslookup be` from the frontend POD i get the correct dns resolution, and running\n`curl be/be/test/hi` I get the right response from backend (a simple 'Hello world' string)\nWhat am I missing?\nbackend.yaml\n```\n`apiVersion: v1\nkind: Service\nmetadata:\n  name: be\nspec:\n  ports:\n    - port: 80\n      targetPort: 8080\n  selector:\n    app: be\n  type: LoadBalancer\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: be\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: be\n  template:\n    metadata:\n      labels:\n        app: be\n    spec:\n      containers:\n      - name: be\n        image: kubebe\n        imagePullPolicy: Never\n        ports:\n          - containerPort: 8080\n`\n```\nfrontend.yaml\n```\n`apiVersion: v1\nkind: Service\nmetadata:\n  name: fe\nspec:\n  ports:\n    - port: 80\n      targetPort: 8080\n  selector:\n    app: fe\n  type: LoadBalancer\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: fe\n  template:\n    metadata:\n      labels:\n        app: fe\n    spec:\n      containers:\n      - name: fe\n        image: kubefe\n        imagePullPolicy: Never\n        ports:\n          - containerPort: 8080\n`\n```\nmain.js\n```\n`const URL=\"http://be/be/test/hi\"\n\n  function httpGetAsync(){\n    var xmlHttp = new XMLHttpRequest();\n    xmlHttp.onreadystatechange = function() { \n        if (xmlHttp.readyState == 4 && xmlHttp.status == 200)\n            console.log(xmlHttp.responseText);\n    }\n    xmlHttp.open(\"GET\", URL, true); \n    xmlHttp.send(null);\n}\n`\n```\nfrontend Dockerfile (kubefe image):\n```\n`FROM centos/httpd-24-centos7:2.4\nRUN mkdir -p /var/www/html/fe\nCOPY ./index.html ./main.js /var/www/html/fe/\n`\n```\nEDIT: I've resolved my issues but I think the actual question is still unanswered.\nTo solve this I've simple removed the protocol and host from my url and set up proxy rules in /etc/httpd/conf.d/default-site.conf\ndefault-site.conf\n```\n` \n  ProxyPreserveHost Off\n  ProxyRequests Off\n  ProxyPass /be/ http://be/be/\n \n`\n```",
      "solution": "Your frontend code (javascript) is executing in your browser, not inside the kubernetes cluster (nginx pod)\nYou have 3 options here,\n\nCreate a NodePort Service for your frontend\nRely on kubeproxy for doing a port forward\nCreate an ingress",
      "question_score": 2,
      "answer_score": 3,
      "created_at": "2021-09-09T09:47:29",
      "url": "https://stackoverflow.com/questions/69114227/vanilla-js-err-name-not-resolved-on-minikube-cluster"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "minikube",
      "question_id": 67691123,
      "title": "How to prevent Minikube to redeploy, when resuming the VM with `minikube start`?",
      "problem": "Suspending & resuming my virtual-machine does break the k8s deployment\nWhen I suspend with `minikube stop` and then resume the Virtual Machine with `minikube start`, Minikube re-deploys my app from scratch.\nI see this behaviour with newer versions of Minikube higher than v1.18 (I run on v1.19).\n\nThe setup:\n\nThe Kubernetes deployment mounts a volume with the source code from my host machine, via `hostPath`.\\\nAlso I have a container of `initContainers` that setups the application.\n\nSince the new \"redeploy behaviour on resume\" happens, the init-container breaks my deploy, if I have work-in-progress code on my host machine..\nThe issue:\nNow, if I have temporary/non-perfectly-running code, I cannot suspend the machine with unfinished work anymore, between working days; because every time I resume it Minikube will try to deploy again but with broken code and fail with an `Init:CrashLoopBackOff`.\nThe workaround:\nFor now, each time I resume the machine I need to\n\nstash/commit my WIP code\ncheckout the last commit with working deployment\nrun the deployment & wait for it to complete the initialization (minutes...)\ncheckout/stash-pop the code saved at point 1).\n\nI can survive, but the workflow is terrible.\nHow do I restore the old behaviour?\n\nHow do I make my deploys to stay untouched, as expected when suspending the VM, instead of being re-deployed every time I resume?",
      "solution": "In short words there are two ways to achieve what you want:\n\nOn current versions of `minikube` and `virtualbox` you can use `save state` option in Virtual box directly.\nMove initContianer's code to a separate `job`.\n\nMore details about minikube + virtual box\nI have an environment with minikube version 1.20, virtual box 6.1.22 (from yesterday) and MacOS. Also minikube driver is set to `virtualbox`.\nFirst with `minikube` + `VirtualBox`. Different scenarios:\n`minikube stop` does following:\n\nStops a local Kubernetes cluster. This command stops the underlying VM\nor container, but keeps user data intact.\n\nWhat happens is virtual machine where minikube is set up stops entirely. `minikube start` starts the VM and all processes in it. All containers are started as well, so if your pod has an init-container, it will run first anyway.\n`minikube pause` pauses all processes and free up CPU resourses while memory will still be allocated. `minikube unpause` brings back CPU resources and continues executing containers from a state when they were paused.\nBased on different scenarios I tried with `minikube` it's not achievable using only minikube commands. To avoid any state loss on your `minikube` environment due to host restart or necessity to stop a VM to get more resources, you can use `save state` feature in VirtualBox in UI or cli. Below what it does:\n\nVBoxManage controlvm  savestate: Saves the current state of the VM to disk and then stops the VM.\n\nVirtual box creates something like a snapshot with all memory content within this snapshot. When virtual machine is restarted, Virtual box will restore the state of VM to the state when the VM was saved.\nOne more assumption is if this works the same way in v. 1.20 - this is expected behaviour and not a bug (otherwise it would've been fixed already)\nInit-container and jobs\nYou may consider moving your init-container's code to a a separate `job` so you will avoid any issues with unintended pod restarts and braking your deployment in the main container.  Also it's advised to have init-container's code idempotent.\nHere's a quote from official documentation:\n\nBecause init containers can be restarted, retried, or re-executed,\ninit container code should be idempotent. In particular, code that\nwrites to files on `EmptyDirs` should be prepared for the possibility\nthat an output file already exists.\n\nThis can be achieved by using `jobs` in Kubernetes which you can run manually when you need to do so.\nTo ensure following the workflow you can place a check for a `Job completion` or a specific file on a data volume to the deployment's pod init container to indicate that code is working, deployment will be fine.\nLinks with more information:\n\nVirtualBox `save state`\n\ninitContainers\n\nkubernetes jobs",
      "question_score": 2,
      "answer_score": 3,
      "created_at": "2021-05-25T17:29:43",
      "url": "https://stackoverflow.com/questions/67691123/how-to-prevent-minikube-to-redeploy-when-resuming-the-vm-with-minikube-start"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "minikube",
      "question_id": 77869181,
      "title": "How to correctly deploy a Blazor Server app on Kubernetes",
      "problem": "I am trying to deploy a test Blazor server-side application in a local Kubernetes cluster. I have it configured such that I can access it via either the service or an nginx-ingress. However, both routes have their own problems that make the application not function properly once served.\nWhen served over the service, it looks like this. I believe this is how it is supposed to look. However, there are a bunch of errors that make it so that I can't intereact with the page at all.\nErrors:\n```\n`blazor.server.js:1 WebSocket connection to 'ws://172.25.217.142:30500/_blazor?id=1qrCqjxomf15Rd6QTY11rw' failed: \n(anonymous) @ blazor.server.js:1\nblazor.server.js:1 [2024-01-23T20:20:31.036Z] Information: (WebSockets transport) There was an error with the transport.\nblazor.server.js:1 [2024-01-23T20:20:31.036Z] Error: Failed to start the transport 'WebSockets': Error: WebSocket failed to connect. The connection could not be found on the server, either the endpoint may not be a SignalR endpoint, the connection ID is not present on the server, or there is a proxy blocking WebSockets. If you have multiple servers check that sticky sessions are enabled.\nlog @ blazor.server.js:1\nblazor.server.js:1 [2024-01-23T20:20:31.107Z] Warning: Failed to connect via WebSockets, using the Long Polling fallback transport. This may be due to a VPN or proxy blocking the connection. To troubleshoot this, visit https://aka.ms/blazor-server-using-fallback-long-polling.\nlog @ blazor.server.js:1\n:30500/_blazor?id=eC3fba0OdI6vu_KNZl9_Kg:1 \n        \n        \n       Failed to load resource: the server responded with a status of 404 (Not Found)\nblazor.server.js:1 Uncaught (in promise) Error: No Connection with that ID: Status code '404'\n    at ut.send (blazor.server.js:1:39443)\n    at async rt (blazor.server.js:1:36427)\n    at async _t._sendLoop (blazor.server.js:1:61855)\n:30500/_blazor?id=eC3fba0OdI6vu_KNZl9_Kg:1 \n        \n        \n       Failed to load resource: the server responded with a status of 404 (Not Found)\nblazor.server.js:1 Uncaught (in promise) Error: No Connection with that ID: Status code '404'\n    at ut.send (blazor.server.js:1:39443)\n    at async rt (blazor.server.js:1:36427)\n    at async _t._sendLoop (blazor.server.js:1:61855)\n:30500/_blazor?id=eC3fba0OdI6vu_KNZl9_Kg:1 \n        \n        \n       Failed to load resource: the server responded with a status of 404 (Not Found)\nblazor.server.js:1 Uncaught (in promise) Error: No Connection with that ID: Status code '404'\n    at ut.send (blazor.server.js:1:39443)\n    at async rt (blazor.server.js:1:36427)\n    at async _t._sendLoop (blazor.server.js:1:61855)\n:30500/_blazor?id=eC3fba0OdI6vu_KNZl9_Kg:1 \n        \n        \n       Failed to load resource: the server responded with a status of 404 (Not Found)\nblazor.server.js:1 Uncaught (in promise) Error: No Connection with that ID: Status code '404'\n    at ut.send (blazor.server.js:1:39443)\n    at async rt (blazor.server.js:1:36427)\n    at async _t._sendLoop (blazor.server.js:1:61855)\n`\n```\nIt appears that there is some problem with the WebSockets, but I don't know how to fix that.\nWhen served over the ingress, it looks like this. This is obviously not correct, but I'm only getting one error:\n```\n`blazor.server.js:2 Uncaught SyntaxError: Unexpected token 'I don't know what file that is because I can't open it from the DevTools Console and I am unable to locate it in the DevTools Source pane. It doesn't appear to be in the codebase for the application either.\nI am running Minikube on Windows Hyper-V. The application is the default server-side Blazor application from Visual Studio 2022. I made some alterations to the default Dockerfile though:\n```\n`#See https://aka.ms/customizecontainer to learn how to customize your debug container and how Visual Studio uses this Dockerfile to build your images for faster debugging.\n\nFROM mcr.microsoft.com/dotnet/aspnet:6.0 AS base\nWORKDIR /app\nEXPOSE 80\nEXPOSE 443\n\nFROM mcr.microsoft.com/dotnet/sdk:6.0 AS build\nARG BUILD_CONFIGURATION=Release\nWORKDIR /src\nCOPY [\"WebApp.csproj\", \"WebApp/\"]\nRUN dotnet restore \"./WebApp/./WebApp.csproj\"\nCOPY . .\nWORKDIR \"/src/WebApp\"\nRUN dotnet build \"./WebApp.csproj\" -c $BUILD_CONFIGURATION -o /app/build\n\nFROM build AS publish\nARG BUILD_CONFIGURATION=Release\nRUN dotnet publish \"./WebApp.csproj\" -c $BUILD_CONFIGURATION -o /app/publish /p:UseAppHost=false\n\nFROM base AS final\nWORKDIR /app\nCOPY --from=publish /app/publish .\nENTRYPOINT [\"dotnet\", \"WebApp.dll\"]\n`\n```\nAny clues as to what could be causing these two issues?\nEdit: Here are my Kubernetes .yaml files:\n```\n`apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: blazor\n  labels:\n    app: blazor\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: blazor\n  template:\n    metadata:\n      labels:\n        app: blazor\n    spec:\n      containers:\n      - name: blazor\n        image: registry/...\n        ports:\n        - containerPort: 80\n      imagePullSecrets:  \n      - name: test-webapp\n\n`\n```\n```\n`apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: blazor\n  name: blazor\n  namespace: default\nspec:\n  ports:\n  - nodePort: 30500\n    port: 80\n    protocol: TCP\n    targetPort: 80\n  selector:\n    app: blazor\n  type: NodePort\n`\n```\n```\n`apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: blazor-ingress\n  annotations:\n    nginx.ingress.kubernetes.io/rewrite-target: /$1\n    nginx.ingress.kubernetes.io/affinity: \"cookie\"\n    nginx.ingress.kubernetes.io/session-cookie-name: \"affinity\"\n    nginx.ingress.kubernetes.io/session-cookie-expires: \"14400\"\n    nginx.ingress.kubernetes.io/session-cookie-max-age: \"14400\"\nspec:\n  rules:\n    - host: blazor.test\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: blazor\n                port:\n                  number: 80\n`\n```",
      "solution": "We have 2 issues here\n\nIngress route\nSignalR with LoadBalancer\n\nIngress route\n`nginx.ingress.kubernetes.io/rewrite-target: /$1\n`\nRewrite has `$1` it means to place first `capturing group`, but what is it? Well it's something that can be used only when `nginx.ingress.kubernetes.io/use-regex` is added, so it just doesn't make sens, when request is send to get for file like `/styles.css` it would end up with something like `/$1styles.css` in Blazor app or anything else but not valid from your app point of view.\nYou can have for example\n`apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: blazor-ingress\n  annotations:\n    nginx.ingress.kubernetes.io/rewrite-target: /\n    nginx.ingress.kubernetes.io/affinity: \"cookie\"\n    nginx.ingress.kubernetes.io/session-cookie-name: \"affinity\"\n    nginx.ingress.kubernetes.io/session-cookie-expires: \"14400\"\n    nginx.ingress.kubernetes.io/session-cookie-max-age: \"14400\"\nspec:\n  rules:\n    - host: blazor.test\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: blazor\n                port:\n                  number: 80\n`\nAnd everything should work, but rewrite `/`(`path: /`) to `/`(`rewrite-target: /` end with what we had at beginning, so removing line with `rewrite-target` has exactly same effect.\nAnother example using regex\n`apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: blazor-ingress\n  annotations:\n    nginx.ingress.kubernetes.io/use-regex: \"true\"\n    nginx.ingress.kubernetes.io/rewrite-target: /$1\n    nginx.ingress.kubernetes.io/affinity: \"cookie\"\n    nginx.ingress.kubernetes.io/session-cookie-name: \"affinity\"\n    nginx.ingress.kubernetes.io/session-cookie-expires: \"14400\"\n    nginx.ingress.kubernetes.io/session-cookie-max-age: \"14400\"\nspec:\n  rules:\n    - host: blazor.test\n      http:\n        paths:\n          - path: /(.*)\n            pathType: Prefix\n            backend:\n              service:\n                name: blazor\n                port:\n                  number: 80\n`\nIn this case in `path` we have `/(.*)` and we have one `capturing group` what is `(.*)`(to make path `/` work correctly you must have `*` instead of `+`), then `rewrite` comes in and it says `/$1` as we have regex enabled in place of `$1` the value stored in first capturing group will be used here.\nSignalR with LoadBalancer\nSignalR to establish connection with with Blazor backend needs to send 2 request\n\nNegotiate\nConnect\n\nThe first one will send `connectionToken` in the response then this token will be used to connect, so if you have more then one replica there is a chance(with number of replica it's growing) that those two requests will end in two different one. To resolve it Microsoft suggest suggest using some additional annotations to force ingress(nginx) to use always the same replica based on cookie, so `Negotiate` and `Connect` will always hit the same replica and issue will not occur(also some GitHub issue about it.\nTo answer the question why it doesn't work with `NodePort` as we already got it sends two request they have to hit the same replica, but service `NodePort` doesn't have feature like `nginx.ingress.kubernetes.io/affinity` to force requests to the same replica all the time, and there is no solution for it expect using just one replica(or at least I don't know).",
      "question_score": 2,
      "answer_score": 2,
      "created_at": "2024-01-23T21:46:03",
      "url": "https://stackoverflow.com/questions/77869181/how-to-correctly-deploy-a-blazor-server-app-on-kubernetes"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "minikube",
      "question_id": 75607181,
      "title": "minikube curl: (6) Could not resolve host: service-name",
      "problem": "I have a ruby on rails deployment and I want to use it in the frontend deployment so I created a service exposing port 3000 called \"flicron-backend-service\"\nhere is the description of the service\n```\n`kubectl describe svc flicron-backend-service                      \nName:              flicron-backend-service\nNamespace:         default\nLabels:            io.kompose.service=flicron-backend-service\nAnnotations:       kompose.cmd: kompose convert -f docker-compose.yml\n                   kompose.version: 1.28.0 (c4137012e)\nSelector:          io.kompose.service=flicron-backend\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.107.112.244\nIPs:               10.107.112.244\nPort:              3000  3000/TCP\nTargetPort:        3000/TCP\nEndpoints:         10.244.0.144:3000\nSession Affinity:  None\nEvents:            \n`\n```\nI am trying to use the service name but it does not get resolved\nI have tried from inside minikube to curl the backend-service-name did not work\n```\n`curl flicron-backend-service:3000\ncurl: (6) Could not resolve host: flicron-backend-service\n\ncurl flicron-backend-service.default.svc.cluster.local:3000\ncurl: (6) Could not resolve host: flicron-backend-service.default.svc.cluster.local\n\n`\n```\nbut if I used the ip it works fine\n```\n`curl 10.107.112.244:3000\n# some HTML\n`\n```\nHere is my DNS configuration\n```\n`kubectl describe cm coredns -n kube-system\nName:         coredns\nNamespace:    kube-system\nLabels:       \nAnnotations:  \n\nData\n====\nCorefile:\n----\n.:53 {\n    log\n    errors\n    health {\n       lameduck 5s\n    }\n    ready\n    kubernetes cluster.local in-addr.arpa ip6.arpa {\n       pods insecure\n       fallthrough in-addr.arpa ip6.arpa\n       ttl 30\n    }\n    prometheus :9153\n    hosts {\n       192.168.49.1 host.minikube.internal\n       fallthrough\n    }\n    forward . /etc/resolv.conf {\n       max_concurrent 1000\n    }\n    cache 30\n    loop\n    reload\n    loadbalance\n}\n\nBinaryData\n====\n\nEvents:  \n\n`\n```\nFrontEnd Service\n```\n`apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    io.kompose.service: flicron-frontend-service\n  name: flicron-frontend-service\nspec:\n  selector:\n    app: flicron-frontend\n  type: NodePort\n  ports:\n    - name: http\n      port: 4000\n      targetPort: 4000\n      nodePort: 32123\n`\n```\nbackend service\n```\n`apiVersion: v1\nkind: Service\nmetadata:\n  annotations:\n    kompose.cmd: kompose convert -f docker-compose.yml\n    kompose.version: 1.28.0 (c4137012e)\n  creationTimestamp: null\n  labels:\n    io.kompose.service: flicron-backend-service\n  name: flicron-backend-service\nspec:\n  # clusterIP: None\n  ports:\n    - name: \"3000\"\n      port: 3000\n      targetPort: 3000\n  selector:\n    io.kompose.service: flicron-backend # this maps this service to that deployment\n\n`\n```",
      "solution": "I have tried from inside minikube to curl the backend-service-name did not work\n\nIf you explicitly meant that you've:\n\nconnected to your minikube instance (i.e. `minikube ssh`)\n`curl` the service name directly from `minikube` instance.\n\nThen it should not work.\nIt will work from a Pod/container that is targeting internal Kubernetes DNS resolution.\n\nLet me explain:\nDNS resolution within Kubernetes environment is handled by `coredns` residing in `kube-system` namespace. It is a containerized service that is hosted inside of Kubernetes. Information on how to connect to it is injected to Pods via Kubelet.\nYou can see it by:\n\n`kubectl run -it basic-pod --image=nginx -- /bin/bash`\n`cat /etc/resolv.conf`\n\n`nameserver 10.96.0.10 # \nMinikube itself does not have the core-dns configured in `/etc/hosts`.\nTry to contact your `Service` with an actual `Pod`:\n\n`kubectl run -it basic-pod --image=nginx -- /bin/bash`\n`apt update && apt install dnsutils -y` - `nginx` image used for simplicity\n`nslookup nginx` - there is a `Service` named `nginx` in my `minikube`\n\n`root@basic-pod:/# nslookup nginx\nServer:     10.96.0.10\nAddress:    10.96.0.10#53\n\nName:   nginx.default.svc.cluster.local\nAddress: 10.109.51.22\n`\nI encourage you to take a look on following documentation:\n\nKubernetes.io: Docs: Concepts: Services networking: DNS Pod Service",
      "question_score": 2,
      "answer_score": 2,
      "created_at": "2023-03-01T18:37:50",
      "url": "https://stackoverflow.com/questions/75607181/minikube-curl-6-could-not-resolve-host-service-name"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "minikube",
      "question_id": 70276205,
      "title": "NodePort type service not accessible outside cluster",
      "problem": "I am trying to setup a local cluster using minikube in a Windows machine. Following some tutorials in `kubernetes.io`, I got the following manifest for the cluster:\n```\n`apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: external-nginx-deployment\n  labels:\n   app: external-nginx\nspec: \n  selector: \n    matchLabels: \n      app: external-nginx\n  replicas: 2 \n  template: \n    metadata: \n      labels:\n        app: external-nginx\n    spec: \n      containers: \n      - name: external-nginx\n        image: nginx \n        ports: \n        - containerPort: 80 \n---\napiVersion: v1\nkind: Service \nmetadata: \n  name: expose-nginx \n  labels: \n   service: expose-nginx\nspec: \n  type: NodePort \n  selector: \n    app: external-nginx \n  ports: \n  - protocol: TCP\n    port: 80\n    targetPort: 80\n    nodePort: 32000\n`\n```\nIf I got things right, this should create a pod with a nginx instance and expose it to the host machine at port 32000.\nHowever, when I run `curl http://$(minikube ip):32000`, I get a connection refused error.\nI ran bash inside the service expose-nginx via `kubectl exec svc/expose-nginx -it bash` and from there I was able to access the external-nginx pods normally, which lead me to believe it is not a problem within the cluster.\nI also tried to change the type of the service to LoadBalancer and enable the `minikube tunnel`, but got the same result.\nIs there something I am missing?",
      "solution": "Almost always by default `minikube` uses `docker` driver for the `minikube` VM creation. In the host system it looks like a big docker container for the VM in which other kubernetes components are run as containers as well. Based on tests `NodePort` for services often doesn't work as it's supposed to like accessing the service exposed via `NodePort` should work on `minikube_IP:NodePort` address.\nSolutions are:\n\nfor local testing use `kubectl port-forward` to expose service to the local machine (which OP did)\n\nuse `minikube service` command which will expose the service to the host machine. Works in a very similar way as `kubectl port-forward`.\n\ninstead of `docker` driver use proper virtual machine which will get its own IP address (`VirtualBox` or `hyperv` drivers - depends on the system). Reference.\n\n(Not related to `minikube`) Use built-in feature `kubernetes` in Docker Desktop for Windows. I've already tested it and service type should be `LoadBalancer` - it will be exposed to the host machine on `localhost`.",
      "question_score": 2,
      "answer_score": 2,
      "created_at": "2021-12-08T14:56:19",
      "url": "https://stackoverflow.com/questions/70276205/nodeport-type-service-not-accessible-outside-cluster"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "minikube",
      "question_id": 74784162,
      "title": "How to access ExternalName service in kubernetes using minikube?",
      "problem": "I understand that a service of type ExternalName will point to a specified deployment that is not exposed externally using the specified external name as the DNS name. I am using minikube in my local machine with docker drive. I created a deployment using a custom image. When I created a service with default type (Cluster IP) and Load Balancer for the specific deployment I was able to access it after port forwarding to the local ip address. This was possible for service type ExternalName also but accessible using the ip address and not the specified external name.\nAccording to my understanding service of type ExternalName should be accessed when using the specified external name. But I wasn't able to do it. Can anyone say how to access an external name service and whether my understanding is correct?\nThis is the `externalName.yaml` file I used.\n```\n`apiVersion: v1\nkind: Service\nmetadata:\n  name: k8s-hello-test\nspec:\n  selector:\n    app: k8s-yaml-hello\n  ports:\n  - port: 3000\n    targetPort: 3000\n  type: ExternalName\n  externalName: k8s-hello-test.com\n`\n```\nAfter port forwarding using `kubectl port-forward service/k8s-hello-test 3000:3000` particular deployment was accessible using `http://127.0.0.1:300`\nBut even after adding it to etc/hosts file, cannot be accessed using `http://k8s-hello-test.com`",
      "solution": "According to my understanding service of type ExternalName should be\naccessed when using the specified external name. But I wasn't able to\ndo it. Can anyone say how to access it using its external name?\n\nYou are wrong, external service is for making external connections. Suppose if you are using the third party Geolocation API like https://findmeip.com you can leverage the External Name service.\n\nAn ExternalName Service is a special case of Service that does not\nhave selectors and uses DNS names instead. For more information, see\nthe ExternalName section later in this document.\n\nFor example\n```\n`apiVersion: v1\nkind: Service\nmetadata:\n  name: geolocation-service\nspec:\n  type: ExternalName\n  externalName: api.findmeip.com\n`\n```\nSo your application can connect to geolocation-service, which which forwards requests to the external DNS mentioned in the service.\nAs ExternalName service does not have selectors you can not use the port-forwarding as it connects to POD and forwards the request.\nRead more at : https://kubernetes.io/docs/concepts/services-networking/service/#externalname",
      "question_score": 2,
      "answer_score": 1,
      "created_at": "2022-12-13T12:31:10",
      "url": "https://stackoverflow.com/questions/74784162/how-to-access-externalname-service-in-kubernetes-using-minikube"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "minikube",
      "question_id": 70639008,
      "title": "Minikube flag not found",
      "problem": "I have a problem with minikube\nwhen I want to create deployment manifest files, I receive an error\nwhen I write this code:\n`minikube kubectl create -f .`\nI got this Error:\nError: unknown shorthand flag: 'f' in -f\nSee 'minikube kubectl --help' for usage.\nbut not only this, I try to write another command but again same error happening\n`minikube kubectl delete daemonsets,replicasets,services,deployments,pods,rc,pvc --all `\nError: unknown flag: --all\nSee 'minikube kubectl --help' for usage.\nplease help me.\nthanks",
      "solution": "AFAIK, --all is not a valid flag. Valid flag is --all-namespaces or just -A.\nHowever, \"kubectl delete\" does not take -A as it needs the resource name for deletion.\nTo accomplish what you are trying to do you will have to write a loop to delete the objects 1 by one using\n```\n`kubectl get daemonsets,replicasets,services,deployments,pods,rc,pvc -A --no-headers | while read line; do \n    namespace=$(echo $line | awk '{print $1}')\n    resource=$(echo $line | awk '{print $2}')\n    kubectl delete ${resource} -n ${namespace}\ndone\n`\n```\nExecution - BE extremely careful with this as it will delete all queried resources in all namespaces including those in kube-system namespace:\n```\n`controlplane $ kubectl get daemonsets,replicasets,services,deployments,pods,rc,pvc -A --no-headers | while read line; do \n>     namespace=$(echo $line | awk '{print $1}')\n>     resource=$(echo $line | awk '{print $2}')\n>     kubectl delete ${resource} -n ${namespace}\n> done\ndaemonset.extensions \"kube-keepalived-vip\" deleted\ndaemonset.extensions \"kube-proxy\" deleted\ndaemonset.extensions \"weave-net\" deleted\nreplicaset.extensions \"coredns-fb8b8dccf\" deleted\nreplicaset.extensions \"katacoda-cloud-provider-d5cb9d656\" deleted\nservice \"kubernetes\" deleted\nservice \"kube-dns\" deleted\ndeployment.extensions \"coredns\" deleted\ndeployment.extensions \"katacoda-cloud-provider\" deleted\n`\n```",
      "question_score": 2,
      "answer_score": 1,
      "created_at": "2022-01-09T07:39:42",
      "url": "https://stackoverflow.com/questions/70639008/minikube-flag-not-found"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "minikube",
      "question_id": 69364566,
      "title": ".Net Core API container on Minikube not running",
      "problem": "I have a deployment on Minikube for a .Net Core 5 API that is not returning response when I try to invoke it from Postman. When I run a GET from Postman to the exposed port (32580) and endpoint `http:// localhost:32580/api/platforms/` I get:\n```\n`Error: getaddrinfo ENOTFOUND\n`\n```\nOddly enough I was previously getting a `Connection refused` (before I restarted my Docker desktop). The container works perfectly when I use Docker but once I deployed it to Kubernetes context it no longer works.\nI am unsure how exactly I can debug the container and get more meaningful error detail.\nI have tried the following:\n\nChecking status of Deployment (platforms-depl)\n\n```\n`NAME             READY   UP-TO-DATE   AVAILABLE   AGE \nhello-minikube   1/1     1            1           134d\nping-google      0/1     1            0           2d2h\nplatforms-depl   1/1     1            1           115m\n`\n```\n\nChecking status of Pod (platforms-depl-84d7f5bdc6-sgxcp)\n\n```\n`NAME                              READY   STATUS             RESTARTS   AGE \nhello-minikube-6ddfcc9757-6mfmf   1/1     Running            21         134d\nping-google-5f84d66fcc-kbb7j      0/1     ImagePullBackOff   151        2d2h\nplatforms-depl-84d7f5bdc6-sgxcp   1/1     Running            1          115m\n`\n```\n\nRunning `kubectl describe pod platforms-depl-84d7f5bdc6-sgxcp` gives below output (truncated):\n\n```\n`Status:       Running\nIP:           172.17.0.3\nIPs:\n  IP:           172.17.0.3\nControlled By:  ReplicaSet/platforms-depl-84d7f5bdc6\nContainers:\n  platformservice:\n    Container ID:   docker://a73ce2dc737206e502df94066247299a6dcb6a038087d0f42ffc6e3b9dd194dd\n    Image:          golide/platformservice:latest\n    Image ID:       docker-pullable://golide/platformservice@sha256:bbed5f1d7238d2c466a6782333f8512d2e464f94aa64d8670214646a81b616c7      \n    Port:           \n    Host Port:      \n    State:          Running\n      Started:      Tue, 28 Sep 2021 15:12:22 +0200\n    Ready:          True\n    Restart Count:  0\n    Environment:    \n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-rl5kf (ro)\nConditions:\n  Type              Status\n  Initialized       True\n  Ready             True\n  ContainersReady   True\n  PodScheduled      True\n`\n```\n\nWhen I run `docker ps` I cannot see the container and it also doesn't appear in the list of running containers in VS Code Docker/Containers extension.\n\n`kubectl get services` gives me the following:\n\n```\n`NAME                    TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE \nhello-minikube          NodePort    10.99.23.44            8080:31378/TCP   134d\nkubernetes              ClusterIP   10.96.0.1              443/TCP          134d\npaymentsapi             NodePort    10.111.243.3           5000:30385/TCP   108d\nplatformnpservice-srv   NodePort    10.98.131.95           80:32580/TCP     2d2h\n`\n```\nThen tried pinging the ClusterIP:\n```\n`Pinging 10.98.131.95 with 32 bytes of data:\nRequest timed out.\nRequest timed out.\n \nPing statistics for 10.98.131.95:\nPackets: Sent = 4, Received = 0, Lost = 4 (100% loss), \n`\n```\nWhat am I missing?\nI read in suggestions I have to exec into the pod so that I get meaningful output but I'm not sure of the exact commands to run. I tried:\n```\n`kubectl exec POD -p platforms-depl-84d7f5bdc6-sgxcp\n`\n```\nonly to get error:\n```\n`kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead.\nError from server (NotFound): pods \"POD\" not found\n`\n```\nMy environment Docker Linux containers with WSL2 on Windows 10.\nWhat am I missing?",
      "solution": "First thing which is worth to note that generally minikube has a lot of possible drivers to choose from - in my case I found the `docker` drive to be most easiest to use.\nMy setup is:\n\nWSL2\nDocker Desktop with Docker Desktop WLS 2 backend enabled\n\nI used following command to start minikube: `minikube start --driver=docker`.\nIf you are using other driver I suggest moving to the `docker` one.\nAnswering your question:\nWhat am I missing ?\nBy setting nodePort service type you are exposing your deployment / replica set using node IP address which not accessible from Windows host (when using `docker` driver). It's because all Kubernetes cluster resources are setup inside Docker container, which is isolated.\nHowever, minikube offers simple solution to make available specified nodePort service to your Windows host. Just run `minikube service` command which will create a tunnel. Let's check it.\nYou setup `platformnpservice-srv` service so you need to use this name in `minikube service` command instead of `testmini` which I used:\n```\n`minikube service --url testmini\n\ud83c\udfc3  Starting tunnel for service testmini.\n|-----------|----------|-------------|------------------------|\n| NAMESPACE |   NAME   | TARGET PORT |          URL           |\n|-----------|----------|-------------|------------------------|\n| default   | testmini |             | http://127.0.0.1:33849 |\n|-----------|----------|-------------|------------------------|\nhttp://127.0.0.1:33849\n\u2757  Because you are using a Docker driver on linux, the terminal needs to be open to run it.\n`\n```\nNote the last sentence - we need to keep this terminal window to be open, otherwise the tunnel won't be established.\nNow, on my Windows host, in the browser I'm opening: `http://127.0.0.1:33849/api/platforms` website. The output is following:\n```\n`[{\"id\":1,\"name\":\"Dot Net\",\"publisher\":\"Microsoft\",\"cost\":\"Free\"},{\"id\":2,\"name\":\"Linux\",\"publisher\":\"Ubuntu\",\"cost\":\"Free\"},{\"id\":3,\"name\":\"Kubernetes\",\"publisher\":\"CNCF\",\"cost\":\"Free\"},{\"id\":4,\"name\":\"SQL Express\",\"publisher\":\"Microsoft\",\"cost\":\"Free\"}]\n`\n```\nVoila! Seems that everything is working properly.\nAlso, other notes:\n\ntried pinging the ClusterIP\n\nThe ClusterIP is internal address which is only accessible from the cluster, so you can't ping it from Windows host neither WSL2.\n\nkubectl exec POD -p platforms-depl-84d7f5bdc6-sgxcp\n\nAs output suggests, you need to specify command you want to exec on the pod. If you just want to get bash shell, use the following:\n```\n`kubectl exec -it platforms-depl-84d7f5bdc6-sgxcp -- sh\n`\n```",
      "question_score": 2,
      "answer_score": 1,
      "created_at": "2021-09-28T17:32:44",
      "url": "https://stackoverflow.com/questions/69364566/net-core-api-container-on-minikube-not-running"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "minikube",
      "question_id": 68936694,
      "title": "Minikube Nginx Ingress not finding service endpoint",
      "problem": "I'm having some trouble getting the Nginx ingress controller working in my Minikube cluster. It's likely to be some faults in Ingress configuration but I cannot pick it out.\nFirst, I deployed a service and it worked well without ingress.\n```\n`kind: Service\napiVersion: v1\nmetadata:\n  name: online\n  labels:\n    app: online\nspec:\n  selector:\n    app: online\n  ports:\n  - protocol: TCP\n    port: 8080\n    targetPort: 5001\n  type: LoadBalancer\n\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: online\n  labels:\n    app: online\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: online\n  template:\n    metadata:\n      labels:\n        app: online\n      annotations:\n        dapr.io/enabled: \"true\"\n        dapr.io/app-id: \"online\"\n        dapr.io/app-port: \"5001\"\n        dapr.io/log-level: \"debug\"\n        dapr.io/sidecar-liveness-probe-threshold: \"300\"\n        dapr.io/sidecar-readiness-probe-threshold: \"300\"\n    spec:\n      containers:\n      - name: online\n        image: online:latest\n        ports:\n        - containerPort: 5001\n        env:\n        - name: ADDRESS\n          value: \":5001\"\n        - name: DAPR_HTTP_PORT\n          value: \"8080\"\n        imagePullPolicy: Never\n`\n```\nThen check its url\n```\n`minikube service online --url\nhttp://192.168.49.2:32323\n\n`\n```\nIt looks ok for requests.\n```\n`curl http://192.168.49.2:32323/userOnline\nOK\n`\n```\nAfter that I tried to apply nginx ingress offered by minikube.\nI installed ingress and run an example by referring to this and it's all ok.\nLastly, I configured my Ingress.\n```\n`apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: online-ingress\n  annotations:\nspec:\n  rules:\n    - host: online\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: online\n                port:\n                  number: 8080\n`\n```\nAnd changed /etc/hosts by adding line\n```\n`192.168.49.2    online\n`\n```\nAnd Test:\n```\n`curl online/userOnline\n502 Bad Gateway\n`\n```\nThe logs are like this:\n```\n`192.168.49.1 - - [26/Aug/2021:09:45:56 +0000] \"GET /userOnline HTTP/1.1\" 502 150 \"-\" \"curl/7.68.0\" 80 0.002 [default-online-8080] [] 172.17.0.5:5001, 172.17.0.5:5001, 172.17.0.5:5001 0, 0, 0 0.004, 0.000, 0.000 502, 502, 502 578ea1b1471ac973a2ac45ec4c35d927\n2021/08/26 09:45:56 [error] 2514#2514: *426717 upstream prematurely closed connection while reading response header from upstream, client: 192.168.49.1, server: online, request: \"GET /userOnline HTTP/1.1\", upstream: \"http://172.17.0.5:5001/userOnline\", host: \"online\"\n2021/08/26 09:45:56 [error] 2514#2514: *426717 connect() failed (111: Connection refused) while connecting to upstream, client: 192.168.49.1, server: online, request: \"GET /userOnline HTTP/1.1\", upstream: \"http://172.17.0.5:5001/userOnline\", host: \"online\"\n2021/08/26 09:45:56 [error] 2514#2514: *426717 connect() failed (111: Connection refused) while connecting to upstream, client: 192.168.49.1, server: online, request: \"GET /userOnline HTTP/1.1\", upstream: \"http://172.17.0.5:5001/userOnline\", host: \"online\"\nW0826 09:45:56.918446       7 controller.go:977] Service \"default/online\" does not have any active Endpoint.\nI0826 09:46:21.345177       7 status.go:281] \"updating Ingress status\" namespace=\"default\" ingress=\"online-ingress\" currentValue=[] newValue=[{IP:192.168.49.2 Hostname: Ports:[]}]\nI0826 09:46:21.349078       7 event.go:282] Event(v1.ObjectReference{Kind:\"Ingress\", Namespace:\"default\", Name:\"online-ingress\", UID:\"b69e2976-09e9-4cfc-a8e8-7acb51799d6d\", APIVersion:\"networking.k8s.io/v1beta1\", ResourceVersion:\"23100\", FieldPath:\"\"}): type: 'Normal' reason: 'Sync' Scheduled for sync\n\n`\n```\nI found the error is very about annotations of Ingress. If I changed it to:\n```\n`  annotations:\n    nginx.ingress.kubernetes.io/rewrite-target: /$1\n`\n```\nThe error would be:\n```\n`404 page not found\n`\n```\nand logs:\n```\n`I0826 09:59:21.342251       7 status.go:281] \"updating Ingress status\" namespace=\"default\" ingress=\"online-ingress\" currentValue=[] newValue=[{IP:192.168.49.2 Hostname: Ports:[]}]\nI0826 09:59:21.347860       7 event.go:282] Event(v1.ObjectReference{Kind:\"Ingress\", Namespace:\"default\", Name:\"online-ingress\", UID:\"8ba6fe97-315d-4f00-82a6-17132095fab4\", APIVersion:\"networking.k8s.io/v1beta1\", ResourceVersion:\"23760\", FieldPath:\"\"}): type: 'Normal' reason: 'Sync' Scheduled for sync\n192.168.49.1 - - [26/Aug/2021:09:59:32 +0000] \"GET /userOnline HTTP/1.1\" 404 19 \"-\" \"curl/7.68.0\" 80 0.002 [default-online-8080] [] 172.17.0.5:5001 19 0.000 404 856ddd3224bbe2bde9d7144b857168e0\n`\n```\nOther infos.\n```\n`NAME     TYPE           CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE\nonline   LoadBalancer   10.111.34.87        8080:32323/TCP   6h54m\n`\n```\nThe example I mentioned above is a `NodePort` service and mine is a `LoadBalancer`, that's the biggest difference. But I don't know why it does not work for me.",
      "solution": "Moving this out of comments so it will be visible.\n\nIngress\nMain issue was with `path` in ingress rule since application serves traffic on `online/userOnline`. If requests go to `online` then ingress returns `404`.\nRewrite annotation is not needed in this case as well.\n`ingress.yaml` should look like:\n```\n`apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: online-ingress\n#  annotations:\nspec:\n  rules:\n    - host: online\n      http:\n        paths:\n          - path: /userOnline\n            pathType: Prefix\n            backend:\n              service:\n                name: online\n                port:\n                  number: 8080\n`\n```\nMore details about ingress\n\nLoadBalancer on Minikube\nSince minikube is considered as `bare metal` installation, to get `external IP` for service/ingress, it's necessary to use specially designed `metallb` solution.\nMetalLB is a load-balancer implementation for bare metal Kubernetes clusters, using standard routing protocols.\nIt ships as add-on for `minikube` and can be enabled with:\n```\n`minikube addons enable metallb\n`\n```\nAnd it needs to create a `configMap` with setup. Please refer to metallb configuration",
      "question_score": 2,
      "answer_score": 1,
      "created_at": "2021-08-26T12:17:40",
      "url": "https://stackoverflow.com/questions/68936694/minikube-nginx-ingress-not-finding-service-endpoint"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "minikube",
      "question_id": 68623069,
      "title": "Rails - ActionCable: &quot;action_cable.url&quot; for Kubernetes/Minikube",
      "problem": "What config.action_cable.url should be configured for websockets / Rails / Kubernetes /Minukube with Nginx?\nWhen running \"docker-compose\" locally an Nginx processes in front of a Rails process (not API only, but with SSR)  and a standalone Cable process (cf the guides), the websockets work fine by passing the following server-side (in say \"/config/application.rb\", with `action_cable_meta_tag` set in the layouts):\n`config.action_cable.url = 'ws://localhost:28080'\n`\nI am targetting Kubernetes with Minikube locally:  I deployed Nginx in front of a Rails deployment (RAILS_ENV=production) along with a Cable deployment but I can't make it work. The Cable service is internal of type \"ClusterIP\", with \"port\" and \"targetPort\". I tried several variations.\nAny advice?\nNote that I use Nginx -> Rails + Cable on Minikube, and the entry-point is the Nginx service, external of kind LoadBalancer where I used:\n```\n`upstream rails {\n  server rails-svc:3000;\n}\nserver {\n  listen 9000 default_server;\n  root /usr/share/nginx/html;\n  try_files  $uri @rails;\n  add_header  Cache-Control public;\n  add_header  Last-Modified \"\";\n  add_header  Etag \"\";\n      \n\n  location @rails {\n    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n    proxy_set_header X-Forwarded-Host $http_host;\n    proxy_set_header Host $http_host;\n    proxy_pass_header   Set-Cookie;\n    proxy_redirect off;\n    proxy_pass http://rails;\n\n    proxy_http_version 1.1;\n    proxy_set_header Upgrade $http_upgrade;\n    proxy_set_header Connection \"upgrade\";\n  }\n}\n`\n```\nTo allow any origin, I also set:\n`config.action_cable.disable_request_forgery_protection = true\n`",
      "solution": "I have an answer: in a Minikube cluster, don't put anything and disable forgery protection, Rails will default the correct value. When Nginx is front of a Rails pod and a standalone ActionCable/websocket pod (the Rails image is launched with `bundle exec puma -p 28080 cable/config.ru`), if I name \"cable-svc\" the service that exposes the ActionCable container, and \"rails-svc\" the one for the Rails container, you need to:\n\nin K8, don't set the config for `CABLE_URI`\nin the Rails backend, you don't have the URL (unknown `127.0.0.1:some_port`), do:\n\n`# config.action_cable.url \n\nin the Nginx config, add a specific location for the \"/cable\" path:\n\n```\n`upstream rails {\n  server rails-svc:3000;\n}\nserver {\n  [...root, location @rails {...}]\n  location /cable {\n    proxy_http_version 1.1;\n    proxy_set_header Upgrade $http_upgrade;\n    proxy_set_header Connection \"Upgrade\";\n    proxy_pass \"http://cable-svc:28080\";\n  }\n}\n`\n```\nCheck the logs, no more WebSockets in Rails, and Cable responds.",
      "question_score": 2,
      "answer_score": 1,
      "created_at": "2021-08-02T16:10:37",
      "url": "https://stackoverflow.com/questions/68623069/rails-actioncable-action-cable-url-for-kubernetes-minikube"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "minikube",
      "question_id": 66122566,
      "title": "Accessing local PostgreSQL instance inside Minikube",
      "problem": "Is there anything wrong with how I am trying to configure my Minikube cluster in a way the pods can access the PostgreSQL instance within the same machine?\nI've access the `/etc/hosts` within the Minikube cluster via `minikube ssh` and returns:\n```\n`127.0.0.1       localhost\n127.0.1.1       minikube\n192.168.99.1    host.minikube.internal\n192.168.99.110  control-plane.minikube.internal\n`\n```\ndatabase-service.yaml\n```\n`apiVersion: v1\nkind: Service\nmetadata:\n  name: service-database\nspec:\n  type: ExternalName\n  externalName: host.minikube.internal\n  ports:\n    - port: 5432\n      targetPort: 5432\n`\n```\npod-deployment.yaml\n```\n`apiVersion: apps/v1\nkind: Deployment\nspec:\n  ...\n  template:\n    ...\n    spec:\n      containers:\n        - name: \n          image: \n          env:\n            - name: DB_URL\n              value: \"jdbc:postgresql://service-database/\"\n          ports:\n            - containerPort: 8080\n`\n```\nNote: `DB_URL` environment variable points to the `spring.datasource.url` in the `application.properties` in SpringBoot.\nThen when I tried to get the logs printed, I am getting this exception:\n```\n`Caused by: java.net.UnknownHostException: service-database\n`\n```",
      "solution": "I've access the /etc/hosts within the Minikube cluster via minikube ssh and returns\n\nThat may be true, but for the same reason kubernetes does not expose the `/etc/hosts` of its Nodes, nor will minikube do the same thing. Kubernetes has its own DNS resolver, and thus its own idea of what should be in `/etc/hosts` (docker does the same thing -- it similarly does not just expose the host's `/etc` but rather allows the user to customize that behavior on container launch)\nThere is a formal mechanism to tell kubernetes that you wish to manage the DNS resolution endpoints manually -- that's what a Headless Service does, although usually the \"manually\" part is done by the `StatefulSet` controller, but there's nothing stopping other mechanisms from grooming that list:\n`apiVersion: v1\nkind: Service\nmetadata:\n  name: service-database\nspec:\n  type: ClusterIP\n  # yes, literally the word \"None\"\n  clusterIP: None\n  ports:\n    - name: 5432-5432\n      port: 5432\n      targetPort: 5432\n---\napiVersion: v1\nkind: Endpoints\nmetadata:\n  name: service-database\nsubsets:\n- addresses:\n  - ip: 192.168.99.1\n  ports:\n  - name: 5432-5432\n    port: 5432\n    protocol: TCP\n`\nand now the internal DNS will resolve `service-database` to be the answers `192.168.99.1` and also populate the SRV records just like normal",
      "question_score": 2,
      "answer_score": 1,
      "created_at": "2021-02-09T17:00:44",
      "url": "https://stackoverflow.com/questions/66122566/accessing-local-postgresql-instance-inside-minikube"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "minikube",
      "question_id": 73588464,
      "title": "minikube failed to start : Exiting due to HOST_JUJU_LOCK_PERMISSION",
      "problem": "i wanted to install minikube and after the start command a got the following error text :\n```\n`\ud83d\ude04  minikube v1.26.1 on Ubuntu 22.04\n\u2757  minikube skips various validations when --force is supplied; this may lead to unexpected behavior\n\u2728  Using the docker driver based on existing profile\n\ud83d\uded1  The \"docker\" driver should not be used with root privileges. If you wish to continue as root, use --force.\n\ud83d\udca1  If you are running minikube within a VM, consider using --driver=none:\n\ud83d\udcd8    https://minikube.sigs.k8s.io/docs/reference/drivers/none/\n\ud83d\udca1  Tip: To remove this root owned cluster, run: sudo minikube delete\n\ud83d\udc4d  Starting control plane node minikube in cluster minikube\n\ud83d\ude9c  Pulling base image ...\n\u270b  Stopping node \"minikube\"  ...\n\ud83d\uded1  Powering off \"minikube\" via SSH ...\n\ud83d\udd25  Deleting \"minikube\" in docker ...\n\ud83e\udd26  StartHost failed, but will try again: boot lock: unable to open /tmp/juju-mke11f63b5835bf422927bf558fccac7a21a838f: permission denied\n\ud83d\ude3f  Failed to start docker container. Running \"minikube delete\" may fix it: boot lock: unable to open /tmp/juju-mke11f63b5835bf422927bf558fccac7a21a838f: permission denied\n\n\u274c  Exiting due to HOST_JUJU_LOCK_PERMISSION: Failed to start host: boot lock: unable to open /tmp/juju-mke11f63b5835bf422927bf558fccac7a21a838f: permission denied\n\ud83d\udca1  Suggestion: Run 'sudo sysctl fs.protected_regular=0', or try a driver which does not require root, such as '--driver=docker'\n\ud83c\udf7f  Related issue: https://github.com/kubernetes/minikube/issues/6391\n`\n```",
      "solution": "i tried this command and it worked for me :\n```\n`minikube delete && minikube start\n`\n```",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2022-09-03T00:40:54",
      "url": "https://stackoverflow.com/questions/73588464/minikube-failed-to-start-exiting-due-to-host-juju-lock-permission"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "minikube",
      "question_id": 75204589,
      "title": "minikube ingress on macos appears to be working but never connects (times out no error)",
      "problem": "MacOS Big Sur 11.6.8\nminikube version: v1.28.0\nFollowing several tutorials on ingress and attempting to get it working locally. Everything appears to work: manual `minikube service foo` works, `kubectl get ingress` shows an IP, pinging the designated host name resolves the expected IP, etc. I went through a few different tutes with the same results.\nI boiled it down to the simplest replication from the tutorial at kubernetes.io :\n```\n`# kubectl create deployment web --image=gcr.io/google-samples/hello-app:1.0\n# kubectl expose deployment web --type=NodePort --port=8080\n# kubectl get service web (ensure it's a node port)\n# minikube service web --url (test url)\n# kubectl apply -f ingress_hello_world.yaml\n# curl localkube.com\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: example-ingress\n  annotations:\n    nginx.ingress.kubernetes.io/rewrite-target: /$1\nspec:\n  rules:\n    - host: localkube.com\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: web\n                port:\n                  number: 8080\n`\n```\nManual service works:\n```\n`>minikube service web --url\nhttp://127.0.0.1:50111\n\u2757  Because you are using a Docker driver on darwin, the terminal needs to be open to run it.\n\n>curl http://127.0.0.1:50111\nHello, world!\nVersion: 1.0.0\nHostname: web-84fb9498c7-hnphb\n`\n```\nIngress looks good:\n```\n`>minikube addons list | grep ingress\n| ingress                     | minikube | enabled \u2705   | Kubernetes                     |\n\n>kubectl get ingress\nNAME              CLASS   HOSTS           ADDRESS        PORTS   AGE\nexample-ingress   nginx   localkube.com   192.168.49.2   80      15m\n`\n```\nping resolves the address mapped in /etc/hosts:\n```\n`>ping localkube.com\nPING localkube.com (192.168.49.2): 56 data bytes\n`\n```\nI have looked through similar questions with no positive results. I have gone from this simple example to apache to mongo deployments via config files. Each time I can get to the app through a manual service mapping or by creating an external service (LoadBalancer / nodePort), but when I get to the Ingress part the config applies with no errors and everything appears to be working except for it actually... working.",
      "solution": "Based on Veera's answer, I looked into the ingress issue with macOS and `minikube tunnel`. To save others the hassle, here is how I resolved the issue:\n\ningress doesn't seem to work on macOS (the different pages say \"with docker\" but I had the same outcome with other drivers like hyperkit.\nthe issue seems to be IP / networking related. You can not get to the minikube IP from your local workstation. If you first run `minikube ssh` you can ping and curl the minikube IP and the domain name you mapped to that IP in /etc/hosts. However, this does not help trying to access the service from a browser.\nthe solution is to map the domain names to 127.0.0.1 in /etc/hosts (instead of the ingress assigned IP) and use ingress components to control the domain-name -> service mappings as before...\nthen starting a tunnel with `sudo minikube tunnel` will keep a base tunnel open, and create tunneling for any existing or new ingress components. This combined with the ingress rules will mimic host header style connecting to any domain resolving to the local host.\n\nHere is a full example of a working solution on mac. Dump this to a file named ingress_hello_world.yaml and follow the commented instructions to achieve a simple ingress solution that routes 2 domains to 2 different services (note this will work with pretty much any internal service, and can be a ClusterIP instead of NodePort):\n```\n`apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: example-ingress\nspec:\n  ingressClassName: nginx\n  rules:\n    - host: test1.com\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: web\n                port:\n                  number: 8080\n    - host: test2.com\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: web2\n                port:\n                  number: 8080\n\n# Instructions:\n# start minikube if not already\n# >minikube start --vm-driver=docker\n#\n# enable ingress if not already\n# >minikube addons enable ingress\n# >minikube addons list | grep \"ingress \"\n# | ingress                     | minikube | enabled \u2705   | Kubernetes                     |\n#\n# >kubectl create deployment web --image=gcr.io/google-samples/hello-app:1.0\n# deployment.apps/web created\n#\n# >kubectl expose deployment web --type=NodePort --port=8080\n# service/web exposed\n#\n# >kubectl create deployment web2 --image=gcr.io/google-samples/hello-app:2.0\n# deployment.apps/web2 created\n#\n# >kubectl expose deployment web2 --port=8080 --type=NodePort\n# service/web2 exposed\n#\n# >kubectl get service | grep web\n# web          NodePort    10.101.19.188           8080:31631/TCP   21m\n# web2         NodePort    10.102.52.139           8080:30590/TCP   40s\n#\n# >minikube service web --url\n# http://127.0.0.1:51813\n# \u2757  Because you are using a Docker driver on darwin, the terminal needs to be open to run it.\n#\n# ------ in another console ------\n# >curl http://127.0.0.1:51813\n#                          ^---- this must match the port from the output above\n# Hello, world!\n# Version: 1.0.0     sudo minikube tunnel\n# \u2705  Tunnel successfully started\n#\n# (leave open, will show the following when you start an ingress component)\n# Starting tunnel for service example-ingress.\n# --------------------------------\n#\n# >kubectl apply -f ingress_hello_world.yaml\n# ingress.networking.k8s.io/example-ingress created\n#\n# >kubectl get ingress example-ingress --watch\n# NAME              CLASS   HOSTS                 ADDRESS   PORTS   AGE\n# example-ingress   nginx   test1.com,test2.com             80      15s\n# example-ingress   nginx   test1.com,test2.com   192.168.49.2   80      29s\n#                wait for this to be populated ----^\n#\n# >cat /etc/hosts | grep test\n# 127.0.0.1    test1.com\n# 127.0.0.1    test2.com\n#        ^---- set this to localhost ip\n#\n# >ping test1.com\n# PING test1.com (127.0.0.1): 56 data bytes\n#\n# >curl test1.com\n# Hello, world!\n# Version: 1.0.0\n# Hostname: web-84fb9498c7-w6bkc\n#\n# >curl test2.com\n# Hello, world!\n# Version: 2.0.0\n# Hostname: web2-7df4dcf77b-66g5b\n\n# ------- Cleanup:\n# stop tunnel\n#\n# >kubectl delete -f ingress_hello_world.yaml\n# ingress.networking.k8s.io \"example-ingress\" deleted\n#\n# >kubectl delete service web\n# service \"web\" deleted\n#\n# >kubectl delete service web2\n# service \"web2\" deleted\n#\n# >kubectl delete deployment web\n# deployment.apps \"web\" deleted\n#\n# >kubectl delete deployment web2\n# deployment.apps \"web2\" deleted\n`\n```",
      "question_score": 1,
      "answer_score": 6,
      "created_at": "2023-01-23T00:17:54",
      "url": "https://stackoverflow.com/questions/75204589/minikube-ingress-on-macos-appears-to-be-working-but-never-connects-times-out-no"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "minikube",
      "question_id": 68820608,
      "title": "What is the correct URL to use for GET requests to another Kubernetes container?",
      "problem": "I'm trying to create a simple microservice, where a JQuery app in one Docker container uses this code to get a JSON object from another (analytics) app that runs in a different container:\n```\n`\n$(document).ready(function(){\n$('#get-info-btn').click(function(){\n  $.get(\"http://localhost:8084/productinfo\", \n  function(data, status){          \n    $.each(data, function(i, obj) {\n      //some code\n    });   \n  });\n});\n});\n \n`\n```\nThe other app uses this for the `Deployment` containerPort.\n```\n`  ports:\n    - containerPort: 8082\n`\n```\nand these for the `Service` ports.\n```\n`  type: ClusterIP\n  ports:\n    - targetPort: 8082\n      port: 8084   \n`\n```\nThe 'analytics' app is a golang program that listens on 8082.\n```\n`func main() {\n    http.HandleFunc(\"/productinfo\", getInfoJSON)    \n    log.Fatal(http.ListenAndServe(\":8082\", nil))\n}\n`\n```\nWhen running this on Minikube, I encountered issues with CORS, which was resolved by using this in the golang code when returning a JSON object as a response:\n```\n`w.Header().Set(\"Access-Control-Allow-Origin\", \"*\")\nw.Header().Set(\"Access-Control-Allow-Headers\", \"Content-Type\") \n`\n```\nAll this worked fine on Minikube (though in Minikube I was using `localhost:8082`). The first app would send a GET request to `http://localhost:8084/productinfo` and the second app would return a JSON object.\nBut when I tried it on a cloud Kubernetes setup by accessing the first app via :, when I open the browser console, I keep getting the error `Cross-Origin Request Blocked: The Same Origin Policy disallows reading the remote resource at http://localhost:8084/productinfo`.\nQuestion:\nWhy is it working on Minikube but not on the cloud Kubernetes worker nodes? Is using `localhost` the right way to access another container? How can I get this to work? How do people who implement microservices use their GET and POST requests across containers? All the microservice examples I found are built for simple demos on Minikube, so it's difficult to get a handle on this nuance.",
      "solution": "@P.... is absolutely right, I just want to provide some more details about DNS for Services and communication between containers in the same Pod.\nDNS for Services\nAs we can find in the documentation, Kubernetes Services are assigned a DNS A (or AAAA) record, for a name of the form `..svc.`. This resolves to the cluster IP of the Service.\n\n\"Normal\" (not headless) Services are assigned a DNS A or AAAA record, depending on the IP family of the service, for a name of the form my-svc.my-namespace.svc.cluster-domain.example. This resolves to the cluster IP of the Service.\n\nLet's break down the form `..svc.` into individual parts:\n\n`` - The name of the Service you want to connect to.\n\n`` - The name of the Namespace in which the Service to which you want to connect resides.\n\n`svc` - This should not be changed - `svc` stands for Service.\n\n`` - cluster domain, by default it's `cluster.local`.\n\nWe can use `` to access a Service in the same Namespace, however we can also use `.` or `..svc` or FQDN `..svc.`.\nIf the Service is in a different Namespace, a single `` is not enough and we need to use  `.` (we can also use: `..svc` or `..svc.`).\nIn the following example, `app-1` and `app-2` are in the same Namespace and `app-2` is exposed with ClusterIP on port `8084` (as in your case):\n```\n`$ kubectl run app-1 --image=nginx\npod/app-1 created\n\n$ kubectl run app-2 --image=nginx\npod/app-2 created\n\n$ kubectl expose pod app-2 --target-port=80 --port=8084\nservice/app-2 exposed\n\n$ kubectl get pod,svc\nNAME        READY   STATUS    RESTARTS   AGE\npod/app-1   1/1     Running   0          45s\npod/app-2   1/1     Running   0          41s\n\nNAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)    AGE\nservice/app-2        ClusterIP   10.8.12.83           8084/TCP   36s\n`\n```\nNOTE: The `app-2` is in the same Namespace as `app-1`, so we can use  `` to access it from `app-1`, you can also notice that we got the FQDN for `app-2` (`app-2.default.svc.cluster.local`):\n```\n`$ kubectl exec -it app-1 -- bash\nroot@app-1:/# nslookup app-2\nServer:         10.8.0.10\nAddress:        10.8.0.10#53\n\nName:   app-2.default.svc.cluster.local\nAddress: 10.8.12.83\n`\n```\nNOTE: We need to provide the port number because `app-2` is listening on `8084`:\n```\n`root@app-1:/# curl app-2.default.svc.cluster.local:8084\n\nWelcome to nginx!\n...\n`\n```\nLet's create `app-3` in a different Namespace and see how to connect to it from `app-1`:\n```\n`$ kubectl create ns test-namespace\nnamespace/test-namespace created\n\n$ kubectl run app-3 --image=nginx -n test-namespace\npod/app-3 created\n\n$ kubectl expose pod app-3 --target-port=80 --port=8084 -n test-namespace\nservice/app-3 exposed\n`\n```\nNOTE: Using `app-3` (``) is not enough, we also need to provide the name of the Namespace in which `app-3` resides (`.`):\n```\n`# nslookup app-3\nServer:         10.8.0.10\nAddress:        10.8.0.10#53\n\n** server can't find app-3: NXDOMAIN\n\n# nslookup app-3.test-namespace\nServer:         10.8.0.10\nAddress:        10.8.0.10#53\n\nName:   app-3.test-namespace.svc.cluster.local\nAddress: 10.8.12.250\n\n# curl app-3.test-namespace.svc.cluster.local:8084\n\nWelcome to nginx!\n...\n`\n```\nCommunication Between Containers in the Same Pod\nWe can use `localhost` to communicate with other containers, but only within the same Pod (Multi-container pods).\nI've created a simple multi-container Pod with two containers: `nginx-container` and `alpine-container`:\n```\n`$ cat multi-container-app.yml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: multi-container-app\nspec:\n  containers:\n  - image: nginx\n    name: nginx-container\n  - image: alpine\n    name: alpine-container\n    command: [\"sleep\", \"3600\"]\n\n$ kubectl apply -f multi-container-app.yml\npod/multi-container-app created\n`\n```\nWe can connect to the `alpine-container` container and check if we can access the nginx web server located in the `nginx-container` with `localhost`:\n```\n`$ kubectl exec -it multi-container-app -c alpine-container -- sh\n\n/ # netstat -tulpn\nActive Internet connections (only servers)\nProto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name\ntcp        0      0 0.0.0.0:80              0.0.0.0:*               LISTEN      -\ntcp        0      0 :::80                   :::*                    LISTEN      -\n\n/ # curl localhost\n\nWelcome to nginx!\n...\n`\n```\nMore information on communication between containers in the same Pod can be found here.",
      "question_score": 1,
      "answer_score": 6,
      "created_at": "2021-08-17T18:01:07",
      "url": "https://stackoverflow.com/questions/68820608/what-is-the-correct-url-to-use-for-get-requests-to-another-kubernetes-container"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "minikube",
      "question_id": 77633616,
      "title": "Unable to access Minikube! message starting miniKube worries me: &quot;Unable to resolve the Docker CLI context&quot; &quot;The system cannot find the path&quot;",
      "problem": "I'm facing error in Minikube\nI was trying to start with `Minikube start` command\nIt gets following error\n\nUnable to resolve the current Docker CLI context \"default\": context\n\"default\": context not found: open\nC:\\Users\\wmmth.docker\\contexts\\meta\\37a8eec1ce19687d132fe29051dca629d164e2c4958ba141d5f4133a33f0688f\\meta.json:\nThe system cannot find the path specified.\n\nI tried to check `minikube status` and it shows the same error:\n```\n`Unable to resolve the \ncurrent Docker CLI context \"default\": context \"default\": context not found: open C:\\Users\\wmmth\\.docker\\contexts\\meta\\37a8eec1ce19687d132fe29051dca629d164e2c4958ba141d5f4133a33f0688f\\meta.json: The system cannot find the \npath specified.\nminikube\ntype: Control Plane\nhost: Running\nkubelet: Running\napiserver: Running\nkubeconfig: Configured\n`\n```",
      "solution": "If you are getting below error  Identifying  what exactly went wrong with your Minikube Kubernetes cluster\n```\n`current Docker CLI context \"default\": context \"default\": context not found: open C:\\\\Users\\\\wmmth.docker\\\\contexts\\\\meta\\\\37a8eec1ce19687d132fe29051dca629d164e2c4958ba141d5f4133a33f0688f\\\\meta.json: The system cannot find the path specified.\n`\n```\nTo fix the above error, take the actions listed below. We can solve your issue in four different ways. I'm hoping one of them can help you with your problem.\nOption 1: To change Docker's default context, use this command.\n```\n`docker context use default\n`\n```\nOption 2: If the last command  docker context use default. didn't work, please try this one:\n```\n` kubectl config set-context minikube \n`\n```\nMostly first command helps to resolve your issue\nFor more information refer to this gitlink",
      "question_score": 1,
      "answer_score": 4,
      "created_at": "2023-12-10T04:53:32",
      "url": "https://stackoverflow.com/questions/77633616/unable-to-access-minikube-message-starting-minikube-worries-me-unable-to-reso"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "minikube",
      "question_id": 73630663,
      "title": "on minikube with WSL2, loadbalancer or nodeport not working, but kubectl expose is work well",
      "problem": "I'm using minikube on WSL2.\nI deployed a simple flask app image and write a LoadBalancer to expose the service.\nMy question is, how do I modify service manifest to get the same result as expose?\nBelow are more details.\nflask app deployment yaml.\nrss.yaml\n```\n`apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: esg\nspec:\n  selector:\n    matchLabels:\n      app: rss\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: rss\n    spec: \n      containers:\n      - name: rss\n        image: \"idioluck/00esg_rss:v01\"\n        ports:\n        - containerPort: 5000\n`\n```\nservice yaml (I tried nodeport and loadbalanacer either.)\nrss_lb.yaml\n```\n`apiVersion: v1\nkind: Service\nmetadata:\n  name: esg-lb\nspec:\n  type: NodePort # LoadBalancer\n  selector:\n    app: rss  \n  ports:\n  - protocol: TCP\n    port: 8080 \n    targetPort: 5000\n`\n```\nkubectl command is\n```\n`sjw@DESKTOP-MFPNHRC:~/esg_kube/kubesvc/rss$ kubectl apply -f rss.yaml\ndeployment.apps/esg created\nsjw@DESKTOP-MFPNHRC:~/esg_kube/kubesvc/rss$ kubectl apply -f rss_lb.yaml\nservice/esg-lb created\nsjw@DESKTOP-MFPNHRC:~/esg_kube/kubesvc/rss$ kubectl get pods\nNAME                  READY   STATUS    RESTARTS   AGE\nesg-757f659b4-4vndc   1/1     Running   0          13s\nesg-757f659b4-4wd2w   1/1     Running   0          13s\nesg-757f659b4-sf5q6   1/1     Running   0          13s\nsjw@DESKTOP-MFPNHRC:~/esg_kube/kubesvc/rss$ kubectl get all\nNAME                      READY   STATUS    RESTARTS   AGE\npod/esg-757f659b4-4vndc   1/1     Running   0          16s\npod/esg-757f659b4-4wd2w   1/1     Running   0          16s\npod/esg-757f659b4-sf5q6   1/1     Running   0          16s\n\nNAME                 TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE\nservice/esg-lb       LoadBalancer   10.101.221.26        8080:31308/TCP   8s\nservice/kubernetes   ClusterIP      10.96.0.1               443/TCP          23h\n\nNAME                  READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/esg   3/3     3            3           16s\n\nNAME                            DESIRED   CURRENT   READY   AGE\nreplicaset.apps/esg-757f659b4   3         3         3       16s\n`\n```\nExteranl ip is pending.\nso i delete loabdbalancer and use expose\n```\n`sjw@DESKTOP-MFPNHRC:~/esg_kube/kubesvc/rss$ kubectl expose deployment esg --type=LoadBalancer --port=8080\nservice/esg exposed\nsjw@DESKTOP-MFPNHRC:~/esg_kube/kubesvc/rss$ kubectl get svc\nNAME         TYPE           CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE\nesg          LoadBalancer   10.99.208.98   127.0.0.1     8080:30929/TCP   46s\nkubernetes   ClusterIP      10.96.0.1              443/TCP          23h\n`\n```\nThe service has been successfully exposed. And the service is a load balancer.",
      "solution": "Your `LoadBalancer` type service is showing `Pending` as status because it is waiting for you to provision an external Load Balancer like AWS's Elastic Load Balancer or GCP's Load Balancer. LoadBalancer type services are usually used together with managed Kubernetes service e.g EKS, GKE etc.\nOn the other hand, you're able to expose your service because it already has `clusterIP` assigned to it.\nIf you want to use LB in Minikube, this official doc may help you. Otherwise, you can use `NodePort` type service directly to expose your flask app.",
      "question_score": 1,
      "answer_score": 4,
      "created_at": "2022-09-07T07:49:51",
      "url": "https://stackoverflow.com/questions/73630663/on-minikube-with-wsl2-loadbalancer-or-nodeport-not-working-but-kubectl-expose"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "minikube",
      "question_id": 70360562,
      "title": "Minikube with PhpStorm and Xdebug",
      "problem": "I have developed a PHP based application. This application runs actually in Kubernetes and mainly using minikube on my machine. I am using PhpStorm as IDE and I also use Xdebug for debugging purposes.\nWhat I know from my researches are that when you start PhpStorm with Xdebug, it will start listening on a port (9000 by default). When I connect to my container (in minikube), I am able to reach the IDE at the port 9000 with netcat :\n```\n`nc  9000\n`\n```\nThis shows me a message telling that connection is open so I am able to reach the IDE from my container.\nThen, when I try to use Xdebug, it is not working and Xdebug doesn't stop at the breakpoint. I was guessing that IDE should also reach the container and that part I am not sure and I don't know how to do it..\nAnyone already setup this kind of configuration with minikube and PhpStorm / Xdebug?",
      "solution": "For xdebug to work, it only needs to connect to client host. there is no need for client (in this case phpstorm) to connect to your pods as well.\nI have the same setup using docker for mac. What i did to make it work:\n\nchanged xdebug.client_host configuration to host.docker.internal which is defined automatically in minikube /etc/host and can access host machine resources\nmade sure I have proper xdebug key defined in php.ini xdebug.idekey\nmade sure I use the xdebug helper extension and have the same idekey defined there\nmade sure I use 9003 to listen in phpstorm which is the default port for xdebug 3",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2021-12-15T09:36:51",
      "url": "https://stackoverflow.com/questions/70360562/minikube-with-phpstorm-and-xdebug"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "minikube",
      "question_id": 70307437,
      "title": "Kubernetes - Frontend pod can&#39;t reach backend pod",
      "problem": "I have a minikube Kubernetes set up with two pods, each having one container. One for my Vue frontend and one for my backend API. I've also got two services attached to the pods.\nMy understanding is because the frontend and backend IP addresses change when the Pod is restarted or moved to a different node, we shouldn't use the IP Addresses to link them but a Service instead.\nSo in my case, my frontend would call my backend through the Service (which can also be used as the hostname) e.g. Service is called `myapi-service`, use `http://myapi-service`\nMy problem is after I launch my front end, any request it sends using the above hostname doesn't work, it's not able to connect to my backend.\napp-deployment.yaml\n```\n`apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myapi-deployment\n  labels:\n    app: myrapi\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: myapi\n  template:\n    metadata:\n      labels:\n        app: myapi\n    spec:\n      containers:\n      - name: myapi\n        image: myapi\n        imagePullPolicy: Never\n        ports:\n        - containerPort: 80\n        env:\n        - name: TZ\n          value: America/Toronto\n        - name: ASPNETCORE_ENVIRONMENT\n          value: Development_Docker\n---\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myui-deployment\n  labels:\n    app: myui\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: myui\n  template:\n    metadata:\n      labels:\n        app: myui\n    spec:\n      containers:\n      - name: myui\n        image: myui\n        imagePullPolicy: Never\n        ports:\n        - containerPort: 8080\n        env:\n        - name: NODE_ENV\n          value: Development\n\n`\n```\napp-service.yaml\n```\n`apiVersion: v1\nkind: Service\nmetadata:\n  name: myapi-service\n  labels:\n    run: myapi-service\nspec:\n  ports:\n  - port: 80\n    protocol: TCP\n  selector:\n    app: myapi\n  type: NodePort\n  \n---\n\napiVersion: v1\nkind: Service\nmetadata:\n  name: myui-service\n  labels:\n    run: myui-service\nspec:\n  ports:\n  - port: 8080\n    protocol: TCP\n  selector:\n    app: myui\n  type: NodePort\n`\n```\nKubernetes Service\nAm I missing a piece here/doing something wrong? Thanks so much.\nUPDATE: If I go into my frontend container\n`curl myapi-service/swagger/index.html`\nIt's able to pull up the API's swagger page\nUPDATE 2, SOLUTION:\nI refactored my `Dockerfile` to use NGINX to serve my front end Vue app\nDockerfile\n```\n`FROM node:14 as builder\n\n# make the 'app' folder the current working directory\nWORKDIR /app\n\n# copy both 'package.json' and 'package-lock.json' (if available)\nCOPY package*.json ./\n\n# install project dependencies\nRUN npm install\n\n# copy project files and folders to the current working directory (i.e. 'app' folder)\nCOPY . .\n\n# build app\nRUN npm run build\n\nFROM nginx:alpine\nCOPY ./.nginx/nginx.conf /etc/nginx/nginx.conf\n\n## Remove default nginx index pagec\nRUN rm -rf /usr/share/nginx/html/*\n\n# Copy from the stage 1\nCOPY --from=builder /app/dist /usr/share/nginx/html\n\nEXPOSE 80\nENTRYPOINT [\"nginx\", \"-g\", \"daemon off;\"]\n`\n```\nand created a folder called `.nginx` in my front end's root folder with the `nginx.conf` file inside it.\nnginx.conf\n```\n`worker_processes 4;\n\nevents { worker_connections 1024; }\n\nhttp {\n    server {\n        listen 80;\n        root  /usr/share/nginx/html;\n        include /etc/nginx/mime.types;\n\n        location /appui {\n            try_files $uri /index.html;\n        }\n        \n        location /api/ {\n            proxy_pass http://myapi-service;\n        }\n    }\n}\n`\n```\nNo Ingress controller required. The front end was able to talk to the backend as explained in Mikolaj's answer.\nHope someone out there can find this useful!~ ^",
      "solution": "You cannot reach your backend pod from your frontend pod using kubernetes DNS like http://myapi-service because your frontend is running in the browser - outside your cluster. The browser doesn't undestrand the kubernetes DNS therefore cannot resolve your http://myapi-service url.\nIf you want to communicate frontend with your backend using `K8S DNS` you need to use any web server like `nginx`. The web server that host your frontend app is actually run on the kubernetes cluster so it understands the `K8S DNS`.\nIn your frontend code you need to change the api calls. Insead of directly call the api you need to first call youe web server.\nFor example: replace http://api-service/api/getsomething to /api/getsomething\n/api/getsomething - this will tell the browser that it will send the request to the same server that served your frontend app (`nginx` in this case)\nThen via `nginx` server the call can be forwarder to your api using the `K8S DNS`.\n(It is called reverse proxy)\nTo forward your requests to api add some code to nginx config file.\n```\n`location /api/ {\n    proxy_pass http://api-service.default:port;\n}\n`\n```\n*api-service - your k8s service name\n*default - name of k8s api-service namespace\n*port - api-service port\nFrom now all your frontend requests contains /api/.. phrase will be forwarded to your api-service/api/..\n/api/getsomething -> http://api-service/api/getsomething",
      "question_score": 1,
      "answer_score": 4,
      "created_at": "2021-12-10T17:15:15",
      "url": "https://stackoverflow.com/questions/70307437/kubernetes-frontend-pod-cant-reach-backend-pod"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "minikube",
      "question_id": 67173591,
      "title": "Access a demo service in remote Kubernetes cluster using ingress",
      "problem": "I have a minikube installed on a Linux machine, and I tried following the Deploy Hello World App\nto get access to the demo service from outside (my browser).\n```\n`$ kubectl get service web\nNAME   TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE\nweb    NodePort   10.103.184.174           8080:30068/TCP   17h\n~$ minikube service web --url\nhttp://192.168.49.2:30068\n`\n```\nBut when trying to access this url from my browser I'm getting:\n\nConnection timeout. The server at 192.168.49.2 is taking too long to respond.\n\nI tried doing port forwarding (on Linux machine), but it didn't help as well!\n```\n`~$ kubectl port-forward svc/web 30068:8080\n`\n```\nNote::\nThere is a ping to that ip from inside the linux machine, but no ping from outside.\nWhat am I missing?",
      "solution": "What am I missing?\n\nWhat you are missing is the basics of networking knowledge. Your minikube is most likely running in a VM. This VM has assigned a virtual net interface with an IP address that is known only to the mentioned linux machine it's running on and no other device in your network is aware of it, and therefore has no idea where to send the packets. Connection timeout suggests that packets get dropped somewhere (most likely on a router but firewall can also be a cause in some cases).\nWhat can you do to solve it?\nTry starting minikube with `--driver=none`, so that minikube doesn't start in a vm and gets direct access to host's network interface an its IP address.\n```\n`$ minikube start --driver=none\n`\n```\nOther solution would be using kubectl port-forward. The way you used it is almost correct. You only forgot one thing: kubectl port-forward by default binds on loopback/localhost. You need to explicitly tell it to bind on all interfaces so that it can be accessed from the outside. Adding `--address=0.0.0.0` should do the job, but remember to use ip of the linux machine, not the minikube to access the website.\n```\n`$ kubectl port-forward svc/web 30068:8080 --address=0.0.0.0\nForwarding from 0.0.0.0:30068 -> 8080\n`\n```",
      "question_score": 1,
      "answer_score": 4,
      "created_at": "2021-04-20T08:28:12",
      "url": "https://stackoverflow.com/questions/67173591/access-a-demo-service-in-remote-kubernetes-cluster-using-ingress"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "minikube",
      "question_id": 65646258,
      "title": "Helm and minikube: service ip missing",
      "problem": "I am trying examples from the book Learning Helm. I seem to be missing something. I cannot install chart from helm repo:\n```\n`xxxxx:~ $ helm install my-nginx bitnami/nginx\nNAME: my-nginx\nLAST DEPLOYED: Sat Jan  9 20:26:22 2021\nNAMESPACE: default\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\nNOTES:\n** Please be patient while the chart is being deployed **\n\nNGINX can be accessed through the following DNS name from within your cluster:\n\n    my-nginx.default.svc.cluster.local (port 80)\n\nTo access NGINX from outside the cluster, follow the steps below:\n\n1. Get the NGINX URL by running these commands:\n\n  NOTE: It may take a few minutes for the LoadBalancer IP to be available.\n        Watch the status with: 'kubectl get svc --namespace default -w my-nginx'\n\n    export SERVICE_PORT=$(kubectl get --namespace default -o jsonpath=\"{.spec.ports[0].port}\" services my-nginx)\n    export SERVICE_IP=$(kubectl get svc --namespace default my-nginx -o jsonpath='{.status.loadBalancer.ingress[0].ip}')\n    echo \"http://${SERVICE_IP}:${SERVICE_PORT}\"\n`\n```\n```\n`xxxxx:~ $ export SERVICE_PORT=$(kubectl get --namespace default -o jsonpath=\"{.spec.ports[0].port}\" services my-nginx)\nxxxxx:~ $ export SERVICE_IP=$(kubectl get svc --namespace default my-nginx -o jsonpath='{.status.loadBalancer.ingress[0].ip}')\nxxxxx:~ $ echo \"http://${SERVICE_IP}:${SERVICE_PORT}\"\nhttp://:80\n`\n```\n```\n`$ kubectl get svc --namespace default -w my-nginx\nNAME       TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE\nmy-nginx   LoadBalancer   10.104.16.177        80:30977/TCP  17h\n`\n```\nSome more details.",
      "solution": "The line in the following to extract the service IP is to get the external IP in the service object.\n```\n`export SERVICE_IP=$(kubectl get svc --namespace default my-nginx -o jsonpath='{.status.loadBalancer.ingress[0].ip}')\n`\n```\nIn `LoadBalancer` service type, Kubernetes tries to get an external IP address and release the target service with that IP address. In self-hosted Kubernetes cluster, the external IP cannot be provisioned automatically. In most of the hosted clusters in public cloud, like GKE, EKS etc, they have had the integration with the external IP address provision. So, you can get it automatically once you set up the service as `LoadBalancer`.\nIt is still possible to do this automation with 3rd party operators/applications, like MetalLB. But in most of the self-hosted Kubernetes cluster, it is suggested to access the service with `NodePort` service type.\nPlease rerun the helm command with the follow argument. It will change the service type from `LoadBalancer` to `NodePort`. Following the instruction from the stdout may allow you to access your service.\n```\n`> helm install my-nginx bitnami/nginx --set service.type=NodePort\n`\n```\nOn the other hand, you can follow the minikube official doc here to set up the support in `LoadBalancer` service.",
      "question_score": 1,
      "answer_score": 4,
      "created_at": "2021-01-09T19:38:58",
      "url": "https://stackoverflow.com/questions/65646258/helm-and-minikube-service-ip-missing"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "minikube",
      "question_id": 75211377,
      "title": "Access a web service in kubernetes pod from local browser using NodePort yields Connection refused",
      "problem": "What do I need to do in order to get my local browser to and request a resource to a web service running inside Minikube instance running locally on my machine?\nI am getting a `Connection refused` when trying to `kubectl port-forward`.\nMy workflow is:\n\nCreating Dockerfile with web service on\nStart minikube in docker\nBuild docker image\nImport image locally into Minikube\nCreated a deployment with one container and a NodePort service\nApplied deployment/service\nRan kubectl port-forward (to hopefully forward requests to my container)\nOpen browser to 127.0.0.1:31000\n\nPort Configuration Summary\n\nDockerfile:\n\nExpose: 80\nuvicorn: 80\n\nDeployment\n\nNodePort Service:\n\nPort: 80\nTarget Port: 80\nNode Port: 31000\n\nKubectl Command: 8500:31000\nBrowser: 127.0.0.1:8500\n\nSetup and run through\ndev.dockerfile (Step 1)\n```\n`FROM python:3.11-buster # Some Debian Python image...  I built my own\n\nCOPY ../sources/api/ /app/\nRUN pip install --no-cache-dir --upgrade -r /app/requirements.txt\n\nENV PYTHONPATH=/app/\n\nEXPOSE 80\n\nCMD [\"uvicorn\", \"app.main:app\", \"--proxy-headers\", \"--host\", \"0.0.0.0\", \"--port\", \"80\"]\n`\n```\nBuild Sequence (Steps 2 to 4)\n```\n`# 2 - start minikube\nminikube start --bootstrapper=kubeadm --vm-driver=docker\nminikube docker-env\n\n## 3 - build image\ndocker build -f ../../service1/deploy/dev.dockerfile ../../service1 -t acme-app.service1:latest \n\n## 4 - load image into minikube\nminikube image load acme-app.service1:latest\n`\n```\nDeployment (Step 5 and 6)\ndeployment.yaml\n`---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: acme-service-1-deployment\n  namespace: acme-app-dev\n  labels:\n    app: service-1\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: service-1\n  template:\n    metadata:\n      labels:\n        app: service-1\n    spec:\n      containers:\n        - name: service1-container\n          image: docker.io/library/acme-app.service1:latest\n          imagePullPolicy: Never\n          ports:\n          - containerPort: 80\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: service-1-service\n  namespace: acme-app-dev\nspec:\n  type: NodePort\n  selector:\n    app: service-1\n  ports:\n  - protocol: TCP\n    port: 80\n    targetPort: 80\n    nodePort: 31000\n`\nDeploy\n```\n`kubectl apply -f deployment.yaml\n`\n```\nkubectl port forward (Step 7)\nFind Pod\n```\n`kubectl get pods -n acme-app-dev\nNAME                                         READY   STATUS    RESTARTS   AGE\nacme-service-1-deployment-76748d7ff6-llcsr   1/1     Running   0          11s\n`\n```\nPort Forward to pod\n```\n`port-forward acme-service-1-deployment-76748d7ff6-llcsr 8500:31000 -n acme-app-dev\nForwarding from 127.0.0.1:8500 -> 31000\nForwarding from [::1]:8500 -> 31000\n`\n```\nTest in Browser (Step 8)\nOpen favorite browser and navigate to 127.0.0.1:31000.\nThe console running the port forward now outputs:\n```\n`E0123 14:54:16.208010   25932 portforward.go:406] an error occurred forwarding 8500 -> 31000: error forwarding port 31000 to pod d4c0fa6cb16ce02335a05cad904fbf2ab7818e2073d7c7ded8ad05f193aa37e7, uid : exit status 1: 2023/01/23 14:54:16 socat[39370] E connect(5, AF=2 127.0.0.1:31000, 16): Connection refused\nE0123 14:54:16.213268   25932 portforward.go:234] lost connection to pod\n`\n```\nWhat have I looked at?\nI've tried looking through the docs on kubernetes website as well as issues on here (yes there are similar).  This is pretty similar - although no marked answer and still an issue by the looks of it.  I couldn't see a solution for my issue here.\nNodePort exposed Port connection refused\nI am running Minikube on Windows and I'm just setting out on a kubernetes journey.\nThe image itself works in docker from a docker compose.  I can see the pod is up and running in minikube from the logs (minikube dashboard).",
      "solution": "You got your wires crossed:\n\nThe pod is listening on port 80\nThe NodePort service is listening on port 31000 on the node, but its underlying ClusterIP service is listening on port 80 as well.\nYou are trying to port-forward to port 31000 on the Pod. This will not work.\n\nCall one of the following instead:\n\n`kubectl port-forward -n acme-app-dev deploy/acme-service-1-deployment 8500:80`\nor `kubectl port-forward -n acme-app-dev service/service-1-service 8500:80`\nor use `minikube service -n acme-app-dev service-1-service` and use the provided URL.",
      "question_score": 1,
      "answer_score": 3,
      "created_at": "2023-01-23T16:09:20",
      "url": "https://stackoverflow.com/questions/75211377/access-a-web-service-in-kubernetes-pod-from-local-browser-using-nodeport-yields"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "minikube",
      "question_id": 75071083,
      "title": "cannot access external ip in minikube with EndpointSlice and Service",
      "problem": "Following the example on kubernetes.io I'm trying to connect to an external IP from within the cluster (and i need some port proxy, so not ExternalName service). However it is not working. This is the response I'm expecting\n`ubuntu:/opt$ curl http://216.58.208.110:80\n\n301 Moved\n301 Moved\nThe document has moved\nhere.\n\n`\nif I use the following config\n`apiVersion: discovery.k8s.io/v1\nkind: EndpointSlice\nmetadata:\n  name: my-service-1\n  labels:\n    kubernetes.io/service-name: my-service\naddressType: IPv4\nports:\n  - name: http\n    appProtocol: http\n    protocol: TCP \n    port: 80\nendpoints:\n  - addresses:\n      - \"216.58.208.110\"\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: my-service\nspec:\n  ports:\n    - protocol: TCP \n      port: 8888\n      targetPort: 80\n`\nI expect the following command to get same result:\n`minikube kubectl -- run -it --rm --restart=Never curl --image=curlimages/curl curl -- my-service:8888\n`\nbut I get nothing.\nif I start an debian image with\n`minikube kubectl -- run -it --rm --restart=Never debian --image=debian:latest\n`\nthen\n`apt update && apt install dnsutils curl -y && nslookup my-service && curl my-service:8888\n`\ngives\n`Server:     10.96.0.10\nAddress:    10.96.0.10#53\n\nName:   my-service.default.svc.cluster.local\nAddress: 10.111.116.160\ncurl: (28) Failed to connect to my-service port 8888: Connection timed out\n`\nAm i missing something? or is it not supposed to work this way?",
      "solution": "After some trial and error it seem that if `ports[0].name = http` is set for the `endpointslice` it stops working.\nit stops working for when for the `service` `spec.ports[0].targetPort` is set to `80` or `http` as well.\n(it does work when `ports[0].name = ''`)\nFurther investing shows that it works if:\nfor `service`\n`spec:\n  ports:\n  - port: 8888\n    name: http\n    targetPort: http\n`\nfor `endpointslice`\n`ports:\n  - port: 80\n    name: http\n`\nI guess if you want to name them both the `service` and the `endpointslice` have to have corresponding `.name` values.",
      "question_score": 1,
      "answer_score": 4,
      "created_at": "2023-01-10T15:02:13",
      "url": "https://stackoverflow.com/questions/75071083/cannot-access-external-ip-in-minikube-with-endpointslice-and-service"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "minikube",
      "question_id": 71221736,
      "title": "How to change docker default storage size",
      "problem": "I've been having loads of issues with `kubectl` not having enough space. How can I increase the default storage size allocated to `docker`?\nNone of minikube recommendations worked.\n```\n`1. Run \"docker system prune\" to remove unused Docker data (optionally with \"-a\")\n2. Increase the storage allocated to Docker for Desktop by clicking on:\n   Docker icon > Preferences > Resources > Disk Image Size\n3. Run \"minikube ssh -- docker system prune\" if using the Docker container runtime\n`\n```\nAnd the second is not possible from command line...",
      "solution": "Taking your comment into consideration\n\nI get ImagePullBackOff when I try to deploy nginx on the cluster \u2013\nCaterina\n\nYou can specify minikube's disk allocations separately:\n`minikube start --memory=8192 --cpus=4 --disk-size=50g`\nWhich can help you to work around the disk space issues as the default is significantly smaller: ` --disk-size string Disk size allocated to the minikube VM (format: [], where unit = b, k, m or g). (default \"20000mb\")`",
      "question_score": 1,
      "answer_score": 3,
      "created_at": "2022-02-22T13:57:30",
      "url": "https://stackoverflow.com/questions/71221736/how-to-change-docker-default-storage-size"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "minikube",
      "question_id": 68873776,
      "title": "minikube service is failing to expose URL",
      "problem": "```\n`F:\\Udemy\\GitRepo\\Kubernetes-Tutorial>kubectl get pod -o wide\nNAME                             READY   STATUS    RESTARTS   AGE   IP           NODE       NOMINATED NODE   READINESS GATES\nmy-app-deploy-68698d9757-wrs9z   1/1     Running   0          14m   172.17.0.3   minikube              \n\nF:\\Udemy\\GitRepo\\Kubernetes-Tutorial>minikube service my-app-svc\n|-----------|------------|-------------|-----------------------------|\n| NAMESPACE |    NAME    | TARGET PORT |             URL             |\n|-----------|------------|-------------|-----------------------------|\n| default   | my-app-svc |          80 | http://172.30.105.146:30365 |\n|-----------|------------|-------------|-----------------------------|\n* Opening service default/my-app-svc in default browser...\n\nF:\\Udemy\\GitRepo\\Kubernetes-Tutorial>kubectl describe service my-app-svc\nName:                     my-app-svc\nNamespace:                default\nLabels:                   \nAnnotations:              \nSelector:                 app=my-app\nType:                     NodePort\nIP:                       10.98.9.115\nPort:                       80/TCP\nTargetPort:               9001/TCP\nNodePort:                   30365/TCP\nEndpoints:                172.17.0.3:9001\nSession Affinity:         None\nExternal Traffic Policy:  Cluster\nEvents:                   \n\nF:\\Udemy\\GitRepo\\Kubernetes-Tutorial>kubectl logs my-app-deploy-68698d9757-wrs9z\n\n  .   ____          _            __ _ _\n /\\\\ / ___'_ __ _ _(_)_ __  __ _ \\ \\ \\ \\\n( ( )\\___ | '_ | '_| | '_ \\/ _` | \\ \\ \\ \\\n \\\\/  ___)| |_)| | | | | || (_| |  ) ) ) )\n  '  |____| .__|_| |_|_| |_\\__, | / / / /\n =========|_|==============|___/=/_/_/_/\n :: Spring Boot ::        (v2.3.1.RELEASE)\n\n2021-08-21 13:37:21.046  INFO 1 --- [           main] c.d.d.DockerpublishApplication           : Starting DockerpublishApplication v0.0.3 on my-app-deploy-68698d9757-wrs9z with PID 1 (/app.jar started by root in /)\n2021-08-21 13:37:21.050  INFO 1 --- [           main] c.d.d.DockerpublishApplication           : No active profile set, falling back to default profiles: default\n2021-08-21 13:37:22.645  INFO 1 --- [           main] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat initialized with port(s): 9091 (http)\n2021-08-21 13:37:22.659  INFO 1 --- [           main] o.apache.catalina.core.StandardService   : Starting service [Tomcat]\n2021-08-21 13:37:22.660  INFO 1 --- [           main] org.apache.catalina.core.StandardEngine  : Starting Servlet engine: [Apache Tomcat/9.0.36]\n2021-08-21 13:37:22.785  INFO 1 --- [           main] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring embedded WebApplicationContext\n2021-08-21 13:37:22.785  INFO 1 --- [           main] w.s.c.ServletWebServerApplicationContext : Root WebApplicationContext: initialization completed in 1646 ms\n2021-08-21 13:37:23.302  INFO 1 --- [           main] o.s.s.concurrent.ThreadPoolTaskExecutor  : Initializing ExecutorService 'applicationTaskExecutor'\n2021-08-21 13:37:23.496  INFO 1 --- [           main] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat started on port(s): 9091 (http) with context path ''\n2021-08-21 13:37:23.510  INFO 1 --- [           main] c.d.d.DockerpublishApplication           : Started DockerpublishApplication in 3.279 seconds (JVM running for 4.077)\n\nF:\\Udemy\\GitRepo\\Kubernetes-Tutorial>\n`\n```\nEverything seems to be good, but not working well.\nRefused to connect issue comes as below for\n\nminikube service my-app-svc",
      "solution": "Your service or application is running on different port as you are getting connection refused.\nSpring boot running on the 9091 : `Tomcat started on port(s): 9091 (http) with context path ''`\nBut your service is redirecting the traffic `TargetPort: 9001/TCP`\nYour target port should be 9091 instead of 9001\nYour will access the application over the node port Ip request, which will reach to the K8s service and be forwarded to the `TargetPort: 9091/TCP` on which the application is running.",
      "question_score": 1,
      "answer_score": 3,
      "created_at": "2021-08-21T16:16:13",
      "url": "https://stackoverflow.com/questions/68873776/minikube-service-is-failing-to-expose-url"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "minikube",
      "question_id": 68741040,
      "title": "minikube install failes on OS X",
      "problem": "I'm downloading minikube on OS X (Catalina 10.15.7) from `releases/latest/minikube-darwin-amd64` and it fails when run, see below:\n```\n`minikube version\n/usr/local/bin/minikube: line 1: syntax error near unexpected token `NoSuchKeyThe specified key does not exist.No such object: minikube/releases/latest/minikube-darwin-amd'\n`\n```\nNot sure what key it refers to.",
      "solution": "This problem was caused by an invalid URL in the `curl` command. Looking at the error message, my guess is that the `64` was missing at the end.\nI've reproduced this error and we can see that the error message is exactly the same as in the question (I'm using Linux so instead of `minikube-darwin-amd64` I used `minikube-linux-amd64`):\nNOTE: I didn't provide `64` at the end.\n```\n`$ curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100   205  100   205    0     0   6029      0 --:--:-- --:--:-- --:--:--  6029\n$ sudo install minikube-linux-amd /usr/local/bin/minikube\n`\n```\nThe size is too small for the `minikube`:\n```\n`$ ls -lh /usr/local/bin/minikube\n-rwxr-xr-x 1 root root 205 Aug 13 07:30 /usr/local/bin/minikube\n`\n```\nThe error message:\n```\n`$ minikube version\n/usr/local/bin/minikube: line 1: syntax error near unexpected token `NoSuchKeyThe specified key does not exist.No such object: minikube/releases/latest/minikube-linux-amd'\n`\n```\nNow let's install the `minikube` correctly:\n```\n`$ curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64\n$ sudo install minikube-linux-amd64 /usr/local/bin/minikube\n`\n```\nWe can see that the real size of the `minikube` is >65M:\n```\n`$ ls -lh /usr/local/bin/minikube\n-rwxr-xr-x 1 root root 67M Aug 13 07:32 /usr/local/bin/minikube\n`\n```\nAnd everything works as expected:\n```\n`$ minikube version\nminikube version: v1.22.0\ncommit: a03fbcf166e6f74ef224d4a63be4277d017bb62e\n`\n```",
      "question_score": 1,
      "answer_score": 3,
      "created_at": "2021-08-11T13:23:57",
      "url": "https://stackoverflow.com/questions/68741040/minikube-install-failes-on-os-x"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "minikube",
      "question_id": 68278355,
      "title": "Empty custom metrics resource list with Prometheus adapter",
      "problem": "I have a minikube installation on my windows machine. My application exposes a custom metric 'http_requests_total'. I installed Prometheus operator first and configured it scrape the custom metrics. I can see the custom metric appear in Prometheus dashboard.\nTrouble arises when I install Prometheus Adapter. I use following helm command to install the adapter:\n```\n`helm install my-release prometheus-community/prometheus-adapter\n`\n```\nThen I run following command to edit the configmap and add rule for my custom metric:\n```\n`kubectl edit cm my-release-prometheus-adapter\n`\n```\nI add following section in this configmap:\n```\n`- seriesQuery: 'http_requests_total{kubernetes_namespace!=\"\",kubernetes_pod_name!=\"\"}'\n      resources:\n        overrides:\n          kubernetes_namespace: {resource: \"namespace\"}\n          kubernetes_pod_name: {resource: \"pod\"}\n      name:\n        matches: \"^(.*)_total\"\n        as: \"${1}_per_second\"\n      metricsQuery: sum(rate(>{>}[2m])) by (>)\n`\n```\nHaving done this, when I execute the following command:\n```\n`kubectl get --raw /apis/custom.metrics.k8s.io/v1beta1\n`\n```\nI do not see my custom resource being read:\n```\n`{\"kind\":\"APIResourceList\",\"apiVersion\":\"v1\",\"groupVersion\":\"custom.metrics.k8s.io/v1beta1\",\"resources\":[]}\n`\n```\nI have read multiple similar questions posted on stackoverflow but none seems to have the working answer. I tried to change the prometheus url for my adapter but didn't succeed with that. What am I missing? Following is the log from the adapter pod:\n```\n`I0706 22:41:15.240788       1 adapter.go:101] successfully using in-cluster auth\nE0706 22:41:15.274396       1 provider.go:227] unable to update list of all metrics: unable to fetch metrics for query \"{namespace!=\\\"\\\",__name__!~\\\"^container_.*\\\"}\": Get \"http://prometheus-kube-prometheus-prometheus.prom.svc.cluster.local:9090/api/v1/series?match%5B%5D=%7Bnamespace%21%3D%22%22%2C__name__%21~%22%5Econtainer_.%2A%22%7D&start=1625611215.271\": dial tcp: lookup prometheus-kube-prometheus-prometheus.prom.svc.cluster.local on 10.96.0.10:53: no such host\nI0706 22:41:15.636300       1 serving.go:325] Generated self-signed cert (/tmp/cert/apiserver.crt, /tmp/cert/apiserver.key)\nI0706 22:41:15.637101       1 dynamic_serving_content.go:111] Loaded a new cert/key pair for \"serving-cert::/tmp/cert/apiserver.crt::/tmp/cert/apiserver.key\"\nI0706 22:41:16.192597       1 requestheader_controller.go:244] Loaded a new request header values for RequestHeaderAuthRequestController\nI0706 22:41:16.193884       1 config.go:655] Not requested to run hook priority-and-fairness-config-consumer\nI0706 22:41:16.249643       1 requestheader_controller.go:169] Starting RequestHeaderAuthRequestController\nI0706 22:41:16.249689       1 shared_informer.go:240] Waiting for caches to sync for RequestHeaderAuthRequestController\nI0706 22:41:16.249807       1 configmap_cafile_content.go:202] Starting client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file\nI0706 22:41:16.249829       1 shared_informer.go:240] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file\nI0706 22:41:16.249928       1 configmap_cafile_content.go:202] Starting client-ca::kube-system::extension-apiserver-authentication::client-ca-file\nI0706 22:41:16.249977       1 shared_informer.go:240] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file\nI0706 22:41:16.250288       1 dynamic_serving_content.go:130] Starting serving-cert::/tmp/cert/apiserver.crt::/tmp/cert/apiserver.key\nI0706 22:41:16.250322       1 reflector.go:219] Starting reflector *v1.ConfigMap (12h0m0s) from k8s.io/apiserver/pkg/authentication/request/headerrequest/requestheader_controller.go:172\nI0706 22:41:16.250390       1 reflector.go:255] Listing and watching *v1.ConfigMap from k8s.io/apiserver/pkg/authentication/request/headerrequest/requestheader_controller.go:172\nI0706 22:41:16.250322       1 reflector.go:219] Starting reflector *v1.ConfigMap (12h0m0s) from k8s.io/apiserver/pkg/server/dynamiccertificates/configmap_cafile_content.go:206\nI0706 22:41:16.250536       1 reflector.go:255] Listing and watching *v1.ConfigMap from k8s.io/apiserver/pkg/server/dynamiccertificates/configmap_cafile_content.go:206\nI0706 22:41:16.250555       1 reflector.go:219] Starting reflector *v1.ConfigMap (12h0m0s) from k8s.io/apiserver/pkg/server/dynamiccertificates/configmap_cafile_content.go:206\nI0706 22:41:16.250608       1 reflector.go:255] Listing and watching *v1.ConfigMap from k8s.io/apiserver/pkg/server/dynamiccertificates/configmap_cafile_content.go:206\nI0706 22:41:16.250577       1 tlsconfig.go:200] loaded serving cert [\"serving-cert::/tmp/cert/apiserver.crt::/tmp/cert/apiserver.key\"]: \"localhost@1625611275\" [serving] validServingFor=[127.0.0.1,localhost,localhost] issuer=\"localhost-ca@1625611275\" (2021-07-06 21:41:15 +0000 UTC to 2022-07-06 21:41:15 +0000 UTC (now=2021-07-06 22:41:16.2505337 +0000 UTC))\nI0706 22:41:16.250950       1 named_certificates.go:53] loaded SNI cert [0/\"self-signed loopback\"]: \"apiserver-loopback-client@1625611276\" [serving] validServingFor=[apiserver-loopback-client] issuer=\"apiserver-loopback-client-ca@1625611275\" (2021-07-06 21:41:15 +0000 UTC to 2022-07-06 21:41:15 +0000 UTC (now=2021-07-06 22:41:16.2509375 +0000 UTC))\nI0706 22:41:16.251003       1 secure_serving.go:197] Serving securely on [::]:6443\nI0706 22:41:16.251321       1 tlsconfig.go:240] Starting DynamicServingCertificateController\nI0706 22:41:16.349970       1 shared_informer.go:270] caches populated\nI0706 22:41:16.350023       1 shared_informer.go:247] Caches are synced for RequestHeaderAuthRequestController \nI0706 22:41:16.350129       1 shared_informer.go:270] caches populated\nI0706 22:41:16.350142       1 shared_informer.go:247] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file \nI0706 22:41:16.350164       1 shared_informer.go:270] caches populated\nI0706 22:41:16.350186       1 shared_informer.go:247] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file \nI0706 22:41:16.350698       1 tlsconfig.go:178] loaded client CA [0/\"client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file\"]: \"minikubeCA\" [client,serving] issuer=\"\" (2021-06-07 20:39:52 +0000 UTC to 2031-06-06 20:39:52 +0000 UTC (now=2021-07-06 22:41:16.3506808 +0000 UTC))\nI0706 22:41:16.350796       1 tlsconfig.go:178] loaded client CA [1/\"client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file\"]: \"front-proxy-ca\" [] issuer=\"\" (2021-06-08 20:39:52 +0000 UTC to 2031-06-06 20:39:52 +0000 UTC (now=2021-07-06 22:41:16.3507595 +0000 UTC))\nI0706 22:41:16.351138       1 tlsconfig.go:200] loaded serving cert [\"serving-cert::/tmp/cert/apiserver.crt::/tmp/cert/apiserver.key\"]: \"localhost@1625611275\" [serving] validServingFor=[127.0.0.1,localhost,localhost] issuer=\"localhost-ca@1625611275\" (2021-07-06 21:41:15 +0000 UTC to 2022-07-06 21:41:15 +0000 UTC (now=2021-07-06 22:41:16.3511272 +0000 UTC))\nI0706 22:41:16.351349       1 named_certificates.go:53] loaded SNI cert [0/\"self-signed loopback\"]: \"apiserver-loopback-client@1625611276\" [serving] validServingFor=[apiserver-loopback-client] issuer=\"apiserver-loopback-client-ca@1625611275\" (2021-07-06 21:41:15 +0000 UTC to 2022-07-06 21:41:15 +0000 UTC (now=2021-07-06 22:41:16.3513364 +0000 UTC))\nI0706 22:41:26.255679       1 reflector.go:530] k8s.io/apiserver/pkg/authentication/request/headerrequest/requestheader_controller.go:172: Watch close - *v1.ConfigMap total 0 items received\nI0706 22:41:26.256047       1 reflector.go:530] k8s.io/apiserver/pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Watch close - *v1.ConfigMap total 0 items received\nI0706 22:41:26.257735       1 reflector.go:530] k8s.io/apiserver/pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Watch close - *v1.ConfigMap total 0 items received\n`\n```\nFollowing are the arguments in the deployment for the adapter:\n```\n`Args:\n      /adapter\n      --secure-port=6443\n      --cert-dir=/tmp/cert\n      --logtostderr=true\n      --prometheus-url=http://prometheus.default.svc:9090\n      --metrics-relist-interval=1m\n      --v=6\n      --config=/etc/adapter/config.yaml\n`\n```\nFollowing are various services present on my local cluster:\n```\n`NAME                                      TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                      AGE\nalertmanager-operated                     ClusterIP   None                     9093/TCP,9094/TCP,9094/UDP   2d6h\nkubernetes                                ClusterIP   10.96.0.1                443/TCP                      28d\nmy-release-prometheus-adapter             ClusterIP   10.106.147.10            443/TCP                      23m\nprometheus-grafana                        ClusterIP   10.108.144.246           80/TCP                       2d6h\nprometheus-kube-prometheus-alertmanager   ClusterIP   10.108.19.130            9093/TCP                     2d6h\nprometheus-kube-prometheus-operator       ClusterIP   10.108.170.84            443/TCP                      2d6h\nprometheus-kube-prometheus-prometheus     ClusterIP   10.98.67.168             9090/TCP                     2d6h\nprometheus-kube-state-metrics             ClusterIP   10.97.252.48             8080/TCP                     2d6h\nprometheus-operated                       ClusterIP   None                     9090/TCP                     2d6h\nprometheus-prometheus-node-exporter       ClusterIP   10.107.194.240           9100/TCP                     2d6h\nsample-app                                ClusterIP   10.97.86.241             80/TCP                       4h10m\n`\n```\nIn the beginning of logs for the adapter pod, I see following line of error:\n```\n`unable to update list of all metrics: unable to fetch metrics for query \"{__name__=~\\\"^container_.*\\\",container!=\\\"POD\\\",namespace!=\\\"\\\",pod!=\\\"\\\"}\": Get \"http://prometheus.default.svc:9090/api/v1/series?match%5B%5D=%7B__name__%3D~%22%5Econtainer_.%2A%22%2Ccontainer%21%3D%22POD%22%2Cnamespace%21%3D%22%22%2Cpod%21%3D%22%22%7D&start=1625614091.754\": dial tcp: lookup prometheus.default.svc on 10.96.0.10:53: no such host\n`\n```",
      "solution": "The prometheus-adapter is not able to connect to your prometheus instance, this line in the log indicates as much:\n```\n`E0706 22:41:15.274396       1 provider.go:227] unable to update list of all metrics: unable to fetch metrics for query \"{namespace!=\\\"\\\",__name__!~\\\"^container_.*\\\"}\": Get \"http://prometheus-kube-prometheus-prometheus.prom.svc.cluster.local:9090/api/v1/series?match%5B%5D=%7Bnamespace%21%3D%22%22%2C__name__%21~%22%5Econtainer_.%2A%22%7D&start=1625611215.271\": dial tcp: lookup prometheus-kube-prometheus-prometheus.prom.svc.cluster.local on 10.96.0.10:53: no such host\n`\n```\nAlthough you have a service called `prometheus-kube-prometheus-prometheus`, the url is expecting that service to live in the `prom` namespace.  You should edit the prometheus url (in the same way you added the series query) to have the proper url.  If this service is in the default namespace you can specify `prometheus-kube-prometheus-prometheus.svc.cluster.local` otherwise it would be `prometheus-kube-prometheus-prometheus..svc.cluster.local`",
      "question_score": 1,
      "answer_score": 3,
      "created_at": "2021-07-07T01:22:06",
      "url": "https://stackoverflow.com/questions/68278355/empty-custom-metrics-resource-list-with-prometheus-adapter"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "minikube",
      "question_id": 68238075,
      "title": "deployment in version &quot;v1&quot; cannot be handled as a Deployment: no kind &quot;deployment&quot; is registered for version",
      "problem": "```\n`apiVersion: apps/v1\nkind: deployment\nmetadata:\n  name: mongodb-deployment\n  labels:\n    app: mongodb\nspec:\n  replicas: 1\n  selector:\n    matchLebels:\n       app: mongodb\n  template:\n    metadata:\n      lebels:\n        app: mongodb\n    spec:\n     containers:\n     - name: mongodb\n       image: mongo\n       ports:\n       - containerPort: 27017\n       env:\n       - name: MONGO_INITDB_ROOT_USERNAE\n         valueFrom:\n           secretKeyref:\n             name: mongodb-secret\n             key: mongo-root-username\n       - name: MONGO_INITDB_ROOT_PASSWORD\n         valueFrom:\n          secretKeyref:\n            name: mongodb-secret\n            key: mongo-root-password\n`\n```\nenter image description here",
      "solution": "There are multiple typos in the yaml you have provided in the question.\nI have corrected them as following , use following yaml and check\n\n```\n`apiVersion: apps/v1\nkind: Deployment    #corrected typo deployment to Deployment\nmetadata:\n  name: mongodb-deployment\n  labels:\n    app: mongodb\nspec:\n  replicas: 1\n  selector:\n    matchLabels:            #corrected typo matchLebels to matchLabels\n       app: mongodb\n  template:\n    metadata:\n      labels:               #corrected typo lebels to labels\n        app: mongodb\n    spec:\n     containers:\n     - name: mongodb\n       image: mongo\n       ports:\n       - containerPort: 27017\n       env:\n       - name: MONGO_INITDB_ROOT_USERNAE\n         valueFrom:\n           secretKeyRef:                  #corrected typo secretKeyref to secretKeyRef\n             name: mongodb-secret\n             key: mongo-root-username\n       - name: MONGO_INITDB_ROOT_PASSWORD\n         valueFrom:\n          secretKeyRef:                  #corrected typo secretKeyref to secretKeyRef\n            name: mongodb-secret\n            key: mongo-root-password\n`\n```",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2021-07-03T18:25:57",
      "url": "https://stackoverflow.com/questions/68238075/deployment-in-version-v1-cannot-be-handled-as-a-deployment-no-kind-deploymen"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "minikube",
      "question_id": 67537060,
      "title": "Unable to add `linkerd.io/inject: enabled` to ArgoCD manifest - invalid type for io.k8s.apimachinery.pkg.apis.meta.v1.ObjectMeta.annotations",
      "problem": "I can install bitnami/redis with this helm command:\n```\n`helm upgrade --install \"my-release\" bitnami/redis \\\n     --set auth.existingSecret=redis-key \\\n     --set metrics.enabled=true \\\n     --set metrics.podAnnotations.release=prom \\\n     --set master.podAnnotations.\"linkerd\\.io/inject\"=enabled \\\n     --set replica.podAnnotations.\"linkerd\\.io/inject\"=enabled\n`\n```\nNow I want to install it using ArgoCD Manifest.\n```\n`project: default\nsource:\n  repoURL: 'https://charts.bitnami.com/bitnami'\n  targetRevision: 14.1.1\n  helm:\n    valueFiles:\n      - values.yaml\n    parameters:\n      - name: metrics.enabled\n        value: 'true'\n      - name: metrics.podAnnotations.release\n        value: 'prom'\n      - name: master.podAnnotations.linkerd.io/inject\n        value: enabled\n      - name: replica.podAnnotations.linkerd.io/inject\n        value: enabled\n      - name: auth.existingSecret\n        value: redis-key\n  chart: redis\ndestination:\n  server: 'https://kubernetes.default.svc'\n  namespace: default\nsyncPolicy: {}\n`\n```\nBut I'm getting validation error because of `master.podAnnotations.linkerd.io/inject` and `replica.podAnnotations.linkerd.io/inject`\n```\n`error validating data: ValidationError(StatefulSet.spec.template.metadata.annotations.\"linkerd): invalid type for io.k8s.apimachinery.pkg.apis.meta.v1.ObjectMeta.annotations: got \"map\", expected \"string\"\n\nerror validating data: ValidationError(StatefulSet.spec.template.metadata.annotations.\"linkerd): invalid type for io.k8s.apimachinery.pkg.apis.meta.v1.ObjectMeta.annotations: got \"map\", expected \"string\"\n\n`\n```\nIf I remove these two annotation settings the app can be installed.\nI've tried `master.podAnnotations.\"linkerd.io\\/inject\"`, but it doesn't work. I guess it has something to do with the \".\" or \"/\". Can anyone help me solve this issue?",
      "solution": "Look at this example, parameters containing dots need to be escaped.",
      "question_score": 1,
      "answer_score": 3,
      "created_at": "2021-05-14T17:39:45",
      "url": "https://stackoverflow.com/questions/67537060/unable-to-add-linkerd-io-inject-enabled-to-argocd-manifest-invalid-type-for"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "minikube",
      "question_id": 67212876,
      "title": "error when creating &quot;STDIN&quot;: Internal error occurred while running skaffold dev",
      "problem": "So, I'm using minikube v1.19.0 in ubuntu and using nginx-ingress with kubernetes. I have two node files: auth and client having docker image made respectively\ni got 4 kubernetes cinfig files which are as follows:\nauth-deply.yaml:\n`apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: auth-depl\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: auth\n  template:\n    metadata:\n      labels:\n        app: auth\n    spec:\n      containers:\n        - name: auth\n          image: xyz/auth\n          env:\n            - name: JWT_KEY\n              valueFrom:\n                secretKeyRef:\n                  name: jwt-secret\n                  key: JWT_KEY\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: auth-srv\nspec:\n  selector:\n    app: auth\n  ports:\n    - name: auth\n      protocol: TCP\n      port: 3000\n      targetPort: 3000\n\n`\nauth-moongo-depl.yaml:\n`apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: auth-mongo-depl\nspec:\n  selector:\n    matchLabels:\n      app: auth-mongo\n  template:\n    metadata:\n      labels:\n        app: auth-mongo\n    spec:\n      containers:\n      - name: auth-mongo\n        image: mongo\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: auth-mongo-srv\nspec:\n  selector:\n    app: auth-mongo\n  ports:\n    - name: db\n      protocol: TCP\n      port: 27017\n      targetPort: 27017\n`\nclient-depl.yaml:\n`apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: client-depl\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: client\n  template:\n    metadata:\n      labels:\n        app: client\n    spec:\n      containers:\n        - name: client\n          image: xyz/client\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: client-srv\nspec:\n  selector:\n    app: client\n  ports:\n    - name: client\n      protocol: TCP\n      port: 3000\n      targetPort: 3000\n`\ningress-srv.yaml:\n`apiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: ingress-service\n  annotations:\n    kubernetes.io/ingress.class: nginx\n    nginx.ingress.kubernetes.io/use-regex: 'true'\nspec:\n  rules:\n    - host: ticketing.dev\n      http:\n        paths:\n          - path: /api/users/?(.*)\n            backend:\n              serviceName: auth-srv\n              servicePort: 3000\n          - path: /?(.*)\n            backend:\n              serviceName: client-srv\n              servicePort: 3000\n\n`\nskaffold.yaml:\n`apiVersion: skaffold/v2alpha3\nkind: Config\ndeploy:\n  kubectl:\n    manifests:\n      - ./infra/k8s/*\nbuild:\n  local:\n    push: false\n  artifacts:\n    - image: xyz/auth\n      context: auth\n      docker:\n        dockerfile: Dockerfile\n      sync:\n        manual:\n          - src: 'src/**/*.ts'\n            dest: .\n    - image: xyz/client\n      context: client\n      docker:\n        dockerfile: Dockerfile\n      sync:\n        manual:\n          - src: '**/*.js'\n            dest: .\n`\nNow, when I run `skaffold dev` the following error is coming:\n`Listing files to watch...\n - xyz/auth\n - xyz/client\nGenerating tags...\n - xyz/auth -> xyz/auth:abcb6e4\n - xyz/client -> xyz/client:abcb6e4\nChecking cache...\n - xyz/auth: Found Locally\n - xyz/client: Found Locally\nStarting test...\nTags used in deployment:\n - xyz/auth -> xyz/auth:370487d5c0136906178e602b3548ddba9db75106b22a1af238e02ed950ec3f21\n - xyz/client -> xyz/client:a56ea90769d6f31e983a42e1c52275b8ea2480cb8905bf19b08738e0c34eafd3\nStarting deploy...\n - deployment.apps/auth-depl configured\n - service/auth-srv configured\n - deployment.apps/auth-mongo-depl configured\n - service/auth-mongo-srv configured\n - deployment.apps/client-depl configured\n - service/client-srv configured\n - Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress\n - Error from server (InternalError): error when creating \"STDIN\": Internal error occurred: failed calling webhook \"validate.nginx.ingress.kubernetes.io\": an error on the server (\"\") has prevented the request from succeeding\nexiting dev mode because first deploy failed: kubectl apply: exit status 1\n\n`\nActually everything was working fine until i reinstall minikube again and getting this problem.\nNeed some help here.",
      "solution": "Actually I just found out the issue was when reinstalling the minikube, Validating Webhook was not deleted and creating the issue hence, should be removed using following command.\n`kubectl delete -A ValidatingWebhookConfiguration ingress-nginx-admission\n`\nI found out that while reinstalling i forgot to remove this webhook that is installed in the manifests which created this problem.\nAdditional links related to this problem:\nNginx Ingress: service \"ingress-nginx-controller-admission\" not found\nNginx Ingress Controller - Failed Calling Webhook",
      "question_score": 1,
      "answer_score": 3,
      "created_at": "2021-04-22T13:59:31",
      "url": "https://stackoverflow.com/questions/67212876/error-when-creating-stdin-internal-error-occurred-while-running-skaffold-dev"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "minikube",
      "question_id": 66530133,
      "title": "Installing RabbitMQ on Minikube",
      "problem": "I'm currently building a backend, that among other things, involves sending RabbitMQ messages from localhost into a K8s cluster where containers can run and pickup specific messages.\nSo far I've been using Minikube to carry out all of my Docker and K8s development but have ran into a problem when trying to install RabbitMQ.\nI've been following the RabbitMQ Cluster Operator official documentation (installing) (using). I got to the \"Create a RabbitMQ Instance\" section and ran into this error:\n```\n`1 pod has unbound immediate persistentVolumeClaims\n`\n```\nI fixed it by continuing with the tutorial and adding a PV and PVC into my RabbitMQCluster YAML file. Tried to apply it again and came across my next issue:\n```\n`1 insufficient cpu\n`\n```\nI've tried messing around with resource limits and requests in the YAML file but no success yet. After Googling and doing some general research I noticed that my specific problems and setup (Minikube and RabbitMQ) doesn't seem to be very popular. My question is, have I passed the scope or use case of Minikube by trying to install external services like RabbitMQ? If so what should be my next step?\nIf not, are there any useful tutorials out there for installing RabbitMQ in Minikube?\nIf it helps, here's my current YAML file for the RabbitMQCluster:\n```\n`apiVersion: rabbitmq.com/v1beta1\nkind: RabbitmqCluster\nmetadata:\n  name: rabbitmq-cluster\nspec:\n  persistence:\n    storageClassName: standard\n    storage: 5Gi\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: rabbimq-pvc\nspec:\n  resources:\n    requests:\n      storage: 5Gi\n  volumeMode: Filesystem\n  accessModes:\n    - ReadWriteOnce\n---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: rabbitmq-pv\nspec:\n  capacity:\n    storage: 5Gi\n  volumeMode: Filesystem\n  accessModes:\n    - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Recycle\n  storageClassName: standard\n  hostPath:\n    path: /mnt/app/rabbitmq\n    type: DirectoryOrCreate\n`\n```\nEdit:\nCommand used to start Minikube:\n```\n`minikube start\n`\n```\nOutput:\n```\n`\ud83d\ude04  minikube v1.17.1 on Ubuntu 20.04\n\u2728  Using the docker driver based on existing profile\n\ud83d\udc4d  Starting control plane node minikube in cluster minikube\n\ud83d\udd04  Restarting existing docker container for \"minikube\" ...\n\ud83c\udf89  minikube 1.18.1 is available! Download it: https://github.com/kubernetes/minikube/releases/tag/v1.18.1\n\ud83d\udca1  To disable this notice, run: 'minikube config set WantUpdateNotification false'\n\n\ud83d\udc33  Preparing Kubernetes v1.20.2 on Docker 20.10.2 ...\n\ud83d\udd0e  Verifying Kubernetes components...\n\ud83c\udf1f  Enabled addons: storage-provisioner, default-storageclass, dashboard\n\ud83c\udfc4  Done! kubectl is now configured to use \"minikube\" cluster and \"default\" namespace by default\n`\n```",
      "solution": "According to the command you used to start minikube, the error is because you don't have enough resources assigned to your cluster.\nAccording to the source code from the rabbitmq cluster operator, it seems that it needs 2CPUs.\nYou need to adjust the number of CPUs (and probably the memory also) when you initialize your cluster. Below is an example to start a kubernetes cluster with 4 cpus and 8G of RAM :\n`minikube start --cpus=4 --memory 8192\n`\nIf you want to check your current allocated ressources, you can run `kubectl describe node`.",
      "question_score": 1,
      "answer_score": 3,
      "created_at": "2021-03-08T13:47:05",
      "url": "https://stackoverflow.com/questions/66530133/installing-rabbitmq-on-minikube"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "minikube",
      "question_id": 65656335,
      "title": "Unable to update Minikube Deployment with new image",
      "problem": "I'm running a local minikube cluster as of version v1.15.1 with Kubernetes version v1.19.4 and Docker 19.03.13. Local hypervisor is hyperkit.\nI'm unable to update the deployment with a new image version from Docker Hub.\nI'm using `imagePullPolicy: Always`.\nThe Image is clearly updated on Docker Hub. I pulled it locally and ran the image outside the kubernetes cluster to test that.\nI tried the following methods to update within the cluster without success:\n\nDeleting the pod and letting it be restarted by kubernetes (because the deployment specifies one replica)\nDeleting the whole deployment via `kubectl delete deployment service-frontend-voting`\nDoing a rolling update via `kubectl rollout restart deployment service-frontend-voting`, `kubectl get pods` shows that the pod is only seconds old. But still, old image.\nUpdating the deployment via  `kubectl set image deployment/service-frontend-voting service-frontend-voting=starax/service-frontend-voting:latest`\nPushing same image to new tag on docker hub and doing steps 1-3 switching `:latest` to `:next` and vice versa\nUsing only `:latest` or `imagePullPolicy: Always` in the deployment spec, because apparently Kubernetes should pull the image if either of those are specified.\n\nNone of that methods have updated the image. What am I missing here?\nThe application itself is a simple VueJS app. The wanted change is the backend-url. Using environment variables with a production vue-js app is not trivial and therefor skipped here.\nDeployment.yaml for reference:\n```\n`apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: service-frontend-voting\n  labels:\n    app: service-frontend-voting\n    role: app\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: service-frontend-voting\n  template:\n    metadata:\n      labels:\n        app: service-frontend-voting\n    spec:\n      containers:\n      - name: service-frontend-voting\n        image: starax/service-frontend-voting\n        ports:\n        - containerPort: 8080\n        imagePullPolicy: Always\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: service-frontend-voting-service\nspec:\n  selector:\n    app: service-frontend-voting\n  type: LoadBalancer\n  ports:\n    - protocol: TCP\n      port: 8080\n      targetPort: 8080\n      nodePort: 31112\n`\n```\nDockerfile\n```\n`FROM node:lts-alpine\n\n# install simple http server for serving static content\nRUN npm install -g http-server\n\n# make the 'app' folder the current working directory\nWORKDIR /app\n\n# copy both 'package.json' and 'package-lock.json' (if available)\nCOPY package*.json ./\n\n# install project dependencies\nRUN npm install\n\n# copy project files and folders to the current working directory (i.e. 'app' folder)\nCOPY . .\n\n# build app for production with minification\nRUN npm run build\n\nEXPOSE 8080\nCMD [ \"http-server\", \"dist\" ]\u23ce\n`\n```",
      "solution": "If you did pull the image outside the minikube cluster and you successfully run it as docker container .. then It should work(check if the IMAGE ID is unique, else if  you didnt change anything in dockerfile then newly created image will have the same image ID ) If successfully created , tagged and uploaded like :  `dockerhub starax/service-frontend-voting:test`\nYou can do these steps\n\n`eval $(minikube docker-env) `\n\n`docker images` => check the IMAGE IDs and name of images and their tags\n\n`docker pull  starax/service-frontend-voting:test ` => this will download\nthe test tag\n\nrun `docker images `again  => to check  if image ID , tags are different than the previous onrd\n\nAfter that in manifest file  `kubectl edit deploy service-frontend-voting ` change `imagePullPolicy: IfNotPresent`\nand `image: starax/service-frontend-voting:test`\n\nNotes the command 1. you used wont do anything, because deployment will recreate another pod even if you deleted that pod , the new pod will have the image that is indicated in deployment which is same the old version.`kubectl describe service-frontend-voting ` to check name and tag of the image that being used ..\nThe command 2,3 will delete, then create a same  deployment as before, and the image pulled will be starax/service-frontend-voting:latest\nTo update image use =>  4\nAbout 6, if you don't specify the tag of an image K8s will pulled that image with tag latest  tag.Also If you dont indicate imagePullPolicy then policy by default is Always ..\nAlso check `kubectl get events` is good command to tell you if you pulled a new image or no\nI hope I helped",
      "question_score": 1,
      "answer_score": 3,
      "created_at": "2021-01-10T18:34:43",
      "url": "https://stackoverflow.com/questions/65656335/unable-to-update-minikube-deployment-with-new-image"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "minikube",
      "question_id": 65552399,
      "title": "Skaffold dev fails",
      "problem": "I am having this error, after running skaffold dev.\n```\n`Step 1/6 : FROM node:current-alpine3.11\nexiting dev mode because first build failed: unable to stream build output: Get https://registry-1.docker.io/v2/: dial tcp: lookup registry-1.docker.io on 192.168.49.1:53: read udp 192.168.49.2:35889->192.168.49.1:53: i/o timeout. Please fix the Dockerfile and try again..\n\n`\n```\nHere is skaffold.yml\n```\n`apiVersion: skaffold/v2beta11\nkind: Config\nmetadata:\n  name: *****\nbuild:\n  artifacts:\n    - image: 127.0.0.1:32000/auth\n      context: auth\n      docker:\n        dockerfile: Dockerfile\ndeploy:\n  kubectl:\n    manifests:\n      - infra/k8s/auth-depl.yaml\n  local:\n    push: false\n  artifacts:\n    - image: 127.0.0.1:32000/auth\n      context: auth\n      docker:\n        dockerfile: Dockerfile\n      sync:\n        manual:\n          - src: \"src/**/*.ts\"\n            dest: .\n\n`\n```\nI have tried all possible solutions I saw online, including adding 8.8.8.8 as the DNS, but the error still persists. I am using Linux and running ubuntu, I am also using Minikube locally. Please assist.",
      "solution": "This is a Community Wiki answer, posted for better visibility, so feel free to edit it and add any additional details you consider important.\nIn this case:\n```\n`minikube delete && minikube start\n`\n```\nsolved the problem but you can start from restarting docker daemon. Since this is Minikube cluster and Skaffold uses for its builds Minikube's Docker daemon, as suggested by Brian de Alwis in his comment, you may start from:\n```\n`minikube stop && minikube start\n`\n```\nor\n```\n`minikube ssh\nsu\nsystemctl restart docker\n`\n```\nI searched for similar errors and in many cases e.g. here or in this thread, setting up your DNS to something reliable like `8.8.8.8` may also help:\n```\n`sudo echo \"nameserver 8.8.8.8\" >> /etc/resolv.conf\n`\n```\nin case you use Minikube you should first:\n```\n`minikube ssh\n\nsu ### to become root\n`\n```\nand then run:\n```\n`echo \"nameserver 8.8.8.8\" >> /etc/resolv.conf\n`\n```\nThe following error message:\n```\n`Please fix the Dockerfile and try again\n`\n```\nmay be somewhat misleading in similar cases as `Dockerfile` is probably totally fine, but as we can read in other part:\n```\n`lookup registry-1.docker.io on 192.168.49.1:53: read udp 192.168.49.2:35889->192.168.49.1:53: i/o timeout.\n`\n```\nit's definitely related with failing DNS lookup. This is well described here as well known issue.\n\nGet i/o timeout\nGet https://index.docker.io/v1/repositories//images: dial tcp: lookup  on :53: read udp :53: i/o timeout\nDescription\nThe DNS resolver configured on the host cannot resolve the registry\u2019s\nhostname.\nGitHub link\nN/A\nWorkaround\nRetry the operation, or if the error persists, use another DNS\nresolver. You can do this by updating your  `/etc/resolv.conf`  file\nwith these or other DNS servers:\n`nameserver 8.8.8.8 nameserver 8.8.4.4`",
      "question_score": 1,
      "answer_score": 3,
      "created_at": "2021-01-03T17:47:38",
      "url": "https://stackoverflow.com/questions/65552399/skaffold-dev-fails"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "minikube",
      "question_id": 79327393,
      "title": "Unknown feature gate KafkaNodePools found in the configuration",
      "problem": "I was trying to configure a KafkaNodePools functionality (and that's were probably working) using this guide -> click but now my cluster-operator falls with a error. How can I fix it?\n```\n`> kubectl logs strimzi-cluster-operator-759856c456-lfxqx -n kafka\n`\n```\n```\n`Exception in thread \"main\" io.strimzi.operator.common.InvalidConfigurationException: Unknown feature gate KafkaNodePools found in the configuration\n        at io.strimzi.operator.common.featuregates.FeatureGates.(FeatureGates.java:51)\n        at io.strimzi.operator.common.config.ConfigParameter.get(ConfigParameter.java:89)\n        at io.strimzi.operator.common.config.ConfigParameter.define(ConfigParameter.java:72)\n        at io.strimzi.operator.cluster.ClusterOperatorConfig.buildFromMap(ClusterOperatorConfig.java:310)\n        at io.strimzi.operator.cluster.ClusterOperatorConfig.buildFromMap(ClusterOperatorConfig.java:292)\n        at io.strimzi.operator.cluster.Main.main(Main.java:62)\n`\n```\n```\n`apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  annotations:\n    deployment.kubernetes.io/revision: \"25\"\n  creationTimestamp: \"2024-11-08T15:41:13Z\"\n  generation: 25\n  labels:\n    app: strimzi\n  name: strimzi-cluster-operator\n  namespace: kafka\n  resourceVersion: \"331173\"\n  uid: 97ce684c-e542-4ac4-bc9e-e3314a844828\nspec:\n  progressDeadlineSeconds: 600\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      name: strimzi-cluster-operator\n      strimzi.io/kind: cluster-operator\n  strategy:\n    rollingUpdate:\n      maxSurge: 25%\n      maxUnavailable: 25%\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/restartedAt: \"2025-01-03T21:20:24+03:00\"\n      creationTimestamp: null\n      labels:\n        name: strimzi-cluster-operator\n        strimzi.io/kind: cluster-operator\n    spec:\n      containers:\n      - args:\n        - /opt/strimzi/bin/cluster_operator_run.sh\n        env:\n        - name: STRIMZI_FEATURE_GATES\n          value: +KafkaNodePools\n        - name: STRIMZI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: STRIMZI_FULL_RECONCILIATION_INTERVAL_MS\n          value: \"120000\"\n        - name: STRIMZI_OPERATION_TIMEOUT_MS\n          value: \"300000\"\n        - name: STRIMZI_DEFAULT_KAFKA_EXPORTER_IMAGE\n          value: quay.io/strimzi/kafka:0.45.0-kafka-3.9.0\n        - name: STRIMZI_DEFAULT_CRUISE_CONTROL_IMAGE\n          value: quay.io/strimzi/kafka:0.45.0-kafka-3.9.0\n        - name: STRIMZI_KAFKA_IMAGES\n          value: |\n            3.8.0=quay.io/strimzi/kafka:0.44.0-kafka-3.8.0\n            3.8.1=quay.io/strimzi/kafka:0.44.0-kafka-3.8.1\n            3.9.0=quay.io/strimzi/kafka:0.45.0-kafka-3.9.0\n        - name: STRIMZI_KAFKA_CONNECT_IMAGES\n          value: |\n            3.8.1=quay.io/strimzi/kafka:0.44.0-kafka-3.8.1\n            3.8.0=quay.io/strimzi/kafka:0.44.0-kafka-3.8.0\n            3.9.0=quay.io/strimzi/kafka:0.45.0-kafka-3.9.0\n        - name: STRIMZI_KAFKA_MIRROR_MAKER_IMAGES\n          value: |\n            3.8.1=quay.io/strimzi/kafka:0.44.0-kafka-3.8.1\n            3.8.0=quay.io/strimzi/kafka:0.44.0-kafka-3.8.0\n            3.9.0=quay.io/strimzi/kafka:0.45.0-kafka-3.9.0\n        - name: STRIMZI_KAFKA_MIRROR_MAKER_2_IMAGES\n          value: |\n            3.8.1=quay.io/strimzi/kafka:0.44.0-kafka-3.8.1\n            3.8.0=quay.io/strimzi/kafka:0.44.0-kafka-3.8.0\n            3.9.0=quay.io/strimzi/kafka:0.45.0-kafka-3.9.0\n        - name: STRIMZI_DEFAULT_TOPIC_OPERATOR_IMAGE\n          value: quay.io/strimzi/operator:0.45.0\n        - name: STRIMZI_DEFAULT_USER_OPERATOR_IMAGE\n          value: quay.io/strimzi/operator:0.45.0\n        - name: STRIMZI_DEFAULT_KAFKA_INIT_IMAGE\n          value: quay.io/strimzi/operator:0.45.0\n        - name: STRIMZI_DEFAULT_KAFKA_BRIDGE_IMAGE\n          value: quay.io/strimzi/kafka-bridge:0.31.1\n        - name: STRIMZI_DEFAULT_KANIKO_EXECUTOR_IMAGE\n          value: quay.io/strimzi/kaniko-executor:0.45.0\n        - name: STRIMZI_DEFAULT_MAVEN_BUILDER\n          value: quay.io/strimzi/maven-builder:0.45.0\n        - name: STRIMZI_OPERATOR_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: STRIMZI_LEADER_ELECTION_ENABLED\n          value: \"true\"\n        - name: STRIMZI_LEADER_ELECTION_LEASE_NAME\n          value: strimzi-cluster-operator\n        - name: STRIMZI_LEADER_ELECTION_LEASE_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: STRIMZI_LEADER_ELECTION_IDENTITY\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.name\n        image: quay.io/strimzi/operator:0.45.0\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthy\n            port: http\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 1\n        name: strimzi-cluster-operator\n        ports:\n        - containerPort: 8080\n          name: http\n          protocol: TCP\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /ready\n            port: http\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 1\n        resources:\n          limits:\n            cpu: \"1\"\n            memory: 384Mi\n          requests:\n            cpu: 200m\n            memory: 384Mi\n        terminationMessagePath: /dev/termination-log\n        terminationMessagePolicy: File\n        volumeMounts:\n        - mountPath: /tmp\n          name: strimzi-tmp\n        - mountPath: /opt/strimzi/custom-config/\n          name: co-config-volume\n      dnsPolicy: ClusterFirst\n      restartPolicy: Always\n      schedulerName: default-scheduler\n      securityContext: {}\n      serviceAccount: strimzi-cluster-operator\n      serviceAccountName: strimzi-cluster-operator\n      terminationGracePeriodSeconds: 30\n      volumes:\n      - emptyDir:\n          medium: Memory\n          sizeLimit: 1Mi\n        name: strimzi-tmp\n      - configMap:\n          defaultMode: 420\n          name: strimzi-cluster-operator\n        name: co-config-volume\nstatus:\n  conditions:\n  - lastTransitionTime: \"2025-01-03T18:15:00Z\"\n    lastUpdateTime: \"2025-01-03T18:15:00Z\"\n    message: Deployment does not have minimum availability.\n    reason: MinimumReplicasUnavailable\n    status: \"False\"\n    type: Available\n  - lastTransitionTime: \"2025-01-03T18:10:41Z\"\n    lastUpdateTime: \"2025-01-03T18:25:54Z\"\n    message: ReplicaSet \"strimzi-cluster-operator-759856c456\" is progressing.\n    reason: ReplicaSetUpdated\n    status: \"True\"\n    type: Progressing\n  observedGeneration: 25\n  replicas: 2\n  unavailableReplicas: 2\n  updatedReplicas: 1\n`\n```\nI was trying to fix it changing version of strimzi (because the internet said it could be a reason for a problem) but instead all my deployment has been crashed.\n```\n`PS C:\\Users\\user> kubectl get pods -n kafka\nNAME                                         READY   STATUS             RESTARTS        AGE\nmy-bridge-bridge-54fcdb8865-nkfbb            1/1     Running            122 (22m ago)   55d\nmy-cluster-dual-role-0                       1/1     Running            32 (24m ago)    19d\nmy-cluster-dual-role-1                       1/1     Running            86 (24m ago)    51d\nmy-cluster-dual-role-2                       1/1     Running            81 (24m ago)    51d\nmy-cluster-entity-operator-596ff884f-jtqcf   2/2     Running            346 (22m ago)   56d\nstrimzi-cluster-operator-6cfb7f8f4d-zr47s    0/1     CrashLoopBackOff   7 (4m ago)      14m\nstrimzi-cluster-operator-759856c456-lfxqx    0/1     CrashLoopBackOff   8 (4m39s ago)   20m\n`\n```",
      "solution": "The `KafkaNodePools` feature gate has matured to GA and cannot be enabled / disabled anymore (the Kafka node pools are now always enabled). So you should remove it from the configuration. You can just remove the whole section:\n`        - name: STRIMZI_FEATURE_GATES\n          value: +KafkaNodePools\n`",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2025-01-03T19:41:17",
      "url": "https://stackoverflow.com/questions/79327393/unknown-feature-gate-kafkanodepools-found-in-the-configuration"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "minikube",
      "question_id": 77479976,
      "title": "Cannot SSH log into docker created by minikube with Hyper-V driver",
      "problem": "I have installed minikube and did successfully create a cluster with `minikube start --driver=hyperv`, perhaps. I deduce that it is successfully created because after `minikube status` I got output that all is running and configured.\nThe next step I do is to `ssh docker@IP_ADDRESS` while `IP_ADDRESS` is the one I got from `minikube ip` command.\nThe result is that I am prompted for the password. I type `tcuser` but it seems that password is incorrect.\nWhere to look for now? Is password wrong or maybe something is not set up properly?",
      "solution": "Please use the SSH private key on Minikube (already created), no need for password:\n```\n`ssh -i $(minikube ssh-key) docker@$(minikube ip)\n`\n```",
      "question_score": 1,
      "answer_score": 1,
      "created_at": "2023-11-14T11:48:13",
      "url": "https://stackoverflow.com/questions/77479976/cannot-ssh-log-into-docker-created-by-minikube-with-hyper-v-driver"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "minikube",
      "question_id": 76415056,
      "title": "Configure Mongo with TLS",
      "problem": "I'm working on configuring my Mongo 4.2 with TLS using minikube.\nThese are my arguments: `--auth --tlsMode requireTLS --tlsCertificateKeyFile /etc/ssl/mongodb-test-ca.crt  --tlsCAFile /etc/ssl/test-ca.pem --oplogSize 32 --quiet --replSet myreplicaset --logpath /dev/stdout`\nI'm getting this error:\n`cannot read certificate file: /etc/ssl/mongodb-test-ca.key error:0909006C:PEM routines:get_name:no start line`\nIt looks like there is some problem with the .pem files that I'm using. To configure them, I've followed the instructions here https://www.mongodb.com/docs/manual/appendix/security/appendixA-openssl-ca/\nMore specifically, the commands I've used are, after creating that `openssl-test-ca.cnf` file that they suggest are:\n```\n`openssl genrsa -out mongodb-test-ca.key 4096\nopenssl req -new -x509 -days 1826 -key mongodb-test-ca.key -out mongodb-test-ca.crt -config openssl-test-ca.cnf\nopenssl genrsa -out mongodb-test-ia.key 4096\nopenssl req -new -key mongodb-test-ia.key -out mongodb-test-ia.csr -config openssl-test-ca.cnf\nopenssl x509 -sha256 -req -days 730 -in mongodb-test-ia.csr -CA mongodb-test-ca.crt -CAkey mongodb-test-ca.key -set_serial 01 -out mongodb-test-ia.crt -extfile openssl-test-ca.cnf -extensions v3_ca\ncat mongodb-test-ca.crt mongodb-test-ia.crt  > test-ca.pem\n`\n```\nWhat am I doing wrong? One idea is that the files that I'm using were not the correct ones, but I'm only seeing one .pem file on my process, the `test-ca.pem`. For the key `tlsCertificateKeyFile` I've tested also `mongodb-test-ca.key` `and mongodb-test-ia.key` without success",
      "solution": "You must create the `.pem` like this:\n```\n`cat mongodb-test-ia.crt mongodb-test-ia.key > mongodb-test-ia.pem\n`\n```\nand then run mongod with\n```\n`--tlsCertificateKeyFile /etc/ssl/mongodb-test-ia.pem --tlsCAFile /etc/ssl/test-ca.crt \n`\n```\nIn order to test the certificates you can also use `openssl`. Try\n```\n`openssl verify -CAfile /etc/ssl/mongodb-test-ca.crt /etc/ssl/mongodb-test-ia.pem\n`\n```\nOr if you like to do it a bit more advanced, open a shell and enter\n```\n`openssl s_server -cert /etc/ssl/mongodb-test-ia.pem\n`\n```\nThen open another shell and use\n```\n`openssl s_client -CAfile /etc/ssl/mongodb-test-ca.crt -quiet -no_ign_eof -status See also How Security in MongoDB works (using x.509 cert)",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2023-06-06T15:14:28",
      "url": "https://stackoverflow.com/questions/76415056/configure-mongo-with-tls"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "helm",
      "question_id": 67597403,
      "title": "argocd - stuck at deleting but resources are already deleted",
      "problem": "argoproj/argocd:v1.8.7\nHave a helm chart (1 with ingress, 1 with deployment/service/cm).\nIt has sync policies of automated (prune and self-heal). When trying to delete them from the argocd dashboard, they are getting deleted (no more on the k8s cluster), however the status on the dashboard has been stuck at Deleting.\n\nIf I try to click sync, it shows -> Unable to deploy revision: application is deleting.\nAny ideas why it's stuck on Deleting status even though all resources have been deleted already ?  Is there a way to refresh the status in the dashboard to reflect that actual state?\nThanks!\n================\nUpdate:\nAfter doing cascade delete, this is the screenshot (i've removed the app names that why it's white for some part)\n\nDoing kubectl get all -A shows all resources isn't present anymore (e.g even the cm, svc, deploy, etc)",
      "solution": "I was actually able to make this work by updating the Application yaml:\n\nAdd `spec.syncPolicy.allowEmpty: true`\nRemove `metadata.finalizers`\n\nThe working version without getting stuck at Deleting status:\n```\n`apiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n    name: service-name\n    namespace: argocd\nspec:\n    destination:\n        server: https://kubernetes.default.svc\n        namespace: argocd\n    project: proj-name\n    source:\n        path: service-name\n        repoURL: ssh://...git\n        targetRevision: dev\n        helm:\n            valueFiles:\n                - ../values.yaml\n                - ../values_version_dev.yaml\n    syncPolicy:\n        automated:\n            prune: true\n            allowEmpty: true\n            selfHeal: true\n`\n```",
      "question_score": 22,
      "answer_score": 28,
      "created_at": "2021-05-19T07:27:24",
      "url": "https://stackoverflow.com/questions/67597403/argocd-stuck-at-deleting-but-resources-are-already-deleted"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "helm",
      "question_id": 66881237,
      "title": "How can I tell what causes a helm chart deployment to fail?",
      "problem": "If a Helm deployment's status is `failed`, what can I do to determine what made it fail?",
      "solution": "`helm history `\nShows the kubernetes errors for the attempted deployment of that release.",
      "question_score": 15,
      "answer_score": 22,
      "created_at": "2021-03-31T05:26:17",
      "url": "https://stackoverflow.com/questions/66881237/how-can-i-tell-what-causes-a-helm-chart-deployment-to-fail"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "helm",
      "question_id": 76483758,
      "title": "helm upgrade fail after upgrade to Kubernetes 1.25 due to change in HorizontalPodAutoscaler API version",
      "problem": "After upgrading Kubernetes to 1.25\n`helm upgrade --install ...` fails with the following error:\n`Error: UPGRADE FAILED: unable to recognize \"\": no matches for kind \"HorizontalPodAutoscaler\" in version \"autoscaling/v2beta1\"\n`\nIn order to resolve it I changed the HPA from `autoscaling/v2beta1` to `autoscaling/v2`\nand update the new API syntax. but I keep getting the same error when trying to upgrade the helm release.\nThe only way to resolve it was to uninstall and reinstall the release.\ncan someone explain the reason for the error and how to fix it without deleting and reinstall it?",
      "solution": "helm3 keeps the release state in secret, the last release helm state contains the old API `autoscaling/v2beta1`, and for some reason, it cause an error on the upgrade.\nTo resolve it, I edit the helm secret unpack the `.data.release` with base64 encode twice, and unzip replace  `autoscaling/v2beta1` with `autoscaling/v2` then zip it encode twice.\nAfter this change and the change for the new API version (and syntax), the problem was solved, and I could upgrade the chart again.\nMy fix:\n\nUpdate the hpa template to the latest API version\nUpdate (patch) the last helm secret (`secret/sh.helm.release.v1....`) with the following commands:\n\n`UPDATE=$(kubectl get secret \"${SECRET}\" -n \"${NAMESPACE}\" -otemplate='{{.data.release |base64decode |base64decode }}'|gzip -d|sed 's#autoscaling/v2beta1#autoscaling/v2#'| gzip |base64 -w0 |base64 -w0)\nkubectl patch secret \"${SECRET}\" -n \"${NAMESPACE}\" --patch=\"{\\\"data\\\": { \\\"release\\\": \\\"$UPDATE\\\" }}\"\n\n`\nI used this script for updating all helm secrets in all my namespaces.\n```\n`#!/bin/bash\n\nSECRET=$1\nNAMESPACE=$2\n\nif [[ -z \"${SECRET}\" ]]\nthen\n    echo \"Usage: $0  \"\n    exit\nfi\n\nif [[ -z \"${NAMESPACE}\" ]]\nthen\n    echo \"Usage: $0  \"\n    exit\nfi\n\nUPDATE=$(kubectl get secret \"${SECRET}\" -n \"${NAMESPACE}\" -otemplate='{{.data.release |base64decode |base64decode }}'|gzip -d|sed 's#autoscaling/v2beta1#autoscaling/v2#'| gzip |base64 -w0 |base64 -w0)\nkubectl patch secret \"${SECRET}\" -n \"${NAMESPACE}\" --patch=\"{\\\"data\\\": { \\\"release\\\": \\\"$UPDATE\\\" }}\"\n\n# Running example:\n## Fix single secret\n# ./fix-helm-hpa.sh  \n\n## Fix all secrets\n#kubectl get secret --field-selector=type=helm.sh/release.v1 -otemplate='{{range .items}}{{printf \"%s %s\\n\" .metadata.name .metadata.namespace }}{{end}}' |while read line ; do ./fix-helm-hpa.sh $line; done\n`\n```",
      "question_score": 15,
      "answer_score": 28,
      "created_at": "2023-06-15T18:32:56",
      "url": "https://stackoverflow.com/questions/76483758/helm-upgrade-fail-after-upgrade-to-kubernetes-1-25-due-to-change-in-horizontalpo"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "helm",
      "question_id": 71170639,
      "title": "Error converting YAML to JSON: yaml: line 15: did not find expected alphabetic or numeric character",
      "problem": "I want to set wildcard subdomain for my project, using k8s, nginx ingress controller, helm chart:\nIn `ingress.yaml` file:\n```\n`...\nrules:\n  - host: {{ .Values.ingress.host }}\n...\n`\n```\nIn `values.yaml` file, I change host `example.local` to `*.example.local`:\n```\n`...\ningress:\n  enabled: true\n  host: \"*.example.local\"\n...\n`\n```\nThen, when I install chart using helm chart:\n```\n`Error: YAML parse error on example/templates/ingress.yaml: error converting YAML to JSON: yaml: line 15: did not find expected alphabetic or numeric character\n`\n```\nHow can I fix it?\nThank for your support.",
      "solution": "YAML treats strings starting with asterisk in a special way - that's why the hostname with wildcards like `*.example.local`  breaks the ingress on `helm install`.\nIn order to be recognized as strings, the values in `ingress.yaml` file should be quoted with  `\" \"` characters:\n`...\nrules:\n  - host: \"{{ .Values.ingress.host }}\"\n...\n`\nOne more option here - adding `| quote` :\n`...\nrules:\n  - host: {{ .Values.ingress.host | quote }}\n...\n`\nI've reproduced your issue, both these options worked correctly. More information on quoting special characters for YAML is here.",
      "question_score": 14,
      "answer_score": 28,
      "created_at": "2022-02-18T09:47:06",
      "url": "https://stackoverflow.com/questions/71170639/error-converting-yaml-to-json-yaml-line-15-did-not-find-expected-alphabetic-o"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "helm",
      "question_id": 73130776,
      "title": "How do I set helm values (not files) in ArgoCD Application spec",
      "problem": "I looked all over the ArgoCD docs for this but somehow I cannot seem to find an answer. I have an application spec like so:\n```\n`apiVersion: argoproj.io/v1alpha1                                                                                                                                                                                                                                                          \nkind: Application                                                                                                                                                                                                                                                                         \nmetadata:                                                                                                                                                                                                                                                                                 \n  name: myapp                                                                                                                                                                                                                                                                            \n  namespace: argocd                                                                                                                                                                                                                                                                       \nspec:                                                                                                                                                                                                                                                                                     \n  destination:                                                                                                                                                                                                                                                                            \n    namespace: default                                                                                                                                                                                                                                                                    \n    server: https://kubernetes.default.svc                                                                                                                                                                                                                                                \n  project: default                                                                                                                                                                                                                                                                        \n  source:                                                                                                                                                                                                                                                                                 \n    helm:                                                                                                                                                                                                                                                                                 \n      valueFiles:                                                                                                                                                                                                                                                                         \n      - my-values.yaml                                                                                                                                                                                                                                                     \n    path: .                                                                                                                                                                                                                                        \n    repoURL: ssh://git@blah.git                                                                                                                                                                                                                        \n    targetRevision: HEAD\n`\n```\nHowever, I also need to specify a particular helm value (like you'd do with `--set` in the helm command. I see in the ArgoCD web UI that it has a spot for Values, but I have tried every combination of entries I can think of (somekey=somevalue, somekey:somevalue, somekey,somevalue). I also tried editing the manifest directly, but I still get similar errors trying to do so.\n\nThe error is long nonsense that ends with `error unmarshaling JSON: while decoding JSON: json: cannot unmarshal string into Go value of type map[string]interface {}`\nWhat is the correct syntax to set a single value, either through the web UI or the manifest file?",
      "solution": "you would use `parameters` via `spec.source.helm.parameters`\nsomething like:\n```\n`apiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: my-app\n  namespace: argocd\nspec:\n  project: my-project\n  source:\n    repoURL: https://charts.my-company.com\n    targetRevision: \"1234\"\n    chart: my-chart\n    helm:\n      parameters:\n        - name: my.helm.key\n          value: some-val\n  destination:\n    name: k8s-dev\n    namespace: my-ns\n`\n```\nSample from Argo Docs - https://argo-cd.readthedocs.io/en/stable/user-guide/helm/#build-environment",
      "question_score": 14,
      "answer_score": 7,
      "created_at": "2022-07-27T01:36:16",
      "url": "https://stackoverflow.com/questions/73130776/how-do-i-set-helm-values-not-files-in-argocd-application-spec"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "helm",
      "question_id": 72472488,
      "title": "Helm &#39;if&#39; condition in one line",
      "problem": "I was trying to put the `if` condition in a single line of the helm template:\n```\n`- name: ENV_VARIABLE\n  value: {{- if .Values.condition }}\"Value1\"{{- else }}\"Value2\"{{- end }}\n`\n```\nHowever, I'm getting error:\n\nError: YAML parse error on chart/templates/deployment.yaml: error\nconverting YAML to JSON: yaml: line NN: could not find expected ':'\n\nSo I've ended up with multi-line condition:\n```\n`- name: ENV_VARIABLE\n{{- if .Values.condition }}\n  value: \"Value1\"\n{{- else }}\n  value: \"Value2\"\n{{- end }}\n  \n`\n```\nwhich is working fine, but is very uncompact.\nIs there a way to use one-liner `if` condition in helm?",
      "solution": "What you do works, but you use the leading hyphen, which removes all preceding whitespace.\nThe below will render as `value:foo` due to the hyphen.\n```\n`value: {{- \"foo\" }}\n`\n```\nIf you remove the hyphen, it should work.\nThat said, there is also the ternary function which could fit here.\n\nternary\nThe ternary function takes two values, and a test value. If the test value is true, the first value will be returned. If the test value is empty, the second value will be returned. This is similar to the c ternary operator.\ntrue test value\n`ternary \"foo\" \"bar\" true`\nor\n`true | ternary \"foo\" \"bar\"`\n\n`- name: ENV_VARIABLE\n  value: {{ .Values.condition | ternary \"value1\" \"value2\" | quote }}\n`",
      "question_score": 12,
      "answer_score": 14,
      "created_at": "2022-06-02T09:46:45",
      "url": "https://stackoverflow.com/questions/72472488/helm-if-condition-in-one-line"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "helm",
      "question_id": 66601827,
      "title": "kubectl get last restart time of a pod",
      "problem": "as a preparation I'll tell what I know\nWhen I run `kubectl logs --previous ` I'll get last 20lines of logs before restart and timestamp when than happened, but I wanted to have that timestamp displayed when I run `kubectl get pods` as an additional column.\nThe kube docs on how to create custom columns show that\n`kubectl get pods \nbut my question is where is this metadata/spec stored??? where can I list all possible metadata so I know what column to search or write, and same for spec and any other possible column\nI know I can look at the pod with `kubectl describe pod ` but how do I translate fields there to this `.metadata.name` format ?\nPart of my descirbe looks like so\n`MacBook-Pro% kubectl describe pod xxx\nName:         xxx\nNamespace:    xx\nPriority:     0\nNode:         myname/myip\nStart Time:   Tue, 23 Feb 2021 11:37:01 +0100\nLabels:       app=xx\n              app.kubernetes.io/instance=xx\n              app.kubernetes.io/managed-by=Helm\n              app.kubernetes.io/name=xx\n              env=xx\n              helm.sh/chart=xx\n              pod-template-hash=yy\nAnnotations:  kubectl.kubernetes.io/restartedAt: 2020-10-23T11:21:09+02:00\nStatus:       Running\nIP:           10.xx\nIPs:\n  IP:           10.xx\nControlled By:  ReplicaSet/xxx\nContainers:\n  my_app:\n    Container ID:   docker://xxxx\n    Image:          gcr.ioxxxx\n    Image ID:       docker-pullable://gcrxxx\n    Port:           8080/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Fri, 12 Mar 2021 11:34:42 +0100\n    Last State:     Terminated\n      Reason:       Error\n      Exit Code:    143\n      Started:      Fri, 05 Mar 2021 12:34:58 +0100\n      Finished:     Fri, 12 Mar 2021 11:34:41 +0100\n`\nany thought how to get to `Last State.Finished` field ?\nWould appreciate any help, thanks.",
      "solution": "You can use `kubectl get pod -o yaml` to view your POD resource in the YAML format (or `-o json` if you prefer).\nIn this format, you can see the `metadata` keys and values. e.g.:\n```\n`$ kubectl get po -o yaml my-nginx-5b56ccd65f-4vmms | head -n 5\napiVersion: v1\nkind: Pod\nmetadata:\n  creationTimestamp: \"2021-03-09T15:19:38Z\"\n  generateName: my-nginx-5b56ccd65f-\n`\n```\nYou can also use `kubectl explain pod` to list and describe all possible fields associated with the pod API resource. It's possible to get the documentation of a specific field of a resource (e.g., `kubectl explain pods.status.containerStatuses`).\n\nTo get the `Last State.Finished` value you can use:\n```\n`$ kubectl get pods my-nginx-5b56ccd65f-4vmms -o custom-columns=NAME:.metadata.name,FINISHED:.status.containerStatuses[*].lastState.terminated.finishedAt\nNAME                        FINISHED\nmy-nginx-5b56ccd65f-4vmms   2021-03-09T15:35:45Z\n`\n```",
      "question_score": 12,
      "answer_score": 11,
      "created_at": "2021-03-12T15:31:56",
      "url": "https://stackoverflow.com/questions/66601827/kubectl-get-last-restart-time-of-a-pod"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "helm",
      "question_id": 77274854,
      "title": "Ingress controller does not allow snippets",
      "problem": "I'm trying to add a snippet to an existing Ingress that is attached to an ingress-controller.  However, when trying this, i get an error which is:\n`for: \"app-ingress.yml\": error when patching \"app-ingress.yml\": admission webhook \"validate.nginx.ingress.kubernetes.io\" denied the request: nginx.ingress.kubernetes.io/server-snippet annotation cannot be used. Snippet directives are disabled by the Ingress administrator `\nthe ingress already exists and I'm trying to patch it to add a header.  The updated ingress yaml is as follows:\n`apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: app-web-dev\n  namespace: application\n  annotations:\n    nginx.ingress.kubernetes.io/ssl-redirect: \"true\"\n    nginx.ingress.kubernetes.io/use-regex: \"true\"\n    nginx.ingress.kubernetes.io/server-snippet: |\n      expires 1M;\n      add_header Cache-Control \"public\";\n\nspec:\n  ingressClassName: nginx\n  tls:\n  - hosts:\n    - dev.address.nl\n    secretName: app-wildcard\n  rules:\n  - host: dev.address.nl\n    http:\n      paths:\n      - path: /?(.*)\n        pathType: Prefix\n        backend:\n          service:\n            name: appwebsvc\n            port:\n              number: 80\n`\nI'm trying to use helm to update my ingress controller to enable snippets, but i can't find the right commands to do this.\nAny ideas?\nI am trying something like this:\n`helm upgrade --namespace ingress-nginx --install --set controller.config.server-snippet=true,controller.service.annotations.nginx\\.ingress\\.kubernetes\\.io/allow-snippet-annotations=true --wait ingress-nginx ingress-nginx/ingress-nginx`",
      "solution": "If your ingress controller was previously installed using helm chart then you can use helm upgrade.\nPlease note that you should do an upgrade if and only if you have access to the exact chart with the same values file which was used to install nginx ingress earlier. You cannot just use a new chart to try to upgrade an existing installation which will cause all the earlier settings to be lost.\nI am assuming your nginx ingress helm chart is\nhttps://github.com/kubernetes/ingress-nginx/blob/main/charts/ingress-nginx\nthe `allowSnippetAnnotations` is the setting which needs to be turned on.\nOption 1:\nset `allowSnippetAnnotations=true` in values file under the controller\nOption 2:\ntry to set setting the value \"`--set controller.allowSnippetAnnotations=true`\" in your helm upgrade command.\nPlease share more details on how your nginx ingress controller was installed if you need more help as my answer is on the assumptions I have mentioned above.",
      "question_score": 11,
      "answer_score": 13,
      "created_at": "2023-10-11T18:32:54",
      "url": "https://stackoverflow.com/questions/77274854/ingress-controller-does-not-allow-snippets"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "helm",
      "question_id": 65790905,
      "title": "nil pointer evaluating interface when installing a helm chart",
      "problem": "I'm trying to install a chart to my cluster but I'm getting a error\n```\n`Error: template: go-api/templates/deployment.yaml:18:24: executing \"go-api/templates/deployment.yaml\" \nat : nil pointer evaluating interface {}.name\n`\n```\nHowever I executed the same commands for another 2 charts and it worked fine.\nThe template file I'm using is this:\n```\n`apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: {{ .Values.namespace}}\n  labels: {{- include \"chart.labels\" . | nindent 4 }}\n  name: {{ .Values.deployment.name}}\nspec:\n  replicas: {{ .Values.deployment.replicas}}\n  selector:\n    matchLabels: {{ include \"chart.matchLabels\" . | nindent 6 }}\n  template:\n    metadata:\n      labels: \n        app.kubernetes.io/name: {{ template \"chart.name\" . }}\n        app.kubernetes.io/instance: {{ .Release.Name }}\n    spec:\n      containers:\n      - name: {{ .Values.deployment.container.name }}\n        image: {{ .Values.deployment.container.image }}\n        imagePullPolicy: Never\n        ports:\n          - containerPort: {{ .Values.deployment.container.port }}\n\n`\n```",
      "solution": "This can happen if the Helm values you're using to install don't have that particular block:\n`namespace: default\ndeployment:\n  name: a-name\n  replicas: 1\n  # but no container:\n`\nTo avoid this specific error in the template code, it's useful to pick out the parent dictionary into a variable; then if the parent is totally absent, you can decide what to do about it.  This technique is a little more useful if there are optional fields or sensible defaults:\n`{{- $deployment := .Values.deployment | default dict }}\nmetadata:\n  name: {{ $deployment.name | default (include \"chart.fullname\" .) }}\nspec:\n{{- if $deployment.replicas }}\n  replicas: {{ $deployment.replicas }}\n{{- end }}\n`\nIf you really can't work without the value, Helm has an undocumented `required` function that can print a more specific error message.\n`{{- $deployment := .Values.deployment | required \"deployment configuration is required\" }}\n`\n(My experience has been that `required` values are somewhat frustrating as an end user, particularly if you're trying to run someone else's chart, and I would try to avoid this if possible.)\nGiven what you show, it's also possible you're making the chart too configurable.  The container name, for example, is mostly a detail that only appears if you have a multi-container pod (or are using Istio); the container port is a fixed attribute of the image you're running.  You can safely fix these values in the Helm template file, and then it's reasonable to provide defaults for things like the replica count or image name (consider setting the repository name, image name, and tag as separate variables).\n`{{- $deployment := .Values.deployment | default dict }}\n{{- $registry := $deployment.registry | default \"docker.io\" }}\n{{- $image := $deployment.image | default \"my/image\" }}\n{{- $tag := $deployment.tag | default \"latest\" }}\ncontainers:\n  - name: app # fixed\n    image: {{ printf \"%s/%s:%s\" $registry $image $tag }}\n{{- with .Values.imagePullPolicy }}\n    imagePullPolicy: {{ . }}\n{{- end }}\n    ports:\n      - name: http\n        containerPort: 3000 # fixed\n`",
      "question_score": 11,
      "answer_score": 15,
      "created_at": "2021-01-19T12:47:19",
      "url": "https://stackoverflow.com/questions/65790905/nil-pointer-evaluating-interface-when-installing-a-helm-chart"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "helm",
      "question_id": 68272235,
      "title": "How to set ingress.hosts[0].host in Helm chart via --set command line argument",
      "problem": "In a Helm chart, when trying to set the \"ingress.hosts[0].host\" via the --set command line argument, the \"paths\" array values specified in the values.yaml file do not get added to the final output.\nI want to override the \"ingress.hosts[0].host\" key for deployment to different DNS zones on the command line. If I add the host to the values.yaml file, it does generate the correct ingress, but this would mean having different values.yaml files instead of re-using the same one, but with a different DNS zone.\ningress.yaml Helm template\n```\n`rules:\n    {{- range .Values.ingress.hosts }}\n    - host: {{ .host | quote }}\n      http:\n        paths:\n          {{- range .paths }}\n          - path: {{ .path }}\n            {{- if and .pathType (semverCompare \">=1.18-0\" $.Capabilities.KubeVersion.GitVersion) }}\n            pathType: {{ .pathType }}\n            {{- end }}\n            backend:\n              {{- if semverCompare \">=1.19-0\" $.Capabilities.KubeVersion.GitVersion }}\n              service:\n                name: {{ $fullName }}\n                port:\n                  number: {{ $svcPort }}\n              {{- else }}\n              serviceName: {{ $fullName }}\n              servicePort: {{ $svcPort }}\n              {{- end }}\n          {{- end }}\n    {{- end }}\n`\n```\nvalues.yaml (Notice the single \"path\" object that should get added to the deployment.yaml file).\n```\n`ingress:\n  enabled: true\n  className: \"\"\n  annotations:\n    kubernetes.io/ingress.class: addon-http-application-routing\n  hosts:\n  - host: \n    paths:\n    - path: \"/\"\n      pathType: Prefix\n`\n```\ncommand line\n```\n`helm template ./deploy/vehicleregistrationservice --set ingress.hosts[0].host=vehicleregistrationservice.aksapp.io --debug\n`\n```\nInvalid deployment file (notice how the \"paths\" array is empty)\n```\n`kind: Ingress\nmetadata:\n  name: vehicleregistrationservice\n  labels:\n    helm.sh/chart: vehicleregistrationservice-0.1.0\n    app.kubernetes.io/name: vehicleregistrationservice\n    app.kubernetes.io/instance: RELEASE-NAME\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    kubernetes.io/ingress.class: addon-http-application-routing\nspec:\n  rules:\n    - host: \"vehicleregistrationservice.aksapp.io\"\n      http:\n        paths:\n`\n```\nThe deployment file is supposed to look like:\n```\n`kind: Ingress\nmetadata:\n  name: vehicleregistrationservice\n  annotations:\n    kubernetes.io/ingress.class: addon-http-application-routing\nspec:\n  rules:\n  - host: vehicleregistrationservice.aksapp.io\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service: \n            name: vehicleregistrationservice\n            port: \n              number: 80\n`\n```",
      "solution": "Just had the same problem.\nTry something like `--set \"ingress.hosts[0].host=yourhost.com,ingress.hosts[0].paths[0].path=/\"`",
      "question_score": 11,
      "answer_score": 13,
      "created_at": "2021-07-06T16:16:30",
      "url": "https://stackoverflow.com/questions/68272235/how-to-set-ingress-hosts0-host-in-helm-chart-via-set-command-line-argument"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "helm",
      "question_id": 70908774,
      "title": "Nginx-ingress-controller fails to start after AKS upgrade to v1.22",
      "problem": "We performed our kubernetes cluster upgrade from v1.21 to v1.22. After this operation we discovered that our nginx-ingress-controller deployment\u2019s pods are failing to start with the following error message:\n`pkg/mod/k8s.io/client-go@v0.18.5/tools/cache/reflector.go:125: Failed to list *v1beta1.Ingress: the server could not find the requested resource`\nWe have found out that this issue is tracked over here: https://github.com/bitnami/charts/issues/7264\nBecause azure doesn't let to downgrade the cluster back to the 1.21 could you please help us fixing the nginx-ingress-controller deployment? Could you please be specific with what should be done and from where (local machine or azure cli, etc) as we are not very familiar with `helm`.\nThis is our deployment current yaml:\n```\n`kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: nginx-ingress-controller\n  namespace: ingress\n  uid: 575c7699-1fd5-413e-a81d-b183f8822324\n  resourceVersion: '166482672'\n  generation: 16\n  creationTimestamp: '2020-10-10T10:20:07Z'\n  labels:\n    app: nginx-ingress\n    app.kubernetes.io/component: controller\n    app.kubernetes.io/managed-by: Helm\n    chart: nginx-ingress-1.41.1\n    heritage: Helm\n    release: nginx-ingress\n  annotations:\n    deployment.kubernetes.io/revision: '2'\n    meta.helm.sh/release-name: nginx-ingress\n    meta.helm.sh/release-namespace: ingress\n  managedFields:\n    - manager: kube-controller-manager\n      operation: Update\n      apiVersion: apps/v1\n      fieldsType: FieldsV1\n      fieldsV1:\n        f:spec:\n          f:replicas: {}\n      subresource: scale\n    - manager: Go-http-client\n      operation: Update\n      apiVersion: apps/v1\n      time: '2020-10-10T10:20:07Z'\n      fieldsType: FieldsV1\n      fieldsV1:\n        f:metadata:\n          f:annotations:\n            .: {}\n            f:meta.helm.sh/release-name: {}\n            f:meta.helm.sh/release-namespace: {}\n          f:labels:\n            .: {}\n            f:app: {}\n            f:app.kubernetes.io/component: {}\n            f:app.kubernetes.io/managed-by: {}\n            f:chart: {}\n            f:heritage: {}\n            f:release: {}\n        f:spec:\n          f:progressDeadlineSeconds: {}\n          f:revisionHistoryLimit: {}\n          f:selector: {}\n          f:strategy:\n            f:rollingUpdate:\n              .: {}\n              f:maxSurge: {}\n              f:maxUnavailable: {}\n            f:type: {}\n          f:template:\n            f:metadata:\n              f:labels:\n                .: {}\n                f:app: {}\n                f:app.kubernetes.io/component: {}\n                f:component: {}\n                f:release: {}\n            f:spec:\n              f:containers:\n                k:{\"name\":\"nginx-ingress-controller\"}:\n                  .: {}\n                  f:args: {}\n                  f:env:\n                    .: {}\n                    k:{\"name\":\"POD_NAME\"}:\n                      .: {}\n                      f:name: {}\n                      f:valueFrom:\n                        .: {}\n                        f:fieldRef: {}\n                    k:{\"name\":\"POD_NAMESPACE\"}:\n                      .: {}\n                      f:name: {}\n                      f:valueFrom:\n                        .: {}\n                        f:fieldRef: {}\n                  f:image: {}\n                  f:imagePullPolicy: {}\n                  f:livenessProbe:\n                    .: {}\n                    f:failureThreshold: {}\n                    f:httpGet:\n                      .: {}\n                      f:path: {}\n                      f:port: {}\n                      f:scheme: {}\n                    f:initialDelaySeconds: {}\n                    f:periodSeconds: {}\n                    f:successThreshold: {}\n                    f:timeoutSeconds: {}\n                  f:name: {}\n                  f:ports:\n                    .: {}\n                    k:{\"containerPort\":80,\"protocol\":\"TCP\"}:\n                      .: {}\n                      f:containerPort: {}\n                      f:name: {}\n                      f:protocol: {}\n                    k:{\"containerPort\":443,\"protocol\":\"TCP\"}:\n                      .: {}\n                      f:containerPort: {}\n                      f:name: {}\n                      f:protocol: {}\n                  f:readinessProbe:\n                    .: {}\n                    f:failureThreshold: {}\n                    f:httpGet:\n                      .: {}\n                      f:path: {}\n                      f:port: {}\n                      f:scheme: {}\n                    f:initialDelaySeconds: {}\n                    f:periodSeconds: {}\n                    f:successThreshold: {}\n                    f:timeoutSeconds: {}\n                  f:resources:\n                    .: {}\n                    f:limits: {}\n                    f:requests: {}\n                  f:securityContext:\n                    .: {}\n                    f:allowPrivilegeEscalation: {}\n                    f:capabilities:\n                      .: {}\n                      f:add: {}\n                      f:drop: {}\n                    f:runAsUser: {}\n                  f:terminationMessagePath: {}\n                  f:terminationMessagePolicy: {}\n              f:dnsPolicy: {}\n              f:restartPolicy: {}\n              f:schedulerName: {}\n              f:securityContext: {}\n              f:serviceAccount: {}\n              f:serviceAccountName: {}\n              f:terminationGracePeriodSeconds: {}\n    - manager: kube-controller-manager\n      operation: Update\n      apiVersion: apps/v1\n      time: '2022-01-24T01:23:22Z'\n      fieldsType: FieldsV1\n      fieldsV1:\n        f:status:\n          f:conditions:\n            .: {}\n            k:{\"type\":\"Available\"}:\n              .: {}\n              f:type: {}\n            k:{\"type\":\"Progressing\"}:\n              .: {}\n              f:type: {}\n    - manager: Mozilla\n      operation: Update\n      apiVersion: apps/v1\n      time: '2022-01-28T23:18:41Z'\n      fieldsType: FieldsV1\n      fieldsV1:\n        f:spec:\n          f:template:\n            f:spec:\n              f:containers:\n                k:{\"name\":\"nginx-ingress-controller\"}:\n                  f:resources:\n                    f:limits:\n                      f:cpu: {}\n                      f:memory: {}\n                    f:requests:\n                      f:cpu: {}\n                      f:memory: {}\n    - manager: kube-controller-manager\n      operation: Update\n      apiVersion: apps/v1\n      time: '2022-01-28T23:29:49Z'\n      fieldsType: FieldsV1\n      fieldsV1:\n        f:metadata:\n          f:annotations:\n            f:deployment.kubernetes.io/revision: {}\n        f:status:\n          f:conditions:\n            k:{\"type\":\"Available\"}:\n              f:lastTransitionTime: {}\n              f:lastUpdateTime: {}\n              f:message: {}\n              f:reason: {}\n              f:status: {}\n            k:{\"type\":\"Progressing\"}:\n              f:lastTransitionTime: {}\n              f:lastUpdateTime: {}\n              f:message: {}\n              f:reason: {}\n              f:status: {}\n          f:observedGeneration: {}\n          f:replicas: {}\n          f:unavailableReplicas: {}\n          f:updatedReplicas: {}\n      subresource: status\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: nginx-ingress\n      app.kubernetes.io/component: controller\n      release: nginx-ingress\n  template:\n    metadata:\n      creationTimestamp: null\n      labels:\n        app: nginx-ingress\n        app.kubernetes.io/component: controller\n        component: controller\n        release: nginx-ingress\n    spec:\n      containers:\n        - name: nginx-ingress-controller\n          image: us.gcr.io/k8s-artifacts-prod/ingress-nginx/controller:v0.34.1\n          args:\n            - /nginx-ingress-controller\n            - '--default-backend-service=ingress/nginx-ingress-default-backend'\n            - '--election-id=ingress-controller-leader'\n            - '--ingress-class=nginx'\n            - '--configmap=ingress/nginx-ingress-controller'\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n            - name: https\n              containerPort: 443\n              protocol: TCP\n          env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  apiVersion: v1\n                  fieldPath: metadata.name\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  apiVersion: v1\n                  fieldPath: metadata.namespace\n          resources:\n            limits:\n              cpu: 300m\n              memory: 512Mi\n            requests:\n              cpu: 200m\n              memory: 256Mi\n          livenessProbe:\n            httpGet:\n              path: /healthz\n              port: 10254\n              scheme: HTTP\n            initialDelaySeconds: 10\n            timeoutSeconds: 1\n            periodSeconds: 10\n            successThreshold: 1\n            failureThreshold: 3\n          readinessProbe:\n            httpGet:\n              path: /healthz\n              port: 10254\n              scheme: HTTP\n            initialDelaySeconds: 10\n            timeoutSeconds: 1\n            periodSeconds: 10\n            successThreshold: 1\n            failureThreshold: 3\n          terminationMessagePath: /dev/termination-log\n          terminationMessagePolicy: File\n          imagePullPolicy: IfNotPresent\n          securityContext:\n            capabilities:\n              add:\n                - NET_BIND_SERVICE\n              drop:\n                - ALL\n            runAsUser: 101\n            allowPrivilegeEscalation: true\n      restartPolicy: Always\n      terminationGracePeriodSeconds: 60\n      dnsPolicy: ClusterFirst\n      serviceAccountName: nginx-ingress\n      serviceAccount: nginx-ingress\n      securityContext: {}\n      schedulerName: default-scheduler\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n  revisionHistoryLimit: 10\n  progressDeadlineSeconds: 600\nstatus:\n  observedGeneration: 16\n  replicas: 3\n  updatedReplicas: 2\n  unavailableReplicas: 3\n  conditions:\n    - type: Available\n      status: 'False'\n      lastUpdateTime: '2022-01-28T22:58:07Z'\n      lastTransitionTime: '2022-01-28T22:58:07Z'\n      reason: MinimumReplicasUnavailable\n      message: Deployment does not have minimum availability.\n    - type: Progressing\n      status: 'False'\n      lastUpdateTime: '2022-01-28T23:29:49Z'\n      lastTransitionTime: '2022-01-28T23:29:49Z'\n      reason: ProgressDeadlineExceeded\n      message: >-\n        ReplicaSet \"nginx-ingress-controller-59d9f94677\" has timed out\n        progressing.\n`\n```",
      "solution": "Kubernetes 1.22 is supported only with NGINX Ingress Controller 1.0.0 and higher = https://github.com/kubernetes/ingress-nginx#supported-versions-table\nYou need tu upgrade your `nginx-ingress-controller` Bitnami Helm Chart to Version 9.0.0 in `Chart.yaml`. Then run a `helm upgrade nginx-ingress-controller bitnami/nginx-ingress-controller`.\nYou should also regularly update specially your ingress controller, as the version v0.34.1 is very very old bcs the ingress is normally the only entry appoint from outside to your cluster.",
      "question_score": 9,
      "answer_score": 12,
      "created_at": "2022-01-29T20:01:20",
      "url": "https://stackoverflow.com/questions/70908774/nginx-ingress-controller-fails-to-start-after-aks-upgrade-to-v1-22"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "helm",
      "question_id": 68618761,
      "title": "Helm chart volumes and volumeMounts in deployment file",
      "problem": "I can't make my chart use my `volumes`, and `volumeMounts` values. In my values.yaml file I have something like this:\n```\n`volumes:\n- name: docker1\n  hostPath:\n    path: /var/\n- name: docker2\n  hostPath:\n    path: /usr/\n- name: docker3\n  hostPath:\n    path: /opt/\n    \nvolumeMounts:\n- name: docker1\n  mountPath: /var/\n- name: docker2\n  mountPath: /usr/\n- name: docker3\n  mountPath: /opt/\n`\n```\nIn my _deployment.tpl file I have something like this:\n```\n`apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: {{ include \"fullname\" . }}\n  namespace: {{ .Values.namespace }}\n  labels:\n    {{- include \"labels\" . | nindent 4 }}\nspec:\n  replicas: {{ .Values.replicaCount }}\n  revisionHistoryLimit: {{ .Values.revisionHistory | default 2 }}\n  selector:\n    matchLabels:\n      {{- include \"selectorLabels\" . | nindent 6 }}\n  template:\n    metadata:\n      annotations:\n        {{- toYaml .Values.podAnnotations | nindent 8 }}\n      labels:\n        {{- include \"labels\" . | nindent 8 }}\n    spec:\n      imagePullSecrets:\n        {{- toYaml .Values.imagePullSecrets | nindent 8 }}\n      containers:\n        - name: {{ .Chart.Name }}\n          image: \"{{ .Values.image.repository }}:{{ .Values.image.tag | default .Chart.AppVersion }}\"\n          imagePullPolicy: {{ .Values.image.pullPolicy }}\n          ports:\n              {{- toYaml .Values.image.ports | nindent 10 }}\n          env:\n              {{- toYaml .Values.env | nindent 10 }}\n          volumeMounts:\n            - name: {{- toYaml .Values.volumeMounts | default \"\" | nindent 10 }} \n          volumes:\n            - name: {{- toYaml .Values.volumes | default \"\" | nindent 10 }}     \n      nodeSelector:\n        {{- toYaml .Values.nodeSelector | nindent 8 }}\n      tolerations:\n        {{- toYaml .Values.tolerations | nindent 8 }}\n{{- end }}\n \n`\n```\nI tried to mount volumes and volumeMounts the same way I do with env variables (they work) but sadly it doesn't work.",
      "solution": "There is a problem with the indentation of your code.\nVolumes should be at the same indentation level as containers.\nAs follow:\n```\n`    spec:\n      imagePullSecrets:\n        {{- toYaml .Values.imagePullSecrets | nindent 8 }}\n      containers:\n        - name: {{ .Chart.Name }}\n          image: \"{{ .Values.image.repository }}:{{ .Values.image.tag | default .Chart.AppVersion }}\"\n          imagePullPolicy: {{ .Values.image.pullPolicy }}\n          ports:\n            {{- toYaml .Values.image.ports | nindent 12 }}\n          env:\n            {{- toYaml .Values.env | nindent 12 }}\n          volumeMounts:\n            {{- toYaml .Values.volumeMounts | default \"\" | nindent 12 }} \n      volumes:\n        {{- toYaml .Values.volumes | default \"\" | nindent 8 }}     \n      nodeSelector:\n        {{- toYaml .Values.nodeSelector | nindent 8 }}\n      tolerations:\n        {{- toYaml .Values.tolerations | nindent 8 }}\n`\n```\nIf you want to debug the template, you can refer to the official helm document operation.\nhelm debug",
      "question_score": 9,
      "answer_score": 7,
      "created_at": "2021-08-02T10:47:28",
      "url": "https://stackoverflow.com/questions/68618761/helm-chart-volumes-and-volumemounts-in-deployment-file"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "helm",
      "question_id": 73070417,
      "title": "How to pass values from the terraform to the Helm chart values.yaml file?",
      "problem": "I am creating ingress-nginx controller using the Helm chart in Terraform. I do have a values.yaml file where I can add the customized information, but I need to pass the SSL Certificate value from the Terraform resource so how can I do that? I am using the below code but getting an error.\n```\n`resource \"aws_acm_certificate\" \"ui_cert\" {\n  domain_name       = var.DOMAIN_NAME\n  validation_method = \"DNS\"\n\n  tags = {\n    Environment = var.ENVIRONMENT \n  }\n\n  lifecycle {\n    create_before_destroy = true\n  }\n\n}\n\n`\n```\n```\n`resource \"helm_release\" \"nginix_ingress\" {\n\n  depends_on = [module.eks, kubernetes_namespace.nginix_ingress,aws_acm_certificate.ui_cert]\n\n  name       = \"ingress-nginx\"\n  repository = \"https://kubernetes.github.io/ingress-nginx\"\n  chart      = \"ingress-nginx\"\n  namespace  = var.NGINX_INGRESS_NAMESPACE\n   \n  values = [templatefile(\"values.yaml\", {\n    controller.service.beta.kubernetes.io/aws-load-balancer-internal = aws_acm_certificate.ui_cert.name,\n  })]\n}\n`\n```\nI am getting the following error:\n```\n` Error: Reference to undeclared resource\n\u2502\n\u2502   on ui.tf line 27, in resource \"helm_release\" \"nginix_ingress\":\n\u2502   27:     controller.service.beta.kubernetes.io/aws-load-balancer-internal = aws_acm_certificate.ui_cert.name,\n\u2502\n\u2502 A managed resource \"controller\" \"service\" has not been declared in the root\n\u2502 module.\n\u2575\n\u2577\n\u2502 Error: Invalid reference\n\u2502\n\u2502   on ui.tf line 27, in resource \"helm_release\" \"nginix_ingress\":\n\u2502   27:     controller.service.beta.kubernetes.io/aws-load-balancer-internal = aws_acm_certificate.ui_cert.name,\n\u2502\n\u2502 A reference to a resource type must be followed by at least one attribute\n\u2502 access, specifying the resource name.\n\u2575\n\u2577\n\u2502 Error: Unsupported attribute\n\u2502\n\u2502   on ui.tf line 27, in resource \"helm_release\" \"nginix_ingress\":\n\u2502   27:     controller.service.beta.kubernetes.io/aws-load-balancer-internal = aws_acm_certificate.ui_cert.name,\n\u2502\n\u2502 This object has no argument, nested block, or exported attribute named\n\u2502 \"name\".\n\u2575\n\u2577\n\u2502 Error: Reference to undeclared resource\n\u2502\n\u2502   on ui.tf line 27, in resource \"helm_release\" \"nginix_ingress\":\n\u2502   27:     controller.service.beta.kubernetes.io/aws-load-balancer-internal = aws_acm_certificate.ui_cert.name,\n\u2502\n\u2502 A managed resource \"controller\" \"service\" has not been declared in the root\n\u2502 module.\n\u2575\n\u2577\n\u2502 Error: Invalid reference\n\u2502\n\u2502   on ui.tf line 27, in resource \"helm_release\" \"nginix_ingress\":\n\u2502   27:     controller.service.beta.kubernetes.io/aws-load-balancer-internal = aws_acm_certificate.ui_cert.name,\n\u2502\n\u2502 A reference to a resource type must be followed by at least one attribute\n\u2502 access, specifying the resource name.\n\u2575\n\u2577\n\u2502 Error: Unsupported attribute\n\u2502\n\u2502   on ui.tf line 27, in resource \"helm_release\" \"nginix_ingress\":\n\u2502   27:     controller.service.beta.kubernetes.io/aws-load-balancer-internal = aws_acm_certificate.ui_cert.name,\n\u2502\n\u2502 This object has no argument, nested block, or exported attribute named\n\u2502 \"name\".\n`\n```\n```\n`controller:\n  name: controller\n  image:\n    chroot: false\n    registry: registry.k8s.io\n    image: ingress-nginx/controller\n    tag: \"v1.3.0\"\n    digest: sha256:d1707ca76d3b044ab8a28277a2466a02100ee9f58a86af1535a3edf9323ea1b5\n    digestChroot: sha256:0fcb91216a22aae43b374fc2e6a03b8afe9e8c78cbf07a09d75636dc4ea3c191\n    pullPolicy: IfNotPresent\n    runAsUser: 101\n    allowPrivilegeEscalation: true\n  containerName: controller\n  containerPort:\n    # http: 80\n    https: 443\n  config:\n    use-proxy-protocol: \"true\"\n  # -- Optionally customize the pod hostname.\n  hostname: {}\n\n  # -- Process IngressClass per name (additionally as per spec.controller).\n  ingressClassByName: false\n\n  # -- This configuration defines if Ingress Controller should allow users to set\n  # their own *-snippet annotations, otherwise this is forbidden / dropped\n  # when users add those annotations.\n  # Global snippets in ConfigMap are still respected\n  allowSnippetAnnotations: true\n\n  ## This section refers to the creation of the IngressClass resource\n  ## IngressClass resources are supported since k8s >= 1.18 and required since k8s >= 1.19\n  ingressClassResource:\n    # -- Name of the ingressClass\n    name: nginx\n    # -- Is this ingressClass enabled or not\n    enabled: true\n    # -- Is this the default ingressClass for the cluster\n    default: false\n    # -- Controller-value of the controller that is processing this ingressClass\n    controllerValue: \"k8s.io/ingress-nginx\"\n\n    # -- Parameters is a link to a custom resource containing additional\n    # configuration for the controller. This is optional if the controller\n    # does not require extra parameters.\n    parameters: {}\n\n  # -- For backwards compatibility with ingress.class annotation, use ingressClass.\n  # Algorithm is as follows, first ingressClassName is considered, if not present, controller looks for ingress.class annotation\n  ingressClass: nginx\n\n  # -- Labels to add to the pod container metadata\n  podLabels: {}\n  #  key: value\n\n  # -- Security Context policies for controller pods\n\n  # -- Allows customization of the source of the IP address or FQDN to report\n  # in the ingress status field. By default, it reads the information provided\n  # by the service. If disable, the status field reports the IP address of the\n  # node or nodes where an ingress controller pod is running.\n  publishService:\n    # -- Enable 'publishService' or not\n    enabled: true\n    # -- Allows overriding of the publish service to bind to\n    # Must be /\n    pathOverride: \"\"\n\n  tcp:\n    # -- Allows customization of the tcp-services-configmap; defaults to $(POD_NAMESPACE)\n    configMapNamespace: \"\"\n    # -- Annotations to be added to the tcp config configmap\n    annotations: {}\n\n  udp:\n    # -- Allows customization of the udp-services-configmap; defaults to $(POD_NAMESPACE)\n    configMapNamespace: \"\"\n    # -- Annotations to be added to the udp config configmap\n    annotations: {}\n\n  # -- Use a `DaemonSet` or `Deployment`\n  kind: Deployment\n\n  # -- Annotations to be added to the controller Deployment or DaemonSet\n  ##\n  annotations: {}\n  #  keel.sh/pollSchedule: \"@every 60m\"\n\n  # -- Labels to be added to the controller Deployment or DaemonSet and other resources that do not have option to specify labels\n  ##\n  labels: {}\n  #  keel.sh/policy: patch\n  #  keel.sh/trigger: poll\n\n  # -- The update strategy to apply to the Deployment or DaemonSet\n  ##\n  updateStrategy: {}\n  #  rollingUpdate:\n  #    maxUnavailable: 1\n  #  type: RollingUpdate\n\n  # -- `minReadySeconds` to avoid killing pods before we are ready\n  ##\n  minReadySeconds: 0\n\n  # -- Topology spread constraints rely on node labels to identify the topology domain(s) that each Node is in.\n  ## Ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/\n  ##\n  topologySpreadConstraints: []\n    # - maxSkew: 1\n    #   topologyKey: topology.kubernetes.io/zone\n    #   whenUnsatisfiable: DoNotSchedule\n    #   labelSelector:\n    #     matchLabels:\n    #       app.kubernetes.io/instance: ingress-nginx-internal\n\n  # -- `terminationGracePeriodSeconds` to avoid killing pods before we are ready\n  ## wait up to five minutes for the drain of connections\n  ##\n  terminationGracePeriodSeconds: 300\n\n  # -- Node labels for controller pod assignment\n  ## Ref: https://kubernetes.io/docs/user-guide/node-selection/\n  ##\n  nodeSelector:\n    kubernetes.io/os: linux\n\n  ## Liveness and readiness probe values\n  ## Ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#container-probes\n  ##\n  ## startupProbe:\n  ##   httpGet:\n  ##     # should match container.healthCheckPath\n  ##     path: \"/healthz\"\n  ##     port: 10254\n  ##     scheme: HTTP\n  ##   initialDelaySeconds: 5\n  ##   periodSeconds: 5\n  ##   timeoutSeconds: 2\n  ##   successThreshold: 1\n  ##   failureThreshold: 5\n  livenessProbe:\n    httpGet:\n      # should match container.healthCheckPath\n      path: \"/healthz\"\n      port: 10254\n      scheme: HTTP\n    initialDelaySeconds: 10\n    periodSeconds: 10\n    timeoutSeconds: 1\n    successThreshold: 1\n    failureThreshold: 5\n  readinessProbe:\n    httpGet:\n      # should match container.healthCheckPath\n      path: \"/healthz\"\n      port: 10254\n      scheme: HTTP\n    initialDelaySeconds: 10\n    periodSeconds: 10\n    timeoutSeconds: 1\n    successThreshold: 1\n    failureThreshold: 3\n\n  # -- Path of the health check endpoint. All requests received on the port defined by\n  # the healthz-port parameter are forwarded internally to this path.\n  healthCheckPath: \"/healthz\"\n\n  # -- Address to bind the health check endpoint.\n  # It is better to set this option to the internal node address\n  # if the ingress nginx controller is running in the `hostNetwork: true` mode.\n  healthCheckHost: \"\"\n\n  # -- Annotations to be added to controller pods\n  ##\n  podAnnotations: {}\n\n  replicaCount: 1\n\n  minAvailable: 1\n\n  ## Define requests resources to avoid probe issues due to CPU utilization in busy nodes\n  ## ref: https://github.com/kubernetes/ingress-nginx/issues/4735#issuecomment-551204903\n  ## Ideally, there should be no limits.\n  ## https://engineering.indeedblog.com/blog/2019/12/cpu-throttling-regression-fix/\n  resources:\n  ##  limits:\n  ##    cpu: 100m\n  ##    memory: 90Mi\n    requests:\n      cpu: 100m\n      memory: 90Mi\n\n  # Mutually exclusive with keda autoscaling\n  autoscaling:\n    enabled: false\n    minReplicas: 1\n    maxReplicas: 11\n    targetCPUUtilizationPercentage: 50\n    targetMemoryUtilizationPercentage: 50\n    behavior: {}\n      # scaleDown:\n      #   stabilizationWindowSeconds: 300\n      #  policies:\n      #   - type: Pods\n      #     value: 1\n      #     periodSeconds: 180\n      # scaleUp:\n      #   stabilizationWindowSeconds: 300\n      #   policies:\n      #   - type: Pods\n      #     value: 2\n      #     periodSeconds: 60\n\n  autoscalingTemplate: []\n  # Custom or additional autoscaling metrics\n  # ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#support-for-custom-metrics\n  # - type: Pods\n  #   pods:\n  #     metric:\n  #       name: nginx_ingress_controller_nginx_process_requests_total\n  #     target:\n  #       type: AverageValue\n  #       averageValue: 10000m\n\n  # Mutually exclusive with hpa autoscaling\n  keda:\n    apiVersion: \"keda.sh/v1alpha1\"\n    ## apiVersion changes with keda 1.x vs 2.x\n    ## 2.x = keda.sh/v1alpha1\n    ## 1.x = keda.k8s.io/v1alpha1\n    enabled: false\n    minReplicas: 1\n    maxReplicas: 11\n    pollingInterval: 30\n    cooldownPeriod: 300\n    restoreToOriginalReplicaCount: false\n    scaledObject:\n      annotations: {}\n      # Custom annotations for ScaledObject resource\n      #  annotations:\n      # key: value\n    triggers: []\n #     - type: prometheus\n #       metadata:\n #         serverAddress: http://:9090\n #         metricName: http_requests_total\n #         threshold: '100'\n #         query: sum(rate(http_requests_total{deployment=\"my-deployment\"}[2m]))\n\n    behavior: {}\n #     scaleDown:\n #       stabilizationWindowSeconds: 300\n #       policies:\n #       - type: Pods\n #         value: 1\n #         periodSeconds: 180\n #     scaleUp:\n #       stabilizationWindowSeconds: 300\n #       policies:\n #       - type: Pods\n #         value: 2\n #         periodSeconds: 60\n\n  # -- Enable mimalloc as a drop-in replacement for malloc.\n  ## ref: https://github.com/microsoft/mimalloc\n  ##\n  enableMimalloc: true\n\n  ## Override NGINX template\n  customTemplate:\n    configMapName: \"\"\n    configMapKey: \"\"\n\n  service:\n    enabled: true\n    annotations:\n      service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\"\n      service.beta.kubernetes.io/aws-load-balancer-proxy-protocol: \"*\"\n      service.beta.kubernetes.io/aws-load-balancer-ssl-cert: \"ssl-cert\"\n      service.beta.kubernetes.io/aws-load-balancer-backend-protocol: \"http\"\n      service.beta.kubernetes.io/aws-load-balancer-ssl-ports: \"443\"\n`\n```\nCan someone plz tell me how I can make it work?",
      "solution": "The `templatefile` built-in function is used to pass the values to variables defined in the templated file. So for example, in your case, you would define a variable in the template file called e.g., `ssl_cert`. Then, when calling the `templatefile` function, you would pass it the value provided by the ACM resource:\n```\n`  values = [templatefile(\"values.yaml\", {\n    ssl_cert = aws_acm_certificate.ui_cert.name\n  })]\n`\n```\nThe `ssl_cert` variable inside of the `templatefile` would be associated with the annotation `controller.service.beta.kubernetes.io/aws-load-balancer-ssl-cert`. Based on the YML file, the variable should be added in the last section of the file:\n`  service:\n    enabled: true\n    annotations:\n      service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\"\n      service.beta.kubernetes.io/aws-load-balancer-proxy-protocol: \"*\"\n      service.beta.kubernetes.io/aws-load-balancer-ssl-cert: \"${ssl_cert}\" # here is where the replacement will be made\n      service.beta.kubernetes.io/aws-load-balancer-backend-protocol: \"http\"\n      service.beta.kubernetes.io/aws-load-balancer-ssl-ports: \"443\"\n`\nThe `templatefile` function is really powerful, but I strongly suggest understanding how variables and variable substitution works when using it [1]. It is unaware of the substitution you want to make unless you have a placeholder variable both when calling the function and in the template file.\n\n[1] https://www.terraform.io/language/functions/templatefile",
      "question_score": 8,
      "answer_score": 18,
      "created_at": "2022-07-21T19:28:25",
      "url": "https://stackoverflow.com/questions/73070417/how-to-pass-values-from-the-terraform-to-the-helm-chart-values-yaml-file"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "helm",
      "question_id": 69368262,
      "title": "Error: unknown command &quot;chart&quot; for &quot;helm&quot; on github actions",
      "problem": "My CI deployments in github actions for helm/kubernetes have started failing with the following error:\n`Error: unknown command \"chart\" for \"helm\" on github actions`\nIn my `CI.yaml` file, I have the following helm commands:\n```\n`\necho *****************\necho SAVING HELM CHART\necho *****************\n# log in to aws and push the helm chart\naws ecr get-login-password --region us-west-2 | helm registry login --username AWS --password-stdin XXXX.amazonaws.com\nhelm chart save ./server-helm-chart/ XXXXX.amazonaws.com/test/helm:$helmChartVersion\necho *****************\necho PUSHING HELM CHART\necho *****************\nhelm chart push XXXX.amazonaws.com/test/helm:$helmChartVersion\n`\n```",
      "solution": "Helm has removed the `chart` functions, so they are now replaced by `push` and `package`.\n\nThese changes are listed here:\nhttps://github.com/helm/helm/releases/tag/v3.7.0\nSo I needed to rename my `Chart.yaml` to match the AWS registry name, and then my code became:\n```\n`echo *****************\necho SAVING HELM CHART\necho *****************\n# log in to aws and push the helm chart\n# aws ecr get-login-password --region us-west-2 | helm registry login --username AWS --password-stdin XXXX.amazonaws.com\n# export CHART_FILE=`helm package ./server-helm-chart/ | awk -F'[:]' '{gsub(/ /, \"\", $2); print $2}'`\necho *****************\necho PUSHING HELM CHART\necho *****************\n# helm push $CHART_FILE oci://XXXX.dkr.ecr.us-west-2.amazonaws.com/staging/helm:$helmChartVersion\n`\n```",
      "question_score": 8,
      "answer_score": 17,
      "created_at": "2021-09-28T22:44:15",
      "url": "https://stackoverflow.com/questions/69368262/error-unknown-command-chart-for-helm-on-github-actions"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "helm",
      "question_id": 69183331,
      "title": "What causes Helm chart template to throw &#39;unexpected EOF&#39;?",
      "problem": "I'm trying to add ingress to my nginx container.\nThe following ingress template gives me \"parse error (<>/ingress.yaml:71: unexpected EOF)\". I went through trying mark possible missing end statements, but even adding arbitrary end at the end of file didn't fix it. I am out of ideas as to what causes this EOF.\nSo the question is the general: What causes \"unexpected EOF\" in the file?\n`{{- if .Values.web.ingress.enabled }}\n\napiVersion: networking.k8s.io/v1beta1\nkind: Ingress\nmetadata:\n  name: {{ .Release.Name }}-proxy-ingress\n  labels:\n    tier: intelowl\n    component: proxy\n    release: {{ .Release.Name }}\n    chart: {{ .Chart.Name }}\n    heritage: {{ .Release.Service }}\n{{- with .Values.labels }}\n{{ toYaml . | indent 4 }}\n{{- end }} # {{- with .Values.labels }}\n\n{{- if .Values.web.ingress.annotations }}\n  annotations:\n{{- with .Values.web.ingress.annotations }}\n{{ toYaml . | indent 4 }}\n{{- end }} # {{- with .Values.web.ingress.annotations }}\n{{- end }} # {{- if .Values.web.ingress.annotations }}\n\nspec:\n\n{{- if .Values.web.ingress.tls.enabled }}\n  tls:\n    - hosts:\n        - {{ .Values.web.ingress.host }}\n      secretName: {{ .Values.web.ingress.tls.secretName }}\n{{- end }} # {{- if .Values.web.ingress.tls.enabled }}\n\n  rules:\n    - http:\n        paths:\n{{- range .Values.web.ingress.precedingPaths }}\n          - path: {{ .path }}\n            backend:\n              service:\n                name: {{ .serviceName }}\n                port: \n                  number: {{ .servicePort }}\n{{- end }} # {{- range .Values.web.ingress.precedingPaths }}\n\n          - backend:\n              service:\n                name: {{ .Release.Name }}-proxy\n                port: \n                  number: {{ ternary 443 80 .Values.web.ingress.tls.enabled }}\n{{- if .Values.web.ingress.path }}\n            path: {{ .Values.web.ingress.path }}\n{{- end }} # {{- if .Values.web.ingress.path }}\n\n{{- range .Values.web.ingress.succeedingPaths }}\n          - path: {{ .path }}\n            backend:\n              service:\n                name: {{ .serviceName }}\n                port: \n                  number: {{ .servicePort }}\n{{- end }} # {{- range .Values.web.ingress.succeedingPaths }}\n\n{{- if .Values.web.ingress.host }}\n      host: {{ .Values.web.ingress.host }}\n{{- end }} # {{- if .Values.web.ingress.host }}\n\n{{- end }} # {{- if .Values.web.ingress.enabled }}\n\n`",
      "solution": "Your file is generally structured like so:\n`{{- if .Values.someCondition }}\n...\n{{- end }} # {{- if .Values.someCondition }}\n`\nHowever, the Go `text/template` engine runs before any YAML parsing happens.  There is not a comment in this example; there is an `if` statement, the matching `end`, and an unterminated `if`.\nThe `text/template` language has its own `{{/* comment */}}` syntax, and in principle you could use this\n`{{- if .Values.someCondition }}\n...\n{{- end }}{{/* if .Values.someCondition */}}\n`\nBeyond this, the file you show seems to have the right number of `{{ end }}`s.\nI'd probably avoid this style myself.  Usually these condition blocks are fairly short; you could break the template into multiple `define` named templates if that helps.\n`metadata:\n  labels:\n    tier: intelowl\n    et: cetera\n{{- include \"more-labels\" . | indent 4 }}\n{{- include \"ingress-annotations\" . | indent 2 }}\n\n{{- define \"more-labels\" -}}\n{{ with .Values.labels }}{{ toYaml . }}{{ end }}\n{{- end -}}\n\n{{- define \"ingress-annotations\" -}}\n{{- with .Values.web.ingress.annotations }}\nannotations:\n{{ toYaml . | indent 2 }}\n{{- end -}}\n{{- end -}}\n`\nEspecially for labels you might find that some of the values will be reused in all of your objects, and so including a template in the `_helpers.tpl` file to produce the common values will reduce some of the repetitiveness.",
      "question_score": 8,
      "answer_score": 12,
      "created_at": "2021-09-14T21:07:54",
      "url": "https://stackoverflow.com/questions/69183331/what-causes-helm-chart-template-to-throw-unexpected-eof"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "helm",
      "question_id": 72956149,
      "title": "$ does not resolve to the root context from within a helm template",
      "problem": "I'm trying to iterate over `.Files.Glob` in a Helm template. I pass a string value into the template, so I've swapped over to using `$.Files.Glob` to refer back to the root context. However I get an error saying that there's no field \"Files\" on type string.\nThe error:\n```\n`error calling include: template: example/templates/_helpers.tpl:77:29: executing \"apiMounts\" at : can't evaluate field Files in type string\n`\n```\nThe template:\n```\n`{{- define \"apiMounts\" -}}\n{{- range $path, $_ := $.Files.Glob . }}\n{{- $name := (base $path) }}\n- name: specs\n  mountPath: {{ printf \"/etc/apis/%s\" $name }}\n  subPath: {{ sha256sum $name }}\n{{- end }}\n{{- end }}\n`\n```\nHow I'm using the template (deployment):\n```\n`          volumeMounts:\n            {{- include \"apiMounts\" \"common/**.json\" | indent 12 }}\n            {{- include \"apiMounts\" \"v1/**.json\" | indent 12 }}\n`\n```\nAccording to the Helm documentation, `$` should always refer back to the root context, but in this case it seems to still be referring to the string that I've passed to the template.\n\nHowever, there is one variable that is always global - `$` - this variable will always point to the root context. This can be very useful when you are looping in a range and you need to know the chart's release name.\n\nHow can I used `.Files.Glob` in this template, while still passing in the string value?",
      "solution": "The value you pass into the include function becomes the root context.\nYou can get the original root context inside the template function by passing in a list which includes it and your argument:\n```\n`{{- include \"apiMounts\" (list $ \"common/**.json\") | indent 12 }}\n`\n```\nInside your template function, restore the \"root\" context and grab your argument:\n```\n`{{- define \"apiMounts\" -}}\n{{- $ := index . 0 }}\n{{- $arg := index . 1 }}\n{{- range $path, $_ := $.Files.Glob $arg }}\n...\n`\n```",
      "question_score": 7,
      "answer_score": 18,
      "created_at": "2022-07-12T19:20:51",
      "url": "https://stackoverflow.com/questions/72956149/does-not-resolve-to-the-root-context-from-within-a-helm-template"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "helm",
      "question_id": 69471950,
      "title": "Problem creating another ingress-nginx/ingress-nginx via helm &quot;Error: rendered manifests contain a resource that already exists&quot;",
      "problem": "I keep getting this error or variations when I try to do as it asks:\n```\n`Error: rendered manifests contain a resource that already exists. Unable to continue with install: IngressClass \"nginx\" in namespace \"\" exists and cannot be imported into the current release: invalid ownership metadata; annotation validation error: key \"meta.helm.sh/release-name\" must equal \"new-ingress-nginx\": current value is \"old-ingress-nginx\"\n`\n```\nI'm using helm to install:\n```\n`helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx\nhelm repo update\n\nhelm install new-ingress-nginx ingress-nginx/ingress-nginx --set-string controller.podAnnotations.\"app\\.kubernetes\\.io/instance\"=\"new\"\n`\n```\nI've tried with and without the podAnnotations as I found a post mentioning to try that.\nI'm using google kubernetes engine and what I have done is merged all my api's under one load balancer/ingress-nginx but I would like to figure out the issue.",
      "solution": "I was able to solve my problem after reading through this on github\nSome changes were made and `--set controller.ingressClassResource.name=` is now used and the yaml file no longer looks like:\n```\n`apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: some-ingress\n  namespace: somenamespace\n  annotations:\n    kubernetes.io/ingress.class: \n    ...\nspec:\n  tls:\n  ...\n`\n```\nit looks like:\n```\n`apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: some-ingress\n  namespace: somenamespace\n  annotations:\n    ...\nspec:\n  ingressClassName: \n  tls:\n  ...\n`\n```",
      "question_score": 7,
      "answer_score": 10,
      "created_at": "2021-10-06T21:53:26",
      "url": "https://stackoverflow.com/questions/69471950/problem-creating-another-ingress-nginx-ingress-nginx-via-helm-error-rendered-m"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "helm",
      "question_id": 66875139,
      "title": "How to configure kube-prometheus-stack helm installation to scrape a Kubernetes service?",
      "problem": "I have installed kube-prometheus-stack as a dependency in my helm chart on a local docker for Mac Kubernetes cluster v1.19.7. I can view the default prometheus targets provided by the kube-prometheus-stack.\nI have a python flask service that provides metrics which I can view successfully in the kubernetes cluster using `kubectl port forward`.\nHowever, I am unable to get these metrics displayed on the prometheus targets web interface.\nThe kube-prometheus-stack documentation states that Prometheus.io/scrape does not support annotation-based discovery of services. Instead the the reader is referred to the concept of `ServiceMonitors` and `PodMonitors`.\nSo, I have configured my service as follows:\n`---\nkind:                       Service\napiVersion:                 v1  \nmetadata:\n  name:                     flask-api-service                    \n  labels:\n    app:                    flask-api-service\nspec:\n  ports:\n    - protocol:             TCP \n      port:                 4444\n      targetPort:           4444\n      name:                 web \n  selector:\n    app:                    flask-api-service                    \n    tier:                   backend \n  type:                     ClusterIP\n---\napiVersion:                 monitoring.coreos.com/v1\nkind:                       ServiceMonitor\nmetadata:\n  name:                     flask-api-service\nspec:\n  selector:\n    matchLabels:\n      app:                  flask-api-service\n  endpoints:\n  - port:                   web \n`\nSubsequently, I have setup a port forward to view the metrics:\n`Kubectl port-forward prometheus-flaskapi-kube-prometheus-s-prometheus-0 9090\n`\nThen visited prometheus web page at `http://localhost:9090`\nWhen I select the Status->Targets menu option, my flask-api-service is not displayed.\nI know that the service is up and running and I have checked that I can view the metrics for a pod for my flask-api-service using `kubectl port-forward  4444`.\nLooking at a similar issue it looks as though there is a  configuration value `serviceMonitorSelectorNilUsesHelmValues` that defaults to true. Setting this to false makes the operator look outside it\u2019s release labels in helm??\nI tried adding this to the `values.yml` of my helm chart in addition to the `extraScrapeConfigs` configuration value. However, the flask-api-service still does not appear as an additional target on the prometheus web page when clicking the Status->Targets menu option.\n`prometheus:\n  prometheusSpec:\n    serviceMonitorSelectorNilUsesHelmValues: false\n  extraScrapeConfigs: |\n    - job_name: 'flaskapi'\n    static_configs:\n      - targets: ['flask-api-service:4444']\n`\nHow do I get my flask-api-service recognised on the prometheus targets page at `http://localhost:9090`?\nI am installing Kube-Prometheus-Stack as a dependency via my helm chart with default values as shown below:\nChart.yaml\n`apiVersion: v2\nappVersion: \"0.0.1\"\ndescription: A Helm chart for flaskapi deployment\nname: flaskapi\nversion: 0.0.1\ndependencies:\n- name: kube-prometheus-stack\n  version: \"14.4.0\"\n  repository: \"https://prometheus-community.github.io/helm-charts\"\n- name: ingress-nginx\n  version: \"3.25.0\"\n  repository: \"https://kubernetes.github.io/ingress-nginx\"\n- name: redis\n  version: \"12.9.0\"\n  repository: \"https://charts.bitnami.com/bitnami\"\n`\nValues.yaml\n`docker_image_tag: dcs3spp/\nhostname: flaskapi-service\nredis_host: flaskapi-redis-master.default.svc.cluster.local \nredis_port: \"6379\"\n\nprometheus:\n  prometheusSpec:\n    serviceMonitorSelectorNilUsesHelmValues: false\n  extraScrapeConfigs: |\n    - job_name: 'flaskapi'\n    static_configs:\n      - targets: ['flask-api-service:4444']\n`",
      "solution": "Prometheus custom resource definition has a field called `serviceMonitorSelector`. Prometheus only listens to those matched serviceMonitor. In case of helm deployment it is your release name.\n```\n`release: {{ $.Release.Name | quote }}\n`\n```\nSo adding this field in your serviceMonitor  should solve the issue. Then you serviceMonitor manifest file will be:\n```\n`\napiVersion:                 monitoring.coreos.com/v1\nkind:                       ServiceMonitor\nmetadata:\n  name:                     flask-api-service\n  labels:\n      release: \nspec:\n  selector:\n    matchLabels:\n      app:                  flask-api-service\n  endpoints:\n  - port:                   web \n`\n```",
      "question_score": 7,
      "answer_score": 11,
      "created_at": "2021-03-30T18:51:24",
      "url": "https://stackoverflow.com/questions/66875139/how-to-configure-kube-prometheus-stack-helm-installation-to-scrape-a-kubernetes"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "helm",
      "question_id": 72020957,
      "title": "how can I provide multipe secrets in one yaml file?",
      "problem": "How can I define multiple secrets in one file?\nSeems that providing multiple secrets doesn't work.\n```\n`apiVersion: v1\nkind: Secret\nmetadata:\n  name: ca-secret\n  labels:\n    app.kubernetes.io/managed-by: Helm\ntype: kubernetes.io/tls\ndata:\n  tls.crt: LS0tLDR\n  tls.key: LS0tLDR\napiVersion: v1\nkind: Secret\nmetadata:\n  name: envoy-secret\n  labels:\n    app.kubernetes.io/managed-by: Helm\ntype: kubernetes.io/tls\ndata:\n  tls.crt: LS0tLS1\n  tls.key: LS0tLS1\n`\n```\nI am not able to use multiple files because I need to generate a single template using helm.",
      "solution": "You can have separate manifests in one yaml file by separating them with `---`. This will work:\n`apiVersion: v1\nkind: Secret\nmetadata:\n  name: ca-secret\n  labels:\n    app.kubernetes.io/managed-by: Helm\ntype: kubernetes.io/tls\ndata:\n  tls.crt: LS0tLDR\n  tls.key: LS0tLDR\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: envoy-secret\n  labels:\n    app.kubernetes.io/managed-by: Helm\ntype: kubernetes.io/tls\ndata:\n  tls.crt: LS0tLS1\n  tls.key: LS0tLS1\n`",
      "question_score": 7,
      "answer_score": 7,
      "created_at": "2022-04-26T23:55:35",
      "url": "https://stackoverflow.com/questions/72020957/how-can-i-provide-multipe-secrets-in-one-yaml-file"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "helm",
      "question_id": 67901891,
      "title": "Prometheus pod crashlooping on error &quot;opening storage failed&quot;",
      "problem": "I restarted the Prometheus pod and now the Prometheus pod is crashlooping. Found this error in the logs:\n```\n`level=error ts=2021-06-09T09:27:29.066Z caller=main.go:758 err=\"opening storage failed: block dir: \\\"/prometheus/01F6J0P4KBBWVJD2M8B1PE7C5E\\\": open /prometheus/01F6J0P4KBBWVJD2M8B1PE7C5E/meta.json: no such file or directory\"\n`\n```\nNoticed that the `01F6J0P4KBBWVJD2M8B1PE7C5E` folder only has the chunks folder in it.\nAny idea why this occurs and is there a way to fix this?",
      "solution": "So you have already found correct github related issues and asked there also.\n\nStill seeing \"opening storage failed: block dir\" on more recent version of prometheus #7090\n\ndoes not start up after corrupted meta.json file\n\nAnd seems currently the only way to fix the problem is to delete `01F6J0P4KBBWVJD2M8B1PE7C5E` folder that contains empty `meta.json` either nothing (link)",
      "question_score": 7,
      "answer_score": 6,
      "created_at": "2021-06-09T12:01:34",
      "url": "https://stackoverflow.com/questions/67901891/prometheus-pod-crashlooping-on-error-opening-storage-failed"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "helm",
      "question_id": 71214664,
      "title": "How to change bitnami postgresql helm chart configs? (ph_hba.conf, postgresql.conf)",
      "problem": "How do you update postgresql.conf or pg_hba.conf for the main postgresql chart on https://artifacthub.io? (In particular I have been trying to update the wal_level within the postgresql.conf)\nRelated Links:\n\nhttps://github.com/bitnami/charts/tree/master/bitnami/postgresql\nhttps://artifacthub.io/packages/helm/bitnami/postgresql\nhttps://github.com/bitnami/charts/issues/6830",
      "solution": "Using `values.yaml` with the following content\n```\n`primary:\n  extendedConfiguration: |-\n    wal_level = logical\n`\n```\nyou can easily override only the `wal_level`\n```\n`helm install my-postgresql bitnami/postgresql -f values.yaml\n`\n```",
      "question_score": 6,
      "answer_score": 8,
      "created_at": "2022-02-22T01:47:36",
      "url": "https://stackoverflow.com/questions/71214664/how-to-change-bitnami-postgresql-helm-chart-configs-ph-hba-conf-postgresql-co"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "helm",
      "question_id": 73328545,
      "title": "How do I know which repository the installed Chart belongs to",
      "problem": "I have many helm repositories in my kubernetes,\nAnd I installed a lot of charts,\nSo How do I know which repository the installed Chart belongs to?\nFor example:\n```\n`$> helm repo list\n  NAME            URL\n  lucy-dev        https://harbor.mydomain.net/chartrepo/lucy-dev\n  lucy-prod       https://harbor.mydomain.net/chartrepo/lucy-prod\n\n$> helm ls -n msgbox-lucy -o yaml\n- app_version: \"1.0\"\n  chart: msgbox-lucy-8.27.3\n  name: msgbox-lucy\n  namespace: msgbox-lucy\n  revision: \"48\"\n  status: deployed\n  updated: 2022-04-19 08:11:16.761059067 +0000 UTC\n\n`\n```\nI can't use `helm show` because:\n```\n`$> helm show all msgbox-lucy -n msgbox-lucy --debug\nshow.go:195: [debug] Original chart version: \"\"\nError: non-absolute URLs should be in form of repo_name/path_to_chart, got: msgbox-lucy\n...\n`\n```",
      "solution": "I don't believe you're guaranteed to get the info you're looking for, however we can try.\nFind the latest Helm secret for your Helm release.\n```\n`kubectl get secret -n msgbox-lucy\n`\n```\nYours might look something like this:\n```\n`sh.helm.release.v1.msgbox-lucy.v5\n`\n```\nand run this command to view the chart's metadata:\n```\n`SECRET_NAME=\"sh.helm.release.v1.msgbox-lucy.v5\"\n\nkubectl get secret $SECRET_NAME -o json | jq .data.release \\\n  | tr -d '\"' | base64 -d | base64 -d | gzip -d \\\n  | jq '.chart.metadata'\n`\n```\nThe metadata should hopefully show you 2 things you're looking for. The chart name will be under the `name` field. The chart repository URL might be under `sources`.\nI say \"might\" because the chart developer should have added it there, but they might not have.\nThen you can match the URL to your repo alias.\n\nIf it's not included in the metadata, you're probably out of luck for now.\nThere is an open Github issue about exactly this feature you're wanting:\nhttps://github.com/helm/helm/issues/4256\nAnd an open PR that adds that feature:\nhttps://github.com/helm/helm/pull/11378\nAnd a merged PR to add a HIP (Helm Improvement Proposal) for adding that feature:\nhttps://github.com/helm/community/pull/269",
      "question_score": 6,
      "answer_score": 11,
      "created_at": "2022-08-12T04:01:45",
      "url": "https://stackoverflow.com/questions/73328545/how-do-i-know-which-repository-the-installed-chart-belongs-to"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "helm",
      "question_id": 65901186,
      "title": "kube-prometheus-stack issue scraping metrics",
      "problem": "General Cluster Information:\n\nKubernetes version: 1.19.13\nCloud being used: private\nInstallation method: kubeadm init\nHost OS: Ubuntu 20.04.1 LTS\nCNI and version: Weave Net: 2.7.0\nCRI and version: Docker: 19.3.13\n\nI am trying to get `kube-prometheus-stack` helm chart to work. This seems for most targets to work, however, some targets stay down as shown in the screenshot below.\n\nAre there any suggestions, how I can get `kube-etcd`, `kube-controller-manager` and `kube-scheduler` monitored by `Prometheus`?\nI deployed the helm chart as mentioned here and applied the suggestion here to get the kube-proxy monitored by `Prometheus`.\nThanks in advance for any help!\nEDIT 1:\n`- job_name: monitoring/my-stack-kube-prometheus-s-kube-controller-manager/0\n  honor_timestamps: true\n  scrape_interval: 30s\n  scrape_timeout: 10s\n  metrics_path: /metrics\n  scheme: http\n  bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token\n  relabel_configs:\n  - source_labels: [__meta_kubernetes_service_label_app]\n    separator: ;\n    regex: kube-prometheus-stack-kube-controller-manager\n    replacement: $1\n    action: keep\n  - source_labels: [__meta_kubernetes_service_label_release]\n    separator: ;\n    regex: my-stack\n    replacement: $1\n    action: keep\n  - source_labels: [__meta_kubernetes_endpoint_port_name]\n    separator: ;\n    regex: http-metrics\n    replacement: $1\n    action: keep\n  - source_labels: [__meta_kubernetes_endpoint_address_target_kind, __meta_kubernetes_endpoint_address_target_name]\n    separator: ;\n    regex: Node;(.*)\n    target_label: node\n    replacement: ${1}\n    action: replace\n  - source_labels: [__meta_kubernetes_endpoint_address_target_kind, __meta_kubernetes_endpoint_address_target_name]\n    separator: ;\n    regex: Pod;(.*)\n    target_label: pod\n    replacement: ${1}\n    action: replace\n  - source_labels: [__meta_kubernetes_namespace]\n    separator: ;\n    regex: (.*)\n    target_label: namespace\n    replacement: $1\n    action: replace\n  - source_labels: [__meta_kubernetes_service_name]\n    separator: ;\n    regex: (.*)\n    target_label: service\n    replacement: $1\n    action: replace\n  - source_labels: [__meta_kubernetes_pod_name]\n    separator: ;\n    regex: (.*)\n    target_label: pod\n    replacement: $1\n    action: replace\n  - source_labels: [__meta_kubernetes_pod_container_name]\n    separator: ;\n    regex: (.*)\n    target_label: container\n    replacement: $1\n    action: replace\n  - source_labels: [__meta_kubernetes_service_name]\n    separator: ;\n    regex: (.*)\n    target_label: job\n    replacement: ${1}\n    action: replace\n  - source_labels: [__meta_kubernetes_service_label_jobLabel]\n    separator: ;\n    regex: (.+)\n    target_label: job\n    replacement: ${1}\n    action: replace\n  - separator: ;\n    regex: (.*)\n    target_label: endpoint\n    replacement: http-metrics\n    action: replace\n  - source_labels: [__address__]\n    separator: ;\n    regex: (.*)\n    modulus: 1\n    target_label: __tmp_hash\n    replacement: $1\n    action: hashmod\n  - source_labels: [__tmp_hash]\n    separator: ;\n    regex: \"0\"\n    replacement: $1\n    action: keep\n  kubernetes_sd_configs:\n  - role: endpoints\n    namespaces:\n      names:\n      - kube-system\n- job_name: monitoring/my-stack-kube-prometheus-s-kube-etcd/0\n  honor_timestamps: true\n  scrape_interval: 30s\n  scrape_timeout: 10s\n  metrics_path: /metrics\n  scheme: http\n  bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token\n  relabel_configs:\n  - source_labels: [__meta_kubernetes_service_label_app]\n    separator: ;\n    regex: kube-prometheus-stack-kube-etcd\n    replacement: $1\n    action: keep\n  - source_labels: [__meta_kubernetes_service_label_release]\n    separator: ;\n    regex: my-stack\n    replacement: $1\n    action: keep\n  - source_labels: [__meta_kubernetes_endpoint_port_name]\n    separator: ;\n    regex: http-metrics\n    replacement: $1\n    action: keep\n  - source_labels: [__meta_kubernetes_endpoint_address_target_kind, __meta_kubernetes_endpoint_address_target_name]\n    separator: ;\n    regex: Node;(.*)\n    target_label: node\n    replacement: ${1}\n    action: replace\n  - source_labels: [__meta_kubernetes_endpoint_address_target_kind, __meta_kubernetes_endpoint_address_target_name]\n    separator: ;\n    regex: Pod;(.*)\n    target_label: pod\n    replacement: ${1}\n    action: replace\n  - source_labels: [__meta_kubernetes_namespace]\n    separator: ;\n    regex: (.*)\n    target_label: namespace\n    replacement: $1\n    action: replace\n  - source_labels: [__meta_kubernetes_service_name]\n    separator: ;\n    regex: (.*)\n    target_label: service\n    replacement: $1\n    action: replace\n  - source_labels: [__meta_kubernetes_pod_name]\n    separator: ;\n    regex: (.*)\n    target_label: pod\n    replacement: $1\n    action: replace\n  - source_labels: [__meta_kubernetes_pod_container_name]\n    separator: ;\n    regex: (.*)\n    target_label: container\n    replacement: $1\n    action: replace\n  - source_labels: [__meta_kubernetes_service_name]\n    separator: ;\n    regex: (.*)\n    target_label: job\n    replacement: ${1}\n    action: replace\n  - source_labels: [__meta_kubernetes_service_label_jobLabel]\n    separator: ;\n    regex: (.+)\n    target_label: job\n    replacement: ${1}\n    action: replace\n  - separator: ;\n    regex: (.*)\n    target_label: endpoint\n    replacement: http-metrics\n    action: replace\n  - source_labels: [__address__]\n    separator: ;\n    regex: (.*)\n    modulus: 1\n    target_label: __tmp_hash\n    replacement: $1\n    action: hashmod\n  - source_labels: [__tmp_hash]\n    separator: ;\n    regex: \"0\"\n    replacement: $1\n    action: keep\n  kubernetes_sd_configs:\n  - role: endpoints\n    namespaces:\n      names:\n      - kube-system\n- job_name: monitoring/my-stack-kube-prometheus-s-kube-scheduler/0\n  honor_timestamps: true\n  scrape_interval: 30s\n  scrape_timeout: 10s\n  metrics_path: /metrics\n  scheme: http\n  bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token\n  relabel_configs:\n  - source_labels: [__meta_kubernetes_service_label_app]\n    separator: ;\n    regex: kube-prometheus-stack-kube-scheduler\n    replacement: $1\n    action: keep\n  - source_labels: [__meta_kubernetes_service_label_release]\n    separator: ;\n    regex: my-stack\n    replacement: $1\n    action: keep\n  - source_labels: [__meta_kubernetes_endpoint_port_name]\n    separator: ;\n    regex: http-metrics\n    replacement: $1\n    action: keep\n  - source_labels: [__meta_kubernetes_endpoint_address_target_kind, __meta_kubernetes_endpoint_address_target_name]\n    separator: ;\n    regex: Node;(.*)\n    target_label: node\n    replacement: ${1}\n    action: replace\n  - source_labels: [__meta_kubernetes_endpoint_address_target_kind, __meta_kubernetes_endpoint_address_target_name]\n    separator: ;\n    regex: Pod;(.*)\n    target_label: pod\n    replacement: ${1}\n    action: replace\n  - source_labels: [__meta_kubernetes_namespace]\n    separator: ;\n    regex: (.*)\n    target_label: namespace\n    replacement: $1\n    action: replace\n  - source_labels: [__meta_kubernetes_service_name]\n    separator: ;\n    regex: (.*)\n    target_label: service\n    replacement: $1\n    action: replace\n  - source_labels: [__meta_kubernetes_pod_name]\n    separator: ;\n    regex: (.*)\n    target_label: pod\n    replacement: $1\n    action: replace\n  - source_labels: [__meta_kubernetes_pod_container_name]\n    separator: ;\n    regex: (.*)\n    target_label: container\n    replacement: $1\n    action: replace\n  - source_labels: [__meta_kubernetes_service_name]\n    separator: ;\n    regex: (.*)\n    target_label: job\n    replacement: ${1}\n    action: replace\n  - source_labels: [__meta_kubernetes_service_label_jobLabel]\n    separator: ;\n    regex: (.+)\n    target_label: job\n    replacement: ${1}\n    action: replace\n  - separator: ;\n    regex: (.*)\n    target_label: endpoint\n    replacement: http-metrics\n    action: replace\n  - source_labels: [__address__]\n    separator: ;\n    regex: (.*)\n    modulus: 1\n    target_label: __tmp_hash\n    replacement: $1\n    action: hashmod\n  - source_labels: [__tmp_hash]\n    separator: ;\n    regex: \"0\"\n    replacement: $1\n    action: keep\n  kubernetes_sd_configs:\n  - role: endpoints\n    namespaces:\n      names:\n      - kube-system\n`",
      "solution": "This is because `Prometheus` is monitoring wrong endpoints of those targets and/or targets don't expose metrics endpoint.\nTake `controller-manager` for example:\n\nChange bind-address (default: 127.0.0.1):\n\n`$ sudo vi /etc/kubernetes/manifests/kube-controller-manager.yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  ...\nspec:\n  containers:\n  - command:\n    - kube-controller-manager\n    ...\n    - --bind-address=\n    ...\n`\nIf you are using control-plane IP, you need to change `livenessProbe` and `startupProbe` host, too.\n\nChange endpoint(service) port (default: 10252):\n\n`$ kubectl edit service prometheus-kube-prometheus-kube-controller-manager -n kube-system\napiVersion: v1\nkind: Service\n...\nspec:\n  clusterIP: None\n  ports:\n  - name: http-metrics\n    port: 10257\n    protocol: TCP\n    targetPort: 10257\n ...\n`\n\nChange servicemonitor scheme (default: http):\n\n`$ kubectl edit servicemonitor prometheus-kube-prometheus-kube-controller-manager -n prometheus\napiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\n...\nspec:\n  endpoints:\n  - bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token\n    port: http-metrics\n    scheme: https\n    tlsConfig:\n      caFile: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n      insecureSkipVerify: true\n  jobLabel: jobLabel\n  ...\n`",
      "question_score": 6,
      "answer_score": 9,
      "created_at": "2021-01-26T13:14:27",
      "url": "https://stackoverflow.com/questions/65901186/kube-prometheus-stack-issue-scraping-metrics"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "helm",
      "question_id": 72184415,
      "title": "How to get current date in Helm",
      "problem": "I need to get the current date in Helm with the following format YY-MM-DD. I'm trying `{{ now | date }}` but this returns an error because the number of arguments expected by date is two, not one.",
      "solution": "Try Format the date to YEAR-MONTH-DAY:\n```\n`{{ now | date \"20060102150405\" }}\n`\n```\nSource here:\nor add the following annotation :\n```\n`annotations:\n   deploymentTime: {{ now | date \"2006-01-02T15:04:05\" }}\n`\n```",
      "question_score": 6,
      "answer_score": 9,
      "created_at": "2022-05-10T12:10:50",
      "url": "https://stackoverflow.com/questions/72184415/how-to-get-current-date-in-helm"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "helm",
      "question_id": 70956755,
      "title": "How can we override dependencies &quot;version&quot; in helm chart Chart.yaml file?",
      "problem": "I am wondering and apparently have not yet found anywhere if there is a way via which I can specifically override the value of \"version\" key's value from the dependencies chart in Helm\n```\n`# Chart.yaml\ndependencies:\n- name: bitnami/postgresql\n  version: **\"8.10.5\"**\n  repository: \"https://charts.bitnami.com/bitnami\"\n`\n```\nI tried something like below:\n```\n`# Chart.yaml\ndependencies:\n- name: bitnami/postgresql\n  version: \"{{.Values.postgresql.version}}\"\n  repository: \"https://charts.bitnami.com/bitnami\"\n`\n```\nand\n```\n`# Values.yaml\npostgreSQL:\n  version: \"8.10.5\" \n`\n```\nBut I am getting below error:\n```\n`Error: cannot load Chart.yaml: error converting YAML to JSON: yaml: invalid map key: map[interface {}]interface {}{\".Values.postgresql.version\":interface {}(nil)}\n`\n```\nIf this is currently not possible then can someone advise how do you update the \"version\" of dependencies in Charts.yaml whenever new version is available of them?",
      "solution": "It's not super well documented in the Helm documentation \u2013 the generic `helm dependency` command documentation mentions it, but not the main discussion of chart dependencies \u2013 but the `version:` field is optional, and it uses semantic versioning if it is present.  Helm maintains a separate `Chart.lock` file that lists precise versions of chart dependencies, and the `helm dependency update` command will update that file.\nSo for your uses you might say:\n`dependencies:\n- name: bitnami/postgresql\n  version: '^8' # Any 8.x.x version, but not version 7 or 9\n  repository: \"https://charts.bitnami.com/bitnami\"\n`\nOr, if you're not configuring the dependency chart at all, just leave out the `version:` line entirely and use whatever the latest version is.\n`# Install the chart using the specific Chart.lock version\nhelm install myservice .\n\n# Get a newer version in Chart.lock and upgrade the database\nrm Chart.lock\nhelm dependency update\nhelm upgrade myservice .\n`\nDo check the `Chart.lock` file into source control, so you have reproducible deployments.\nAll of this is also true if you're using the older, Helm v2-compatible layout that lists dependencies in a separate `requirements.yaml` file.  In this case the lock file is `requirements.lock`, but `version:` is still a semantic version constraint and the same `helm dependency` commands update the lock file.",
      "question_score": 6,
      "answer_score": 5,
      "created_at": "2022-02-02T14:46:39",
      "url": "https://stackoverflow.com/questions/70956755/how-can-we-override-dependencies-version-in-helm-chart-chart-yaml-file"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "helm",
      "question_id": 68960083,
      "title": "Bitnami Redis on Kubernetes Authentication Failure with Existing Secret",
      "problem": "I'm trying to install Redis on Kubernetes environment with Bitnami Redis HELM Chart. I want to use a defined password rather than randomly generated one. But i'm getting error below when i want to connect to redis master or replicas with redis-cli.\n```\n`I have no name!@redis-client:/$ redis-cli -h redis-master -a $REDIS_PASSWORD \nWarning: Using a password with '-a' or '-u' option on the command line interface may not be safe.\nWarning: AUTH failed\n`\n```\nI created a Kubernetes secret like this.\n```\n`---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: redis-secret\n  namespace: redis\ntype: Opaque\ndata:\n  redis-password: YWRtaW4xMjM0Cg==\n`\n```\nAnd in values.yaml file i updated auth spec like below.\n```\n`auth:\n  enabled: true\n  sentinel: false\n  existingSecret: \"redis-secret\"\n  existingSecretPasswordKey: \"redis-password\"\n  usePasswordFiles: false\n`\n```\nIf i don't define `existingSecret` field and use randomly generated password then i can connect without an issue. I also tried `AUTH admin1234` after `Warning: AUTH failed` error but it didn't work either.",
      "solution": "The issue was about how i encoded password with echo command. There was a newline character at the end of my password. I tried with printf command rather than echo and it created a different result.\n```\n`printf admin1234 | base64\n`\n```",
      "question_score": 6,
      "answer_score": 3,
      "created_at": "2021-08-28T00:58:00",
      "url": "https://stackoverflow.com/questions/68960083/bitnami-redis-on-kubernetes-authentication-failure-with-existing-secret"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "helm",
      "question_id": 70988791,
      "title": "Helm - Kubernetes cluster unreachable: the server has asked for the client to provide credentials",
      "problem": "I'm trying to deploy an EKS self managed with Terraform. While I can deploy the cluster with addons, vpc, subnet and all other resources, it always fails at helm:\n```\n`Error: Kubernetes cluster unreachable: the server has asked for the client to provide credentials\nwith module.eks-ssp-kubernetes-addons.module.ingress_nginx[0].helm_release.nginx[0]\non .terraform/modules/eks-ssp-kubernetes-addons/modules/kubernetes-addons/ingress-nginx/main.tf line 19, in resource \"helm_release\" \"nginx\":\nresource \"helm_release\" \"nginx\" {\n`\n```\nThis error repeats for `metrics_server`, `lb_ingress`, `argocd`, but `cluster-autoscaler` throws:\n```\n`Warning: Helm release \"cluster-autoscaler\" was created but has a failed status.\nwith module.eks-ssp-kubernetes-addons.module.cluster_autoscaler[0].helm_release.cluster_autoscaler[0]\non .terraform/modules/eks-ssp-kubernetes-addons/modules/kubernetes-addons/cluster-autoscaler/main.tf line 1, in resource \"helm_release\" \"cluster_autoscaler\":\nresource \"helm_release\" \"cluster_autoscaler\" {\n`\n```\nMy `main.tf` looks like this:\n```\n`terraform {\n\n  backend \"remote\" {}\n\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \">= 3.66.0\"\n    }\n    kubernetes = {\n      source  = \"hashicorp/kubernetes\"\n      version = \">= 2.7.1\"\n    }\n    helm = {\n      source  = \"hashicorp/helm\"\n      version = \">= 2.4.1\"\n    }\n  }\n}\n\ndata \"aws_eks_cluster\" \"cluster\" {\n  name = module.eks-ssp.eks_cluster_id\n}\n\ndata \"aws_eks_cluster_auth\" \"cluster\" {\n  name = module.eks-ssp.eks_cluster_id\n}\n\nprovider \"aws\" {\n  access_key = \"xxx\"\n  secret_key = \"xxx\"\n  region     = \"xxx\"\n  assume_role {\n    role_arn = \"xxx\"\n  }\n}\n\nprovider \"kubernetes\" {\n  host                   = data.aws_eks_cluster.cluster.endpoint\n  cluster_ca_certificate = base64decode(data.aws_eks_cluster.cluster.certificate_authority.0.data)\n  token                  = data.aws_eks_cluster_auth.cluster.token\n}\n\nprovider \"helm\" {\n  kubernetes {\n    host                   = data.aws_eks_cluster.cluster.endpoint\n    token                  = data.aws_eks_cluster_auth.cluster.token\n    cluster_ca_certificate = base64decode(data.aws_eks_cluster.cluster.certificate_authority.0.data)\n  }\n}\n`\n```\nMy `eks.tf` looks like this:\n```\n`module \"eks-ssp\" {\n    source = \"github.com/aws-samples/aws-eks-accelerator-for-terraform\"\n\n    # EKS CLUSTER\n    tenant            = \"DevOpsLabs2b\"\n    environment       = \"dev-test\"\n    zone              = \"\"\n    terraform_version = \"Terraform v1.1.4\"\n\n    # EKS Cluster VPC and Subnet mandatory config\n    vpc_id             = \"xxx\"\n    private_subnet_ids = [\"xxx\",\"xxx\", \"xxx\", \"xxx\"]\n\n    # EKS CONTROL PLANE VARIABLES\n    create_eks         = true\n    kubernetes_version = \"1.19\"\n\n  # EKS SELF MANAGED NODE GROUPS\n    self_managed_node_groups = {\n    self_mg = {\n      node_group_name        = \"DevOpsLabs2b\"\n      subnet_ids             = [\"xxx\",\"xxx\", \"xxx\", \"xxx\"]\n      create_launch_template = true\n      launch_template_os     = \"bottlerocket\"       # amazonlinux2eks  or bottlerocket or windows\n      custom_ami_id          = \"xxx\"\n      public_ip              = true                   # Enable only for public subnets\n      pre_userdata           = <<-EOT\n            yum install -y amazon-ssm-agent \\\n            systemctl enable amazon-ssm-agent && systemctl start amazon-ssm-agent \\\n        EOT\n\n      disk_size     = 10\n      instance_type = \"t2.small\"\n      desired_size  = 2\n      max_size      = 10\n      min_size      = 0\n      capacity_type = \"\" # Optional Use this only for SPOT capacity as  capacity_type = \"spot\"\n\n      k8s_labels = {\n        Environment = \"dev-test\"\n        Zone        = \"\"\n        WorkerType  = \"SELF_MANAGED_ON_DEMAND\"\n      }\n\n      additional_tags = {\n        ExtraTag    = \"t2x-on-demand\"\n        Name        = \"t2x-on-demand\"\n        subnet_type = \"public\"\n      }\n      create_worker_security_group = false # Creates a dedicated sec group for this Node Group\n    },\n  }\n}\n\n    enable_amazon_eks_vpc_cni             = true\n        amazon_eks_vpc_cni_config = {\n        addon_name               = \"vpc-cni\"\n        addon_version            = \"v1.7.5-eksbuild.2\"\n        service_account          = \"aws-node\"\n        resolve_conflicts        = \"OVERWRITE\"\n        namespace                = \"kube-system\"\n        additional_iam_policies  = []\n        service_account_role_arn = \"\"\n        tags                     = {}\n    }\n    enable_amazon_eks_kube_proxy          = true\n        amazon_eks_kube_proxy_config = {\n        addon_name               = \"kube-proxy\"\n        addon_version            = \"v1.19.8-eksbuild.1\"\n        service_account          = \"kube-proxy\"\n        resolve_conflicts        = \"OVERWRITE\"\n        namespace                = \"kube-system\"\n        additional_iam_policies  = []\n        service_account_role_arn = \"\"\n        tags                     = {}\n    }\n\n    #K8s Add-ons\n    enable_aws_load_balancer_controller   = true\n    enable_metrics_server                 = true\n    enable_cluster_autoscaler             = true\n    enable_aws_for_fluentbit              = true\n    enable_argocd                         = true\n    enable_ingress_nginx                  = true\n\n    depends_on = [module.eks-ssp.self_managed_node_groups]\n}\n`\n```",
      "solution": "OP has confirmed in the comment that the problem was resolved:\n\nOf course. I think I found the issue. Doing \"kubectl get svc\" throws: \"An error occurred (AccessDenied) when calling the AssumeRole operation: User: arn:aws:iam::xxx:user/terraform_deploy is not authorized to perform: sts:AssumeRole on resource: arn:aws:iam::xxx:user/terraform_deploy\"\nSolved it by using my actual role, that's crazy. No idea why it was calling itself.\n\nFor similar problem look also this issue.",
      "question_score": 6,
      "answer_score": 3,
      "created_at": "2022-02-04T16:20:55",
      "url": "https://stackoverflow.com/questions/70988791/helm-kubernetes-cluster-unreachable-the-server-has-asked-for-the-client-to-pr"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "helm",
      "question_id": 66163286,
      "title": "helm template escaped values for Grafana charts",
      "problem": "wise SOers.  It turns out Grafana dashboard json files use the same `{{ }}` to do variable substitution as helm does.  I have a grafana chart that is laden with these `{{ }}` to a disagreeable degree.\nWhen I want to put that chart into a template, like so:\n```\n`apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: super-dashboard\n  namespace: monitoring\n  labels:\n    grafana_dashboard: \"1\"\ndata:\n  super-dashboard.json: |-\n{{ .Files.Get \"super-dashboard.json\"  | indent 4 }\n`\n```\nIt works great as long as the `super-dashboard.json` doesn't have any thing in it like:\n`\"legendFormat\": \"{{status}} Status\",`.\nUnfortunately, our dashboard does have such a woeful line.  When I run helm, I get:\n```\n`Error: UPGRADE FAILED: parse error at (templates/dashboards/super-dashboard.json:282): function \"status\" not defined\n`\n```\nNaturally, it's looking for some method `status` which does not exist in the helm template language and fails thusly.  If only I could ignore parsing of that pestering file. Oh, ye wise masters of the Internet, have you any sage advice for the humble seeker of your collective wisdom?",
      "solution": "The issue was my `super-dashboard.json` file was in the same directories as the templates and helm tried to templatize it.  The solution is to have a directory structure like:\n```\n`mychart/\n  templates/\n    super-dashboard.yaml\n  files/\n    super-dashboard.json\n`\n```\nThen the `yaml` file has:\n```\n`{{ .Files.Get \"files/super-dashboard.json\" | indent 4 }}\n`\n```\nI thought you had to put the files in the same directory but it just has to be at the root of the chart.",
      "question_score": 6,
      "answer_score": 5,
      "created_at": "2021-02-11T22:37:54",
      "url": "https://stackoverflow.com/questions/66163286/helm-template-escaped-values-for-grafana-charts"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "helm",
      "question_id": 68197155,
      "title": "Adding an SSH GitHub repository to ArgoCD using declarative DSL gives &quot;authentication required&quot;",
      "problem": "I have an ArgoCD installation and want to add a GitHub repository using SSH access with an SSH key pair to it using the declarative DSL.\nWhat I have is:\n```\n`apiVersion: v1\ndata:\n  sshPrivateKey: \n  url: \nkind: Secret\nmetadata:\n  annotations:\n    meta.helm.sh/release-name: argocd-config\n    meta.helm.sh/release-namespace: argocd\n  creationTimestamp: \"2021-06-30T12:39:35Z\"\n  labels:\n    app.kubernetes.io/managed-by: Helm\n    argocd.argoproj.io/secret-type: repo-creds\n  name: repo-creds\n  namespace: argocd\n  resourceVersion: \"364936\"\n  selfLink: /api/v1/namespaces/argocd/secrets/repo-creds\n  uid: 8ca64883-302b-4a41-aaf6-5277c34dfbfc\ntype: Opaque\n---\napiVersion: v1\ndata:\n  url: \nkind: Secret\nmetadata:\n  annotations:\n    meta.helm.sh/release-name: argocd-config\n    meta.helm.sh/release-namespace: argocd\n  creationTimestamp: \"2021-06-30T12:39:35Z\"\n  labels:\n    app.kubernetes.io/managed-by: Helm\n    argocd.argoproj.io/secret-type: repository\n  name: argocd-repo\n  namespace: argocd\n  resourceVersion: \"364935\"\n  selfLink: /api/v1/namespaces/argocd/secrets/argocd-repo\n  uid: 09de56e0-3b0a-4032-8fb5-81b3a6e1899e\ntype: Opaque\n\n`\n```\nI can manually connect to  that GitHub private repo using that SSH key pair, but using the DSL, the repo doesn't appear in the ArgoCD GUI.\nIn the log of the argocd-repo-server I am getting the error:\n```\n`time=\"2021-06-30T14:48:25Z\" level=error msg=\"finished unary call with code Unknown\" error=\"authentication required\" grpc.code=Unknown grpc.method=GenerateManifest grpc.request.deadline=\"2021-06-30T14:49:25Z\" grpc.service=repository.RepoServerService grpc.start_time=\"2021-06-30T14:48:25Z\" grpc.time_ms=206.505 span.kind=server system=grpc\n\n`\n```\nI deploy the secrets with helm.\nSo can anyone help me point in the right direction? What am I doing wrong?\nI basically followed the declarative documentation under: https://argoproj.github.io/argo-cd/operator-manual/declarative-setup/\nThanks in advance.\nBest regards,\nrforberger",
      "solution": "I am not sure about helm, since I am working with the yaml files for now, before moving into helm. You could take a look at this Github issue here to configure SSH Key for helm\nI had this issue, when I was working with manifests. The repo config should be in argocd-cm configmap. The fix was this:\n`---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: argocd-cm\n  namespace: argocd\n  labels:\n    app.kubernetes.io/name: argocd-cm\n    app.kubernetes.io/part-of: argocd\ndata:\n  repositories: |\n    - name: my-test-repo\n      url: ssh://git@repo-url/path/to/repo.git\n      type: git\n      insecure: true.                  // To skip verification\n      insecureIgnoreHostKey: true      // to ignore host key for ssh\n      sshPrivateKeySecret:\n        name: private-repo-creds\n        key: sshPrivateKey\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: private-repo-creds\n  namespace: argocd\n  labels:\n    argocd.argoproj.io/secret-type: repo-creds\ndata:\n  sshPrivateKey: \n`\nAnd I am not sure if the documentation is correct or not, because I can see the document in stable is a bit different, although both your link and this stable doc link are from the same version",
      "question_score": 6,
      "answer_score": 2,
      "created_at": "2021-06-30T16:55:17",
      "url": "https://stackoverflow.com/questions/68197155/adding-an-ssh-github-repository-to-argocd-using-declarative-dsl-gives-authentic"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "helm",
      "question_id": 65771723,
      "title": "Install Helm in specific version with brew",
      "problem": "I tried:\n```\n`~ \u276f brew install helm@3.4.2                                                                                                        at 11:05:02\n==> Searching for similarly named formulae...\nError: No similarly named formulae found.\nError: No available formula or cask with the name \"helm@3.4.2\".\n==> Searching for a previously deleted formula (in the last month)...\nError: No previously deleted formula found.\n==> Searching taps on GitHub...\nError: No formulae found in taps.\n`\n```\nAlso tried to install from a specific commit hash as several guides online suggested but got this error:\n```\n`/usr/local/Homebrew/Library/Homebrew/formulary.rb:227:in `load_file': Invalid usage: Installation of helm from a GitHub commit URL is unsupported! 'brew extract helm' to stable tap on GitHub instead. (UsageError)\n`\n```\nWhat am I doing wrong?",
      "solution": "Use `$ brew search` to list all available formulas:\n`$ brew search helm\nhelm  helm@2  helmfile  helmsman\n`\nYou can install one of the versions from the shown result:\n`$ brew install helm\nor \n$ brew install helm@2\n`\nFrom Helm:\n\nMembers of the Helm community have contributed a Helm formula build to Homebrew. This formula is generally up to date.",
      "question_score": 6,
      "answer_score": 2,
      "created_at": "2021-01-18T10:20:49",
      "url": "https://stackoverflow.com/questions/65771723/install-helm-in-specific-version-with-brew"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "helm",
      "question_id": 65897034,
      "title": "ArgoCD Helm chart - Repository not accessible",
      "problem": "I'm trying to add a helm chart (https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack) to ArgoCD.\nWhen I do this, I get the following error:\n\nUnable to save changes: application spec is invalid: InvalidSpecError: repository not accessible: repository not found\n\nCan you guys help me out please? I think I did everything right but it seems something's wrong...\nHere's the Project yaml.\n```\n`apiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: prom-oper\n  namespace: argocd\nspec:\n  project: prom-oper\n\n  source:\n    repoURL: https://prometheus-community.github.io/helm-charts\n    targetRevision: \"13.2.1\"\n    path: prometheus-community/kube-prometheus-stack\n\n    helm:\n      # Release name override (defaults to application name)\n      releaseName: prom-oper\n      version: v3\n      values: |\n        ... redacted\n\n    directory:\n      recurse: false\n\n  destination:\n    server: https://kubernetes.default.svc\n    namespace: prom-oper\n\n  syncPolicy:\n    automated: # automated sync by default retries failed attempts 5 times with following delays between attempts ( 5s, 10s, 20s, 40s, 80s ); retry controlled using `retry` field.\n      prune: false # Specifies if resources should be pruned during auto-syncing ( false by default ).\n      selfHeal: false # Specifies if partial app sync should be executed when resources are changed only in target Kubernetes cluster and no git change detected ( false by default ).\n      allowEmpty: false # Allows deleting all application resources during automatic syncing ( false by default ).\n    syncOptions:     # Sync options which modifies sync behavior\n    - CreateNamespace=true # Namespace Auto-Creation ensures that namespace specified as the application destination exists in the destination cluster.\n    # The retry feature is available since v1.7\n    retry:\n      limit: 5 # number of failed sync attempt retries; unlimited number of attempts if less than 0\n      backoff:\n        duration: 5s # the amount to back off. Default unit is seconds, but could also be a duration (e.g. \"2m\", \"1h\")\n        factor: 2 # a factor to multiply the base duration after each failed retry\n        maxDuration: 3m # the maximum amount of time allowed for the backoff strategy\n\n`\n```\nand also the configmap where I added the helm repo\n```\n`apiVersion: v1\nkind: ConfigMap\nmetadata:\n  labels:\n    app.kubernetes.io/name: argocd-cm\n    app.kubernetes.io/part-of: argocd\n  name: argocd-cm\n  namespace: argocd\ndata:\n  admin.enabled: \"false\"\n  repositories: |\n    - type: helm\n      url: https://prometheus-community.github.io/helm-charts\n      name: prometheus-community\n`\n```",
      "solution": "The reason you are getting this error is because the way the Application is defined, Argo thinks it's a Git repository instead of Helm.\nDefine the source object with a \"chart\" property instead of \"path\" like so:\n```\n`apiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: prom-oper\n  namespace: argocd\nspec:\n  project: prom-oper\n\n  source:\n    repoURL: https://prometheus-community.github.io/helm-charts\n    targetRevision: \"13.2.1\"\n    chart: kube-prometheus-stack\n`\n```\nYou can see it defined on line 128 in Argo's application-crd.yaml",
      "question_score": 5,
      "answer_score": 13,
      "created_at": "2021-01-26T07:56:32",
      "url": "https://stackoverflow.com/questions/65897034/argocd-helm-chart-repository-not-accessible"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "helm",
      "question_id": 70899601,
      "title": "Helm3 template error calling include: template: no template &quot;microservice.labels&quot; associated with template &quot;gotpl&quot;",
      "problem": "I have helm 3 template created using `helm create microservice` command. it has below files.\n```\n`/Chart.yaml\n./values.yaml\n./.helmignore\n./templates/ingress.yaml\n./templates/deployment.yaml\n./templates/service.yaml\n./templates/serviceaccount.yaml\n./templates/hpa.yaml\n./templates/NOTES.txt\n./templates/_helpers.tpl\n./templates/tests/test-connection.yaml\n`\n```\nUpdated values file based on my application, when I try to install the helm chat its giving below error message.\n```\n`Error: UPGRADE FAILED: template: microservice/templates/ingress.yaml:20:8: executing \"microservice/templates/ingress.yaml\" at : error calling include: template: no template \"microservice.labels\" associated with template \"gotpl\"\nhelm.go:75: [debug] template: microservice/templates/ingress.yaml:20:8: executing \"microservice/templates/ingress.yaml\" at : error calling include: template: no template \"microservice.labels\" associated with template \"gotpl\"\n`\n```\nHere is the `ingress.yaml` file.\n```\n`{{- if .Values.ingress.enabled -}}\n{{- $fullName := include \"microservice.fullname\" . -}}\n{{- $svcPort := .Values.service.port -}}\n{{- if and .Values.ingress.className (not (semverCompare \">=1.18-0\" .Capabilities.KubeVersion.GitVersion)) }}\n  {{- if not (hasKey .Values.ingress.annotations \"kubernetes.io/ingress.class\") }}\n  {{- $_ := set .Values.ingress.annotations \"kubernetes.io/ingress.class\" .Values.ingress.className}}\n  {{- end }}\n{{- end }}\n{{- if semverCompare \">=1.19-0\" .Capabilities.KubeVersion.GitVersion -}}\napiVersion: networking.k8s.io/v1\n{{- else if semverCompare \">=1.14-0\" .Capabilities.KubeVersion.GitVersion -}}\napiVersion: networking.k8s.io/v1beta1\n{{- else -}}\napiVersion: extensions/v1beta1\n{{- end }}\nkind: Ingress\nmetadata:\n  name: {{ $fullName }}\n  labels:\n    {{- include \"microservice.labels\" . | nindent 4 }}\n  {{- with .Values.ingress.annotations }}\n  annotations:\n    {{- toYaml . | nindent 4 }}\n  {{- end }}\nspec:\n  {{- if and .Values.ingress.className (semverCompare \">=1.18-0\" .Capabilities.KubeVersion.GitVersion) }}\n  ingressClassName: {{ .Values.ingress.className }}\n  {{- end }}\n  {{- if .Values.ingress.tls }}\n  tls:\n    {{- range .Values.ingress.tls }}\n    - hosts:\n        {{- range .hosts }}\n        - {{ . | quote }}\n        {{- end }}\n      secretName: {{ .secretName }}\n    {{- end }}\n  {{- end }}\n  rules:\n    {{- range .Values.ingress.hosts }}\n    - host: {{ .host | quote }}\n      http:\n        paths:\n          {{- range .paths }}\n          - path: {{ .path }}\n            {{- if and .pathType (semverCompare \">=1.18-0\" $.Capabilities.KubeVersion.GitVersion) }}\n            pathType: {{ .pathType }}\n            {{- end }}\n            backend:\n              {{- if semverCompare \">=1.19-0\" $.Capabilities.KubeVersion.GitVersion }}\n              service:\n                name: {{ $fullName }}\n                port:\n                  number: {{ $svcPort }}\n              {{- else }}\n              serviceName: {{ $fullName }}\n              servicePort: {{ $svcPort }}\n              {{- end }}\n          {{- end }}\n    {{- end }}\n{{- end }}\n`\n```\nHow to I added `microservice.labels` template?. Do I need to create `microservice.labels.tlp` file?\nAny tips to fix this error.\nThanks\nSR",
      "solution": "I copied the `ingress.yaml` file to, chart created older version helm. this value was missing in `_helpers.tpl` file. Now I copied new version of hellpers.tpl file. deployment works now.",
      "question_score": 5,
      "answer_score": 12,
      "created_at": "2022-01-28T20:58:14",
      "url": "https://stackoverflow.com/questions/70899601/helm3-template-error-calling-include-template-no-template-microservice-labels"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "helm",
      "question_id": 66138370,
      "title": "Permission problem w/ helm3 installation of traefik on port 80 (hostNetwork)",
      "problem": "I'm studying helm3 and k8s (microk8s).\nWhile tryingi the following command:\n```\n`helm install traefik traefik/traefik  -n traefik --values traefik-values.yaml\n`\n```\nand traefik-values.yaml has the following value:\n```\n`additionalArguments:\n  - \"--certificatesresolvers.letsencrypt.acme.email=\"\n  - \"--certificatesresolvers.letsencrypt.acme.storage=/data/acme.json\"\n  - \"--certificatesresolvers.letsencrypt.acme.caserver=https://acme-v02.api.letsencrypt.org/directory\"\n  - \"--certificatesResolvers.letsencrypt.acme.tlschallenge=true\"\n  - \"--api.insecure=true\"\n  - \"--accesslog=true\"\n  - \"--log.level=INFO\"\nhostNetwork: true\nipaddress: \nservice:\n  type: ClusterIP\nports:\n  web:\n    port: 80\n  websecure:\n    port: 443\n`\n```\nI receive this bind-permission error\n```\n`traefik.go:76: command traefik error: error while building entryPoint web: error preparing server: error opening listener: listen tcp :80: bind: permission denied\n`\n```\non the other hand, I can install Traefik on the same ports (80 and 443) using the following `yaml` file (approximately the example on Traefik's site):\n```\n`---\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: traefik\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: traefik-ingress-controller\n  namespace: traefik\n---\nkind: DaemonSet\napiVersion: apps/v1\nmetadata:\n  name: traefik-ingress-controller\n  namespace: traefik\n  labels:\n    k8s-app: traefik-ingress-lb\nspec:\n  selector:\n    matchLabels:\n      k8s-app: traefik-ingress-lb\n      name: traefik-ingress-lb\n  template:\n    metadata:\n      labels:\n        k8s-app: traefik-ingress-lb\n        name: traefik-ingress-lb\n    spec:\n      tolerations:\n      - effect: NoSchedule\n        operator: Exists\n      serviceAccountName: traefik-ingress-controller\n      terminationGracePeriodSeconds: 60\n      hostNetwork: true\n      containers:\n      - image: traefik:2.4\n        name: traefik-ingress-lb\n        ports:\n        - name: http\n          containerPort: 80\n          hostPort: 80\n        # - name: admin\n        #   containerPort: 8080\n        #   hostPort: 8080\n        securityContext:\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_BIND_SERVICE\n        args:\n        - --providers.kubernetesingress=true\n        # you need to manually set this IP to the incoming public IP\n        # that your ingress resources would use. Note it only affects\n        # status and kubectl UI, and doesn't really do anything\n        # It could even be left out https://github.com/containous/traefik/issues/6303\n        - --providers.kubernetesingress.ingressendpoint.ip=\n        ## uncomment these and the ports above and below to enable\n        ## the web UI on the host NIC port 8080 in **insecure** mode\n        - --api.dashboard=true\n        - --api.insecure=true\n        - --log=true\n        - --log.level=INFO\n        - --accesslog=true\n        - --entrypoints.web.address=:80\n        - --entrypoints.websecure.address=:443\n        - --certificatesresolvers.leresolver.acme.tlschallenge=true #  # The two specs are not identical but quite similar as far as I can understand. They both create a ServiceAccount in the 'traefik' namespace and grant a ClusterRole.\nWhat part determines the permission on port 80?",
      "solution": "There's an open issue on the Traefik helm chart where Jasper Ben suggests a working solution:\n```\n`hostNetwork: true\nports:\n  web:\n    port: 80\n    redirectTo: websecure\n  websecure:\n    port: 443\n\nsecurityContext:\n  capabilities:\n    drop: [ALL]\n    add: [NET_BIND_SERVICE]\n  readOnlyRootFilesystem: true\n  runAsGroup: 0\n  runAsNonRoot: false\n  runAsUser: 0\n`\n```\nThe missing part in the helm chart is `NET_BIND_SERVICE` capability in the securityContext.",
      "question_score": 5,
      "answer_score": 11,
      "created_at": "2021-02-10T15:01:59",
      "url": "https://stackoverflow.com/questions/66138370/permission-problem-w-helm3-installation-of-traefik-on-port-80-hostnetwork"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "helm",
      "question_id": 70371814,
      "title": "Unable to install Jenkins from Helm chart",
      "problem": "I'm attempting to install Jenkins from a Helm chart for the first time.\nI run\n```\n`helm repo add jenkins https://charts.jenkins.io\nhelm repo update\nhelm upgrade --install myjenkins jenkins/jenkins\n`\n```\nBut the service never starts. The pod logs show the following errors:\n```\n`Plugin git:4.10.0 (via credentials:1055.v1346ba467ba1) depends on configuration-as-code:1.55, but there is an older version defined on the top level - configuration-as-code:1.54\n        at io.jenkins.tools.pluginmanager.impl.PluginManager.start(PluginManager.java:222)\n        at io.jenkins.tools.pluginmanager.impl.PluginManager.start(PluginManager.java:171)\n        at io.jenkins.tools.pluginmanager.cli.Main.main(Main.java:70)\n        Suppressed: io.jenkins.tools.pluginmanager.impl.PluginDependencyException: Plugin kubernetes:1.30.11 (via credentials:1055.v1346ba467ba1) depends on configuration-as-code:1.55, but there is an older version defined on the top level - configuration-as-code:1.54\n                at io.jenkins.tools.pluginmanager.impl.PluginManager.resolveRecursiveDependencies(PluginManager.java:1074)\n                at io.jenkins.tools.pluginmanager.impl.PluginManager.findPluginsAndDependencies(PluginManager.java:649)\n                at io.jenkins.tools.pluginmanager.impl.PluginManager.start(PluginManager.java:214)\n                ... 2 more\n        Suppressed: io.jenkins.tools.pluginmanager.impl.PluginDependencyException: Plugin workflow-aggregator:2.6 (via credentials:1055.v1346ba467ba1) depends on configuration-as-code:1.55, but there is an older version defined on the top level - configuration-as-code:1.54\n                at io.jenkins.tools.pluginmanager.impl.PluginManager.resolveRecursiveDependencies(PluginManager.java:1074)\n                at io.jenkins.tools.pluginmanager.impl.PluginManager.findPluginsAndDependencies(PluginManager.java:649)\n                at io.jenkins.tools.pluginmanager.impl.PluginManager.start(PluginManager.java:214)\n                ... 2 more\n        Suppressed: io.jenkins.tools.pluginmanager.impl.PluginDependencyException: Plugin git:4.10.0 (via credentials:1055.v1346ba467ba1) depends on configuration-as-code:1.55, but there is an older version defined on the top level - configuration-as-code:1.54\n                at io.jenkins.tools.pluginmanager.impl.PluginManager.resolveRecursiveDependencies(PluginManager.java:1074)\n                at io.jenkins.tools.pluginmanager.impl.PluginManager.findPluginsAndDependencies(PluginManager.java:649)\n                at io.jenkins.tools.pluginmanager.impl.PluginManager.start(PluginManager.java:214)\n                ... 2 more\nMultiple plugin prerequisites not met:\nPlugin kubernetes:1.30.11 (via credentials:1055.v1346ba467ba1) depends on configuration-as-code:1.55, but there is an older version defined on the top level - configuration-as-code:1.54,\nPlugin workflow-aggregator:2.6 (via credentials:1055.v1346ba467ba1) depends on configuration-as-code:1.55, but there is an older version defined on the top level - configuration-as-code:1.54,\nPlugin git:4.10.0 (via credentials:1055.v1346ba467ba1) depends on configuration-as-code:1.55, but there is an older version defined on the top level - configuration-as-code:1.54\n`\n```\nHow can I fix this?",
      "solution": "The solution appears to be to force the Helm chart to install updated plugins. The following values.yaml file allowed me to complete the deployment:\n```\n`controller:\n    installPlugins:\n    - configuration-as-code:1.55\n    - kubernetes:1.31.1 \n    - workflow-aggregator:2.6 \n    - git:4.10.1 \n`\n```",
      "question_score": 5,
      "answer_score": 7,
      "created_at": "2021-12-16T01:10:34",
      "url": "https://stackoverflow.com/questions/70371814/unable-to-install-jenkins-from-helm-chart"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "helm",
      "question_id": 68248917,
      "title": "How to override helm environment variables through CLI?",
      "problem": "I am using helm environment variables to override some of my spring boot application.yaml config and its working perfectly fine.\n```\n`helm install deploy-name-1 mychartname --values=.helm/deployment/values.yaml\n`\n```\nvalues.yaml\n```\n`env:\n  - name: WORD\n    value: hello\n`\n```\nOn executing helm install command, I can see the right WORD being picked up during helm deployment that's all good.\nHowever I would like to override the value of this environment variable \"WORD\" through helm install command on CLI. On trying I am facing the following error...\nCommand (taking from here):\n```\n`helm install deployment2 mychartname --values=.helm/deployment/values.yaml --set env.WORD=tree\n`\n```\nError\n```\n`panic: interface conversion: interface {} is []interface {}, not map[string]interface {}\n\ngoroutine 1 [running]:\nhelm.sh/helm/v3/pkg/strvals.(*parser).key(0xc0004eff60, 0xc000538840, 0x1592d34, 0x1838b20)\n        /home/circleci/helm.sh/helm/pkg/strvals/parser.go:211 +0xdf1\nhelm.sh/helm/v3/pkg/strvals.(*parser).parse(0xc0004eff60, 0xc000538840, 0x0)\n        /home/circleci/helm.sh/helm/pkg/strvals/parser.go:133 +0x3f\nhelm.sh/helm/v3/pkg/strvals.ParseInto(0xc0000b60c0, 0x23, 0xc000538840, 0x0, 0x0)\n        /home/circleci/helm.sh/helm/pkg/strvals/parser.go:70 +0xc5\nhelm.sh/helm/v3/pkg/cli/values.(*Options).MergeValues(0xc000080c60, 0xc0004efb40, 0x1, 0x1, 0x0, 0x0, 0x0)\n        /home/circleci/helm.sh/helm/pkg/cli/values/options.go:62 +0x232\nmain.newUpgradeCmd.func1(0xc0001e0500, 0xc0004ffd80, 0x2, 0x8, 0x0, 0x0)\n        /home/circleci/helm.sh/helm/cmd/helm/upgrade.go:82 +0x1fe\ngithub.com/spf13/cobra.(*Command).execute(0xc0001e0500, 0xc0004ffc80, 0x8, 0x8, 0xc0001e0500, 0xc0004ffc80)\n        /go/pkg/mod/github.com/spf13/cobra@v0.0.5/command.go:826 +0x467\ngithub.com/spf13/cobra.(*Command).ExecuteC(0xc00069d180, 0x1c2f380, 0xc000676160, 0xc0004586d0)\n        /go/pkg/mod/github.com/spf13/cobra@v0.0.5/command.go:914 +0x302\ngithub.com/spf13/cobra.(*Command).Execute(...)\n        /go/pkg/mod/github.com/spf13/cobra@v0.0.5/command.go:864\nmain.main()\n        /home/circleci/helm.sh/helm/cmd/helm/helm.go:74 +0x1e9\n`\n```\ndeployment.yaml\n```\n`...\n    spec:\n      containers:\n        - name: {{ .Release.Name }}\n          env:\n            {{- range .Values.env }}\n            - name: {{ .name }}\n              value: {{ .value }}\n            {{ end }}\n`\n```",
      "solution": "The `helm install --set` option allows only basic path-based navigation and no more advanced query operations.  You can't look for the `env:` value with `name: WORD` and set the corresponding `value:`; all you can do is blindly set the first `env:` value.\n`helm install ... --set 'env[0].value=tree'\n`\nRather than provide whole chunks of Kubernetes YAML through Helm values, it's more common to provide very specific settings; provide \"the word\" as configuration, rather than \"a set of environment variables, which should include `WORD`\".  Then you can straightforwardly override this specific thing.\n`# templates/deployment.yaml\nenv:\n  - name: WORD\n    value: {{ .Values.word }}\n`\n`# values.yaml\nword: hello\n`\n`helm install ... --set word=tree\n`",
      "question_score": 5,
      "answer_score": 10,
      "created_at": "2021-07-04T23:28:49",
      "url": "https://stackoverflow.com/questions/68248917/how-to-override-helm-environment-variables-through-cli"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "helm",
      "question_id": 66579259,
      "title": "Error: file &#39;home/user/values.yaml&#39; seems to be a YAML file, but expected a gzipped archive",
      "problem": "I am trying to install Kube Prometheus Stack using helm.\nI have already setup ingress, so it needs to be running behind a proxy.\nFor that I have updated values of the chart by using below command.\n```\n`helm show values prometheus-com/kube-prometheus-stack > values.yaml\n\n`\n```\nI followed this doc and changed configurations,\n```\n`[server]\ndomain = example.com\n`\n```\nNow I am trying to install using below command.\n```\n`helm install monitoring ./values.yaml  -n monitoring\n`\n```\nI have already created a namespace `monitoring`\nI get below error on running above command.\n```\n`Error: file '/home/user/values.yaml' seems to be a YAML file, but expected a gzipped archive\n`\n```",
      "solution": "Your helm command should be something like this:\n```\n`$ helm install  / --values ./values.yaml  -n monitoring\n\n`\n```",
      "question_score": 5,
      "answer_score": 4,
      "created_at": "2021-03-11T10:00:09",
      "url": "https://stackoverflow.com/questions/66579259/error-file-home-user-values-yaml-seems-to-be-a-yaml-file-but-expected-a-gzip"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "helm",
      "question_id": 74781273,
      "title": "Helm install fails on K3s : ensure CRDs are installed first",
      "problem": "My team and I are new to Kubernetes and are experimenting with running a few applications on it.\nFor proof of concept, we have a running Lightweight Kubernetes (K3s) install, which presumably does not have the full range of CRDs available on a standard Kubernetes. While trying to install Envoy proxy via Helm Chart, we ran into the below error:\n```\n`# helm install my-envoy cloudnativeapp/envoy --version 1.5.0\nError: INSTALLATION FAILED: unable to build kubernetes objects from release manifest: resource mapping not found for name: \"my-envoy\" namespace: \"\" from \"\": no matches for kind \"PodDisruptionBudget\" in version \"policy/v1beta1\"\nensure CRDs are installed first\n`\n```\nPresumably the message `ensure CRDs are installed first` refers to components that are missing in K3s. Is there a way to get these components installed (via Helm or some other methods)?",
      "solution": "Most likely the problem is not related to missing CRDs but to the kubernetes version. I assume you are using the latest K3S version, which is v1.25.4. `PodDisruptionBudget` was moved from `policy/v1beta1` to `policy/v1` in version v1.25. As the Envoy helm chart that you are using does not seem to be actively maintained, probably you will have to downgrade K3S or find a different chart.",
      "question_score": 5,
      "answer_score": 6,
      "created_at": "2022-12-13T08:25:16",
      "url": "https://stackoverflow.com/questions/74781273/helm-install-fails-on-k3s-ensure-crds-are-installed-first"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "helm",
      "question_id": 70402279,
      "title": "Issue injecting helm value on configmap",
      "problem": "can someone help?\nI am trying to inject a helm value on a config map, but it breaks the format. If I use the value directly instead of .Values, it works fine.\nWhat I have:\n```\n`data:\n  application.instanceLabelKey: argocd.argoproj.io/instance\n  oidc.config: |\n    name: Okta\n    issuer: https://mycompany.okta.com\n    clientID: {{ .Values.okta.clientID }}\n    clientSecret: {{ .Values.okta.clientSecret }}\n    requestedScopes: [\"openid\", \"profile\", \"email\", \"groups\"]\n    requestedIDTokenClaims: {\"groups\": {\"essential\": true}}\n`\n```\nThe result\n```\n`data:\n  application.instanceLabelKey: argocd.argoproj.io/instance\n  oidc.config: \"name: Okta\\nissuer: https://mycompany.okta.com\\nclientID: myClientId \\nclientSecret:\n    mySecret\\nrequestedScopes: [\\\"openid\\\", \\\"profile\\\",\n    \\\"email\\\", \\\"groups\\\"]\\nrequestedIDTokenClaims: {\\\"groups\\\": {\\\"essential\\\": true}}\\n\"\n`\n```",
      "solution": "After lots of tries, it worked when I skipped the a whitespace at the beginning\n```\n`data:\n  application.instanceLabelKey: argocd.argoproj.io/instance\n  oidc.config: |\n    name: Okta\n    issuer: \"https://mycompany.okta.com\"\n    clientID: {{- .Values.okta.clientId }}\n    clientSecret: {{- .Values.okta.clientSecret }}\n    requestedScopes: [\"openid\", \"profile\", \"email\", \"groups\"]\n    requestedIDTokenClaims: {\"groups\": {\"essential\": true}}\n`\n```",
      "question_score": 5,
      "answer_score": 1,
      "created_at": "2021-12-18T10:27:03",
      "url": "https://stackoverflow.com/questions/70402279/issue-injecting-helm-value-on-configmap"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "helm",
      "question_id": 68505269,
      "title": "Helm Prometheus operator doesn&#39;t add new ServiceMonitor endpoints to targets",
      "problem": "I'm trying to monitor my app using helm prometheus https://github.com/prometheus-community/helm-charts. I've installed this helm chart successfully.\n```\n`prometheus-kube-prometheus-operator-5d8dcd5988-bw222   1/1     Running   0          11h\nprometheus-kube-state-metrics-5d45f64d67-97vxt         1/1     Running   0          11h\nprometheus-prometheus-kube-prometheus-prometheus-0     2/2     Running   0          11h\nprometheus-prometheus-node-exporter-gl4cz              1/1     Running   0          11h\nprometheus-prometheus-node-exporter-mxrsm              1/1     Running   0          11h\nprometheus-prometheus-node-exporter-twvdb              1/1     Running   0          11h\n`\n```\nApp Service and Deployment created in the same namespace, by these yml configs:\n```\n`apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: appservice\n  namespace: monitoring\n  labels:\n    app: appservice\n  annotations:\n    prometheus.io/scrape: 'true'\n    prometheus.io/path: '/actuator/prometheus'\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: appservice\n  template:\n    metadata:\n      labels:\n        app: appservice\n...\n`\n```\n```\n`apiVersion: v1\nkind: Service\nmetadata:\n  name: appservice\n  namespace: monitoring\n  annotations:\n    prometheus.io/scrape: 'true'\n    prometheus.io/path: '/actuator/prometheus'\nspec:\n  selector:\n    app: appservice\n  type: ClusterIP\n  ports:\n    - name: web\n      protocol: TCP\n      port: 8080\n      targetPort: 8080\n    - name: jvm-debug\n      protocol: TCP\n      port: 5005\n      targetPort: 5005\n`\n```\nAnd after app was deployed, I had created ServiceMonitor:\n```\n`apiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: appservice-servicemonitor\n  namespace: monitoring\n  labels:\n    app: appservice\n    release: prometheus-repo\nspec:\n  selector:\n    matchLabels:\n      app: appservice # target app service\n  namespaceSelector:\n    matchNames:\n      - monitoring\n  endpoints:\n  - port: web\n    path: '/actuator/prometheus'\n    interval: 15s\n`\n```\nI expect that after adding this ServiceMonitor, my prometheus instance create new target``` like \"http://appservice:8080/actuator/prometheus\", but it is not, new endpoints doesn't appears in prometheus UI.\nI tried to change helm values by adding additionalServiceMonitors\n```\n`namespaceOverride: \"monitoring\"\nnodeExporter:\n  enabled: true\n\nprometheus:\n  enabled: true\n  prometheusSpec:\n    serviceMonitorSelectorNilUsesHelmValues: false\n    serviceMonitorSelector:\n      matchLabels:\n       release: prometheus-repo\n    additionalServiceMonitors:\n      namespaceSelector:\n        any: true\n    replicas: 1\n    shards: 1\n    storageSpec:\n      ...\n    securityContext:\n      ...\n    nodeSelector:\n      assignment: monitoring\n\n  nodeSelector:\n    assignment: monitoring\n\nprometheusOperator:\n  nodeSelector:\n    assignment: monitoring\n  admissionWebhooks:\n    patch:\n      securityContext:\n        ...\n  securityContext:\n    ...\n\nglobal:\n  alertmanagerSpec:\n    nodeSelector:\n      assignment: monitoring\n`\n```\nBut it didn't help.\nIt is really hard to say what is going wrong, no error logs, all configs applies successfully.",
      "solution": "I found this guide very helpful.\nPlease keep in mind that depending on the prometheus stack you are using labels and names can have different default values (for me, using kube-prometheus-stack, for example the secret name was prometheus-kube-prometheus-stack-prometheus instead of prometheus-k8s).\nEssential quotes:\n\nHas my ServiceMonitor been picked up by Prometheus?\nServiceMonitor objects and the namespace where they belong are selected by the serviceMonitorSelector and serviceMonitorNamespaceSelectorof a Prometheus object. The name of a ServiceMonitor is encoded in the Prometheus configuration, so you can simply grep whether it is present there. The configuration generated by the Prometheus Operator is stored in a Kubernetes Secret, named after the Prometheus object name prefixed with prometheus- and is located in the same namespace as the Prometheus object. For example for a Prometheus object called k8s one can find out if the ServiceMonitor named my-service-monitor has been picked up with:\n```\n`kubectl -n monitoring get secret prometheus-k8s -ojson | jq -r '.data[\"prometheus.yaml.gz\"]' | base64 -d | gunzip | grep \"my-service-monitor\n`\n```",
      "question_score": 5,
      "answer_score": 4,
      "created_at": "2021-07-23T23:40:18",
      "url": "https://stackoverflow.com/questions/68505269/helm-prometheus-operator-doesnt-add-new-servicemonitor-endpoints-to-targets"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "helm",
      "question_id": 69837364,
      "title": "helm configmap in values.yaml",
      "problem": "I am trying to do the following, so where MYVALUE in host needs to change to include Release Name. Can't figure how to do this, as you can't use env variables like `{{ .Release.Name }}` directly in to a values.yaml file.\nI did do a `fullnameOverride` and put `fullnameOverride: myrelease-mysql` for the mysql pod and then jasper has `host: myrelease-mysql` that works  but wanted to know if there was a clever way to put release name into a values.yaml file.\nI assumed I would need to use a configMap as can use `.Release.Name` there and then embed that config key into values.yaml.\nValues.yaml\n```\n`jasperreports:\n  mariadb:\n    enabled: false\n  externalDatabase:\n    host: MYVALUE   // Also tried $MVALUE\n    user: sqluser\n    database: jasper\n  jasperreportsUsername: jasper\n  env:\n      - name: MYVALUE\n        valueFrom:\n          configMapKeyRef:\n              name: mysql-jasper\n              key: mysql_releasename\n`\n```\nConfigMap\n```\n`kind: ConfigMap\nmetadata:\n  name: mysql-jasper\ndata:\n  mysql_releasename: {{ .Release.Name }}-\"mysql\"\n`\n```",
      "solution": "It seems that helm does not support any template rendering capabilities in  a `values.yaml` file - there are multiple topics on the helm GitHub:\n\nCanonical way of using dynamic object names within values.yaml\nAdding values templates in order to customize values with go-template, for the chart and its dependencies\nProposal: Allow templating in values.yaml\n\nFor now this feature is not implemented so you need to find a workaround - the suggestion from David Maze seems to be a good direction, but if you want to follow your approach you can use below workaround using `--set` flag in the `helm install` command or use `sed` command and pipe to `helm install` command.\nFirst solution with `--set` flag.\nMy `values.yaml` file is little bit different than yours:\n`mariadb:\n  enabled: false\nexternalDatabase:\n  user: sqluser\n  database: jasper\njasperreportsUsername: jasper\n`\nThat's because when I was using your `values.yaml` I couldn't manage to apply these values to `bitnami/jasperreports` chart, the `helm install` command was using default values from here.\nI'm setting a shell variable `RELEASE_NAME` which I will use both for setting chart name and `externalDatabase.host` value.\n```\n`RELEASE_NAME=my-test-release\nhelm install $RELEASE_NAME bitnami/jasperreports -f values.yaml --set externalDatabase.host=$RELEASE_NAME-mysql\n`\n```\nThe above `helm install` command will override default values both by setting values from the `values.yaml` file + setting `externalDatabase.host` value.\nBefore applying you can check if this solution works as expected by using `helm template` command:\n```\n`RELEASE_NAME=my-test-release\nhelm template $RELEASE_NAME bitnami/jasperreports -f values.yaml --set externalDatabase.host=$RELEASE_NAME-mysql\n...\n- name: MARIADB_HOST\n  value: \"my-test-release-mariadb\"\n...\n`\n```\nAnother approach is to set a bash variable `RELEASE_NAME` which will be used in the `sed` command to output modified `values.yaml` file (I'm not editing `values.yaml` file itself).  This output will be pipe into a `helm install` command (where I also used the`RELEASE_NAME` variable).\n`values.yaml`:\n`mariadb:\n  enabled: false\nexternalDatabase:\n  host: MYHOST\n  user: sqluser\n  database: jasper\njasperreportsUsername: jasper\n`\n```\n`RELEASE_NAME=my-test-release\nsed \"s/MYHOST/$RELEASE_NAME-mysql/g\" values.yaml | helm install $RELEASE_NAME bitnami/jasperreports -f -\n`\n```\nThis approach will set chart configuration the same as in the first approach.",
      "question_score": 5,
      "answer_score": 3,
      "created_at": "2021-11-04T10:58:02",
      "url": "https://stackoverflow.com/questions/69837364/helm-configmap-in-values-yaml"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "helm",
      "question_id": 67415405,
      "title": "Use kubeconfig for helm client install panding",
      "problem": "Im using the helm SDK and it works great, for my testing using the fake option which works(for kubeconfig),\nNow when I update the `kubeconfig` to my cluster I notice that during installation the chart is stuck on status pending,\nand it stuck forever in this state until I'm deleting & installing it again,(manually)\nMy question is how to solve this issue with the\nHelm SDK (via code only) https://pkg.go.dev/helm.sh/helm/v3,\nI mean wait for a while and if the status is pending after 3 min delete and reinstall it again... or try upgrade before\nThis is the code\n```\n`    kubeConfigPath, err := findKubeConfig()\n    if err != nil {\n        fmt.Println()\n    }\n\n    actionConfig := &action.Configuration{\n    }\n\n    cfg := cli.New()\n    clientGetter := genericclioptions.NewConfigFlags(false)\n    clientGetter.KubeConfig = &kubeConfigPath\n\n    actionConfig.Init(clientGetter, \"def\", \"memory\", log.Printf)\n    if err != nil {\n        fmt.Println(err)\n    }\n\n    chart, err := installation.InstallChart(cfg, \"test\", \"chart1\", \"./charts/dns\", nil, actionConfig)\n    if err != nil {\n        fmt.Println(err)\n    }\n    fmt.Println(chart)\n}\n\nfunc findKubeConfig() (string, error) {\n    env := os.Getenv(\"KUBECONFIG\")\n    if env != \"\" {\n        return env, nil\n    }\n    path, err := homedir.Expand(\"~/.kube/config\")\n    if err != nil {\n        return \"\", err\n    }\n    return path, nil\n}\n`\n```",
      "solution": "Looking at the example, I don't know what the `installation` package is but I feel like you would need to use a `Loader` (maybe you are using it in that pkg)\nfrom a quick search over the github issues I found someone with a similar problem - and they got the same suggestion. here is a derived example:\n`package main\n\nimport (\n    \"fmt\"\n    \"os\"\n\n    \"helm.sh/helm/v3/pkg/action\"\n    \"helm.sh/helm/v3/pkg/chart/loader\"\n    \"helm.sh/helm/v3/pkg/kube\"\n    \"helm.sh/helm/v3/pkg/release\"\n    _ \"k8s.io/client-go/plugin/pkg/client/auth\"\n)\n\nfunc main() {\n    chartPath := \"./charts/dns\"\n    chart, err := loader.Load(chartPath)\n    if err != nil {\n        panic(err)\n    }\n\n    kubeconfigPath := findKubeConfig()\n    releaseName := \"test\"\n    releaseNamespace := \"default\"\n    actionConfig := new(action.Configuration)\n    if err := actionConfig.Init(kube.GetConfig(kubeconfigPath, \"\", releaseNamespace), releaseNamespace, os.Getenv(\"HELM_DRIVER\"), func(format string, v ...interface{}) {\n        fmt.Sprintf(format, v)\n    }); err != nil {\n        panic(err)\n    }\n\n    iCli := action.NewInstall(actionConfig)\n    iCli.Namespace = releaseNamespace\n    iCli.ReleaseName = releaseName\n    rel, err := iCli.Run(chart, nil)\n    if err != nil {\n        panic(err)\n    }\n    fmt.Println(\"Successfully installed release: \", rel.Name)\nrel.Info.Status\n    // check Release Status, feel free to run it in a go routine along the deletetion logic\n    upCli := action.NewUpgrade(actionConfig)  \n    upgradedRel, err := pollAndUpdate(rel.Info.Status, upCli) // see if its better to just run that code here directly :shrug:\n\n// if we still on pending, then delete it\n    if upgradedRel.Info.Status.IsPending() {\n      unCli := action.NewUninstall(actionConfig)\n      res, err := unCli.Run(rel.Name)\n      if err != nil {\n          panic(err)\n      }\n    }\n}\n\nfunc pollAndUpdate(originalRel *release.Release, upgradeCli *action.Upgrade) (*release.Release, error) {\n    if !originalRel.Info.Status.IsPending() {\n        return originalRel, nil\n    }\n    c := time.Tick(10 * time.Second) // we gonna time it out besides checking repeatedly\n    var rel *release.Release = originalRel\n    for _ = range c {\n         //check the status and try and upgrade\n         for rel.Info.Status.IsPending() { // https://pkg.go.dev/helm.sh/helm/v3@v3.5.4/pkg/release#Status.IsPending\n             // run the upgrade command you have\n             // its this function: https://github.com/helm/helm/blob/main/pkg/action/upgrade.go#L111\n             rel, err := upgradeCli.Run(/*you gotta get all the values this needs*/)\n             if err != nil {\n                 panic(err)\n             }\n         }\n    }\n    return rel, nil\n}\n`",
      "question_score": 5,
      "answer_score": 1,
      "created_at": "2021-05-06T11:27:55",
      "url": "https://stackoverflow.com/questions/67415405/use-kubeconfig-for-helm-client-install-panding"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "helm",
      "question_id": 74441381,
      "title": "Datadog: API Key invalid dropping transaction when installing Datadog agent",
      "problem": "I'm trying to install Datadog agent for a Kubernetes cluster using Helm.\nThis is the helm command I'm using for it:\n```\n`helm repo add datadog https://helm.datadoghq.com\n\nhelm repo update\n\nhelm upgrade --install datadog datadog/datadog \\\n  --namespace monitoring \\\n  --create-namespace \\\n  --atomic \\\n  --set datadog.apiKey= \\\n  --set targetSystem=linux \\\n  --values values.yaml\n`\n```\nValues file:\n```\n`datadog:\n  kubelet:\n    host:\n      valueFrom:\n        fieldRef:\n          fieldPath: spec.nodeName\n    hostCAPath: /etc/kubernetes/certs/kubeletserver.crt\n    tlsVerify: false # Required as of Agent 7.35. See Notes.\n`\n```\nHowever, when I run the liveness probe error with error 500 which shows the error below:\n\nCLUSTER | ERROR | (pkg/forwarder/transaction/transaction.go:344 in internalProcess) | API Key invalid, dropping transaction for https://orchestrator.datadoghq.com/api/v1/orchestrator.",
      "solution": "Here's how I solved it:\nThe issue had to do with the Datadog Destination Site. The Destination site for my metrics, traces, and logs is supposed to be `datadoghq.eu`. This is set using the variable `DD_SITE`, and it defaults to `datadoghq.com` if it is not set.\nTo check what your Datadog Destination Site just look at the URL of your Datadog dashboard:\n\nFor US it will be - https://app.datadoghq.com/\nFor EU it will be - https://app.datadoghq.eu/\n\nTo set this in your helm chart simply do either of the following:\n```\n`helm repo add datadog https://helm.datadoghq.com\n\nhelm repo update\n\nhelm upgrade --install datadog datadog/datadog \\\n  --namespace monitoring \\\n  --create-namespace \\\n  --atomic \\\n  --set datadog.apiKey= \\\n  --set targetSystem=linux \\\n  --set datadog.site=datadoghq.eu \\\n  --values values.yaml\n`\n```\nOR set it in your values file:\n```\n`datadog:\n  site: datadoghq.eu\n  kubelet:\n    host:\n      valueFrom:\n        fieldRef:\n          fieldPath: spec.nodeName\n    hostCAPath: /etc/kubernetes/certs/kubeletserver.crt\n    tlsVerify: false # Required as of Agent 7.35. See Notes.\n`\n```\nReferences:\n\nDatadog Agent Forwarder fails liveness probe when new spot instance joins cluster, causing multiple restarts #1697\n\nDD_SITE Set to us3.datadoghq.com, but process-agent and security-agent Still Try to Connect to non us3 endpoints #9180",
      "question_score": 4,
      "answer_score": 20,
      "created_at": "2022-11-15T07:28:47",
      "url": "https://stackoverflow.com/questions/74441381/datadog-api-key-invalid-dropping-transaction-when-installing-datadog-agent"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "helm",
      "question_id": 73204128,
      "title": "Issues migrating from v1beta to v1 for kubernetes ingress",
      "problem": "In my firm our Kubernetes Cluster was recently updated to 1.22+ and we are using AKS. So I had to change the manifest of our ingress yaml file which was using : networking.k8s.io/v1beta1, to be compliant to the new apiVersion : networking.k8s.io/v1\nThis is the earlier manifest for the ingress file :\n```\n`{{- if .Values.ingress.enabled -}}\n{{- $fullName := include \"amroingress.fullname\" . -}}\n{{- $svcPort := .Values.service.port -}}\n{{- if semverCompare \">=1.14-0\" .Capabilities.KubeVersion.GitVersion -}}\napiVersion: networking.k8s.io/v1beta1\n{{- else -}}\napiVersion: extensions/v1beta1\n{{- end }}\nkind: Ingress\nmetadata:\n  name: {{ $fullName }}\n  labels:\n    {{- include \"amroingress.labels\" . | nindent 4 }}\n  {{- with .Values.ingress.annotations }}\n  annotations:\n    {{- toYaml . | nindent 4 }}\n  {{- end }}\nspec:\n  {{- if .Values.ingress.tls }}\n  tls:\n    {{- range .Values.ingress.tls }}\n    - hosts:\n        {{- range .hosts }}\n        - {{ . | quote }}\n        {{- end }}\n      secretName: {{ .secretName }}\n    {{- end }}\n  {{- end }}\n  rules:\n    {{- range .Values.ingress.hosts }}\n    - host: {{ .host | quote }}\n      http:\n        paths:\n          #{{- range .paths }}\n          #- path: {{ . }}\n          #  backend:\n          #    serviceName: {{ $fullName }}\n          #    servicePort: {{ $svcPort }}\n          #{{- end }}\n          - path: /callista/?(.*)\n            backend:\n              serviceName: amro-amroingress\n              servicePort: 8080\n    {{- end }}\n  {{- end }}\n`\n```\nand after my changes it looks like this:\n```\n`{{- if .Values.ingress.enabled -}}\n{{- $fullName := include \"amroingress.fullname\" . -}}\n{{- $svcPort := .Values.service.port -}}\napiVersion: networking.k8s.io/v1\n{{- end }}\nkind: Ingress\nmetadata:\n  name: {{ include \"amroingress.fullname\" . }}\n  labels:\n    {{- include \"amroingress.labels\" . | nindent 4 }}\n  {{- with .Values.ingress.annotations }}\n  annotations:\n    {{- toYaml . | nindent 4 }}\n  {{- end }}\nspec:\n  {{- if .Values.ingress.tls }}\n  tls:\n    {{- range .Values.ingress.tls }}\n    - hosts:\n        {{- range .hosts }}\n        - {{ . | quote }}\n        {{- end }}\n      secretName: {{ .secretName }}\n    {{- end }}\n  {{- end }}\n  rules:\n    {{- range .Values.ingress.hosts }}\n    - host: {{ .host | quote }}\n      http:\n        paths:\n          {{- range .paths }}\n          - path: /callista/?(.*)\n            pathType: Prefix\n            backend:\n              service:\n                name: amro-amroingres\n                port: \n                  number: 8080\n    {{- end }}\n  {{- end }}\n\n`\n```\nBut, after I made the changes and tried to deploy using helm, I receive this error:\n`Error: UPGRADE FAILED: current release manifest contains removed kubernetes api(s) for this kubernetes version and it is therefore unable to build the kubernetes objects for performing the diff. error from kubernetes: unable to recognize \"\": no matches for kind \"Ingress\" in version \"networking.k8s.io/v1beta1\"`\nI am not sure why this error occurs even though the ingress manifest has changed and I have been stuck at this for a few days now. I am new to kubernetes and ingress in general, any help will be massively appreciated.",
      "solution": "After trying out a lot more stuff I just decided to finally use helm unistall to remove the deployments and the charts currently in the cluster.\nI then simply tried to install with the new ingress manifest which I have mentioned in the question and that worked out and was finally able to deploy. So, the manifest itself which I had modified did not have any issues it seems.",
      "question_score": 4,
      "answer_score": 2,
      "created_at": "2022-08-02T10:17:26",
      "url": "https://stackoverflow.com/questions/73204128/issues-migrating-from-v1beta-to-v1-for-kubernetes-ingress"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "helm",
      "question_id": 66029873,
      "title": "helm3 / function &quot;lookup&quot; not defined",
      "problem": "I recently installed `microk8s`, and enabled helm3 and dns addons on microk8s.\nDeployment from `stable/chart` works fine but any deployment from `bitnami/chart` fails.\nOS: Ubuntu 20.04.1 LTS -- microk8s:  1.19/stable\n```\n`microk8s.helm3 install my-release bitnami/jenkins\n=> Error: parse error at (jenkins/charts/common/templates/_secrets.tpl:84): function \"lookup\" not defined\n\nmicrok8s.helm3 install my-release bitnami/magento\n=> Error: parse error at (magento/charts/elasticsearch/charts/common/templates/_secrets.tpl:84): function \"lookup\" not defined\n`\n```",
      "solution": "There was a bug reported here and here which was caused by the conditional inclusion of `lookup` into the function map.\nA fix for it was merged here and is now available from Helm version 3.2.0.\nSo, in order to fix this issue you should update your Helm to version 3.2.0 or newer.",
      "question_score": 4,
      "answer_score": 10,
      "created_at": "2021-02-03T15:48:16",
      "url": "https://stackoverflow.com/questions/66029873/helm3-function-lookup-not-defined"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "helm",
      "question_id": 75926123,
      "title": "Getting [INFO] core: security barrier not initialized : When Deploying Vault using Helm chart",
      "problem": "The error message \"core: security barrier not initialized\" is appearing when trying to deploy Vault using a Helm chart. I haven't setup the seal How to setup a seal\nI'm getting this error. Need a Solution to solve this",
      "solution": "You're getting this message because you haven't initialized Vault yet.  The first thing you'll need to do when Vault is up and running is run the command: `vault operator init`.  If you didn't go about setting up some sort of auto unseal (which it sounds like you didn't), the default seal type is Shamir seal and Vault will give you back 5 unseal keys.  Enter 3 of those keys in with the command `vault operator unseal` and you'll have an unsealed, working Vault instance.",
      "question_score": 4,
      "answer_score": 11,
      "created_at": "2023-04-04T08:30:15",
      "url": "https://stackoverflow.com/questions/75926123/getting-info-core-security-barrier-not-initialized-when-deploying-vault-usi"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "helm",
      "question_id": 65882761,
      "title": "helm3 - upgrade does not refresh pod with force",
      "problem": "We are deploying Java microservices to AWS 'ECR > EKS' using helm3 and Jenkins CI/CD pipeline. However what we see is, if we re-run Jenkins job to re-install the deployment/pod, then the pod does not re-install if there are no code changes. It still keeps the old running pod as is. Use case considered here is, AWS Secrets Manager configuration for db secret pulled during deployment has changed, so service needs to be redeployed by re-triggering the Jenkins job.\nApproach 1 :   https://helm.sh/docs/helm/helm_upgrade/\nI tried using 'helm upgrade --install --force ....' as suggested in helm3 upgrade documentation but it fails with below error in Jenkins log\n\n\"Error: UPGRADE FAILED: failed to replace object: Service \"dbservice\" is invalid: spec.clusterIP: Invalid value: \"\": field is immutable\"\n\nApproach 2 : using --recreate-pods from earlier helm version\nWith 'helm upgrade --install --recreate-pods ....', I  am getting below warning in Jenkins log\n\n\"Flag --recreate-pods has been deprecated, functionality will no longer be updated. Consult the documentation for other methods to recreate pods\"\n\nHowever, the pod gets recreated. But as we know --recreate-pods is not soft-restart. Thus we would have downtime, which breaks the microservice principle.\nhelm version used\nversion.BuildInfo{Version:\"v3.4.0\", GitCommit:\"7090a89efc8a18f3d8178bf47d2462450349a004\", GitTreeState:\"clean\", GoVersion:\"go1.14.10\"}\nquestion\n\nHow to use --force with helm 3 with helm upgrade for above error ?\nHow to achieve soft-restart with deprecated --recreate-pods ?",
      "solution": "Below is how I configured it - Thanks to @vasili-angapov for redirecting to correct documentation section.\nIn `deployment.yaml`, I added annotations and rollme\n```\n`kind: Deployment\nspec:\n  template:\n    metadata:\n      annotations:\n        rollme: {{ randAlphaNum 5 | quote }}\n\n`\n```\nAs per documentation, each invocation of the template function `randAlphaNum` will generate a unique random string. Thus random string always changes and causes the deployment to roll.\nThe other way described in the document is with respect to a changing SHA value for a file.\nIn the past helm recommended using the --recreate-pods flag as another option. This flag has been marked as `deprecated in Helm 3` in favor of the more declarative method above.",
      "question_score": 4,
      "answer_score": 6,
      "created_at": "2021-01-25T11:15:24",
      "url": "https://stackoverflow.com/questions/65882761/helm3-upgrade-does-not-refresh-pod-with-force"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "helm",
      "question_id": 67617808,
      "title": "Helm: How to avoid recreating secrets on upgrade?",
      "problem": "I have something in a secret template like this:\n```\n`apiVersion: v1\nkind: Secret\nmetadata:\n  # not relevant\ntype: Opaque\ndata:\n  password: {{ randAlphaNum 32 | b64enc | quote }}\n`\n```\nNow, when doing `helm upgrade`, the secret is recreated, but the pods using this aren't (they also shouldn't, this is OK).\nThis causes the pods to fail when they are restarted or upgraded as the new password now doesn't match the old one.\nIs it possible to skip re-creation of the secret when it exists, like, a `{{- if not(exists theSecret) }}` and how to do it?",
      "solution": "You can use the look up function in HELM to check the if secret exist or not\nhttps://helm.sh/docs/chart_template_guide/functions_and_pipelines/#using-the-lookup-function\nFunction in helm chart goes like : https://github.com/sankalp-r/helm-charts-examples/blob/1081ab5a5af3a1c7924c826c5a2bed4c19889daf/sample_chart/templates/_helpers.tpl#L67\n```\n`{{/*\nExample for function\n*/}}\n{{- define \"gen.secret\" -}}\n{{- $secret := lookup \"v1\" \"Secret\" .Release.Namespace \"test-secret\" -}}\n{{- if $secret -}}\n{{/*\n   Reusing value of secret if exist\n*/}}\npassword: {{ $secret.data.password }}\n{{- else -}}\n{{/*\n    add new data\n*/}}\npassword: {{ randAlphaNum 32 | b64enc | quote }}\n{{- end -}}\n{{- end -}}\n`\n```\nsecret creation will be something like\nexample file : https://github.com/sankalp-r/helm-charts-examples/blob/main/sample_chart/templates/secret.yaml\n```\n`apiVersion: v1\nkind: Secret\nmetadata:\n  name: \"test-secret\"\ntype: Opaque\ndata:\n{{- ( include \"gen.secret\" . ) | indent 2 -}}\n`\n```\nchart example : https://github.com/sankalp-r/helm-charts-examples\n```\n`{{- $secret := (lookup \"v1\" \"Secret\" .Release.Namespace \"test-secret\" -}}\napiVersion: v1\nkind: Secret\nmetadata:\n  name: test-secret\ntype: Opaque\n\n# 2. If the secret exists, write it back\n{{ if $secret -}}\ndata:\n  password: {{ $secret.data.password }}\n\n# 3. If it doesn't exist ... create new\n{{ else -}}\nstringData:\n  password: {{ randAlphaNum 32 | b64enc | quote }}\n{{ end }}\n`\n```",
      "question_score": 4,
      "answer_score": 10,
      "created_at": "2021-05-20T11:37:55",
      "url": "https://stackoverflow.com/questions/67617808/helm-how-to-avoid-recreating-secrets-on-upgrade"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "helm",
      "question_id": 70675176,
      "title": "Install HorizontalPodAutoscaler (HPA) using Helm",
      "problem": "I'd like to do all k8s installation, configuration, and maintenance using Helm v3 (v3.7.2).\nThus I have setup yaml templates for:\n\ndeployment\nconfigmap\nservice\ningress\n\nYet I can't find any information in the Helm v3 docs on setting up an HPA (HorizontalPodAutoscaler). Can this be done using an hpa.yaml that pulls from values.yaml?",
      "solution": "Yes. Example, try `helm create nginx` will create a template project call \"nginx\", and inside the \"nginx\" directory you will find a templates/hpa.yaml example. Inside the values.yaml -> autoscaling is what control the HPA resources:\n```\n`autoscaling:\n  enabled: false  # <-- change to true to create HPA\n  minReplicas: 1\n  maxReplicas: 100\n  targetCPUUtilizationPercentage: 80\n  # targetMemoryUtilizationPercentage: 80\n`\n```",
      "question_score": 4,
      "answer_score": 5,
      "created_at": "2022-01-12T02:00:02",
      "url": "https://stackoverflow.com/questions/70675176/install-horizontalpodautoscaler-hpa-using-helm"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "helm",
      "question_id": 67732738,
      "title": "Helm configMap support for binary files",
      "problem": "I am trying to create a helm chart of kind ConfigMap that will replace the following command from kubernates.\n```\n`kubectl create configmap my-config -n $namespace --from-file=./my-directory\n`\n```\n`my-directory` contains around 5 files, 2 of them are properties file and 2 of them jpg file. I see the following result for `kubectl get cm`, I can see `4` DATA files in configMap\n```\n`[admin@cluster ~]$ kubectl get cm\nNAME                   DATA   AGE\nwarm-up-config         4      41m\n`\n```\nI created a template as follows, it work if I specify only properties file but If I add jpg files it doesn't work at all\n```\n`apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: my-config\ndata:\n{{ (.Files.Glob \"resources/*\").AsConfig | nindent 2 }}\n`\n```\nDoes anyone know how I make this work.",
      "solution": "JPG files are binary, and should be added as such.\n```\n`data:\n  binaryData:\n    {{ .Files.Get \"/path/to/file.jpg\" }}\n`\n```\nFiles in `binaryData` field must be encoded with base64, so:\n```\n`{{ .Files.Get \"/path/to/file.jpg\" | b64enc }}\n`\n```\nDon't forget proper indentation:\n```\n`{{ .Files.Get \"/path/to/file.jpg\" | b64enc | nindent 4 }}\n`\n```",
      "question_score": 4,
      "answer_score": 8,
      "created_at": "2021-05-28T06:00:09",
      "url": "https://stackoverflow.com/questions/67732738/helm-configmap-support-for-binary-files"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "helm",
      "question_id": 72099540,
      "title": "How to range optional array on Helm",
      "problem": "Consider the following template:\n`...\n    {{- range .Values.additionalMetrics }}\n    - interval: 1m\n      port: {{ .name }}\n    {{- end }}\n...\n`\nAnd the following values:\n`additionalMetrics:\n  - name: kamon-metrics\n    port: 9096\n    targetPort: 9096\n`\nIf `additionalMetrics` is missing, the helm template will fail.\nIs it possible to check first if `additionalMetrics` is defined, and then the range the values or continue otherwise?\nNote: without making first if and then range, but in one condition, for example this is a not my desired solution:\n```\n`    {{- if .Values.additionalMetrics }}\n    {{- range .Values.additionalMetrics }}\n    - name:       {{ .name }}\n      port:       {{ .port }}\n      targetPort: {{ .targetPort }}\n    {{- end }}\n    {{- end }}\n`\n```\nThanks in advvance",
      "solution": "The solution, which is not desired by you, is alright imo. It's simple and does what it's supposed to do. There is no need to make things complicated.\nYou could make it a bit more pretty with a `with` clause:\n`{{- with .Values.additionalMetrics }}\n{{- range . }}\n- name:       {{ .name }}\n  port:       {{ .port }}\n  targetPort: {{ .targetPort }}\n{{- end }}\n{{- end }}\n`\nIf you really want to do it in a single statement, you could use as `default` an empty list:\n`{{- range .Values.additionalMetrics | default list }}\n- name:       {{ .name }}\n  port:       {{ .port }}\n  targetPort: {{ .targetPort }}\n{{- end }}\n`",
      "question_score": 4,
      "answer_score": 8,
      "created_at": "2022-05-03T14:42:31",
      "url": "https://stackoverflow.com/questions/72099540/how-to-range-optional-array-on-helm"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "helm",
      "question_id": 69059086,
      "title": "How can I pass imagePullSecrets, as it is defined in the default template, to helm via set commands",
      "problem": "When you run `helm create mychart` it has imagePullSecrets defined like this:\n```\n`spec:\n  {{- with .Values.imagePullSecrets }}\n  imagePullSecrets:\n    {{- toYaml . | nindent 8 }}\n  {{- end }\n`\n```\nIn default values file it looks like it's passing it a blank array:\n```\n`imagePullSecrets: []\n`\n```\nI already have a bunch of charts built from this default template that have this setting. Previously I didn't need to use imagePullSecrets so I just left it as is, but now I have some cases where I want to set this at deploy time via the cli.\nHelm supports arrays now but this doesn't seem to work:\n```\n`--set \"mychart.imagePullSecrets[0].name={reg-creds}\"\n`\n```\nReturns:\n```\n`Error: UPGRADE FAILED: error validating \"\": error validating data: ValidationError(Deployment.spec.template.spec.imagePullSecrets[0].name): invalid type for io.k8s.api.core.v1.LocalObjectReference.name: got \"array\", expected \"string\"\n`\n```\nThen I tried passing a string:\n`--set \"mychart.imagePullSecrets='- name: reg-creds'\" `\n```\n`Error: unable to build kubernetes objects from release manifest: error validating \"\": error validating data: ValidationError(Deployment.spec.template.spec.imagePullSecrets): invalid type for io.k8s.api.core.v1.PodSpec.imagePullSecrets: got \"string\", expected \"array\"\n`\n```\nThese error messages are infuriating. Is it possible to set this value with `--set` so I can avoid refactoring all my charts?",
      "solution": "The `helm install --set` syntax is unique and complex.  One unusual bit of syntax there is that a value `{foo,bar}` in curly braces sets the value to an array.  In your example, then, `--set object.path={value}` sets the value to a single-element array; the error you see is that it needs to be a string instead.\nThat means a simple workaround here is to remove the curly braces on the right-hand side of `--set`.  There is also a `--set-string` option that forces the value to be interpreted as a string, even if it contains curly braces or commas.\n`helm install ... --set \"mychart.imagePullSecrets[0].name=reg-creds\"\n#                       no curly braces around the value ^^^^^^^^^\n`\nIt might be clearer, and have a more standard syntax, to use a YAML file to provide this value instead.\n`# image-pull-secrets.yaml\nimagePullSecrets:\n  - name: reg-creds\n`\nYou can include this in a per-environment values file, or pass it as a standalone values file.  In either case you'd use the `helm install -f` option to supply the file.  It's fine to have multiple `helm install -f` values files.\n`helm install ... -f image-pull-secrets.yaml\n`",
      "question_score": 4,
      "answer_score": 8,
      "created_at": "2021-09-04T23:32:10",
      "url": "https://stackoverflow.com/questions/69059086/how-can-i-pass-imagepullsecrets-as-it-is-defined-in-the-default-template-to-he"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "helm",
      "question_id": 76966845,
      "title": "NGINX Ingress with Helm - configure rate limiting response code",
      "problem": "I'm trying to set the `limit-req-status-code` for my nginx ingress, but I'm failing to do so. According to the docs, this setting belongs in a ConfigMap (as opposed to other rate limiting settings that are annotations). I created a configmap, but the setting doesn't get respected. How do I know? I've used fortio to run into the rate limit and it's still returning 503.\nI've tried finding out how to name the map correctly and tried a lot of different names like suggested in these answers, no effect. I also tried to manually pass the configmap name to the helm call, no luck either. That's what I have right now:\ningress-configmap.yaml\n```\n`apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: nginx-ingress-config\ndata:\n  limit-req-status-code: \"429\"\n\n`\n```\ningress.yaml\n```\n`apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: {{ include \"fullname\" . }}-ingress\n  labels:\n    app.kubernetes.io/name: {{ include \"name\" . }}\n    app.kubernetes.io/instance: {{ .Release.Name }}\n    app.kubernetes.io/version: \"{{ .Release.Revision }}\"\n    app.kubernetes.io/managed-by: {{ .Release.Service }}\n    helm.sh/chart: {{ .Chart.Name }}-{{ .Chart.Version | replace \"+\" \"_\" }}\n  annotations:\n    nginx.ingress.kubernetes.io/force-ssl-redirect: \"true\"\n    nginx.ingress.kubernetes.io/limit-rpm: \"1000\"\n    nginx.ingress.kubernetes.io/limit-rps: \"100\"\nspec:\n  ingressClassName: \"nginx-public\"\n  rules:\n    - host: {{ .Values.ingress.host }}\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: {{ include \"fullname\" . }}-service\n                port:\n                  name: http\n`\n```\ndeploy call\n```\n`helm upgrade ${{ inputs.release-name }} ${{ inputs.working-directory }} \\\n          --install \\\n          --namespace=${{ inputs.aws-role-name }} \\\n          --wait \\\n          --timeout=5m0s \\\n          --atomic \\\n          --values=${{ inputs.working-directory }}/values.yaml \\\n          --values=${{ inputs.working-directory }}/values-${{ inputs.stage-name }}.yaml \\\n          --set controller.config.name=nginx-ingress-config \\\n          --set-string deployment.image.registry=\"${{ secrets.mgmt-aws-account-id }}.dkr.ecr.${{ inputs.mgmt-aws-region }}.amazonaws.com\" \\\n          --set-string deployment.image.repository=\"${{ inputs.image-name }}\" \\\n          --set-string deployment.image.digest=\"${{ inputs.image-digest }}\" \\\n          --set-string database.user='${{ steps.fetch-secret-postgres-username.outputs.aws-secret-value }}' \\\n          --set-string database.password='${{ steps.fetch-secret-postgres-password.outputs.aws-secret-value }}'\n`\n```\nWhat am I doing wrong?",
      "solution": "Ok so I found a solution in another question: I can pass the setting as a configuration snippet in an annotation:\n```\n`apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: {{ include \"fullname\" . }}-ingress\n  labels:\n    app.kubernetes.io/name: {{ include \"name\" . }}\n    app.kubernetes.io/instance: {{ .Release.Name }}\n    app.kubernetes.io/version: \"{{ .Release.Revision }}\"\n    app.kubernetes.io/managed-by: {{ .Release.Service }}\n    helm.sh/chart: {{ .Chart.Name }}-{{ .Chart.Version | replace \"+\" \"_\" }}\n  annotations:\n    nginx.ingress.kubernetes.io/force-ssl-redirect: \"true\"\n    nginx.ingress.kubernetes.io/limit-rpm: \"{{ .Values.ingress.requestsPerMinute }}\"\n    nginx.ingress.kubernetes.io/limit-rps: \"{{ .Values.ingress.requestsPerSecond }}\"\n    nginx.ingress.kubernetes.io/configuration-snippet: |\n      limit_req_status 429;                                      The configmap and the configured controller.config.name are not necessary.",
      "question_score": 4,
      "answer_score": 7,
      "created_at": "2023-08-24T08:54:26",
      "url": "https://stackoverflow.com/questions/76966845/nginx-ingress-with-helm-configure-rate-limiting-response-code"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "helm",
      "question_id": 73833680,
      "title": "How to load local file with cdktf for helm release?",
      "problem": "I'd like to reference a local yaml file when creating a helm chart with cdktf.\nI have the following cdktf config:\n```\n`{\n  \"language\": \"typescript\",\n  \"app\": \"npx ts-node main.ts\",\n  \"projectId\": \"...\",\n  \"terraformProviders\": [\n    \"hashicorp/aws@~> 3.42\",\n    \"hashicorp/kubernetes@ ~> 2.7.0\",\n    \"hashicorp/http@ ~> 2.1.0\",\n    \"hashicorp/tls@ ~> 3.1.0\",\n    \"hashicorp/helm@ ~> 2.4.1\",\n    \"hashicorp/random@ ~> 3.1.0\",\n    \"gavinbunney/kubectl@ ~> 1.14.0\"\n  ],\n  \"terraformModules\": [\n    {\n      \"name\": \"secrets-store-csi\",\n      \"source\": \"app.terraform.io/goldsky/secrets-store-csi/aws\",\n      \"version\": \"0.1.5\"\n    }\n  ],\n  \"context\": {\n    \"excludeStackIdFromLogicalIds\": \"true\",\n    \"allowSepCharsInLogicalIds\": \"true\"\n  }\n}\n`\n```\nNote `npx ts-node main.ts` as the app.\nIn main.ts I have the following helm release\n```\n`    new helm.Release(this, \"datadog-agent\", {\n      chart: \"datadog\",\n      name: \"datadog\",\n      repository: \"https://helm.datadoghq.com\",\n      version: \"3.1.3\",\n      set: [\n        {\n          name: \"datadog.clusterChecks.enabled\",\n          value: \"true\",\n        },\n        {\n          name: \"clusterAgent.enabled\",\n          value: \"true\"\n        },\n      ],\n      values: [\"${file(\\\"datadog-values.yaml\\\")}\"],\n    });\n`\n```\nNote that I'm referencing a yaml file called `datadog-values.yaml` similar to this example from the helm provider.\n`datadog-values.yaml` is a sister file to `main.ts`\n\nHowever, when I try to deploy this with `cdktf deploy` I get the following error\n```\n`\u2502 Error: Invalid function argument\n\u2502\n\u2502   on cdk.tf.json line 1017, in resource.helm_release.datadog-agent.values:\n\u2502 1017:           \"${file(\\\"datadog-values.yaml\\\")}\"\n\u2502\n\u2502 Invalid value for \"path\" parameter: no file exists at\n\u2502 \"datadog-values.yaml\"; this function works only with files that are\n\u2502 distributed as part of the configuration source code, so if this file will\n\u2502 be created by a resource in this configuration you must instead obtain this\ngoldsky-infra-dev  \u2577\n                   \u2502 Error: Invalid function argument\n                   \u2502\n                   \u2502   on cdk.tf.json line 1017, in resource.helm_release.datadog-agent (datadog-agent).values:\n                   \u2502 1017:           \"${file(\\\"datadog-values.yaml\\\")}\"\n                   \u2502\n                   \u2502 Invalid value for \"path\" parameter: no file exists at\n                   \u2502 \"datadog-values.yaml\"; this function works only with files that are\n                   \u2502 distributed as part of the configuration source code, so if this file will\n                   \u2502 be created by a resource in this configuration you must instead obtain this\n                   \u2502 result from an attribute of that resource.\n`\n```\nTo run a deployment I execute `npm run deploy:dev` which is a customer script in my `package.json`:\n```\n`    \"build\": \"tsc\",\n    \"deploy:dev\": \"npm run build && npx cdktf deploy\",\n`\n```\nHow can I reference my datadog yaml file in a helm release like in the example shown by the helm provider?",
      "solution": "To reference local files in CDKTF, you need to use assets. Assuming at the root level of your project there's a values folder where you store your values yaml file:\n```\n`     const valuesAsset = new TerraformAsset(this, 'values-asset', {\n        path: `${process.cwd()}/values/${this.chartValues}`,\n        type: AssetType.FILE,\n      });\n\n      new HelmRelease(this, 'helm-release', {\n        name: this.releaseName,\n        chart: this.chartName,\n        repository: this.chartRepository,\n        values: [ Fn.file(valuesAsset.path) ]\n      })\n    }\n`\n```\nNote that I've used the file Terraform function to read the content of the file.",
      "question_score": 4,
      "answer_score": 7,
      "created_at": "2022-09-24T00:58:50",
      "url": "https://stackoverflow.com/questions/73833680/how-to-load-local-file-with-cdktf-for-helm-release"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "helm",
      "question_id": 69878685,
      "title": "ingress-nginx not working when using ingressClassName instead of kubernetes.io/ingress.class in annotations",
      "problem": "I have a baremetal cluster deployed using Kubespray with kubernetes 1.22.2, MetalLB, and ingress-nginx enabled. I am getting `404 Not found` when trying to access any service deployed via helm when setting `ingressClassName: nginx`. However, everything works fine if I don't use `ingressClassName: nginx` but `kubernetes.io/ingress.class: nginx` instead in the helm chart values.yaml. How can I get it to work using `ingressClassName`?\nThese are my kubespray settings for `inventory/mycluster/group_vars/k8s_cluster/addons.yml`\n```\n`# Nginx ingress controller deployment\ningress_nginx_enabled: true\ningress_nginx_host_network: false\ningress_publish_status_address: \"\"\ningress_nginx_nodeselector:\n  kubernetes.io/os: \"linux\"\ningress_nginx_tolerations:\n  - key: \"node-role.kubernetes.io/master\"\n    operator: \"Equal\"\n    value: \"\"\n    effect: \"NoSchedule\"\n  - key: \"node-role.kubernetes.io/control-plane\"\n    operator: \"Equal\"\n    value: \"\"\n    effect: \"NoSchedule\"\ningress_nginx_namespace: \"ingress-nginx\"\ningress_nginx_insecure_port: 80\ningress_nginx_secure_port: 443\ningress_nginx_configmap:\n  map-hash-bucket-size: \"128\"\n  ssl-protocols: \"TLSv1.2 TLSv1.3\"\ningress_nginx_configmap_tcp_services:\n  9000: \"default/example-go:8080\"\ningress_nginx_configmap_udp_services:\n  53: \"kube-system/coredns:53\"\ningress_nginx_extra_args:\n  - --default-ssl-certificate=default/mywildcard-tls\ningress_nginx_class: \"nginx\"\n`\n```\ngrafana helm values.yaml\n```\n`ingress:\n  enabled: true\n  # For Kubernetes >= 1.18 you should specify the ingress-controller via the field ingressClassName\n  # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress\n  ingressClassName: nginx\n  # Values can be templated\n  annotations:\n  #  kubernetes.io/ingress.class: nginx\n  #  kubernetes.io/tls-acme: \"true\"\n  labels: {}\n  path: /\n\n  # pathType is only for k8s >= 1.1=\n  pathType: Prefix\n\n  hosts:\n    - grafana.mycluster.org\n  tls:\n   - secretName: mywildcard-tls\n     hosts:\n       - grafana.mycluster.org\n`\n```\n`kubectl describe pod grafana-679bbfd94-p2dd7`\n```\n`...\nEvents:\n  Type     Reason     Age                From               Message\n  ----     ------     ----               ----               -------\n  Normal   Scheduled  25m                default-scheduler  Successfully assigned default/grafana-679bbfd94-p2dd7 to node1\n  Normal   Pulled     25m                kubelet            Container image \"grafana/grafana:8.2.2\" already present on machine\n  Normal   Created    25m                kubelet            Created container grafana\n  Normal   Started    25m                kubelet            Started container grafana\n  Warning  Unhealthy  24m (x3 over 25m)  kubelet            Readiness probe failed: Get \"http://10.233.90.33:3000/api/health\": dial tcp 10.233.90.33:3000: connect: connection refused\n`\n```\n`kubectl get svc`\n```\n`NAME         TYPE           CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE\ngrafana      LoadBalancer   10.233.14.90   10.10.30.52   80:30285/TCP   55m\nkubernetes   ClusterIP      10.233.0.1             443/TCP        9d\n`\n```\n`kubectl get ing` (no node address assigned)\n```\n`NAME      CLASS   HOSTS                    ADDRESS   PORTS     AGE\ngrafana   nginx   grafana.mycluster.org             80, 443   25m\n`\n```\n`kubectl describe ing grafana` (no node address assigned)\n```\n`Name:             grafana\nNamespace:        default\nAddress:\nDefault backend:  default-http-backend:80 ()\nTLS:\n  mywildcard-tls terminates grafana.mycluster.org\nRules:\n  Host                    Path  Backends\n  ----                    ----  --------\n  grafana.mycluster.org\n                          /   grafana:80 (10.233.90.33:3000)\nAnnotations:              meta.helm.sh/release-name: grafana\n                          meta.helm.sh/release-namespace: default\nEvents:                   \n`\n```\n`kubectl get all --all-namespaces`\n```\n`NAMESPACE        NAME                                                              READY   STATUS    RESTARTS   AGE\ndefault          pod/grafana-b988b9b6-pxccw                                        1/1     Running   0          2m53s\ndefault          pod/nfs-client-nfs-subdir-external-provisioner-68f44cd9f4-wjlpv   1/1     Running   0          17h\ningress-nginx    pod/ingress-nginx-controller-6m2vt                                1/1     Running   0          17h\ningress-nginx    pod/ingress-nginx-controller-xkgxl                                1/1     Running   0          17h\nkube-system      pod/calico-kube-controllers-684bcfdc59-kmsst                      1/1     Running   0          17h\nkube-system      pod/calico-node-dhlnt                                             1/1     Running   0          17h\nkube-system      pod/calico-node-r8ktz                                             1/1     Running   0          17h\nkube-system      pod/coredns-8474476ff8-9sbwh                                      1/1     Running   0          17h\nkube-system      pod/coredns-8474476ff8-fdgcb                                      1/1     Running   0          17h\nkube-system      pod/dns-autoscaler-5ffdc7f89d-vskvq                               1/1     Running   0          17h\nkube-system      pod/kube-apiserver-node1                                          1/1     Running   0          17h\nkube-system      pod/kube-controller-manager-node1                                 1/1     Running   1          17h\nkube-system      pod/kube-proxy-hbjz6                                              1/1     Running   0          16h\nkube-system      pod/kube-proxy-lfqzt                                              1/1     Running   0          16h\nkube-system      pod/kube-scheduler-node1                                          1/1     Running   1          17h\nkube-system      pod/kubernetes-dashboard-548847967d-qqngw                         1/1     Running   0          17h\nkube-system      pod/kubernetes-metrics-scraper-6d49f96c97-2h7hc                   1/1     Running   0          17h\nkube-system      pod/nginx-proxy-node2                                             1/1     Running   0          17h\nkube-system      pod/nodelocaldns-64cqs                                            1/1     Running   0          17h\nkube-system      pod/nodelocaldns-t5vv6                                            1/1     Running   0          17h\nkube-system      pod/registry-proxy-kljvw                                          1/1     Running   0          17h\nkube-system      pod/registry-proxy-nz4qk                                          1/1     Running   0          17h\nkube-system      pod/registry-xzh9d                                                1/1     Running   0          17h\nmetallb-system   pod/controller-77c44876d-c92lb                                    1/1     Running   0          17h\nmetallb-system   pod/speaker-fkjqp                                                 1/1     Running   0          17h\nmetallb-system   pod/speaker-pqjgt                                                 1/1     Running   0          17h\n\nNAMESPACE     NAME                                TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)                  AGE\ndefault       service/grafana                     LoadBalancer   10.233.1.104    10.10.30.52   80:31116/TCP             2m53s\ndefault       service/kubernetes                  ClusterIP      10.233.0.1              443/TCP                  17h\nkube-system   service/coredns                     ClusterIP      10.233.0.3              53/UDP,53/TCP,9153/TCP   17h\nkube-system   service/dashboard-metrics-scraper   ClusterIP      10.233.35.124           8000/TCP                 17h\nkube-system   service/kubernetes-dashboard        ClusterIP      10.233.32.133           443/TCP                  17h\nkube-system   service/registry                    ClusterIP      10.233.30.221           5000/TCP                 17h\n\nNAMESPACE        NAME                                      DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE\ningress-nginx    daemonset.apps/ingress-nginx-controller   2         2         2       2            2           kubernetes.io/os=linux   17h\nkube-system      daemonset.apps/calico-node                2         2         2       2            2           kubernetes.io/os=linux   17h\nkube-system      daemonset.apps/kube-proxy                 2         2         2       2            2           kubernetes.io/os=linux   17h\nkube-system      daemonset.apps/nodelocaldns               2         2         2       2            2           kubernetes.io/os=linux   17h\nkube-system      daemonset.apps/registry-proxy             2         2         2       2            2                              17h\nmetallb-system   daemonset.apps/speaker                    2         2         2       2            2           kubernetes.io/os=linux   17h\n\nNAMESPACE        NAME                                                         READY   UP-TO-DATE   AVAILABLE   AGE\ndefault          deployment.apps/grafana                                      1/1     1            1           2m53s\ndefault          deployment.apps/nfs-client-nfs-subdir-external-provisioner   1/1     1            1           17h\nkube-system      deployment.apps/calico-kube-controllers                      1/1     1            1           17h\nkube-system      deployment.apps/coredns                                      2/2     2            2           17h\nkube-system      deployment.apps/dns-autoscaler                               1/1     1            1           17h\nkube-system      deployment.apps/kubernetes-dashboard                         1/1     1            1           17h\nkube-system      deployment.apps/kubernetes-metrics-scraper                   1/1     1            1           17h\nmetallb-system   deployment.apps/controller                                   1/1     1            1           17h\n\nNAMESPACE        NAME                                                                    DESIRED   CURRENT   READY   AGE\ndefault          replicaset.apps/grafana-b988b9b6                                        1         1         1       2m53s\ndefault          replicaset.apps/nfs-client-nfs-subdir-external-provisioner-68f44cd9f4   1         1         1       17h\nkube-system      replicaset.apps/calico-kube-controllers-684bcfdc59                      1         1         1       17h\nkube-system      replicaset.apps/coredns-8474476ff8                                      2         2         2       17h\nkube-system      replicaset.apps/dns-autoscaler-5ffdc7f89d                               1         1         1       17h\nkube-system      replicaset.apps/kubernetes-dashboard-548847967d                         1         1         1       17h\nkube-system      replicaset.apps/kubernetes-metrics-scraper-6d49f96c97                   1         1         1       17h\nkube-system      replicaset.apps/registry                                                1         1         1       17h\nmetallb-system   replicaset.apps/controller-77c44876d                                    1         1         1       17h\n`\n```\n`kubectl get ing grafana -o yaml`\n```\n`apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  annotations:\n    kubernetes.io/tls-acme: \"true\"\n    meta.helm.sh/release-name: grafana\n    meta.helm.sh/release-namespace: default\n    nginx.ingress.kubernetes.io/force-ssl-redirect: \"true\"\n    nginx.ingress.kubernetes.io/ssl-redirect: \"true\"\n  creationTimestamp: \"2021-11-11T07:16:12Z\"\n  generation: 1\n  labels:\n    app.kubernetes.io/instance: grafana\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/version: 8.2.2\n    helm.sh/chart: grafana-6.17.5\n  name: grafana\n  namespace: default\n  resourceVersion: \"3137\"\n  uid: 6c34d3bd-9ab6-42fe-ac1b-7620a9566f62\nspec:\n  ingressClassName: nginx\n  rules:\n  - host: grafana.mycluster.org\n    http:\n      paths:\n      - backend:\n          service:\n            name: ssl-redirect\n            port:\n              name: use-annotation\n        path: /*\n        pathType: Prefix\n      - backend:\n          service:\n            name: grafana\n            port:\n              number: 80\n        path: /\n        pathType: Prefix\nstatus:\n  loadBalancer: {}\n`\n```",
      "solution": "Running `kubectl get ingressclass` returned 'No resources found'.\n\nThat's the main reason of your issue.\nWhy?\nWhen you are specifying `ingressClassName: nginx` in your Grafana `values.yaml` file you are setting your Ingress resource to use `nginx` Ingress class which does not exist.\nI replicated your issue using minikube, MetalLB and NGINX Ingress installed via modified deploy.yaml file with commented `IngressClass` resource + set NGINX Ingress controller name to `nginx` as in your example. The result was exactly the same - `ingressClassName: nginx` didn't work (no address), but annotation `kubernetes.io/ingress.class: nginx` worked.\n\n(For the below solution I'm using controller pod name `ingress-nginx-controller-86c865f5c4-qwl2b`, but in your case it will be different - check it using `kubectl get pods -n ingress-nginx` command. Also keep in mind it's kind of a workaround - usually `ingressClass` resource should be installed automatically with a whole installation of NGINX Ingress. I'm presenting this solution to understand why it's not worked for you before, and why it works with NGINX Ingress installed using helm)\nIn the logs of the Ingress NGINX controller I found (`kubectl logs ingress-nginx-controller-86c865f5c4-qwl2b -n ingress-nginx`):\n```\n`\"Ignoring ingress because of error while validating ingress class\" ingress=\"default/minimal-ingress\" error=\"no object matching key \\\"nginx\\\" in local store\"\n`\n```\nSo it's clearly shown that there is no matching key to `nginx` controller class - because there is no `ingressClass` resource which is the \"link\" between the NGINX Ingress controller and running Ingress resource.\nYou can verify which name of controller class is bidden to controller by running `kubectl get pod ingress-nginx-controller-86c865f5c4-qwl2b -n ingress-nginx -o yaml`:\n`...\nspec:\n  containers:\n  - args:\n    - /nginx-ingress-controller\n    - --publish-service=$(POD_NAMESPACE)/ingress-nginx-controller\n    - --election-id=ingress-controller-leader\n    - --controller-class=k8s.io/nginx\n...\n`\nNow I will create and apply following Ingress class resource:\n`apiVersion: networking.k8s.io/v1\nkind: IngressClass\nmetadata:\n  name: nginx\nspec:\n  controller: k8s.io/nginx\n`\nNow in the logs I can see that it's properly configured:\n```\n`I1115 12:13:42.410384       7 main.go:101] \"successfully validated configuration, accepting\" ingress=\"minimal-ingress/default\"\nI1115 12:13:42.420408       7 store.go:371] \"Found valid IngressClass\" ingress=\"default/minimal-ingress\" ingressclass=\"nginx\"\nI1115 12:13:42.421487       7 event.go:282] Event(v1.ObjectReference{Kind:\"Ingress\", Namespace:\"default\", Name:\"minimal-ingress\", UID:\"c708a672-a8dd-45d3-a2ec-f2e2881623ea\", APIVersion:\"networking.k8s.io/v1\", ResourceVersion:\"454362\", FieldPath:\"\"}): type: 'Normal' reason: 'Sync' Scheduled for sync\n`\n```\nI re-applied the ingress resource definition, I get IP address for Ingress resource.\n\nAs I said before, instead of using this workaround, I'd suggest installing the NGINX Ingress resource using a solution that automatically installs `IngressClass` as well. As you have chosen helm chart, it has Ingress Class resource so the problem is gone. Other possible ways to install are here.",
      "question_score": 4,
      "answer_score": 7,
      "created_at": "2021-11-08T05:32:21",
      "url": "https://stackoverflow.com/questions/69878685/ingress-nginx-not-working-when-using-ingressclassname-instead-of-kubernetes-io-i"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "helm",
      "question_id": 69787881,
      "title": "Dropping metrics from the kubernetes-service-endpoints job in Prometheus",
      "problem": "I'm using the kube-prometheus-stack chart and I'm trying to get rid of all the `kube_secret` metrics.\nWhen I query these metrics I see that they originate from a job named `kubernetes-service-endpoints`, but I can't figure out what service monitor controls it in `values.yaml`, so I tried dropping them from every single configurable service monitor - in each one of them I put:\n`    metricRelabelings:\n    - action: drop\n      regex: 'kube_secret_.+'\n      sourceLabels: [__name__]\n`\nDidn't help of course. What do I need to define in `values.yaml` in order to drop those metrics?\nThanks",
      "solution": "I'm silly - I defined the `kubernetes-service-endpoints` job in `prometheus.prometheusSpec.additionalScrapeConfigs`...\nIn order to remove all `kube_secret` metrics I dropped metric names that matched `kube_secret_.+` for this job and in the service monitor of `kubeStateMetrics` (`kubeStateMetrics.serviceMonitor.metricRelabelings`)",
      "question_score": 4,
      "answer_score": 7,
      "created_at": "2021-10-31T16:10:21",
      "url": "https://stackoverflow.com/questions/69787881/dropping-metrics-from-the-kubernetes-service-endpoints-job-in-prometheus"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "helm",
      "question_id": 68352522,
      "title": "How do I concatenate strings inside a variable assignment?",
      "problem": "I'm trying to assign the output of a function to a var and append to the string at the same time but can't figure out the right syntax.\nNone of these examples work:\n```\n`{{- $myvar := include \"mychart.helper\" . \"-myprefix\" -}}\n{{- $myvar := {{include \"mychart.helper\" .}} \"-myprefix\" -}}\n{{- $myvar := (include \"mychart.helper\" .) \"-myprefix\" -}}\n`\n```",
      "solution": "It seems the right way to do this is to use the print() function\n```\n`{{- $myvar := print (include \"mychart.helper\" .) \"-myprefix\" -}}\n`\n```",
      "question_score": 4,
      "answer_score": 7,
      "created_at": "2021-07-12T20:47:02",
      "url": "https://stackoverflow.com/questions/68352522/how-do-i-concatenate-strings-inside-a-variable-assignment"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "helm",
      "question_id": 70307243,
      "title": "How do i update helm repo to the latest version",
      "problem": "I am trying to update my helm repo to the latest version using the below command. The repo name is returned from `helm repo list`. My helm version is `v3.3.1`\n```\n`helm repo update \n`\n```\nbut instead i get the below.\n```\n`Error: \"helm repo update\" accepts no arguments\n\nUsage:  helm repo update [flags]\n`\n```",
      "solution": "You need a newer version of Helm; the option you're looking for seems to have been added in Helm 3.7.0.  The Helm 3.7.0 release notes include:\n\n`helm repo update` now accepts an optional repository name\n\nIf you can't upgrade to a newer version of Helm, you can still run `helm repo update` with no additional arguments to update all repositories' data.",
      "question_score": 4,
      "answer_score": 6,
      "created_at": "2021-12-10T16:59:09",
      "url": "https://stackoverflow.com/questions/70307243/how-do-i-update-helm-repo-to-the-latest-version"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "helm",
      "question_id": 69648216,
      "title": "yq - issue adding yaml into yaml",
      "problem": "Hi I would like to update a yaml like string into a yaml\ni do have the following yaml file `argocd.yaml`\n```\n`---\napiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  namespace: mynamespace\n  name:my-app\nspec:\n  project: xxx\n  destination:\n    server: xxx\n    namespace: xxx\n  source:\n    repoURL: xxx\n    targetRevision: dev\n    path: yyy\n    helm:\n      values: |-\n        image:\n          tag: \"mytag\"\n          repository: \"myrepo-image\"\n          registry: \"myregistry\"\n`\n```\nand ultimatively I want to replace the value of tag. Unfortunately this is a yaml in a yaml configuration.\nMy Idea so far was:\n\nextract value into another values.yaml\nUpdate the tag\nevaluate the values in the argocd.yaml with the values.yaml\nSo what worked is:\n\n```\n`# get the yaml in the yaml and save as yaml\nyq e .spec.source.helm.values argocd.yaml > helm_values.yaml\n# replace the tag value\nyq e '.image.tag=newtag' helm_values.yaml\n`\n```\nand then I want to add the content of the `helm_values.yaml` file as string into the `argocd.yaml`\nI tried it the following but I can't get it work\n```\n`# idea 1\n###################\nyq eval 'select(fileIndex==0).spec.source.helm.values =  select(fileIndex==1) | select(fileIndex==0)' argocd.yaml values.yaml\n\n# this does not update the values but add back slashes \n\nvalues: \"\\nimage:\\n  tag: \\\"mytag\\\"\\n  repository: \\\"myrepo-image\\\"\\n  registry: \\\"myregistry\\\"\"\n\n# idea 2\n##################\nyq eval '.spec.source.helm.values = \"'\"$(Any Idea how to solve this or is there a better way to replace a value in such a file?\nI am using yq from https://mikefarah.gitbook.io/yq",
      "solution": "Your second approach can work, but in a roundabout way, as mikefarah/yq does not support updating multi-line block literals yet\nOne way to solve this, with the existing constructs would be to do below, without having to create a temporary YAML file\n`o=\"$(yq e '.spec.source.helm.values' yaml | yq e '.image.tag=\"footag\"' -)\" yq e -i '.spec.source.helm.values = strenv(o)' argocd.yaml\n`\nThe solution above relies on passing a user defined variable as a string, that can be used in the yq expression using `strenv()`. The value for the variable `o` is set by using Command substitution $(..) feature in bash. Inside the substitution construct, we extract the block literal content as a YAML object and apply another filter to modify the tag as we need. So at the end of substitution, the value of `o` will be set to\n`image:\n  tag: \"footag\"\n  repository: \"myrepo-image\"\n  registry: \"myregistry\"\n`\nThe obtained result above is now set to the value of `.spec.source.helm.values` directly which the inplace modification flag (`-i`) to apply the changes on the actual file\n\nI have raised a feature request in the author's repo to provide a simpler way to do this Support for updating YAML multi-line strings #974",
      "question_score": 4,
      "answer_score": 3,
      "created_at": "2021-10-20T16:47:41",
      "url": "https://stackoverflow.com/questions/69648216/yq-issue-adding-yaml-into-yaml"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "helm",
      "question_id": 76824951,
      "title": "&quot;/cloud_sql_proxy&quot;: no such file or directory: unknown",
      "problem": "I want to move from cloud sql proxy version 1.11 to 2.6.0\nI am using helm charts to deploy kubes in gcp\nold helm yaml spec:\n```\n`.....\n    image: \"{{ .Values.cloudSQL.repository }}:{{ .Values.cloudSQL.tag }}\"\n    imagePullPolicy: {{ .Values.cloudSQL.pullPolicy }}\n    command: [\"/cloud_sql_proxy\", \"-dir=/cloudsql\", \"-instances=$(GCP_PROJECT):$(GCP_REGION):$(CLOUD_SQL_DATABASE)=tcp:0.0.0.0:5432\", \"-credential_file=/credentials/credentials.json\"]\n.....\n`\n```\nvalues -\n```\n`cloudSQL:\n  repository: b.gcr.io/cloudsql-docker/gce-proxy\n  tag: 1.11\n`\n```\nNew changes -\n```\n`cloudSQL:\n  repository: gcr.io/cloud-sql-connectors/cloud-sql-proxy\n  tag: 2.6.0\n`\n```\n```\n`.... [remains same]\nimage: \"{{ .Values.cloudSQL.repository }}:{{ .Values.cloudSQL.tag }}\"\n          imagePullPolicy: {{ .Values.cloudSQL.pullPolicy }}\n          command: [\"/cloud_sql_proxy\", \"--unix-socket /cloudsql\", \"--address 0.0.0.0\", \"--private-ip\", \"$(GCP_PROJECT):$(GCP_REGION):$(CLOUD_SQL_DATABASE)\", \"--credentials-file /credentials/credentials.json\"]\n\n.... [remains same]\n`\n```\nI get the following error -\n```\n`failed to create container task: failed to create shim task: OCI runtime create failed: runc create failed:unable to start container process: exec: \"/cloud_sql_proxy\": stat \"/cloud_sql_proxy\":  no such file or directory: unknown\n`\n```\n[image pull policy is the same]\nCommand was working fine with 1.11 and image is pulled for 2.6 but says cloud_sql_proxy not found with new version.\nI followed the migration guide for these changes",
      "solution": "The binary in the `gcr.io/cloud-sql-connectors/cloud-sql-proxy` image is named `/cloud-sql-proxy` with hyphens and not underscores, while the old image is using underscores.",
      "question_score": 4,
      "answer_score": 4,
      "created_at": "2023-08-03T06:47:45",
      "url": "https://stackoverflow.com/questions/76824951/cloud-sql-proxy-no-such-file-or-directory-unknown"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "helm",
      "question_id": 71067374,
      "title": "DevOps CI/CD pipelines broken after Kubernetes upgrade to v1.22",
      "problem": "Present state\nIn v1.22 Kubernetes dropped support for `v1beta1` API. That made our release pipeline crash and we are not sure how to fix it.\nWe use build pipelines to build .NET Core applications and deploy them to the Azure Container Registry. Then there are release pipelines that use `helm` to upgrade them in the cluster from that ACR. This is how it looks exactly.\nBuild pipeline:\n\n.NET download, restore, build, test, publish\nDocker task v0: Build task\nDocker task v0: Push to the `ACR` task\nArtifact publish to Azure Pipelines\n\nRelease pipeline:\n\nHelm tool installer: Install `helm` v3.2.4 (check for latest version of Helm unchecked) and install newest `Kubectl` (Check for latest version checked)\nBash task:\n\n```\n`az acr login --name \naz acr helm repo add --name \n`\n```\n\nHelm upgrade task:\n\nchart name `/`\nversion `empty`\nrelease name `\n\nAfter the upgrade to Kubernetes v1.22 we are getting the following error in Release step 3.:\n`Error: UPGRADE FAILED: unable to recognize \"\": no matches for kind \"Ingress\" in version \"extensions/v1beta1\"`.\nWhat I've already tried\nError is pretty obvious and from Helm compatibility table it states clearly that I need to upgrade the release pipelines to use at least Helm v3.7.x. Unfortunately in this version OCI functionality (about this shortly) is still in experimental phase so at least v3.8.x has to be used.\nBumping helm version to v3.8.0\nThat makes release step 3. report:\n`Error: looks like \"https://.azurecr.io/helm/v1/repo\" is not a valid chart repository or cannot be reached: error unmarshaling JSON: while decoding JSON: json: unknown field \"acrMetadata\"`\nAfter reading Microsoft tutorial on how to live with `helm` and `ACR` I learned that `az acr helm` commands use helm v2 so are deprecated and `OCI` artifacts should be used.\nSwitching to OCI part 1\nAfter reading that I changed release step 2. to a one-liner:\n`helm registry login .azurecr.io  --username  --password `\nThat now gives me `Login Succeeded` in release step 2. but release step 3. fails with\n`Error: failed to download \"/\"`.\nSwitching to OCI part 2\nI thought that the helm task is incompatible or something with the new approach so I removed release step 3. and decided to make it from the command line in step 2. So now step 2. looks like this:\n```\n`helm registry login .azurecr.io  --username  --password \nhelm upgrade --install --wait -n   oci://.azurecr.io/ --version latest --values ./values.yaml\n`\n```\nUnfortunately, that still gives me:\n`Error: failed to download \"oci://.azurecr.io/\" at version \"latest\"`\nHelm pull, export, upgrade instead of just upgrade\nThe next try was to split the `help upgrade` into separately `helm pull`, `helm export` and then `helm upgrade` but\n`helm pull oci://.azurecr.io/ --version latest`\ngives me:\n`Error: manifest does not contain minimum number of descriptors (2), descriptors found: 0`\nChanging `docker build` and `docker push` tasks to v2\nI also tried changing the docker tasks in the build pipelines to v2. But that didn't change anything at all.",
      "solution": "Have you tried changing the Ingress object's `apiVersion` to `networking.k8s.io/v1beta1` or `networking.k8s.io/v1`? Support for Ingress in the extensions/v1beta1 API version is dropped in k8s 1.22.\nOur `ingress.yaml` file in our helm chart looks something like this to support multiple k8s versions. You can ignore the AWS-specific annotations since you're using Azure. Our chart has a global value of  `ingress.enablePathType` because at the time of writing the yaml file, AWS Load Balancer did not support pathType and so we set the value to false.\n`{{- if .Values.global.ingress.enabled -}}\n{{- $useV1Ingress := and (.Capabilities.APIVersions.Has \"networking.k8s.io/v1/Ingress\") .Values.global.ingress.enablePathType -}}\n{{- if $useV1Ingress -}}\napiVersion: networking.k8s.io/v1\n{{- else if semverCompare \">=1.14-0\" .Capabilities.KubeVersion.GitVersion -}}\napiVersion: networking.k8s.io/v1beta1\n{{- else -}}\napiVersion: extensions/v1beta1\n{{- end }}\nkind: Ingress\nmetadata:\n  name: example-ingress\n  labels:\n    {{- include \"my-chart.labels\" . | nindent 4 }}\n  annotations:\n    {{- if .Values.global.ingress.group.enabled }}\n    alb.ingress.kubernetes.io/group.name: {{ required \"ingress.group.name is required when ingress.group.enabled is true\" .Values.global.ingress.group.name }}\n    {{- end }}\n    {{- with .Values.global.ingress.annotations }}\n    {{- toYaml . | nindent 4 }}\n    {{- end }}\n    # Add these tags to the AWS Application Load Balancer\n    alb.ingress.kubernetes.io/tags: k8s.namespace/{{ .Release.Namespace }}={{ .Release.Namespace }}\nspec:\n  rules:\n    - host: {{ include \"my-chart.applicationOneServerUrl\" . | quote }}\n      http:\n        paths:\n          {{- if $useV1Ingress }}\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: {{ $applicationOneServiceName }}\n                port:\n                  name: http-grails\n          {{- else }}\n          - path: /*\n            backend:\n              serviceName: {{ $applicationOneServiceName }}\n              servicePort: http-grails\n          {{- end }}\n    - host: {{ include \"my-chart.applicationTwoServerUrl\" . | quote }}\n      http:\n        paths:\n          {{- if $useV1Ingress }}\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: {{ .Values.global.applicationTwo.serviceName }}\n                port:\n                  name: http-grails\n          {{- else }}\n          - path: /*\n            backend:\n              serviceName: {{ .Values.global.applicationTwo.serviceName }}\n              servicePort: http-grails\n          {{- end }}\n{{- end }}\n`",
      "question_score": 4,
      "answer_score": 3,
      "created_at": "2022-02-10T15:57:36",
      "url": "https://stackoverflow.com/questions/71067374/devops-ci-cd-pipelines-broken-after-kubernetes-upgrade-to-v1-22"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "helm",
      "question_id": 66462924,
      "title": "Promtail basic auth using kubernetes secret in helm values.yaml",
      "problem": "I am using the promtail helm chart to connect to a Loki server running on a different stack. I have Loki behind an Nginx ingress secured with basic auth.\nI can't find any documentation on this, and it's very possible it's just my admittedly limited understanding of helm.\nI'm simply trying to use basic auth to connect to the Loki instance while using a Kubernetes secret instead of plaintext credentials in the helm values\nThis works perfect:\n```\n`  snippets:\n    extraClientConfigs: |\n      basic_auth:\n        username: myusername\n        password: mypassword\n`\n```\nI created a secret like this:\n```\n`kubectl create secret generic loki-credentials -n monitoring --from-literal=password=\"mypassword\" --from-literal=username=\"myusername\"\n`\n```\nand now I want to use that in the `values.yaml` file.\nThis is what I got so far:\n```\n`extraEnv:\n  - name: LOKI_USERNAME\n    valueFrom:\n      secretKeyRef:\n        name: loki-credentials\n        key: username\n  - name: LOKI_PASSWORD\n    valueFrom:\n      secretKeyRef:\n        name: loki-credentials\n        key: password\n\nextraArgs:\n  - -client.external-labels=stack=development\n  - -config.expand-env\n\nconfig:\n  serverPort: 3101\n  lokiAddress: myurl\n  snippets:\n    extraClientConfigs: |\n      basic_auth:\n        username: ${LOKI_USERNAME}\n        password: ${LOKI_PASSWORD}\n`\n```\nI just get a 401 response.\n```\n` Chart version: 3.1.0\n Promtail version: 2.1.0\n`\n```\nEDIT\nHere is my ingress yaml:\n```\n`controller:\n  replicaCount: 1\n\n  config:\n    force-ssl-redirect: \"true\"\n    use-forwarded-headers: \"true\"\n\n  service:\n    targetPorts:\n      http: http\n      https: http\n    annotations:\n      service.beta.kubernetes.io/aws-load-balancer-ssl-cert: \"arn:aws:acm:us-west-2:123456:certificate/123456\"\n      service.beta.kubernetes.io/aws-load-balancer-backend-protocol: \"http\"\n      service.beta.kubernetes.io/aws-load-balancer-ssl-ports: \"https\"\n\n    enableHttp: false\n    enableHttps: true\n    type: LoadBalancer\n    loadBalancerSourceRanges:\n      - \"0.0.0.0/0\"\n\n   ## Name of the ingress class to route through this controller\n  ingressClass: nginx-external\n`\n```\n...and my loki `values.yaml`\n```\n`ingress:\n  enabled: true\n  annotations:\n    kubernetes.io/ingress.class: \"nginx-external\"\n    nginx.ingress.kubernetes.io/auth-type: basic\n    nginx.ingress.kubernetes.io/auth-secret: loki-ingress-auth\n    nginx.ingress.kubernetes.io/auth-realm: \"Authentication Required\"\n  hosts:\n    - host: loki.mydomain.com\n      paths: \n        - \"/\"\n  tls: []\n\nconfig:\n  auth_enabled: false\n  ingester:\n    chunk_idle_period: 3m\n    chunk_block_size: 262144\n    chunk_retain_period: 1m\n    max_transfer_retries: 3\n    lifecycler:\n      ring:\n        kvstore:\n          store: inmemory\n        replication_factor: 1\n  \n  schema_config:\n    configs:\n    - from: 2021-03-06\n      store: boltdb-shipper\n      object_store: aws\n      schema: v11\n      index:\n        prefix: loki_index_\n        period: 24h\n        \n  server:\n    http_listen_port: 3100\n\n  storage_config:\n    aws:\n      bucketnames: my-bucket-name\n      region: us-west-2\n      s3forcepathstyle: true\n\n    boltdb_shipper:\n      active_index_directory: /data/loki/boltdb-shipper-active\n      cache_location: /data/loki/boltdb-shipper-cache\n      shared_store: s3\n\n  chunk_store_config:\n    max_look_back_period: 0s\n  \n  table_manager:\n    retention_deletes_enabled: false\n    retention_period: 0s\n\n  compactor:\n    working_directory: /data/loki/boltdb-shipper-compactor\n    shared_store: aws\n\nreplicas: 1\n\npodAnnotations:\n  iam.amazonaws.com/role: \"arn:aws:iam::123456:role/my-loki-role\"\n\nresources:\n  limits:\n    cpu: 500m\n    memory: 2G\n  requests:\n    cpu: 250m\n    memory: 1G\n\n# The values to set in the PodDisruptionBudget spec\n# If not set then a PodDisruptionBudget will not be created\npodDisruptionBudget:\n  minAvailable: 1\n`\n```\nMy logs from the nginx pod that Loki is sitting behind:\n```\n`2021/03/09 04:23:44 [error] 37#37: *925 user \"myusername\": password mismatch, client: xxx.xx.xxx.xxx, server: loki.mydomain.com, request: \"POST /loki/api/v1/push HTTP/1.1\", host: \"loki.mydomain.com\"\n2021/03/09 04:23:44 [error] 37#37: *921 user \"myusername\": password mismatch, client: xxx.xx.xxx.xxx, server: loki.mydomain.com, request: \"POST /loki/api/v1/push HTTP/1.1\", host: \"loki.mydomain.com\"\nxx.xxx.xxx.xx - myusername [09/Mar/2021:04:23:44 +0000] \"POST /loki/api/v1/push HTTP/1.1\" 401 172 \"-\" \"promtail/2.1.0\" 326 0.000 [monitoring-loki-3100] [] - - - - 63294b16fe010a8c9ec1d4684f0472f5\nxxx.xx.xxx.xxx: - myusername [09/Mar/2021:04:23:44 +0000] \"POST /loki/api/v1/push HTTP/1.1\" 204 0 \"-\" \"promtail/2.1.0\" 2744 0.003 [monitoring-loki-3100] [] xxx.xx.xxx.xxx:3100 0 0.004 204 029e0a9d1ee6242cad8b9a6d2ee50940\n2021/03/09 04:23:44 [error] 37#37: *925 user \"myusername\": password mismatch, client: xx.xxx.xxx.xx, server: loki.mydomain.com, request: \"POST /loki/api/v1/push HTTP/1.1\", host: \"loki.mydomain.com\"\nxxx.xx.xxx.xxx - myusername [09/Mar/2021:04:23:44 +0000] \"POST /loki/api/v1/push HTTP/1.1\" 401 172 \"-\" \"promtail/2.1.0\" 325 0.000 [monitoring-loki-3100] [] - - - - b75a2cfcf6c62b81953dd4fb26f1a844\nxxx.xx.xxx.xxx - myusername [09/Mar/2021:04:23:44 +0000] \"POST /loki/api/v1/push HTTP/1.1\" 204 0 \"-\" \"promtail/2.1.0\" 1513 0.014 [monitoring-loki-3100] [] xxx.xx.xxx.xxx:3100 0 0.016 204 0049965a49877cb5d336ac6ec869feb4\n2021/03/09 04:23:45 [error] 36#36: *941 user \"myusername\": password mismatch, client: xxx.xx.xxx.xxx, server: loki.mydomain.com, request: \"POST /loki/api/v1/push HTTP/1.1\", host: \"loki.mydomain.com\"\nxxx.xx.xxx.xxx - myusername [09/Mar/2021:04:23:45 +0000] \"POST /loki/api/v1/push HTTP/1.1\" 401 172 \"-\" \"promtail/2.1.0\" 326 0.000 [monitoring-loki-3100] [] - - - - e5954bd055db5b3e9bd3227f57651847\n`\n```",
      "solution": "To give a bit of background to anyone new to Loki, as stated in the documentation: Loki does not come with any included authentication layer. Operators are expected to run an authenticating reverse proxy in front of your services, such as NGINX using basic auth or an OAuth2 proxy.\nThis basically means that you'll have to place something in between the client(s) and Loki to enforce e.g. basic authentication. In this case there's a Ingress (Nginx) acting as a reverse proxy with basic authentication.\nTo troubleshoot problems with authentication using Nginx there's a number of things to check:\n\nLogs of the Nginx Ingress Pod, check for authentication errors.\nThat the added Kubernetes Secret contains what you expect.\nThat you have configured the Ingress object with the needed `annotations`.\n\nWhen it comes to using Nginx as Ingress and adding basic authentication this resource from the official docs is really helpful.\nIn general when creating Kubernetes secrets, especially from the command line using `kubectl` you'll have to single quote the password if it contains special characters. This is so that the special characters wont be interpreted by your shell. More info here.",
      "question_score": 4,
      "answer_score": 1,
      "created_at": "2021-03-03T19:36:35",
      "url": "https://stackoverflow.com/questions/66462924/promtail-basic-auth-using-kubernetes-secret-in-helm-values-yaml"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "helm",
      "question_id": 73310865,
      "title": "Pass values helm package",
      "problem": "We are using helm charts to deploy our charts in Openshift.\nThis is our workflow:\n\nWe create a new version of the helm and docker image at each sprint/ e.g 1.0.0 (saving them in a snapshot artifactory)\nDuring the sprint we build several times the helm chart and the docker image and push them in our snapshot artifactory every time.\nOnce the helm chart and the docker image are published, we automatically deploy our chart in our test environment\nonce we are ready we create the production version of the charts and the docker image: we basically publish the helm chart and the docker image in a release artifactory with the same version. From now on the helm chart and the docker images are immutable\nNow we deploy in PROD\n\nThe issue is that usually the helm-chart does not change but the docker image v1.0.0 (snapshot) may change several times during the sprint therefore when we try to upgrade the helm chart in our test env, helm does not detect any change and then the application is not updated.\nTo solve this situation, currently, every time that we have to deploy in the test environment, we uninstall the application and re install the helm chart (with the image pull policy == always)\nI was wondering if there is a way to modify our helm chart in order to force it to redeploy when we build a new version.\ne.g we tried to add an annotation in the deployment.yaml : build-time: {{ now }} but this changes every time so the helm chart is always redeployed in the test environment (and usually is fine but not if we trigger a manual re-deploy of all our components).\nIs it possible for example to provide a parameter during the `helm package` command?\nSomething like `helm package --set package-time=timestamp` and then we could save this value as annotation.\nAny better solution?",
      "solution": "In addition to you functional tag (eg v1.0.0), add a second tag to your image with something unique, for example the git-sha commit tag coming from git used at build time.\nThe functionnal tag is a \"floating\" tag, it will move from one image to the other but the \"git-sha\" tag will be unique\nOne way to get a short version of git-sha:\n```\n`git log -n 1 --pretty=format:'%h'\n`\n```\nIn your deployment, specify the \"git-sha\" tag for your image in your helm chart as a variable\nThe functional tag/version could be read from a single line file in your source, So when you are ready to push v2.0.0, just change the file and commit it",
      "question_score": 4,
      "answer_score": 3,
      "created_at": "2022-08-10T20:06:36",
      "url": "https://stackoverflow.com/questions/73310865/pass-values-helm-package"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "helm",
      "question_id": 70924030,
      "title": "Stopping all pods in Kubernetes cluster before running database migration job",
      "problem": "I deploy my App into the Kubernetes cluster using Helm. App works with database, so i have to run db migrations before installing new version of the app. I run migrations with Kubernetes Job object using Helm \"pre-upgrade\" hook.\nThe problem is when the migration job starts old version pods are still working with database. They can block objects in database and because of that migration job may fail.\nSo, i want somehow to automatically stop all the pods in cluster before migration job starts. Is there any way to do that using Kubernetes + Helm? Will appreciate all the answers.",
      "solution": "There are two ways I can see that you can do this.\nFirst option is to scale down the pods before the deployment (for example, via Jenkins, CircleCI, GitLab CI, etc)\n```\n`kubectl scale --replicas=0 -n {namespace} {deployment-name}\nhelm install .....\n`\n```\nThe second option (which might be easier depending on how you want to maintain this going forward) is to add an additional pre-upgrade hook with a higher priority than the migrations hook so it runs before the upgrade job; and then use that do the `kubectl` scale down.",
      "question_score": 4,
      "answer_score": 3,
      "created_at": "2022-01-31T10:56:07",
      "url": "https://stackoverflow.com/questions/70924030/stopping-all-pods-in-kubernetes-cluster-before-running-database-migration-job"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "helm",
      "question_id": 69366467,
      "title": "Updating Helm subcharts fails without a clear error",
      "problem": "While trying to add/update a dependency to a helm chart I'm getting this error. No helm plugins are installed with the name `helm-manager`.\n`$ helm dep update                                                                                                                                \nGetting updates for unmanaged Helm repositories...\n...Unable to get an update from the \"https://kubernetes-charts.storage.googleapis.com/\" chart repository:\n        failed to fetch https://kubernetes-charts.storage.googleapis.com/index.yaml : 403 Forbidden\n...Unable to get an update from the \"https://kubernetes-charts.storage.googleapis.com/\" chart repository:\n        failed to fetch https://kubernetes-charts.storage.googleapis.com/index.yaml : 403 Forbidden\nHang tight while we grab the latest from your chart repositories...\n...Successfully got an update from the \"bitnami\" chart repository\nUpdate Complete. Happy Helming!\nError: no cached repository for helm-manager-1067d9c6027b8c3f27b49e40521d64be96ea412858d8e45064fa44afd3966ddc found. (try 'helm repo update'): open /Users//Library/Caches/helm/repository/helm-manager-1067d9c6027b8c3f27b49e40521d64be96ea412858d8e45064fa44\nafd3966ddc-index.yaml: no such file or directory\n`",
      "solution": "The stable and incubator repositories of the Helm charts have been moved to a new location.\nYou must updated URI in charts.yaml (or requirements.yaml) to point to the new repositories in order to let the Helm dependency resolver find the correct location.\n\nName\nOld Location\nNew Location\n\nstable\nhttps://kubernetes-charts.storage.googleapis.com\nhttps://charts.helm.sh/stable\n\nincubator\nhttps://kubernetes-charts-incubator.storage.googleapis.com\nhttps://charts.helm.sh/incubator\n\nAfter that you should be able to run `helm dep update` without further modifications.",
      "question_score": 4,
      "answer_score": 1,
      "created_at": "2021-09-28T20:00:55",
      "url": "https://stackoverflow.com/questions/69366467/updating-helm-subcharts-fails-without-a-clear-error"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "helm",
      "question_id": 77007803,
      "title": "What is causing a &quot;no template associated with gotpl&quot; error with using helm unittest?",
      "problem": "I should start by saying I'm fairly new to the world of helm - however I'm picking up some helm charts developed by others, and looking to add some unit tests. I've installed https://github.com/helm-unittest/helm-unittest, and created a basic test file:\n```\n`suite: test xyz deployment\ntemplates:\n  - xyz-deployment.yaml\ntests:\n  - it: will not do much yet\n    set:\n      image.tag: latest\n    asserts:\n      - isKind:\n        of: Deployment\n`\n```\nAnd my chart file structure looks like:\n```\n`> xyz\n  > templates\n     xyz.configmap.yaml\n     xyz.deployment.yaml\n     \n  > tests\n     xyz_test.yaml\n`\n```\nwithin `xyz.deployment.yaml` I have (snipped for brevity):\n```\n`spec:\n  template:\n    metadata:\n      annotations:\n        checksum/config: {{ include (print $.Template.BasePath \"/xyz-configmap.yaml\") . | sha256sum }}\n`\n```\nlinting runs cleanly (other than an icon recommendation). However, when I run `helm unittest xyz` I get:\n```\n`$ helm unittest xyz\n\n### Chart [ xyz ] xyz\n\n FAIL  test xyz deployment xyz/tests/xyz_test.yaml\n        - deployment should render\n                Error: template: xyz/templates/xyz-deployment.yaml:16:30: executing \"xyz/templates/xyz-deployment.yaml\" at : error calling include: template: no template \"xyz/templates/xyz-configmap.yaml\" associated with template \"gotpl\"\n\nCharts:      1 failed, 0 passed, 1 total\nTest Suites: 1 failed, 0 passed, 1 total\nTests:       1 failed, 1 errored, 0 passed, 1 total\nSnapshot:    0 passed, 0 total\nTime:        14.423497ms\n\nError: plugin \"unittest\" exited with error\n`\n```\nClearly the configmap.yaml is there, so I'm at a loss as to why the unit test plugin is reporting it's not found. Can anyone help?",
      "solution": "Finally found the answer! Changing my test file to look more like:\n```\n`suite: test xyz deployment\ntests:\n  - it: will not do much yet\n    set:\n      image.tag: latest\n    template: xyz-deployment.yaml\n    asserts:\n      - isKind:\n        of: Deployment\n`\n```\nie, specifying the template within the individual spec block, rather than at the top level, worked a treat!",
      "question_score": 4,
      "answer_score": 2,
      "created_at": "2023-08-30T14:00:34",
      "url": "https://stackoverflow.com/questions/77007803/what-is-causing-a-no-template-associated-with-gotpl-error-with-using-helm-unit"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "helm",
      "question_id": 67887859,
      "title": "Unable to push helm charts to JFrog Artifactory: &#39;not a valid chart repository&#39; error",
      "problem": "I am trying to add the helm repository to my JFrog account but it is giving me an error. I have created a remote repository.\nHow can I push using helm client?\n```\n`helm repo add  https://ip/artifactory/ --username username --password password\n`\n```\n\nError: looks like \"https://ip/artifactory/\" is not a valid chart repository or cannot be reached: failed to fetch https://ip/artifactory//index.yaml : 401 Unauthorized",
      "solution": "You cannot push to a remote repository. It's meant to proxy and cache remote repositories only. Not to allow you to push your artifacts to them via Artifactory.\nIf you look into the documentation, you'll see you need to have a virtual, local and remote helm repositories.\nFor example:\n```\n`helm-local  -> your local repo\nhelm-remote -> your remote repo\nhelm        -> your virtual repo\n`\n```\nFor the virtual repository, you set the default deployment repository (where the charts you upload are pushed to). This would be the local repository you created.\nOnce you have them setup, you do the login against the virtual repository - `https://ip/artifactory/helm`. The `helm repo add` command uses this URL.",
      "question_score": 4,
      "answer_score": 1,
      "created_at": "2021-06-08T15:23:33",
      "url": "https://stackoverflow.com/questions/67887859/unable-to-push-helm-charts-to-jfrog-artifactory-not-a-valid-chart-repository"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "helm",
      "question_id": 76537667,
      "title": "How to integrate grafana agent running in AKS with grafana cloud",
      "problem": "I have installed grafana agent in my Azure Kubernetes cluster by using helm chart\nLink: https://artifacthub.io/packages/helm/grafana/grafana-agent\nStep-1:\n```\n`helm repo add grafana https://grafana.github.io/helm-charts\nhelm repo update\nhelm install my-release grafana/grafana-agent\n`\n```\nStep-2:\nI have created grafana cloud account by refering to following documentation: https://grafana.com/auth/sign-up/create-user\nStep-3:\nI have updated configmap of grafana agent by adding agent.yaml: | information\n```\n`apiVersion: v1\ndata:\n  config.river: \"logging {\\n\\tlevel  = \\\"info\\\"\\n\\tformat = \\\"logfmt\\\"\\n}\\n\\ndiscovery.kubernetes\n    \\\"pods\\\" {\\n\\trole = \\\"pod\\\"\\n}\\n\\ndiscovery.kubernetes \\\"nodes\\\" {\\n\\trole =\n    \\\"node\\\"\\n}\\n\\ndiscovery.kubernetes \\\"services\\\" {\\n\\trole = \\\"service\\\"\\n}\\n\\ndiscovery.kubernetes\n    \\\"endpoints\\\" {\\n\\trole = \\\"endpoints\\\"\\n}\\n\\ndiscovery.kubernetes \\\"endpointslices\\\"\n    {\\n\\trole = \\\"endpointslice\\\"\\n}\\n\\ndiscovery.kubernetes \\\"ingresses\\\" {\\n\\trole\n    = \\\"ingress\\\"\\n}\"\n  agent.yaml: |\n    global:\n        scrape_interval: 60s\n        external_labels:\n          cluster: example.cluster.dev\n    configs:\n      - name: integrations\n        remote_write:\n        - url: https://prometheus-prod-13-prod-us-east-0.grafana.net/api/prom/push\n          basic_auth:\n            username: xxxx\n            password: xxxx\nkind: ConfigMap\nmetadata:\n  annotations:\n    meta.helm.sh/release-name: my-release\n    meta.helm.sh/release-namespace: default\n  creationTimestamp: \"2023-06-22T13:45:11Z\"\n  labels:\n    app.kubernetes.io/instance: my-release\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: grafana-agent\n    app.kubernetes.io/version: v0.34.2\n    helm.sh/chart: grafana-agent-0.16.0\n  name: my-release-grafana-agent\n  namespace: default\n  resourceVersion: \"35520419\"\n  uid: 3980b7b3-e09a-48ce-b8d6-3d9e681d5b10\n\n`\n```\nGot username and password details from grafana cloud\n\nBut I am not able see any metrics being sent to my grafana cloud.\n\nI am completely new to grafana cloud. Please help me to resolve this error. I am new to grafana tech stack.\nThanks in Advance",
      "solution": "To integrate AKS with Grafana Cloud using Grafana Agent, please follow the steps below:\nSteps:\n\nGo to the Grafana portal, i.e., .grafana.net. Click on Home > Observability > Kubernetes > Configuration.\n\nSelect the correct data sources.\n\nInstall the dashboard and alert rules. You can install them by clicking the button in the portal.\n\nClick on the configuration instructions, which will guide you through the installation process.\n\nNote:\nIf, after completing step 3, your pod continues to crash, it may be due to missing configuration in the scrape_configs list in the agent.yaml file created during the deployment of the ConfigMap. Review the logs to identify the line number and remove the corresponding index from the scrape_configs list.\nIn my case, the pod is crashing due to the entry `job_name: integrations/grafana-mimir/kubelet`. After removing this entry, wait for 60 seconds, and you should start seeing metrics in the dashboard you installed during step 2.\nAfter updating the configmap. Please delete the pod so that it will fetch latest files.",
      "question_score": 4,
      "answer_score": 1,
      "created_at": "2023-06-23T08:44:24",
      "url": "https://stackoverflow.com/questions/76537667/how-to-integrate-grafana-agent-running-in-aks-with-grafana-cloud"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "helm",
      "question_id": 71147077,
      "title": "Helm: Overwrite configuration from json files, is there a better way?",
      "problem": "We use helm to deploy a microservice on different systems.\nAmong other things, we have a ConfigMap template and of course a value file with the default values in the repo of the service. Some of these values are JSON and so far stored as JSON string:\n```\n`apiVersion: v1\ndata:\n  Configuration.json: {{ toYaml .Values.config | indent 4 }}\nkind: ConfigMap\nmetadata:\n  name: service-cm\n`\n```\n```\n`config: |-\n  {\n    \"val1\": \"key1\",\n    \"val2\": \"key2\"\n  }\n`\n```\nWe also have a deployment repo where the different systems are defined. Here we overwrote the values with json strings as well.\nSince the usability of these json strings is not so good, we want to move them to json files.\nWe use AKS and Azure Pipelines to deploy the service.\nWe create the chart with:\n```\n`helm chart save Chart $(acr.name).azurecr.io/$(acr.repo.name):$(BUILD_VERSION)\n`\n```\nand push it with:\n```\n`helm chart push $(acr.name).azurecr.io/$(acr.repo.name):$(BUILD_VERSION)\n`\n```\nand upgrade after pull and export in another job:\n```\n`helm upgrade --install --set image-name --wait -f demo-values.yaml service service-chart\n`\n```\nWhat we have already done is to set the json config in the upgrade command with --set-file:\n```\n`upgrade --install --set image-name --wait -f demo-values.yaml --set-file config=demo-config.json service service-chart\n`\n```\nWhat works though only for the values, of the different systems, not for the default values. But we also want to outsource these and also do not want to do without them.\nTherefore at this point the first question, is there a way to inject the default values already per file, so that they are in the saved chart?\nWe know that you can read files in the templates with the following syntax:\n```\n`  Configuration.json: |-\n{{ .Files.Get \"default-config.json\" | indent 4 }}\n`\n```\nBut we can't override that. Another idea was to inject the path from the values:\n```\n` Configuration.json: |-\n{{ .Files.Get (printf \"%s\" .Values.config.filename) | indent 4 }}\n`\n```\nBut the path seems to be relative to the chart folder. So there is no path to the deployment repo.\nWe now have the following solution with conditional templates:\n```\n`data:\n  {{ if .Values.config.overwrite }}\n  Configuration.json: {{ toYaml .Values.config.value | indent 4 }}\n  {{ else }}\n  Configuration.json: |-\n{{ .Files.Get \"default-config\" | indent 4 }}\n  {{ end }}\n`\n```\nIn the deployment repo the value file then looks like this:\n```\n`config:\n  overwrite: true\n  value: will_be_replaced_by_file_content\n`\n```\nAnd `demo-config.json` is set with the upgrade command in the pipeline.\nThis works, but seems a bit fiddly to us. So the question: Do you know a better way?",
      "solution": "In your very first setup, `.Values.config` is a string.  The `key: |-` syntax creates a YAML block scalar that contains an indented text block that happens to be JSON.  `helm install --set-file` also sets a value to a string, and `.Files.Get` returns a string.\nAll of these things being strings means you can simplify the logic around them.  For example, consider the Helm `default` template function: if its parameter is an empty string, it is logically false, and so `default` falls back to its default value.\nIn your final layout you want to keep the default configuration in a separate file, but use it only if an override configuration isn't provided.  So you can go with an approach where:\n\nIn `values.yaml`, `config` is an empty string.  (`null` or just not defining it at all will also work for this setup.)\n`# config is a string containing JSON-format application configuration.\nconfig: ''\n`\n\nAs you already have it, `.Files.Get \"default-config.json\"` can be the fallback value.\n\nUse the `default` function to check if `.Values.config` is non-empty, and if not, fall back to that default.\n\nUse `helm install --set-file config=demo-config.json` to provide an alternate config at deploy time.\n\nThe updated ConfigMap could look like:\n`apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: {{ include \"myapp.fullname\" . }}\ndata:\n  Configuration.json:\n{{ .Values.config | default (.Files.Get \"default-config.json\") | indent 4 }}\n`\n(Since any form of `.Values.config` is a string, it's not a complex structure and you don't need to call `toYaml` on it.)",
      "question_score": 4,
      "answer_score": 1,
      "created_at": "2022-02-16T19:04:54",
      "url": "https://stackoverflow.com/questions/71147077/helm-overwrite-configuration-from-json-files-is-there-a-better-way"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "helm",
      "question_id": 70219893,
      "title": "Failed to move past 1 pod has unbound immediate PersistentVolumeClaims",
      "problem": "I am new to Kubernetes, and trying to get apache airflow working using helm charts. After almost a week of struggling, I am nowhere - even to get the one provided in the apache airflow documentation working. I use Pop OS 20.04 and microk8s.\nWhen I run these commands:\n```\n`kubectl create namespace airflow\nhelm repo add apache-airflow https://airflow.apache.org\nhelm install airflow apache-airflow/airflow --namespace airflow\n`\n```\nThe helm installation times out after five minutes.\n```\n`kubectl get pods -n airflow\n`\n```\nshows this list:\n```\n`NAME                                   READY   STATUS     RESTARTS   AGE\nairflow-postgresql-0                   0/1     Pending    0          4m8s\nairflow-redis-0                        0/1     Pending    0          4m8s\nairflow-worker-0                       0/2     Pending    0          4m8s\nairflow-scheduler-565d8587fd-vm8h7     0/2     Init:0/1   0          4m8s\nairflow-triggerer-7f4477dcb6-nlhg8     0/1     Init:0/1   0          4m8s\nairflow-webserver-684c5d94d9-qhhv2     0/1     Init:0/1   0          4m8s\nairflow-run-airflow-migrations-rzm59   1/1     Running    0          4m8s\nairflow-statsd-84f4f9898-sltw9         1/1     Running    0          4m8s\nairflow-flower-7c87f95f46-qqqqx        0/1     Running    4          4m8s\n`\n```\nThen when I run the below command:\n```\n`kubectl describe pod airflow-postgresql-0 -n airflow\n`\n```\nI get the below (trimmed up to the events):\n```\n`Events:\n  Type     Reason            Age                From               Message\n  ----     ------            ----               ----               -------\n  Warning  FailedScheduling  58s (x2 over 58s)  default-scheduler  0/1 nodes are available: 1 pod has unbound immediate PersistentVolumeClaims.\n`\n```\nThen I deleted the namespace using the following commands\n```\n`kubectl delete ns airflow\n`\n```\nAt this point, the termination of the pods gets stuck. Then I bring up the proxy in another terminal:\n```\n`kubectl proxy\n`\n```\nThen issue the following command to force deleting the namespace and all it's pods and resources:\n```\n`kubectl get ns airflow -o json | jq '.spec.finalizers=[]' | curl -X PUT http://localhost:8001/api/v1/namespaces/airflow/finalize -H \"Content-Type: application/json\" --data @-\n`\n```\nThen I deleted the PVC's using the following command:\n```\n`kubectl delete pvc --force --grace-period=0 --all -n airflow\n`\n```\nYou get stuck again, so I had to issue another command to force this deletion:\n```\n`kubectl patch pvc data-airflow-postgresql-0 -p '{\"metadata\":{\"finalizers\":null}}' -n airflow\n`\n```\nThe PVC's gets terminated at this point and these two commands return nothing:\n```\n`kubectl get pvc -n airflow\nkubectl get all -n airflow\n`\n```\nThen I restarted the machine and executed the helm install again (using first and last commands in the first section of this question), but the same result.\nI executed the following command then (using the suggestions I found here):\n```\n`kubectl describe pvc -n airflow\n`\n```\nI got the following output (I am posting the event portion of PostgreSQL):\n```\n`Type    Reason         Age                   From                         Message\n  ----    ------         ----                  ----                         -------\n  Normal  FailedBinding  2m58s (x42 over 13m)  persistentvolume-controller  no persistent volumes available for this claim and no storage class is set\n`\n```\nSo my assumption is that I need to provide storage class as part of the values.yaml\nIs my understanding right? How do I provide the required (and what values) in the values.yaml?",
      "solution": "If you installed with helm, you can uninstall with `helm delete airflow -n airflow`.\nHere's a way to install airflow for testing purposes using default values:\nGenerate the manifest `helm template airflow apache-airflow/airflow -n airflow > airflow.yaml`\nOpen the \"airflow.yaml\" with your favorite editor, replace all \"volumeClaimTemplates\" with emptyDir. Example:\n\nCreate the namespace and install:\n```\n`kubectl create namespace airflow\nkubectl apply -f airflow.yaml --namespace airflow\n`\n```\n\nYou can copy files out from the pods if needed.\nTo delete `kubectl delete -f airflow.yaml --namespace airflow`.",
      "question_score": 4,
      "answer_score": 1,
      "created_at": "2021-12-03T20:42:35",
      "url": "https://stackoverflow.com/questions/70219893/failed-to-move-past-1-pod-has-unbound-immediate-persistentvolumeclaims"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "helm",
      "question_id": 73920549,
      "title": "unable to pass dict into tpl function (actually anything but &lt;dot&gt;&#39;)",
      "problem": "I'm new to helm/go templating, and it seems that I still don't understand how context work. Can someone explain, where is problem and why dict cannot be used like this?\nI have this template in file `stubs/.test.yaml` (which I'd like to use with tpl function):\n```\n`test: abc\ntest2: {{.Values.key}}\n`\n```\nin values.yaml there is just:\n```\n`key: value\n`\n```\nand I include this template like this:\n```\n`{{ tpl (.Files.Get \"stubs/.test.yaml\" ) (.) }}\n`\n```\nproduces:\n```\n`test: abc\ntest2: value\n`\n```\nSo I loaded file from stubs/.test.yaml, and passed it current (root) context, and `.Values.key` was found and substituted. As expected.\nNow lets redefine `stubs/.test.yaml` as:\n```\n`test: abc\ntest2: {{.key}}\n`\n```\nand pass a dict as a context when substituting template, as:\n```\n`{{ tpl (.Files.Get \"stubs/.test.yaml\" ) (dict \"key\" .Values.key) }}\n`\n```\nso I'd expect the same result, since I passed dict in place of root context, dict has single value named `key`, with value taken from `.Values.key`, which is correctly referenced in that template. But I got:\n```\n`error calling tpl: cannot retrieve Template.Basepath from values inside tpl function: test: abc\ntest2: {{.key}}: \"BasePath\" is not a value\n`\n```\ninstead. What is happening here?\nEDIT: I was testing it more, and it's not about dict. I have to pass `.` as second parameter into tpl function, otherwise it fails.\nVersion 3.10",
      "solution": "I think I found answer in this issue\nIIUC: before `tpl` starts, it creates object `.Template` (see builtin_objects) in root context, where there are data about currently processed template. So if you are using `tpl` you must not change context and pass `.`, or you must pass something as context, from what is valid $.Template accessible.\nSo if we're passing dict as in example in my question:\n```\n`{{ tpl (.Files.Get \"stubs/.test.yaml\" ) (dict \"key\" .Values.key) }}\n`\n```\nit will not work. But if you extend dict definition like this:\n```\n`{{ tpl (.Files.Get \"stubs/.test.yaml\" ) (dict \"key\" .Values.key \"Template\" $.Template) }}\n`\n```\nit will work now.",
      "question_score": 3,
      "answer_score": 11,
      "created_at": "2022-10-01T19:50:14",
      "url": "https://stackoverflow.com/questions/73920549/unable-to-pass-dict-into-tpl-function-actually-anything-but-dot"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "helm",
      "question_id": 75475449,
      "title": "using Helm behind a proxy",
      "problem": "I want to use Helm on new environment that only have access to Internte via Proxy.\nHow can I use Helm commands with proxy?\nhelm repo add ....\nhelm repo update\nI tried to give proxy in command but I get following error:\nError: \"helm repo add\" requires 2 arguments",
      "solution": "You can set `HTTPS_PROXY=http://myproxy.com:8080` and then use helm commands.\nexample:\n```\n`HTTPS_PROXY=http://myproxy.com:8080 helm repo add minio https://helm.min.io/\n`\n```",
      "question_score": 3,
      "answer_score": 7,
      "created_at": "2023-02-16T17:47:58",
      "url": "https://stackoverflow.com/questions/75475449/using-helm-behind-a-proxy"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "helm",
      "question_id": 67884564,
      "title": "How to access variable out of current range control in helm template",
      "problem": "I have a helm template like this,\n```\n`1  {{- range $device_type := include \"example.supportedDevices\" . }}\n3  {{- $total_worker_sets := get $.Values.all_worker_sets $device_type }}\n4  {{- range $item := $total_worker_sets }}\n5  {{- if eq $device_type \"cpu\" }}\n6  {{- $type := \"cpu\" }}\n7  {{- else }}\n8  {{- $type := $item.name }}\n9  {{- end }}\n10  {{- range $i, $e := until ($item.worker_sets|int) }}\n11  ---\n12  apiVersion: apps/v1\n13  kind: StatefulSet\n14  metadata:\n15    name: {{ template \"example.fullname\" $ }}-worker-{{ $type }}-{{ $i }}\n16  spec:\n...\n{{- end }}\n{{- end }}\n{{- end }}\n`\n```\nand `helm lint` returns:\n[ERROR] templates/: parse error at (example/templates/worker.yaml:15): undefined variable \"$type\"",
      "solution": "You can use `$` to get to the root scope. So instead of `.type` you can use `$.type`. Here you used `$type`, that is why it's showing undefined (`undefined variable \"$type\"`, rather it would be `$.type`.",
      "question_score": 3,
      "answer_score": 3,
      "created_at": "2021-06-08T11:00:56",
      "url": "https://stackoverflow.com/questions/67884564/how-to-access-variable-out-of-current-range-control-in-helm-template"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "helm",
      "question_id": 67837291,
      "title": "elasticsearch installation in helm fails with statefulset error",
      "problem": "I'm installing elasticsearch in my EKS cluster using below commands\n```\n`helm repo add elastic https://helm.elastic.co\ncurl -O https://raw.githubusercontent.com/elastic/helm-charts/master/elasticsearch/values.yaml\nhelm install --name elasticsearch elastic/elasticsearch -f ./values.yaml\n`\n```\nIt fails with below error\nError: template: elasticsearch/templates/statefulset.yaml:298:27: executing \"elasticsearch/templates/statefulset.yaml\" at : can't evaluate field master in type interface {}",
      "solution": "I got the same issue. It was due to the following PR which change the role settings.\nDocumentation on master branch describes the new settings but it is not compatible with the last published chart version (7.13.2). Use instead values from the version you are using\n```\n`helm repo add elastic https://helm.elastic.co\nhelm repo update\nhelm search repo elastic/elasticsearch\n\nNAME                    CHART VERSION   APP VERSION     DESCRIPTION                                  \nelastic/elasticsearch   7.13.2          7.13.2          Official Elastic helm chart for Elasticsearch\n\n`\n```\nSo use `values.yaml` from `7.13` branch\n```\n`curl -O https://raw.githubusercontent.com/elastic/helm-charts/7.13/elasticsearch/values.yaml\n`\n```",
      "question_score": 3,
      "answer_score": 5,
      "created_at": "2021-06-04T14:31:50",
      "url": "https://stackoverflow.com/questions/67837291/elasticsearch-installation-in-helm-fails-with-statefulset-error"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "helm",
      "question_id": 74003216,
      "title": "Helm does not resolve local dependencies repository file path",
      "problem": "i'm triyng to do one Helm Chart for different environments. In many tutorials such scheme should works, but my structure does not read value from dependency repository. Helm just ignores it.\nI have following folder structure\n```\n`helm\n   - charts\n       - core-web\n           - Chart.yaml\n           - values.yaml\n           - templates\n       - frontend\n           - Chart.yaml\n           - values.yaml\n           - templates\n   - prod\n       - Chart.yaml\n       - values.yaml\n   - dev\n       - Chart.yaml\n       - values.yaml\n`\n```\nprod/Chart.yaml\n```\n`apiVersion: v1\nname: test\nversion: 1.0.0\n\ndependencies:\n  - name: core-web\n    version: \"1.37.0\"\n    repository: file://../charts/core-web/\n  - name: frontend\n    version: \"1.6.0\"\n    repository: \"file://../charts/frontend\"\n`\n```\nFrom helm folder i execute following command\n```\n`helm install ./prod --dry-run --generate-name -n sandbox -f prod/values.yaml\nError: INSTALLATION FAILED: found in Chart.yaml, but missing in charts/ directory: core-web, frontend\n`\n```\nIf i move charts forlder to prod folder, then everithing works.\nWhy helm does not accept file path from dependency repository?\nIt should: https://helm.sh/docs/helm/helm_dependency/\nThanks for the help.",
      "solution": "Try to replicate the issue, seems like a cache issue\nyou can verify that helm dependency on which path it's looking for `charts`.\n```\n`helm template test ./prod\n#output Error: found in Chart.yaml, but missing in charts/ directory: backend, web-app\n`\n```\nthen I tried to verify the path on which the helm looking\n```\n`helm dep ls ./prod\n`\n```\nfrom the output its clear it's still looking into the wrong path with the status `missing` as its still looking for chart inside `prod` folder.\n```\n`NAME    VERSION REPOSITORY              STATUS\nbackend 1.2.3   file://charts/backend/  missing\nweb-app 1.2.3   file://charts/web-app/  missing\n`\n```\nso to fix this\n```\n`helm dependency update  ./prod\n`\n```\nthen I can see\n```\n`helm dep ls ./prod\n`\n```",
      "question_score": 3,
      "answer_score": 7,
      "created_at": "2022-10-09T10:29:25",
      "url": "https://stackoverflow.com/questions/74003216/helm-does-not-resolve-local-dependencies-repository-file-path"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "helm",
      "question_id": 68982845,
      "title": "Fail to install helm-diff",
      "problem": "I have helm3 install from homebrew (https://formulae.brew.sh/formula/helm).\nI am going to install helm-diff (https://github.com/databus23/helm-diff).\nThis is my command:\n```\n`helm plugin install https://github.com/databus23/helm-diff\n`\n```\nThe result is:\n```\n`Error: plugin already exists\n`\n```\nI then:\n```\n`helm plugin list\n`\n```\nand returns\n```\n`NAME    VERSION DESCRIPTION\n`\n```\nAlso I have tried:\n```\n`helm plugin uninstall https://github.com/databus23/helm-diff\n`\n```\nand returns\n```\n`Error: Plugin: https://github.com/databus23/helm-diff not found\n`\n```\nWhat should I do to install this plugin?",
      "solution": "Go to ~/Library/helm/plugins to see if helm-diff exist. Then delete the directory. It should fix your problem.",
      "question_score": 3,
      "answer_score": 7,
      "created_at": "2021-08-30T12:48:36",
      "url": "https://stackoverflow.com/questions/68982845/fail-to-install-helm-diff"
    },
    {
      "tech": "kubernetes",
      "source": "stackoverflow",
      "tag": "helm",
      "question_id": 77912105,
      "title": "Upgrading kube-prometheus-stack via Helm to chart v56.2.1 fails on Grafana with Sensitive key error",
      "problem": "I recently tried upgrading kube-prometheus-stack on my AWS EKS Kubernetes cluster via Helm to chart `v56.2.1` using Terraform and it failed while upgrading Grafana with the following error:\n`Sensitive key \u2018auth.generic_oauth.client_secret\u2019 should not be defined explicitly in values. Use variable expansion instead.`\nI'm not specifying any client_secret via `values.yml` file for that Helm chart but we are using AzureAD for authentication.\nHow to fix this?",
      "solution": "There are 2 possible solutions to fix this issue:\n\nSet `grafana.assertNoLeakedSecrets` to `false` in the `values.yml` file.\n\nIf `auth.generic.oauth.client_secret` is specified in the `values.yml` file, remove it from that file and set `GF_AUTH_GENERIC_OAUTH_CLIENT_SECRET` as the environment variable for Grafana.\n\nReferences:\n\nGrafana - Sensitive Key Known Issue\nGrafana - Override configuration with environment variables",
      "question_score": 3,
      "answer_score": 6,
      "created_at": "2024-01-31T10:20:30",
      "url": "https://stackoverflow.com/questions/77912105/upgrading-kube-prometheus-stack-via-helm-to-chart-v56-2-1-fails-on-grafana-with"
    }
  ]
}