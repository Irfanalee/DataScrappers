{
  "tech": "terraform",
  "count": 289,
  "examples": [
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 67650019,
      "title": "Need to display sensitive data output variables in terraform",
      "problem": "The following code snippet is my terraform configuration to create an Azure SignalR Service:\n```\n`output \"signalrserviceconnstring\" {\n  value = azurerm_signalr_service.mysignalrservice.primary_connection_string\n  description = \"signalR service's primary connection string\"\n  sensitive = true\n}\n`\n```\nI got an error when `sensitive = true` is not included but I still do not see the output results on the console. What's the solution or workaround for this problem?",
      "solution": "The entire point of `sensitive = true` is to prevent the values from being displayed on the console every time you run `terraform apply`. You have to output the sensitive value explicitly, like this:\n```\n`terraform output signalrserviceconnstring\n`\n```\nI highly suggest reading the documentation.",
      "question_score": 80,
      "answer_score": 131,
      "created_at": "2021-05-22T15:25:30",
      "url": "https://stackoverflow.com/questions/67650019/need-to-display-sensitive-data-output-variables-in-terraform"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 68397972,
      "title": "How to use AWS account_id variable in Terraform",
      "problem": "I want access to my AWS Account ID in terraform. I am able to get at it with `aws_caller_identity` per the documentation. How do I then use the variable I created? In the below case I am trying to use it in an S3 bucket name:\n```\n`data \"aws_caller_identity\" \"current\" {}\noutput \"account_id\" {\n  value = data.aws_caller_identity.current.account_id\n}\n\nresource \"aws_s3_bucket\" \"test-bucket\" {\n  bucket = \"test-bucket-${account_id}\"\n}\n`\n```\nTrying to use the `account_id` variable in this way gives me the error `A reference to a resource type must be followed by at least one attribute access, specifying the resource name.` I expect I'm not calling it correctly?",
      "solution": "If you have a\n```\n`data \"aws_caller_identity\" \"current\" {}\n`\n```\nthen you can define a `local` for that value:\n```\n`locals {\n    account_id = data.aws_caller_identity.current.account_id\n}\n`\n```\nand then use it like\n```\n`output \"account_id\" {\n  value = local.account_id // via local or just\n  value = data.aws_caller_identity.current.account_id\n}\n\nresource \"aws_s3_bucket\" \"test-bucket\" {\n  bucket = \"test-bucket-${local.account_id}\"\n}\n`\n```\nTerraform resolves the `locals` based on their dependencies so you can create `locals` that depend on other `locals`, on `resources`, on `data` blocks, etc.",
      "question_score": 68,
      "answer_score": 111,
      "created_at": "2021-07-15T18:59:41",
      "url": "https://stackoverflow.com/questions/68397972/how-to-use-aws-account-id-variable-in-terraform"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 76129509,
      "title": "Terraform will damage your computer on macOS Intel",
      "problem": "I have problem with terraform on my macOS Ventura 13.3.1. When I try to initialize terragrunt:\n\nterragrunt init\n\nI have information that\n\nTerraform will damage your computer\n\nMy colleague is using M1 and terraform version 1.0.11 and he don't have problem. I tried latest version and also 1.0.11, but I still have this error. I installed terraform via tfenv.",
      "solution": "As per the announcement from HashiCorp:\n\nThe certificate used to sign Apple artifacts was rotated on January 23rd, with existing artifacts re-signed with the new certificate. The previous signing key was revoked on April 24th, 2023.\n\nTo fix the issue:\n\nAfter certificate revocation, users are expected to encounter errors using Apple artifacts that were downloaded before January 23rd.\nUsers will need to re-download Apple artifacts from the Releases Site, which have been signed using the new certificate.",
      "question_score": 53,
      "answer_score": 32,
      "created_at": "2023-04-28T14:18:29",
      "url": "https://stackoverflow.com/questions/76129509/terraform-will-damage-your-computer-on-macos-intel"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 65838989,
      "title": "&quot;Variables may not be used here&quot; during terraform init",
      "problem": "I am using Terraform snowflake plugins. I want to use `${terraform.workspace}` variable in `terraform` scope.\n```\n`terraform {\n  required_providers {\n    snowflake = {\n      source  = \"chanzuckerberg/snowflake\"\n      version = \"0.20.0\"\n    }\n  }\n  backend \"s3\" {\n    bucket         = \"data-pf-terraform-backend-${terraform.workspace}\"\n    key            = \"backend/singlife/landing\"\n    region         = \"ap-southeast-1\"\n    dynamodb_table = \"data-pf-snowflake-terraform-state-lock-${terraform.workspace}\"\n  }\n}\n`\n```\nBut I got this error. Variables are not available in this scope?\n```\n`Error: Variables not allowed\n\n  on provider.tf line 9, in terraform:\n   9:     bucket         = \"data-pf-terraform-backend-${terraform.workspace}\"\n\nVariables may not be used here.\n\nError: Variables not allowed\n\n  on provider.tf line 12, in terraform:\n  12:     dynamodb_table = \"data-pf-snowflake-terraform-state-lock-${terraform.workspace}\"\n\nVariables may not be used here.\n`\n```",
      "solution": "Set backend.tf\n`terraform {\n  backend \"azurerm\" {}\n}\n`\n\nCreate a file backend.conf\n`storage_account_name = \"deploymanager\"\ncontainer_name       = \"terraform\"\nkey                  = \"production.terraform.tfstate\"\n`\n\nRun:\n`terraform init -backend-config=backend.conf\n`",
      "question_score": 51,
      "answer_score": 85,
      "created_at": "2021-01-22T04:41:53",
      "url": "https://stackoverflow.com/questions/65838989/variables-may-not-be-used-here-during-terraform-init"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 71078462,
      "title": "Terraform AWS Provider Error: Value for unconfigurable attribute. Can&#39;t configure a value for &quot;acl&quot;: its value will be decided automatically",
      "problem": "Just today, whenever I run `terraform apply`, I see an error something like this: `Can't configure a value for \"lifecycle_rule\": its value will be decided automatically based on the result of applying this configuration.`\nIt was working yesterday.\nFollowing is the command I run: `terraform init && terraform apply`\nFollowing is the list of initialized provider plugins:\n`- Finding latest version of hashicorp/archive...\n- Finding latest version of hashicorp/aws...\n- Finding latest version of hashicorp/null...\n- Installing hashicorp/null v3.1.0...\n- Installed hashicorp/null v3.1.0 (signed by HashiCorp)\n- Installing hashicorp/archive v2.2.0...\n- Installed hashicorp/archive v2.2.0 (signed by HashiCorp)\n- Installing hashicorp/aws v4.0.0...\n- Installed hashicorp/aws v4.0.0 (signed by HashiCorp)\n`\nFollowing are the errors:\n`Acquiring state lock. This may take a few moments...\nReleasing state lock. This may take a few moments...\n\u2577\n\u2502 Error: Value for unconfigurable attribute\n\u2502 \n\u2502   with module.ssm-parameter-store-backup.aws_s3_bucket.this,\n\u2502   on .terraform/modules/ssm-parameter-store-backup/s3_backup.tf line 1, in resource \"aws_s3_bucket\" \"this\":\n\u2502    1: resource \"aws_s3_bucket\" \"this\" {\n\u2502 \n\u2502 Can't configure a value for \"lifecycle_rule\": its value will be decided\n\u2502 automatically based on the result of applying this configuration.\n\u2575\n\u2577\n\u2502 Error: Value for unconfigurable attribute\n\u2502 \n\u2502   with module.ssm-parameter-store-backup.aws_s3_bucket.this,\n\u2502   on .terraform/modules/ssm-parameter-store-backup/s3_backup.tf line 1, in resource \"aws_s3_bucket\" \"this\":\n\u2502    1: resource \"aws_s3_bucket\" \"this\" {\n\u2502 \n\u2502 Can't configure a value for \"server_side_encryption_configuration\": its\n\u2502 value will be decided automatically based on the result of applying this\n\u2502 configuration.\n\u2575\n\u2577\n\u2502 Error: Value for unconfigurable attribute\n\u2502 \n\u2502   with module.ssm-parameter-store-backup.aws_s3_bucket.this,\n\u2502   on .terraform/modules/ssm-parameter-store-backup/s3_backup.tf line 3, in resource \"aws_s3_bucket\" \"this\":\n\u2502    3:   acl    = \"private\"\n\u2502 \n\u2502 Can't configure a value for \"acl\": its value will be decided automatically\n\u2502 based on the result of applying this configuration.\n\u2575\nERRO[0012] 1 error occurred:\n        * exit status 1\n`\nMy code is as follows:\n```\n`resource \"aws_s3_bucket\" \"this\" {\n  bucket = \"${var.project}-${var.environment}-ssm-parameter-store-backups-bucket\"\n  acl    = \"private\"\n\n  server_side_encryption_configuration {\n    rule {\n      apply_server_side_encryption_by_default {\n        kms_master_key_id = data.aws_kms_key.s3.arn\n        sse_algorithm     = \"aws:kms\"\n      }\n    }\n  }\n\n  lifecycle_rule {\n    id      = \"backups\"\n    enabled = true\n\n    prefix = \"backups/\"\n\n    transition {\n      days          = 90\n      storage_class = \"GLACIER_IR\"\n    }\n\n    transition {\n      days          = 180\n      storage_class = \"DEEP_ARCHIVE\"\n    }\n\n    expiration {\n      days = 365\n    }\n  }\n\n  tags = {\n    Name        = \"${var.project}-${var.environment}-ssm-parameter-store-backups-bucket\"\n    Environment = var.environment\n  }\n}\n`\n```",
      "solution": "Terraform AWS Provider is upgraded to version 4.0.0 which is published on 10 February 2022.\nMajor changes in the release include:\n\nVersion 4.0.0 of the AWS Provider introduces significant changes to the aws_s3_bucket resource.\nVersion 4.0.0 of the AWS Provider will be the last major version to support EC2-Classic resources as AWS plans to fully retire EC2-Classic Networking. See the AWS News Blog for additional details.\nVersion 4.0.0 and 4.x.x versions of the AWS Provider will be the last versions compatible with Terraform 0.12-0.15.\n\nThe reason for this change by Terraform is as follows: To help distribute the management of S3 bucket settings via independent resources, various arguments and attributes in the `aws_s3_bucket` resource have become read-only. Configurations dependent on these arguments should be updated to use the corresponding `aws_s3_bucket_*` resource. Once updated, `new aws_s3_bucket_*` resources should be imported into Terraform state.\nSo, I updated my code accordingly by following the guide here: Terraform AWS Provider Version 4 Upgrade Guide | S3 Bucket Refactor\nThe new working code looks like this:\n```\n`resource \"aws_s3_bucket\" \"this\" {\n  bucket = \"${var.project}-${var.environment}-ssm-parameter-store-backups-bucket\"\n\n  tags = {\n    Name        = \"${var.project}-${var.environment}-ssm-parameter-store-backups-bucket\"\n    Environment = var.environment\n  }\n}\n\nresource \"aws_s3_bucket_acl\" \"this\" {\n  bucket = aws_s3_bucket.this.id\n  acl    = \"private\"\n}\n\nresource \"aws_s3_bucket_server_side_encryption_configuration\" \"this\" {\n  bucket = aws_s3_bucket.this.id\n\n  rule {\n    apply_server_side_encryption_by_default {\n      kms_master_key_id = data.aws_kms_key.s3.arn\n      sse_algorithm     = \"aws:kms\"\n    }\n  }\n}\n\nresource \"aws_s3_bucket_lifecycle_configuration\" \"this\" {\n  bucket = aws_s3_bucket.this.id\n\n  rule {\n    id     = \"backups\"\n    status = \"Enabled\"\n\n    filter {\n      prefix = \"backups/\"\n    }\n\n    transition {\n      days          = 90\n      storage_class = \"GLACIER_IR\"\n    }\n\n    transition {\n      days          = 180\n      storage_class = \"DEEP_ARCHIVE\"\n    }\n\n    expiration {\n      days = 365\n    }\n  }\n}\n`\n```\nIf you don't want to upgrade your Terraform AWS Provider version to 4.0.0, you can use the existing or older version by specifying it explicitly in the code as below:\n```\n`terraform {\n  required_version = \"~> 1.0.11\"\n  required_providers {\n    aws  = \"~> 3.73.0\"\n  }\n}\n`\n```",
      "question_score": 47,
      "answer_score": 48,
      "created_at": "2022-02-11T11:20:18",
      "url": "https://stackoverflow.com/questions/71078462/terraform-aws-provider-error-value-for-unconfigurable-attribute-cant-configur"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 67204811,
      "title": "Terraform: Failed to install provider, doesn&#39;t match checksums from dependency lock file",
      "problem": "I use a CI system to compile terraform providers and bundle them into an image, but every time I run terraform init, I am getting the following error/failure.\n```\n`\u2502 Error: Failed to install provider\n\u2502 \n\u2502 Error while installing rancher/rancher2 v1.13.0: the current package for\n\u2502 registry.terraform.io/rancher/rancher2 1.13.0 doesn't match any of the\n\u2502 checksums previously recorded in the dependency lock file\n`\n```\nThis message is repeated for all of the providers listed in my provider file, which looks like this:\n```\n`terraform {\n  required_version = \">= 0.13\"\n\n  required_providers {\n    azurerm = {\n      source  = \"hashicorp/azurerm\"\n      version = \"2.55.0\"\n    }\n    github = {\n      source  = \"integrations/github\"\n      version = \"4.8.0\"\n    }\n  }\n...snip...\n}\n`\n```\nThe terraform hcl lock file is stored in the repo and it's only when the lock file exists in the repo that these errors appear and terraform init fails. What could be the cause?",
      "solution": "The issue is that my local workstation is a Mac which uses the darwin platform, so all of the providers are downloaded for darwin and the hashes stored in the lockfile for that platform. When the CI system, which is running on Linux runs, it attempts to retrieve the providers listed in the lockfile, but the checksums don't match because they use a different platform.\nThe solution is to use the following command locally to generate a new terraform dependency lock file with all of the platforms for terraform, other systems running on different platforms will then be able to obey the dependency lock file.\n```\n`terraform providers lock -platform=windows_amd64 -platform=darwin_amd64 -platform=linux_amd64\n`\n```",
      "question_score": 43,
      "answer_score": 72,
      "created_at": "2021-04-22T01:38:13",
      "url": "https://stackoverflow.com/questions/67204811/terraform-failed-to-install-provider-doesnt-match-checksums-from-dependency-l"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 67368339,
      "title": "Error installing provider &quot;aws&quot;: openpgp: signature made by unknown entity",
      "problem": "I am using terraform version 0.11.13, and this afternoon I am getting the following error in terraform init step\nDoes it mean I've to upgrade the terraform version, is there a deprecation for this version for aws provider?\nFull logs:\n```\n`Successfully configured the backend \"s3\"! Terraform will automatically\nuse this backend unless the backend configuration changes.\n\n[1mInitializing provider plugins...\n- Checking for available provider plugins on https://releases.hashicorp.com...\n\nError installing provider \"aws\": openpgp: signature made by unknown entity.\n\nTerraform analyses the configuration and state and automatically downloads\nplugins for the providers used. However, when attempting to download this\nplugin an unexpected error occured.\n\nThis may be caused if for some reason Terraform is unable to reach the\nplugin repository. The repository may be unreachable if access is blocked\nby a firewall.\n\nIf automatic installation is not possible or desirable in your environment,\nyou may alternatively manually install plugins by downloading a suitable\ndistribution package and placing the plugin's executable file in the\nfollowing directory:\n    terraform.d/plugins/linux_amd64\n`\n```",
      "solution": "Important - While this answer can solve the immediate problem, it creates  a potential security risk by disabling the security check. Use with caution\n\nyou can also do\n\n`terraform init -verify-plugins=false`\n\nThis worked for me.",
      "question_score": 33,
      "answer_score": 12,
      "created_at": "2021-05-03T13:41:03",
      "url": "https://stackoverflow.com/questions/67368339/error-installing-provider-aws-openpgp-signature-made-by-unknown-entity"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 66281882,
      "title": "How can I get `terraform init` to run on my Apple Silicon Macbook Pro for the Google Provider?",
      "problem": "When I run `terraform init` for my Google Cloud Platform project on my Apple Silicon macbook pro I get this error.\n```\n`Provider registry.terraform.io/hashicorp/google v3.57.0 does not have a package available for your current platform, darwin_arm64.\n`\n```\nHow can I work around this? I thought that the Rosetta2 emulator would check this box, but alas...",
      "solution": "Most providers already have packages available in newer versions.\nYou can update the provider via:\n`terraform init -upgrade`\nIf this route is not acceptable for you or if it does not solve the problem, look at the answer below.\nBuild Terraform's GCP provider from scratch! I modified this walkthrough. https://github.com/hashicorp/terraform/issues/27257#issuecomment-754777716\n```\n`brew install --build-from-source terraform\n`\n```\nThis will install Golang as well (and that appears to be working as of this post)\n```\n`git clone https://github.com/hashicorp/terraform-provider-google.git\ncd terraform-provider-google\ngit checkout v3.22.0\ngo get -d github.com/pavius/impi/cmd/impi\nmake tools\ngo fmt\nmake build\n`\n```\nThe following directory probably does not already exist so lets create it and copy the binary we just built.\n```\n`mkdir -p ${HOME}/.terraform.d/plugins/registry.terraform.io/hashicorp/google/3.22.0/darwin_arm64\ncp ${HOME}/go/bin/terraform-provider-google ${HOME}/.terraform.d/plugins/registry.terraform.io/hashicorp/google/3.22.0/darwin_arm64\n`\n```\nNote that `${HOME}/go` is where your golang install will be located if you don't already have `${GOPATH}` already defined. If you do, then modify the above commands to account for the location of your new build binaries.\n```\n`cp ${GOPATH}/bin/terraform-provider-google ${HOME}/.terraform.d/plugins/registry.terraform.io/hashicorp/google/3.22.0/darwin_arm64\n`\n```\nAfter going back to my project voila!\n```\n`\u279c terraform init\n\nInitializing the backend...\n\nInitializing provider plugins...\n- Finding latest version of hashicorp/google...\n- Installing hashicorp/google v3.22.0...\n- Installed hashicorp/google v3.22.0 (unauthenticated)\n\nTerraform has created a lock file .terraform.lock.hcl to record the provider\nselections it made above. Include this file in your version control repository\nso that Terraform can guarantee to make the same selections by default when\nyou run \"terraform init\" in the future.\n\nTerraform has been successfully initialized!\n\nYou may now begin working with Terraform. Try running \"terraform plan\" to see\nany changes that are required for your infrastructure. All Terraform commands\nshould now work.\n\nIf you ever set or change modules or backend configuration for Terraform,\nrerun this command to reinitialize your working directory. If you forget, other\n\n`\n```",
      "question_score": 33,
      "answer_score": 17,
      "created_at": "2021-02-19T18:00:10",
      "url": "https://stackoverflow.com/questions/66281882/how-can-i-get-terraform-init-to-run-on-my-apple-silicon-macbook-pro-for-the-go"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 75254685,
      "title": "GPG error: https://apt.releases.hashicorp.com bionic InRelease: The following signatures couldn&#39;t be verified because the public key is not available",
      "problem": "I ran this command to update packages in my ubuntu VM.\n```\n`sudo apt-get update\n`\n```\nIt gave me the below error at the end.\n```\n`Err:5 https://apt.releases.hashicorp.com bionic InRelease\n  The following signatures couldn't be verified because the public key is not available: NO_PUBKEY XXXXXXXXXXXXXXXX\nFetched 12.0 kB in 1s (10.4 kB/s)\nReading package lists... Done\nW: An error occurred during the signature verification. The repository is not updated and the previous index files will be used. GPG error: https://apt.releases.hashicorp.com bionic InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY XXXXXXXXXXXXXXXX\nW: Failed to fetch https://apt.releases.hashicorp.com/dists/bionic/InRelease  The following signatures couldn't be verified because the public key is not available: NO_PUBKEY XXXXXXXXXXXXXXXX\nW: Some index files failed to download. They have been ignored, or old ones used instead.\n`\n```\nWhat does this mean and how can I fix it?",
      "solution": "This means that the gpg key for this HashiCorp repository is not available in the apt-key database.\nAs the fix, it can be re-added with the below commands.\n```\n`# GPG is required for the package signing key\nsudo apt install gpg\n\n# Download the signing key to a new keyring\nwget -O- https://apt.releases.hashicorp.com/gpg | gpg --dearmor | sudo tee /usr/share/keyrings/hashicorp-archive-keyring.gpg\n\n# Verify the key's fingerprint\ngpg --no-default-keyring --keyring /usr/share/keyrings/hashicorp-archive-keyring.gpg --fingerprint\n\n# The fingerprint must match 798A EC65 4E5C 1542 8C8E 42EE AA16 FCBC A621 E701, which can also be verified at https://www.hashicorp.com/security under \"Linux Package Checksum Verification\".\n\n# Add the HashiCorp repo\necho \"deb [signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] https://apt.releases.hashicorp.com $(lsb_release -cs) main\" | sudo tee /etc/apt/sources.list.d/hashicorp.list\n\n# apt update successfully\nsudo apt update\n`\n```\nNote that these commands were taken from Hashicorp's Official Packaging Guide.",
      "question_score": 30,
      "answer_score": 85,
      "created_at": "2023-01-27T07:21:14",
      "url": "https://stackoverflow.com/questions/75254685/gpg-error-https-apt-releases-hashicorp-com-bionic-inrelease-the-following-si"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 66427129,
      "title": "Terraform: Error: Kubernetes cluster unreachable: invalid configuration",
      "problem": "After deleting kubernetes cluster with \"terraform destroy\" I can't create it again anymore.\n\"terraform apply\" returns the following error message:\n\nError: Kubernetes cluster unreachable: invalid configuration: no\nconfiguration has been provided, try setting KUBERNETES_MASTER\nenvironment variable\n\nHere is the terraform configuration:\n```\n`terraform {\n  backend \"s3\" {\n    bucket = \"skyglass-msur\"\n    key    = \"terraform/backend\"\n    region = \"us-east-1\"\n  }\n}\n\nlocals {\n  env_name         = \"staging\"\n  aws_region       = \"us-east-1\"\n  k8s_cluster_name = \"ms-cluster\"\n}\n\nvariable \"mysql_password\" {\n  type        = string\n  description = \"Expected to be retrieved from environment variable TF_VAR_mysql_password\"\n}\n\nprovider \"aws\" {\n  region = local.aws_region\n}\n\ndata \"aws_eks_cluster\" \"msur\" {\n  name = module.aws-kubernetes-cluster.eks_cluster_id\n}\n\nmodule \"aws-network\" {\n  source = \"github.com/skyglass-microservices/module-aws-network\"\n\n  env_name              = local.env_name\n  vpc_name              = \"msur-VPC\"\n  cluster_name          = local.k8s_cluster_name\n  aws_region            = local.aws_region\n  main_vpc_cidr         = \"10.10.0.0/16\"\n  public_subnet_a_cidr  = \"10.10.0.0/18\"\n  public_subnet_b_cidr  = \"10.10.64.0/18\"\n  private_subnet_a_cidr = \"10.10.128.0/18\"\n  private_subnet_b_cidr = \"10.10.192.0/18\"\n}\n\nmodule \"aws-kubernetes-cluster\" {\n  source = \"github.com/skyglass-microservices/module-aws-kubernetes\"\n\n  ms_namespace       = \"microservices\"\n  env_name           = local.env_name\n  aws_region         = local.aws_region\n  cluster_name       = local.k8s_cluster_name\n  vpc_id             = module.aws-network.vpc_id\n  cluster_subnet_ids = module.aws-network.subnet_ids\n\n  nodegroup_subnet_ids     = module.aws-network.private_subnet_ids\n  nodegroup_disk_size      = \"20\"\n  nodegroup_instance_types = [\"t3.medium\"]\n  nodegroup_desired_size   = 1\n  nodegroup_min_size       = 1\n  nodegroup_max_size       = 5\n}\n\n# Create namespace\n# Use kubernetes provider to work with the kubernetes cluster API\nprovider \"kubernetes\" {\n  # load_config_file       = false\n  cluster_ca_certificate = base64decode(data.aws_eks_cluster.msur.certificate_authority.0.data)\n  host                   = data.aws_eks_cluster.msur.endpoint\n  exec {\n    api_version = \"client.authentication.k8s.io/v1alpha1\"\n    command     = \"aws-iam-authenticator\"\n    args        = [\"token\", \"-i\", \"${data.aws_eks_cluster.msur.name}\"]\n  }\n}\n\n# Create a namespace for microservice pods\nresource \"kubernetes_namespace\" \"ms-namespace\" {\n  metadata {\n    name = \"microservices\"\n  }\n}\n`\n```\nP.S. There seems to be the issue with terraform kubernetes provider for 0.14.7\nI couldn't use \"load_config_file\" = false in this version, so I had to comment it, which seems to be the reason of this issue.\nP.P.S. It could also be the issue with outdated cluster_ca_certificate, which terraform tries to use: deleting this certificate could be enough, although I'm not sure, where it is stored.",
      "solution": "Before doing something radical like manipulating the state directly, try setting the KUBE_CONFIG_PATH variable:\n```\n`export KUBE_CONFIG_PATH=/path/to/.kube/config\n`\n```\nAfter this rerun the `plan` or `apply` command.\nThis has fixed the issue for me.",
      "question_score": 29,
      "answer_score": 58,
      "created_at": "2021-03-01T18:53:50",
      "url": "https://stackoverflow.com/questions/66427129/terraform-error-kubernetes-cluster-unreachable-invalid-configuration"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 65934606,
      "title": "What does &quot;eksctl create iamserviceaccount&quot; do under the hood on an EKS cluster?",
      "problem": "AWS supports IAM Roles for Service Accounts (IRSA) that allows cluster operators to map AWS IAM Roles to Kubernetes Service Accounts.\nTo do so, one has to create an iamserviceaccount in an EKS cluster:\n```\n`eksctl create iamserviceaccount \\\n    --name  \\\n    --namespace kube-system \\\n    --cluster  \\\n    --attach-policy-arn  \\\n    --approve \\\n    --override-existing-serviceaccounts\n`\n```\nThe problem is that I don't want to use the above `eksctl` command because I want to declare my infrastructure using `terraform`.\nDoes eksctl command do anything other than creating a service account? If it only creates a service account, what is the `YAML` representation of it?",
      "solution": "After `Vasili Angapov`'s helps, now I can answer the question:\nYes It does more than just creating a service account. It does three things:\n\nIt Creates an IAM role.\nIt attaches the desired iam-policy (--attach-policy-arn\n) to the created IAM role.\nIt creates a new kubernetes service account annotated with the arn of the created IAM role.\n\nNow It's easy to declare the above steps using kubernetes and aws providers in terraform.",
      "question_score": 27,
      "answer_score": 3,
      "created_at": "2021-01-28T10:48:04",
      "url": "https://stackoverflow.com/questions/65934606/what-does-eksctl-create-iamserviceaccount-do-under-the-hood-on-an-eks-cluster"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 71606880,
      "title": "setting up terraform v0.14.0 on Apple M1",
      "problem": "Any pointers how to setup Terraform v0.14.0 on a Apple M1 , as tfenv doesn't support v0.14.0 on Apple M1\n```\n`tfenv install v0.14.0\nInstalling Terraform v0.14.0\nDownloading release tarball from https://releases.hashicorp.com/terraform/0.14.0/terraform_0.14.0_darwin_arm64.zip\ncurl: (22) The requested URL returned error: 403\n\nTarball download failed\n`\n```",
      "solution": "If you are using `tfenv`, you can override the architecture with `TFENV_ARCH` environment variables: `TFENV_ARCH=amd64`. See docs.\nIf you are not using `tfenv`:\nTerraform is a simple executable, you can download it and unzip it from here:\nhttps://releases.hashicorp.com/terraform/0.14.0/:\n```\n`wget https://releases.hashicorp.com/terraform/0.14.0/terraform_0.14.0_darwin_amd64.zip\n\nunzip terraform_0.14.0_darwin_amd64.zip\n`\n```\nPlease note, there is no `arm64` build for osx, but the `amd64` works just fine on a Mac M1.\nNow you can copy the extracted executable in a folder like `/usr/local/bin`, which should be on your `PATH`, so you can run `terraform` command from anywhere in your system.",
      "question_score": 23,
      "answer_score": 21,
      "created_at": "2022-03-24T18:25:14",
      "url": "https://stackoverflow.com/questions/71606880/setting-up-terraform-v0-14-0-on-apple-m1"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 70689512,
      "title": "Terraform check if resource exists before creating it",
      "problem": "Is there a way in Terraform to check if a resource in Google Cloud exists prior to trying to create it?\nI want to check if the following resources below exist in my CircleCI CI/CD pipeline during a job. I have access to terminal commands, bash, and gcloud commands. If the resources do exist, I want to use them. If they do not exist, I want to create them. I am doing this logic in CircleCI's config.yml as steps where I have access to terminal commands and bash. My goal is to create my necessary infrastructure (resources) in GCP when they are needed, otherwise use them if they are created, without getting Terraform errors in my CI/CD builds.\nIf I try to create a resource that already exists, Terraform apply will result in an error saying something like, \"you already own this resource,\" and now my CI/CD job fails.\nBelow is pseudo code describing the resources I am trying to get.\n```\n`resource \"google_artifact_registry_repository\" \"main\" {\n  # this is the repo for hosting my Docker images\n  # it does not have a data source afaik because it is beta\n}\n`\n```\nFor my google_artifact_registry_repository resource. One approach I have is to do a Terraform apply using a data source block and see if a value is returned. The problem with this is that the google_artifact_registry_repository does not have a data source block. Therefore, I must create this resource once using a resource block and every CI/CD build thereafter can rely on it being there. Is there a work-around to read that it exists?\n```\n`resource \"google_storage_bucket\" \"bucket\" {\n  # bucket containing the folder below\n}\n\nresource \"google_storage_bucket_object\" \"content_folder\" {\n  # folder containing Terraform default.tfstate for my Cloud Run Service\n}\n`\n```\nFor my google_storage_bucket and google_storage_bucket_object resources. If I do a Terraform apply using a data source block to see if these exist, one issue I run into is when the resources are not found, Terraform takes forever to return that status. It would be great if I could determine if a resource exists within like 10-15 seconds or something, and if not assume these resources do not exist.\n```\n`data \"google_storage_bucket\" \"bucket\" {\n  # bucket containing the folder below\n}\n\noutput bucket {\n  value = data.google_storage_bucket.bucket\n}\n`\n```\nWhen the resource exists, I can use Terraform output bucket to get that value. If it does not exist, Terraform takes too long to return a response. Any ideas on this?",
      "solution": "TF does not have any built-in tools for checking if there are pre-existing resources, as this is not what TF is meant to do. However, you can create your own custom data source.\nUsing the custom data source, you can program any logic you want, including checking for pre-existing resources and return that information to TF for future use.",
      "question_score": 23,
      "answer_score": 16,
      "created_at": "2022-01-13T00:04:10",
      "url": "https://stackoverflow.com/questions/70689512/terraform-check-if-resource-exists-before-creating-it"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 66476009,
      "title": "error updating CloudFront Distribution (E32RNPFGEUHQ6J): InvalidWebACLId: Web ACL is not accessible by the requester",
      "problem": "I am using `terraform` to create a `web-acl` in `aws` and want to associate that `web-acl` with CloudFront distribution.\nSo, here's how my code looks like:\n```\n`provider \"aws\" {\n  alias  = \"east1\"\n  region = \"us-east-1\"\n}\n\n# -------------------------------------------\n# -------------------------------------------\n# Cloud Front\nmodule \"front_end_cloudfront\" {\n  source = \"./modules/front-end/CF\"\n\n  # CF_ALIASES = [\"terraformer-frontend.dev.effi.com.au\"]  \n  CF_LAMBDA_ARN = module.frontend_lambda.cf_lambda_qualified_arn\n  CF_BUCKET_DOMAIN_NAME = module.front_end_bucket.website_endpoint\n  CF_BUCKET_ORIGIN_ID = module.front_end_bucket.website_domain\n  CF_TAGS_LIST = { \"Name\" : \"terraformer-front-end-cloudfrontv2\" }\n  CF_CERTFICATE_ARN = var.CLOUDFRONT_US_EAST_1_ACM_ARN\n  # WEB_ACL = module.waf.web_acl_id\n  WEB_ACL = module.waf_cf.web_acl_id\n\n  depends_on = [module.waf_cf]\n}\n\n# -------------------------------------------\n# -------------------------------------------\n# WAF for CF\nmodule \"waf_cf\" {\n  source = \"./modules/waf\"\n\n  providers = {\n    aws = aws.east1\n  }  \n\n  WAF_NAME  = \"terraform-web-acl-cf\"\n  WAF_DESCRIPTION = \"terraform web acl-cf\"\n  WAF_SCOPE = \"CLOUDFRONT\"\n  WAF_RULE_NAME_1 = \"AWSManagedRulesCommonRuleSet\"\n  WAF_RULE_NAME_2 = \"AWSManagedRulesAmazonIpReputationList\"\n  WAF_RULE_NAME_3 = \"AWSManagedRulesLinuxRuleSet\"\n  WAF_RULE_NAME_4 = \"AWSManagedRulesKnownBadInputsRuleSet\"\n  WAF_VENDOR = \"AWS\"\n  WAF_METRIC_1 = \"aws-waf-logs-terraformer-metric\"\n  WAF_METRIC_2 = \"aws-waf-logs-terraformer-metric\"\n  WAF_METRIC_3 = \"aws-waf-logs-terraformer-metric\"\n  WAF_METRIC_4 = \"aws-waf-logs-terraformer-metric\"\n  WAF_TAG_LIST = {\n    \"Tag1\" : \"Name\"\n    \"Tag2\" : \"terraformer-rule-cf\"\n  }\n  WAF_METRIC = \"aws-waf-logs-friendly-metric-name\"\n  CLOUDWATCH_METRICS_ENABLED = false\n  SAMPLE_REQUESTS_ENABLED = false\n}\n\n`\n```\nThese are `terraform` modules I have wrote, the specific `resource` files for above modules are below respectively.\n```\n`# CF\nresource \"aws_cloudfront_distribution\" \"aws_cloudfront_distribution\" {\n  # aliases = var.CF_ALIASES\n\n  default_cache_behavior {\n    allowed_methods = [\"GET\", \"HEAD\"]\n    cached_methods  = [\"GET\", \"HEAD\"]\n    compress        = \"true\"\n    default_ttl     = \"0\"\n\n    forwarded_values {\n      cookies {\n        forward = \"none\"\n      }\n\n      query_string = \"false\"\n    }\n\n    lambda_function_association {\n      event_type   = \"origin-response\"\n      include_body = \"false\"\n      lambda_arn   = var.CF_LAMBDA_ARN\n    }\n\n    max_ttl                = \"0\"\n    min_ttl                = \"0\"\n    smooth_streaming       = \"false\"\n    target_origin_id       = var.CF_BUCKET_ORIGIN_ID\n    viewer_protocol_policy = \"redirect-to-https\"\n  }\n\n  enabled         = \"true\"\n  http_version    = \"http2\"\n  is_ipv6_enabled = \"true\"\n\n  origin {\n    custom_origin_config {\n      http_port                = \"80\"\n      https_port               = \"443\"\n      origin_keepalive_timeout = \"5\"\n      origin_protocol_policy   = \"http-only\"\n      origin_read_timeout      = \"30\"\n      origin_ssl_protocols     = [\"TLSv1\", \"TLSv1.1\", \"TLSv1.2\"]\n    }\n\n    domain_name = var.CF_BUCKET_DOMAIN_NAME\n    origin_id   = var.CF_BUCKET_ORIGIN_ID\n  }\n\n  price_class = \"PriceClass_All\"\n\n  restrictions {\n    geo_restriction {\n      restriction_type = \"none\"\n    }\n  }\n\n  retain_on_delete = \"false\"\n\n  tags = var.CF_TAGS_LIST\n\n  viewer_certificate {\n    acm_certificate_arn            = var.CF_CERTFICATE_ARN\n    cloudfront_default_certificate = \"false\"\n    minimum_protocol_version       = \"TLSv1.2_2018\"\n    ssl_support_method             = \"sni-only\"\n  }\n\n  web_acl_id = var.WEB_ACL\n}\n\n# WAF\nresource \"aws_wafv2_web_acl\" \"aws_wafv2_web_acl\" {\n  name        = var.WAF_NAME\n  description = var.WAF_DESCRIPTION\n  scope       = var.WAF_SCOPE\n\n  default_action {\n    allow {}\n  }\n\n  rule {\n    name     = var.WAF_RULE_NAME_1\n    priority = 1\n\n    override_action {\n      count {}\n    }\n\n    statement {\n      managed_rule_group_statement {\n        name        = var.WAF_RULE_NAME_1\n        vendor_name = var.WAF_VENDOR\n\n        # excluded_rule {\n        #   name = \"SizeRestrictions_QUERYSTRING\"\n        # }\n\n        # excluded_rule {\n        #   name = \"NoUserAgent_HEADER\"\n        # }\n      }\n    }\n\n    visibility_config {\n      cloudwatch_metrics_enabled = var.CLOUDWATCH_METRICS_ENABLED\n      metric_name                = var.WAF_METRIC_1\n      sampled_requests_enabled   = var.SAMPLE_REQUESTS_ENABLED\n    }\n  }\n\n  rule {\n    name     = var.WAF_RULE_NAME_2\n    priority = 2\n\n    override_action {\n      count {}\n    }\n\n    statement {\n      managed_rule_group_statement {\n        name        = var.WAF_RULE_NAME_2\n        vendor_name = var.WAF_VENDOR\n      }\n    }\n\n    visibility_config {\n      cloudwatch_metrics_enabled = var.CLOUDWATCH_METRICS_ENABLED\n      metric_name                = var.WAF_METRIC_2\n      sampled_requests_enabled   = var.SAMPLE_REQUESTS_ENABLED\n    }\n  }  \n\n  rule {\n    name     = var.WAF_RULE_NAME_3\n    priority = 3\n\n    override_action {\n      count {}\n    }\n\n    statement {\n      managed_rule_group_statement {\n        name        = var.WAF_RULE_NAME_3\n        vendor_name = var.WAF_VENDOR\n      }\n    }\n\n    visibility_config {\n      cloudwatch_metrics_enabled = var.CLOUDWATCH_METRICS_ENABLED\n      metric_name                = var.WAF_METRIC_3\n      sampled_requests_enabled   = var.SAMPLE_REQUESTS_ENABLED\n    }\n  } \n\n  rule {\n    name     = var.WAF_RULE_NAME_4\n    priority = 4\n\n    override_action {\n      count {}\n    }\n\n    statement {\n      managed_rule_group_statement {\n        name        = var.WAF_RULE_NAME_4\n        vendor_name = var.WAF_VENDOR\n      }\n    }\n\n    visibility_config {\n      cloudwatch_metrics_enabled = var.CLOUDWATCH_METRICS_ENABLED\n      metric_name                = var.WAF_METRIC_4\n      sampled_requests_enabled   = var.SAMPLE_REQUESTS_ENABLED\n    }\n  } \n\n  tags = var.WAF_TAG_LIST\n\n  visibility_config {\n    cloudwatch_metrics_enabled = var.CLOUDWATCH_METRICS_ENABLED\n    metric_name                = var.WAF_METRIC\n    sampled_requests_enabled   = var.SAMPLE_REQUESTS_ENABLED\n  }\n}\n`\n```\nBut I am getting the below error\n\nerror updating CloudFront Distribution (E32RNPFGEUHQ6J): InvalidWebACLId: Web ACL is not accessible by the requester.\n\nHere the `cloudfront` is created in `ap-southeast-2` region and the `waf` is created in `us-east-1` region.\nCan someone please help me on this?",
      "solution": "When using WAFv2, you need to specify the the ARN not the ID to `web_acl_id` in `aws_cloudfront_distribution`.\nSee the note here https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/cloudfront_distribution#web_acl_id\nor this GitHub issue https://github.com/hashicorp/terraform-provider-aws/issues/13902",
      "question_score": 22,
      "answer_score": 50,
      "created_at": "2021-03-04T14:27:45",
      "url": "https://stackoverflow.com/questions/66476009/error-updating-cloudfront-distribution-e32rnpfgeuhq6j-invalidwebaclid-web-ac"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 72480751,
      "title": "Why ${path.module} points to the current directory &quot;.&quot;, but not to the module directory?",
      "problem": "I have used the following code:\n```\n`module \"instance\" {\n  for_each               = var.worker_private_ip\n  source                 = \"../../modules/ec2\"\n  env                    = var.env\n  project_name           = var.project_name\n  ami                    = var.ami\n  instance_type          = var.instance_type\n  subnet_id              = module.vpc.subnet_id\n  vpc_security_group_ids = [module.security_group.security_group_id]\n  private_ip             = each.value\n  key_name               = var.public_key_name\n  user_data              = file(\"${path.module}/startup.sh.tpl\")\n}\n`\n```\nWhen I send terraform plan/apply command, I receive an error\n```\n`path.module is \".\"\n\n\u2502 Invalid value for \"path\" parameter: no file exists at \"./startup.sh.tpl\";\n`\n```\nSo, terraform searching for file in current directory, but I expect for searching in (../../modules/ec2/)\nI have startup.sh.tpl  file in module folder and if I set user_data to:\n```\n`user_data              = file(\"../../modules/ec2/startup.sh.tpl\")\n`\n```\neverything is fine, also if I move sciprt to current directory everything ok too, plan and apply commands are correct\nWhat am I doing wrong? Why `${path.module}` points to the current directory `\".\"`, but not to `../../modules/ec2/`?",
      "solution": "From the docs:\n\n`path.module` is the filesystem path of the module where the expression is placed.\n\nThis means that it will return the relative path between your project's root folder and the location where `path.module` is used.\nFor example:\n\nif you are using it in a `.tf` file which is inside your `../../modules/ec2/` folder, it will return `../../modules/ec2`;\nif your are using it inside your `main.tf` (or any `.tf` file) in your project's root folder, it will return `.`\n\nIn your case, if the user data is inside the module, probably there would be need for it to make it as an input.",
      "question_score": 20,
      "answer_score": 26,
      "created_at": "2022-06-02T20:21:54",
      "url": "https://stackoverflow.com/questions/72480751/why-path-module-points-to-the-current-directory-but-not-to-the-module-di"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 76773869,
      "title": "How to import resource which is held in a module, not root?",
      "problem": "I have an existing resource which i'd like to import into my config, to keep with style I have written this into a module, as below.\nWhen I try to import it (using `terraform import resource.module id`) I get an error, as below\n`>Before importing this resource, please create its configuration in the root module.`\nIf I add this to my root, it errors on a dupe resource.\nHow do I go about importing a resource which is not in my root module?\nCurrent structure:\n#modules.tf\n```\n`module my_module {\n   source ./modules\n   ...\n} \n`\n```\n#./modules/main.tf\n```\n`resource 'my_resource' 'my_resource_name' {\n   ...\n}\n`\n```\n#./modules/output.tf\n```\n`output {\n   value = ...\n}\n`\n```",
      "solution": "The easiest way to import resources into terraform is to first run `terraform plan`, check the command output and see what's the resource names it generates under the module.\nThen run `terraform import  `.\nSo in your case, you would probably need to run something like\n`terraform import 'module.my_module.my_resource.my_resource_name' 'id'\n`\nNote the quotes around the module name; if the module contains `count`, this is necessary so it's a good habit to get used to.",
      "question_score": 20,
      "answer_score": 24,
      "created_at": "2023-07-26T19:48:28",
      "url": "https://stackoverflow.com/questions/76773869/how-to-import-resource-which-is-held-in-a-module-not-root"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 66536427,
      "title": "Incorrect Service Networking config for instance: xxxx:SERVICE_NETWORKING_NOT_ENABLED",
      "problem": "I'm trying to replicate a SQL instance in GCP via terraform. The active instance has a public IP, however subnets from a secondary project are shared with the project hosing the SQL instance, and the SQL instance is associated with the secondary project's network.\nI've added the `private_network` setting properly (I think) in the `ip_configuration` section, however I'm getting the following error:\n\nError: Error, failed to create instance xxxx: googleapi: Error 400: Invalid request: Incorrect Service Networking config for instance: xxxx:xxxxx:SERVICE_NETWORKING_NOT_ENABLED., invalid\n\nI can't find much documentation when I google that particular error, and I'm relatively new to Terraform, so I'm hoping someone can point out what I'm missing from either this section of my Terraform config, or another resource altogether.\n```\n`resource \"google_sql_database_instance\" \"cloudsql-instance-qa\" {\n  depends_on       = [google_project_service.project_apis]\n  database_version = \"MYSQL_5_7\"\n  name             = \"${var.env_shorthand}-${var.resource_name}\"\n  project          = var.project_id\n  region           = var.region\n\n  settings {\n    activation_policy = \"ALWAYS\"\n    availability_type = \"ZONAL\"\n\n    backup_configuration {\n      binary_log_enabled             = \"true\"\n      enabled                        = \"true\"\n      point_in_time_recovery_enabled = \"false\"\n      start_time                     = \"15:00\"\n    }\n\n    crash_safe_replication = \"false\"\n    disk_autoresize        = \"true\"\n    disk_size              = \"5003\"\n    disk_type              = \"PD_SSD\"\n\n    ip_configuration {\n      ipv4_enabled    = \"true\"\n      private_network = \"projects/gcp-backend/global/networks/default\"\n      require_ssl     = \"false\"\n    }\n\n    location_preference {\n      zone = var.zone\n    }\n\n    maintenance_window {\n      day  = \"7\"\n      hour = \"4\"\n    }\n\n    pricing_plan     = \"PER_USE\"\n    replication_type = \"SYNCHRONOUS\"\n    tier             = \"db-n1-standard-1\"\n  }\n}\n`\n```",
      "solution": "If you see the following error:\n\nError: Error, failed to create instance xxxx: googleapi: Error 400:\nInvalid request: Incorrect Service Networking config for instance:\nxxxx:xxxxx:SERVICE_NETWORKING_NOT_ENABLED., invalid\n\nEnable the Service Networking API:\n```\n`gcloud services enable servicenetworking.googleapis.com --project=[PSM_PROJECT_NUMBER]\n`\n```\nGetting Started with the Service Networking API",
      "question_score": 19,
      "answer_score": 37,
      "created_at": "2021-03-08T21:07:21",
      "url": "https://stackoverflow.com/questions/66536427/incorrect-service-networking-config-for-instance-xxxxservice-networking-not-en"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 67115574,
      "title": "Terraform has no command named &quot;sh&quot;. Did you mean &quot;show&quot;?",
      "problem": "I'm trying to set up Terrafom validation on Gitlab CI.\nHowever a build fails with an error:\n\nTerraform has no command named \"sh\". Did you mean \"show\"?\n\nWhy does it happen? How could it be fixed?\nMy `.gitlab-ci.yml`\n```\n`image: hashicorp/terraform:light\n\nbefore_script:\n  - terraform init\n\nvalidate:\n  script:\n    - terraform validate\n`\n```",
      "solution": "You need to override the entrypoint in the terraform image so you have access to the shell.\n```\n`image: \n  name: hashicorp/terraform:light\n  entrypoint: [\"\"]\n\nbefore_script:\n  - terraform init\n\nvalidate:\n  script:\n    - terraform validate\n`\n```\nYou can also take a look at the official gitlab documentation how to integrate terraform with gitlab, as the have a template for that.",
      "question_score": 19,
      "answer_score": 30,
      "created_at": "2021-04-15T22:22:51",
      "url": "https://stackoverflow.com/questions/67115574/terraform-has-no-command-named-sh-did-you-mean-show"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 67297188,
      "title": "Terraform - Unable to run multiple commands in local exec",
      "problem": "I'm new to Terraform world. I'm trying to run a shell script using Terraform.\nBelow is the main.tf file\n```\n`#Executing shell script via Null Resource\n\nresource \"null_resource\" \"install_istio\" {\n provisioner \"local-exec\" {\n    command = Below is the install-istio.sh file which it needs to run\n```\n`#!/bin/sh\n\n# Download and install the Istio istioctl client binary\n\n# Specify the Istio version that will be leveraged throughout these instructions\nISTIO_VERSION=1.7.3\n\ncurl -sL \"https://github.com/istio/istio/releases/download/$ISTIO_VERSION/istioctl-$ISTIO_VERSION-linux-amd64.tar.gz\" | tar xz\n\nsudo mv ./istioctl /usr/local/bin/istioctl\nsudo chmod +x /usr/local/bin/istioctl\n\n# Install the Istio Operator on EKS\nistioctl operator init\n\n# The Istio Operator is installed into the istio-operator namespace. Query the namespace.\nkubectl get all -n istio-operator\n\n# Install Istio components\nistioctl profile dump default\n\n# Create the istio-system namespace and deploy the Istio Operator Spec to that namespace.\nkubectl create ns istio-system\nkubectl apply -f istio-eks.yaml\n\n# Validate the Istio installation\nkubectl get all -n istio-system\n\n`\n```\nI'm getting below warning:\n```\n`Warning: Interpolation-only expressions are deprecated\n  on .terraform/modules/istio_module/Istio-Operator/main.tf line 10, in resource \"null_resource\" \"install_istio\":\n  10:     working_dir = \"${path.module}\"\nTerraform 0.11 and earlier required all non-constant expressions to be\nprovided via interpolation syntax, but this pattern is now deprecated. To\nsilence this warning, remove the \"${ sequence from the start and the }\"\nsequence from the end of this expression, leaving just the inner expression.\nTemplate interpolation syntax is still used to construct strings from\nexpressions when the template includes multiple interpolation sequences or a\nmixture of literal strings and interpolations. This deprecation applies only\nto templates that consist entirely of a single interpolation sequence.\n`\n```\nThe above script in main.tf does run command in the background.\nCan someone help me with the missing part? How can I run multiple commands using local exec Also, How can I get rid of the warning message?\nAppreciate all your help, Thanks!",
      "solution": "I think there are two separate things going on here which are actually not related.\nThe main problem here is in how you've written your `local-exec` script:\n```\n`    command = This will become the following shell script to run:\n```\n`\"chmod +x install-istio.sh\"\n\"./install-istio.sh\"\n`\n```\nBy putting the first command line in quotes, you're telling the shell to try to run a program that is called `chmod +x install-istio.sh` without any arguments. That is, the shell will try to find an executable in your `PATH` called `chmod +x install-istio.sh`, rather than trying to run a command called just `chmod` with some arguments as I think you intended.\nRemove the quotes around the command lines to make this work. Quotes aren't needed here because neither of these commands contain any special characters that would require quoting:\n```\n`    command = \nThe warning message about interpolation-only expressions is unrelated to the problem of running these commands. This is telling you that you've used a legacy syntax that is still supported for backward compatibility but no longer recommended.\nIf you are using the latest version of Terraform at the time of writing (one of the v0.15 releases, or later) then you may be able to resolve this and other warnings like it by switching into this module directory and running `terraform fmt`, which is a command that updates your configuration to match the expected style conventions.\nAlternatively, you could manually change what that command would update automatically, which is to remove the redundant interpolation markers around `path.module`:\n```\n`    working_dir = path.module\n`\n```",
      "question_score": 18,
      "answer_score": 23,
      "created_at": "2021-04-28T11:09:34",
      "url": "https://stackoverflow.com/questions/67297188/terraform-unable-to-run-multiple-commands-in-local-exec"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 67276619,
      "title": "How to find the list of applicable instance types for an AMI",
      "problem": "Used `ami-0fd3c3c68a2a8066f` from `ap-south-1` region `http://cloud-images.ubuntu.com/locator/ec2/`, but unable to use t2.micro instance type against this.\n```\n`Error: Error launching source instance: InvalidParameterValue: The architecture 'x86_64' of the specified instance type does not match the architecture 'arm64' of the specified AMI. Specify an instance type and an AMI that have matching architectures, and try again. You can use 'describe-instance-types' or 'describe-images' to discover the architecture of the instance type or AMI.\n`\n```\nHow to find the list of applicable instance types for an AMI, before trying to launch an instance using terraform",
      "solution": "Using AWS CLI you can use describe-instance-types:\n```\n`aws ec2 describe-instance-types --filters Name=processor-info.supported-architecture,Values=arm64 --query \"InstanceTypes[*].InstanceType\" --output text\n`\n```\nE.g. output:\n```\n`r6gd.large  m6g.metal   m6gd.medium c6gd.metal  m6gd.12xlarge   c6g.16xlarge    r6g.large   r6gd.medium r6g.8xlarge m6gd.metal  r6gd.xlarge t4g.medium  r6gd.2xlarge    m6gd.xlarge c6g.xlarge  c6g.12xlarge    r6g.medium  a1.medium   m6g.xlarge  m6gd.4xlarge    t4g.nano    r6g.16xlarge\nt4g.2xlarge m6g.12xlarge    r6gd.8xlarge    a1.large    m6g.4xlarge c6gd.16xlarge   t4g.xlarge  c6g.large   m6g.large   c6gd.xlarge a1.metal    m6g.8xlarge m6gd.16xlarge   a1.xlarge   r6g.12xlarge    r6gd.metal  t4g.micro   r6g.4xlarge t4g.small   a1.2xlarge  r6gd.4xlarge    t4g.large\nm6g.16xlarge    c6g.4xlarge m6gd.2xlarge    c6gd.medium c6gd.8xlarge    r6gd.16xlarge   m6gd.8xlarge    c6g.2xlarge r6gd.12xlarge   a1.4xlarge  c6g.8xlarge r6g.2xlarge m6g.2xlarge m6g.medium  c6gd.large  c6g.medium  c6gd.2xlarge    r6g.metal   c6gd.4xlarge    m6gd.large  r6g.xlarge\n`\n```\nI don't see any equivalent in TF for that. In the worst case, you could define external data source for that.\nupdate\nThere is no single call to get the list of instance types based on ami. It must be done in two steps.\n\nUse aws_ami data source to get architecture of a given AMI.\nUse describe-instance-types to get instance types for that architecture.",
      "question_score": 17,
      "answer_score": 13,
      "created_at": "2021-04-27T05:31:19",
      "url": "https://stackoverflow.com/questions/67276619/how-to-find-the-list-of-applicable-instance-types-for-an-ami"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 70407525,
      "title": "Terraform Gives errors Failed to load plugin schemas",
      "problem": "I have below code which I am using for create s3 bucket and cloud front in aws through terraform but terraform gives error.\nI am using latest version of terraform cli exe for windows.\nMain.tf\nPlease find below code of main.tf file :\n```\n`terraform {\n  required_providers {\n    aws = {\n      source = \"hashicorp/aws\"\n      version = \"3.70.0\"\n    }\n  }\n}\n\nprovider \"aws\" {\n    access_key = \"${var.aws_access_key}\"\n    secret_key = \"${var.aws_secret_key}\"\n    region = \"${var.aws_region}\"\n}\n\nresource \"aws_s3_bucket\" \"mybucket\" {\n    bucket = \"${var.bucket_name}\"\n    acl = \"public-read\"\n\n    website {\n        redirect_all_requests_to = \"index.html\"\n    }\n\n    cors_rule {\n        allowed_headers = [\"*\"]\n        allowed_methods = [\"PUT\",\"POST\"]\n        allowed_origins = [\"*\"]\n        expose_headers = [\"ETag\"]\n        max_age_seconds = 3000\n    }\n\n    policy = Please find below error message:\n```\n`Error: Failed to load plugin schemas\n\u2502\n\u2502 Error while loading schemas for plugin components: Failed to obtain provider schema: Could not load the schema for provider registry.terraform.io/hashicorp/aws: failed to retrieve schema\n\u2502 from provider \"registry.terraform.io/hashicorp/aws\": Plugin did not respond: The plugin encountered an error, and failed to respond to the plugin.(*GRPCProvider).GetProviderSchema call. The\n\u2502 plugin logs may contain more details...\n`\n```\nPlease help to resolve the issue , I'm new in terraforms.\nP.S. This error generated while terraform plan",
      "solution": "It happens when initializing it fails to upgrade or detect corrupt cached providers, or if you have changed the version but when you only run `terraform init` it finds the version in cache and decides to go with it. Delete the terraform directory and lock file, and then init again `terraform init -upgrade`\nIf you're on running it on Apple M1 chip, you might as well need to set this:\n```\n`export GODEBUG=asyncpreemptoff=1;\n`\n```\n\nIf you are using `.terraformrc` or have `TF_PLUGIN_CACHE_DIR` set in your environment, consider cleaning the plugin cache directory.\n```\n`> cat ~/.terraformrc\nplugin_cache_dir   = \"$HOME/.terraform.d/plugin-cache\"\n\n> rm -rf ~/.terraform.d/plugin-cache/*\n`\n```\n\nhttps://discuss.hashicorp.com/t/terraform-aws-provider-panic-plugin-did-not-respond/23396\nhttps://github.com/hashicorp/terraform/issues/26104",
      "question_score": 16,
      "answer_score": 38,
      "created_at": "2021-12-18T23:06:26",
      "url": "https://stackoverflow.com/questions/70407525/terraform-gives-errors-failed-to-load-plugin-schemas"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 69079945,
      "title": "Terraform: Inappropriate value for attribute &quot;ingress&quot; while creating SG",
      "problem": "I'm creating a Security group using terraform, and when I'm running terraform plan. It is giving me an error like some fields are required, and all those fields are optional.\nTerraform Version: v1.0.5\nAWS Provider version: v3.57.0\n\nmain.tf\n\n```\n`resource \"aws_security_group\" \"sg_oregon\" {\n  name        = \"tf-sg\"\n  description = \"Allow web traffics\"\n  vpc_id      = aws_vpc.vpc_terraform.id\n\n  ingress = [\n    {\n      description      = \"HTTP\"\n      from_port        = 80\n      to_port          = 80\n      protocol         = \"tcp\"\n      cidr_blocks      = [\"0.0.0.0/0\"]  \n    },\n  {\n      description      = \"HTTPS\"\n      from_port        = 443\n      to_port          = 443\n      protocol         = \"tcp\"\n      cidr_blocks      = [\"0.0.0.0/0\"]  \n  },\n\n    {\n      description      = \"SSH\"\n      from_port        = 22\n      to_port          = 22\n      protocol         = \"tcp\"\n      cidr_blocks      = [\"0.0.0.0/0\"]  \n    }\n  ]\n\n  egress = [\n    {\n      description      = \"for all outgoing traffics\"\n      from_port        = 0\n      to_port          = 0\n      protocol         = \"-1\"\n      cidr_blocks      = [\"0.0.0.0/0\"]\n      ipv6_cidr_blocks = [\"::/0\"]\n      \n    }\n  ]\n\n  tags = {\n    Name = \"sg-for-subnet\"\n  }\n}\n`\n```\n\nerror in console\n\n```\n`\u2502 Inappropriate value for attribute \"ingress\": element 0: attributes \"ipv6_cidr_blocks\", \"prefix_list_ids\", \"security_groups\", and \"self\" are required.\n\n\u2502 Inappropriate value for attribute \"egress\": element 0: attributes \"prefix_list_ids\", \"security_groups\", and \"self\" are required.\n`\n```\nI'm following this doc: https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/security_group\nAny help would be appreciated.",
      "solution": "Since you are using Attributes as Blocks you have to provide values for all options:\n```\n`resource \"aws_security_group\" \"sg_oregon\" {\n  name        = \"tf-sg\"\n  description = \"Allow web traffics\"\n  vpc_id      = aws_vpc.vpc_terraform.id\n\n  ingress = [\n    {\n      description      = \"HTTP\"\n      from_port        = 80\n      to_port          = 80\n      protocol         = \"tcp\"\n      cidr_blocks      = [\"0.0.0.0/0\"]  \n      ipv6_cidr_blocks = []\n      prefix_list_ids = []\n      security_groups = []\n      self = false\n    },\n  {\n      description      = \"HTTPS\"\n      from_port        = 443\n      to_port          = 443\n      protocol         = \"tcp\"\n      cidr_blocks      = [\"0.0.0.0/0\"]  \n      ipv6_cidr_blocks = []\n      prefix_list_ids = []\n      security_groups = []\n      self = false      \n  },\n\n    {\n      description      = \"SSH\"\n      from_port        = 22\n      to_port          = 22\n      protocol         = \"tcp\"\n      cidr_blocks      = [\"0.0.0.0/0\"]  \n      ipv6_cidr_blocks = []\n      prefix_list_ids = []\n      security_groups = []\n      self = false      \n    }\n  ]\n\n  egress = [\n    {\n      description      = \"for all outgoing traffics\"\n      from_port        = 0\n      to_port          = 0\n      protocol         = \"-1\"\n      cidr_blocks      = [\"0.0.0.0/0\"]\n      ipv6_cidr_blocks = [\"::/0\"]\n      prefix_list_ids = []\n      security_groups = []\n      self = false\n    }\n  ]\n\n  tags = {\n    Name = \"sg-for-subnet\"\n  }\n}\n`\n```",
      "question_score": 15,
      "answer_score": 32,
      "created_at": "2021-09-06T22:23:10",
      "url": "https://stackoverflow.com/questions/69079945/terraform-inappropriate-value-for-attribute-ingress-while-creating-sg"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 70674928,
      "title": "Terraform/GCP Error: project: required field is not set",
      "problem": "Problem\nThe google_project document says the project_id is optional.\n\nproject_id - (Optional) The project ID. If it is not provided, the provider project is used.\n\nHowever, Terraform complains it is required.\ngcp.tf\n```\n`data \"google_project\" \"project\" {\n}\n\noutput \"project_number\" {\n  value = data.google_project.project.number\n}\n`\n```\n```\n` Error: project: required field is not set\n\u2502 \n\u2502   with data.google_project.project,\n\u2502   on gcp.tf line 1, in data \"google_project\" \"project\":\n\u2502    1: data \"google_project\" \"project\" {\n`\n```\nQuestion\nPlease help understand if this is a documentation defect and the argument is mandatory actually.\nWorkaround\nSet the GOOGLE_PROJECT environment variable.\n```\n`export GOOGLE_PROJECT=...\nterraform apply\n`\n```",
      "solution": "Your 'Workaround' is functionally equivalent to what the documentation suggests. Namely that the `provider` `project` should be set, i.e.:\n```\n`provider \"google\" {\n  project = \"...\"\n}\n`\n```\nYou don't include your `provider` config but, I assume, it doesn't include the default `project` to be used.\nSo, either|or but, somewhere you need to define the default project.\nOtherwise, you should expect to get the error.",
      "question_score": 15,
      "answer_score": 24,
      "created_at": "2022-01-12T01:15:23",
      "url": "https://stackoverflow.com/questions/70674928/terraform-gcp-error-project-required-field-is-not-set"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 70497809,
      "title": "Terraform fails to create ingress (could not find the requested resource ingresses.extensions)",
      "problem": "I'm using minikube locally.\nThe following is the `.tf` file I use to create my kubernetes cluster:\n```\n`provider \"kubernetes\" {\n  config_path = \"~/.kube/config\"\n}\n\nresource \"kubernetes_namespace\" \"tfs\" {\n  metadata {\n    name = \"tfs\" # terraform-sandbox\n  }\n}\n\nresource \"kubernetes_deployment\" \"golang_webapp\" {\n  metadata {\n    name      = \"golang-webapp\"\n    namespace = \"tfs\"\n    labels = {\n      app = \"webapp\"\n    }\n  }\n  spec {\n    replicas = 3\n    selector {\n      match_labels = {\n        app = \"webapp\"\n      }\n    }\n    template {\n      metadata {\n        labels = {\n          app = \"webapp\"\n        }\n      }\n      spec {\n        container {\n          image             = \"golang-docker-example\"\n          name              = \"golang-webapp\"\n          image_pull_policy = \"Never\" # this is set so that kuberenetes wont try to download the image but use the localy built one\n          liveness_probe {\n            http_get {\n              path = \"/\"\n              port = 8080\n            }\n            initial_delay_seconds = 15\n            period_seconds        = 15\n          }\n\n          readiness_probe {\n            http_get {\n              path = \"/\"\n              port = 8080\n            }\n            initial_delay_seconds = 3\n            period_seconds        = 3\n          }\n        }\n      }\n    }\n  }\n}\n\nresource \"kubernetes_service\" \"golang_webapp\" {\n  metadata {\n    name      = \"golang-webapp\"\n    namespace = \"tfs\"\n    labels = {\n      app = \"webapp_ingress\"\n    }\n  }\n  spec {\n    selector = {\n      app = kubernetes_deployment.golang_webapp.metadata.0.labels.app\n    }\n    port {\n      port        = 8080\n      target_port = 8080\n      protocol    = \"TCP\"\n    }\n    # type = \"ClusterIP\"\n    type = \"NodePort\"\n  }\n}\n\nresource \"kubernetes_ingress\" \"main_ingress\" {\n  metadata {\n    name      = \"main-ingress\"\n    namespace = \"tfs\"\n  }\n\n  spec {\n    rule {\n      http {\n        path {\n          backend {\n            service_name = \"golang-webapp\"\n            service_port = 8080\n          }\n          path = \"/golang-webapp\"\n        }\n      }\n    }\n  }\n}\n\n`\n```\nWhen executing `terraform apply`, I am successfully able to create all of the resources except for the ingress.\nThe error is:\n```\n`Error: Failed to create Ingress 'tfs/main-ingress' because: the server could not find the requested resource (post ingresses.extensions)\n\nwith kubernetes_ingress.main_ingress,\n   on main.tf line 86, in resource \"kubernetes_ingress\" \"main_ingress\":\n   86: resource \"kubernetes_ingress\" \"main_ingress\" {\n`\n```\nWhen I try to create an ingress service with kubectl using the same configuration as the one above (only in `.yaml` and using the `kubectl apply` command) it works, so it seems that kubectl & minikube are able to create this type of ingress, but terraform cant for some reason...\nThanks in advance for any help!\nEdit 1:\nadding the `.yaml` that I'm able to create the ingress with\n```\n`apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: example-ingress\n  namespace: tfs\n  annotations:\n    nginx.ingress.kubernetes.io/rewrite-target: /$1\nspec:\n  rules:\n    - http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: golang-webapp\n                port:\n                  number: 8080\n`\n```",
      "solution": "I think the issue can be related to the ingress classname. May be you need to explicitely provide it in your .tf:\n```\n`metadata {\n    name = \"example\"\n    annotations = {\n      \"kubernetes.io/ingress.class\" = \"nginx or your classname\"\n    }\n`\n```\nOr may be it's ingresses.extensions that does not exist in your cluster. Can you provide the .yaml that executed correctly ?",
      "question_score": 15,
      "answer_score": 2,
      "created_at": "2021-12-27T17:09:03",
      "url": "https://stackoverflow.com/questions/70497809/terraform-fails-to-create-ingress-could-not-find-the-requested-resource-ingress"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 66180680,
      "title": "Merging maps with variables",
      "problem": "Running Terraform v0.11.3 and I am trying to merge two maps into a single map using the `merge()` function. However I can't get the syntax right. Does `merge()` support using dynamic variables?\n```\n`  tags = \"${merge({\n    Name         = \"${var.name}\"\n    Env          = \"${var.environment}\"\n    AutoSnapshot = \"${var.auto_snapshot}\"\n  }, \"${var.tags}\")}\"\n`\n```",
      "solution": "The syntax for `merge` in TF 0.11 is shown here:\n```\n`${merge(map(\"a\", \"b\"), map(\"c\", \"d\"))} \n`\n```\nSo in your case, you should have something as follows:\n```\n`tags = \"${merge(map(\"Name\", var.name,\n                    \"Env\", var.environment,\n                    \"AutoSnapshot\", var.auto_snapshot\n               ), var.tags)}\"\n`\n```",
      "question_score": 14,
      "answer_score": 9,
      "created_at": "2021-02-13T01:15:40",
      "url": "https://stackoverflow.com/questions/66180680/merging-maps-with-variables"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 68911814,
      "title": "combine &quot;count&quot; and &quot;for_each&quot; is not possible",
      "problem": "This is my code source\n```\n`resource \"aws_s3_bucket_object\" \"object\" {\n  count               = var.s3_create[1] ? 1 : 0 \n  depends_on          = [aws_s3_bucket.bucket_backup]\n  for_each            = local.buckets_and_folders \n    bucket            = each.value.bucket_backup\n    key               = format(\"%s/\", each.value.folder)\n  force_destroy       = true\n} \n`\n```\nIn other words, I'm traying to create object `aws_s3_bucket_object` depends on variable `s3_create` ... create if true else not create.\nIssue: I am not able to use the combination of both the below syntax in creating the terraform resource and I'm geeting :\n```\n`Error: Invalid combination of \"count\" and \"for_each\"\n\u2502\n\u2502   on ..\\s3\\resources.tf line 51, in resource \"aws_s3_bucket_object\" \"object\":\n\u2502   51:   for_each            = local.buckets_and_folders\n\u2502\n\u2502 The \"count\" and \"for_each\" meta-arguments are mutually-exclusive, only one should be used to be explicit about the number of resources to be created.\n`\n```",
      "solution": "Both count and for_each apply to the whole block. Indenting lines underneath a for_each doesn't impact anything but human readability.\nTry using the ternary operator with a for_each instead of a count. If the value is false, return an empty set.\n```\n`resource \"aws_s3_bucket_object\" \"object\" {\n  for_each       = var.s3_create[1] ? tomap({local.buckets_and_folders}) : {}\n  bucket         = each.value.bucket_backup\n  key            = format(\"%s/\", each.value.folder)\n  depends_on     = [aws_s3_bucket.bucket_backup]\n  force_destroy  = true\n} \n`\n```",
      "question_score": 14,
      "answer_score": 27,
      "created_at": "2021-08-24T19:39:45",
      "url": "https://stackoverflow.com/questions/68911814/combine-count-and-for-each-is-not-possible"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 66388494,
      "title": "Is it possible to trigger an AWS Fargate task upon an item being added to an SQS queue?",
      "problem": "For clarification, what I'm trying to do is fire off a Fargate task when theres an item in a specific queue. I've used this tutorial to get pretty much where I am. This worked fine but the problem I ran into was every file upload (the structure of the s3 bucket is s3_bucket_name/{unknown_name}/known_file_names) was resulting in a task being triggered and I only want/need it to trigger once per {unknown_name} . I've since changed my configuration to add an item to a queue when it detects a test_file.txt file. Is it possible to trigger a fargate task on a queue like this? If so, how?",
      "solution": "SQS doesn't trigger or \"push\" messages to anything. As mentioned in the comments, AWS Lambda has an SQS integration that can automatically poll SQS for you and trigger a Lambda function with new messages, which you could use to create your Fargate tasks.\nHowever I would recommend refactoring your Fargate task like this:\n\nReconfigure the code running in your container to poll the SQS queue for messages.\nRun your task as an ECS service.\nConfigure ECS service autoscaling to spin up instances of your task based on the depth of the SQS queue.",
      "question_score": 14,
      "answer_score": 18,
      "created_at": "2021-02-26T16:26:32",
      "url": "https://stackoverflow.com/questions/66388494/is-it-possible-to-trigger-an-aws-fargate-task-upon-an-item-being-added-to-an-sqs"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 67630533,
      "title": "how to create a set in terraform",
      "problem": "I am trying to create a set to use as an argument in the setproduct function in terraform. When I try:\n```\n`toset([a,b,c])\n`\n```\nI get an error saying I can't convert a tuple to a list. I've tried various things like using `tolist` and `...` and just having one pair of `()` braces in various places but I still can't get this to work - would anyone know how I can create a set from a,b and c?",
      "solution": "`set` must have elements of the same type. Thus it can be:\n```\n`# set of strings\ntoset([\"a\", \"b\", \"c\"])\n\n# set of numbers\ntoset([1, 2, 3])\n\n# set of lists\ntoset([[\"b\"], [\"c\", 4], [3,3]])\n`\n```\nYou can't mix types, so the error you are getting is because your are mixing types, e.g. list and number\n```\n`# will not work because different types\ntoset([[\"b\"], [\"c\", 4], 3])\n`\n```",
      "question_score": 14,
      "answer_score": 21,
      "created_at": "2021-05-21T05:40:39",
      "url": "https://stackoverflow.com/questions/67630533/how-to-create-a-set-in-terraform"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 66935287,
      "title": "How to remove Terraform double quotes?",
      "problem": "I created a YML pipeline using terraform .\nIt uses a script task and returns in output the web app name\n```\n`steps:\n- script: | \n    [......] \n    terraform apply -input=false -auto-approve\n    \n    # Get the App Service name for the dev environment.\n    WebAppNameDev=$(terraform output appservice_name_dev)\n    \n    # Write the WebAppNameDev variable to the pipeline.\n    echo \"##vso[task.setvariable variable=WebAppNameDev;isOutput=true]$WebAppNameDev\"\n  name: 'RunTerraform'\n`\n```\nThe task works fine but when i deploy the webapp it crashes because seems variable `$WebAppNameDev` has double quotes.\n```\n`      - task: AzureWebApp@1\n        displayName: 'Azure App Service Deploy: website'\n        inputs:\n          azureSubscription: 'MySubscription'\n          appName: $(WebAppNameDev)\n          package: '$(Pipeline.Workspace)/drop/*.zip'\n`\n```\nThe error looks like:\n```\n` Got service connection details for Azure App Service:'\"spikeapp-dev-6128\"'\n ##[error]Error: Resource '\"spikeapp-dev-6128\"' doesn't exist. Resource should exist before deployment.\n`\n```\nHow can i remove double quotes or fix the terraform output?",
      "solution": "I solved by adding `-raw` parameter to terraform output.\n```\n`WebAppNameDev=$(terraform output -raw appservice_name_dev)\n`\n```\nref. https://www.terraform.io/docs/cli/commands/output.html",
      "question_score": 13,
      "answer_score": 37,
      "created_at": "2021-04-03T21:47:25",
      "url": "https://stackoverflow.com/questions/66935287/how-to-remove-terraform-double-quotes"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 71940888,
      "title": "Local state cannot be unlocked by another process on terraform",
      "problem": "My terraform remote states and lockers are configured on s3 and dynamodb under aws account, On gitlab runner some plan task has been crashed and on the next execution plan the following error pops up:\n```\n`Error: Error locking state: Error acquiring the state lock: ConditionalCheckFailedException:\nThe conditional request failed\n\nLock Info:\n  ID:        \n  Path:      remote-terrform-states/app/terraform.tfstate\n  Operation: OperationTypePlan\n  Who:       root@runner-abc-project-123-concurrent-0\n  Version:   0.14.10\n  Created:   2022-01-01 00:00:00 +0000 UTC\n  Info:  some really nice info\n`\n```\nWhile trying to unlock this locker in order to perform additional execution plan again - I get the following error:\n```\n`  terraform force-unlock \n\n  #output:\n  Local state cannot be unlocked by another process\n`\n```\nHow do we release this terraform locker?",
      "solution": "According to reference of terraform command: `force-unlock`\n\nManually unlock the state for the defined configuration.\nThis will not modify your infrastructure. This command removes the\nlock on the state for the current configuration. The behavior of this\nlock is dependent on the backend being used. Local state files cannot\nbe unlocked by another process.\n\nExplanation: apparently the execution plan is processing the plan output file locally and being apply on the second phase of terraform steps, like the following example:\nphase 1: `terraform plan -out execution-plan.out`\nphase 2: `terraform apply -input=false execution-plan.out`\nMake sure that filename is same in phase 1 and 2\nHowever - if phase 1 is being terminated or accidentally crashing, the locker will be assigned to the local state file and therefore must be removed on the dynamodb itself and not with the terraform force-unlock command.\nSolution: Locate this specific item under the dynamodb terraform lockers table and explicitly remove the locked item, you can do either with aws console or through the api.\nFor example:\n```\n`aws dynamodb delete-item \\\n    --table-name terraform-locker-bucket \\\n    --key file://key.json\n`\n```\nContents of key.json:\n```\n`{\n \"LockID\": \"remote-terrform-states/app/terraform.tfstate\",\n \"Info\": {\n   \"ID\":\"\",\n   \"Operation\":\"OperationTypePlan\",\n   \"Who\":\"root@runner-abc-project-123-concurrent-0\",\n   \"Version\":\"0.14.10\",\n   \"Created\":\"2022-01-01 00:00:00 +0000 UTC\",\n   \"Info\":\"some really nice info\"\n   }\n }\n`\n```",
      "question_score": 13,
      "answer_score": 19,
      "created_at": "2022-04-20T15:47:53",
      "url": "https://stackoverflow.com/questions/71940888/local-state-cannot-be-unlocked-by-another-process-on-terraform"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 65615449,
      "title": "Writing CloudWatch log resource policy failed: LimitExceededException: Resource limit exceeded",
      "problem": "I'm trying to create elasticsearch cluster using terraform, But i'm getting this error\n```\n`11:58:07 * aws_cloudwatch_log_resource_policy.elasticsearch-log-publishing-policy: Writing CloudWatch log resource policy failed: LimitExceededException: Resource limit exceeded.\n11:58:07 * aws_elasticsearch_domain.es2: 1 error(s) occurred:\n`\n```\nI initially thought that this resource limit error is unable  to create log groups. But when i raised a Ticket with AWS team , they said there is \"no throttling on CreateLogGroup API for this account in IAD\"\nElasticSearch has about 10 clusters running. I'm not sure which resource limit has exceeded.\nCan someone pls explain me the above error.\nUpdate:\n```\n`data \"aws_iam_policy_document\" \"elasticsearch-log-publishing-policy\" {\n  statement {\n    actions = [\n      \"logs:CreateLogStream\",\n      \"logs:PutLogEvents\",\n      \"logs:PutLogEventsBatch\",\n    ]\n\n    resources = [\"arn:aws:logs:*\"]\n\n    principals {\n      identifiers = [\"es.amazonaws.com\"]\n      type        = \"Service\"\n    }\n  }\n}\n\nresource \"aws_cloudwatch_log_resource_policy\" \"elasticsearch-log-publishing-policy\" {\n  policy_document = \"${data.aws_iam_policy_document.elasticsearch-log-publishing-policy.json}\"\n  policy_name     = \"elasticsearch-log-publishing-policy\"\n}\n`\n```\nI tried to apply this using terraform target, i think the error is here, does AWS have a limit on number of custom policies we create, I could not find an option to request an increase.",
      "solution": "does AWS have a limit on number of custom policies we create, I could not find an option to request an increase.\n\nYes, the limit can't be change and it is:\n\nUp to 10 CloudWatch Logs resource policies per Region per account. This quota can't be changed.",
      "question_score": 13,
      "answer_score": 12,
      "created_at": "2021-01-07T16:42:46",
      "url": "https://stackoverflow.com/questions/65615449/writing-cloudwatch-log-resource-policy-failed-limitexceededexception-resource"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 69126254,
      "title": "terraform resource creation - this keyword",
      "problem": "I came across a pattern in couple of `terraform` code in Github.\n```\n`resource \"aws_vpc\" \"this\"\n`\n```\nI wanted to know how keyword `this` provides a particular advantage over a named resource. I can't find a Hashicorp documentation on `this` keyword.\nhttps://github.com/terraform-aws-modules/terraform-aws-vpc/blob/3210728ee26665fab6b1f07417bcb0e518573a1d/main.tf\nhttps://github.com/cloudposse/terraform-aws-vpn-connection/blob/master/context.tf",
      "solution": "No, there is nothing special about `this` in terms of TF syntax or handling. Its just a name that may indicate that you have only one VPC in your setup. But this is not enforced by TF mechanism. Other common names are `main` or just `vpc`.",
      "question_score": 13,
      "answer_score": 18,
      "created_at": "2021-09-10T03:32:00",
      "url": "https://stackoverflow.com/questions/69126254/terraform-resource-creation-this-keyword"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 66453747,
      "title": "How to check the value is present in list or not using terraform 0.13.5?",
      "problem": "I need to check the value that exists in a variable or not and based on that I need to create resources.\nIf `value_list` doesn't have these `values('abc','def','ghi')` it should not create the resource.\nWhat I'm trying here is:\n\nConverting the string variable  to list\nCheck that list is having values 'abc' or 'def' or 'ghi'. If `value_list` contains any one of the values then proceed with the next steps to create resources.\nIf `value_list` doesn't have these `values('abc','def','ghi')` it should not create the resource.\n\nvariables.tf\n```\n`variable \"value_list\" {\n    default = \"abc,def,ghi\"\n    type= string\n}\n`\n```\nresource.tf\n```\n`resource \"azurerm_kubernetes_cluster_node_pool\" \"user\" {\n  value_list = ${split(\",\", var.value_list)}\n  count = \"${contains(value_list,\"abc\") ? 1 : 0 || contains(value_list,\"def\") ? 1 : 0 || contains(value_list,\"ghi\") ? 1 : 0 \n}\n`\n```\nError:\nThis character is not used within the language.\nExpected the start of an expression, but found an invalid expression token.\nHow to check if the value_list is having the desired value or not?",
      "solution": "Terraform has functions that can help with that:\n\nhttps://www.terraform.io/docs/language/functions/contains.html\nhttps://www.terraform.io/docs/language/functions/setintersection.html\nhttps://www.terraform.io/docs/language/functions/setsubtract.html\nhttps://www.terraform.io/docs/language/functions/setunion.html\n\nIt looks like you are using `contains`, but in a strange way, if you need to split something you can do it in a local that way it is available to multiple resources, also the expression in your count does not look right you might want to look at the documentation for that:\nhttps://www.terraform.io/docs/language/meta-arguments/count.html#using-expressions-in-count\nHere is a sample usage:\n```\n`variable \"value_list\" {\n  default = \"abc,def,ghi\"\n  type    = string\n}\n\nlocals {\n  vlist = split(\",\", var.value_list)\n}\n\nresource \"null_resource\" \"test_abc\" {\n  count = contains(local.vlist, \"abc\") ? 1 : 0\n\n  provisioner \"local-exec\" {\n    command = \"echo FOUND;\"\n  }\n}\n\nresource \"null_resource\" \"test_xyz\" {\n  count = contains(local.vlist, \"xyz\") ? 1 : 0\n\n  provisioner \"local-exec\" {\n    command = \"echo FOUND;\"\n  }\n}\n\nresource \"null_resource\" \"test_abc_or_def\" {\n  count = (contains(local.vlist, \"abc\") || contains(local.vlist, \"def\")) ? 1 : 0\n\n  provisioner \"local-exec\" {\n    command = \"echo FOUND;\"\n  }\n}\n`\n```\nSee the count in that last resource:\n`count = (contains(local.vlist, \"abc\") || contains(local.vlist, \"def\")) ? 1 : 0`\nthat is a conditional expression in the format:\n` ?  : `\nthe condition is what looks strange in your sample code, you can have as many or in your condition as you want but don't mix the values there\n(  `vlist contains \"abc\"` OR `vlist contains \"def\"` )\n( `contains(local.vlist, \"abc\")` || `contains(local.vlist, \"def\")` )",
      "question_score": 13,
      "answer_score": 13,
      "created_at": "2021-03-03T09:58:36",
      "url": "https://stackoverflow.com/questions/66453747/how-to-check-the-value-is-present-in-list-or-not-using-terraform-0-13-5"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 66912991,
      "title": "Terraform custom validation for variable that can be null",
      "problem": "I'm trying to use Terraform 0.13's custom variable validation to ensure that the value is either `null` or between a specified range. I've tried every combination I can think of (using `can`, `try`, etc) but can't get it working for every case.\nHere is the full non-working example:\n```\n`variable \"target_group_stickiness_duration\" {\n  default = null\n  type    = number\n \n  validation {\n    condition = ( \n      var.target_group_stickiness_duration == null ||\n      ( var.target_group_stickiness_duration >= 1 && var.target_group_stickiness_duration = 1 and This will fail if the value is `null` with the following error:\n\nError during operation: argument must not be null.\n\nI believe this is due to eager evaluation of the conditional expression. Am I missing something obvious? Or any clever workarounds for this?",
      "solution": "It fails becasue when your `target_group_stickiness_duration` is `null`, the\n```\n`(var.target_group_stickiness_duration >= 1 && var.target_group_stickiness_duration is invalid.\nYou can try the following which uses coalesce:\n```\n`variable \"target_group_stickiness_duration\" {\n  default = null\n  type    = number\n \n  validation {\n    condition = ( \n      var.target_group_stickiness_duration == null ||      \n     (coalesce(var.target_group_stickiness_duration, 0) >= 1 && coalesce(var.target_group_stickiness_duration, 604801) = 1 and <= 604800.\"\n  }\n  \n}\n`\n```",
      "question_score": 12,
      "answer_score": 13,
      "created_at": "2021-04-02T02:11:56",
      "url": "https://stackoverflow.com/questions/66912991/terraform-custom-validation-for-variable-that-can-be-null"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 68169018,
      "title": "How do I specify unmarshall types with JSON encode in Terraform?",
      "problem": "I am working on building a fargate cluster and am having difficulties following the documentation for the aws_ecs_task_definition section (https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/ecs_task_definition)\n```\n`\u2502 Error: ECS Task Definition container_definitions is invalid: Error decoding JSON: json: cannot unmarshal string into Go struct field ContainerDefinition.Cpu of type int64\n\u2502 \n\u2502   with aws_ecs_task_definition.app,\n\u2502   on ecs.tf line 40, in resource \"aws_ecs_task_definition\" \"app\":\n\u2502   40:   container_definitions = jsonencode([\n\u2502   41:     {\n\u2502   42:       \"name\": \"${var.prefix}\",\n\u2502   43:       \"image\": \"${var.app_image}\",\n\u2502   44:       \"cpu\": \"256\",\n\u2502   45:       \"memory\": \"${var.fargate_memory}\",\n\u2502   46:       \"networkMode\": \"awsvpc\",\n\u2502   47:       \"logConfiguration\": {\n\u2502   48:           \"logDriver\": \"awslogs\",\n\u2502   49:           \"options\": {\n\u2502   50:             \"awslogs-group\": \"${aws_cloudwatch_log_group.ecs.name}\",\n\u2502   51:             \"awslogs-region\": \"${var.region}\",\n\u2502   52:             \"awslogs-stream-prefix\": \"ecs\"\n\u2502   53:           }\n\u2502   54:       },\n\u2502   55:       \"environment\": [\n\u2502   56:         {\"name\": \"ENV_RUNNER_SLEEP_SECS\", \"value\": \"${var.app_env_runner_sleep_secs}\"}\n\u2502   57:       ]\n\u2502   58:     }\n\u2502   59:   ])\n`\n```\nThe error is pointing to the value for CPU. This would normally be another variable but I'm just testing out inputs to try and get it to pass. Annoyingly enough, if I set the value to a number, I get a different error: `cannot unmarshal number into Go struct field KeyValuePair.Environment.Value of type string`.\nAny ideas?",
      "solution": "Your analysis of the first error is correct: it is due to `cpu` having to be an integer / number / int64. That means you need to specify it as `\"cpu\": 256`.\nThe second error then tells you not to look at `ContainerDefinition.Cpu` but `KeyValuePair.Environment.Value` which is the `\"environment\": [ ... ]` section. The problem here is that keys and values have to be `string`s and even though you write `\"${var.app_env_runner_sleep_secs}\"` terraform still outputs a number despite the `\"` surrounding it. To fix that you need to put a `tostring` around the argument: `\"value\": \"${tostring(var.app_env_runner_sleep_secs)}\"`.\nNote that additionally depending on your terraform version it is a lot shorter and cleaner to write e.g. `\"awslogs-region\": var.region` by removing the `\"${...}\"` in all places.",
      "question_score": 12,
      "answer_score": 29,
      "created_at": "2021-06-28T21:25:11",
      "url": "https://stackoverflow.com/questions/68169018/how-do-i-specify-unmarshall-types-with-json-encode-in-terraform"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 69925970,
      "title": "How to save Terraform output variable into a Github Action\u2019s environment variable",
      "problem": "My project uses Terraform for setting up the infrastructure and Github Actions for CI/CD. After running `terraform apply` I would like to save the value of a Terraform output variable as Github Action environment variable to be later used by the workflow.\nAccording to Github Action's docs, this is the way to create or update environment variables using workflow commands.\nHere is my simplified Github Action workflow:\n`name: Setup infrastructure\njobs:\n  run-terraform:\n    name: Apply infrastructure changes\n    runs-on: ubuntu-latest\n    steps:\n      ...\n      - run: terraform output vm_ip\n      - run: echo TEST=$(terraform output vm_ip) >> $GITHUB_ENV\n      - run: echo ${{ env.TEST }}\n`\nWhen running locally the command `echo TEST_VAR=$(terraform output vm_ip)` outputs exactly `TEST=\"192.168.23.23\"` but from the Github Action CLI output I get something very strange:\n\nI've tried with single quotes, double quotes. At some point I changed the strategy and tried to use `jq`. So I've added the following steps in order to export all Terraform Outputs to a json file and parse it using `jq`:\n```\n`- run: terraform output -json >> /tmp/tf.out.json\n- run: jq '.vm_ip.value' /tmp/tf.out.json\n`\n```\nBut now it throws the following error:\n```\n`parse error: Invalid numeric literal at line 1, column 9\n`\n```\nEven though the JSON generated is perfectly valid:\n`{\n  \"cc_host\": {\n    \"sensitive\": false,\n    \"type\": \"string\",\n    \"value\": \"private.c.db.ondigitalocean.com\"\n  },\n  \"cc_port\": {\n    \"sensitive\": false,\n    \"type\": \"number\",\n    \"value\": 1234\n  },\n  \"db_host\": {\n    \"sensitive\": false,\n    \"type\": \"string\",\n    \"value\": \"private.b.db.ondigitalocean.com\"\n  },\n  \"db_name\": {\n    \"sensitive\": false,\n    \"type\": \"string\",\n    \"value\": \"XXX\"\n  },\n  \"db_pass\": {\n    \"sensitive\": true,\n    \"type\": \"string\",\n    \"value\": \"XXX\"\n  },\n  \"db_port\": {\n    \"sensitive\": false,\n    \"type\": \"number\",\n    \"value\": 1234\n  },\n  \"db_user\": {\n    \"sensitive\": false,\n    \"type\": \"string\",\n    \"value\": \"XXX\"\n  },\n  \"vm_ip\": {\n    \"sensitive\": false,\n    \"type\": \"string\",\n    \"value\": \"206.189.15.70\"\n  }\n}\n`\nThe commands `terraform output -json >> /tmp/tf.out.json` and `jq '.vm_ip.value' /tmp/tf.out.json` work accordingly on local.",
      "solution": "After hours searching I've finally figured it out.\nIt seems that the Terraform's Github Action offers an additional parameter called `terraform_wrapper` which needs to be set to `false` if you plan using the output in commands. You can read a more in depth article here.\nOtherwise, they will be automatically exposed to the step's output and they can be accessed like `steps..outputs.`. You can read more about them here and here.",
      "question_score": 12,
      "answer_score": 18,
      "created_at": "2021-11-11T10:29:21",
      "url": "https://stackoverflow.com/questions/69925970/how-to-save-terraform-output-variable-into-a-github-action-s-environment-variabl"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 69794727,
      "title": "How can I invalidate AWS CloudFront Distribution cache using Terraform?",
      "problem": "I am looking for a way to invalidate the CloudFront distribution cache using Terraform.\nI could not find any information in the docs.\nIs this possible and if so, how?",
      "solution": "There is no in-built support within the `aws_cloudfront_distribution` or `aws_cloudfront_cache_policy` resource for cache invalidation.\nAs a last resort, the `local_exec` provisioner can be used.\n\nTypically, from my experience, the cache is invalidated within the CI/CD pipeline using the AWS CLI `create-invalidation` command.\nHowever, if this must be done within Terraform,  you can use the `local-exec` provisioner to run commands on the local machine running Terraform after the resource has been created/updated.\nWe can use this to run the above CLI invalidation command to invalidate the distribution cache.\nUse the `self` object to access all of the CloudFront distribution's attributes, including `self.id` to reference the CloudFront distribution ID for the invalidation\n\nExample:\n```\n`resource \"aws_cloudfront_distribution\" \"s3_distribution\" {\n  # ...\n\n  provisioner \"local-exec\" {\n    command = \"aws cloudfront create-invalidation --distribution-id ${self.id} --paths '...'\"\n  }\n}\n`\n```",
      "question_score": 12,
      "answer_score": 16,
      "created_at": "2021-11-01T10:06:03",
      "url": "https://stackoverflow.com/questions/69794727/how-can-i-invalidate-aws-cloudfront-distribution-cache-using-terraform"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 71320503,
      "title": "Is it possible to update the source code of a GCP Cloud Function in Terraform?",
      "problem": "I use Terraform to manage resources of Google Cloud Functions. But while the inital deployment of the cloud function worked, further deploments with changed cloud function source code (the source archive `sourcecode.zip`) were not redeployed when I use `terraform apply` after updating the source archive.\nThe storage bucket object gets updated but this does not trigger an update/redeployment of the cloud function resource.\nIs this an error of the provider?\nIs there a way to redeploy a function in terraform when the code changes?\nThe simplified source code I am using:\n```\n`resource \"google_storage_bucket\" \"cloud_function_source_bucket\" {\n  name                        = \"${local.project}-function-bucket\"\n  location                    = local.region\n  uniform_bucket_level_access = true\n}\n\nresource \"google_storage_bucket_object\" \"function_source_archive\" {\n  name   = \"sourcecode.zip\"\n  bucket = google_storage_bucket.cloud_function_source_bucket.name\n  source = \"./../../../sourcecode.zip\"\n}\n\nresource \"google_cloudfunctions_function\" \"test_function\" {\n  name                          = \"test_func\"\n  runtime                       = \"python39\"\n  region                        = local.region\n  project                       = local.project\n  available_memory_mb           = 256\n  source_archive_bucket         = google_storage_bucket.cloud_function_source_bucket.name\n  source_archive_object         = google_storage_bucket_object.function_source_archive.name\n  trigger_http                  = true\n  entry_point                   = \"trigger_endpoint\"\n  service_account_email         = google_service_account.function_service_account.email\n  vpc_connector                 = \"projects/${local.project}/locations/${local.region}/connectors/serverless-main\"\n  vpc_connector_egress_settings = \"ALL_TRAFFIC\"\n  ingress_settings              = \"ALLOW_ALL\"\n}\n`\n```",
      "solution": "You can append MD5 or SHA256 checksum of the content of zip to the bucket object's name. That will trigger recreation of cloud function whenever source code changes.\n\n${data.archive_file.function_src.output_md5}\n\n```\n`data \"archive_file\" \"function_src\" {\ntype = \"zip\"\nsource_dir = \"SOURCECODE_PATH/sourcecode\"\noutput_path = \"./SAVING/PATH/sourcecode.zip\"\n}\n\nresource \"google_storage_bucket_object\" \"function_source_archive\" {\nname   = \"sourcecode.${data.archive_file.function_src.output_md5}.zip\"\nbucket = google_storage_bucket.cloud_function_source_bucket.name\nsource = data.archive_file.function_src.output_path\n}\n`\n```\nYou can read more about terraform archive here - terraform archive_file",
      "question_score": 12,
      "answer_score": 13,
      "created_at": "2022-03-02T10:44:23",
      "url": "https://stackoverflow.com/questions/71320503/is-it-possible-to-update-the-source-code-of-a-gcp-cloud-function-in-terraform"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 69193030,
      "title": "Terraform - How to attach SSL certificate stored in Azure KeyVault to an Application Gateway",
      "problem": "I have a Terraform script that create an Azure Key Vault, imports my SSL certificate (3DES .pfx file with a password), and creates an Application Gateway with a HTTP listener. I'm trying to change this to a HTTPS listener that uses my SSL certificate from KeyVault.\nI've stepped through this process manually in Azure Portal and I have this working with PowerShell. Unfortunately I don't find Terraform's documentation clear on how this is supposed to be achieved.\nHere are relevant snippets of my Application Gateway and certificate resources:\n```\n`resource \"azurerm_application_gateway\" \"appgw\" {\n  name                = \"my-appgw\"\n  location            = \"australiaeast\"\n  resource_group_name = \"my-rg\"\n  \n  http_listener {\n    protocol                       = \"https\"\n    ssl_certificate_name           = \"appgw-listener-cert\"\n    ...\n  }\n\n  identity {\n    type         = \"UserAssigned\"\n    identity_ids = [azurerm_user_assigned_identity.appgw_uaid.id]\n  }\n\n  ssl_certificate {\n    key_vault_secret_id = azurerm_key_vault_certificate.ssl_cert.secret_id\n    name                = \"appgw-listener-cert\"\n  }\n\n  ...\n}\n\nresource \"azurerm_key_vault\" \"kv\" {\n  name                       = \"my-kv\"\n  location                   = \"australiaeast\"\n  resource_group_name        = \"my-rg\"\n  ...\n  access_policy {\n    object_id    = data.azurerm_client_config.current.object_id\n    tenant_id    = data.azurerm_client_config.current.tenant_id\n\n    certificate_permissions = [\n      \"Create\",\n      \"Delete\",\n      \"DeleteIssuers\",\n      \"Get\",\n      \"GetIssuers\",\n      \"Import\",\n      \"List\",\n      \"ListIssuers\",\n      \"ManageContacts\",\n      \"ManageIssuers\",\n      \"Purge\",\n      \"SetIssuers\",\n      \"Update\"\n    ]\n\n    key_permissions = [\n      \"Backup\",\n      \"Create\",\n      \"Decrypt\",\n      \"Delete\",\n      \"Encrypt\",\n      \"Get\",\n      \"Import\",\n      \"List\",\n      \"Purge\",\n      \"Recover\",\n      \"Restore\",\n      \"Sign\",\n      \"UnwrapKey\",\n      \"Update\",\n      \"Verify\",\n      \"WrapKey\"\n    ]\n\n    secret_permissions = [\n      \"Backup\",\n      \"Delete\",\n      \"Get\",\n      \"List\",\n      \"Purge\",\n      \"Restore\",\n      \"Restore\",\n      \"Set\"\n    ]\n  }\n\n  access_policy {\n    object_id    = azurerm_user_assigned_identity.uaid_appgw.principal_id\n    tenant_id    = data.azurerm_client_config.current.tenant_id\n\n    secret_permissions = [\n      \"Get\"\n    ]\n  }\n}\n\nresource \"azurerm_key_vault_certificate\" \"ssl_cert\" {\n  name         = \"my-ssl-cert\"\n  key_vault_id = azurerm_key_vault.kv.id\n\n  certificate {\n    # These are stored as sensitive variables in Terraform Cloud\n    # ssl_cert_b64 value was retrieved by: $ cat my-ssl-cert.pfx | base64 > o.txt\n    contents = var.ssl_cert_b64\n    password = var.ssl_cert_passwd\n  }\n\n  certificate_policy {\n    issuer_parameters {\n      name = \"Unknown\"\n    }\n\n    key_properties {\n      exportable = false\n      key_size   = 2048\n      key_type   = \"RSA\"\n      reuse_key  = false\n    }\n\n    secret_properties {\n      content_type = \"application/x-pkcs12\"\n    }\n  }\n}\n`\n```\nHere is the (sanitised) error I get in Terraform Cloud:\n\nError: waiting for create/update of Application Gateway: (Name \"my-appgw\" / Resource Group \"my-rg\"): Code=\"ApplicationGatewayKeyVaultSecretException\" Message=\"Problem occured while accessing and validating KeyVault Secrets associated with Application Gateway '/subscriptions/1324/resourceGroups/my-rg/providers/Microsoft.Network/applicationGateways/my-appgw'. See details below:\" Details=[{\"code\":\"ApplicationGatewaySslCertificateDoesNotHavePrivateKey\",\"message\":\"Certificate /subscriptions/1324/resourceGroups/my-rg/providers/Microsoft.Network/applicationGateways/my-appgw/sslCertificates/appgw-listener-cert does not have Private Key.\"}]\n\nI downloaded the certificate from Key Vault and it appears to be a valid, not corrupted or otherwise broken. I don't understand why the error says it doesn't have a Private Key.\nCan someone point out what I've missed or I'm doing wrong?",
      "solution": "I tested 2 scenarios in my environment :\nScenario 1: Generating a new certificate in Keyvault and uploading it in application gateway ssl certificate.\n```\n`provider \"azurerm\" {\n    features{}\n}\ndata \"azurerm_client_config\" \"current\" {}\n\ndata \"azurerm_resource_group\" \"example\"{\n    name = \"ansumantest\"\n}\n\nresource \"azurerm_user_assigned_identity\" \"base\" {\n  resource_group_name = data.azurerm_resource_group.example.name\n  location            = data.azurerm_resource_group.example.location\n  name                = \"mi-appgw-keyvault\"\n}\n\nresource \"azurerm_key_vault\" \"kv\" {\n  name                       = \"ansumankeyvault01\"\n  location                   = data.azurerm_resource_group.example.location\n  resource_group_name        = data.azurerm_resource_group.example.name\n  tenant_id = data.azurerm_client_config.current.tenant_id\n  sku_name = \"standard\"\n  access_policy {\n    object_id    = data.azurerm_client_config.current.object_id\n    tenant_id    = data.azurerm_client_config.current.tenant_id\n\n    certificate_permissions = [\n      \"Create\",\n      \"Delete\",\n      \"DeleteIssuers\",\n      \"Get\",\n      \"GetIssuers\",\n      \"Import\",\n      \"List\",\n      \"ListIssuers\",\n      \"ManageContacts\",\n      \"ManageIssuers\",\n      \"Purge\",\n      \"SetIssuers\",\n      \"Update\"\n    ]\n\n    key_permissions = [\n      \"Backup\",\n      \"Create\",\n      \"Decrypt\",\n      \"Delete\",\n      \"Encrypt\",\n      \"Get\",\n      \"Import\",\n      \"List\",\n      \"Purge\",\n      \"Recover\",\n      \"Restore\",\n      \"Sign\",\n      \"UnwrapKey\",\n      \"Update\",\n      \"Verify\",\n      \"WrapKey\"\n    ]\n\n    secret_permissions = [\n      \"Backup\",\n      \"Delete\",\n      \"Get\",\n      \"List\",\n      \"Purge\",\n      \"Restore\",\n      \"Restore\",\n      \"Set\"\n    ]\n  }\n\n  access_policy {\n    object_id    = azurerm_user_assigned_identity.base.principal_id\n    tenant_id    = data.azurerm_client_config.current.tenant_id\n\n    secret_permissions = [\n      \"Get\"\n    ]\n  }\n}\n\noutput \"secret_identifier\" {\n  value = azurerm_key_vault_certificate.example.secret_id\n}\n\nresource \"azurerm_key_vault_certificate\" \"example\" {\n  name         = \"generated-cert\"\n  key_vault_id = azurerm_key_vault.kv.id\n\n  certificate_policy {\n    issuer_parameters {\n      name = \"Self\"\n    }\n\n    key_properties {\n      exportable = true\n      key_size   = 2048\n      key_type   = \"RSA\"\n      reuse_key  = true\n    }\n\n    lifetime_action {\n      action {\n        action_type = \"AutoRenew\"\n      }\n\n      trigger {\n        days_before_expiry = 30\n      }\n    }\n\n    secret_properties {\n      content_type = \"application/x-pkcs12\"\n    }\n\n    x509_certificate_properties {\n      # Server Authentication = 1.3.6.1.5.5.7.3.1\n      # Client Authentication = 1.3.6.1.5.5.7.3.2\n      extended_key_usage = [\"1.3.6.1.5.5.7.3.1\"]\n\n      key_usage = [\n        \"cRLSign\",\n        \"dataEncipherment\",\n        \"digitalSignature\",\n        \"keyAgreement\",\n        \"keyCertSign\",\n        \"keyEncipherment\",\n      ]\n\n      subject_alternative_names {\n        dns_names = [\"internal.contoso.com\", \"domain.hello.world\"]\n      }\n\n      subject            = \"CN=hello-world\"\n      validity_in_months = 12\n    }\n  }\n}\n\nresource \"azurerm_virtual_network\" \"example\" {\n  name                = \"example-network\"\n  resource_group_name = data.azurerm_resource_group.example.name\n  location            = data.azurerm_resource_group.example.location\n  address_space       = [\"10.254.0.0/16\"]\n}\n\nresource \"azurerm_subnet\" \"frontend\" {\n  name                 = \"frontend\"\n  resource_group_name  = data.azurerm_resource_group.example.name\n  virtual_network_name = azurerm_virtual_network.example.name\n  address_prefixes     = [\"10.254.0.0/24\"]\n}\n\nresource \"azurerm_subnet\" \"backend\" {\n  name                 = \"backend\"\n  resource_group_name  = data.azurerm_resource_group.example.name\n  virtual_network_name = azurerm_virtual_network.example.name\n  address_prefixes     = [\"10.254.2.0/24\"]\n}\n\nresource \"azurerm_public_ip\" \"example\" {\n  name                = \"example-pip\"\n  resource_group_name = data.azurerm_resource_group.example.name\n  location            = data.azurerm_resource_group.example.location\n  allocation_method   = \"Static\"\n  sku = \"standard\"\n}\n\n#&nbsp;since these variables are re-used - a locals block makes this more maintainable\nlocals {\n  backend_address_pool_name      = \"${azurerm_virtual_network.example.name}-beap\"\n  frontend_port_name             = \"${azurerm_virtual_network.example.name}-feport\"\n  frontend_ip_configuration_name = \"${azurerm_virtual_network.example.name}-feip\"\n  http_setting_name              = \"${azurerm_virtual_network.example.name}-be-htst\"\n  listener_name                  = \"${azurerm_virtual_network.example.name}-httplstn\"\n  request_routing_rule_name      = \"${azurerm_virtual_network.example.name}-rqrt\"\n  redirect_configuration_name    = \"${azurerm_virtual_network.example.name}-rdrcfg\"\n\n}\n\nresource \"null_resource\" \"previous\" {}\n\nresource \"time_sleep\" \"wait_240_seconds\" {\n  depends_on = [azurerm_key_vault.kv]\n\n  create_duration = \"240s\"\n}\n\nresource \"azurerm_application_gateway\" \"network\" {\n  name                = \"example-appgateway\"\n  resource_group_name = data.azurerm_resource_group.example.name\n  location            = data.azurerm_resource_group.example.location\n\n  sku {\n    name     = \"Standard_v2\"\n    tier     = \"Standard_v2\"\n    capacity = 2\n  }\n\n  gateway_ip_configuration {\n    name      = \"my-gateway-ip-configuration\"\n    subnet_id = azurerm_subnet.frontend.id\n  }\n\n  frontend_port {\n    name = local.frontend_port_name\n    port = 443\n  }\n\n  frontend_ip_configuration {\n    name                 = local.frontend_ip_configuration_name\n    public_ip_address_id = azurerm_public_ip.example.id\n  }\n\n  backend_address_pool {\n    name = local.backend_address_pool_name\n  }\n\n  backend_http_settings {\n    name                  = local.http_setting_name\n    cookie_based_affinity = \"Disabled\"\n    path                  = \"/path1/\"\n    port                  = 443\n    protocol              = \"Https\"\n    request_timeout       = 60\n  }\n\n  http_listener {\n    name                           = local.listener_name\n    frontend_ip_configuration_name = local.frontend_ip_configuration_name\n    frontend_port_name             = local.frontend_port_name\n    protocol                       = \"Https\"\n    ssl_certificate_name = \"app_listener\"\n  }\n\n  identity {\n    type = \"UserAssigned\"\n    identity_ids = [azurerm_user_assigned_identity.base.id]\n  }\n\n  ssl_certificate {\n    name = \"app_listener\"\n    key_vault_secret_id = azurerm_key_vault_certificate.example.secret_id\n  }\n\n  request_routing_rule {\n    name                       = local.request_routing_rule_name\n    rule_type                  = \"Basic\"\n    http_listener_name         = local.listener_name\n    backend_address_pool_name  = local.backend_address_pool_name\n    backend_http_settings_name = local.http_setting_name\n  }\n  depends_on = [time_sleep.wait_240_seconds]\n}\n`\n```\nOutput:\n\nScenario 2 : Using one certificate which I import from local machine to keyvault and using it in application gateway.\n```\n`provider \"azurerm\" {\n    features{}\n}\ndata \"azurerm_client_config\" \"current\" {}\n\ndata \"azurerm_resource_group\" \"example\"{\n    name = \"ansumantest\"\n}\n\nresource \"azurerm_user_assigned_identity\" \"base\" {\n  resource_group_name = data.azurerm_resource_group.example.name\n  location            = data.azurerm_resource_group.example.location\n  name                = \"mi-appgw-keyvault\"\n}\n\nresource \"azurerm_key_vault\" \"kv\" {\n  name                       = \"ansumankeyvault01\"\n  location                   = data.azurerm_resource_group.example.location\n  resource_group_name        = data.azurerm_resource_group.example.name\n  tenant_id = data.azurerm_client_config.current.tenant_id\n  sku_name = \"standard\"\n  access_policy {\n    object_id    = data.azurerm_client_config.current.object_id\n    tenant_id    = data.azurerm_client_config.current.tenant_id\n\n    certificate_permissions = [\n      \"Create\",\n      \"Delete\",\n      \"DeleteIssuers\",\n      \"Get\",\n      \"GetIssuers\",\n      \"Import\",\n      \"List\",\n      \"ListIssuers\",\n      \"ManageContacts\",\n      \"ManageIssuers\",\n      \"Purge\",\n      \"SetIssuers\",\n      \"Update\"\n    ]\n\n    key_permissions = [\n      \"Backup\",\n      \"Create\",\n      \"Decrypt\",\n      \"Delete\",\n      \"Encrypt\",\n      \"Get\",\n      \"Import\",\n      \"List\",\n      \"Purge\",\n      \"Recover\",\n      \"Restore\",\n      \"Sign\",\n      \"UnwrapKey\",\n      \"Update\",\n      \"Verify\",\n      \"WrapKey\"\n    ]\n\n    secret_permissions = [\n      \"Backup\",\n      \"Delete\",\n      \"Get\",\n      \"List\",\n      \"Purge\",\n      \"Restore\",\n      \"Restore\",\n      \"Set\"\n    ]\n  }\n\n  access_policy {\n    object_id    = azurerm_user_assigned_identity.base.principal_id\n    tenant_id    = data.azurerm_client_config.current.tenant_id\n\n    secret_permissions = [\n      \"Get\"\n    ]\n  }\n}\n\noutput \"secret_identifier\" {\n  value = azurerm_key_vault_certificate.example.secret_id\n}\n\nresource \"azurerm_key_vault_certificate\" \"example\" {\n  name         = \"imported-cert\"\n  key_vault_id = azurerm_key_vault.kv.id\n\n  certificate {\n    contents = filebase64(\"C:/appgwlistner.pfx\")\n    password = \"password\"\n  }\n\n  certificate_policy {\n    issuer_parameters {\n      name = \"Self\"\n    }\n\n    key_properties {\n      exportable = true\n      key_size   = 2048\n      key_type   = \"RSA\"\n      reuse_key  = false\n    }\n\n    secret_properties {\n      content_type = \"application/x-pkcs12\"\n    }\n  }\n}\n\nresource \"azurerm_virtual_network\" \"example\" {\n  name                = \"example-network\"\n  resource_group_name = data.azurerm_resource_group.example.name\n  location            = data.azurerm_resource_group.example.location\n  address_space       = [\"10.254.0.0/16\"]\n}\n\nresource \"azurerm_subnet\" \"frontend\" {\n  name                 = \"frontend\"\n  resource_group_name  = data.azurerm_resource_group.example.name\n  virtual_network_name = azurerm_virtual_network.example.name\n  address_prefixes     = [\"10.254.0.0/24\"]\n}\n\nresource \"azurerm_subnet\" \"backend\" {\n  name                 = \"backend\"\n  resource_group_name  = data.azurerm_resource_group.example.name\n  virtual_network_name = azurerm_virtual_network.example.name\n  address_prefixes     = [\"10.254.2.0/24\"]\n}\n\nresource \"azurerm_public_ip\" \"example\" {\n  name                = \"example-pip\"\n  resource_group_name = data.azurerm_resource_group.example.name\n  location            = data.azurerm_resource_group.example.location\n  allocation_method   = \"Static\"\n  sku = \"standard\"\n}\n\n#&nbsp;since these variables are re-used - a locals block makes this more maintainable\nlocals {\n  backend_address_pool_name      = \"${azurerm_virtual_network.example.name}-beap\"\n  frontend_port_name             = \"${azurerm_virtual_network.example.name}-feport\"\n  frontend_ip_configuration_name = \"${azurerm_virtual_network.example.name}-feip\"\n  http_setting_name              = \"${azurerm_virtual_network.example.name}-be-htst\"\n  listener_name                  = \"${azurerm_virtual_network.example.name}-httplstn\"\n  request_routing_rule_name      = \"${azurerm_virtual_network.example.name}-rqrt\"\n  redirect_configuration_name    = \"${azurerm_virtual_network.example.name}-rdrcfg\"\n\n}\n\nresource \"null_resource\" \"previous\" {}\n\nresource \"time_sleep\" \"wait_240_seconds\" {\n  depends_on = [azurerm_key_vault.kv]\n\n  create_duration = \"240s\"\n}\n\nresource \"azurerm_application_gateway\" \"network\" {\n  name                = \"example-appgateway\"\n  resource_group_name = data.azurerm_resource_group.example.name\n  location            = data.azurerm_resource_group.example.location\n\n  sku {\n    name     = \"Standard_v2\"\n    tier     = \"Standard_v2\"\n    capacity = 2\n  }\n\n  gateway_ip_configuration {\n    name      = \"my-gateway-ip-configuration\"\n    subnet_id = azurerm_subnet.frontend.id\n  }\n\n  frontend_port {\n    name = local.frontend_port_name\n    port = 443\n  }\n\n  frontend_ip_configuration {\n    name                 = local.frontend_ip_configuration_name\n    public_ip_address_id = azurerm_public_ip.example.id\n  }\n\n  backend_address_pool {\n    name = local.backend_address_pool_name\n  }\n\n  backend_http_settings {\n    name                  = local.http_setting_name\n    cookie_based_affinity = \"Disabled\"\n    path                  = \"/path1/\"\n    port                  = 443\n    protocol              = \"Https\"\n    request_timeout       = 60\n  }\n\n  http_listener {\n    name                           = local.listener_name\n    frontend_ip_configuration_name = local.frontend_ip_configuration_name\n    frontend_port_name             = local.frontend_port_name\n    protocol                       = \"Https\"\n    ssl_certificate_name = \"app_listener\"\n  }\n\n  identity {\n    type = \"UserAssigned\"\n    identity_ids = [azurerm_user_assigned_identity.base.id]\n  }\n\n  ssl_certificate {\n    name = \"app_listener\"\n    key_vault_secret_id = azurerm_key_vault_certificate.example.secret_id\n  }\n\n  request_routing_rule {\n    name                       = local.request_routing_rule_name\n    rule_type                  = \"Basic\"\n    http_listener_name         = local.listener_name\n    backend_address_pool_name  = local.backend_address_pool_name\n    backend_http_settings_name = local.http_setting_name\n  }\n  depends_on = [time_sleep.wait_240_seconds]\n}\n`\n```\nOutputs:\n\nNote:\nPlease make sure to have the pfx certificate with private keys. While you are exporting a pfx certificate using a security certificate, please make sure to have the following propeties selected as shown below and then give a password and export it.",
      "question_score": 12,
      "answer_score": 12,
      "created_at": "2021-09-15T14:20:25",
      "url": "https://stackoverflow.com/questions/69193030/terraform-how-to-attach-ssl-certificate-stored-in-azure-keyvault-to-an-applica"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 76419099,
      "title": "Access Denied when creating S3 Bucket ACL &amp; S3 Policy using Terraform",
      "problem": "I'm trying to create an S3 bucket using Terraform, but keep getting Access Denied errors.\nI have the following Terraform code:\n```\n`resource \"aws_s3_bucket\" \"prod_media\" {\n  bucket = var.prod_media_bucket\n  acl = \"public-read\"\n}\n\nresource \"aws_s3_bucket_cors_configuration\" \"prod_media\" {\n  bucket = aws_s3_bucket.prod_media.id  \n\n  cors_rule {\n    allowed_headers = [\"*\"]\n    allowed_methods = [\"GET\", \"HEAD\"]\n    allowed_origins = [\"*\"]\n    expose_headers  = [\"ETag\"]\n    max_age_seconds = 3000\n  }  \n}\n\nresource \"aws_s3_bucket_acl\" \"prod_media\" {\n    bucket = aws_s3_bucket.prod_media.id\n    acl    = \"public-read\"\n}\n\nresource \"aws_iam_user\" \"prod_media_bucket\" {\n  name = \"prod-media-bucket\"\n}\n\nresource \"aws_s3_bucket_policy\" \"prod_media_bucket\" {\n    bucket = aws_s3_bucket.prod_media.id\n    policy = jsonencode({\n    Version = \"2012-10-17\"\n    Statement = [\n      {\n        Principal = \"*\"\n        Action = [\n          \"s3:*\",\n        ]\n        Effect = \"Allow\"\n        Resource = [\n          \"arn:aws:s3:::${var.prod_media_bucket}\",\n          \"arn:aws:s3:::${var.prod_media_bucket}/*\"\n        ]\n      },\n      {\n        Sid = \"PublicReadGetObject\"\n        Principal = \"*\"\n        Action = [\n          \"s3:GetObject\",\n        ]\n        Effect   = \"Allow\"\n        Resource = [\n          \"arn:aws:s3:::${var.prod_media_bucket}\",\n          \"arn:aws:s3:::${var.prod_media_bucket}/*\"\n        ]\n      },\n    ]\n  })\n}\n\nresource \"aws_iam_user_policy\" \"prod_media_bucket\" {\n  user = aws_iam_user.prod_media_bucket.name\n  policy = aws_s3_bucket_policy.prod_media_bucket.id\n}\n\nresource \"aws_iam_access_key\" \"prod_media_bucket\" {\n  user = aws_iam_user.prod_media_bucket.name\n}\n`\n```\nWhenever I run `terraform apply` I get the following error:\n```\n`\u2577\n\u2502 Error: error creating S3 bucket ACL for prod-media-8675309: AccessDenied: Access Denied\n\u2502       status code: 403, request id: XNW2R0KWFYB3KB9R, host id: CuBMdZSaJJgu+0Rprzlptt7oRsjMxBNNHJPhFq98ROGC9l9BUmfmv5YxYZuxf/V3GJBoiGJKJkg=\n\u2502\n\u2502   with aws_s3_bucket_acl.prod_media,\n\u2502   on s3.tf line 18, in resource \"aws_s3_bucket_acl\" \"prod_media\":\n\u2502   18: resource \"aws_s3_bucket_acl\" \"prod_media\" {\n\u2502\n\u2575\n\u2577\n\u2502 Error: Error putting S3 policy: AccessDenied: Access Denied\n\u2502       status code: 403, request id: XNW60T7SQXW1Y4SV, host id: AyGS46L37yIcI4JwddrjHo4GRF7T9JrnfD8TGNdUhpO5uLOWBbgY3+c4opoQTFc2jRdHtXwkqO8=\n\u2502\n\u2502   with aws_s3_bucket_policy.prod_media_bucket,\n\u2502   on s3.tf line 28, in resource \"aws_s3_bucket_policy\" \"prod_media_bucket\":\n\u2502   28: resource \"aws_s3_bucket_policy\" \"prod_media_bucket\" {\n\u2502\n`\n```\nThe account running the Terraform has Administrator Access to all resources.\n`{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": \"*\",\n            \"Resource\": \"*\"\n        }\n    ]\n}\n`\nPlease help identify what is causing this error.",
      "solution": "There are few issues in your code:\n\n`acl` attribute of `aws_s3_bucket` is deprecated and shouldn't be used.\nYou don't have `aws_s3_bucket_ownership_controls`\nYou don't have `aws_s3_bucket_public_access_block`\nYou are missing relevant `depends_on`\n`aws_iam_user_policy` can't use `aws_s3_bucket_policy.prod_media_bucket.id` (its not even clear what do you want to accomplish here, so I removed it from the code below).\n\nThe working code is:\n```\n`\nresource \"aws_s3_bucket\" \"prod_media\" {\n  bucket = var.prod_media_bucket\n}\n\nresource \"aws_s3_bucket_cors_configuration\" \"prod_media\" {\n  bucket = aws_s3_bucket.prod_media.id  \n\n  cors_rule {\n    allowed_headers = [\"*\"]\n    allowed_methods = [\"GET\", \"HEAD\"]\n    allowed_origins = [\"*\"]\n    expose_headers  = [\"ETag\"]\n    max_age_seconds = 3000\n  }  \n}\n\nresource \"aws_s3_bucket_acl\" \"prod_media\" {\n    bucket = aws_s3_bucket.prod_media.id\n    acl    = \"public-read\"\n    depends_on = [aws_s3_bucket_ownership_controls.s3_bucket_acl_ownership]\n}\n\nresource \"aws_s3_bucket_ownership_controls\" \"s3_bucket_acl_ownership\" {\n  bucket = aws_s3_bucket.prod_media.id\n  rule {\n    object_ownership = \"BucketOwnerPreferred\"\n  }\n  depends_on = [aws_s3_bucket_public_access_block.example]\n}\n\nresource \"aws_iam_user\" \"prod_media_bucket\" {\n  name = \"prod-media-bucket\"\n}\n\nresource \"aws_s3_bucket_public_access_block\" \"example\" {\n  bucket = aws_s3_bucket.prod_media.id\n\n  block_public_acls       = false\n  block_public_policy     = false\n  ignore_public_acls      = false\n  restrict_public_buckets = false\n}\n\nresource \"aws_s3_bucket_policy\" \"prod_media_bucket\" {\n    bucket = aws_s3_bucket.prod_media.id\n    policy = jsonencode({\n    Version = \"2012-10-17\"\n    Statement = [\n      {\n        Principal = \"*\"\n        Action = [\n          \"s3:*\",\n        ]\n        Effect = \"Allow\"\n        Resource = [\n          \"arn:aws:s3:::${var.prod_media_bucket}\",\n          \"arn:aws:s3:::${var.prod_media_bucket}/*\"\n        ]\n      },\n      {\n        Sid = \"PublicReadGetObject\"\n        Principal = \"*\"\n        Action = [\n          \"s3:GetObject\",\n        ]\n        Effect   = \"Allow\"\n        Resource = [\n          \"arn:aws:s3:::${var.prod_media_bucket}\",\n          \"arn:aws:s3:::${var.prod_media_bucket}/*\"\n        ]\n      },\n    ]\n  })\n  \n  depends_on = [aws_s3_bucket_public_access_block.example]\n}\n`\n```",
      "question_score": 11,
      "answer_score": 36,
      "created_at": "2023-06-07T01:54:32",
      "url": "https://stackoverflow.com/questions/76419099/access-denied-when-creating-s3-bucket-acl-s3-policy-using-terraform"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 73741756,
      "title": "Disabling allow public blob access using terraform",
      "problem": "I have created a storage account using a Terraform. I would like to disable the option found under the storage account settings and configuration in the Azure portal called Allow public blob access, however under the azurerm_storage_account command, I cannot seem to find the option required to achieve this.\nBelow is my code so far to create the storage account, which works, but if anyone could point me in the right direction that would be great, thank you.\nStorage Account\n```\n`resource \"azurerm_storage_account\" \"st\" {\n    name = var.st.name\n    resource_group_name = var.rg_shared_name\n    location = var.rg_shared_location\n    account_tier = var.st.tier\n    account_replication_type = var.st.replication\n    public_network_access_enabled = false\n}\n`\n```",
      "solution": "As soon as I've posted this question, I found the command, so I apologise for wasting your time.\nThe command to use is allow_nested_items_to_be_public, if you set this to false it will disable the feature found under Storage Account > Settings > Configuration, Allow blob public access\nSource\nhttps://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/storage_account#allow_nested_items_to_be_public\nUpdated Code\n```\n`resource \"azurerm_storage_account\" \"st\" {\n    name = var.st.name\n    resource_group_name = var.rg_shared_name\n    location = var.rg_shared_location\n    account_tier = var.st.tier\n    account_replication_type = var.st.replication\n    public_network_access_enabled = false\n    allow_nested_items_to_be_public = false\n}\n`\n```",
      "question_score": 11,
      "answer_score": 11,
      "created_at": "2022-09-16T10:06:52",
      "url": "https://stackoverflow.com/questions/73741756/disabling-allow-public-blob-access-using-terraform"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 73731278,
      "title": "Getting error as &quot;The scope is not valid., field: SCOPE_VALUE, parameter: CLOUDFRONT&quot;, in terraform",
      "problem": "I tried to create waf web acl using below terraform script with the region of one of my aws account (abc) as ap-southeast-1 in .aws/config file, But getting below error after applying it. whereas Same script created waf web acl successfully if my another aws account (xyz) profile region was us-east-1 in .aws/config file.\n```\n`resource \"aws_wafv2_web_acl\" \"waf_acl\" {\n  name        = local.waf_name\n  description = \"waf setup infront of cloudfront\"\n  scope       = \"CLOUDFRONT\"\n\n  default_action {\n    allow {}\n  }\n\n  rule {\n    name     = \"AWS-AWSManagedRulesAmazonIpReputationList\"\n    priority = 0\n\n    override_action {\n      none {}\n    }\n\n    statement {\n      managed_rule_group_statement {\n        name        = \"AWSManagedRulesAmazonIpReputationList\"\n        vendor_name = \"AWS\"\n      }\n    }\n\n    visibility_config {\n      cloudwatch_metrics_enabled = true\n      metric_name                = \"AWS-AWSManagedRulesAmazonIpReputationList\"\n      sampled_requests_enabled   = true\n    }\n  }\n\n  rule {\n    name     = \"AWS-AWSManagedRulesAnonymousIpList\"\n    priority = 1\n\n    override_action {\n      none {}\n    }\n\n    statement {\n      managed_rule_group_statement {\n        name        = \"AWSManagedRulesAnonymousIpList\"\n        vendor_name = \"AWS\"\n      }\n    }\n\n    visibility_config {\n      cloudwatch_metrics_enabled = true\n      metric_name                = \"AWS-AWSManagedRulesAnonymousIpList\"\n      sampled_requests_enabled   = true\n    }\n  }\n\n  rule {\n    name     = \"AWS-AWSManagedRulesCommonRuleSet\"\n    priority = 2\n\n    override_action {\n      none {}\n    }\n\n    statement {\n      managed_rule_group_statement {\n        name        = \"AWSManagedRulesCommonRuleSet\"\n        vendor_name = \"AWS\"\n      }\n    }\n\n    visibility_config {\n      cloudwatch_metrics_enabled = true\n      metric_name                = \"AWS-AWSManagedRulesCommonRuleSet\"\n      sampled_requests_enabled   = true\n    }\n  }\n\n  visibility_config {\n    cloudwatch_metrics_enabled = true\n    metric_name                = local.waf_name\n    sampled_requests_enabled   = true\n  }\n}\n`\n```\nError as below\n```\n`\u2502 Error: Error creating WAFv2 WebACL: WAFInvalidParameterException: Error reason: The scope is not valid., field: SCOPE_VALUE, parameter: CLOUDFRONT\n\u2502 {\n\u2502   RespMetadata: {\n\u2502     StatusCode: 400,\n\u2502     RequestID: \"b83b40074r-b3a55-49e76-b2353-e16f32830518632\"\n\u2502   },\n\u2502   Field: \"SCOPE_VALUE\",\n\u2502   Message_: \"Error reason: The scope is not valid., field: SCOPE_VALUE, parameter: CLOUDFRONT\",\n\u2502   Parameter: \"CLOUDFRONT\",\n\u2502   Reason: \"The scope is not valid.\"\n\u2502 }\n\u2502 \n\u2502   with aws_wafv2_web_acl.waf_acl,\n\u2502   on main.tf line 122, in resource \"aws_wafv2_web_acl\" \"waf_acl\":\n\u2502  122: resource \"aws_wafv2_web_acl\" \"waf_acl\" {\n`\n```\nPlease Note:- same script worked perfectly fine in us-east-1 region with the scope=\"CLOUDFRONT\".\nAny help would be really appreciable.\nThanks in advance.",
      "solution": "You already answered on your question. `CLOUDFRONT` scope should be created at `us-east-1` region.\n\nAWS WAF is available globally for CloudFront distributions, but you must use the Region US East (N. Virginia) to create your web ACL and any resources used in the web ACL, such as rule groups, IP sets, and regex pattern sets. Some interfaces offer a region choice of \"Global (CloudFront)\". Choosing this is identical to choosing Region US East (N. Virginia) or \"us-east-1\".\n\nHowever it is possible to use multi-region deployment in terraform\n```\n`provider \"aws\" {\n  region = \"ap-southeast-1\"\n}\n\n# Additional provider configuration for us-east-1 region; resources can\n# reference this as `aws.east`.\nprovider \"aws\" {\n  alias  = \"east\"\n  region = \"us-east-1\"\n}\n\nresource \"aws_wafv2_web_acl\" \"waf_acl\" {\n  provider = aws.east\n\n  # ...\n}\n`\n```",
      "question_score": 11,
      "answer_score": 28,
      "created_at": "2022-09-15T14:36:15",
      "url": "https://stackoverflow.com/questions/73731278/getting-error-as-the-scope-is-not-valid-field-scope-value-parameter-cloudf"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 65583605,
      "title": "upgrade from 0.12 to 0.13: Failed to instantiate provider &quot;registry.terraform.io/-/aws&quot; to obtain",
      "problem": "I'm trying to upgrade from terraform 0.12 to 0.13.\nit seems to have no specific problem of syntax when I run `terraform 0.13upgrade` nothing is changed.\nonly a file `version.tf` is added\n```\n`+terraform {\n+  required_providers {\n+    aws = {\n+      source = \"hashicorp/aws\"\n+    }\n+  }\n+  required_version = \">= 0.13\"\n+}\n`\n```\nand when I run `terraform plan` I got\n```\n`\nError: Could not load plugin\n\nPlugin reinitialization required. Please run \"terraform init\".\n\nPlugins are external binaries that Terraform uses to access and manipulate\nresources. The configuration provided requires plugins which can't be located,\ndon't satisfy the version constraints, or are otherwise incompatible.\n\nTerraform automatically discovers provider requirements from your\nconfiguration, including providers used in child modules. To see the\nrequirements and constraints, run \"terraform providers\".\n\n2 problems:\n\n- Failed to instantiate provider \"registry.terraform.io/-/aws\" to obtain\nschema: unknown provider \"registry.terraform.io/-/aws\"\n- Failed to instantiate provider \"registry.terraform.io/-/template\" to obtain\nschema: unknown provider \"registry.terraform.io/-/template\"\n\n`\n```\nrunning `terraform providers` shows\n```\n`Providers required by configuration:\n.\n\u251c\u2500\u2500 provider[registry.terraform.io/hashicorp/aws]\n\u251c\u2500\u2500 module.bastion\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 provider[registry.terraform.io/hashicorp/template]\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 provider[registry.terraform.io/hashicorp/aws]\n\u2514\u2500\u2500 module.vpc\n    \u2514\u2500\u2500 provider[registry.terraform.io/hashicorp/aws] >= 2.68.*\n\nProviders required by state:\n\n    provider[registry.terraform.io/-/aws]\n\n    provider[registry.terraform.io/-/template]\n\n`\n```\nSo my guess is form some reason I have `-/aws` instead of `hashicorp/aws` in my tfstate, however I can't find this specific string at all in the tfstate.\nI tried:\n\nrunning `terraform init`\n`terraform init -reconfigure`\ndeleting the `.terraform` folder\ndeleting the `~/.terraform.d` folder\n\nSo I'm running out of ideas on how to solve this problem",
      "solution": "I followed the steps here\n`terraform state replace-provider   registry.terraform.io/-/template  registry.terraform.io/hashicorp/template\nterraform state replace-provider   registry.terraform.io/-/aws  registry.terraform.io/hashicorp/aws\n`\nand it fixed my problem.",
      "question_score": 11,
      "answer_score": 27,
      "created_at": "2021-01-05T18:20:54",
      "url": "https://stackoverflow.com/questions/65583605/upgrade-from-0-12-to-0-13-failed-to-instantiate-provider-registry-terraform-io"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 65935259,
      "title": "terraform: add public IP to only one azure VM",
      "problem": "I'm creating 4 vms through count in azurerm_virtual_machine but i want to create only one public IP and associate it with the first VM ? is that possible if so how ?\nbelow is my template file\n```\n`resource \"azurerm_network_interface\" \"nics\" {\n  count               = 4\n  name                = ...\n  location            = ...\n  resource_group_name = ...\n  ip_configuration {\n    subnet_id                     = ... \n    private_ip_address_allocation = \"Static\"\n    private_ip_address = ...\n  }\n}\n\nresource \"azurerm_public_ip\" \"public_ip\" {\n  name                = ...\n  location            = ...\n  resource_group_name = ...\n}\n\nresource \"azurerm_virtual_machine\" \"vms\" {\n  count                             = 4\n  network_interface_ids             = [element(azurerm_network_interface.nics.*.id, count.index)]\n}\n`\n```\ni have already gone through below questions but they are create multiple public ip's & add them to all vms.\nmultiple-vms-with-public-ip\nset-dynamic-ip\nattach-public-ip",
      "solution": "Public IPs are created using azurerm_public_ip:\n```\n`resource \"azurerm_public_ip\" \"public_ip\" {\n  name                = \"acceptanceTestPublicIp1\"\n  resource_group_name = azurerm_resource_group.example.name\n  location            = azurerm_resource_group.example.location\n  allocation_method   = \"Dynamic\"\n}\n`\n```\nHaving the address in your `azurerm_network_interface` you could do the following using Conditional Expressions:\n```\n`resource \"azurerm_network_interface\" \"nics\" {\n  count               = 4\n  name                = ...\n  location            = ...\n  resource_group_name = ...\n\n  ip_configuration {\n\n    subnet_id                     = ... \n    private_ip_address_allocation = \"Static\"\n    private_ip_address            = ...\n    \n    public_ip_address_id = count.index == 1 ? azurerm_public_ip.public_ip.id : null\n  }\n}\n`\n```",
      "question_score": 11,
      "answer_score": 13,
      "created_at": "2021-01-28T11:28:34",
      "url": "https://stackoverflow.com/questions/65935259/terraform-add-public-ip-to-only-one-azure-vm"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 76272116,
      "title": "Terraform: resolve &quot;no available releases match the given constraints&quot; error",
      "problem": "I am trying to update hashicorp/aws provider version.\nI added terraform.tf file with following content:\n```\n`terraform {\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~> 4.0\"\n    }\n  }\n}\n`\n```\nLater I tried to update modules using:\n```\n`terraform init -upgrade\n`\n```\nHowever, I have started to get:\n`Could not retrieve the list of available versions for provider hashicorp/aws: \nno available releases match the given constraints >= 2.0.0, ~> 3.27, ~> 4.0\n`\nHow could this problem be resolved?",
      "solution": "This is the important part of the error message.\n```\n`>= 2.0.0, ~> 3.27, ~> 4.0\n`\n```\n\nYou request a version greater than or equal to 2.0.0\nYou prefer a version 3.27\nYou prefer a version 4.0\n\nBoth 2 and 3 cannot be possible at the same time.\nThe solution for this specific case is the stop requesting 2 different versions at the same time.\nCheck versions of available providers:\n```\n`!+?main ~/Projects/x/src/x-devops/terraform/env/test> terraform providers\n\nProviders required by configuration:\n.\n\u251c\u2500\u2500 module.test-sonar\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 provider[registry.terraform.io/hashicorp/aws]\n\u251c\u2500\u2500 module.client_vpn\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 provider[registry.terraform.io/hashicorp/aws]\n\u251c\u2500\u2500 module.test-appserver\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 provider[registry.terraform.io/hashicorp/aws] ~> 3.27\n\u251c\u2500\u2500 module.test-vpn-server\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 provider[registry.terraform.io/hashicorp/aws]\n\u251c\u2500\u2500 module.test-networking\n...\n`\n```\nThere is a module which requests `3.27`.\nFind all modules which request 3.27 and update them to 4.0.\nThis should resolve such problems.",
      "question_score": 10,
      "answer_score": 30,
      "created_at": "2023-05-17T14:28:44",
      "url": "https://stackoverflow.com/questions/76272116/terraform-resolve-no-available-releases-match-the-given-constraints-error"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 74836213,
      "title": "`Error 403: Insufficient regional quota to satisfy request: resource &quot;SSD_TOTAL_GB&quot;` when creating kubernetes cluster with terraform",
      "problem": "Hi I am playing around with kubernetes and terraform in a google cloud free tier account (trying to use the free 300$). Here is my terraform resource declaration, it is something very standard I copied from the terraform resource page. Nothing particularly strange here.\n```\n`resource \"google_container_cluster\" \"cluster\" {\n  name = \"${var.cluster-name}-${terraform.workspace}\"\n  location = var.region\n  initial_node_count = 1\n  project = var.project-id\n  remove_default_node_pool = true\n}\n\nresource \"google_container_node_pool\" \"cluster_node_pool\" {\n  name       = \"${var.cluster-name}-${terraform.workspace}-node-pool\"\n  location   = var.region\n  cluster    = google_container_cluster.cluster.name\n  node_count = 1\n\n  node_config {\n    preemptible  = true\n    machine_type = \"e2-medium\"\n    service_account = google_service_account.default.email\n    oauth_scopes    = [\n      \"https://www.googleapis.com/auth/cloud-platform\"\n    ]\n  }\n}\n`\n```\nThis terraform snippet used to work fine. In order to not burn through the 300$ too quickly, at the end of every day I used to destroy the cluster with `terraform destroy`.\nHowever one day the kubernetes cluster creation just stopped working. Here is the error:\n```\n`Error: googleapi: Error 403: Insufficient regional quota to satisfy request: resource \"SSD_TOTAL_GB\": request requires '300.0' and is short '50.0'. project has a quota of '250.0' with '250.0' available. View and manage quotas at https://console.cloud.google.com/iam-admin/quotas?usage=USED&project=xxxxxx., forbidden\n`\n```\nIt looks like something didn't get cleaned up after all the terraform destroy and eventually some quota built up and I am not able to create a cluster anymore. I am still able to create a cluster through the google cloud web interface (I tried only with autopilot, and in the same location). I am a bit puzzled why this is happening. Is my assumption correct? Do I need to delete something that doesn't get deleted automatically with terraform? if yes why? Is there a way to fix this and be able to create the cluster with terraform again?",
      "solution": "I ran into the same issue and I think I figured out what's going. The crucial thing here is to understand the difference between zonal and regional clusters.\ntldr; A zonal cluster operates in only zone, where a regional cluster may be replicated across multiple zones.\nFrom the doc,\n\nBy default, GKE replicates each node pool across three zones of the control plane's region\n\nI think this is why we're seeing the requirement going to 300GB (3 * 100GB), where the `--disk-size` defaults to 100GB.\nThe solution is to set the `location` to a `zone` than a `region`. Of course, here I'm assuming a zonal cluster would satisfy your requirements. E.g.\n```\n`resource \"google_container_cluster\" \"cluster\" {\n  name = \"${var.cluster-name}-${terraform.workspace}\"\n  location = \"us-central1-f\"\n  initial_node_count = 1\n  project = var.project-id\n  remove_default_node_pool = true\n}\n\nresource \"google_container_node_pool\" \"cluster_node_pool\" {\n  name       = \"${var.cluster-name}-${terraform.workspace}-node-pool\"\n  location   = \"us-central1-f\"\n  cluster    = google_container_cluster.cluster.name\n  node_count = 1\n\n  node_config {\n    ...\n  }\n}\n`\n```",
      "question_score": 10,
      "answer_score": 16,
      "created_at": "2022-12-17T18:48:40",
      "url": "https://stackoverflow.com/questions/74836213/error-403-insufficient-regional-quota-to-satisfy-request-resource-ssd-total"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 67907211,
      "title": "How to resolve &quot;googleapi: Error 403: The caller does not have permission, forbidden&quot;",
      "problem": "I am using terraform to build infra in GCP. I am trying to assign roles to a `service account` using terraform but unable to do so. Below is my code:\nsa.tf:\n```\n`resource \"google_service_account\" \"mojo-terra\" {\n  account_id   = \"mojo-terra\"\n  description  = \"Service account used for terraform script\"\n}\n\nresource \"google_project_iam_member\" \"mojo-roles\" {\n  count = length(var.rolesList)\n  role =  var.rolesList[count.index]\n  member = \"serviceAccount:${google_service_account.mojo-terra.email}\"\n}\n`\n```\ndev.tfvars:\n```\n`rolesList = [\n    \"roles/iam.serviceAccountUser\"\n]\n`\n```\ncloudbuild logs:\n```\n`Step #2: Error: Error when reading or editing Resource \"project \\\"poc-dev\\\"\" with IAM Policy: Error retrieving IAM policy for project \"poc-dev\": googleapi: Error 403: The caller does not have permission, forbidden\nStep #2: \nStep #2: \nStep #2: \nStep #2: Error: Error when reading or editing Resource \"project \\\"poc-dev\\\"\" with IAM Member: Role \"roles/iam.serviceAccountUser\" Member \"serviceAccount:asadsfs@poc-dev-1221.iam.gserviceaccount.com\": Error retrieving IAM policy for project \"poc-dev\": googleapi: Error 403: The caller does not have permission, forbidden\nStep #2: \n`\n```\nBelow are the roles attached to my `cloudbuild service account`:\n`Custom Role cloudbuild, Cloud Build Service Account, Service Account Admin, Create Service Accounts, Delete Service Accounts, Service Account User, Storage Admin`",
      "solution": "The service account providing authorization to Terraform is missing the permission `resourcemanager.projects.getIamPolicy` which is the source of the error message.\nThe service account is also missing the permission `resourcemanager.projects.setIamPolicy` which is required to change IAM policies.\nThose permissions are part of the role `roles/resourcemanager.projectIamAdmin` (Project IAM Admin).\nTo list the roles assigned to the service account:\n```\n`gcloud projects get-iam-policy  \\\n--flatten=\"bindings[].members\" \\\n--format='table(bindings.role)' \\\n--filter=\"bindings.members:\"\n`\n```\nTo list the permissions that a role contains:\n```\n`gcloud iam roles describe roles/resourcemanager.projectIamAdmin\n`\n```\nTo add the required role to the service account:\n```\n`gcloud projects add-iam-policy-binding  \\\n--member=serviceAccount: \\\n--role=roles/resourcemanager.projectIamAdmin\n`\n```",
      "question_score": 10,
      "answer_score": 24,
      "created_at": "2021-06-09T17:25:41",
      "url": "https://stackoverflow.com/questions/67907211/how-to-resolve-googleapi-error-403-the-caller-does-not-have-permission-forbi"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 72681536,
      "title": "azure cli $Path error running in terraform cloud",
      "problem": "Setting up terraform cloud for the first time and getting this error. Not sure why as on my local machine azure CLI is installed and the path is set, but I think has something to do with setting it in the terraform cloud platform.\n```\n`Error: building AzureRM Client: please ensure you have installed Azure CLI version 2.0.79 or newer. Error parsing json result from the Azure CLI: launching Azure CLI: exec: \"az\": executable file not found in $PATH.\nwith provider[\"registry.terraform.io/hashicorp/azurerm\"]\non versions.tf line 21, in provider \"azurerm\":\n\nprovider \"azurerm\" {\n`\n```\nMy currently tf code\nversions.tf\n```\n`terraform {\n\n  cloud {\n    organization = \"myorg\"\n\n    workspaces {\n      name = \"dev\"\n    }\n  }\n\n  required_providers {\n    azurerm = {\n      source  = \"hashicorp/azurerm\"\n      version = \"~>3.10.0\"\n    }\n  }\n\n  required_version = \">= 1.2.3\"\n}\n\nprovider \"azurerm\" {\n  features {}\n}\n`\n```\nvariables.tf\n```\n`variable \"tenant_id\" {\n    description = \"tenant id for azure subscription\"\n}\n`\n```\nmain.tf\n```\n`resource \"azurerm_resource_group\" \"testrg\" {\n  name     = \"test-rg\"\n  location = \"Central US\"\n}\n`\n```\nnot doing anything fancy, but not sure how to get past the azure CLI error. I know where variables can be set in the terraform cloud platform, but not specifically where to set a $Path for the azure cli or even how to install azure cli in terraform cloud. On my local machine, I am logging in with az login on an account with sufficient permissions to the subscription.",
      "solution": "I'm trying to \"boil down\" kavya Saraboju's answer, which is formally correct, to a bare minimum that helped me.\nThe Error message seems to be very confusing, if it has anything at all to do with the actual problem. I had to set the environment variables `ARM_CLIENT_ID`, `ARM_TENANT_ID`, `ARM_CLIENT_SECRET` and `ARM_SUBSCRIPTION_ID` in Terraform Cloud. Go to Terraform Cloud's web admin panel, choose your workspace, click on \"Variables\" and set all the required values:\n\nRead here how to obtain the values for those variables.\nI'm a beginner as well on both Terraform and Azure, but I anyway hope this answer will help anybody who stumbles across this issue.\nAnd also, my solution is described comprehensively in this tutorial.",
      "question_score": 10,
      "answer_score": 15,
      "created_at": "2022-06-20T03:53:58",
      "url": "https://stackoverflow.com/questions/72681536/azure-cli-path-error-running-in-terraform-cloud"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 73293980,
      "title": "Terraform: Failed to query available provider packages (Azapi)",
      "problem": "I try to use the Azure/Azapi Provider within my Terraform project but after I add the provider and run `terraform init`, I get the following error:\n```\n`Error: Failed to query available provider packages\nCould not retrieve the list of available versions for provider hashicorp/azapi: provider registry registry.terraform.io does not have a provider named registry.terraform.io/hashicorp/azapi \n`\n```\nThis is how my providers.tf looks like:\n```\n`terraform {\n  required_providers {\n    azurerm = {\n      source  = \"hashicorp/azurerm\"\n      version = \"=3.16.0\"\n    }\n    azapi = {\n      source  = \"azure/azapi\"\n      version = \"=0.4.0\"\n    }\n\n  }\n\n  required_version = \"=1.2.6\"\n}\n\nprovider \"azurerm\" {\n  features {}\n}\n\nprovider \"azapi\" {\n}\n`\n```\nWhen I run `terraform providers`, I can see that the provider has a wrong registry URL within my module:\n```\n`\u251c\u2500\u2500 module.az-aca-env\n\u2502   \u2514\u2500\u2500 provider[registry.terraform.io/hashicorp/azapi]\n`\n```\nI would expect something like registry.terraform.io/azure/azapi.\nAny ideas?",
      "solution": "Okay, I found a workaround. I have to add a `providers.tf` inside my module directory (/modules/az-aca-env) with the following content:\n```\n`terraform {\n  required_providers {\n    azapi = {\n      source  = \"Azure/azapi\"\n      version = \"=0.4.0\"\n    }\n  }\n}\n`\n```\nAfter adding it, the `terraform init` works \u2705.",
      "question_score": 10,
      "answer_score": 20,
      "created_at": "2022-08-09T16:58:16",
      "url": "https://stackoverflow.com/questions/73293980/terraform-failed-to-query-available-provider-packages-azapi"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 69180684,
      "title": "How do I apply a CRD from github to a cluster with terraform?",
      "problem": "I want to install a CRD with terraform, I was hoping it would be easy as doing this:\n```\n`data \"http\" \"crd\" {\n  url = \"https://raw.githubusercontent.com/kubernetes-sigs/application/master/deploy/kube-app-manager-aio.yaml\"\n  request_headers = {\n    Accept = \"text/plain\"\n  }\n}\n\nresource \"kubernetes_manifest\" \"install-crd\" {\n  manifest = data.http.crd.body\n}\n`\n```\nBut I get this error:\n```\n`can't unmarshal tftypes.String into *map[string]tftypes.Value, expected\nmap[string]tftypes.Value\n`\n```\nTrying to convert it to yaml with `yamldecode` also doesn't work because `yamldecode` doesn't support multi-doc yaml files.\nI could use exec, but I was already doing that while waiting for the `kubernetes_manifest` resource to be released. Does `kubernetes_manifest` only support a single resource or can it be used to create several from a raw text manifest file?",
      "solution": "`kubernetes_manifest` (emphasis mine)\n\nRepresents one Kubernetes resource by supplying a manifest attribute\n\nThat sounds to me like it does not support multiple resources / a multi doc yaml file.\nHowever you can manually split the incoming document and `yamldecode` the parts of it:\n```\n`locals {\n  yamls = [for data in split(\"---\", data.http.crd.body): yamldecode(data)]\n}\n\nresource \"kubernetes_manifest\" \"install-crd\" {\n  count = length(local.yamls)\n  manifest = local.yamls[count.index]\n}\n`\n```\nUnfortunately on my machine this then complains about\n\n'status' attribute key is not allowed in manifest configuration\n\nfor exactly one of the 11 manifests.\nAnd since I have no clue of kubernetes I have no idea what that means or wether or not it needs fixing.\nAlternatively you can always use a `null_resource` with a script that fetches the yaml document and uses bash tools or python or whatever is installed to convert and split and filter the incoming yaml.",
      "question_score": 10,
      "answer_score": 8,
      "created_at": "2021-09-14T17:34:46",
      "url": "https://stackoverflow.com/questions/69180684/how-do-i-apply-a-crd-from-github-to-a-cluster-with-terraform"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 72183481,
      "title": "How to describe an object type variable in Terraform?",
      "problem": "I am trying to write a clear documentation/description of my terraform modules.\nAccording to Hashicorp's doc, the `description` attribute will permit that, but I cannot find a way to describe an `object` type variable in details.\nHere's more or less I want to do :\n```\n`variable \"sa_to_impersonate_info\" {\n  type = object(\n    {\n      id                   = string\n      email                = string\n      belonging_org_id     = string\n      belonging_project_id = string\n      token_scopes         = list(string)\n      token_lifetime       = string\n    }\n  )\n  description = {\n    id : \"an identifier for the resource with format projects/{{project}}/serviceAccounts/{{email}}\"\n    email : \"Email of the service account to impersonate\"\n    belonging_org_id : \"Organization ID where the service account belongs\"\n    belonging_project_id : \"Porject ID where the service account belongs\"\n    token_scopes : \"List of scopes affected by this service account impersonation\"\n    token_lifetime : \"Time the token will be active\"\n  }\n}\n`\n```\nFor this format I get this error when I perform a `terraform plan`:\n```\n` Error: Unsuitable value type\n`\n```",
      "solution": "You can have it in below format with `EOT` delimiter.\n```\n`variable \"sa_to_impersonate_info\" {\n  type = object(\n    {\n      id                   = string\n      email                = string\n      belonging_org_id     = string\n      belonging_project_id = string\n      token_scopes         = list(string)\n      token_lifetime       = string\n    }\n  )\n  description = <<EOT\n    sa_to_impersonate_info = {\n      id : \"an identifier for the resource with format projects/{{project}}/serviceAccounts/{{email}}\"\n      email : \"Email of the service account to impersonate\"\n      belonging_org_id : \"Organization ID where the service account belongs\"\n      belonging_project_id : \"Porject ID where the service account belongs\"\n      token_scopes : \"List of scopes affected by this service account impersonation\"\n      token_lifetime : \"Time the token will be active\"\n    }\n  EOT\n}\n`\n```",
      "question_score": 10,
      "answer_score": 12,
      "created_at": "2022-05-10T11:04:21",
      "url": "https://stackoverflow.com/questions/72183481/how-to-describe-an-object-type-variable-in-terraform"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 67057913,
      "title": "Adding count to terraform module",
      "problem": "I have already deployed my VPC via this module listed below before I added a count.\nThis worked just fine, however do to changes in our infrastructure, I need to add a count to the module\n```\n`module \"core_vpc\" {\n  source = \"./modules/vpc\"\n\n  count          = var.environment == \"qa\" || var.environment == \"production\" ? 1 : 0\n  aws_region     = var.aws_region\n  environment    = var.environment\n  system         = var.system\n  role           = var.system\n  vpc_name       = var.system\n  vpc_cidr       = var.vpc_cidr\n  ssh_key_name   = var.ssh_key_name\n  ssh_key_public = var.ssh_key_public\n  nat_subnets    = var.nat_subnets\n  nat_azs        = var.vpc_subnet_azs\n\n}\n`\n```\nNow Terraform wants to update my state file and destroy much of my configuration and replace it with what is shown in the example below. This is of course not just limited to route association, but all resources created within the module.I can't let this happen as I have production running and not want to mess with that.\n```\n` module.K8_subnets.aws_route_table_association.subnet[0] will be destroyed\n\n`\n```\nand replace it with:\n```\n`module.K8_subnets[0].aws_route_table_association.subnet[0] will be created\n`\n```\nIs there a way of preventing Terraform of making these changes? Short of changing it manually in the tf-state.\nAll I want is the for the VPC not to be deployed in DEV.\nThanks.",
      "solution": "You can \"move\" the terraform state using `tf state mv src target`. Specifically you can move the old non-counter version into the new counted version at index 0:\n```\n`terraform state mv 'module.K8_subnets' 'module.K8_subnets[0]'\n`\n```\nThis works for individual resources as well as for entire modules. And it works for `for_each` resource as well, there you would not have an index but a key to move to. And this even works the other way around, if you remove the `count` / `for_each` but want to still keep the resource(s).",
      "question_score": 10,
      "answer_score": 10,
      "created_at": "2021-04-12T13:56:40",
      "url": "https://stackoverflow.com/questions/67057913/adding-count-to-terraform-module"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 71675828,
      "title": "Terraform not accepting block parameters on &quot;web_app&quot; Module",
      "problem": "I am trying to create a web app through Terraform, the new azurerm provider 3.0 has come out and so the new module azurerm_windows_web_app. The documentation states that the block application_stack supports the following: current_stack, docker, Java, etc.\nazurerm version = \"=3.0.0\"\nLink to documentation of Terraform: https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/windows_web_app#example-usage\nOnce I try to run the module created for this, it throws an error:\nError: Unsupported block type\nBlocks of type \"application_stack\" are not expected here.\n\nHere is a snippet of my code, I am not sure what is going on. Tried to google it but it seems to new to have documentation from other users. Any insight?\n```\n`resource \"azurerm_windows_web_app\" \"web_app_resource\" {\n   name                = var.resource_name\n   resource_group_name = var.resource_group_name\n   location            = var.location\n   service_plan_id     = var.app_service_plan_id\n   https_only = true\n   tags = var.tags\n   count = var.create\n\n   site_config {}\n\n   application_stack {\n      current_stack = var.current_stack\n      dotnet_version = var.dotnet_version\n   }\n\n }\n`\n```",
      "solution": "If we check the resource schema for the  `windows_web_app` resource at version 3.0.2 of the provider, then we see that it is currently missing a definition for the `application_stack` attribute. It also seems to be missing other blocks specified in the documentation.\nYou need to file an issue for your error as this is a bug in the provider.\nUpdate: As this answer is now the victim of sock puppets revisiting, it should be noted `application_stack` is now a nested block in the `site_config` block from its schema. Deleting the answer once un-accepted to avoid further issues.",
      "question_score": 10,
      "answer_score": 1,
      "created_at": "2022-03-30T12:48:47",
      "url": "https://stackoverflow.com/questions/71675828/terraform-not-accepting-block-parameters-on-web-app-module"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 67188052,
      "title": "How to fix LimitExceeded issue in AWS with terraform?",
      "problem": "When I run a terraform apply, got this error:\n```\n`running \"...terraform apply\" in \".../aws-config\": exit status 1\nmodule.aws_config_role.aws_iam_policy.default: Modifying... [id=arn:aws:iam::10391031030:policy/my-aws-config-role]\n\nError: Error updating IAM policy arn:aws:iam::10391031030:policy/my-aws-config-role: LimitExceeded: Cannot exceed quota for PolicySize: 6144\n    status code: 409, request id: 313b0l20-a29d-0121-la32-01210d0103c01\n`\n```\nIn fact this task will update the IAM policy and add many new items, knows from terraform plan:\n```\n`~ update in-place\n\nTerraform will perform the following actions:\n\n  # module.aws_config_role.aws_iam_policy.default will be updated in-place\n~ resource \"aws_iam_policy\" \"default\" {\n        arn    = \"arn:aws:iam::10391031030:policy/my-aws-config-role\"\n        id     = \"arn:aws:iam::10391031030:policy/my-aws-config-role\"\n        name   = \"my-aws-config-role\"\n        path   = \"/\"\n      ~ policy = jsonencode(\n          ~ {\n              ~ Statement = [\n                  ~ {\n                      ~ Action   = [\n                            \"...\" // old\n                            \"...\" // old\n                            \"...\" // old\n                          + \"...\" // new\n                          + \"...\" // new\n                          + \"...\" // new\n                          + \"...\" // new\n                          + \"...\" // new\n                          ...\n\n Plan: 0 to add, 1 to change, 0 to destroy.\n`\n```\nAfter an aws provider version upgrade, got this change. How to avoid it? Ignore this task or is it possible to extend AWS' plicy size?",
      "solution": "The limit of `6144` characters for a managed policy is from AWS, not TF. So you have to fix/workaround it from AWS perspective, not TF.\nYou can follow official AWS recommendations:\n\nHow can I increase the default managed policies or character size limit for an IAM role or user?",
      "question_score": 10,
      "answer_score": 11,
      "created_at": "2021-04-21T03:04:54",
      "url": "https://stackoverflow.com/questions/67188052/how-to-fix-limitexceeded-issue-in-aws-with-terraform"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 65798783,
      "title": "How to get AWS Lambda ARN using Terraform?",
      "problem": "I am trying to define a terraform output block that returns the ARN of a Lambda function. The Lambda is defined in a sub-module. According to the documentation it seems like the lambda should just have an ARN attribute already: https://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/lambda_function#arn\nUsing that as a source I thought I should be able to do the following:\n```\n`output \"lambda_arn\" {\n  value = module.aws_lambda_function.arn\n}\n`\n```\nThis generates the following error:\n```\n`Error: Unsupported attribute\n\n  on main.tf line 19, in output \"lambda_arn\":\n  19:   value = module.aws_lambda_function.arn\n\nThis object does not have an attribute named \"arn\".\n`\n```\nI would appreciate any input, thanks.",
      "solution": "Documentation is correct. Data source `data.aws_lambda_function` has `arn` attribute. However, you are trying to access the `arn` from a custom module `module.aws_lambda_function`. To do this you have to define output `arn` in your module.\nSo in your module you should have something like this:\n```\n`data \"aws_lambda_function\" \"existing\" {\n  function_name = \"function-to-get\"\n}\n\noutput \"arn\" {\n  value = data.aws_lambda_function.existing.arn\n}\n`\n```\nThen if you have your module called `aws_lambda_function`:\n```\n`module \"aws_lambda_function\" {\n   source = \"path-to-module\"   \n}\n`\n```\nyou will be able to access the `arn`:\n```\n`module.aws_lambda_function.arn\n`\n```",
      "question_score": 10,
      "answer_score": 11,
      "created_at": "2021-01-19T21:11:14",
      "url": "https://stackoverflow.com/questions/65798783/how-to-get-aws-lambda-arn-using-terraform"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 65678433,
      "title": "terraform import an existing AWS resource to a module",
      "problem": "I have got an AWS account that already has ECR and IAM ready. I am now creating a new environment using terraform modules. But I could not find a way to import the existing IAM and ECR resources to my modules. When I run the command `terraform import aws_ecr_repository.c2m_an c2m_an`, I am getting an error as\n```\n`Error: resource address \"aws_ecr_repository.c2m_cs\" does not exist in the configuration.\n\nBefore importing this resource, please create its configuration in the root module. For example:\n\nresource \"aws_ecr_repository\" \"c2m_cs\" {\n  # (resource arguments)\n}\n`\n```\nMy ECR module definition is as follows:\n```\n`resource \"aws_ecr_repository\" \"c2m_cs\" {\n  name = var.c2m_cs#\"c2m_cs\"\n}\n\noutput \"c2m_cs\" {\n  value = \"terraform import aws_ecr_repository.c2m_cs c2m_cs\"\n}\n`\n```\nAnd in my `main.tf` file withtin my environment folder, I have module definition as follows:\n```\n`module \"ecr\" {\n  source = \"../modules/ecr\"\n  c2m_cs = module.ecr.c2m_cs\n}\n`\n```",
      "solution": "To correct way to import a resource into the module is exemplified in the docs:\n```\n`terraform import module.foo.aws_instance.bar i-abcd1234\n`\n```\nThus in your case it would be:\n```\n`terraform import module.aws_ecr_repository.c2m_an c2m_an \n`\n```",
      "question_score": 10,
      "answer_score": 10,
      "created_at": "2021-01-12T06:27:31",
      "url": "https://stackoverflow.com/questions/65678433/terraform-import-an-existing-aws-resource-to-a-module"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 67801948,
      "title": "Assign pre-existing static (elastic) IP to EC2 instance",
      "problem": "Assuming I have an existing Elastic IP on my AWS account.\nFor reasons beyond the scope of this question, this EIP is not (and cannot) be managed via Terraform.\nI know want to assign this EIP (say `11.22.33.44`) to an EC2 instance I create via TF\nThe traditional approach would to of course create both the EIP and the EC2 instance via TF\n```\n`resource \"aws_eip\" \"my_instance_eip\" {\n  instance = \"my_instance.id\"\n  vpc      = true\n}\n\nresource \"aws_eip_association\" \"my_eip_association\" {\n  instance_id   = \"my_instance.id\"\n  allocation_id = \"aws_eip.my_instance_eip.id\"\n}\n\n`\n```\nIs there a way however to let EC2 know via TF that it should be assigned as EIP, `11.22.33.44` that is outside of TF lifecycle?",
      "solution": "You can use aws_eip data source to get info of your existing EIP and then use that in your `aws_eip_association`:\n```\n`data \"aws_eip\" \"my_instance_eip\" {\n  public_ip = \"11.22.33.44\"\n}\n\nresource \"aws_eip_association\" \"my_eip_association\" {\n  instance_id   = aws_instance.my_instance.id\n  allocation_id = data.aws_eip.my_instance_eip.id\n}\n\n`\n```",
      "question_score": 10,
      "answer_score": 6,
      "created_at": "2021-06-02T10:53:58",
      "url": "https://stackoverflow.com/questions/67801948/assign-pre-existing-static-elastic-ip-to-ec2-instance"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 73856640,
      "title": "Terraform Lifecycle Ignore changes",
      "problem": "I am trying to apply lifecycle ignore_changes rule against parameter in resource resource \"aws_servicecatalog_provisioned_product\" as shown below.\n```\n`resource \"aws_servicecatalog_provisioned_product\" \"example\" {\n  name                       = \"example\"\n  product_name               = \"Example product\"\n  provisioning_artifact_name = \"Example version\"\n\n  provisioning_parameters {\n    key   = \"foo\"\n    value = \"bar\"\n  }\n\n  provisioning_parameters {\n    key   = \"key2\"\n    value = lookup(var.parameter_group, \"key2\", \"test2\")\n  }\n\n  provisioning_parameters {\n    key   = \"key3\"\n    value = \"test3\"\n  }\n\n  tags = {\n    foo = \"bar\"\n  }\n\n  lifecycle {\n   ignore_changes = [\n    tags[\"foo\"],\n    aws_servicecatalog_provisioned_product.provisioning_parameters.example[\"key2\"]\n   ]\n }\n  \n}\n`\n```\n```\n`variable parameter_group {\n  description = \"Parameters map required for modules. \n  type        = map(any)\n  default     = {}\n}\n`\n```\nwhen i am running the plan i am getting the error below\n```\n`\u2502 Error: Unsupported attribute\n\u2502 \n\u2502   on modules/example_provision/main.tf line 28, in resource \"aws_servicecatalog_provisioned_product\" \"example\":\n\u2502  28:        aws_servicecatalog_provisioned_product.provisioning_parameters.example[\"key2\"]\n\u2502 \n\u2502 This object has no argument, nested block, or exported attribute named \"aws_servicecatalog_provisioned_product\".\n\n`\n```\nI would like to ignore the changes made to this parameter value. The Ignore on tags is working fine but as soon as I add my second line which is\n`aws_servicecatalog_provisioned_product.provisioning_parameters.example[\"key2\"]` the error starts to come in.\nHow do I fix this?",
      "solution": "`ignore_changes` can only ignore changes to the configuration of the same resource where it's declared, and so you only need to name the argument you wish to ignore and not the resource type or resource name:\n```\n`  lifecycle {\n    ignore_changes = [\n      tags[\"foo\"],\n      provisioning_parameters,\n    ]\n  }\n`\n```\nThe `provisioning_parameters` block type doesn't seem to be represented as a mapping (the `provisioning_parameter` blocks don't have labels in their headers) so you won't be able to refer to a specific block by its name.\nHowever, the provider does declare it as being a list of objects and so I think you will be able to ignore a specific item by its index, where the indices are assigned in order of declaration. Therefore in your current example the one with `key = \"key2\"` will have index 1, due to being the second block where Terraform counts up from zero:\n```\n`  lifecycle {\n    ignore_changes = [\n      tags[\"foo\"],\n      provisioning_parameters[1],\n    ]\n  }\n`\n```",
      "question_score": 9,
      "answer_score": 19,
      "created_at": "2022-09-26T17:46:27",
      "url": "https://stackoverflow.com/questions/73856640/terraform-lifecycle-ignore-changes"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 65905029,
      "title": "How to validate a character set in terraform variable?",
      "problem": "i need to validate a variable in terraform. The content of the variable should be only 0-9, a-z and A-Z. Im tried it with following code:\n```\n`variable \"application_name\" {\n    type = string\n    default = \"foo\"\n\n    validation {\n    # regex(...) fails if it cannot find a match\n    condition     = can(regex(\"([0-9A-Za-z])\", var.application_name))\n    error_message = \"For the application_name value only a-z, A-Z and 0-9 are allowed.\"\n  }\n}\n`\n```\nIt doesen't work. When i set abcd- in the variable the validation returns true.\nHow can i fix the regex ?\nThanks for help ;)\n\n@vgersh99 This doesn't work for me:\n```\n`variable \"application_name\" {\n    type = string\n    default = \"foo\"\n\n    validation {\n    # regex(...) fails if it cannot find a match\n    condition     = can(regex(\"[^[:alnum:]]\", var.application_name))\n    error_message = \"For the application_name value only a-z, A-Z and 0-9 are allowed.\"\n  }\n}\n`\n```\nHere is the error:\n```\n`$ terraform validate\nError: Invalid value for variable\n  on main.tf line 23, in module \"ecs_cluster\":\n  23:   application_name = \"frdlso\"\nFor the application_name value only a-z, A-Z and 0-9 are allowed.\nThis was checked by the validation rule at\n.terraform/modules/ecs_cluster/variables.tf:34,5-15\n`\n```",
      "solution": "The `regex` function attempts to match a substring of the given string against the specified pattern, so the pattern in your first example will succeed as long as there is at least one ASCII digit or letter in the input.\nTo implement the rule you described you'll need to expand the pattern to cover the entire string. There are three parts of the regular expression syntax you can use together to achieve that:\n\nThe `^` symbol matches only at the start of the given string.\nThe `$` symbol matches only at the end of the given string.\nThe `+` operator allows the preceding pattern to appear one or more times.\n\nPutting those together, we get the pattern `^[0-9A-Za-z]+$`: the beginning of the string, followed by one or more ASCII letters or digits, followed by the end of the string. This pattern will therefore only succeed if the entire string matches it.\nPutting that into your full example gives us the following:\n```\n`variable \"application_name\" {\n    type = string\n    default = \"foo\"\n\n    validation {\n    # regex(...) fails if it cannot find a match\n    condition     = can(regex(\"^[0-9A-Za-z]+$\", var.application_name))\n    error_message = \"For the application_name value only a-z, A-Z and 0-9 are allowed.\"\n  }\n}\n`\n```",
      "question_score": 9,
      "answer_score": 23,
      "created_at": "2021-01-26T17:16:42",
      "url": "https://stackoverflow.com/questions/65905029/how-to-validate-a-character-set-in-terraform-variable"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 68298964,
      "title": "RDS Proxy: PENDING_PROXY_CAPACITY and &quot;DBProxy Target unavailable due to an internal error&quot;",
      "problem": "When deploying an RDS database via Terraform, my Default target is unavailable.\nRunning the following command:\naws rds describe-db-proxy-targets --db-proxy-name \nI get two errors:\ninitially its in state: PENDING_PROXY_CAPACITY\neventually that times out with the following error: DBProxy Target unavailable due to an internal error",
      "solution": "Following extensive research, a two hour call with AWS support and very few search results for the error: PENDING_PROXY_CAPACITY\nI stumbled across the following discussion: https://github.com/hashicorp/terraform-provider-aws/issues/16379\nI had a couple of issues with my config:\n\nOutbound rules for my RDS proxy security group was limited to internal traffic only. This causes problems as you need public internet access to access AWS Secrets manager!\n\nAt the time of writing the Terraform documentation here suggests you can pass a \"username\" option to the Auth block for the rds_proxy resource (see: https://registry.terraform.io/providers/hashicorp/aws/4.26.0/docs/resources/db_proxy). This does not work, and returns an error stating the username option is not expected. This is because the rds_proxy expects all the information for Auth to be contained in one json object within the secret arn provided. For this reason I created a 2nd secret containing all the auth information like so:\n\n```\n`resource \"aws_secretsmanager_secret_version\" \"lambda_rds_test_proxy_creds\" {\n  secret_id     = aws_secretsmanager_secret.lambda_rds_test_proxy_creds.id\n  secret_string = jsonencode({\n    \"username\"             = aws_db_instance.lambda_rds_test.username\n    \"password\"             = module.lambda_rds_secret.secret\n    \"engine\"               = \"postgres\"\n    \"host\"                 = aws_db_instance.lambda_rds_test.address\n    \"port\"                 = 5432\n    \"dbInstanceIdentifier\" = aws_db_instance.lambda_rds_test.id\n  })\n}\n`\n```\n\nFixing both issues still gave me an Auth error for credentials, this required the IAM permissions fixing (this is discussed in the above github issue). But by creating the new Secret to contain all the info required both the proxy, It no longer had access to the new secret so I updated my IAM role for the newly created resource\n\nI am posting this here as the Github issue is archived and I am unable to update the comments to include some of my search terms to assist those searching for the same issue to come across the issue quicker as there is very little info out there regarding RDS_PROXY errors experienced here.",
      "question_score": 9,
      "answer_score": 18,
      "created_at": "2021-07-08T11:27:43",
      "url": "https://stackoverflow.com/questions/68298964/rds-proxy-pending-proxy-capacity-and-dbproxy-target-unavailable-due-to-an-inte"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 69032013,
      "title": "Yaml Azure Devops TerraformInstaller is ambiguous",
      "problem": "Here i am trying to create aks using terraform, using azure-devops to deploy the resource to azure.\npipeline job has failed within a sec.\nbelow is the pipeline code.\n```\n`trigger:\n- main\n\npool:\n  vmImage: 'ubuntu-latest'\n\nstages:\n- stage: TerraformValidate\n  jobs:\n    - job: TerraformValidateJob\n      continueOnError: false\n      steps:\n      - task: PublishPipelineArtifact@1\n        displayName: Publish Artifacts\n        inputs:\n          targetPath: '$(System.DefaultWorkingDirectory)/terraform-manifests'\n          artifact: 'terraform-manifests-out'\n          publishLocation: 'pipeline'\n      - task: TerraformInstaller@0\n        displayName: Terraform Install\n        inputs:\n          terraformVersion: 'latest'\n      - task: TerraformCLI@0\n        displayName: Terraform Init\n        inputs:\n          command: 'init'\n          workingDirectory: '$(System.DefaultWorkingDirectory)/terraform-manifests'\n          backendType: 'azurerm'\n          backendServiceArm: ''\n          backendAzureRmResourceGroupName: ''\n          backendAzureRmStorageAccountName: ''\n          backendAzureRmContainerName: ''\n          backendAzureRmKey: 'aks-base.tfstate'\n          allowTelemetryCollection: false\n      - task: TerraformCLI@0\n        displayName: Terraform Validate\n        inputs:\n          command: 'validate'\n          workingDirectory: '$(System.DefaultWorkingDirectory)/terraform-manifests'\n          allowTelemetryCollection: false       \n`\n```\ngetting below error :\n\nI have installed both the extensions:",
      "solution": "After installing these two extensions at the same time, I can reproduce the same issue.\n\nThe root cause of the issue is that terraform install task exists in both extensions at the same time.\n\nTheir simplified version of YAML task names are all `TerraformInstaller@0`.\nTo solve this issue, you can uninstall one of the two extensions.\nOr you can  specify the full name.\nFor example:\n```\n`- task: ms-devlabs.custom-terraform-tasks.custom-terraform-installer-task.TerraformInstaller@0\n`\n```\nOR\n```\n`- task: charleszipp.azure-pipelines-tasks-terraform.azure-pipelines-tasks-terraform-installer.TerraformInstaller@0\n`\n```",
      "question_score": 9,
      "answer_score": 21,
      "created_at": "2021-09-02T16:15:34",
      "url": "https://stackoverflow.com/questions/69032013/yaml-azure-devops-terraforminstaller-is-ambiguous"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 70731154,
      "title": "Terraform apply results in Error populating Client ID from the Azure CLI",
      "problem": "I try to apply terraform plan with `terraform apply`. But when I run the command I get the following error\n```\n`Error: Error building AzureRM Client: Error populating Client ID from the Azure CLI: \nNo Authorization Tokens were found - \nplease ensure the Azure CLI is installed and then log-in with `az login`.\n`\n```\nI do have the Azure CLI installed and I'm logged in with `az login`. When I run `az login` I am redirected to the landing page where I am able to log in just fine.\nAlso `terraform init` works without any problems.\nBelow my terraform file:\n```\n`provider \"azurerm\" {\n  version = \"1.38.0\"\n}\n`\n```\nI also tried to provide subscription and tenant IDs but it didn't help:\n```\n`provider \"azurerm\" {\n  version = \"1.38.0\"\n\n  subscription_id = \"00000000-0000-0000-0000-000000000000\"\n  tenant_id       = \"00000000-0000-0000-0000-000000000001\"\n}\n`\n```",
      "solution": "Error: Error building AzureRM Client: Error populating Client ID from\nthe Azure CLI:  No Authorization Tokens were found -  please ensure\nthe Azure CLI is installed and then log-in with `az login`.\n\nThis error is due to the Azure CLI version that you are using . There was a breaking change in the Azure CLI version `2.30.0` where Azure migrated the authentication from `ADAL` to `MSAL`. For which if you are using Azure CLI latest and Terraform azurerm old then , it will fail in authentication which will result in the error.\nTo Fix the problem you will have to use the latest Azure CLI version i.e. `2.32.0` and also at the same time try to use the terraform latest `azurerm` Provider i.e. `2.92.0`.\nTo Upgrade CLI version , You can run `az upgrade` command and in terraform you can use the below :\n```\n`terraform {\n  required_providers {\n    azurerm = {\n      source = \"hashicorp/azurerm\"\n      version = \"2.92.0\"\n    }\n  }\n}\n\nprovider \"azurerm\" {\n  # Configuration options\n}\n`\n```\nYou can refer these Similar Github Issues are well : `Issue 1` and `Issue 2`",
      "question_score": 9,
      "answer_score": 18,
      "created_at": "2022-01-16T15:48:45",
      "url": "https://stackoverflow.com/questions/70731154/terraform-apply-results-in-error-populating-client-id-from-the-azure-cli"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 70789771,
      "title": "Terraform JSON MalformedPolicyDocument: The policy failed legacy parsing",
      "problem": "I am having a hard time resolving this error\n```\n`Error: error creating IAM policy policy-assumerole-test: MalformedPolicyDocument: The policy failed legacy parsing\n    status code: 400, request id: b06e5c24-0b3b-42f3-8580-9e0393434dc1\n  on ../modules/assume/main.tf line 47, in resource \"aws_iam_policy\" \"permit_assume_role\":\n  47: resource \"aws_iam_policy\" \"permit_assume_role\" {\n`\n```\nThe module creates group with the assume policy attached\nThe module is here:\n```\n`terraform {\n  required_providers {\n    template = {\n      source  = \"hashicorp/template\"\n      version = \"2.2.0\"\n    }\n\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \">= 3.72.0\"\n    }\n  }\n\n  required_version = \"~> 0.14\"\n}\n\n## Generate the assume roles policy for this group\ndata \"template_file\" \"policy\" {\n  template = file(\"${path.module}/assets/assume_role.json\")\n\n  vars = {\n    accounts = join(\n      \",\\n\",\n      formatlist(\n        \"\\\"arn:aws:iam::%s:role/%s\\\"\",\n        var.account_id,\n        coalesce(var.role_override, var.role_name),\n      ),\n    )\n  }\n}\n\n## Create an AWS group\nresource \"aws_iam_group\" \"group\" {\n  name = var.group_name\n}\n\n## Add the user membership to the group\nresource \"aws_iam_group_membership\" \"group\" {\n  name  = \"group_membership\"\n  group = aws_iam_group.group.name\n  users = var.users_list\n}\n\n## The IAM policy to allow the central account permission to STS assume role\nresource \"aws_iam_policy\" \"permit_assume_role\" {\n  name        = \"policy-assumerole-${var.group_name}\"\n  description = \"Permit central account users to assume roles in this account\"\n  policy      = data.template_file.policy.rendered\n}\n\n## Assigning the IAM policy to the user group\nresource \"aws_iam_policy_attachment\" \"permit_group_policy\" {\n  name       = \"permit_group_policy\"\n  groups     = [aws_iam_group.group.name]\n  policy_arn = aws_iam_policy.permit_assume_role.arn\n}\n`\n```\nThe assume_role.json template is here:\n```\n`{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": {\n    \"Effect\": \"Allow\",\n    \"Action\": \"sts:AssumeRole\",\n    \"Resource\": [\n        ${accounts}\n    ]\n  }\n}\n`\n```\nThis allows me to call and construct a policy when i call my module\nExample:\n```\n`module \"assume_group\" {\n  source = \"../modules/assume\"\n\n  account_id = [\n    var.accounts[\"account1\"],\n    var.accounts[\"account2\"],\n  ]\n\n  group_name = \"test\"\n  role_name  = \"test\"\n\n  users_list = [\n  ]\n\n  providers = {\n    aws = aws.login\n  }\n}\n`\n```\nThis is throwing me an error which i am struggling to resolve\nVScode is pointing to the template not hav",
      "solution": "I did find out i was missing the \"[\" at the start of the statement and at the end of the statement. This shouldn't matter for a single resource but it was causing me issues.  Adding this resolved my issue\nThanks\n```\n`{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [{\n    \"Effect\": \"Allow\",\n    \"Action\": \"sts:AssumeRole\",\n    \"Resource\": [\n        ${accounts}\n    ]\n  }]\n}\n`\n```",
      "question_score": 9,
      "answer_score": 12,
      "created_at": "2022-01-20T17:20:54",
      "url": "https://stackoverflow.com/questions/70789771/terraform-json-malformedpolicydocument-the-policy-failed-legacy-parsing"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 68358277,
      "title": "Output from module with splat operator not working",
      "problem": "I have defined a module with a loop:\n```\n`module \"stamp\" {\n  for_each = toset(var.stamps)\n  source   = \"./modules/stamp\"\n  ...\n}\n`\n```\nFrom this I am trying to create an output list, based on this example:\n```\n`output \"stamp_locations\" {\n  value = module.stamp.*.location\n}\n`\n```\nHowever, this validates but on `terraform plan` I get the error:\n```\n`\u2502 Error: Unsupported attribute\n\u2502 \n\u2502   on output.tf line 3, in output \"stamp_locations\":\n\u2502    3:   value = module.stamp.*.location\n\u2502 \n\u2502 This object does not have an attribute named \"location\"\n`\n```\nOnly this worked in the end:\n```\n`output \"stamp_locations\" {\n  value = [for instance in module.stamp : instance.location]\n}\n`\n```\nSo I am wondering: Did I make any mistake or is the splat-syntax not supported with modules and loops?",
      "solution": "`module.stamp` is a map, not a list. The following should work with your map:\n```\n`value = values(module.stamp)[*].location \n`\n```\n`values` will return a list of values from your `module.stamp`.",
      "question_score": 9,
      "answer_score": 13,
      "created_at": "2021-07-13T09:33:56",
      "url": "https://stackoverflow.com/questions/68358277/output-from-module-with-splat-operator-not-working"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 66019421,
      "title": "Error: Unsupported attribute. This object does not have an attribute named &quot;nsg_name&quot;",
      "problem": "NSG gets created fine, so then I create and enter all the config for the nsg rules in my env/dev and modules folders\n\nI run terraform plan, this is the error I am getting:\n```\n`Error: Unsupported attribute\n\n  on nsg_rules.tf line 6, in module \"nsgrules_app1\":\n\n   6:   nsg_name                  = module.nsg_app1.nsg_name\n\nThis object does not have an attribute named \"nsg_name\".\n`\n```\n\nI know my code is incorrect, i'm just not sure how to write up the nsg_rule module by using map variables and then attach it to the NSG in my other module.\nAny assistance would be appreciated :)\n\nmy terraform relevant folder structure is:\n```\n`dev\n    |_ backend.tf\n    |_ outputs.tf\n    |_ provider.tf\n    |_ resource_groups.tf\n    |_ nsg.tf\n    |_ nsg_rules.tf\n    |_ storage.tf\n    |_ subnets.tf\n    |_ variables.tf\n    |_ vnets.tf\n    |_ vms_lin.tf\n\nmodules\n|_ nsg\n          |_ outputs.tf\n          |_ variables.tf\n          |_ main.tf\n\n|_ nsg_rules\n          |_ outputs.tf\n          |_ variables.tf\n          |_ main.tf\n\n|_ resource_group\n          |_ outputs.tf\n          |_ variables.tf\n          |_ main.tf\n|_ storage\n          |_ outputs.tf\n          |_ variables.tf\n          |_ main.tf\n|_ network\n          |_ vnet\n                 |_ outputs.tf\n                 |_ variables.tf\n                 |_ main.tf\n          |_ subnet\n                 |_ outputs.tf\n                 |_ variables.tf\n                 |_ main.tf\n`\n```\ndev/nsg.tf\n```\n`module \"nsg_app1\" {\n  source                    = \"git::ssh://git@ssh.dev.azure.com/v3/myorg/my_code/terraform_modules//nsg\"\n  nsg_name                  = \"nsg-ansible\"\n  rg_name                   = module.rg_app1.rg_name\n  location                  = module.rg_app1.rg_location\n}\n`\n```\ndev/nsg_rules.tf\n```\n`module \"nsgrules_app1\" {\n  source                    = \"git::ssh://git@ssh.dev.azure.com/v3/myorg/my_code/terraform_modules//nsg_rule\"\n  rg_name                   = module.rg_app1.rg_name\n  nsg_name                  = module.nsg_app1.nsg_name\n  # rules_map                 = var.rules_map     \n  # rules_map = {\n  #   http_inbound  = { priority = 150, direction = \"Inbound\", access = \"Allow\", protocol = \"TCP\", destination_port_range = \"80\" },\n  #   https_inbound = { priority = 151, direction = \"Inbound\", access = \"Allow\", protocol = \"TCP\", destination_port_range = \"443\" }\n  # }\n}\n`\n```\nmodules/nsg/main.tf\n```\n`resource \"azurerm_network_security_group\" \"nsg\" {\n  name                = var.nsg_name\n  location            = var.location\n  resource_group_name = var.rg_name\n}\n`\n```\nmodules/nsg/variables.tf\n```\n`variable \"rg_name\" {\n  description = \"name of resource group\"\n}\n\nvariable \"location\" {\n  description = \"location of resource group\"\n}\n\nvariable \"nsg_name\" {\n  description = \"name of nsg group\"\n}\n`\n```\nmodules/nsg_rule/main.tf\n```\n`resource \"azurerm_network_security_rule\" \"nsg-rule-rdp\" {\n    \n  name                        = \"RDP\"\n  priority                    = \"105\"\n  direction                   = \"Inbound\"\n  access                      = \"Allow\"\n  protocol                    = \"TCP\"\n  source_port_range           = \"*\"\n  destination_port_range      = \"3389\"\n  source_address_prefixes     = var.default_ip_whitelist\n  destination_address_prefix  = \"*\"\n  resource_group_name         = var.rg_name\n  network_security_group_name = var.nsg_name\n}\n`\n```\nmodules/nsg_rule/variables.tf\n```\n`variable \"rg_name\" {\n  description = \"name of resource group\"\n}\n\nvariable \"default_ip_whitelist\" {\n  description = \"List of IPs to whitelist on all RDP | SSH enabled NSG rules.\"\n  default     = []\n}\n\nvariable \"nsg_name\" {\n  description = \"name of nsg group\"\n}\n\nvariable \"rules_map\" {\n  type    = map\n  default = {\n        rule1 = {priority = 105, direction = \"Inbound\", access = \"Allow\", protocol = \"TCP\", source_port_range = \"*\", destination_port_range = \"*\",source_address_prefix = \"*\", destination_address_prefix = \"*\"  } ,\n        rule2 = {priority = 105, direction = \"Outbound\", access = \"Deny\", protocol = \"TCP\", source_port_range = \"*\", destination_port_range = \"*\",source_address_prefix = \"*\", destination_address_prefix = \"*\"  }    \n    }\n\n}\n`\n```",
      "solution": "The module that you are using `module.nsg_app1` does not have `nsg_name` attribute. This means that it does not output such a variable in its.\nEither you have to modify `module.nsg_app1` module to output such variable, or in `module.nsgrules_app1` hard-code the name:\n```\n`module \"nsgrules_app1\" {\n  source                    = \"git::ssh://git@ssh.dev.azure.com/v3/myorg/my_code/terraform_modules//nsg_rule\"\n  rg_name                   = module.rg_app1.rg_name\n  nsg_name                  = \"nsg-ansible\"\n  # rules_map                 = var.rules_map     \n  # rules_map = {\n  #   http_inbound  = { priority = 150, direction = \"Inbound\", access = \"Allow\", protocol = \"TCP\", destination_port_range = \"80\" },\n  #   https_inbound = { priority = 151, direction = \"Inbound\", access = \"Allow\", protocol = \"TCP\", destination_port_range = \"443\" }\n  # }\n}\n`\n```",
      "question_score": 9,
      "answer_score": 12,
      "created_at": "2021-02-03T01:08:20",
      "url": "https://stackoverflow.com/questions/66019421/error-unsupported-attribute-this-object-does-not-have-an-attribute-named-nsg"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 68577207,
      "title": "Terraform - Dynamic block to loop through list of objects",
      "problem": "I'm trying to have this Terraform configured so that all someone needs to do to add a new SG ingress rule is to add a block to the `.tfvars` file, and the new ingress rule will be added to the single SG.\nI have a `.tfvars` file that looks like this:\n```\n`rules = [\n  {\n    protocol = \"tcp\"\n    port = 22\n    cidr = [\"10.0.0.0/8\"],\n  },\n  {\n    protocol = \"tcp\"\n    port = 80\n    cidr = [\"10.0.0.0/8\"]\n  },\n  {\n    protocol = \"tcp\"\n    port = 443\n    cidr = [\"10.0.0.0/8\"]\n  },\n  {\n    protocol = \"icmp\"\n    port = -1\n    cidr = [\"10.0.0.0/8\"]\n  }\n]\n`\n```\nAnd my Terraform is:\n```\n`variable \"rules\" {\n  type = list(object({\n    protocol = string\n    port = number\n    cidr = list(string)\n  }))\n  description = \"Specify the protocol, port range, and CIDRs.\"\n}\n\nresource \"aws_security_group\" \"this\" {\n...\n\ndynamic \"ingress\" {\n  for_each = { for rule in var.rules : rule.id => rule }\n\n  content {\n    from_port   = var.rules.port\n    to_port     = var.rules.port\n    protocol    = var.rules.protocol\n    cidr_blocks = var.rules.cidr\n  }\n}\n\n...\n`\n```\nI've tried to loop through the above, and also with using `for_each = var.rules` but I just get the following error (occurs for each of the below):\n```\n`protocol     = var.rules.protocol\nfrom_port    = var.rules.port\nto_port      = var.rules.port\ncidr_blocks  = var.rules.cidr\n-----------------------------\nvar.rules is list of object with 4 elements. \nThis value does not have any attributes.`\n`\n```\nI'm a bit stuck and not sure what else to try, so does anyone have any ideas?",
      "solution": "There are a couple of issues here. The first is in your `for_each` meta-argument value `for` expression lambda, you are attempting to access a non-existent object key of `id`. Also, you can easily just use a `list(object)` here instead of a `map(object)` like you are trying to do in your transformation, and then your input variable can be used directly. We can discard those and fix the value accordingly:\n```\n`for_each = var.rules\n`\n```\nThe second issue is that your temporary lambda iterator variable in a dynamic block is the name of the block itself. In this case, your block is named `ingress`. Therefore, we need to access values from its object keys:\n```\n`content {\n  from_port   = ingress.value.port\n  to_port     = ingress.value.port\n  protocol    = ingress.value.protocol\n  cidr_blocks = ingress.value.cidr\n}\n`\n```\nCombining these fixes, we arrive at:\n```\n`dynamic \"ingress\" {\n  for_each = var.rules\n\n  content {\n    from_port   = ingress.value.port\n    to_port     = ingress.value.port\n    protocol    = ingress.value.protocol\n    cidr_blocks = ingress.value.cidr\n  }\n}\n`\n```",
      "question_score": 9,
      "answer_score": 10,
      "created_at": "2021-07-29T15:52:02",
      "url": "https://stackoverflow.com/questions/68577207/terraform-dynamic-block-to-loop-through-list-of-objects"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 69639973,
      "title": "Getting Insufficient privileges to complete the operation error while creating service principal from terraform",
      "problem": "I want to create service principal with terraform and have written terraform script for that. I have Azure DevOps pipelone in which I ma running this pipeline. Service principal which I am using to run the terraform script has owner access on subscription. I am getting below error while creating azure ad application\n```\n`\u2502 \n\u2502   with module.appregister.azuread_application.auth,\n\u2502   on modules/appregister/main.tf line 6, in resource \"azuread_application\" \"auth\":\n\u2502    6: resource \"azuread_application\" \"auth\" {\n\u2502 \n\u2502 ApplicationsClient.BaseClient.Post(): unexpected status 403 with OData\n\u2502 error: Authorization_RequestDenied: Insufficient privileges to complete the\n\u2502 operation.\n\u2575\n##[error]Error: The process '/agent/_work/_tool/terraform/1.0.3/x64/terraform' failed with exit code\n`\n```\nWhat sort of Permissions are required to run this?",
      "solution": "Considering Service Principals are created in Azure AD, the Service Principal used to run your Terraform script needs to have proper permission in Azure AD and not in Azure Subscription.\nAt the very least, I believe your Service Principal should be either in `Application Administrator` or `Application Developer`. For a list of complete Azure AD built-in roles, please see this link: https://learn.microsoft.com/en-us/azure/active-directory/roles/permissions-reference.",
      "question_score": 9,
      "answer_score": 8,
      "created_at": "2021-10-20T05:26:24",
      "url": "https://stackoverflow.com/questions/69639973/getting-insufficient-privileges-to-complete-the-operation-error-while-creating-s"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 67601244,
      "title": "Terraform: ignore changes to a certain environment variable",
      "problem": "I have an AWS Lambda function I created using terraform. Code-changes are auto-deployed from our CI-server and the commit-sha is passed as an environment variable (`GIT_COMMIT_HASH`) - so this changes the Lambda function outside of the Terraform-scope (because people were asking...).\nThis works good so far. But now I wanted to update the function's node-version and terraform tries to reset the env var to the initial value of `\"unknown\"`.\nI tried to use the `ignore_changes` block but couldn't get terraform to ignore the changes made elsewhere ...\n```\n`resource \"aws_lambda_function\" \"test\" {\n  filename      = data.archive_file.helloworld.output_path\n  function_name = \"TestName_${var.environment}\"\n  role          = aws_iam_role.test.arn\n  handler       = \"src/index.handler\"\n\n  runtime = \"nodejs14.x\"\n  timeout = 1\n  memory_size = 128\n\n  environment {\n    variables = {\n      GIT_COMMIT_HASH = \"unknown\"\n    }\n  }\n\n  lifecycle {\n    ignore_changes = [\n      environment.0.variables[\"GIT_COMMIT_HASH\"],\n    ]\n  }\n}\n`\n```\nIs this possible? How do I have to reference the variable?\n** edit **\nPlan output looks like this:\n```\n`# aws_lambda_function.test will be updated in-place\n  ~ resource \"aws_lambda_function\" \"test\" {\n        # ... removed some lines\n\n        source_code_size               = 48012865\n        tags                           = {}\n        timeout                        = 1\n        version                        = \"12\"\n\n      ~ environment {\n          ~ variables = {\n              ~ \"GIT_COMMIT_HASH\" = \"b7a77d0\" -> \"unknown\"\n            }\n        }\n\n        tracing_config {\n            mode = \"PassThrough\"\n        }\n    }\n`\n```",
      "solution": "I tried to replicate the issue and in my tests it works exactly as expected. I can only suspect that you are using an old version of TF, where this issue occurs. There has been numerous GitHub Issues reported regarding the limitations of `ignore_changes`. For example, here, here or here.\nI performed tests using `Terraform v0.15.3` with `aws v3.31.0`, and I can confirm that `ignore_changes` works as it should. Since this is a TF internal problem, the only way to rectify the problem, to the best of my knowledge, would be to upgrade your TF.",
      "question_score": 9,
      "answer_score": 5,
      "created_at": "2021-05-19T12:12:14",
      "url": "https://stackoverflow.com/questions/67601244/terraform-ignore-changes-to-a-certain-environment-variable"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 71307150,
      "title": "Getting `Argument list too long` in GitHub Actions",
      "problem": "I am following HashiCorp's learning guide on how to set up GitHub Actions and terraform. All is running great besides the step to update the PR with the Terraform Plan.\nI am hitting the following error:\n```\n`\nAn error occurred trying to start process '/home/runner/runners/2.287.1/externals/node12/bin/node' with working directory '/home/runner/work/ccoe-aws-ou-scp-manage/ccoe-aws-ou-scp-manage'. Argument list too long\n\n`\n```\nThe code I am using is:\n```\n`    - uses: actions/github-script@0.9.0\n      if: github.event_name == 'pull_request'\n      env:\n        PLAN: \"terraform\\n${{ steps.plan.outputs.stdout }}\"\n      with:\n        github-token: ${{ secrets.GITHUB_TOKEN }}\n        script: |\n          const output = `#### Terraform Format and Style \ud83d\udd8c\\`${{ steps.fmt.outcome }}\\`\n          #### Terraform Initialization \u2699\ufe0f\\`${{ steps.init.outcome }}\\`\n          #### Terraform Plan \ud83d\udcd6\\`${{ steps.plan.outcome }}\\`\n          Show Plan\n          \\`\\`\\`${process.env.PLAN}\\`\\`\\`\n          \n          *Pusher: @${{ github.actor }}, Action: \\`${{ github.event_name }}\\`*`;\n            \n          github.issues.createComment({\n            issue_number: context.issue.number,\n            owner: context.repo.owner,\n            repo: context.repo.repo,\n            body: output\n          })\n`\n```\nA clear COPY/Paste from the docs: https://learn.hashicorp.com/tutorials/terraform/github-actions\nI have tried with\nactions/github-script version 5 and 6 and still the same problem, But when I copy paste the plan all is great. If I do not use the output variable and use some place holder text for the body all is working great. I can see that the step.plan.outputs.stdout is Ok if I print only that.\nI will be happy to share more details if needed.",
      "solution": "I also encountered a similar issue.\nI seem `github-script` can't give to argument for too long script.\nreference:\n\nhttps://github.com/robburger/terraform-pr-commenter/issues/6#issuecomment-826966670\nhttps://github.community/t/maximum-length-for-the-comment-body-in-issues-and-pr/148867\n\nmy answer:\n```\n`      - name: truncate terraform plan result\n        run: |\n          plan=$(cat > $GITHUB_ENV\n          echo \"EOF\" >> $GITHUB_ENV\n\n      - name: create comment from plan result\n        uses: actions/github-script@0.9.0\n        if: github.event_name == 'pull_request'\n        with:\n          github-token: ${{ secrets.GITHUB_TOKEN }}\n          script: |\n            const output = `#### Terraform Initialization \u2699\ufe0f\\`${{ steps.init.outcome }}\\`\n            #### Terraform Plan \ud83d\udcd6\\`${{ steps.plan.outcome }}\\`\n            \n            Show Plan\n            \n            \\`\\`\\`\\n\n            ${ process.env.PLAN }\n            \\`\\`\\`\n            \n            \n            \n            *Pusher: @${{ github.actor }}, Action: \\`${{ github.event_name }}\\`, Working Directory: \\`${{ inputs.TF_WORK_DIR }}\\`, Workflow: \\`${{ github.workflow }}\\`*`;\n\n            github.issues.createComment({\n              issue_number: context.issue.number,\n              owner: context.repo.owner,\n              repo: context.repo.repo,\n              body: output\n            })```\n`\n```",
      "question_score": 9,
      "answer_score": 3,
      "created_at": "2022-03-01T11:37:09",
      "url": "https://stackoverflow.com/questions/71307150/getting-argument-list-too-long-in-github-actions"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 69473114,
      "title": "Why is aws lambda getting &quot;AccessDeniedExceptionKMS&quot; Error Message?",
      "problem": "I just deployed a lambda (using Terraform from gitlab runner) to a new aws account.  This pipeline deploys a lambda to another (dev/test) account without issues, but when I try to deploy to my prod account, I get the following error:\n\nI'm honing in on the statement, \"The ciphertext refers to a customer master key that does not exist, does not exist in this region, or you are not allowed to access.\"\nI have confirmed that the encryption config for the env vars are set to use default aws/lambda key instead of a customer master key.  That seems to contradict the language of the error which refers to a customer master key...?\nThe role assumed by the lambda does have a policy which includes two kms actions:\n```\n`        \"Sid\": \"AWSKeyManagementService\",\n\n        \"Action\": [\n\n            \"kms:Decrypt\",\n\n            \"kms:DescribeKey\"\n\n        ]\n`\n```\nBy process of elimination, I wonder if the issue is a lack on the part of the resource-based policy on the kms key.  Looking in the kms keys, under aws managed, I find the aws/lambda key has the following key policy:\n```\n`{\n    \"Version\": \"2012-10-17\",\n    \"Id\": \"auto-awslambda\",\n    \"Statement\": [\n        {\n            \"Sid\": \"Allow access through AWS Lambda for all principals in the account that are authorized to use AWS Lambda\",\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"*\"\n            },\n            \"Action\": [\n                \"kms:Encrypt\",\n                \"kms:Decrypt\",\n                \"kms:ReEncrypt*\",\n                \"kms:GenerateDataKey*\",\n                \"kms:CreateGrant\",\n                \"kms:DescribeKey\"\n            ],\n            \"Resource\": \"*\",\n            \"Condition\": {\n                \"StringEquals\": {\n                    \"kms:ViaService\": \"lambda.us-east-1.amazonaws.com\",\n                    \"kms:CallerAccount\": \"REDACTED\"#This is very puzzling.  Any pointers appreciated!",
      "solution": "This was solved by simply deleting the lambda and then re-running my pipeline to re-deploy it.  All I can conclude is that something was corrupted in the first deployment.",
      "question_score": 8,
      "answer_score": 16,
      "created_at": "2021-10-06T23:55:59",
      "url": "https://stackoverflow.com/questions/69473114/why-is-aws-lambda-getting-accessdeniedexceptionkms-error-message"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 65680044,
      "title": "Invalid value for &quot;path&quot; parameter: no file exists at",
      "problem": "As mentioned at Terraform Resource: Connection Error while executing apply\nI changed my code to the the below\n```\n`provisioner \"remote-exec\" {\n    connection {\n      type        = \"ssh\"\n      host        = aws_eip.nat-eip.public_ip\n      user        = \"ubuntu\"\n      private_key = file(\"/id_rsa.pem\")\n    }\n    inline = [\n      \"chmod +x /tmp/start_node.sh\",\n      \"sudo sed -i -e 's/\\r$//' /tmp/start_node.sh\", # Remove the spurious CR characters.\n      \"sudo /tmp/start_node.sh\",\n    ]\n  }\n`\n```\nBut I still get the same error\n```\n`Error: Invalid function argument\n\n  on explorer.tf line 60, in resource \"aws_instance\" \"explorer\":\n  60:       private_key = file(\"/id_rsa.pem\")\n\nInvalid value for \"path\" parameter: no file exists at /id_rsa.pem;\n\nthis function works only with files that are distributed as part of the\nconfiguration source code, so if this file will be created by a resource in\nthis configuration you must instead obtain this result from an attribute of\nthat resource.\n`\n```\n`ls -la` ooutput\n```\n`total 156\ndrwxr-xr-x  10 CORP\\mayuresh CORP\\domain users  4096 Jan 12 14:29 .\ndrwxr-xr-x  16 CORP\\mayuresh CORP\\domain users  4096 Jan 10 13:10 ..\ndrwxr-xr-x  12 CORP\\mayuresh CORP\\domain users  4096 Jan 12 09:49 byoc-terraform\ndrwxr-xr-x   2 CORP\\mayuresh CORP\\domain users  4096 Jan 11 11:57 controllers\n-rw-r--r--   1 CORP\\mayuresh CORP\\domain users   188 Jan 10 13:27 .env\n-rw-r--r--   1 CORP\\mayuresh CORP\\domain users  1582 Jan 10 17:12 fetchUserData.js\ndrwxr-xr-x   9 CORP\\mayuresh CORP\\domain users  4096 Jan 12 13:14 .git\n-rw-r--r--   1 CORP\\mayuresh CORP\\domain users   629 Jan 10 13:27 .gitignore\n-rw-r--r--   1 CORP\\mayuresh CORP\\domain users   107 Dec 30 06:49 .gitmodules\n-rw-r--r--   1 CORP\\mayuresh CORP\\domain users  1765 Jan 12 13:21 id_rsa.pem\n-rw-r--r--   1 CORP\\mayuresh CORP\\domain users  1488 Jan 10 13:27 index.js\ndrwxr-xr-x   3 CORP\\mayuresh CORP\\domain users  4096 Jan 10 13:27 models\ndrwxr-xr-x 221 CORP\\mayuresh CORP\\domain users 12288 Jan 10 13:30 node_modules\n-rw-r--r--   1 CORP\\mayuresh CORP\\domain users  1058 Jan 10 13:27 package.json\n-rw-r--r--   1 CORP\\mayuresh CORP\\domain users 78791 Jan 10 13:27 package-lock.json\ndrwxr-xr-x   2 CORP\\mayuresh CORP\\domain users  4096 Jan 10 13:27 routes\ndrwxr-xr-x   2 CORP\\mayuresh CORP\\domain users  4096 Jan 10 17:01 utils\ndrwxr-xr-x   2 CORP\\mayuresh CORP\\domain users  4096 Jan 10 13:27 VMCreationFiles```\n`\n```",
      "solution": "Your path to the `.pem` is wrong. It looks like the file exists in your $HOME directory.\nYou can provide the absolute path of the `id_rsa.pem` file if that file is outside of `path.module, path.root, path.cwd`\nTo provide the absolute path\n\nFetch the full path of the file How to get full path of a file?\nPaste the path in:\n```\n`provisioner \"remote-exec\" {\n connection {\n   type        = \"ssh\"\n   host        = aws_eip.nat-eip.public_ip\n   user        = \"ubuntu\"\n   private_key = file(\"\")\n }\n`\n```",
      "question_score": 8,
      "answer_score": 1,
      "created_at": "2021-01-12T09:06:27",
      "url": "https://stackoverflow.com/questions/65680044/invalid-value-for-path-parameter-no-file-exists-at"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 67690158,
      "title": "Syntax for conditions in aws_iam_policy resources in terraform",
      "problem": "I am currently trying to implement an aws_iam_policy in terraform that looks like:\n```\n`resource \"aws_iam_policy\" \"policyName\" {\n  name        = \"policyName\"\n  path        = \"/\"\n \n  policy = jsonencode({\n    Version = \"2012-10-17\"\n    Statement = [\n      {\n        Effect = \"Allow\"\n        NotAction = [\n          \"iam:*\"\n        ]\n        Resource = \"*\"\n \n        Condition = {\n          test = \"StringEquals\"\n          variable = \"s3:prefix\"\n          values = [\n            \"home/\"\n          ]\n        }\n      }\n    ]\n  })\n}\n`\n```\nHowever, when I try to deploy this in an environment I'm using within a team, I get an error saying there are syntax errors with this (the logs only point out the starting line and nowhere else). When I take out the Condition block the error disappears, so I know it's something to do with the condition. I have checked the terraform documentation and they do not have an `=` after the condition, but when I remove the `=` I get an error saying the `=` is expected. Would anyone know of the correct syntax/the right place to look for documentation regarding this (as stated previously the documentation at https://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/iam_policy_document doesn't work for me)?",
      "solution": "I would try an `aws_iam_policy_document` data block, like the following example:\n```\n`data \"aws_iam_policy_document\" \"example\" {\n\n  statement {\n\n    not_actions = [\n      \"iam:*\",\n    ]\n\n    effect = \"Allow\"\n\n    resources = [\n      \"*\",\n    ]\n\n    condition {\n      test     = \"StringEquals\"\n      variable = \"s3:prefix\"\n      values = [\n        \"home/\"\n      ]\n    }\n  }\n}\n`\n```\nAnd then add a `aws_iam_policy` resource that references this data source:\n```\n`resource \"aws_iam_policy\" \"policyName\" {\n  name   = \"policyName\"\n  path   = \"/\"\n  policy = data.aws_iam_policy_document.example.json\n}\n`\n```\nIn my experience, this pattern has yielded the best validation when provisioning IAM policies.",
      "question_score": 8,
      "answer_score": 8,
      "created_at": "2021-05-25T16:35:01",
      "url": "https://stackoverflow.com/questions/67690158/syntax-for-conditions-in-aws-iam-policy-resources-in-terraform"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 74634860,
      "title": "MissingSecurityHeader error for S3 bucket ACL",
      "problem": "I have the following s3 bucket defined:\n```\n`module \"bucket\" {\n  source  = \"terraform-aws-modules/s3-bucket/aws\"\n  version = \"3.1.0\"\n\n  bucket = local.test-bucket-name\n  acl    = null\n\n  grant = [{\n    type       = \"CanonicalUser\"\n    permission = \"FULL_CONTROL\"\n    id         =  data.aws_canonical_user_id.current.id\n    }, {\n    type       = \"CanonicalUser\"\n    permission = \"FULL_CONTROL\"\n    id         = data.aws_cloudfront_log_delivery_canonical_user_id.cloudfront.id\n    }\n  ]\n  object_ownership = \"BucketOwnerPreferred\"\n}\n`\n```\nBut when I try to `terraform apply` this, I get the error:\n\nError: error updating S3 bucket ACL (logs,private): MissingSecurityHeader: Your request was missing a required header status code: 400\n\nThis error message is not very specific. Am I missing some type of header?",
      "solution": "I hit this when updating the AWS provider from 4.x to 5.3.0 where some buckets previously had an ACL of `private` and it was wanting to set them to `null`, like the previous answer.\nHowever, the issue for me was just transitive - running `terraform apply` a second time came back `No changes. Your infrastructure matches the configuration`. I didn't need to modify any ACLs manually.\nA bug report has been opened with the AWS provider to avoid the need to do `terraform apply` a second time. See here: https://github.com/hashicorp/terraform-provider-aws/issues/31633",
      "question_score": 8,
      "answer_score": 16,
      "created_at": "2022-11-30T23:22:16",
      "url": "https://stackoverflow.com/questions/74634860/missingsecurityheader-error-for-s3-bucket-acl"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 69412371,
      "title": "How to use Terraform to disable deletion protection on AWS RDS?",
      "problem": "I used Terraform to bring up an AWS RDS SQL Server DB with deletion_protection set to true. Now, I am trying to delete the database and hence I tried to first run {terraform apply} with deletion_protection set to false, and I got the following error:\n```\n`Error: error deleting Database Instance \"awsworkerdb-green\": InvalidParameterCombination: Cannot delete protected DB Instance, please disable deletion protection and try again.\n    status code: 400, request id: 7e787deb-af03-4016-9baa-471ab9c0ae1c\n`\n```\nThen I tried to directly do {terraform destroy} using the same TF code with deletion_protection set to false, I got the following error:\n```\n`Error: error deleting Database Instance \"awsworkerdb-green\": InvalidParameterCombination: Cannot delete protected DB Instance, please disable deletion protection and try again.\n    status code: 400, request id: 9a95ef70-8738-4a31-b0cd-cf10ef05bdec\n`\n```\nHow does one go about deleting this database instance using terraform?",
      "solution": "This would be two distinct API invocations, and therefore two consecutive Terraform executions with two different config modifications:\n\nModify `deletion_protection` to be `false` in your config, and `apply` your changes to the RDS instance.\nRemove the RDS from the config and `apply`, or `destroy` the RDS resource directly. Either action will delete the RDS instance.",
      "question_score": 8,
      "answer_score": 13,
      "created_at": "2021-10-02T00:04:10",
      "url": "https://stackoverflow.com/questions/69412371/how-to-use-terraform-to-disable-deletion-protection-on-aws-rds"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 69243571,
      "title": "How can I connect GitHub actions with AWS deployments without using a secret key?",
      "problem": "I'd like to be able to use GitHub Actions to be able to deploy resources with AWS, but without using a hard-coded user.\nI know that it's possible to create an IAM user with a fixed credential, and that can be exported to GitHub Secrets, but this means if the key ever leaks I have a large problem on my hands, and rotating such keys are challenging if forgotten.\nIs there any way that I can enable a password-less authentication flow for deploying code to AWS?",
      "solution": "Yes, it is possible now that GitHub have released their Open ID Connector for use with GitHub Actions. You can configure the Open ID Connector as an Identity Provider in AWS, and then use that for an access point to any role(s) that you wish to enable. You can then configure the action to use the credentials acquired for the duration of the job, and when the job is complete, the credentials are automatically revoked.\nTo set this up in AWS, you need to create an Open Identity Connect Provider using the instructions at AWS or using a Terraform file similar to the following:\n```\n`resource \"aws_iam_openid_connect_provider\" \"github\" {\n  url = \"https://token.actions.githubusercontent.com\"\n  client_id_list = [\n    // original value \"sigstore\",\n    \"sts.amazonaws.com\", // Used by aws-actions/configure-aws-credentials\n  ]\n  thumbprint_list = [\n    // original value \"a031c46782e6e6c662c2c87c76da9aa62ccabd8e\",\n    \"6938fd4d98bab03faadb97b34396831e3780aea1\",\n  ]\n}\n`\n```\nThe client id list is the 'audience' which is used to access this content -- you can vary that, provided that you vary it in all the right places. The thumbprint is a hash/certificate of the Open ID Connector, and `6938...aea1` is the current one used by GitHub Actions -- you can calculate/verify the value by following AWS' instructions. The `thumbprint_list` can hold up to 5 values, so newer versions can be appended when they are being made available ahead of time while continuing to use older ones.\nIf you're interested in where this magic value came from, you can find out at How can I calculate the thumbprint of an OpenID Connect server?\nOnce you have enabled the identity provider, you can use it to create one or more custom roles (replacing  with :\n```\n`data \"aws_caller_identity\" \"current\" {}\nresource \"aws_iam_role\" \"github_alblue\" {\n  name = \"GitHubAlBlue\"\n  assume_role_policy = jsonencode({\n    Version = \"2012-10-17\",\n    Statement = [{\n      Action = \"sts:AssumeRoleWithWebIdentity\"\n      Effect = \"Allow\"\n      Principal = {\n        Federated = \"arn:aws:iam::${data.aws_caller_identity.current.account_id}:oidc-provider/token.actions.githubusercontent.com\"\n      }\n      Condition = {\n        StringLike = {\n          \"token.actions.githubusercontent.com:aud\" :  [\"sts.amazonaws.com\" ],\n          \"token.actions.githubusercontent.com:sub\" : \"repo:alblue/*\"\n        }\n      }\n    }]\n  })\n}\n`\n```\nYou can create as many different roles as you need, and even split them up by audience (e.g. 'production', 'dev'). Provided that the OpenID connector's audience is trusted by the account then you're good to go. (You can use this to ensure that the OpenID Connector in a Dev account doesn't trust the roles in a production account and vice-versa.) You can have, for example, a read-only role for performing `terraform validate` and then another role for `terraform apply`.\nThe subject is passed from GitHub, but looks like:\n```\n`repo:/:ref:refs/heads/\n`\n```\nThere may be different formats that come out later. You could have an action/role specifically for PRs if you use `:ref:refs/pulls/*` for example, and have another role for `:ref:refs/heads/production/*`.\nThe final step is getting your GitHub Actions configured to use the token that comes back from the AWS/OpenID Connect:\nStandard Way\n```\n`jobs:\n  terraform-validate:\n    runs-on: ubuntu-latest\n    permissions:\n      id-token: write\n      contents: read\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v2\n      - name: Configure AWS credentials from Test account\n        uses: aws-actions/configure-aws-credentials@master\n        with:\n          role-to-assume: arn:aws:iam:::role/GitHubAlBlue\n          aws-region: us-east-1\n      - name: Display Identity\n        run: aws sts get-caller-identity\n`\n```\nManual Way\nWhat's actually happening under the covers is something like this:\n```\n`jobs:\n  terraform-validate:\n    runs-on: ubuntu-latest\n    env:\n      AWS_WEB_IDENTITY_TOKEN_FILE: .git/aws.web.identity.token.file\n      AWS_DEFAULT_REGION: eu-west-2\n      AWS_ROLE_ARN: arn:aws:iam:::role/GitHubAlBlue\n    permissions:\n      id-token: write\n      contents: read\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v2\n      - name: Configure AWS\n        run: |\n          sleep 3 # Need to have a delay to acquire this\n          curl -H \"Authorization: bearer $ACTIONS_ID_TOKEN_REQUEST_TOKEN\" \\\n            \"$ACTIONS_ID_TOKEN_REQUEST_URL&audience=sts.amazonaws.com\" \\\n            | jq -r '.value' > $AWS_WEB_IDENTITY_TOKEN_FILE\n          aws sts get-caller-identity\n`\n```\nYou need to ensure that your `AWS_ROLE_ARN` is the same as defined in your AWS account, and that the audience is the same as accepted by the OpenID Connect and the Role name as well.\nEssentially, there's a race condition between the job starting, and the token's validity which doesn't come in until after GitHub has confirmed the job has started; if the size of the `AWS_WEB_IDENITY_TOKEN_FILE` is less than 10 chars, it's probably an error and sleep/spinning will get you the value afterwards.\nThe name of the `AWS_WEB_IDENTITY_TOKEN_FILE` doesn't really matter, so long as it's consistent. If you're using docker containers, then storing it in e.g. `/tmp` will mean that it's not available in any running containers. If you put it under `.git` in the workspace, then not only will git ignore it (if you're doing any hash calculations) but it will also be present in any other docker run actions that you do later on.\nYou might want to configure your role so that the validity of the period used is limited; once you have the web token it's valid until the end of the job, but the token requested has a lifetime of 15 minutes, so it's possible for a longer-running job to expose that.\nIt's likely that GitHub will have a blog post on how to configure/use this in the near future. The above information was inspired by https://awsteele.com/blog/2021/09/15/aws-federation-comes-to-github-actions.html, who has some examples in CloudFormation templates if that's your preferred thing.\nUpdate GitHub (accidentally) changed their thumbprint and the example has been updated. See for more information. The new thumbprint is `6938fd4d98bab03faadb97b34396831e3780aea1` but it's possible to have multiple thumbprints in the IAM OpenID connector.",
      "question_score": 8,
      "answer_score": 20,
      "created_at": "2021-09-19T15:08:35",
      "url": "https://stackoverflow.com/questions/69243571/how-can-i-connect-github-actions-with-aws-deployments-without-using-a-secret-key"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 73070417,
      "title": "How to pass values from the terraform to the Helm chart values.yaml file?",
      "problem": "I am creating ingress-nginx controller using the Helm chart in Terraform. I do have a values.yaml file where I can add the customized information, but I need to pass the SSL Certificate value from the Terraform resource so how can I do that? I am using the below code but getting an error.\n```\n`resource \"aws_acm_certificate\" \"ui_cert\" {\n  domain_name       = var.DOMAIN_NAME\n  validation_method = \"DNS\"\n\n  tags = {\n    Environment = var.ENVIRONMENT \n  }\n\n  lifecycle {\n    create_before_destroy = true\n  }\n\n}\n\n`\n```\n```\n`resource \"helm_release\" \"nginix_ingress\" {\n\n  depends_on = [module.eks, kubernetes_namespace.nginix_ingress,aws_acm_certificate.ui_cert]\n\n  name       = \"ingress-nginx\"\n  repository = \"https://kubernetes.github.io/ingress-nginx\"\n  chart      = \"ingress-nginx\"\n  namespace  = var.NGINX_INGRESS_NAMESPACE\n   \n  values = [templatefile(\"values.yaml\", {\n    controller.service.beta.kubernetes.io/aws-load-balancer-internal = aws_acm_certificate.ui_cert.name,\n  })]\n}\n`\n```\nI am getting the following error:\n```\n` Error: Reference to undeclared resource\n\u2502\n\u2502   on ui.tf line 27, in resource \"helm_release\" \"nginix_ingress\":\n\u2502   27:     controller.service.beta.kubernetes.io/aws-load-balancer-internal = aws_acm_certificate.ui_cert.name,\n\u2502\n\u2502 A managed resource \"controller\" \"service\" has not been declared in the root\n\u2502 module.\n\u2575\n\u2577\n\u2502 Error: Invalid reference\n\u2502\n\u2502   on ui.tf line 27, in resource \"helm_release\" \"nginix_ingress\":\n\u2502   27:     controller.service.beta.kubernetes.io/aws-load-balancer-internal = aws_acm_certificate.ui_cert.name,\n\u2502\n\u2502 A reference to a resource type must be followed by at least one attribute\n\u2502 access, specifying the resource name.\n\u2575\n\u2577\n\u2502 Error: Unsupported attribute\n\u2502\n\u2502   on ui.tf line 27, in resource \"helm_release\" \"nginix_ingress\":\n\u2502   27:     controller.service.beta.kubernetes.io/aws-load-balancer-internal = aws_acm_certificate.ui_cert.name,\n\u2502\n\u2502 This object has no argument, nested block, or exported attribute named\n\u2502 \"name\".\n\u2575\n\u2577\n\u2502 Error: Reference to undeclared resource\n\u2502\n\u2502   on ui.tf line 27, in resource \"helm_release\" \"nginix_ingress\":\n\u2502   27:     controller.service.beta.kubernetes.io/aws-load-balancer-internal = aws_acm_certificate.ui_cert.name,\n\u2502\n\u2502 A managed resource \"controller\" \"service\" has not been declared in the root\n\u2502 module.\n\u2575\n\u2577\n\u2502 Error: Invalid reference\n\u2502\n\u2502   on ui.tf line 27, in resource \"helm_release\" \"nginix_ingress\":\n\u2502   27:     controller.service.beta.kubernetes.io/aws-load-balancer-internal = aws_acm_certificate.ui_cert.name,\n\u2502\n\u2502 A reference to a resource type must be followed by at least one attribute\n\u2502 access, specifying the resource name.\n\u2575\n\u2577\n\u2502 Error: Unsupported attribute\n\u2502\n\u2502   on ui.tf line 27, in resource \"helm_release\" \"nginix_ingress\":\n\u2502   27:     controller.service.beta.kubernetes.io/aws-load-balancer-internal = aws_acm_certificate.ui_cert.name,\n\u2502\n\u2502 This object has no argument, nested block, or exported attribute named\n\u2502 \"name\".\n`\n```\n```\n`controller:\n  name: controller\n  image:\n    chroot: false\n    registry: registry.k8s.io\n    image: ingress-nginx/controller\n    tag: \"v1.3.0\"\n    digest: sha256:d1707ca76d3b044ab8a28277a2466a02100ee9f58a86af1535a3edf9323ea1b5\n    digestChroot: sha256:0fcb91216a22aae43b374fc2e6a03b8afe9e8c78cbf07a09d75636dc4ea3c191\n    pullPolicy: IfNotPresent\n    runAsUser: 101\n    allowPrivilegeEscalation: true\n  containerName: controller\n  containerPort:\n    # http: 80\n    https: 443\n  config:\n    use-proxy-protocol: \"true\"\n  # -- Optionally customize the pod hostname.\n  hostname: {}\n\n  # -- Process IngressClass per name (additionally as per spec.controller).\n  ingressClassByName: false\n\n  # -- This configuration defines if Ingress Controller should allow users to set\n  # their own *-snippet annotations, otherwise this is forbidden / dropped\n  # when users add those annotations.\n  # Global snippets in ConfigMap are still respected\n  allowSnippetAnnotations: true\n\n  ## This section refers to the creation of the IngressClass resource\n  ## IngressClass resources are supported since k8s >= 1.18 and required since k8s >= 1.19\n  ingressClassResource:\n    # -- Name of the ingressClass\n    name: nginx\n    # -- Is this ingressClass enabled or not\n    enabled: true\n    # -- Is this the default ingressClass for the cluster\n    default: false\n    # -- Controller-value of the controller that is processing this ingressClass\n    controllerValue: \"k8s.io/ingress-nginx\"\n\n    # -- Parameters is a link to a custom resource containing additional\n    # configuration for the controller. This is optional if the controller\n    # does not require extra parameters.\n    parameters: {}\n\n  # -- For backwards compatibility with ingress.class annotation, use ingressClass.\n  # Algorithm is as follows, first ingressClassName is considered, if not present, controller looks for ingress.class annotation\n  ingressClass: nginx\n\n  # -- Labels to add to the pod container metadata\n  podLabels: {}\n  #  key: value\n\n  # -- Security Context policies for controller pods\n\n  # -- Allows customization of the source of the IP address or FQDN to report\n  # in the ingress status field. By default, it reads the information provided\n  # by the service. If disable, the status field reports the IP address of the\n  # node or nodes where an ingress controller pod is running.\n  publishService:\n    # -- Enable 'publishService' or not\n    enabled: true\n    # -- Allows overriding of the publish service to bind to\n    # Must be /\n    pathOverride: \"\"\n\n  tcp:\n    # -- Allows customization of the tcp-services-configmap; defaults to $(POD_NAMESPACE)\n    configMapNamespace: \"\"\n    # -- Annotations to be added to the tcp config configmap\n    annotations: {}\n\n  udp:\n    # -- Allows customization of the udp-services-configmap; defaults to $(POD_NAMESPACE)\n    configMapNamespace: \"\"\n    # -- Annotations to be added to the udp config configmap\n    annotations: {}\n\n  # -- Use a `DaemonSet` or `Deployment`\n  kind: Deployment\n\n  # -- Annotations to be added to the controller Deployment or DaemonSet\n  ##\n  annotations: {}\n  #  keel.sh/pollSchedule: \"@every 60m\"\n\n  # -- Labels to be added to the controller Deployment or DaemonSet and other resources that do not have option to specify labels\n  ##\n  labels: {}\n  #  keel.sh/policy: patch\n  #  keel.sh/trigger: poll\n\n  # -- The update strategy to apply to the Deployment or DaemonSet\n  ##\n  updateStrategy: {}\n  #  rollingUpdate:\n  #    maxUnavailable: 1\n  #  type: RollingUpdate\n\n  # -- `minReadySeconds` to avoid killing pods before we are ready\n  ##\n  minReadySeconds: 0\n\n  # -- Topology spread constraints rely on node labels to identify the topology domain(s) that each Node is in.\n  ## Ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/\n  ##\n  topologySpreadConstraints: []\n    # - maxSkew: 1\n    #   topologyKey: topology.kubernetes.io/zone\n    #   whenUnsatisfiable: DoNotSchedule\n    #   labelSelector:\n    #     matchLabels:\n    #       app.kubernetes.io/instance: ingress-nginx-internal\n\n  # -- `terminationGracePeriodSeconds` to avoid killing pods before we are ready\n  ## wait up to five minutes for the drain of connections\n  ##\n  terminationGracePeriodSeconds: 300\n\n  # -- Node labels for controller pod assignment\n  ## Ref: https://kubernetes.io/docs/user-guide/node-selection/\n  ##\n  nodeSelector:\n    kubernetes.io/os: linux\n\n  ## Liveness and readiness probe values\n  ## Ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#container-probes\n  ##\n  ## startupProbe:\n  ##   httpGet:\n  ##     # should match container.healthCheckPath\n  ##     path: \"/healthz\"\n  ##     port: 10254\n  ##     scheme: HTTP\n  ##   initialDelaySeconds: 5\n  ##   periodSeconds: 5\n  ##   timeoutSeconds: 2\n  ##   successThreshold: 1\n  ##   failureThreshold: 5\n  livenessProbe:\n    httpGet:\n      # should match container.healthCheckPath\n      path: \"/healthz\"\n      port: 10254\n      scheme: HTTP\n    initialDelaySeconds: 10\n    periodSeconds: 10\n    timeoutSeconds: 1\n    successThreshold: 1\n    failureThreshold: 5\n  readinessProbe:\n    httpGet:\n      # should match container.healthCheckPath\n      path: \"/healthz\"\n      port: 10254\n      scheme: HTTP\n    initialDelaySeconds: 10\n    periodSeconds: 10\n    timeoutSeconds: 1\n    successThreshold: 1\n    failureThreshold: 3\n\n  # -- Path of the health check endpoint. All requests received on the port defined by\n  # the healthz-port parameter are forwarded internally to this path.\n  healthCheckPath: \"/healthz\"\n\n  # -- Address to bind the health check endpoint.\n  # It is better to set this option to the internal node address\n  # if the ingress nginx controller is running in the `hostNetwork: true` mode.\n  healthCheckHost: \"\"\n\n  # -- Annotations to be added to controller pods\n  ##\n  podAnnotations: {}\n\n  replicaCount: 1\n\n  minAvailable: 1\n\n  ## Define requests resources to avoid probe issues due to CPU utilization in busy nodes\n  ## ref: https://github.com/kubernetes/ingress-nginx/issues/4735#issuecomment-551204903\n  ## Ideally, there should be no limits.\n  ## https://engineering.indeedblog.com/blog/2019/12/cpu-throttling-regression-fix/\n  resources:\n  ##  limits:\n  ##    cpu: 100m\n  ##    memory: 90Mi\n    requests:\n      cpu: 100m\n      memory: 90Mi\n\n  # Mutually exclusive with keda autoscaling\n  autoscaling:\n    enabled: false\n    minReplicas: 1\n    maxReplicas: 11\n    targetCPUUtilizationPercentage: 50\n    targetMemoryUtilizationPercentage: 50\n    behavior: {}\n      # scaleDown:\n      #   stabilizationWindowSeconds: 300\n      #  policies:\n      #   - type: Pods\n      #     value: 1\n      #     periodSeconds: 180\n      # scaleUp:\n      #   stabilizationWindowSeconds: 300\n      #   policies:\n      #   - type: Pods\n      #     value: 2\n      #     periodSeconds: 60\n\n  autoscalingTemplate: []\n  # Custom or additional autoscaling metrics\n  # ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#support-for-custom-metrics\n  # - type: Pods\n  #   pods:\n  #     metric:\n  #       name: nginx_ingress_controller_nginx_process_requests_total\n  #     target:\n  #       type: AverageValue\n  #       averageValue: 10000m\n\n  # Mutually exclusive with hpa autoscaling\n  keda:\n    apiVersion: \"keda.sh/v1alpha1\"\n    ## apiVersion changes with keda 1.x vs 2.x\n    ## 2.x = keda.sh/v1alpha1\n    ## 1.x = keda.k8s.io/v1alpha1\n    enabled: false\n    minReplicas: 1\n    maxReplicas: 11\n    pollingInterval: 30\n    cooldownPeriod: 300\n    restoreToOriginalReplicaCount: false\n    scaledObject:\n      annotations: {}\n      # Custom annotations for ScaledObject resource\n      #  annotations:\n      # key: value\n    triggers: []\n #     - type: prometheus\n #       metadata:\n #         serverAddress: http://:9090\n #         metricName: http_requests_total\n #         threshold: '100'\n #         query: sum(rate(http_requests_total{deployment=\"my-deployment\"}[2m]))\n\n    behavior: {}\n #     scaleDown:\n #       stabilizationWindowSeconds: 300\n #       policies:\n #       - type: Pods\n #         value: 1\n #         periodSeconds: 180\n #     scaleUp:\n #       stabilizationWindowSeconds: 300\n #       policies:\n #       - type: Pods\n #         value: 2\n #         periodSeconds: 60\n\n  # -- Enable mimalloc as a drop-in replacement for malloc.\n  ## ref: https://github.com/microsoft/mimalloc\n  ##\n  enableMimalloc: true\n\n  ## Override NGINX template\n  customTemplate:\n    configMapName: \"\"\n    configMapKey: \"\"\n\n  service:\n    enabled: true\n    annotations:\n      service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\"\n      service.beta.kubernetes.io/aws-load-balancer-proxy-protocol: \"*\"\n      service.beta.kubernetes.io/aws-load-balancer-ssl-cert: \"ssl-cert\"\n      service.beta.kubernetes.io/aws-load-balancer-backend-protocol: \"http\"\n      service.beta.kubernetes.io/aws-load-balancer-ssl-ports: \"443\"\n`\n```\nCan someone plz tell me how I can make it work?",
      "solution": "The `templatefile` built-in function is used to pass the values to variables defined in the templated file. So for example, in your case, you would define a variable in the template file called e.g., `ssl_cert`. Then, when calling the `templatefile` function, you would pass it the value provided by the ACM resource:\n```\n`  values = [templatefile(\"values.yaml\", {\n    ssl_cert = aws_acm_certificate.ui_cert.name\n  })]\n`\n```\nThe `ssl_cert` variable inside of the `templatefile` would be associated with the annotation `controller.service.beta.kubernetes.io/aws-load-balancer-ssl-cert`. Based on the YML file, the variable should be added in the last section of the file:\n`  service:\n    enabled: true\n    annotations:\n      service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\"\n      service.beta.kubernetes.io/aws-load-balancer-proxy-protocol: \"*\"\n      service.beta.kubernetes.io/aws-load-balancer-ssl-cert: \"${ssl_cert}\" # here is where the replacement will be made\n      service.beta.kubernetes.io/aws-load-balancer-backend-protocol: \"http\"\n      service.beta.kubernetes.io/aws-load-balancer-ssl-ports: \"443\"\n`\nThe `templatefile` function is really powerful, but I strongly suggest understanding how variables and variable substitution works when using it [1]. It is unaware of the substitution you want to make unless you have a placeholder variable both when calling the function and in the template file.\n\n[1] https://www.terraform.io/language/functions/templatefile",
      "question_score": 8,
      "answer_score": 18,
      "created_at": "2022-07-21T19:28:25",
      "url": "https://stackoverflow.com/questions/73070417/how-to-pass-values-from-the-terraform-to-the-helm-chart-values-yaml-file"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 73026624,
      "title": "Terraform Intermittent error on Apple&#39;s M1",
      "problem": "When I run `terraform plan`, I expect to see a plan but instead I get\n`\u2502 Error: Failed to load plugin schemas\n\u2502\n\u2502 Error while loading schemas for plugin components: Failed to obtain provider schema: Could not load the schema for provider registry.terraform.io/hashicorp/aws: failed to\n\u2502 instantiate provider \"registry.terraform.io/hashicorp/aws\" to obtain schema: Unrecognized remote plugin message:\n\u2502\n\u2502 This usually means that the plugin is either invalid or simply\n\u2502 needs to be recompiled to support the latest protocol...\n`\nSystem is: arm64 m1 - Monterey - TFEnv - Terraform 1.1.9",
      "solution": "Solution\nExplanation\n\nAn issue in Go runtime running amd64 M1 through Apple's Rosetta\nTFenv installed the amd64 version of Terraform instead of arm64.\n\nPatch\nSet the variable `GODEBUG=asyncpreemptoff=1` by prepending it to the front of your terraform or adding it to your bash profile.\nFix\nMake sure you are running the right build for your system.\n`\u279c terraform version\nTerraform v1.1.9\non darwin_arm64\n`\nIf you are on an M1 and see `darwin_amd64` when you run `terraform version`, try installing Terraform again.\nWith TFenv\n```\n`TFENV_ARCH=arm64 tfenv install 1.2.4\ntfenv use 1.2.4\n`\n```\nDetails\n\nhttps://github.com/hashicorp/terraform-provider-aws/issues/23850\nhttps://yaleman.org/post/2021/2021-01-01-apple-m1-terraform-and-golang/",
      "question_score": 8,
      "answer_score": 17,
      "created_at": "2022-07-18T19:57:10",
      "url": "https://stackoverflow.com/questions/73026624/terraform-intermittent-error-on-apples-m1"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 66046640,
      "title": "Will the terraform fail if the data does not exist?",
      "problem": "Will the terraform fail if a user in the data does not exist?\nI need to specify a user in the nonproduction environment by the data block:\n```\n`data \"aws_iam_user\" \"labUser\" {\n  user_name = \"gitlab_user\"\n}\n`\n```\nThen I use this user in giving the user permissions:\n```\n`resource \"aws_iam_role\" \"ApiAccessRole_abc\" {\n name = \"${var.stack}-ApiAccessRole_abc\"\n tags = \"${var.tags}\"\n assume_role_policy = In the production environment this user does not exist.  Would the terraform break if this user does not exist?  What would be a good approach to use the same terraform in both environments?",
      "solution": "In Terraform a `data` block like you showed here is both a mechanism to fetch data and also an assertion by the author (you) that a particular external object is expected to exist in order for this configuration to be applyable.\nIn your case then, the answer is to ensure that the assertion that the object exists only appears in situations where it should exist. The \"big picture\" answer to this is to review the Module Composition guide and consider whether this part of your module ought to be decomposed into a separate module if it isn't always a part of the module it's embedded in, but I'll also show a smaller solution that uses conditional expressions to get the behavior you wanted without any refactoring:\n```\n`variable \"lab_user\" {\n  type    = string\n  default = null\n}\n\ndata \"aws_iam_user\" \"lab_user\" {\n  count = length(var.lab_user[*])\n\n  user_name = var.lab_user\n}\n\nresource \"aws_iam_role\" \"api_access_role_abc\" {\n count = length(data.aws_iam_user.lab_user)\n\n name = \"${var.stack}-ApiAccessRole_abc\"\n tags = var.tags\n assume_role_policy = jsonencode({\n   Version   = \"2019-11-29\"\n   Statement = [\n     {\n       Sid    = \"\"\n       Action = \"sts:AssumeRole\"\n       Effect = \"Allow\"\n       Principal = {\n         AWS = [data.aws_iam_user.lab_user[count.index].arn]\n       }\n     },\n   ]\n })\n}\n`\n```\nThere's a few different things in the above that I want to draw attention to:\n\nI made the lab username an optional variable rather than a hard-coded value. You can than change the behavior between your environments by assigning a different value to that `lab_user` variable, or leaving it unset altogether for environments that don't need a \"lab user\".\nIn the `data \"aws_iam_user\"` I set count to `length(var.lab_user[*])`. The `[*]` operator here is asking Terraform to translate the possibly-null string variable `var.lab_user` into a list of either zero or one elements, and then using the length of that list to decide how many `aws_iam_user` queries to make. If `var.lab_user` is `null` then the length will be zero and so no queries will be made.\nFinally, I set the `count` for the `aws_iam_role` resource to match the length of the `aws_iam_user` data result, so that in any situation where there's one user expected there will also be one role created.\n\nIf you reflect on the Module Composition guide and conclude that this lab user ought to be a separate concern in a separate module then you'd be able to remove this conditional complexity from the \"gitlab user\" module itself and instead have the calling module either call that module or not depending on whether such a user is needed for that environment. The effect would be the same, but the decision would be happening in a different part of the configuration and thus it would achieve a different separation of concerns. Which separation of concerns is most appropriate for your system is, in the end, a tradeoff you'll need to make for yourself based on your knowledge of the system and how you expect it might evolve in future.",
      "question_score": 8,
      "answer_score": 14,
      "created_at": "2021-02-04T14:29:13",
      "url": "https://stackoverflow.com/questions/66046640/will-the-terraform-fail-if-the-data-does-not-exist"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 72091152,
      "title": "AWS User not authorized to perform PassRole",
      "problem": "I'm trying to create a job in AWS Glue using the Windows AWS Client and I'm receiving that I'm not authorized to perform: iam:PassRole as you can see:\n```\n`Console>aws glue create-job --name \"aws_glue_test\" --role \"My_Role\" --command \"Name=glueetlpythonshell,ScriptLocation=s3://mys3bucket/jobs/aws_glue_test.py,PythonVersion=3\"\n\nAn error occurred (AccessDeniedException) when calling the CreateJob operation: User: arn:aws:iam::1111:user/My_User is not authorized to perform: iam:PassRole on resource: arn:aws:iam::1111:role/My_Role because no identity-based policy allows the iam:PassRole action\n`\n```\nThe configuration in AWS is set by using Terraform, something like this:\n```\n`\n\nresource \"aws_s3_bucket\" \"mys3bucket\" {\n\n  bucket = \"mys3bucket\"\n\n  tags = {\n    Name            = \"mys3bucket\"\n    ITOwnerEmail    = \"my@email.com\"\n  }\n\n}\n\nresource \"aws_s3_bucket_acl\" \"mys3bucket_acl\" {\n  bucket = aws_s3_bucket.mys3bucket.id\n  acl    = \"private\"\n}\n\n#=========IAM user======#\n\nresource \"aws_iam_user\" \"My_User\" {\n  name = \"My_User \"\n  path = \"/\"\n}\n\nresource \"aws_iam_user_policy\" \"My_User-p\" {\n  name = \"My_User-p\"\n  user = \"My_User\"\n\n  policy = I tried to attach IAM Pass Role but it still failing and I don't know why.\nAny help is welcomed. Thank you in advance\nSOLUTION: Added in the Code.",
      "solution": "You need to add `iam:PassRole` action to the policy of the IAM user that is being used to create-job. Something like:\n```\n`{\n            \"Action\": [\n                \"iam:PassRole\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": [\n                \"arn:aws:iam::1111:role/My_Role\"\n            ],\n            \"Condition\": {\n                \"StringLike\": {\n                    \"iam:PassedToService\": [\n                        \"glue.amazonaws.com\"\n                    ]\n                }\n            }\n        }\n`\n```",
      "question_score": 8,
      "answer_score": 16,
      "created_at": "2022-05-02T20:57:46",
      "url": "https://stackoverflow.com/questions/72091152/aws-user-not-authorized-to-perform-passrole"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 68037905,
      "title": "Terraform escape sequence",
      "problem": "I am running some commands on my ec2 using remote-exec provisioner in my terraform file. But I am stuck at escaping special characters in a command. This code portion is from my main.tf file inside remote-exec provisioner portion. The error coming in terraform is \"Invalid character\" and \"Invalid multi-line string\". I wanted correct string sequence, so that these commands can execute on my ec2.\n```\n`\"VAR=$(cat contents.txt | grep '\"token\"'),\n\"VAR=\"${VAR:11}\"\",\n\"VAR=\"${VAR:0:-1}\"\",\n`\n```",
      "solution": "The `${` are also interpreted by terraform (as variable substitutions). You need to escape those with `$` to become `$${`.\nA full working example:\nmain.tf:\n```\n`resource \"null_resource\" \"test\" {\n  provisioner \"local-exec\" {\n    command =  contents.txt\n\nVAR=$(cat contents.txt | grep \\\"token\\\")\nVAR=$${VAR:11}\nVAR=$${VAR:0:5}\n\necho $VAR >log\n    EOF\n  }\n}\n`\n```\n```\n`$ terraform apply -input=false -auto-approve \nnull_resource.test: Creating...\nnull_resource.test: Provisioning with 'local-exec'...\nnull_resource.test (local-exec): Executing: [\"/bin/sh\" \"-c\" \"echo 'some11char hello \\\"token\\\"' > contents.txt\\n\\nVAR=$(cat contents.txt | grep \\\\\\\"token\\\\\\\")\\nVAR=${VAR:11}\\nVAR=${VAR:0:5}\\n\\necho $VAR >log\\n\"]\nnull_resource.test: Creation complete after 0s [id=3425651808766026549]\n\nApply complete! Resources: 1 added, 0 changed, 0 destroyed.\n`\n```\n```\n`$ cat contents.txt \nsome11char hello \"token\"\n\n$ cat log     \nhello\n\n$\n`\n```",
      "question_score": 8,
      "answer_score": 16,
      "created_at": "2021-06-18T17:35:38",
      "url": "https://stackoverflow.com/questions/68037905/terraform-escape-sequence"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 70249255,
      "title": "How to get output for for_each loop condition within terraform?",
      "problem": "I have a resource block with for_each loop condition and I wanted to output name and address_prefixes of the resource block.\nmain.tf:\n```\n`resource \"azurerm_subnet\" \"snets\" {\n    for_each = var.subnets\n    name = each.key\n    resource_group_name = azurerm_resource_group.rg.name\n    virtual_network_name = azurerm_virtual_network.vnet.name\n    address_prefixes = [each.value]\n}\n`\n```\nI have tried something like this, but it didn't worked.\noutput.tf\n```\n`output \"azurerm-subnet\" {\n    value = azurerm_subnet.snets.*.name\n}\n`\n```\nError:\n```\n`\u2502 Error: Unsupported attribute\n\u2502\n\u2502   on output.tf line 2, in output \"azurerm-subnet\":\n\u2502    2:     value = azurerm_subnet.snets.*.name\n\u2502\n\u2502 This object does not have an attribute named \"name\".\n`\n```",
      "solution": "This can most easily be accomplished with a `list` constructor and a `for` expression. We iterate through the `map` of exported attributes for the `azurerm_subnet.snets` and return the `name` value on each iteration:\n```\n`output \"azurerm_subnets\" {\n  value = [ for subnet in azurerm_subnet.snets : subnet.name ]\n}\n`\n```\nand the output `azurerm_subnets` will be a `list(string)` where each element is the subnet name.",
      "question_score": 8,
      "answer_score": 15,
      "created_at": "2021-12-06T18:11:50",
      "url": "https://stackoverflow.com/questions/70249255/how-to-get-output-for-for-each-loop-condition-within-terraform"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 67104574,
      "title": "Terraform: &quot;known only after apply&quot; ISSUE",
      "problem": "I'm creating an `aws_subnet` and referencing it in another resource.\nExample:\n```\n`resource \"aws_subnet\" \"mango\" {\n     vpc_id     = aws_vpc.mango.id\n     cidr_block = \"${var.subnet_cidr}\"\n  }\n`\n```\nThe reference\n```\n` network_configuration {\n    subnets          = \"${aws_subnet.mango.id}\"\n  }\n`\n```\nWhen planning it I get\n`aws_subnet.mango.id is a string, known only after apply`\nerror. I'm new to Terraform. Is there something similar to Cloudformation's `DependsOn` or `Export/Import`?",
      "solution": "This is a case of explicit dependency.\nThe argument `depends_on` similar to CloudFormation's `DependsOn` solves such dependencies.\nNote: \"Since Terraform will wait to create the dependent resource until after the specified resource is created, adding explicit dependencies can increase the length of time it takes for Terraform to create your infrastructure.\"\nExample:\n```\n`depends_on = [aws_subnet.mango]\n`\n```",
      "question_score": 8,
      "answer_score": 8,
      "created_at": "2021-04-15T10:06:56",
      "url": "https://stackoverflow.com/questions/67104574/terraform-known-only-after-apply-issue"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 66248421,
      "title": "Terragrunt import resource created via for_each loop",
      "problem": "I'm creating a GCP buckets with for_each loop, and wanted to import the existing buckets into my terraform state\n`An execution plan has been generated and is shown below.\nResource actions are indicated with the following symbols:\n  + create\n\nTerraform will perform the following actions:\n\n  # google_storage_bucket.buckets[\"a-test-test-test\"] will be created\n  + resource \"google_storage_bucket\" \"buckets\" {\n      + bucket_policy_only          = (known after apply)\n      + force_destroy               = false\n      + id                          = (known after apply)\n      + location                    = \"US\"\n      + name                        = \"a-test-test-test\"\n      + project                     = \"xxx\"\n      + self_link                   = (known after apply)\n      + storage_class               = \"STANDARD\"\n      + uniform_bucket_level_access = false\n      + url                         = (known after apply)\n\n      + versioning {\n          + enabled = true\n        }\n    }\n\nPlan: 1 to add, 0 to change, 0 to destroy.\n\nChanges to Outputs:\n  ~ urls = [\n      - \"gs://a-test-test-test\",\n      + (known after apply),\n    ]\n\nDo you want to perform these actions?\n  Terraform will perform the actions described above.\n  Only 'yes' will be accepted to approve.\n\n  Enter a value: yes\n\ngoogle_storage_bucket.buckets[\"a-test-test-test\"]: Creating...\n\nError: googleapi: Error 409: You already own this bucket. Please select another name., conflict\n`\nThe resource already exists but that's fine, I can just import it, question is how\nbecause running it like this\n`MacBook-Pro% terragrunt import google_storage_bucket.buckets a-test-test-test\n...\nAcquiring state lock. This may take a few moments...\ngoogle_storage_bucket.buckets: Importing from ID \"a-test-test-test\"...\ngoogle_storage_bucket.buckets: Import prepared!\n  Prepared google_storage_bucket for import\ngoogle_storage_bucket.buckets: Refreshing state... [id=a-test-test-test]\n\nImport successful!\n`\nseems to work, but it imported it 'wrongly'\n`terragrunt state list\n...\ngoogle_storage_bucket.buckets\n`\nits show in my tfstate, but it should be like this\n`google_storage_bucket.buckets[\"a-test-test-test\"]\n`\nbecause if I run now apply - it says it wants to delete the `google_storage_bucket.buckets` and create `google_storage_bucket.buckets[\"a-test-test-test\"]`\n`google_storage_bucket.buckets: Refreshing state... [id=a-test-test-test]\n\nAn execution plan has been generated and is shown below.\nResource actions are indicated with the following symbols:\n  + create\n  - destroy\n\nTerraform will perform the following actions:\n\n  # google_storage_bucket.buckets will be destroyed\n  - resource \"google_storage_bucket\" \"buckets\" {\n      - bucket_policy_only          = false -> null\n      - default_event_based_hold    = false -> null\n      - force_destroy               = false -> null\n      - id                          = \"a-test-test-test\" -> null\n      - labels                      = {} -> null\n      - location                    = \"US\" -> null\n      - name                        = \"a-test-test-test\" -> null\n      - project                     = \"xxx\" -> null\n      - requester_pays              = false -> null\n      - self_link                   = \"https://www.googleapis.com/storage/v1/b/a-test-test-test\" -> null\n      - storage_class               = \"STANDARD\" -> null\n      - uniform_bucket_level_access = false -> null\n      - url                         = \"gs://a-test-test-test\" -> null\n\n      - versioning {\n          - enabled = true -> null\n        }\n    }\n\n  # google_storage_bucket.buckets[\"a-test-test-test\"] will be created\n  + resource \"google_storage_bucket\" \"buckets\" {\n      + bucket_policy_only          = (known after apply)\n      + force_destroy               = false\n      + id                          = (known after apply)\n      + location                    = \"US\"\n      + name                        = \"a-test-test-test\"\n      + project                     = \"xxx\"\n      + self_link                   = (known after apply)\n      + storage_class               = \"STANDARD\"\n      + uniform_bucket_level_access = false\n      + url                         = (known after apply)\n\n      + versioning {\n          + enabled = true\n        }\n    }\n\nPlan: 1 to add, 0 to change, 1 to destroy.\n\nChanges to Outputs:\n  + urls = [\n      + (known after apply),\n    ]\n\nDo you want to perform these actions?\n  Terraform will perform the actions described above.\n  Only 'yes' will be accepted to approve.\n`\nAny thoughts how to import into a for_each in terragrunt ?\nI tried the\n`terragrunt import google_storage_bucket.buckets a-test-test-test\nterragrunt import google_storage_bucket.buckets.a-test-test-test a-test-test-test\nterragrunt import google_storage_bucket.buckets[\"a-test-test-test\"] a-test-test-test\nterragrunt import google_storage_bucket.buckets[\\\"a-test-test-test\\\"] a-test-test-test\n`\nand noone work just leaving me with error\n`zsh: no matches found: google_storage_bucket.buckets[\"a-test-test-test\"]\n`\nwhile the first option `terragrunt import google_storage_bucket.buckets a-test-test-test` , imported (aka. worked) but not the right way\n\nThe terraform code is like so:\n`inputs = {\n  project_id  = \"${local.project_id}\"\n    {\n      name                        = \"a-test-test-test\"\n      location                    = \"US\"\n    }\n}\n\nlocals {\n  buckets        = {for b in jsondecode(var.buckets) : b.name => b }\n}\n\nvariable \"buckets\" {\n  description = \"The name of the bucket.\"\n}\n\nresource \"google_storage_bucket\" \"buckets\" {\n  for_each      = local.buckets\n  name          = each.key\n  project       = var.project_id\n  location      = each.value.location\n`",
      "solution": "Importing into the full instance address (including the instance key index part) is the right approach, but the trick here is to determine the best way to work around the grammar of your shell so that the necessary characters can reach Terraform.\nFor Unix-style shells I typically recommend putting the address in single quotes to disable metacharacter interpretation, like this:\n```\n`terragrunt import 'google_storage_bucket.buckets[\"a-test-test-test\"]' a-test-test-test\n`\n```\nI don't have much experience with `zsh` in particular but from referring to a copy of part of its documentation I get the impression that the above is valid `zsh` syntax too. If the above doesn't work, it might be worth trying with a different shell such as `bash` to see if you get a different result.\nAlthough you specifically mentioned `zsh`, for completeness I'll also note that on Windows the rules are a little different: single quotes aren't supported under the conventional Windows command line syntax, and so unfortunately we must escape the quotes instead when running Terraform from the Windows command prompt:\n```\n`terragrunt import google_storage_bucket.buckets[\\\"a-test-test-test\\\"] a-test-test-test\n`\n```\nThe important thing is that the quote characters `\"` in the address make it through the shell to Terraform, so that Terraform can then successfully parse the argument as resource address syntax.",
      "question_score": 8,
      "answer_score": 14,
      "created_at": "2021-02-17T20:04:48",
      "url": "https://stackoverflow.com/questions/66248421/terragrunt-import-resource-created-via-for-each-loop"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 71022815,
      "title": "Creating Azure Storage Containers in a storage account with network rules, with Terraform",
      "problem": "I am trying to write Terraform that will create an Azure Storage account and then a bunch of storage containers inside it. An important detail is that the storage account has network rules that restrict access to a specific address space. This was causing the container creation to fail.\nI managed to get around this by using `azurerm_storage_account_network_rules`, depending on the containers so not to block their creation. Something like this:\n```\n`resource \"azurerm_storage_account\" \"this\" {\n  name                = local.storage_name\n  resource_group_name = azurerm_resource_group.this.name\n  location            = var.location\n\n  account_tier             = \"Standard\"\n  account_kind             = \"StorageV2\"\n  is_hns_enabled           = true\n  account_replication_type = \"LRS\"\n}\n\nresource \"azurerm_storage_container\" \"data\" {\n  for_each = toset(var.storage_containers)\n\n  name                  = each.value\n  storage_account_name  = azurerm_storage_account.this.name\n  container_access_type = \"private\"\n}\n\n# FIXME This order prevents destruction of infrastructure :(\nresource \"azurerm_storage_account_network_rules\" \"this\" {\n  storage_account_id   = azurerm_storage_account.this.id\n\n  default_action = \"Deny\"\n  bypass         = [\"AzureServices\"]\n\n  virtual_network_subnet_ids = [\n    # Some address space here...\n  ]\n\n  # NOTE The order here matters: We cannot create storage\n  # containers once the network rules are locked down\n  depends_on = [\n    azurerm_storage_container.data\n  ]\n}\n`\n```\nThis works for creating the infrastructure, but when I try to `terraform destroy`, I get a 403 authentication error:\n\nError: retrieving Container \"data\" (Account \"XXX\" / Resource Group \"XXX\"): containers.Client#GetProperties: Failure responding to request: StatusCode=403 -- Original Error: autorest/azure: Service returned an error. Status=403 Code=\"AuthorizationFailure\" Message=\"This request is not authorized to perform this operation.\\nRequestId:XXX\\nTime:XXX\"\n\nThis is with my Service Principal, which has `Contributor` and `User Access Administrator` roles on the same subscription. Interestingly, when I'm logged in to the Azure Portal as myself (with the `Owner` role), I can add and remove storage containers regardless of the network rules being present.\nSo, is there a way of setting the Terraform dependencies such that they can both be built and destroyed without hitting any authentication conflicts? Alternatively, would upgrading my SP's role to `Owner` (or adding another, more targeted role) solve the problem?",
      "solution": "This is an expected behavior as you are setting up the `network rules` for the storage account to `deny` and only `bypassing` the `Azure Services`.\nWhen you `deny` and `bypass Azure Services` the `Azure Services` like `Azure Portal's IP` gets the access to the storage account and you are able to delete it . But at the same time , when you use `terraform` to perform a destroy , then it denies because `your IP` which is being used by your machine to send `terraform` requests to Azure is `not bypassed`.\nI tested your code like below with the same permissions:\n\nAs a Solution you will have to add `ip rules` while creating the storage account to the add the `client_ip` like below :\n\n```\n`provider \"azurerm\" {\n    features{}\n  client_id=\"f6a2f33d-xxxx-xxxx-xxxx-xxxx\"\n  client_secret= \"GZ67Q~xxxx~3N-qLT\"\n  tenant_id = \"72f988bf-xxxx-xxxx-xxxx-2d7cd011db47\"\n  subscription_id=\"948d4068-xxxx-xxxx-xxxx-xxxx\"\n}\n\nlocals {\n  storage_name = \"ansumantestsacc12\"\n  subnet_id_list = [\n      \"/subscriptions/xxxx/resourceGroups/xxxx/providers/Microsoft.Network/virtualNetworks/xxxx/subnets/xxxx\"\n  ]\n  my_ip = [\"xx.xx.xx.xxx\"] # IP used by me\n}\nvariable \"storage_containers\" {\n  default = [\n      \"test\",\n      \"terraform\"\n  ]\n}\ndata \"azurerm_resource_group\" \"this\" {\n  name = \"ansumantest\"\n}\n\nresource \"azurerm_storage_account\" \"this\" {\n  name                = local.storage_name\n  resource_group_name = data.azurerm_resource_group.this.name\n  location            = data.azurerm_resource_group.this.location\n\n  account_tier             = \"Standard\"\n  account_kind             = \"StorageV2\"\n  is_hns_enabled           = true\n  account_replication_type = \"LRS\"\n}\n\nresource \"azurerm_storage_container\" \"data\" {\n  for_each = toset(var.storage_containers)\n\n  name                  = each.value\n  storage_account_name  = azurerm_storage_account.this.name\n  container_access_type = \"private\"\n}\n\n# FIXED\nresource \"azurerm_storage_account_network_rules\" \"this\" {\n  storage_account_id   = azurerm_storage_account.this.id\n\n  default_action = \"Deny\"\n  bypass         = [\"AzureServices\"]\n  ip_rules       = local.my_ip # need to set this to use terraform in our machine\n  virtual_network_subnet_ids = local.subnet_id_list\n\n  # NOTE The order here matters: We cannot create storage\n  # containers once the network rules are locked down\n  depends_on = [\n    azurerm_storage_container.data\n  ]\n}\n`\n```\nOutput:",
      "question_score": 8,
      "answer_score": 13,
      "created_at": "2022-02-07T18:29:12",
      "url": "https://stackoverflow.com/questions/71022815/creating-azure-storage-containers-in-a-storage-account-with-network-rules-with"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 72060850,
      "title": "Use Terraform to deploy MySQL 8.0 in AWS Aurora V2",
      "problem": "I'm attempting to deploy a serverless MySQL 8.0 service using AWS Aurora V2 using Terraform.\nTerraform details (not on latest version, but should be compatible with latest AWS provider version):\n```\n`Terraform v0.15.4\non linux_amd64\n+ provider registry.terraform.io/hashicorp/aws v4.12.0\n+ provider registry.terraform.io/hashicorp/consul v2.15.1\n+ provider registry.terraform.io/hashicorp/random v3.1.3\n+ provider registry.terraform.io/hashicorp/template v2.2.0\n+ provider registry.terraform.io/hashicorp/vault v3.5.0\n`\n```\nHere's the relevant `main.tf` file:\n```\n`resource \"aws_rds_cluster\" \"database\" {\n  cluster_identifier      = var.cluster_identifier\n  db_subnet_group_name    = aws_db_subnet_group.db_subnet_group.name\n  vpc_security_group_ids  = var.vpc_security_group_ids\n  engine_mode             = \"serverless\"\n  enable_http_endpoint    = var.enable_http_endpoint\n  master_username         = var.master_username\n  master_password         = random_password.rng.result\n  database_name           = var.name\n  backup_retention_period = var.backup_retention_period\n  skip_final_snapshot     = var.skip_final_snapshot\n  deletion_protection     = var.deletion_protection\n  engine                  = \"aurora-mysql\"\n  engine_version          = \"8.0.mysql_aurora.3.02.0\"\n\n  serverlessv2_scaling_configuration {\n    max_capacity = var.max_capacity\n    min_capacity = var.min_capacity\n  }\n\n  lifecycle {\n    ignore_changes = [\n      engine_version,\n      availability_zones,\n      master_username,\n      master_password,\n    ]\n  }\n\n  tags = {\n    Environment = var.env\n    Name        = var.name\n  }\n}\n\nresource \"aws_rds_cluster_instance\" \"cluster_instances\" {\n  identifier         = \"${var.cluster_identifier}-serverless\"\n  cluster_identifier = aws_rds_cluster.database.id\n  instance_class     = \"db.serverless\"\n  engine             = aws_rds_cluster.database.engine\n  engine_version     = aws_rds_cluster.database.engine_version\n}\n\nresource \"aws_db_subnet_group\" \"db_subnet_group\" {\n  name       = \"${var.cluster_identifier}-subnet-group\"\n  subnet_ids = var.subnet_ids\n\n  tags = {\n    Environment = var.env\n  }\n}\n\nresource \"random_password\" \"rng\" {\n  length  = 16\n  special = false\n\n  keepers = {\n    cluster_identifier = var.cluster_identifier\n  }\n}\n`\n```\nThe above file was originally a serverless MySQL 5.7 service using Aurora V1. I modified this existing `main.tf` file using these resources:\n\nGitHub issue: https://github.com/hashicorp/terraform-provider-aws/issues/24349\nTerraform docs for `aws_rds_cluster`\nTerraform docs for `aws_rds_cluster_instance`\n\nThe `terraform plan` goes fine. When running `terraform apply` this error is present:\n```\n`module.aurora.aws_rds_cluster.database: Creating...\n\u2577\n\u2502 Error: error creating RDS cluster: InvalidParameterValue: The engine mode serverless you requested is currently unavailable.\n\u2502   status code: 400, request id: 060f8bce-4bc4-4462-9735-78495ecaf308\n\u2502 \n\u2502   with module.aurora.aws_rds_cluster.database,\n\u2502   on modules/aws/rds/main.tf line 1, in resource \"aws_rds_cluster\" \"database\":\n\u2502    1: resource \"aws_rds_cluster\" \"database\" {\n\u2502 \n\u2575\n`\n```\nI can't infer much from this error, I'm guessing something isn't supporting this since the AWS provider `v4.12.0` was released yesterday. I assume it could also be the version of Terraform itself, but again, I believe Terraform v0.15.4 is compatible with the v4.12.0 of the AWS provider.\nMy main goal is to switch the service from MySQL 5.7 serverless to MySQL 8.0 serverless.",
      "solution": "The `engine_mode` for Aurora Serverless 2 is `provisioned`, not `serverless`.\n```\n`    engine_mode               = \"provisioned\"\n`\n```",
      "question_score": 8,
      "answer_score": 12,
      "created_at": "2022-04-29T18:13:10",
      "url": "https://stackoverflow.com/questions/72060850/use-terraform-to-deploy-mysql-8-0-in-aws-aurora-v2"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 66246886,
      "title": "Cognito User Pool Lambda Trigger permission",
      "problem": "I'm using Terraform to create a Cognito User pool. I'd like to use a lambda function for sending a custom message when a user signs up. When I run attempt to sign up on the client, I get an error saying that \"CustomMessage invocation failed due to error AccessDeniedException.\" I've used Lambda Permissions before, but I can't find any examples of this configuration. How do I give the lambda function permission? The following is my current configuration.\n```\n`resource \"aws_cognito_user_pool\" \"main\" {\n  name = \"${var.user_pool_name}_${var.stage}\"\n  username_attributes = [ \"email\" ]\n  schema {\n    attribute_data_type = \"String\"\n    mutable             = true\n    name                = \"name\"\n    required            = true\n  }\n  schema {\n    attribute_data_type = \"String\"\n    mutable             = true\n    name                = \"email\"\n    required            = true\n  }\n\n  password_policy {\n    minimum_length    = \"8\"\n    require_lowercase = true\n    require_numbers   = true\n    require_symbols   = true\n    require_uppercase = true\n  }\n  mfa_configuration        = \"OFF\"\n  \n  lambda_config {\n    custom_message    = aws_lambda_function.custom_message.arn\n    post_confirmation = aws_lambda_function.post_confirmation.arn\n  }\n}\n...\nresource \"aws_lambda_permission\" \"get_blog\" {\n  statement_id  = \"AllowExecutionFromCognito\"\n  action        = \"lambda:InvokeFunction\"\n  function_name = aws_lambda_function.custom_message.function_name\n  principal     = \"cognito-idp.amazonaws.com\"\n  source_arn    = \"${aws_cognito_user_pool.main.arn}/*/*\"\n  depends_on = [ aws_lambda_function.custom_message ]\n}\n...\nresource \"aws_lambda_function\" \"custom_message\" {\n  filename         = \"${var.custom_message_path}/${var.custom_message_file_name}.zip\"\n  function_name    = var.custom_message_file_name\n  role             = aws_iam_role.custom_message.arn\n  handler          = \"${var.custom_message_file_name}.handler\"\n  source_code_hash = filebase64sha256(\"${var.custom_message_path}/${var.custom_message_file_name}.zip\")\n  runtime          = \"nodejs12.x\"\n  timeout          = 10\n  layers           = [ var.node_layer_arn ]\n  environment {\n    variables = {\n      TABLE_NAME = var.table_name\n      RESOURCENAME = \"blogAuthCustomMessage\"\n      REGION = \"us-west-2\"\n    }\n  }\n  tags = {\n    Name = var.developer\n  }\n  depends_on = [\n    data.archive_file.custom_message, \n  ]\n}\n`\n```",
      "solution": "Based on OP's feedback in the comment section, changing `source_arn` property in the `aws_lambda_permission.get_blog` to `aws_cognito_user_pool.main.arn` works.",
      "question_score": 8,
      "answer_score": 10,
      "created_at": "2021-02-17T18:17:20",
      "url": "https://stackoverflow.com/questions/66246886/cognito-user-pool-lambda-trigger-permission"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 70797574,
      "title": "(Terraform, Cloud Run) Error: Forbidden Your client does not have permission to get URL / from this server",
      "problem": "I'm trying to run a docker image on Cloud Run with the Terraform code below:\n`provider \"google\" {\n  credentials = file(\"myCredentials.json\")\n  project     = \"myproject-214771\"\n  region      = \"asia-northeast1\"\n}\n\nresource \"google_cloud_run_service\" \"default\" {\n  name     = \"hello-world\"\n  location = \"asia-northeast1\"\n\n  template {\n    spec {\n      containers {\n        image = \"gcr.io/myproject-214771/hello-world:latest\"\n      }\n    }\n  }\n\n  traffic {\n    percent         = 100\n    latest_revision = true\n  }\n}\n`\nThen, it was successful to run the docker image:\n\nBut when I access the URL, it shows this:\n\nError: Forbidden Your client does not have permission to get URL /\nfrom this server\n\nAre there any mistakes in my Terraform code?",
      "solution": "Add(Copy & paste) this code below to your Terraform code to allow unauthenticated invocations for public API or website:\n`data \"google_iam_policy\" \"noauth\" {\n  binding {\n    role = \"roles/run.invoker\"\n    members = [\n      \"allUsers\",\n    ]\n  }\n}\n\nresource \"google_cloud_run_service_iam_policy\" \"noauth\" {\n  location    = google_cloud_run_service.default.location\n  project     = google_cloud_run_service.default.project\n  service     = google_cloud_run_service.default.name\n\n  policy_data = data.google_iam_policy.noauth.policy_data\n}\n`\nSo this is the full code:\n`provider \"google\" {\n  credentials = file(\"myCredentials.json\")\n  project     = \"myproject-214771\"\n  region      = \"asia-northeast1\"\n}\n\nresource \"google_cloud_run_service\" \"default\" {\n  name     = \"hello-world\"\n  location = \"asia-northeast1\"\n\n  template {\n    spec {\n      containers {\n        image = \"gcr.io/myproject-214771/hello-world:latest\"\n      }\n    }\n  }\n\n  traffic {\n    percent         = 100\n    latest_revision = true\n  }\n}\n\ndata \"google_iam_policy\" \"noauth\" {\n  binding {\n    role = \"roles/run.invoker\"\n    members = [\n      \"allUsers\",\n    ]\n  }\n}\n\nresource \"google_cloud_run_service_iam_policy\" \"noauth\" {\n  location    = google_cloud_run_service.default.location\n  project     = google_cloud_run_service.default.project\n  service     = google_cloud_run_service.default.name\n\n  policy_data = data.google_iam_policy.noauth.policy_data\n}\n`\nFinally, your URL shows your website properly:\n\nMoreover, now \"Authentication\" is \"Allow unauthenticated\":\n\nDon't forget to add the role \"Cloud Run Admin\" to your service account:\n\nOtherwise, you cannot allow unauthenticated invocations for public API or website then you will get this error below:\n\nError setting IAM policy for cloudrun service\n\"v1/projects/myproject-214771/locations/asia-northeast1/services/hello-world\":\ngoogleapi: Error 403: Permission 'run.services.setIamPolicy' denied on\nresource\n'projects/myproject-214771/locations/asia-northeast1/services/hello-world'\n(or resource may not exist).\n\nMoreover, with these roles below, you cannot allow unauthenticated invocations for public API or website:\n\nOnly the role \"Cloud Run Admin\" can allow unauthenticated invocations for public API or website.",
      "question_score": 8,
      "answer_score": 11,
      "created_at": "2022-01-21T08:06:25",
      "url": "https://stackoverflow.com/questions/70797574/terraform-cloud-run-error-forbidden-your-client-does-not-have-permission-to"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 69731500,
      "title": "Why do I get Resource&#39;s path part only allow a-zA-Z0-9._-: when trying to create an API gateway resource using Terraform?",
      "problem": "Is it possible to create and AWS API gateway resources with subdirectories from the root with Terraform?\nWhen doing\n```\n`resource \"aws_api_gateway_resource\" \"MyDemoResource\" {\n  rest_api_id = aws_api_gateway_rest_api.MyDemoAPI.id\n  parent_id   = aws_api_gateway_rest_api.MyDemoAPI.root_resource_id\n  path_part   = \"/will/this/work\"\n}\n`\n```\nI get the error:\n```\n`aws_api_gateway_resource.MyDemoResource: 1 error occurred: aws_api_gateway_resource.MyDemoResource: Error creating API Gateway Resource: BadRequestException: Resource's path part only allow a-zA-Z0-9._-: or a valid greedy path variable and curly braces at the beginning and the end.\"\n`\n```\nI've found this GitHub ticket  and a few others like it. I'm not sure if this is a bug or by design with a workaround.\nAdditionally, If I create this manually in the AWS console and then import it, the full path appears in the attribute `path` properly but not in the input `path_part`.\n```\n`\"aws_api_gateway_resource.MyDemoResource\": {\n                    \"type\": \"aws_api_gateway_resource\",\n                    \"depends_on\": [],\n                    \"primary\": {\n                        \"id\": \"foo\",\n                        \"attributes\": {\n                            \"id\": \"foo\",\n                            \"parent_id\": \"foo\",\n                            **\"path\": \"/will/this/work\"**,\n                            **\"path_part\": \"work\"**,\n                            \"rest_api_id\": \"foo\"\n                        }    \n`\n```",
      "solution": "The `path_part` is the last path segment of an API Gateway resource, not the full path. This mirrors the tree-like structure that you see in the AWS Console.\nThe full path is exported, yes, but is not taken in as an argument. That's why you have the full path in the imported Terraform state as `path` and the last path segment - `work` - as the `path_part`.\nCurrently, you're not passing the last path segment; you're passing the full path.\nWhat you need to do is specify 3 different `aws_api_gateway_resource` each for the different parts of the path segment, with the parent IDs correctly specified to create the structure you need:\n\n`will` with the parent ID of `aws_api_gateway_rest_api.{REST-API-NAME}.root_resource_id`\n`this` with the parent ID of `aws_api_gateway_resource.will.id`\n`work` with the parent ID of `aws_api_gateway_resource.this.id`\n\nVisual representation:\n```\n`MyAPI\n\u251c\u2500 /will\n\u2502  \u251c\u2500 /this\n\u2502  \u2502  \u251c\u2500 /work\n`\n```",
      "question_score": 8,
      "answer_score": 11,
      "created_at": "2021-10-27T02:53:58",
      "url": "https://stackoverflow.com/questions/69731500/why-do-i-get-resources-path-part-only-allow-a-za-z0-9-when-trying-to-create"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 73362352,
      "title": "rds unable to grant role to a user using root user",
      "problem": "I have a mysql rds instance, when you make the instance you declare a root user and a password.\nI am then using terraform to create a new user and give the user a role. However i get the following error:\n```\n` Error running SQL (GRANT 'test_role' TO 'test_user'@'%'): Error 1227: Access denied; you need (at least one of) the WITH ADMIN, ROLE_ADMIN, SUPER privilege(s) for this operation\n`\n```\nPutting terraform aside, if i attempt to assign a role to a user with mysql directly. I get the same error\n```\n`CREATE ROLE 'test_role';\nGRANT SELECT, EXECUTE ON checkpoint_gg.* TO 'test_role';\nCREATE USER 'test_user'@'%' IDENTIFIED BY 'password';\nGRANT 'test_role' TO 'test_user'@'%';\n`\n```\nSHOW GRANTS FOR 'root'@'%';\n```\n`'GRANT SELECT, INSERT, UPDATE, DELETE, CREATE, DROP, RELOAD, PROCESS, REFERENCES, INDEX, ALTER, SHOW DATABASES, CREATE TEMPORARY TABLES, LOCK TABLES, EXECUTE, REPLICATION SLAVE, REPLICATION CLIENT, CREATE VIEW, SHOW VIEW, CREATE ROUTINE, ALTER ROUTINE, CREATE USER, EVENT, TRIGGER, CREATE ROLE, DROP ROLE ON *.* TO `root`@`%` WITH GRANT OPTION'\n'GRANT APPLICATION_PASSWORD_ADMIN,BACKUP_ADMIN,FLUSH_OPTIMIZER_COSTS,FLUSH_STATUS,FLUSH_TABLES,FLUSH_USER_RESOURCES,INNODB_REDO_LOG_ARCHIVE,PASSWORDLESS_USER_ADMIN,SHOW_ROUTINE ON *.* TO `root`@`%` WITH GRANT OPTION'\n`\n```\nmysql 8",
      "solution": "I contacted aws technical support and they managed to replicate the issue and suggest a solution.\naws technical support\n\nsince RDS is a managed service, to maintain the system integrity and\nstability, super user privileges are not provided even to the master\nuser of the DB instance, and therefore, such error message is\nexpected, as the RDS MySQL master user by default does not have the\nADMIN, ROLE_ADMIN, SUPER privileges.\n\nThey suggested, interestingly enough the master/root user can assign those roles to itself.\n```\n`GRANT ROLE_ADMIN on *.* to root;\n`\n```\nOnce it has that privilege we can then grant a role to a user\n```\n`GRANT 'test_role' TO 'test_user'@'%';\n`\n```\nI did not know the master root user (not rdsadmin) could give it self admin role, when itself is not an admin or does not have super privileges.",
      "question_score": 8,
      "answer_score": 12,
      "created_at": "2022-08-15T16:23:39",
      "url": "https://stackoverflow.com/questions/73362352/rds-unable-to-grant-role-to-a-user-using-root-user"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 67232847,
      "title": "How do I supply an API token to the GitLab Terraform provider as a Terraform secret resource?",
      "problem": "I am trying to use Terraform to manage some GitLab (self-hosted) configuration.  The Terraform GitLab provider requires a GitLab Personal Access Token to be able to make API calls to read and write the configuration.  When I try to provide this token using a Terraform `secret_resource` Terraform is unable to let me manage the secret.  When I try to import the secret, Terraform fails:\n```\n`$ terraform import secret_resource.api_token \"xxx\"                                                                                        \nsecret_resource.api_token: Importing from ID \"xxx\"...\nsecret_resource.api_token: Import prepared!\n  Prepared secret_resource for import\nsecret_resource.api_token: Refreshing state... [id=-]\n\nError: GET https://gitlab.example.com./api/v4/user/api/v4/user: 404 {error: 404 Not Found}\n\n  on /path/to/providers.tf line 24, in provider \"gitlab\":                                                                                                          \n  24: provider \"gitlab\" {\n\n`\n```\nHere is the minimal Terraform that reproduces this behavior:\n```\n`terraform {\n  required_version = \"~> 0.13.6\"                                                                                     \n\n  required_providers {\n    gitlab = {\n      source = \"nixpkgs/gitlab\"\n      version = \"> 3.4.99\"                                                                                           \n    }\n    secret = {\n      source = \"nixpkgs/secret\"\n      version = \"~> 1.1\"                                                                                             \n      alias = \"default\"                                                                                              \n    }\n  }\n}\n\nresource \"secret_resource\" \"api_token\" {                                                                             \n  lifecycle {\n    prevent_destroy = true\n  }\n}\n\nprovider \"gitlab\" {                                                                                                  \n  base_url = \"https://gitlab.example.com./api/v4/user\"                                             \n  token = secret_resource.api_token.value                                                                            \n}\n\nresource \"gitlab_project\" \"foo\" {\n    name = \"foo\"\n}\n`\n```\nI've omitted the real hostname and GitLab token value.  I can reliably reproduce this failure by initializing a new Terraform root module with this configuration and then trying to import the secret.\nThis seems like an unreasonable failure - `secret_resource` does not depend on the GitLab provider.  If Terraform let the value be imported then it would be available and then the GitLab provider would be properly configured.\nI observe this behavior with:\n\nTerraform v0.13.6\n\nprovider registry.terraform.io/nixpkgs/gitlab v3.4.999 (git rev 68c8c0e4cf14fda698bcacb74cb01fcfe7128815)\nprovider registry.terraform.io/nixpkgs/secret v1.1.1\n\nI would like to be able to continue to use `secret_resource` to manage the GitLab API token.  How can I?",
      "solution": "From the error message, it seems like the `base_url` is incorrectly configured. `/api/v4/user` comes up twice:\n```\n`Error: GET https://gitlab.example.com./api/v4/user/api/v4/user: 404 {error: 404 Not Found}\n`\n```\nTry setting the `base_url` to just the hostname, with a slash:\n```\n`provider \"gitlab\" {                                                                                                  \n  base_url = \"https://gitlab.example.com/\"                                             \n  token = secret_resource.api_token.value                                                                            \n}\n`\n```",
      "question_score": 8,
      "answer_score": 6,
      "created_at": "2021-04-23T17:32:52",
      "url": "https://stackoverflow.com/questions/67232847/how-do-i-supply-an-api-token-to-the-gitlab-terraform-provider-as-a-terraform-sec"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 69834735,
      "title": "Programmatically Connecting a GitHub repo to a Google Cloud Project",
      "problem": "I'm working on a Terraform project that will set up all the GCP resources needed for a large project spanning multiple GitHub repos. My goal is to be able to recreate the cloud infrastructure from scratch completely with Terraform.\nThe issue I'm running into is in order to setup build triggers with Terraform within GCP, the GitHub repo that is setting off the trigger first needs to be connected. Currently, I've only been able to do that manually via the Google Cloud Build dashboard. I'm not sure if this is possible via Terraform or with a script but I'm looking for any solution I can automate this with. Once the projects are connected updating everything with Terraform is working fine.\nTLDR; How can I programmatically connect a GitHub project with a GCP project instead of using the dashboard?",
      "solution": "Currently there is no way to programmatically connect a GitHub repo to a Google Cloud Project. This must be done manually via Google Cloud.\nMy workaround is to manually connect an \"admin\" project, build containers and save them to that project's artifact registry, and then deploy the containers from the registry in the programmatically generated project.",
      "question_score": 8,
      "answer_score": 3,
      "created_at": "2021-11-04T06:09:22",
      "url": "https://stackoverflow.com/questions/69834735/programmatically-connecting-a-github-repo-to-a-google-cloud-project"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 67989620,
      "title": "Error pinging docker server on &quot;terraform apply&quot;",
      "problem": "I am doing the terraform tutorial and reach the step to execute `terraform apply`.\nAfter executing that command I get this error:\n```\n`WARNING: cgroup v2 is not fully supported yet, proceeding with partial confinement\n\nError: Error pinging Docker server: Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get \"http://%2Fvar%2Frun%2Fdocker.sock/_ping\": dial unix /var/run/docker.sock: connect: permission denied\n\n  on main.tf line 9, in provider \"docker\":\n   9: provider \"docker\" {\n`\n```\nThis is what I have in my configuration `main.tf`file:\n```\n`terraform {\n  required_providers {\n    docker = {\n      source = \"kreuzwerker/docker\"\n    }\n  }\n}\n\nprovider \"docker\" {\n  \n}\n\nresource \"docker_image\" \"nginx\" {\n  name         = \"nginx:latest\"\n  keep_locally = false\n}\n\nresource \"docker_container\" \"nginx\" {\n  image = docker_image.nginx.latest\n  name  = \"tutorial\"\n  ports {\n    internal = 80\n    external = 8000\n  }\n}\n`\n```\nI have tried adding `host = \"unix:///var/run/docker.sock\"` in the provider function but still get that error. I have docker and NGINX configured in my pc too.\nDoes anyone know what is causing it?",
      "solution": "When you run docker run hello-world with your user id you will see the same error that you are getting.\nThis is happening because your user doesn't have access to execute the commands of docker. Please do the following steps.\n\n`cat /etc/group`  --> There should be a docker group available if you installed docker correctly.\nAdd your userid to docker group `sudo usermod -aG docker $User_Name`\nLogout from the session and login again\n`docker run hello-world`  --> This should run error free now.\n\nNow try to apply Terraform again and everything will work.",
      "question_score": 7,
      "answer_score": 3,
      "created_at": "2021-06-15T17:54:19",
      "url": "https://stackoverflow.com/questions/67989620/error-pinging-docker-server-on-terraform-apply"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform",
      "question_id": 73146187,
      "title": "aws_launch_configuration: &quot;couldn&#39;t find resource&quot; on terraform apply",
      "problem": "I'm new to Terraform. I've tried everything I know to try. Google has not been helpful in this case.\nI'm building a complex cloud infrastructure using Terraform. This includes an auto-scaled ECS service. In order to version-control, document and simplify the modification of this infrastructure, I chose to use Terraform.\nI'm at 2 things to create from very, very many more things to create, and I've overcome many problems and learned a lot.\nHowever, there's one problem I can't overcome:\n```\n`resource \"aws_launch_configuration\" \"ecs\" {\n  depends_on = [aws_security_group.ecs, aws_iam_instance_profile.ecs, aws_key_pair.production]\n  name_prefix                 = \"${var.ecs_cluster_name}-cluster-\"\n  image_id                    = lookup(var.amis, \"us-east-2\")\n  instance_type               = \"t2.micro\"\n  security_groups             = [aws_security_group.ecs.id]\n  iam_instance_profile        = aws_iam_instance_profile.ecs.name\n  key_name                    = aws_key_pair.production.key_name\n  associate_public_ip_address = true\n  user_data                   = \"#!/bin/bash\\necho ECS_CLUSTER='${var.ecs_cluster_name}-cluster' > /etc/ecs/ecs.config\"\n  provider = aws.us-east-2\n\n  lifecycle {\n    create_before_destroy = true\n  }\n}\n`\n```\nPlanning and applying this results in:\n```\n`\u2502 Error: couldn't find resource\n\u2502\n\u2502   with aws_launch_configuration.ecs,\n\u2502   on 08_ecs.tf line 6, in resource \"aws_launch_configuration\" \"ecs\":\n\u2502    6: resource \"aws_launch_configuration\" \"ecs\" {\n\u2502\n`\n```\nWhat does this mean? What does it mean that the resource isn't found?\nIt's not the first time I've encountered this error message, but I've previously been able to solve it somehow. In this particular case, I'm at a loss, because nothing I've tried works.\nThings I've tried:\n\nSpecifying `depends_on`\nConsulted the Terraform docs on `aws_launch_configuration`\nGoogling the error message (no results related to this exact situation)\n\nSeems like I'm the first to have this problem to me.\nVersion:\n```\n`PS C:\\Users\\admin\\PycharmProjects\\my-project\\terraform> terraform version\nTerraform v1.2.1\non windows_amd64\n+ provider registry.terraform.io/hashicorp/aws v4.20.1\n+ provider registry.terraform.io/hashicorp/template v2.2.0\n`\n```\nI honestly feel like Terraform doesn't give very useful feedback on errors. Something similar to a traceback would be nice, but all I'm getting is \"Couldn't find resource,\" highlighting the \"{\" part of `\"resource \"aws_launch_configuration\" \"ecs\" {`\nAny ideas?",
      "solution": "I was experimenting the same problem. I could solve it changing the AMI. In my case that solve everything. The error it does not specify that, but was enough to apply succesfully the resource.\nBy setting the `TF_LOG` environment variable to \"DEBUG\", you can see that AWS returns a Bad Request when trying to describe the specified AMI. Thanks to @qaziqarta for this tip.",
      "question_score": 7,
      "answer_score": 16,
      "created_at": "2022-07-28T03:16:11",
      "url": "https://stackoverflow.com/questions/73146187/aws-launch-configuration-couldnt-find-resource-on-terraform-apply"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-aws",
      "question_id": 72541598,
      "title": "Using for_each with provider setting",
      "problem": "I try to create VPCs in different regions for doing this I have multiple providers set up that I have given the alias equal to its region.\nThe VPCs is set up with a for_each where the each.key is the region I want to spin up the VPC in.\nThe issue I have is that i can't find a way to use the each.key with the \"aws.\" prefix needed for the provider setting for the resource.\nThis is what I try to do:\n```\n`provider \"aws\" {\n  alias = \"eu-west-1\"\n  profile = \"Terraform\"\n  region = \"eu-west-1\"\n}\nprovider \"aws\" {\n  alias = \"eu-west-2\"\n  profile = \"Terraform\"\n  region = \"eu-west-2\"\n}\n\nlocals {\n  pools = {\n    \"eu-west-1\" = \"${data.aws_vpc_ipam_pool.pooleu-west-1.id}\"\n    \"eu-west-2\" = \"${data.aws_vpc_ipam_pool.pooleu-west-2.id}\"\n  }\n}\n\nresource \"aws_vpc\" \"default\" {\n  for_each = local.pools\n  provider = aws.${each.key}\n  ipv4_ipam_pool_id = each.value\n  enable_dns_support   = true\n  enable_dns_hostnames = true\n\n  tags = {\n    Name = \"main-vpc-${each.key}\"\n  }\n}\n`\n```\nCan this be done or should I try to find another solution?\nI cant find any answers to this and I am to under experienced with terraform to know this.",
      "solution": "After a lot of resource I have found this thread on the topic and it's not possible (yet and maybe never) to assign the provider dynamically...\nYou can read more here:\nhttps://github.com/hashicorp/terraform/issues/24476",
      "question_score": 9,
      "answer_score": 14,
      "created_at": "2022-06-08T09:23:23",
      "url": "https://stackoverflow.com/questions/72541598/using-for-each-with-provider-setting"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-aws",
      "question_id": 75571030,
      "title": "Terraform : Error reason: The ARN isn&#39;t valid. A valid ARN begins with arn: and includes other information separated by colons or slashes",
      "problem": "I am attempting to integrate the `aws_wafv2_web_acl_logging_configuration` resource with the `aws_cloudwatch_log_group` resource in my Terraform configuration. However, I am encountering an error that states:\n```\n`Error reason: The ARN isn't valid. A valid ARN begins with arn: and includes other information separated by colons or slashes\n`\n```\nAccording to the error `aws_cloudwatch_log_group` `arn` is incorrect.\nBut I followed correct format according to the Terraform documentation.\nhttps://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/wafv2_web_acl_logging_configuration\nAnybody knows the reason for this error?\nMy code as below.\n```\n`resource \"aws_cloudwatch_log_group\" \"test_waf_log_group\" {\n  name              = var.waf_log_group_name\n  retention_in_days = 14\n\n}\n\nresource \"aws_wafv2_web_acl_logging_configuration\" \"log_test_waf\" {\n  depends_on = [aws_cloudwatch_log_group.test_waf_log_group]\n\n  log_destination_configs = [aws_cloudwatch_log_group.test_waf_log_group.arn]\n  resource_arn            = aws_wafv2_web_acl.test_waf.arn\n\n}\n`\n```",
      "solution": "`var.waf_log_group_name` can't be a random name. It must  must include `aws-waf-logs-` as explained in the AWS docs.",
      "question_score": 7,
      "answer_score": 16,
      "created_at": "2023-02-26T10:20:24",
      "url": "https://stackoverflow.com/questions/75571030/terraform-error-reason-the-arn-isnt-valid-a-valid-arn-begins-with-arn-and"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-aws",
      "question_id": 67311019,
      "title": "Terraform 0.15.1 Multiple Provider Issue - The argument &quot;region&quot; is required, but was not set",
      "problem": "so below is my project file structure:\n```\n`\u251c\u2500\u2500 main.tf\n\u251c\u2500\u2500 tunnel\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 main.tf\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 variables.tf\n\u2514\u2500\u2500 variables.tf\n`\n```\nI am trying to use multiple providers in Terraform 0.15.1 as described here -> https://www.terraform.io/docs/language/modules/develop/providers.html\nAfter following the example I am not able to get it work. I have now simplified my code down to just use one single provider alias(keep it as simple as possible). The error I am getting is:\n```\n`\u2577\n\u2502 Error: Missing required argument\n\u2502 \n\u2502 The argument \"region\" is required, but was not set.\n\u2575\n`\n```\nMy main.tf file in root directory:\n```\n`module \"tunnel\" {\n  source    = \"./tunnel\"\n  providers = {\n    aws.r = aws.requester\n  }\n}\n`\n```\nmy variables.tf in root directory:\n```\n`provider \"aws\" {\n  alias  = \"requester\"\n  region = \"ap-southeast-2\"\n  profile = \"benchmark\"\n}\n`\n```\nmy tunnel/variables.tf file:\n```\n`terraform {\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \">= 2.7.0\"\n      configuration_aliases = [ aws.r ]\n    }\n  }\n}\n\ndata \"aws_region\" \"current\" {}\n\ndata \"aws_caller_identity\" \"current\" {}\n`\n```\nmy tunnel/main.tf file:\n```\n`# Requester's side of the connection.\nresource \"aws_vpc_peering_connection\" \"peer\" {\n  vpc_id        = \"vpc-xxxxxxxxxxxxxxxxx\"\n  peer_vpc_id   = \"vpc-xxxxxxxxxxxxxxxxx\"\n  peer_owner_id = data.aws_caller_identity.current.account_id\n  peer_region   = data.aws_region.current.name\n  auto_accept   = false\n\n  tags = {\n    Side = \"Requester\"\n  }\n}\n`\n```\nI don't understand why I am getting this error? The goal with this code eventually is to automate both sides of the vpc peering as shown here -> https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/vpc_peering_connection_accepter . But I am currently stuck on getting two aws providers working with different credentials (one provider in the example above to simplify things).\nWhen I remove `alias  = \"requester\"` from the root main.tf:\n```\n`provider \"aws\" {\n//  alias  = \"requester\"\n  region = \"ap-southeast-2\"\n  profile = \"benchmark\"\n}\n`\n```\nand the provider config from main.tf in root path:\n```\n`module \"tunnel\" {\n  source    = \"./tunnel\"\n//  providers = {\n//    aws.r = aws.requester\n//  }\n}\n`\n```\nand the alias config from tunnel/variables.tf:\n```\n`terraform {\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \">= 2.7.0\"\n//      configuration_aliases = [ aws.r ]\n    }\n  }\n}\n`\n```\nplan works fine:\n```\n`Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols:\n  + create\n\nTerraform will perform the following actions:\n\n  # module.tunnel.aws_vpc_peering_connection.peer will be created\n  + resource \"aws_vpc_peering_connection\" \"peer\" {\n      + accept_status = (known after apply)\n      + auto_accept   = false\n      + id            = (known after apply)\n      + peer_owner_id = \"xxxxxxx\"\n      + peer_region   = \"ap-southeast-2\"\n      + peer_vpc_id   = \"vpc-xxxxxxxxxxxxxxxxx\"\n      + tags          = {\n          + \"Side\" = \"Requester\"\n        }\n      + vpc_id        = \"vpc-xxxxxxxxxxx\"\n\n      + accepter {\n          + allow_classic_link_to_remote_vpc = (known after apply)\n          + allow_remote_vpc_dns_resolution  = (known after apply)\n          + allow_vpc_to_remote_classic_link = (known after apply)\n        }\n\n      + requester {\n          + allow_classic_link_to_remote_vpc = (known after apply)\n          + allow_remote_vpc_dns_resolution  = (known after apply)\n          + allow_vpc_to_remote_classic_link = (known after apply)\n        }\n    }\n\nPlan: 1 to add, 0 to change, 0 to destroy.\n`\n```",
      "solution": "This error message is the result of a few automatic behaviors Terraform has to make simpler cases simpler, but which unfortunately lead to the situation being rather unclear in more complicated situations like yours.\nIn your child module you've declared that it expects an alternative (aliased) provider configuration to be passed in the caller, which within this module will be known as `aws.r`. However, the data resources you declared afterwards don't include a `provider` argument to specify which provider configuration they belong to, and so Terraform is choosing the default (unaliased) provider configuration.\nUnfortunately, your configuration doesn't actually have a default provider configuration, because the root module is also using an alternative provider configuration `aws.requester`. As a result, Terraform is automatically constructing one with an empty configuration, because that's a useful behavior for simple providers like `http` which don't require any special configuration. But that doesn't end up working for the `aws` provider, because it requires `region` to be set.\nThere are at least two different ways you could change the child module to make this work. Which of these will be most appropriate will depend on how this module fits in with your broader configuration.\n\nThe first option would be to have the child module not declare `aws.r` at all, and just use its default provider configuration throughout:\n```\n`terraform {\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \">= 2.7.0\"\n    }\n  }\n}\n\ndata \"aws_region\" \"current\" {}\n\ndata \"aws_caller_identity\" \"current\" {}\n`\n```\nBecause your root module doesn't have a default configuration for `hashicorp/aws` you'll still need to be explicit in the module call that the default provider for the module is the `aws.requester` provider as the root module sees it:\n```\n`module \"tunnel\" {\n  source    = \"./tunnel\"\n  providers = {\n    aws = aws.requester\n  }\n}\n`\n```\nThis approach is a good choice for a shared module that doesn't need more than one AWS provider configuration, because the module itself can then be totally unaware of the multiple configurations in its caller, and instead just expect to be given a default configuration for `hashicorp/aws` to use.\nHowever, it wouldn't work if your child module needs to have more than one configuration for `hashicorp/aws` too. In that case, you'll need the other option I'll describe next.\n\nWhen a shared module will work with more than one provider configuration, we need to declare a `configuration_aliases` entry for each configuration it expects to be passed from its caller. You only showed one called `r` in your examples, and I don't know what \"r\" represents, so for the sake of examples here I'm going to call them \"src\" (for \"source\") and \"dst\"  for (\"destination\") just to have some meaningful terms to hook onto for the sake of example.\nWe'll start with the `configuration_aliases` configuration:\n```\n`terraform {\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \">= 2.7.0\"\n      configuration_aliases = [ aws.src, aws.dst ]\n    }\n  }\n}\n`\n```\nEach of the items given in `configuration_aliases` declares a configuration address that must have a corresponding entry in the `providers` argument in any call to this module, which we'll see later.\nSince this module is not expecting to use a default (unaliased) configuration, we are now required to tell Terraform for each resource block which provider configuration it belongs to. Starting with your data resources, let's assume they belong to the \"source\" end:\n```\n`data \"aws_region\" \"current\" {\n  provider = aws.src\n}\n\ndata \"aws_caller_identity\" \"current\" {\n  provider = aws.src\n}\n`\n```\nI suspect that in your real system the `aws_vpc_peering_connection` resource you showed probably logically belongs to the \"source\" side too, but since you didn't show any other resources I'm just going to arbitrarily assign it to `aws.dst` to show what that looks like:\n```\n`resource \"aws_vpc_peering_connection\" \"peer\" {\n  provider = aws.dst\n\n  vpc_id        = \"vpc-xxxxxxxxxxxxxxxxx\"\n  peer_vpc_id   = \"vpc-xxxxxxxxxxxxxxxxx\"\n  peer_owner_id = data.aws_caller_identity.current.account_id\n  peer_region   = data.aws_region.current.name\n  auto_accept   = false\n\n  tags = {\n    Side = \"Requester\"\n  }\n}\n`\n```\nEvery `data` and `resource` block in that module will need to have `provider` set, because there is no default provider configuration to select by default in this module.\nWhen you call the module, you'll need to tell Terraform which of the provider configurations in the caller map to the `src` and `dst` configurations in the called module:\n```\n`module \"tunnel\" {\n  source    = \"./tunnel\"\n  providers = {\n    aws.src = aws.requester\n    aws.dst = aws.peer\n  }\n}\n`\n```\nAs I mentioned earlier, we need one entry in `providers` for each of the `configuration_aliases` declared inside the module. It might be helpful to think of these alternative provider configurations as being somewhat similar to input variables, but they have a more specialized syntax for declaration and definition because providers are so fundamental to Terraform's execution model and need to be resolved before normal expression evaluation is possible.\nHere I just arbitrarily chose \"peer\" as the name for a presumed second configuration you have declared in your root module. It's also valid to assign a default configuration in the caller to an alternative configuration in the called module, like `aws.src = aws`, but that doesn't seem to apply in your situation because you don't have a default configuration in the root module either.",
      "question_score": 7,
      "answer_score": 14,
      "created_at": "2021-04-29T06:23:27",
      "url": "https://stackoverflow.com/questions/67311019/terraform-0-15-1-multiple-provider-issue-the-argument-region-is-required-bu"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-aws",
      "question_id": 68940627,
      "title": "How to Terraform Create and Validate AWS Certificate",
      "problem": "I am attempting to create and validate an AWS Certificate using Terraform by following the example from the Terraform documentation here: https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/acm_certificate_validation#dns-validation-with-route-53\nMy Terraform file looks like:\n```\n`resource \"aws_acm_certificate\" \"vpn_server\" {\n  domain_name = \"stuff.mine.com\"\n  \n  validation_method = \"DNS\"\n\n  tags = {\n    Name = \"certificate\"\n    Scope = \"vpn_server\"\n    Environment = \"vpn\"\n  }\n}\n\nresource \"aws_acm_certificate_validation\" \"vpn_server\" {\n  certificate_arn = aws_acm_certificate.vpn_server.arn\n\n  validation_record_fqdns = [for record in aws_route53_record.my_dns_record_vpn_server : record.fqdn]\n\n  timeouts {\n    create = \"2m\"\n  }\n}\n\nresource \"aws_route53_zone\" \"my_dns\" {\n  name = \"stuff.mine.com\"\n\n  tags = {\n    name = \"dns_zone\"\n  }\n}\n\nresource \"aws_route53_record\" \"my_dns_record_vpn_server\" {\n  for_each = {\n    for dvo in aws_acm_certificate.vpn_server.domain_validation_options : dvo.domain_name => {\n      name   = dvo.resource_record_name\n      record = dvo.resource_record_value\n      type   = dvo.resource_record_type\n    }\n  }\n\n  allow_overwrite = true\n  name            = each.value.name\n  records         = [each.value.record]\n  ttl             = 60\n  type            = each.value.type\n  zone_id         = resource.aws_route53_zone.my_dns.zone_id\n}\n`\n```\nThe problem is that when running `terraform apply` the Validation always reaches the time-out and fails with the error messages:\n```\n`aws_acm_certificate.vpn_server: Creating...\naws_acm_certificate.vpn_server: Creation complete after 8s [id=arn:aws:acm:eu-west-2:320289993971:certificate/7e859491-141f-49d5-b50e-c44cf4e1db4e]\naws_route53_zone.my_dns: Creating...\naws_route53_zone.my_dns: Still creating... [10s elapsed]\naws_route53_zone.my_dns: Creation complete after 52s [id=Z09112516IIP4OEAIIQ7]\naws_route53_record.my_dns_record_vpn_server[\"stuff.mine.com\"]: Creating...\naws_route53_record.my_dns_record_vpn_server[\"stuff.mine.com\"]: Still creating... [10s elapsed]\naws_route53_record.my_dns_record_vpn_server[\"stuff.mine.com\"]: Still creating... [20s elapsed]\naws_route53_record.my_dns_record_vpn_server[\"stuff.mine.com\"]: Still creating... [30s elapsed]\naws_route53_record.my_dns_record_vpn_server[\"stuff.mine.com\"]: Still creating... [40s elapsed]\naws_route53_record.my_dns_record_vpn_server[\"stuff.mine.com\"]: Still creating... [50s elapsed]\naws_route53_record.my_dns_record_vpn_server[\"stuff.mine.com\"]: Creation complete after 58s [id=Z09112516IIP4OEAIIQ7__ebd2853fcbfc7cc8bd6582e65d940d54.stuff.mine.com._CNAME]\naws_acm_certificate_validation.vpn_server: Creating...\naws_acm_certificate_validation.vpn_server: Still creating... [10s elapsed]\naws_acm_certificate_validation.vpn_server: Still creating... [20s elapsed]\naws_acm_certificate_validation.vpn_server: Still creating... [30s elapsed]\naws_acm_certificate_validation.vpn_server: Still creating... [40s elapsed]\naws_acm_certificate_validation.vpn_server: Still creating... [50s elapsed]\naws_acm_certificate_validation.vpn_server: Still creating... [1m0s elapsed]\naws_acm_certificate_validation.vpn_server: Still creating... [1m10s elapsed]\naws_acm_certificate_validation.vpn_server: Still creating... [1m20s elapsed]\naws_acm_certificate_validation.vpn_server: Still creating... [1m30s elapsed]\naws_acm_certificate_validation.vpn_server: Still creating... [1m40s elapsed]\naws_acm_certificate_validation.vpn_server: Still creating... [1m50s elapsed]\naws_acm_certificate_validation.vpn_server: Still creating... [2m0s elapsed]\n\n\u2577\n\u2502 Error: Error describing created certificate: Expected certificate to be issued but was in state PENDING_VALIDATION\n\u2502\n\u2502   with aws_acm_certificate_validation.vpn_server,\n\u2502   on main.tf line 61, in resource \"aws_acm_certificate_validation\" \"vpn_server\":\n\u2502   61: resource \"aws_acm_certificate_validation\" \"vpn_server\" {\n\u2502\n\u2575\n`\n```\nCan someone tell me what I am missing to get the Certificate Validation to complete?",
      "solution": "The domain validation records need to be in a public zone that is properly delegated. So if you owned `mine.com` and then wanted to create a zone called `stuff.mine.com` then you would need to set `NS` records in `mine.com` for `stuff.mine.com` that points to the `stuff.mine.com` zone's NS servers which you aren't doing here and aren't using an already configured zone.\nWithout that, the records will be created in your zone but that zone isn't then properly delegated and so nothing will ever be able to resolve those records. You should be able to test this by attempting to resolve them yourself or using an external resolver tool such as MX Toolbox.\nThere's probably a lot to consider here but you might want to set up a zone that will contain the eventual records you want to create (so the record pointing to the web server/load balancer that you want the certificate for plus the ACM domain validation records) separately and then just refer to the zone by using the `aws_route53_zone` data source so your domain validation records are created there.",
      "question_score": 7,
      "answer_score": 5,
      "created_at": "2021-08-26T16:39:24",
      "url": "https://stackoverflow.com/questions/68940627/how-to-terraform-create-and-validate-aws-certificate"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-aws",
      "question_id": 70798260,
      "title": "Terraform: How to modify a public subnet&#39;s route table that was created by module &#39;vpc&#39;?",
      "problem": "I used the `vpc` module to create my VPC via the following code:\n```\n`module \"vpc\" {\n  source = \"terraform-aws-modules/vpc/aws\"\n  name = \"${var.namespace}-vpc\"\n  cidr = \"10.0.0.0/16\"\n  azs = data.aws_availability_zones.available.names\n  private_subnets = [\"10.0.1.0/24\", \"10.0.2.0/24\"]\n  public_subnets = [\"10.0.101.0/24\", \"10.0.102.0/24\"]\n  #assign_generated_ipv6_cidr_block = true\n  create_database_subnet_group     = true\n  enable_nat_gateway               = true\n  single_nat_gateway               = true\n  enable_dns_hostnames = true\n  enable_dns_support   = true\n}\n`\n```\nThis module automatically creates two public subnets that has a route table that points to an internet gateway. However, I would like to modify one of the two public subnets to have a different route table that points to a firewall that I have created.\nWhat I did was to create a new route table `pub_to_firewall`, and then create a new `aws_route_table_association` to associate the public subnet with the new route table.\n```\n`resource \"aws_route_table_association\" \"sn_to_fw_rt_association\" {\n  subnet_id      = module.vpc.public_subnets[0]\n  route_table_id = aws_route_table.pub_to_firewall.id\n  depends_on = [\n    aws_route_table.pub_to_firewall,\n  ]\n}\n`\n```\nI have been able to follow the instructions to import the original association to this new association, and `terraform apply` to get the public subnet to have this new route table containing the firewall reference.\nHowever, when I run `terraform apply` again, terraform now wants to go back to the 'default' associations:\n```\n`Objects have changed outside of Terraform\n\nTerraform detected the following changes made outside of Terraform since the last \"terraform apply\":\n\n  # module.networking.module.vpc.aws_route_table_association.public[0] has been deleted\n  - resource \"aws_route_table_association\" \"public\" {\n      - id             = \"rtbassoc-[ ]\" -> null\n      - route_table_id = \"rtb-0cabc2388adXXXXX\" -> null\n      - subnet_id      = \"subnet-0a2b011cd7aXXXXX\" -> null\n    }\n\nUnless you have made equivalent changes to your configuration, or ignored the relevant attributes using ignore_changes, the following plan may include actions to undo or respond to these\nchanges.\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\nTerraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols:\n  + create\n  ~ update in-place\n\nTerraform will perform the following actions:\n\n  # module.networking.module.vpc.aws_route_table_association.public[0] will be created\n  + resource \"aws_route_table_association\" \"public\" {\n      + id             = (known after apply)\n      + route_table_id = \"rtb-0cabc2388adXXXXX\"\n      + subnet_id      = \"subnet-0a2b011cd73XXXXX\"\n    }\n`\n```\nI do not want this resource to be recreated because it would throw an error that `\u2502 Error: error creating Route Table (rtb-0cabc2388adXXXXX) Association: Resource.AlreadyAssociated: the specified association for route table rtb-0cabc2388adXXXXX conflicts with an existing association` obviously since I already associated it with the new routing table.\nHow can I either:\n\nForce terraform to 'ignore' the default subnet to routing tables setup\nOr update the `vpc` created aws_route_table_association resource `module.networking.module.vpc.aws_route_table_association.public[0]` to reference the new route table instead?",
      "solution": "You can't change that, as this is how the aws vpc module works. You need custom designed VPC for that. So you have to either fork the entire module and made the changes that you want, or create new VPC module from scratch tailored to your needs.",
      "question_score": 7,
      "answer_score": 4,
      "created_at": "2022-01-21T09:19:45",
      "url": "https://stackoverflow.com/questions/70798260/terraform-how-to-modify-a-public-subnets-route-table-that-was-created-by-modul"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-aws",
      "question_id": 70002403,
      "title": "MalformedPolicyDocument: Policy document should not specify a principal",
      "problem": "I am trying to create a state function with terraform. First I create a policy and assign it to an existing role `processing_lambda_role`.\n```\n`resource \"aws_iam_role_policy\" \"sfn_policy\" {\n  policy = jsonencode(\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"Service\": \"states.amazonaws.com\"\n      },\n      \"Action\": \"sts:AssumeRole\"\n    },\n    {\n            \"Sid\": \"VisualEditor0\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"lambda:InvokeFunction\",\n                \"lambda:InvokeAsync\"\n            ],\n            \"Resource\": \"*\"\n        }\n  ]\n}\n  )\n  role = aws_iam_role.processing_lambda_role.id\n}\n\nresource \"aws_sfn_state_machine\" \"sfn_state_machine\" {\n  name     = local.step_function_name\n  role_arn = aws_iam_role.processing_lambda_role.arn\n\n  definition = I get this error:\n```\n`Error: Error putting IAM role policy terraform-20211117095209110000000005: MalformedPolicyDocument: Policy document should not specify a principal.\n\u2502       status code: 400, request id: 1dd8ac18-a514-4ef3-93ae-91383e5baa07\n\u2502 \n\u2502   with module.ingest_system[\"ems\"].aws_iam_role_policy.sfn_policy,\n\u2502   on ../../modules/ingest_system/step_function.tf line 1, in resource \"aws_iam_role_policy\" \"sfn_policy\":\n\u2502    1: resource \"aws_iam_role_policy\" \"sfn_policy\" {\n`\n```\nand that's how the role was originally defined:\n```\n`resource \"aws_iam_role\" \"processing_lambda_role\" {\n  name = local.processing_lambda_role_name\n  path = \"/service-role/\"\n\n  assume_role_policy = jsonencode({\n    Version   = \"2012-10-17\"\n    Statement = [\n      {\n        Effect    = \"Allow\"\n        Principal = { Service = \"lambda.amazonaws.com\" }\n        Action    = \"sts:AssumeRole\"\n      }\n    ]\n  })\n}\n`\n```",
      "solution": "`sts:AssumeRole` should be in role's `assume_role_policy`. For example, if you want to create `sfn_role` for your sfn, then:\n```\n`\nresource \"aws_iam_role\" \"sfn_role\" {\n  assume_role_policy = jsonencode({\n    Version   = \"2012-10-17\"\n    Statement = [\n      {\n        Effect    = \"Allow\"\n        Principal = { Service = \"states.amazonaws.com\" }\n        Action    = \"sts:AssumeRole\"\n      }\n    ]\n  })\n}\n\nresource \"aws_iam_role_policy\" \"sfn_policy\" {\n  policy = jsonencode(\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [    \n    {\n            \"Sid\": \"VisualEditor0\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"lambda:InvokeFunction\",\n                \"lambda:InvokeAsync\"\n            ],\n            \"Resource\": \"*\"\n        }\n  ]\n}\n  )\n  role = aws_iam_role.sfn_role.id\n}\n\nresource \"aws_sfn_state_machine\" \"sfn_state_machine\" {\n  name     = local.step_function_name\n  role_arn = aws_iam_role.sfn_role.arn\n  # ....\n}\n\n`\n```",
      "question_score": 7,
      "answer_score": 6,
      "created_at": "2021-11-17T11:07:52",
      "url": "https://stackoverflow.com/questions/70002403/malformedpolicydocument-policy-document-should-not-specify-a-principal"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-aws",
      "question_id": 67483448,
      "title": "AWS Config - Resource discovery stuck on &quot;Your resources are being discovered&quot;",
      "problem": "My company has 2 AWS accounts. On the first (lets call it playground), I have full administrative permissions. On the second (lets call it production) I have limited IAM permissions\nI enabled AWS Config (using the terraform file on the appendix) on both accounts.\n\nOn the playground it runs smoothly, everything is fine.\nOne the production, it fails. More specifically, it fails to detect the account's resources with the message \"Your resources are being discovered\" as shown in the screenshot below.\n\nI initially suspected this could be an IAM role permission issue.\ne.g running\n`aws configservice list-discovered-resources --resource-type AWS::EC2::SecurityGroup --profile playground`\ngives me a list of the SecurityGroups discovered by the AWS Config on the playground (pretty much what I see on the console dashboard).\nOn the other hand:\n`aws configservice list-discovered-resources --resource-type AWS::EC2::SecurityGroup --profile production` returns a null list (there are security groups though. Same results with other types such as `AWS::EC2::Instance`)\n```\n`{\n    \"resourceIdentifiers\": []\n}\n`\n```\nSince the IAM role does have the rights to make describe API calls, I discarded the IAM permission suspicion. It works. It is just that it returns null.\nCould it be the AWS Config role `AWSServiceRoleForConfig`? It does not make sense. Since this is a Service Linked Role it should by default have all the required permissions. (Will append the policy at the end of the post nevertheless)\nNow the weird part:\nMy rules validate some resources (e.g EFS) but throw this message: `The specified resource is either unknown or has not been discovered.`\nI am still suspecting this might be an IAM issue, but I can't figure out what is going on. I've been strugglingwith this for days, I could really use some help here.\nAccording to the official docs:\n\nAWS Config keeps track of all changes to your resources by invoking the Describe or the List API call for each resource in your account. The service uses those same API calls to capture configuration details for all related resources.\n\nconfig.tf\n```\n`# Create the configuration recorder\nresource \"aws_config_configuration_recorder\" \"default\" {\n    name     = \"default-recorder\"\n    role_arn = \"arn:aws:iam::${var.account_id}:role/aws-service-role/config.amazonaws.com/AWSServiceRoleForConfig\"\n    recording_group {\n        all_supported                 = true\n        include_global_resource_types = true\n    }\n}\n\n# Enable the configuration recorder\nresource \"aws_config_configuration_recorder_status\" \"default\" {\n  name       = aws_config_configuration_recorder.default.name\n  is_enabled = true\n  depends_on = [aws_config_delivery_channel.default]\n}\n\n# Connect AWS Config to the S3 bucket\nresource \"aws_config_delivery_channel\" \"default\" {\n  name           = \"default-channel\"\n  s3_bucket_name = \"central-config-bucket\" # Central S3 bucket \n  depends_on     = [aws_config_configuration_recorder.default]\n}\n\n# Deploy the default HIPAA compliance comformance pack\nresource \"aws_config_conformance_pack\" \"hipaa\" {\n  name = \"operational-best-practices-for-HIPAA-Security\"\n  template_body = data.http.conformance_pack.body\n}\n\ndata \"http\" \"conformance_pack\" {\n  url = \"https://raw.githubusercontent.com/awslabs/aws-config-rules/master/aws-config-conformance-packs/Operational-Best-Practices-for-HIPAA-Security.yaml\"\n}\n\nresource \"aws_config_aggregate_authorization\" \"main\" {\n  account_id = \"************\" \n  region     = \"eu-central-1\"\n}\n`\n```\nThe default AWSServiceRoleForConfig policy:\n```\n`{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"acm:DescribeCertificate\",\n                \"acm:ListCertificates\",\n                \"acm:ListTagsForCertificate\",\n                \"apigateway:GET\",\n                \"application-autoscaling:DescribeScalableTargets\",\n                \"application-autoscaling:DescribeScalingPolicies\",\n                \"autoscaling:DescribeAutoScalingGroups\",\n                \"autoscaling:DescribeLaunchConfigurations\",\n                \"autoscaling:DescribeLifecycleHooks\",\n                \"autoscaling:DescribePolicies\",\n                \"autoscaling:DescribeScheduledActions\",\n                \"autoscaling:DescribeTags\",\n                \"backup:DescribeBackupVault\",\n                \"backup:DescribeRecoveryPoint\",\n                \"backup:GetBackupPlan\",\n                \"backup:GetBackupSelection\",\n                \"backup:GetBackupVaultAccessPolicy\",\n                \"backup:GetBackupVaultNotifications\",\n                \"backup:ListBackupPlans\",\n                \"backup:ListBackupSelections\",\n                \"backup:ListBackupVaults\",\n                \"backup:ListRecoveryPointsByBackupVault\",\n                \"backup:ListTags\",\n                \"cloudformation:DescribeType\",\n                \"cloudformation:ListTypes\",\n                \"cloudfront:ListTagsForResource\",\n                \"cloudtrail:DescribeTrails\",\n                \"cloudtrail:GetEventSelectors\",\n                \"cloudtrail:GetTrailStatus\",\n                \"cloudtrail:ListTags\",\n                \"cloudwatch:DescribeAlarms\",\n                \"codepipeline:GetPipeline\",\n                \"codepipeline:GetPipelineState\",\n                \"codepipeline:ListPipelines\",\n                \"config:BatchGet*\",\n                \"config:Describe*\",\n                \"config:Get*\",\n                \"config:List*\",\n                \"config:Put*\",\n                \"config:Select*\",\n                \"dax:DescribeClusters\",\n                \"dms:DescribeReplicationInstances\",\n                \"dms:DescribeReplicationSubnetGroups\",\n                \"dms:ListTagsForResource\",\n                \"dynamodb:DescribeContinuousBackups\",\n                \"dynamodb:DescribeLimits\",\n                \"dynamodb:DescribeTable\",\n                \"dynamodb:ListTables\",\n                \"dynamodb:ListTagsOfResource\",\n                \"ec2:Describe*\",\n                \"ec2:GetEbsEncryptionByDefault\",\n                \"ecr:DescribeRepositories\",\n                \"ecr:GetLifecyclePolicy\",\n                \"ecr:GetRepositoryPolicy\",\n                \"ecr:ListTagsForResource\",\n                \"ecs:DescribeClusters\",\n                \"ecs:DescribeServices\",\n                \"ecs:DescribeTaskDefinition\",\n                \"ecs:DescribeTaskSets\",\n                \"ecs:ListClusters\",\n                \"ecs:ListServices\",\n                \"ecs:ListTagsForResource\",\n                \"ecs:ListTaskDefinitions\",\n                \"eks:DescribeCluster\",\n                \"eks:DescribeNodegroup\",\n                \"eks:ListClusters\",\n                \"eks:ListNodegroups\",\n                \"elasticache:DescribeCacheClusters\",\n                \"elasticache:DescribeCacheParameterGroups\",\n                \"elasticache:DescribeCacheSubnetGroups\",\n                \"elasticache:DescribeReplicationGroups\",\n                \"elasticfilesystem:DescribeAccessPoints\",\n                \"elasticfilesystem:DescribeBackupPolicy\",\n                \"elasticfilesystem:DescribeFileSystemPolicy\",\n                \"elasticfilesystem:DescribeFileSystems\",\n                \"elasticfilesystem:DescribeLifecycleConfiguration\",\n                \"elasticfilesystem:DescribeMountTargets\",\n                \"elasticfilesystem:DescribeMountTargetSecurityGroups\",\n                \"elasticloadbalancing:DescribeListeners\",\n                \"elasticloadbalancing:DescribeLoadBalancerAttributes\",\n                \"elasticloadbalancing:DescribeLoadBalancerPolicies\",\n                \"elasticloadbalancing:DescribeLoadBalancers\",\n                \"elasticloadbalancing:DescribeRules\",\n                \"elasticloadbalancing:DescribeTags\",\n                \"elasticmapreduce:DescribeCluster\",\n                \"elasticmapreduce:DescribeSecurityConfiguration\",\n                \"elasticmapreduce:GetBlockPublicAccessConfiguration\",\n                \"elasticmapreduce:ListClusters\",\n                \"elasticmapreduce:ListInstances\",\n                \"es:DescribeElasticsearchDomain\",\n                \"es:DescribeElasticsearchDomains\",\n                \"es:ListDomainNames\",\n                \"es:ListTags\",\n                \"guardduty:GetDetector\",\n                \"guardduty:GetFindings\",\n                \"guardduty:GetMasterAccount\",\n                \"guardduty:ListDetectors\",\n                \"guardduty:ListFindings\",\n                \"iam:GenerateCredentialReport\",\n                \"iam:GetAccountAuthorizationDetails\",\n                \"iam:GetAccountPasswordPolicy\",\n                \"iam:GetAccountSummary\",\n                \"iam:GetCredentialReport\",\n                \"iam:GetGroup\",\n                \"iam:GetGroupPolicy\",\n                \"iam:GetPolicy\",\n                \"iam:GetPolicyVersion\",\n                \"iam:GetRole\",\n                \"iam:GetRolePolicy\",\n                \"iam:GetUser\",\n                \"iam:GetUserPolicy\",\n                \"iam:ListAttachedGroupPolicies\",\n                \"iam:ListAttachedRolePolicies\",\n                \"iam:ListAttachedUserPolicies\",\n                \"iam:ListEntitiesForPolicy\",\n                \"iam:ListGroupPolicies\",\n                \"iam:ListGroupsForUser\",\n                \"iam:ListInstanceProfilesForRole\",\n                \"iam:ListPolicyVersions\",\n                \"iam:ListRolePolicies\",\n                \"iam:ListUserPolicies\",\n                \"iam:ListVirtualMFADevices\",\n                \"kinesis:DescribeStreamSummary\",\n                \"kinesis:ListStreams\",\n                \"kinesis:ListTagsForStream\",\n                \"kms:DescribeKey\",\n                \"kms:GetKeyPolicy\",\n                \"kms:GetKeyRotationStatus\",\n                \"kms:ListKeys\",\n                \"kms:ListResourceTags\",\n                \"lambda:GetAlias\",\n                \"lambda:GetFunction\",\n                \"lambda:GetPolicy\",\n                \"lambda:ListAliases\",\n                \"lambda:ListFunctions\",\n                \"logs:DescribeLogGroups\",\n                \"organizations:DescribeOrganization\",\n                \"rds:DescribeDBClusters\",\n                \"rds:DescribeDBClusterSnapshotAttributes\",\n                \"rds:DescribeDBClusterSnapshots\",\n                \"rds:DescribeDBInstances\",\n                \"rds:DescribeDBSecurityGroups\",\n                \"rds:DescribeDBSnapshotAttributes\",\n                \"rds:DescribeDBSnapshots\",\n                \"rds:DescribeDBSubnetGroups\",\n                \"rds:DescribeEventSubscriptions\",\n                \"rds:ListTagsForResource\",\n                \"redshift:DescribeClusterParameterGroups\",\n                \"redshift:DescribeClusterParameters\",\n                \"redshift:DescribeClusters\",\n                \"redshift:DescribeClusterSecurityGroups\",\n                \"redshift:DescribeClusterSnapshots\",\n                \"redshift:DescribeClusterSubnetGroups\",\n                \"redshift:DescribeEventSubscriptions\",\n                \"redshift:DescribeLoggingStatus\",\n                \"route53:GetHostedZone\",\n                \"route53:ListHostedZones\",\n                \"route53:ListHostedZonesByName\",\n                \"route53:ListResourceRecordSets\",\n                \"route53:ListTagsForResource\",\n                \"s3:GetAccelerateConfiguration\",\n                \"s3:GetAccessPoint\",\n                \"s3:GetAccessPointPolicy\",\n                \"s3:GetAccessPointPolicyStatus\",\n                \"s3:GetAccountPublicAccessBlock\",\n                \"s3:GetBucketAcl\",\n                \"s3:GetBucketCORS\",\n                \"s3:GetBucketLocation\",\n                \"s3:GetBucketLogging\",\n                \"s3:GetBucketNotification\",\n                \"s3:GetBucketObjectLockConfiguration\",\n                \"s3:GetBucketPolicy\",\n                \"s3:GetBucketPublicAccessBlock\",\n                \"s3:GetBucketRequestPayment\",\n                \"s3:GetBucketTagging\",\n                \"s3:GetBucketVersioning\",\n                \"s3:GetBucketWebsite\",\n                \"s3:GetEncryptionConfiguration\",\n                \"s3:GetLifecycleConfiguration\",\n                \"s3:GetReplicationConfiguration\",\n                \"s3:ListAccessPoints\",\n                \"s3:ListAllMyBuckets\",\n                \"s3:ListBucket\",\n                \"sagemaker:DescribeCodeRepository\",\n                \"sagemaker:DescribeEndpointConfig\",\n                \"sagemaker:DescribeNotebookInstance\",\n                \"sagemaker:ListCodeRepositories\",\n                \"sagemaker:ListEndpointConfigs\",\n                \"sagemaker:ListNotebookInstances\",\n                \"sagemaker:ListTags\",\n                \"secretsmanager:ListSecrets\",\n                \"secretsmanager:ListSecretVersionIds\",\n                \"securityhub:describeHub\",\n                \"shield:DescribeDRTAccess\",\n                \"shield:DescribeProtection\",\n                \"shield:DescribeSubscription\",\n                \"sns:GetTopicAttributes\",\n                \"sns:ListSubscriptions\",\n                \"sns:ListTagsForResource\",\n                \"sns:ListTopics\",\n                \"sqs:GetQueueAttributes\",\n                \"sqs:ListQueues\",\n                \"sqs:ListQueueTags\",\n                \"ssm:DescribeAutomationExecutions\",\n                \"ssm:DescribeDocument\",\n                \"ssm:GetAutomationExecution\",\n                \"ssm:GetDocument\",\n                \"ssm:ListDocuments\",\n                \"storagegateway:ListGateways\",\n                \"storagegateway:ListVolumes\",\n                \"support:DescribeCases\",\n                \"tag:GetResources\",\n                \"waf-regional:GetLoggingConfiguration\",\n                \"waf-regional:GetWebACL\",\n                \"waf-regional:GetWebACLForResource\",\n                \"waf:GetLoggingConfiguration\",\n                \"waf:GetWebACL\",\n                \"wafv2:GetLoggingConfiguration\"\n            ],\n            \"Resource\": \"*\"\n        }\n    ]\n}\n`\n```",
      "solution": "This was likely a AWS terraform provider bug.\nThe service linked role `AWSServiceRoleForConfig` does not get activated automatically the first time you apply the terraform plan. You need to manually add it to AWS config. Then it works fine.\n\nEDIT\nThe solution could be another than the aforementioned (or a combination of both). I also noticed that AWS Config get stuck on \"resources are being discovered\" when there are no rules/conformance packs deployed. If you deploy a single rule it discovers resources (?!)",
      "question_score": 7,
      "answer_score": 2,
      "created_at": "2021-05-11T10:50:03",
      "url": "https://stackoverflow.com/questions/67483448/aws-config-resource-discovery-stuck-on-your-resources-are-being-discovered"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-aws",
      "question_id": 66982393,
      "title": "Extracting list of subnets result in error - is tuple with 1 element",
      "problem": "I have a module that creates a VPC with public and private subnets\n```\n`module \"vpc\" {\n  count              = var.vpc_enabled ? 1 : 0\n  source             = \"./vpc\"\n}\n`\n```\nand as an output of that module I'm extracting the private subnets\n```\n`output \"private_subnets\" {\n  value = aws_subnet.private.*.id\n}\n`\n```\nThen I want to use that subnets list as an input of another module:\n```\n`module \"eks\" {\n  source          = \"./eks\"\n  name            = var.name\n  private_subnets = var.vpc_enabled ? module.vpc.private_subnets : var.private_subnets_id\n}\n`\n```\nbasically what I'm trying to achieve is that the user can choose if he want to create a new VPC or use as an input a list of subnets of their existing VPC.\nThe problem that I've right now is that I'm getting the following error in terraform plan:\n```\n`  on main.tf line 32, in module \"eks\":\n  32:   private_subnets = var.vpc_enabled ? module.vpc.private_subnets : var.private_subnets_id\n    |----------------\n    | module.vpc is tuple with 1 element\n\nThis value does not have any attributes.\n`\n```\nDoes anyone knows how to fix this?",
      "solution": "You are defining your `vpc` module with `count`. Thus you need to refer to individual instances of the module, even if you have only 1.\n```\n`private_subnets = var.vpc_enabled ? module.vpc[0].private_subnets : var.private_subnets_id\n`\n```",
      "question_score": 6,
      "answer_score": 13,
      "created_at": "2021-04-07T10:36:33",
      "url": "https://stackoverflow.com/questions/66982393/extracting-list-of-subnets-result-in-error-is-tuple-with-1-element"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-aws",
      "question_id": 73907150,
      "title": "failed to create ec2 instance using terraform if set security group",
      "problem": "I tried to create an EC2 instance. When I don't set security group, it's good, but when set security group it failed with the following message:\n```\n`\u2502 Error: creating EC2 Instance: InvalidParameterValue: Value () for parameter groupId is invalid. The value cannot be empty\n\u2502   status code: 400, request id: 2935799e-2364-4676-ba02-457740336cd1\n\u2502\n\u2502   with aws_instance.my_first_instance,\n\u2502   on main.tf line 44, in resource \"aws_instance\" \"my_first_instance\":\n\u2502   44: resource \"aws_instance\" \"my_first_instance\" {\n`\n```\nThe code is\n```\n` variable \"ecs_cluster_name\" {\n  type    = string\n  default = \"production\"\n}\n\ndata \"aws_ami\" \"ecs_ami\" {\n  most_recent = true\n  owners      = [\"amazon\"]\n\n  filter {\n    name   = \"name\"\n    values = [\"amzn2-ami-ecs-hvm-2.0.202*-x86_64-ebs\"]\n  }\n}\n\noutput \"ami_name\" {\n  value       = data.aws_ami.ecs_ami.name\n  description = \"the name of ecs ami\"\n}\n\noutput \"security_group_id\" {\n  value       = aws_security_group.default.id\n  description = \"id of security group\"\n}\n\nresource \"aws_security_group\" \"default\" {\n  name = \"terraform_Security_group\"\n  ingress {\n    from_port   = 22\n    to_port     = 22\n    protocol    = \"tcp\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n\n  egress {\n    from_port   = 0\n    to_port     = 0\n    protocol    = \"-1\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n}\n\nresource \"aws_instance\" \"my_first_instance\" {\n  ami           = data.aws_ami.ecs_ami.id\n  instance_type = \"t2.micro\"\n\n  # security_groups = [\"sg-06e91dae98b2c44c6\"]\n  security_groups = [aws_security_group.default.id]\n\n  user_data = > /etc/ecs/ecs.config\n                EOF\n}\n`\n```",
      "solution": "You should be using vpc_security_group_ids:\n```\n`  vpc_security_group_ids = [aws_security_group.default.id]\n`\n```",
      "question_score": 6,
      "answer_score": 13,
      "created_at": "2022-09-30T12:11:58",
      "url": "https://stackoverflow.com/questions/73907150/failed-to-create-ec2-instance-using-terraform-if-set-security-group"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-aws",
      "question_id": 73751777,
      "title": "How can I destroy a specific Terraform managed resource?",
      "problem": "I have a DMS task that failed and isn't resuming or restarting.  Unfortunately, according to AWS Support, the only recourse is to destroy and recreate it.  I have a large infrastructure that takes several hours to destroy and recreate with Terraform.  I'm running Terraform version 1.2.X with the AWS provider version 4.17.0.\nI tried running `terraform plan -destroy -target=\".\"`.  I tried with and without quotes, double hyphens prior to the `target` option, module names, etc. Every time the result comes back with this error:\n\nEither you have not created any objects yet or the existing objects were...\n\nMy hierarchy is this: Main module -> sub module -> resource. My spelling and punctuation are correct.\nI've Google it.  I find only the Hashicorp documentation that specifies the syntax but not the naming convention, as well as bug reports from years ago.  How do I selectively destroy a resource?",
      "solution": "It turns out I wasn't naming my resource correctly.\nAfter some trial and error, I ran a destroy plan for my entire infrastructure (`terraform plan  -destroy`).  Using the output from that, I found the name of the resource I wanted to destroy.  The format was `module...`.\nOnce I acquired the resource name directly from Terraform, I first ran the `terraform plan -destroy -target=\"module...\"` command to verify the outcome, then the `terraform destroy -target=\"module...\"` command and it worked!",
      "question_score": 6,
      "answer_score": 12,
      "created_at": "2022-09-17T04:40:53",
      "url": "https://stackoverflow.com/questions/73751777/how-can-i-destroy-a-specific-terraform-managed-resource"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-aws",
      "question_id": 71997615,
      "title": "Terraform data filter : get multiple values",
      "problem": "I want to get subnet values from another repository. For this purpose I added data aws_subnet part. However I'm having a problem with the filtering part. At the end of the values line I need to count for each subnets. I tried to use count.index and different things. But I get this error: `The \"count\" object can only be used in \"module\", \"resource\", and \"data\" blocks, and only when the \"count\" argument is set.` So how can I use * for the filter values part. Such as: `${var.vpcname}-Public-*`\nMy Subnets:\n```\n`myvpc-Private-0\nmyvpc-Private-1\nmyvpc-Private-2\nmyvpc-Public-0\nmyvpc-Public-1\nmyvpc-Public-2\n`\n```\nMy data part:\n```\n`data \"aws_subnet\" \"public\" {\n  filter {\n    name   = \"tag:Name\"    \n    values = [\"${var.vpcname}-Public-\"]\n  }\n}\n\ndata \"aws_subnet\" \"private\" {\n  filter {\n    name   = \"tag:Name\"\n    values = [\"${var.vpcname}-Private-\"]\n  }\n}\n`\n```\nWant to see all subnets with below output part.\n```\n`output \"private\" {\n  value = data.aws_subnet.private.*.id\n}\n\noutput \"public\" {\n  value = data.aws_subnet.public.*.id\n}\n`\n```",
      "solution": "You should be using aws_subnets, not `aws_subnet`:\n```\n`data \"aws_subnets\" \"public\" {\n  filter {\n    name   = \"tag:Name\"    \n    values = [\"${var.vpcname}-Public-*\"]\n  }\n}\n\ndata \"aws_subnets\" \"private\" {\n  filter {\n    name   = \"tag:Name\"\n    values = [\"${var.vpcname}-Private-*\"]\n  }\n}\n`\n```\nthen\n```\n`output \"private\" {\n  value = data.aws_subnets.private.ids\n}\n\noutput \"public\" {\n  value = data.aws_subnets.public.ids\n}\n`\n```",
      "question_score": 6,
      "answer_score": 12,
      "created_at": "2022-04-25T11:52:04",
      "url": "https://stackoverflow.com/questions/71997615/terraform-data-filter-get-multiple-values"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-aws",
      "question_id": 69406956,
      "title": "DynamoDB table created by Terraform in LocalStack not visible in NoSQL Workbench",
      "problem": "Summary: Code and configuration known to show up in NoSQL Workbench when using DynamoDB Local mysteriously don't work with LocalStack: though the connection works, the tables no longer show in NoSQL Workbench (but continue to show up when using the `aws-cli`).\n\nI created a table in DynamoDB Local running in Docker that worked in NoSQL Workbench. I wrote code to seed that database, and it all worked and showed up in NoSQL Workbench.\nI switched to LocalStack (so I can interact with other AWS services locally). I was able to create a table with Terraform and can seed it with my code (using the configuration given here). Using the `aws-cli`, I can see the table, etc.\nBut inside NoSQL Workbench, I couldn't see the table I created and seeded when connecting as shown below. There weren't connection errors; the table just isn't there. It doesn't seem related to the bugginess issue described here, as restarting the application did not help. I didn't change any AWS account settings like region, keys, etc.",
      "solution": "Summary: To use NoSQL Workbench with LocalStack, set the region to `localhost` in your code and Terraform config, and fix the resulting validation error (saying there isn't a `localhost` region) by setting `skip_region_validation` to true in the `aws` provider block in the Terraform config.\n\nThe problem is disclosed in the screenshot above:\n\nNoSQL Workbench uses the `localhost` region.\nWhen using DynamoDB Local, it appears the region is ignored, so this quirk is hidden (i.e. there is a mismatch between the region in the Terraform file and my code on the one hand and NoSQL Workbench on the other, but it doesn't matter with DyanmoDB Local).\nBut with LocalStack region is not ignored, so the problem popped up.\nI wouldn't have written this up except for one more quirk that took a while to figure out. When I updated the Terraform configuration thus:\n`provider \"aws\" {\n  access_key = \"mock_access_key\"\n  // For compatibility with NoSQL workbench local connections\n  region                      = \"localhost\"\n`\nI started getting this error when running `terraform apply`:\n```\n`\u2577\n\u2502 Error: Invalid AWS Region: localhost\n\u2502\n\u2502   with provider[\"registry.terraform.io/hashicorp/aws\"],\n\u2502   on main.tf line 1, in provider \"aws\":\n\u2502    1: provider \"aws\" {\n\u2502\n\u2575\n`\n```\nI dug around a bit and found this issue in the AWS provider repo for Terraform, which explains that you should do this:\n`provider \"aws\" {\n  access_key = \"mock_access_key\"\n  // For compatibility with NoSQL workbench local connections\n  region                      = \"localhost\"\n  skip_region_validation      = true\n`",
      "question_score": 6,
      "answer_score": 5,
      "created_at": "2021-10-01T15:46:04",
      "url": "https://stackoverflow.com/questions/69406956/dynamodb-table-created-by-terraform-in-localstack-not-visible-in-nosql-workbench"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-aws",
      "question_id": 69573759,
      "title": "How to upsize volume of Terraformed EKS node",
      "problem": "We have been using Terraform for almost a year now to manage all kinds of resources on AWS from bastion hosts to VPCs, RDS and also EKS.\nWe are sometimes really baffled by the EKS module. It could however be due to lack of understanding (and documentation), so here it goes:\nProblem: Upsizing Disk (volume)\n```\n`\nmodule \"eks\" {\n  source  = \"terraform-aws-modules/eks/aws\"\n  version = \"12.2.0\"\n\n  cluster_name    = local.cluster_name\n  cluster_version = \"1.19\"\n  subnets         = module.vpc.private_subnets\n\n  #...\n\n  node_groups = {\n    first = {\n      desired_capacity = 1\n      max_capacity     = 5\n      min_capacity     = 1\n\n      instance_type = \"m5.large\"\n    }\n  }\n`\n```\nI thought the default value for this (dev) k8s cluster's node can easily be the default 20GBs but it's filling up fast so I know want to change `disk_size` to let's say 40GBs.\n=> I thought I could just add something like `disk_size=40` and done.\n`terraform plan` tells me I need to replace the node. This is a 1 node cluster, so not good. And even if it were I don't want to e.g. drain nodes. That's why I thought we are using managed k8s like EKS.\nExpected behaviour: since these are elastic volumes I should be able to upsize but not downsize, why is that not possible? I can def. do so from the AWS UI.\nSure with a slightly scary warning:\nAre you sure that you want to modify volume vol-xx?\nIt may take some time for performance changes to take full effect.\nYou may need to extend the OS file system on the volume to use any newly-allocated space\nBut I can work with the provided docs on that: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/recognize-expanded-volume-linux.html?icmpid=docs_ec2_console\nAny guidelines on how to up the storage? If I do so with the UI but don't touch Terraform then my EKS state will be nuked/out of sync.",
      "solution": "By using the terraform-aws-eks terraform module you are actually following the \"ephemeral nodes\" paradigm, because for both ways of creating instances (self-managed workers or managed node groups) the module is creating Autoscaling Groups that create EC2 instances out of a Launch Template.\nASG and Launch Templates are specifically designed so that you don't care anymore about specific nodes, and rather you just care about the number of nodes. This means that for updating the nodes, you just replace them with new ones, which will use the new updated launch template (with more GBs for example, or with a new updated AMI, or a new instance type).\nThis is called \"rolling updates\", and it can be done manually (adding new instances, then draining the node, then deleting the old node), with scripts (see: eks-rolling-update in github by Hellofresh), or it can be done automagically if you use the AWS managed nodes (the ones you are actually using when specifying \"node_groups\", that is why if you add more GB, it will replace the node automatically when you run apply).\nAnd this paradigm is the most common when operating Kubernetes in the cloud (and also very common on-premise datacenters when using virtualization).\nOption 1) Self Managed Workers\nWith self managed nodes, when you change a parameter like disk_size or instance_type, it will change the Launch Template. It will update the $latest version tag, which is commonly where the ASG is pointing to (although can be changed). This means that old instances will not see any change, but new ones will have the updated configuration.\nIf you want to change the existing instances, you actually want to replace them with new ones. That is what this ephemeral nodes paradigm is.\nOne by one you can drain the old instances while increasing the number of desired_instances on the ASG, or let the cluster autoscaler do the job. Alternatively, you can use an automated script which does this for you for each ASG: https://github.com/hellofresh/eks-rolling-update\nIn terraform_aws_eks module, you create self managed workers by either using worker_groups or worker_groups_launch_template (recommended) field\nOption 2) Managed Nodes\nManaged nodes is an EKS-specific feature. You configure them very similarly, but in reality, it is an abstraction, and AWS will create the actual underlying ASG.\nYou can specify a Launch Template to be used by the ASG and its version. Some config can be specified at the managed node level (i.e. AMI and instance_types) and at the Launch Template (if it wasn't specified in the former).\nAny change on the node group level config, or on the Launch Template version, will trigger an automatic rolling update, which will replace all old instances.\nYou can delay the rolling update by just not pointing to the $latest version (or pointing to $default, and not updating the $default tag when changing the LT).\nIn terraform_aws_eks module, you create self managed workers by using the node_groups field. You can also play with these settings: create_launch_template=true and set_instance_types_on_lt=true if you want the module to create the LT for you (alternatively you can just not use it, or pass a reference to one); and to set the instance_type on such LT as specified above.\nBut behavior is similar to worker groups. In no case you will have your existing instances changed. You can only change them manually.\nHowever, there is an alternative: The manual way\nYou can use the EKS module to create the control plane, but then use a regular EC2 resource in terraform (https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/instance) to create one ore multiple (using count or for_each) instances.\nIf you create the instances using the aws_instance resource, then terraform will patch those instances (updated-in-place) when any change is allowed (i.e. increasing the root volue GB or the instance type; whereas changing the AMI will force a replacement).\nThe only tricky part, is that you need to configure the cloud-init script to make the instance join the cluster (something that is automatically done by the EKS module when using self/managed node groups).\nHowever, it is very possible, and you can borrow the script from the module and plug it into the aws_instance's user_data field (https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/instance#user_data)\nIn this case (when talking about disk_size), however, you still need to manually (either by SSH, or by running an hacky exec using terraform) to patch the XFS filesystem so it sees the increased disk space.\nAnother alternative: Consider Kubernetes storage\nThat said, there is also another alternative for certain use cases. If you want to increase the disk space of those instances because of one of your applications using a hostPath, then it might be the case that you can use a kubernetes built-in storage solution using the EBS CSI driver.\nFor example, I manage an ElasticSearch cluster in Kubernetes (and deploy it from terraform with the helm module), and it uses dynamic storage provisioning to request an EBS volume (note that performance is the same, because both root and this other volume are EBS volumes). EBS CSI driver supports volume expansion, so I can just increase this disk by changing a terraform variable.\n\nTo conclude, I would not recommend the aws_instance way, unless you understand it and are sure you really want it. It may make sense in certain cases, but definitely not common",
      "question_score": 6,
      "answer_score": 5,
      "created_at": "2021-10-14T17:54:03",
      "url": "https://stackoverflow.com/questions/69573759/how-to-upsize-volume-of-terraformed-eks-node"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-aws",
      "question_id": 73433767,
      "title": "How can I enable Cognito &#39;Email address or phone number&#39; login using Terraform?",
      "problem": "I'm trying to create a new AWS Cognito user pool using Terraform, and currently have the following problem:\nI've been trying to get Email address or phone number -> Allow email addresses (shown below in red) selected, instead of what is currently selected (Username -> Also allow sign in with verified email address)\n\nThe relevant section of my `main.tf` file looks like this:\n`resource \"aws_cognito_user_pool\" \"app_cognito_user_pool\" {\n  name = \"app_cognito_user_pool\"\n\n  alias_attributes         = [\"email\"]\n  auto_verified_attributes = [\"email\"]\n  account_recovery_setting {\n    recovery_mechanism {\n      name     = \"verified_email\"\n      priority = 1\n    }\n  }\n}\n\nresource \"aws_cognito_user_pool_client\" \"app_cognito_user_pool_client\" {\n  name         = \"app_cognito_user_pool_client\"\n  user_pool_id = aws_cognito_user_pool.app_cognito_user_pool.id\n\n  prevent_user_existence_errors = \"ENABLED\"\n  supported_identity_providers  = [\"COGNITO\"]\n}\n\nresource \"aws_cognito_user_pool_domain\" \"app_cognito_user_pool_domain\" {\n  domain       = \"app\"\n  user_pool_id = aws_cognito_user_pool.app_cognito_user_pool.id\n}\n`\nNo matter what I try, I always get Username, instead of Email address or phone number selected. I want the user pool not to use a username, but use an email address instead.\nWhat Terraform argument(s) or value(s) am I missing?",
      "solution": "Only set `username_attributes` - and not `alias_attributes` - to `[\"email\"]`.\n\nSetting `alias_attributes` specifies the 'top part' i.e. Also sign in with verified email address / phone number.\nIt specifies the extra (alias) ways you can sign in, in addition to the username.\nSetting `username_attributes` specifies the 'bottom part' i.e. Allow email addresses / phone numbers / both email addresses and phone numbers ...\nIt specifies what to use instead of the username.\n\nUnset `alias_attributes` (as it conflicts with `username_attributes`) & then set `username_attributes' to one of the following:\n\n`[\u201cemail\u201d]` - Allow email addresses\n`[\u201cphone_number\u201d]` - Allow phone numbers\n`[\u201cemail\u201d, \u201cphone_number\u201d]` - Allow both email addresses and phone numbers (users can choose one\n\nIn your case, you need to set `username_attributes` to `[\"email\"]`.\n\nThis should work:\n`resource \"aws_cognito_user_pool\" \"app_cognito_user_pool\" {\n  name = \"app_cognito_user_pool\"\n\n  username_attributes      = [\"email\"]\n  auto_verified_attributes = [\"email\"]\n  account_recovery_setting {\n    recovery_mechanism {\n      name     = \"verified_email\"\n      priority = 1\n    }\n  }\n}\n...\n`",
      "question_score": 6,
      "answer_score": 8,
      "created_at": "2022-08-21T12:59:06",
      "url": "https://stackoverflow.com/questions/73433767/how-can-i-enable-cognito-email-address-or-phone-number-login-using-terraform"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-aws",
      "question_id": 67652383,
      "title": "How do I iterate over a &#39;count&#39; resource in Terraform?",
      "problem": "In this example, I'm trying to create 3 EC2 instances which each have an elastic IP assigned. I want to achieve this by saying the following.\n```\n`resource \"aws_instance\" \"web_servers\" {\n  ami                         = \"ami-09e67e426f25ce0d7\"\n  instance_type               = \"t3.micro\"\n  ...\n  count = 3\n}\n`\n```\nand, along with other networking instances,\n```\n`resource \"aws_eip\" \"elastic_ip\" {\n  for_each = aws_instance.web_servers\n  instance = each.key\n  vpc      = true\n}\n`\n```\nHowever, this is saying the following:\n```\n`The given \"for_each\" argument value is unsuitable: the \"for_each\" argument must be a map, or set of strings, and you have provided a value of type tuple.\n`\n```\nI have tried wrapping the `for_each` in a `toset()` which also says there is an issue with an unknown number of instances - I know there are 3 though. Is there something I'm missing around the `count` & `for_each` keywords?",
      "solution": "If you really want to use `for_each`, rather then count again, it should be:\n```\n`resource \"aws_eip\" \"elastic_ip\" {\n  for_each = {for idx, val in aws_instance.web_servers: idx => val}\n  instance = each.value.id\n  vpc      = true\n}\n`\n```\nBut since you are using `count` in the first place, it would be probably better to use `count` for your `aws_eip` as well:\n```\n`resource \"aws_eip\" \"elastic_ip\" {\n  count    = length(aws_instance.web_servers)\n  instance = aws_instance.web_servers[count.index].id\n  vpc      = true\n}\n`\n```",
      "question_score": 6,
      "answer_score": 7,
      "created_at": "2021-05-22T19:31:52",
      "url": "https://stackoverflow.com/questions/67652383/how-do-i-iterate-over-a-count-resource-in-terraform"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-aws",
      "question_id": 68334932,
      "title": "\u2502 Error: Reference to undeclared resource",
      "problem": "I am new to terraform and trying to make an instance of AWS (t2.nano) by the image below.\nthis is my tf file:\n```\n`provider \"aws\" {\n  profile = \"default\"\n  region  = \"us-west-2\"\n}\n\nresource \"aws_s3_bucket\" \"prod_tf_course\" {\n  bucket = \"tf-course-20210607\"\n  acl    = \"private\"\n}\n\nresource \"aws_default_vpc\" \"default\" {}\n\nresource \"aws_security_group\" \"group_web\"{\n  name = \"prod_web\"\n  description = \"allow standard http and https ports inbound and everithing outbound\"\n\n  ingress{\n    from_port = 80\n    to_port = 80\n    protocol = \"tcp\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n\n ingress{\n    from_port = 443 \n    to_port = 443\n    protocol = \"tcp\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n\n  egress{\n    from_port = 0\n    to_port = 0\n    protocol = \"-1\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  \n  }\n  tags = {\n    \"Terraform\" : \"true\"\n  }\n\n}\n\nresource \"aws_instance\" \"prod_web\"{\n  ami = \"ami-05105e44227712eb6\"\n  instance_type =\"t2.nano\"\n\n  vpc_security_group_ids = [\n    aws_security_group.prod_web.id\n  ]\n\n  tags = {\n    \"Terraform\" : \"true\"\n  }\n}\n`\n```\nWhen I run the command `terraform plan`, its produces the following error:\n```\n`$ terraform plan\n\u2577\n\u2502 Error: Reference to undeclared resource\n\u2502\n\u2502   on prod.tf line 50, in resource \"aws_instance\" \"prod_web\":\n\u2502   50:     aws_security_group.prod_web.id\n\u2502\n\u2502 A managed resource \"aws_security_group\" \"prod_web\" has not been declared in\n\u2502 the root module.\n\u2575\n`\n```\nif someone can help me fix it , i will be so happy.",
      "solution": "It should be:\n```\n`  vpc_security_group_ids = [\n    aws_security_group.group_web.id\n  ]\n`\n```\nas your `aws_security_group` is called `group_web`, not `prod_web`.",
      "question_score": 6,
      "answer_score": 5,
      "created_at": "2021-07-11T11:27:40",
      "url": "https://stackoverflow.com/questions/68334932/error-reference-to-undeclared-resource"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-aws",
      "question_id": 66433485,
      "title": "Terraform Error: Reference to undeclared resource",
      "problem": "I am trying to run s3 replication in terraform which will be cross-regional. Most of my code is good but I am only getting 2 errors which I cannot seem to solve.\nPart of my main s3.tf is\n```\n`resource \"aws_kms_key\" \"s3_replica-us-west-2\" {\n  description             = \"S3 master key replica us-west-2\"\n  deletion_window_in_days = 30\n  enable_key_rotation     = \"true\"\n}\n\nmodule \"s3_replica\" {\n  source = \"git@github.com:xxx\"\n\n  providers = {\n    aws     = \"aws.us-west-2\"\n  }\n\n  name                  = \"s3_replica\"\n  logging_bucket_prefix = \"s3_replica\"\n  versioning            = var.versioning\n  bucket_logging        = var.bucket_logging\n  logging_bucket_name   = var.logging_bucket_name\n\n  kms_key_id    = aws_kms_key.s3_replica-us-west-2.key_id\n  sse_algorithm = var.sse_algorithm\n}\n\nmodule \"s3\" {\n  source                = \"git@github.com:xxxx\"\n  name                  = \"s3\"\n  logging_bucket_prefix = \"s3\"\n  versioning            = var.versioning\n  bucket_logging        = var.bucket_logging\n  logging_bucket_name   = var.logging_bucket_name\n\n  kms_key_id    = aws_kms_key.s3.key_id\n  sse_algorithm = var.sse_algorithm\n\n  replication_configuration = {\n    role = aws_iam_role.s3_replication.arn\n\n      rules = {\n         id = \"replicate_to_${local.s3_replica}\"\n         prefix = \"\"\n         status = \"Enabled\"\n\n        destination = {\n          bucket = lookup.module.s3_replica.bucket_arn\n          replica_kms_key_id = lookup.s3_replica_arn\n          }\n        }\n\n      source_selection_criteria = {\n          sse_kms_encrypted_objects = {\n            enabled = true\n          }\n        }\n  }\n`\n```\nand the part of my replication configuration block in the module I use is:\n```\n`dynamic \"replication_configuration\" {\n    for_each = length(keys(var.replication_configuration)) == 0 ? [] : [var.replication_configuration]\n\n    content {\n      role = replication_configuration.value.role\n\n      dynamic \"rules\" {\n        for_each = replication_configuration.value.rules\n\n        content {\n          id       = lookup(rules.value, \"id\", null)\n          priority = lookup(rules.value, \"priority\", null)\n          prefix   = lookup(rules.value, \"prefix\", null)\n          status   = lookup(rules.value, \"status\", null)\n\n          dynamic \"destination\" {\n            for_each = length(keys(lookup(rules.value, \"destination\", {}))) == 0 ? [] : [lookup(rules.value, \"destination\", {})]\n\n            content {\n              bucket             = lookup(destination.value, \"bucket\", null)\n              storage_class      = lookup(destination.value, \"storage_class\", null)\n              replica_kms_key_id = lookup(destination.value, \"replica_kms_key_id\", null)\n              account_id         = lookup(destination.value, \"account_id\", null)\n            }\n          }\n\n          dynamic \"source_selection_criteria\" {\n            for_each = length(keys(lookup(rules.value, \"source_selection_criteria\", {}))) == 0 ? [] : [lookup(rules.value, \"source_selection_criteria\", {})]\n\n            content {\n\n              dynamic \"sse_kms_encrypted_objects\" {\n                for_each = length(keys(lookup(source_selection_criteria.value, \"sse_kms_encrypted_objects\", {}))) == 0 ? [] : [lookup(source_selection_criteria.value, \"sse_kms_encrypted_objects\", {})]\n\n                content {\n\n                  enabled = sse_kms_encrypted_objects.value.enabled\n                }\n              }\n            }\n          }\n\n        }\n      }\n    }\n  }\n}\n`\n```\nNow when I run terraform init... it works.\nBut when I run terraform plan I get the error:\n```\n`Error: Reference to undeclared resource\n\n  on s3.tf line 108, in module \"s3\":\n 108:           bucket = lookup.module.s3_replica.bucket_arn\n\nA managed resource \"lookup\" \"module\" has not been declared in the root module.\n\nError: Reference to undeclared resource\n\n  on s3.tf line 109, in module \"s3\":\n 109:           replica_kms_key_id = lookup.s3_replica-us-west-2_arn\n\nA managed resource \"lookup\" \"s3_replica_arn\" has not been declared\nin the root module.\n`\n```\nNow I do not know why I get these errors..",
      "solution": "From what I understand, your `s3_replica` bucket is created in `module.s3`, and you want to access its ARN to initialize the `module.s3`. Sadly, you can't do this, as you can't reference module outputs before the module is fully created.\nOne way to overcome this issue, is to create `s3_replica` first, and then pass it to  `module.s3`. Below is just an example, probably need many further modifications:\n```\n`resource \"aws_s3_bucket\" \"s3_replica\" {\n  bucket = \"my-replication-bucket-23223\"\n  acl    = \"private\"\n}\n\nresource \"aws_kms_key\" \"s3_replica\" {\n  description             = \"KMS for replication\"\n  deletion_window_in_days = 10\n}\n\nmodule \"s3\" {\n\n  # \n  #\n\n  replication_configuration = {\n    role = aws_iam_role.s3_replication.arn\n\n      rules = {\n         id = \"replicate_to_${local.s3_replica}\"\n         prefix = \"\"\n         status = \"Enabled\"\n\n        destination = {\n          bucket             = resource.aws_s3_bucket.s3_replica.arn\n          replica_kms_key_id = resource.aws_kms_key.s3_replica.arn\n          }\n        }\n\n      source_selection_criteria = {\n          sse_kms_encrypted_objects = {\n            enabled = true\n          }\n        }\n  }\n\n`\n```\nI would recommend having a look at Module Composition  in TF docs. It explains, with examples, how to work with modules.",
      "question_score": 6,
      "answer_score": 5,
      "created_at": "2021-03-02T05:46:38",
      "url": "https://stackoverflow.com/questions/66433485/terraform-error-reference-to-undeclared-resource"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-aws",
      "question_id": 70008141,
      "title": "trigger lambda function from DynamoDB",
      "problem": "Every time a new item arrives in my dynamo table, I want to run a lambda function `trigger_lambda_function`. This is how I define my table and trigger. However, the trigger does not work as expected.\n```\n`resource \"aws_dynamodb_table\" \"filenames\" {\n  name           = local.dynamodb_table_filenames\n  billing_mode   = \"PROVISIONED\"\n  read_capacity  = 1000\n  write_capacity = 1000\n  hash_key       = \"filename\"\n\n  #range_key      = \"\"\n\n  attribute {\n    name = \"filename\"\n    type = \"S\"\n  }\n\n  tags = var.tags\n}\n\nresource \"aws_lambda_event_source_mapping\" \"allow_dynamodb_table_to_trigger_lambda\" {\n  event_source_arn  = aws_dynamodb_table.filenames.stream_arn\n  function_name     = aws_lambda_function.trigger_stepfunction_lambda.arn\n  starting_position = \"LATEST\"\n}\n`\n```\nUpon `terraform apply`, I get an error that:\n```\n`\u2502 Error: error creating Lambda Event Source Mapping (): InvalidParameterValueException: Unrecognized event source.\n\u2502 {\n\u2502   RespMetadata: {\n\u2502     StatusCode: 400,\n\u2502     RequestID: \"5ae68da6-3f6d-4adb-b104-72ae584dbca7\"\n\u2502   },\n\u2502   Message_: \"Unrecognized event source.\",\n\u2502   Type: \"User\"\n\u2502 }\n\u2502 \n\u2502   with module.ingest_system[\"alpegatm\"].aws_lambda_event_source_mapping.allow_dynamodb_table_to_trigger_lambda,\n\u2502   on ../../modules/ingest_system/dynamo.tf line 39, in resource \"aws_lambda_event_source_mapping\" \"allow_dynamodb_table_to_trigger_lambda\":\n\u2502   39: resource \"aws_lambda_event_source_mapping\" \"allow_dynamodb_table_to_trigger_lambda\" {\n`\n```\nI also tried `.arn` instead of `stream_arn`but that threw an error too. What else could I try?\nI followed the documentation for the trigger:\nhttps://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/lambda_event_source_mapping",
      "solution": "From the `aws_dynamodb_table` docs, `stream_arn` is only available if `stream_enabled` is set to `true`. You might want to add `stream_enabled = true` to your DynamoDB table definition.\nBy default `stream_enabled` is set to `false`. You can see all the default values here for `aws_dynamodb_table`.",
      "question_score": 6,
      "answer_score": 2,
      "created_at": "2021-11-17T17:29:01",
      "url": "https://stackoverflow.com/questions/70008141/trigger-lambda-function-from-dynamodb"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-aws",
      "question_id": 74537322,
      "title": "Terraform: AWS Lambda with Image not updating",
      "problem": "We have a new terraform script that is pushing a docker image to an AWS Lambda.  The script works well and correctly connects the fresh image to the Lambda.  I can confirm this by checking the Image URL as shown in the AWS console for the Lambda and it is the newly pushed+connected image.  However when testing the lambda it is clearly running the prior code.  It seems like the Lambda has been updated but the running in-memory instances didnt get the message.\nQuestion: is there a way to force the in-memory Lambdas to be cycled to the new image?\nHere is our TF code for the Lambda:\n```\n`resource \"aws_lambda_function\" \"my_lambda\" {\n  function_name = \"MyLambda_${var.environment}\"\n  role          = data.aws_iam_role.iam_for_lambda.arn\n  image_uri     = \"${data.aws_ecr_repository.my_image.repository_url}:latest\"\n  memory_size   = 512\n  timeout       = 300\n  architectures = [\"x86_64\"]\n  package_type  = \"Image\"\n  environment {variables = {stage = var.environment, commit_hash=var.commit_hash}}\n}\n`\n```",
      "solution": "After more searching I found some discussions (here) that mention the `source_code_hash` option in terraform for the Lambda creation block (docs here).  Its mostly used with a SHA hash of the zip file used for pushing code from an S3 bucket, but in our case we are using a container/image so there is not really a file to get a hash from.  However, it turns out that it is just a string that Lambda checks for changes.  So we added the following:\n```\n`resource \"aws_lambda_function\" \"my_lambda\" {\n  function_name = \"MyLambda_${var.environment}\"\n  role          = data.aws_iam_role.iam_for_lambda.arn\n  image_uri     = \"${data.aws_ecr_repository.my_image.repository_url}:latest\"\n  memory_size   = 512\n  timeout       = 300\n  architectures = [\"x86_64\"]\n  package_type  = \"Image\"\n  environment {variables = {stage = var.environment, commit_hash=var.commit_hash}}\n  source_code_hash = var.commit_hash  And we use a bitbucket pipeline to inject the git hash into the terraform `apply` operation.  This fix allowed the Lambda to correctly update the running version.",
      "question_score": 5,
      "answer_score": 10,
      "created_at": "2022-11-22T19:13:53",
      "url": "https://stackoverflow.com/questions/74537322/terraform-aws-lambda-with-image-not-updating"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-aws",
      "question_id": 73870314,
      "title": "Terraform: remote module not following providers block - Warning: Reference to undefined provider",
      "problem": "I'm trying to use a (private) remote terraform module, and trying to pass a different provider to it. For the remote module, there are no providers defined and to my understanding, it will use a the local provider instead.\nI can't seem to be able to get it to use a provider alias - there are a few files at play here:\n```\n`# main.tf\nprovider \"aws\" {\n  region = var.aws_region\n\n}\n\nprovider \"aws\" {\n  alias  = \"replica_region\"\n  region = var.replica_region\n}\n\nterraform {\n  backend \"s3\" {\n  }\n}\n`\n```\n```\n`# s3.tf\nmodule \"some-remote-module\" {\n  source = 'git::ssh.......'\n  providers = {\n    aws = aws.replica_region\n  }\n}\n`\n```\nWhenever I plan (with terragrunt), The region is that of the primary aws provider config. I get the following warning, too:\n```\n`\u2502 Warning: Reference to undefined provider\n\u2502\n\u2502   on s3.tf line 12, in module \"some-remote-module\":\n\u2502   12:     aws = aws.replica_region\n\u2502\n\u2502 There is no explicit declaration for local provider name \"aws\" in\n\u2502 module.some-remote-module, so Terraform is assuming you\n\u2502 mean to pass a configuration for \"hashicorp/aws\".\n\u2502\n\u2502 If you also control the child module, add a required_providers entry named\n\u2502 \"aws\" with the source address \"hashicorp/aws\".\n\u2575\n`\n```\nAm I passing the providers in incorrectly? Is this even something that terraform is capable of? I'm using terraform 1.3. The remote module doesn't have any provider config.",
      "solution": "The last paragraph of this message is suggesting that you modify the child module to include the following declaration so that it's explicit that when you say \"aws\" it means `hashicorp/aws` rather than a provider in some other namespace that might coincidentally also be called \"aws\":\n```\n`terraform {\n  required_providers {\n    aws = {\n      source = \"hashicorp/aws\"\n    }\n  }\n}\n`\n```\nThis is what the error message means by `a required_providers entry named \"aws\" with the source address \"hashicorp/aws\"`.\nThis allows Terraform to see for certain (rather than guessing) that the short name \"aws\" refers to the same provider in both the calling module and the called module.",
      "question_score": 5,
      "answer_score": 11,
      "created_at": "2022-09-27T17:43:35",
      "url": "https://stackoverflow.com/questions/73870314/terraform-remote-module-not-following-providers-block-warning-reference-to-u"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-aws",
      "question_id": 66770564,
      "title": "Using terragrunt generate provider block causes conflicts with require providers block in module",
      "problem": "I'm using Terragrunt with Terraform version 0.14.8.\nMy project uses mono repo structure as it is a project requirement to package Terragrunt files and Terraform modules together in a single package.\nFolder structure:\n```\n`project root:\n\u251c\u2500\u2500 environments\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 prd\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 rds-cluster\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 terragrunt.hcl\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 terragrunt.hcl\n\u2514\u2500\u2500 modules\n    \u251c\u2500\u2500 rds-cluster\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 README.md\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 main.tf\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 output.tf\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 variables.tf\n    \u2514\u2500\u2500 secretsmanager-secret\n        \u251c\u2500\u2500 README.md\n        \u251c\u2500\u2500 main.tf\n        \u251c\u2500\u2500 output.tf\n        \u2514\u2500\u2500 variables.tf\n`\n```\nIn prd/terragrunt.hcl I define the remote state block and the generate provider block.\n```\n`remote_state {\n  backend = \"s3\"\n  ...\n}\n\ngenerate \"provider\" {\n  path = \"provider.tf\"\n  if_exists = \"overwrite_terragrunt\"\n\n  contents =  3.0\"\n    }\n  }\n}\n\nprovider \"aws\" {\n  region = \"ca-central-1\"\n}\nEOF\n}\n`\n```\nIn environments/prd/rds-cluster/terragrunt.hcl, I defined the following:\n```\n`include {\n  path = find_in_parent_folders()\n}\n\nterraform {\n  source = \"../../../modules//rds-cluster\"\n}\n\ninputs = {\n ...\n}\n`\n```\nIn modules/rds-cluster/main.tf, I defined the following:\n```\n`terraform {\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \">= 3.0\"\n    }\n  }\n}\n\n// RDS related resources...\n`\n```\nMy problem is that when I try to run terragrunt plan under `environments/prd/rds-cluster`, I get the following error message:\n```\n`Error: Duplicate required providers configuration\n\n  on provider.tf line 3, in terraform:\n   3:   required_providers {\n\nA module may have only one required providers configuration. The required\nproviders were previously configured at main.tf:2,3-21.\n`\n```\nI can resolve this by declaring the version within the provider block as shown here. However, the version attribute in provider blocks has been deprecated in Terraform 0.13; Terraform recommends the use of the required_providers sub-block under terraform block instead.\nDoes anyone know what I need to do to use the new required_providers block for my aws provider?",
      "solution": "As you've seen, Terraform expects each module to have only one definition of its required providers, which is intended to avoid a situation where it's unclear why Terraform is detecting a particular when the declarations are spread among multiple files.\nHowever, to support this sort of piecemeal code generation use-case Terraform has an advanced feature called Override Files which allows you to explicitly mark certain files for a different mode of processing where they selectively override particular definitions from other files, rather than creating entirely new definitions.\nThe details of this mechanism depend on which block type you're overriding, but the section on Merging `terraform` blocks` discusses the behavior relevant to your particular situation:\n\nIf the `required_providers` argument is set, its value is merged on an element-by-element basis, which allows an override block to adjust the constraint for a single provider without affecting the constraints for other providers.\nIn both the `required_version` and `required_providers` settings, each override constraint entirely replaces the constraints for the same component in the original block. If both the base block and the override block both set required_version then the constraints in the base block are entirely ignored.\n\nThe practical implication of the above is that if you have an override file with a `required_providers` block that includes an entry for the AWS provider then Terraform will treat it as a full replacement for any similar entry already present in a non-override file, but it won't affect other provider requirements entries which do not appear in the override file at all.\nPutting all of this together, you should be able to get the result you were looking for by asking Terragrunt to name this generated file `provider_override.tf` instead of just `provider.tf`, which will then activate the override file processing behavior and thus allow this generated file to override any existing definition of AWS provider requirements, while allowing the configurations to retain any other provider requirements they might also be defining.",
      "question_score": 5,
      "answer_score": 14,
      "created_at": "2021-03-23T21:08:04",
      "url": "https://stackoverflow.com/questions/66770564/using-terragrunt-generate-provider-block-causes-conflicts-with-require-providers"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-aws",
      "question_id": 65628538,
      "title": "Terraform shows `InvalidGroup.NotFound` while creating an EC2 instance",
      "problem": "I am trying to deploy EC2 instances using Terrafom and I can see the following error:\n```\n`Error: Error launching source instance: InvalidGroup.NotFound: The security group 'prod-web-servers-sg' does not exist in VPC 'vpc-db3a3cb3'\n`\n```\nHere is the Terraform template I'm using:\n```\n`resource \"aws_default_vpc\" \"default\" {\n}\n\nresource \"aws_security_group\" \"prod-web-servers-sg\" {\nname        = \"prod-web-servers-sg\"\ndescription = \"security group for production grade web servers\"\nvpc_id      = \"${aws_default_vpc.default.id}\"\n\ningress {\nfrom_port   = 80\nto_port     = 80\nprotocol    = \"tcp\"\ncidr_blocks = [\"0.0.0.0/0\"]\n}\ningress {\nfrom_port   = 443\nto_port     = 443\nprotocol    = \"tcp\"\ncidr_blocks = [\"0.0.0.0/0\"]\n  }\n}\n\n#Subnet\n\n resource \"aws_subnet\" \"private_subnet\" {\n vpc_id     = \"${aws_default_vpc.default.id}\"\n cidr_block = \"172.31.0.0/24\"\n availability_zone = \"ap-south-1a\"\n }\n\n resource \"aws_instance\" \"prod-web-server\" {\n ami           = \"ami-04b1ddd35fd71475a\"\n count    = 2\n key_name = \"test_key\"\n instance_type = \"r5.large\"\n security_groups = [\"prod-web-servers-sg\"]\n subnet_id = \"${aws_subnet.private_subnet.id}\"\n  }\n`\n```",
      "solution": "You have a race condition there because Terraform doesn't know to wait until the security group is created before creating the instance.\nTo fix this you should interpolate the `aws_security_group.prod-web-servers-sg.id` into `aws_instance.prod-web-server` resource so that it can work out the dependency chain between the resources. You should also use `vpc_security_group_ids` instead of `security_groups` as mentioned in the `aws_instance` resource documentation:\n\nsecurity_groups - (Optional, EC2-Classic and default VPC only) A list of security group names (EC2-Classic) or IDs (default VPC) to associate with.\nNOTE:\nIf you are creating Instances in a VPC, use vpc_security_group_ids instead.\n\nSo you should have something like the following:\n```\n`resource \"aws_default_vpc\" \"default\" {}\n\nresource \"aws_security_group\" \"prod-web-servers-sg\" {\n  name        = \"prod-web-servers-sg\"\n  description = \"security group for production grade web servers\"\n  vpc_id      = aws_default_vpc.default.id\n\n  ingress {\n    from_port   = 80\n    to_port     = 80\n    protocol    = \"tcp\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n  ingress {\n    from_port   = 443\n    to_port     = 443\n    protocol    = \"tcp\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n}\n\n#Subnet\n\nresource \"aws_subnet\" \"private_subnet\" {\n  vpc_id            = aws_default_vpc.default.id\n  cidr_block        = \"172.31.0.0/24\"\n  availability_zone = \"ap-south-1a\"\n}\n\nresource \"aws_instance\" \"prod-web-server\" {\n  ami                    = \"ami-04b1ddd35fd71475a\"\n  count                  = 2\n  key_name               = \"test_key\"\n  instance_type          = \"r5.large\"\n  vpc_security_group_ids = [aws_security_group.prod-web-servers-sg.id]\n  subnet_id              = aws_subnet.private_subnet.id\n}\n`\n```",
      "question_score": 5,
      "answer_score": 13,
      "created_at": "2021-01-08T12:58:09",
      "url": "https://stackoverflow.com/questions/65628538/terraform-shows-invalidgroup-notfound-while-creating-an-ec2-instance"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-aws",
      "question_id": 72670144,
      "title": "How do I use the value of provider default tags in a data source or resource block in terraform?",
      "problem": "Below is a small snippet of a set of terraform script I'm trying to build. The goal is to define an IAM policy that will be attached to a new IAM role that I will create.\nMy problem is I'm trying to use the environment tag that I've defined in my AWS provider's `default_tags` block but I'm not sure how. The goal is to pull the environment value as part of the S3 prefix in the IAM policy document instead of having it hard coded.\nIs there a way to do this?\n```\n`terraform {\n  required_providers {\n    aws = {\n      source = \"hashicorp/aws\"\n      version = \"4.19.0\"\n    }\n  }\n  required_version = \">=1.2.3\"\n}\n\nprovider \"aws\" {\n  default_tags {\n      tags = {\n        Environment = \"dev\"\n        Application = \"myapp\"\n        Terraform = \"true\"\n      }\n    }\n}\n\ndata \"aws_iam_policy_document\" \"this\" {\n\n  statement {\n    sid = \"S3BucketAccess\"\n    actions = \"s3:*\"\n    resources = [\n      \"${data.aws_s3_bucket.this.arn}/dev\"\n    ]\n  }\n}\n\ndata \"aws_s3_bucket\" \"this\" {\n  bucket = \"myBucket\"\n}\n`\n```\nNotice",
      "solution": "A solution without code duplication is to use `aws_default_tags`:\n```\n`terraform {\n  required_providers {\n    aws = {\n      source = \"hashicorp/aws\"\n      version = \"4.19.0\"\n    }\n  }\n  required_version = \">=1.2.3\"\n}\n\nprovider \"aws\" {\n  default_tags {\n      tags = {\n        Environment = \"dev\"\n        Application = \"myapp\"\n        Terraform = \"true\"\n      }\n    }\n}\n\n# Get the default tags from the provider\ndata \"aws_default_tags\" \"my_tags\" {}\n\ndata \"aws_iam_policy_document\" \"this\" {\n  statement {\n    sid = \"S3BucketAccess\"\n    actions = \"s3:*\"\n    resources = [\"${data.aws_s3_bucket.this.arn}/${data.aws_default_tags.my_tags.tags.Environment}/*\"\n    ]\n  }\n}\n`\n```",
      "question_score": 5,
      "answer_score": 9,
      "created_at": "2022-06-18T16:46:01",
      "url": "https://stackoverflow.com/questions/72670144/how-do-i-use-the-value-of-provider-default-tags-in-a-data-source-or-resource-blo"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-aws",
      "question_id": 72143566,
      "title": "Terraform is throwing InvalidArgumentException Duplicate ProcessorParameter passed to ProcessingConfiguration on Kinesis Firehose dynamic partitioning",
      "problem": "I'm trying to create a Kinesis Firehose using terraform with dynamic partitioning using two partition queries from the JSON I'm recieving, my processing configuration looks like this\n```\n`processing_configuration {\n  enabled = true\n  processors {\n    type = \"RecordDeAggregation\"\n    parameters {\n      parameter_name  = \"SubRecordType\"\n      parameter_value = \"JSON\"\n    }\n  }\n  processors {\n    type = \"MetadataExtraction\"\n    parameters {\n      parameter_name  = \"JsonParsingEngine\"\n      parameter_value = \"JQ-1.6\"\n    }\n    parameters {\n      parameter_name  = \"MetadataExtractionQuery\"\n      parameter_value = \"{transaction_id:.transaction_id}\"\n    }\n    parameters {\n      parameter_name  = \"MetadataExtractionQuery\"\n      parameter_value = \"{stage:.stage}\"\n    }\n  }\n}\n`\n```\nBut when I execute this part of the code it returns a duplication error for the processing configuration.\n\nI also tried to create an appart processor for the new ExtractionQuery, it looks like this\n```\n`processing_configuration {\n  enabled = true\n  processors {\n    type = \"RecordDeAggregation\"\n    parameters {\n      parameter_name  = \"SubRecordType\"\n      parameter_value = \"JSON\"\n    }\n  }\n  processors {\n    type = \"MetadataExtraction\"\n    parameters {\n      parameter_name  = \"JsonParsingEngine\"\n      parameter_value = \"JQ-1.6\"\n    }\n    parameters {\n      parameter_name  = \"MetadataExtractionQuery\"\n      parameter_value = \"{transaction_id:.transaction_id}\"\n    }\n  }\n  processors {\n    type = \"MetadataExtraction\"\n    parameters {\n      parameter_name  = \"JsonParsingEngine\"\n      parameter_value = \"JQ-1.6\"\n    }\n    parameters {\n      parameter_name  = \"MetadataExtractionQuery\"\n      parameter_value = \"{stage:.stage}\"\n    }\n  }\n}\n`\n```\nBut it fails with an error that says only one MetadataExtraction processor is allowed.",
      "solution": "Solved by merging both queries in one using JQ format, that way firehose would separate them, tried it using this snippet and worked.\n```\n`processing_configuration {\n  enabled = true\n  processors {\n    type = \"RecordDeAggregation\"\n    parameters {\n      parameter_name  = \"SubRecordType\"\n      parameter_value = \"JSON\"\n    }\n  }\n  processors {\n    type = \"MetadataExtraction\"\n    parameters {\n      parameter_name  = \"JsonParsingEngine\"\n      parameter_value = \"JQ-1.6\"\n    }\n    parameters {\n      parameter_name  = \"MetadataExtractionQuery\"\n      parameter_value = \"{transaction_id:.transaction_id,stage:.stage}\"\n    }\n  }\n}\n`\n```",
      "question_score": 5,
      "answer_score": 9,
      "created_at": "2022-05-06T16:58:36",
      "url": "https://stackoverflow.com/questions/72143566/terraform-is-throwing-invalidargumentexception-duplicate-processorparameter-pass"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-aws",
      "question_id": 70086929,
      "title": "terraform aws_acm_certificate_validation.cert_api: Still creating... [4m21s elapsed] until timeout",
      "problem": "The ACM Certificate Validation never completes, it times out after about 45 mins, looking at the AWS Hosted Zone for the domain, it has a cname record.  It never reaches the create the Api Gateway Domain section.\nmain.tf\n```\n`resource \"aws_acm_certificate\" \"cert_api\" {\n  domain_name       = var.api_domain\n  validation_method = \"DNS\"\n\n  tags = {\n    Name = var.api_domain\n  }\n}\n\nresource \"aws_acm_certificate_validation\" \"cert_api\" {\n  certificate_arn         = aws_acm_certificate.cert_api.arn\n  validation_record_fqdns = aws_route53_record.cert_api_validations.*.fqdn\n}\n\nresource \"aws_route53_zone\" \"api\" {\n  name = var.api_domain\n}\n\nresource \"aws_route53_record\" \"cert_api_validations\" {\n  allow_overwrite = true\n  count           = length(aws_acm_certificate.cert_api.domain_validation_options)\n\n  zone_id = aws_route53_zone.api.zone_id\n  name    = element(aws_acm_certificate.cert_api.domain_validation_options.*.resource_record_name, count.index)\n  type    = element(aws_acm_certificate.cert_api.domain_validation_options.*.resource_record_type, count.index)\n  records = [element(aws_acm_certificate.cert_api.domain_validation_options.*.resource_record_value, count.index)]\n  ttl     = 60\n}\n\nresource \"aws_route53_record\" \"api-a\" {\n  name    = aws_apigatewayv2_domain_name.api.domain_name\n  type    = \"A\"\n  zone_id = aws_route53_zone.api.zone_id\n\n  alias {\n    name                   = aws_apigatewayv2_domain_name.api.domain_name_configuration[0].target_domain_name\n    zone_id                = aws_apigatewayv2_domain_name.api.domain_name_configuration[0].hosted_zone_id\n    evaluate_target_health = false\n  }\n}\n\nresource \"aws_apigatewayv2_domain_name\" \"api\" {\n  domain_name = var.api_domain\n\n  domain_name_configuration {\n    certificate_arn = aws_acm_certificate.cert_api.arn\n    endpoint_type   = \"REGIONAL\"\n    security_policy = \"TLS_1_2\"\n  }\n}\n`\n```",
      "solution": "If the hosted zone is destroyed and re-provisioned, new name server records are associated with the new hosted zone. However,\nthe domain name might still have the previous name server records\nassociated with it.\nIf AWS Route 53 is used as the domain name registrar, head to Route 53 > Registered domains > ${your-domain-name} > Add or edit name servers and add the\nnewly associated name server records from the hosted zone to the registered domain.",
      "question_score": 5,
      "answer_score": 9,
      "created_at": "2021-11-23T20:42:41",
      "url": "https://stackoverflow.com/questions/70086929/terraform-aws-acm-certificate-validation-cert-api-still-creating-4m21s-elap"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-aws",
      "question_id": 66389322,
      "title": "Terraform error &quot;Provisioned Concurrency Configs cannot be applied to unpublished function versions&quot; - how to solve?",
      "problem": "I attempted to create a Provisioned Concurrency Lambda like so:\n```\n`locals {\n  lambda_name = \"mylambda\"\n  provisioned_concurrency = 10\n}\n\nmodule \"mylambda-lambda\" {\n  source = \"terraform-aws-modules/lambda/aws\"\n \n  function_name = \"${var.environment_name}-${local.lambda_name}\"\n  // abriged config detailes\n}\n\nmodule \"mylambda-alias\" {\n  source = \"terraform-aws-modules/lambda/aws//modules/alias\"\n  name = local.lambda_name\n  function_name = module.mylambda-lambda.this_lambda_function_name\n}\n\nresource \"aws_lambda_provisioned_concurrency_config\" \"auth_authorizer\" {\n  function_name = module.mylambda-lambda.this_lambda_function_name\n  provisioned_concurrent_executions = local.provisioned_concurrency\n  qualifier = module.mylambda-alias.this_lambda_alias_name\n}\n`\n```\nIt shows in the console that a qualifier (apparently, the alias) is set to `$LATEST`.\nbut with Concurrency setup I got an error\n```\n`Error: error putting Lambda Provisioned Concurrency Config (): InvalidParameterValueException: Provisioned Concurrency Configs cannot be applied to unpublished function versions.\n{\n  RespMetadata: {\n    StatusCode: 400,\n    RequestID: \"392f5609-086e-43f6-89af-a0ec0f7e3dc7\"\n  },\n  Message_: \"Provisioned Concurrency Configs cannot be applied to unpublished function versions.\",\n  Type: \"User\"\n}\n`\n```\nHow this error can be avoided?",
      "solution": "As pointed out by @jellycsc, you need to publish a new version.\nYou are already using the terraform aws module for this, which accepts the flag publish\n```\n`module \"mylambda-lambda\" {\n  source = \"terraform-aws-modules/lambda/aws\"\n  publish = true\n \n  function_name = \"${var.environment_name}-${local.lambda_name}\"\n  // abriged config detailes\n}\n\n`\n```\nfull example here",
      "question_score": 5,
      "answer_score": 8,
      "created_at": "2021-02-26T17:19:38",
      "url": "https://stackoverflow.com/questions/66389322/terraform-error-provisioned-concurrency-configs-cannot-be-applied-to-unpublishe"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-aws",
      "question_id": 75136993,
      "title": "Error Elements of a set are identified only by their value and don&#39;t have any separate index or key to select with,",
      "problem": "I'm getting this error when I do terraform apply\n```\n`Error: Invalid index\n\u2502\n\u2502   on deployBuildAWS.tf line 57, in resource \"aws_instance\" \"packer_instance\":\n\u2502   57:   subnet_id              = data.aws_subnet_ids.packer_subnet.ids[0]\n\u2502\n\u2502 Elements of a set are identified only by their value and don't have any separate index or key to select with, so it's only possible to perform operations across all elements of the set.\n`\n```\nHere are the relevant bits of my code\n```\n`data \"aws_subnet_ids\" \"packer_subnet\" {\n  vpc_id = data.aws_vpcs.packer_vpc.ids[0]\n\n  tags = {\n    service = \"packerBuildDeploySubnet\"\n  }\n}\n\n...\n\nresource \"aws_instance\" \"packer_instance\" {\n  ami                    = data.aws_ami.packer_image.id\n  instance_type          = \"t2.small\"\n  subnet_id              = data.aws_subnet_ids.packer_subnet.ids[0]\n  vpc_security_group_ids = [\"${data.aws_security_groups.packer_security_group.ids[0]}\"]\n  key_name               = var.packerKeyName\n\n}\n`\n```\nI've tried the following but it didn't improve the situation\n```\n`    data \"aws_subnet_ids\" \"packer_subnet\" {\n     \n    \n     for_each = data.aws_subnet_ids.packer_subnet.ids\n    id = each.value\n    }\n`\n```\nFor completeness , here are some other blocks of code\n```\n`    data \"aws_vpcs\" \"packer_vpc\" {\n      tags = {\n        service = \"packerBuildDeployVPC\"\n      }\n    }\n    \n    data \"aws_subnet_ids\" \"packer_subnet\" {\n      vpc_id = data.aws_vpcs.packer_vpc.ids[0]\n    \n      tags = {\n        service = \"packerBuildDeploySubnet\"\n      }\n    }\n    \n    data \"aws_security_groups\" \"packer_security_group\" {\n      tags = {\n        service = \"packerBuildDeploySecurityGroup\"\n      }\n    }\n`\n```\nI'm using terraform 1.3.7 and packer 1.8.4...I've tried with various different versions of Terraform but still getting very similar errors..\nProvider =\n```\n`provider \"registry.terraform.io/hashicorp/aws\" {\n  version     = \"4.50.0\"\n`\n```\nI'm obviously quite new to terraform, can anybody point me in the right direction?",
      "solution": "The error message is correct: the `set` type in any language is unordered and therefore elements cannot be accessed by index. Since you only want to select one subnet, we can fix this by only attempting to Read one subnet:\n```\n`data \"aws_subnet\" \"packer_subnet\" { # Read single AWS subnet\n  vpc_id = data.aws_vpcs.packer_vpc.ids[0]\n\n  tags = {\n    service = \"packerBuildDeploySubnet\"\n  }\n}\n\nresource \"aws_instance\" \"packer_instance\" {\n  ami                    = data.aws_ami.packer_image.id\n  instance_type          = \"t2.small\"\n  subnet_id              = data.aws_subnet.packer_subnet.id # string value in data attribute\n  vpc_security_group_ids = [\"${data.aws_security_groups.packer_security_group.ids[0]}\"]\n  key_name               = var.packerKeyName\n}\n`\n```\nNote you will likely have similar issues with `data.ws_security_groups.packer_security_group` and `data.aws_vpcs.packer_vpc` which are not shown in the question, and therefore will need to apply similar fixes to those as well.",
      "question_score": 5,
      "answer_score": 7,
      "created_at": "2023-01-16T17:23:24",
      "url": "https://stackoverflow.com/questions/75136993/error-elements-of-a-set-are-identified-only-by-their-value-and-dont-have-any-se"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-aws",
      "question_id": 70950150,
      "title": "How to use an AWS cli command in a Terraform external data source",
      "problem": "If I run the following command on its own I get the expected result -\nThis :\n```\n`aws cloudfront list-cloud-front-origin-access-identities | jq -r ' .CloudFrontOriginAccessIdentityList.Items[] | select(.Comment == \"Created for Nackle Shared CF in pprd\").Id'\n`\n```\nReturns this:\n```\n`E1P6ZIBDB6I6FZ\n`\n```\nHow can I use the Terraform external data source to get the same result?\nI tried this :\n```\n`data \"external\" \"json\" {\nprogram = [\"sh\", \"-c\", \"aws cloudfront list-cloud-front-origin-access-identities | jq -r ' .CloudFrontOriginAccessIdentityList.Items[] | select(.Comment == \"Created for Nackle Shared CF in pprd\").Id'\"] \n}\n\noutput \"map\" {\nvalue = [\"${values(data.external.json.result)}\"] \n}\n`\n```\nBut it returns this error when I run the Terraform apply -\n```\n`Expected a comma to mark the beginning of the next item.\n`\n```\nI assume when it is written properly the \"value\" will be E1P6ZIBDB6I6FZ ?\nHow do I use the value as a variable in another part of my terraform?\nIs there a different way to approach this?\nI am new to Terraform and have never played with external data sources.",
      "solution": "The json parsing ability of external data source is very limited. It should be (escape quote and return new json):\n```\n`data \"external\" \"json\" {\nprogram = [\"sh\", \"-c\", \"aws cloudfront list-cloud-front-origin-access-identities | jq -r ' .CloudFrontOriginAccessIdentityList.Items[] | select(.Comment == \\\"Created for Nackle Shared CF in pprd\\\") |  {id: .Id}'\"] \n}\n`\n```\nThen you access the Id as:\n```\n`data.external.json.result[\"id\"]\n`\n```",
      "question_score": 5,
      "answer_score": 7,
      "created_at": "2022-02-02T04:17:12",
      "url": "https://stackoverflow.com/questions/70950150/how-to-use-an-aws-cli-command-in-a-terraform-external-data-source"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-aws",
      "question_id": 70232248,
      "title": "Not able to create Zip file for AWS Lambda Fx in Gitlab through Terraform",
      "problem": "I am trying to create a lambda function through terraform for files present in gitlab repo however I am getting error in CICD pipeline:\n\"./lambda_function.zip: no such file or directory\"\nThe folder (src folder) containing the lambda function python file is different than the folder (terraform) containing the terraform file.\nMy Gitlab Project looks like\nProjectName\n-src\n\nlambda_function.py\n\n-terraform\n\nlambda.tf\n\nAnd the terraform code in lambda.tf is:\n```\n`data \"archive_file\" \"lambda\" {\ntype = \"zip\"\nsource_file = \"../src/lambda_function.py\"\noutput_path = \"lambda_function.zip\"\n`\n```\n}\n```\n`    resource \"aws_lambda_function\" \"automation-lambda\" \n{filename=data.archive_file.lambda.output_path\n  description       = \"Creating lambda\"\n  function_name     = \"lambda_fx\"\n  role              = \"xxxxxxxxxxxxx\"  \n  handler           = \"lambda_function.lambda_handler\"\n  memory_size       =  128\n  timeout           =  300\n  source_code_hash  = data.archive_file.lambda.output_base64sha256\n  runtime = \"python3.7\"\n}\n\n`\n```\nPlease suggest how the issue can be resolved.\nThanks",
      "solution": "The problem in short:\nThe problem comes down to this: `terraform plan` creates `data.archive_file` resources, that are later used by `terraform apply`. If you do these two terraform commands in separate gitlab pipeline stages. The zip files generated in the `plan` stage, won't be available to the `apply` stage. Unless you add the output directory as an artifact\nThe longer version of the answer\nIf you are using multiple stages of the gitlab pipeline, one to plan, then one to apply. This is your problem.\nThe \"dist\" zip files are created during the plan stage, so you need to add the $PWD/dist as an artifact to your pipeline. Then in your apply stage, tell the pipeline it needs the plan stage, making the artifacts available to the apply command.\nSo in our pipeline, I had something like this:\n```\n`plan_lambda:\n  stage: plan\n  needs:\n    - init_lambda\n    - validate_lambda\n  script:\n    - terraform -chdir=${PROJECT} plan -out=planfile ${args[@]}\n  artifacts:\n    paths:\n      - ${PROJECT}/planfile\n      # This is important, data.archive_file's are generated during plan stage, not apply, so these artifacts need to be stored\n      # https://github.com/hashicorp/terraform-provider-archive/issues/39#issuecomment-1013680518\n      - ${PWD}/dist\n`\n```\nand then in the apply stage\n```\n`apply_lambda:\n  stage: apply\n  when: manual\n  needs:\n    - init_lambda\n    - plan_lambda\n    - apply_execution_role\n  variables:\n    PROJECT: lambda\n  script:\n    - terraform -chdir=${PROJECT} apply -auto-approve planfile\n`\n```\nand here, the generated dist zip files generated in the plan stage will be available to the apply stage and your problem should be fixed.\nFor extra information and the entire conversation that led me to this solution, here is the ticket in the terraform github repository:\nhttps://github.com/hashicorp/terraform-provider-archive/issues/39#issuecomment-1013680518\nWhat does \"dist\" or \"dist zip\" files mean?\nHere is some terraform code which runs yarn to install node_modules and then packages up the source code into a zip file so you can deploy on AWS Lambda\n```\n`resource \"null_resource\" \"run_yarn\" {\n  triggers  =  {\n    always_run = timestamp()\n  }\n\n  provisioner \"local-exec\" {\n    command = \"yarn --cwd ${local.root_path}/app/src install\"\n  }\n}\n\ndata \"archive_file\" \"app_src\" {\n  depends_on    = [null_resource.run_yarn]\n  type          = \"zip\"\n  source_dir    = \"${local.root_path}/app/src\"\n  output_path   = \"${local.root_path}/dist/app.zip\"\n}\n`\n```\nSo \"dist\" files, are the distributed zip files generated and uploaded to AWS Lambda",
      "question_score": 5,
      "answer_score": 6,
      "created_at": "2021-12-05T08:24:57",
      "url": "https://stackoverflow.com/questions/70232248/not-able-to-create-zip-file-for-aws-lambda-fx-in-gitlab-through-terraform"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-aws",
      "question_id": 72680265,
      "title": "How to create secrets manager secret resource policy that references the secret itself using Terraform?",
      "problem": "I'm trying to create a terraform script that creates an AWS secrets manager secret with a resource based policy that grants an IAM role permissions to `secretsmanager:GetSecretValue` on that specific secret.\nI'm currently running into Terraform cycle issue between the secrets manager secret and the IAM policy document. Here's what my code looks like:\n```\n`resource \"aws_secretsmanager_secret\" \"this\" {\n  name = \"mySecret\"    \n  policy = data.aws_iam_policy_document.this.json\n}\n\ndata \"aws_iam_policy_document\" \"this\" {\n  statement {\n    sid = \"ReadPermissions\"\n    principals {\n      type = \"aws\"\n      identifiers = [data.aws_iam_role.this.arn]\n    }\n    actions = [\"secretsmanager:GetSecretValue\"]\n    resources = [aws_secretsmanager_secret.this.arn]\n  }\n}\n\ndata \"aws_iam_role\" \"this\" {\n  name = \"myRole\"\n}\n`\n```\nWhat's the best way to resolve this?",
      "solution": "You can try to use aws_secretsmanager_secret_policy, it can create a resource policy instead of IAM policy.\nExample:\n```\n`resource \"aws_secretsmanager_secret\" \"example\" {\n  name = \"example\"\n}\n\nresource \"aws_secretsmanager_secret_policy\" \"example\" {\n  secret_arn = aws_secretsmanager_secret.example.arn\n\n  policy = <<POLICY\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"EnableAnotherAWSAccountToReadTheSecret\",\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"AWS\": \"arn:aws:iam::123456789012:root\"\n      },\n      \"Action\": \"secretsmanager:GetSecretValue\",\n      \"Resource\": \"*\"\n    }\n  ]\n}\nPOLICY\n}\n`\n```",
      "question_score": 5,
      "answer_score": 5,
      "created_at": "2022-06-19T22:51:22",
      "url": "https://stackoverflow.com/questions/72680265/how-to-create-secrets-manager-secret-resource-policy-that-references-the-secret"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-aws",
      "question_id": 70657780,
      "title": "How to make a single-AZ (non-HA) RDS instance with terraform?",
      "problem": "In this AWS Database Blog, they assert that\n\nYou can set up Amazon RDS in a Single-AZ database (DB) instance or a\nMulti-AZ DB instance for high availability requirements\n\nand that you can\n\n...modify existing Single-AZ instances to become Multi-AZ deployments.\n\nFurthermore,\n\n...you can create a Multi-AZ read replica, synchronize it with your\nSingle-AZ DB instance, and then promote it as your primary DB instance\nto minimize latencies during conversion.\n\nAdditionally, in v1.32 of the official AWS VPC Module there are multiple references to the usage of single_nat_gateway, particularly\n\nIf single_nat_gateway = true, then all private subnets will route\ntheir Internet traffic through this single NAT gateway.\n\nand in the official RDS module, `multi_az` is shown as defaulting to `false` (link).\nDespite this, I am getting the following error\n```\n`\u2577\n\u2502 Error: DBSubnetGroupDoesNotCoverEnoughAZs: The DB subnet group doesn't meet Availability Zone (AZ) coverage requirement. Current AZ coverage: us-west-2a. Add subnets to cover at least 2 AZs.\n\u2502   status code: 400, request id: *****\n\u2502 \n\u2502   with module.rds.module.db_subnet_group.aws_db_subnet_group.this[0],\n\u2502   on .terraform/modules/rds/modules/db_subnet_group/main.tf line 8, in resource \"aws_db_subnet_group\" \"this\":\n\u2502    8: resource \"aws_db_subnet_group\" \"this\" {\n`\n```\nwhen attempting to `terraform apply` this `main.tf` configuration:\n```\n`module \"rds\" {\n  source                                = \"terraform-aws-modules/rds/aws\"\n  version                               = \"~> 3.4.0\"\n  identifier                            = \"${var.env}-${var.user}-${local.db_name}\"\n  engine                                = var.postgres.engine\n  engine_version                        = var.postgres.engine_version\n  family                                = var.postgres.family\n  major_engine_version                  = var.postgres.major_engine_version\n  instance_class                        = var.postgres.instance_class\n  allocated_storage                     = var.postgres.allocated_storage\n  max_allocated_storage                 = var.postgres.max_allocated_storage\n  storage_encrypted                     = var.postgres.storage_encrypted\n  password                              = random_password.password.result\n  port                                  = var.postgres.port\n  multi_az                              = false\n  subnet_ids                            = [data.aws_subnet.priv1.id]\n  vpc_security_group_ids                = [module.db_security_group.security_group_id]\n  maintenance_window                    = var.postgres.maintenance_window\n  backup_window                         = var.postgres.backup_window\n  enabled_cloudwatch_logs_exports       = var.postgres.enabled_cloudwatch_logs_exports\n  backup_retention_period               = var.postgres.backup_retention_period\n  skip_final_snapshot                   = var.postgres.skip_final_snapshot\n  deletion_protection                   = var.postgres.deletion_protection\n  performance_insights_enabled          = var.postgres.performance_insights_enabled\n  performance_insights_retention_period = var.postgres.performance_insights_retention_period\n  create_monitoring_role                = var.postgres.create_monitoring_role\n  monitoring_role_name                  = \"${var.env}-${var.user}-${var.postgres.monitoring_role_name}\"\n  monitoring_interval                   = var.postgres.monitoring_interval\n  snapshot_identifier                   = var.postgres.snapshot_identifier\n  iam_database_authentication_enabled   = var.postgres.iam_auth\n  apply_immediately                     = true\n  tags = {\n    Name        = \"${var.env}-${var.user}-rds\"\n    Terraform   = \"true\"\n    Environment = var.env\n    Created     = timestamp()\n  }\n}\n`\n```\nwith this `postgres` variable defined in my `terraform.tfvars`:\n```\n`postgres = {\n  db_name = \"postgres-db\"\n  # All available versions: https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_PostgreSQL.html#PostgreSQL.Concepts\n  engine                                = \"postgres\"\n  engine_version                        = \"11.12\"\n  family                                = \"postgres11\" # DB parameter group\n  major_engine_version                  = \"11\"         # DB option group\n  instance_class                        = \"db.t2.micro\"\n  allocated_storage                     = 100\n  max_allocated_storage                 = 200\n  storage_encrypted                     = false\n  port                                  = 5432\n  multi_az                              = false\n  maintenance_window                    = \"Mon:00:00-Mon:03:00\"\n  backup_window                         = \"03:00-06:00\"\n  enabled_cloudwatch_logs_exports       = [\"postgresql\", \"upgrade\"]\n  backup_retention_period               = 0\n  skip_final_snapshot                   = true\n  deletion_protection                   = false\n  performance_insights_enabled          = false\n  performance_insights_retention_period = 7\n  create_monitoring_role                = true\n  monitoring_role_name                  = \"monitoring_role\"\n  monitoring_interval                   = 60\n  snapshot_identifier                   = \"arn:aws:rds:us-west-2:999999999999:snapshot:rds-ss\"\n  iam_auth                              = true\n}\n`\n```\nSimilar questions on SO seem to all have answers with the theme that you must provide multiple availability zones, which implies at least two subnets, which - if you are creating them manually as private subnets - would then each need their own NAT gateway.  That seems unnecessarily expensive and constraining, especially for development and test environments.\nHow can I deploy a single-az RDS Postgres instance with these components?",
      "solution": "A DB subnet group has to have multiple subnets. That's a requirement of RDS that you can't bypass. Even if you are only deploying a single instance, if that entire availability zone were to go down, Amazon RDS would automatically spin up a new instance in one of the other availability zones you have specified. That's one of the managed database services that you get automatically with Amazon RDS.\nSo even if you are deploying a single-az instance, you have to specify multiple availability zones in the DB subnet group.",
      "question_score": 5,
      "answer_score": 5,
      "created_at": "2022-01-10T20:14:00",
      "url": "https://stackoverflow.com/questions/70657780/how-to-make-a-single-az-non-ha-rds-instance-with-terraform"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-aws",
      "question_id": 67566102,
      "title": "Terraform using count.index with tags",
      "problem": "using terraform, i'm trying to include the count in the tags of my resource using count.index, but getting this error :\n\nError: Incorrect attribute value type\n\u2502\n\u2502   on ..\\modules\\sn\\ressources.tf line 16, in resource \"aws_subnet\" \"prod_sn\":\n\u2502   16:   tags                    = var.sn_tags[count.index]\n\u2502     \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\u2502     \u2502 count.index is a number, known only after apply\n\u2502     \u2502 var.sn_tags is a list of string, known only after apply\n\u2502\n\u2502 Inappropriate value for attribute \"tags\": map of string required.\n\nvars.tf\n```\n`variable \"sn_tags\" {\n  type        = list (string)\n  default     = [\"aa\", \"bb\"]\n}\n`\n```\nressources.tf\n```\n`resource \"aws_subnet\" \"prod_sn\" {\n  count                   = length(var.sn_cidr)\n  vpc_id                  = var.vpc_id\n  cidr_block              = var.sn_cidr[count.index]\n  availability_zone       = data.aws_availability_zones.azs.names[count.index]\n  tags                    = var.sn_tags[count.index] \n}\n`\n```\nmain.tf\n```\n`# Create Public Subnet on availability_zone \"3a\"\nmodule \"publicSn-a\" {\n  source            = \"../modules/sn\"\n  vpc_id            = module.vpc.vpcId \n  sn_cidr           = [\"10.0.1.0/24\", \"10.0.2.0/24\"]\n  sn_tags           = [\"prodPublicA\",\"prodPublicB\"]\n  \n}\n`\n```",
      "solution": "Your issue is that each loop iteration is trying to pass a `string` type to the `tags` parameter. If you break it down to just a single resource without the count (using the first element for now) then your current code is basically this:\n```\n`resource \"aws_subnet\" \"prod_sn\" {\n  vpc_id                  = var.vpc_id\n  cidr_block              = \"10.0.1.0./24\"\n  availability_zone       = \"eu-west-1a\" # Note may not be this but the data source and the index will at least resolve to a single string AZ\n  tags                    = \"prodPublicA\"\n}\n`\n```\nIf we look at the documentation for the `aws_subnet` resource we can see that the `tags` parameter wants a `map`, not a `string` as the error implies.\nYou could fix this by changing your `list(string)` variable into a `list(map)` so instead you have something like this instead:\n```\n`variable \"sn_tags\" {\n  type = list(map)\n}\n`\n```\nand\n```\n`# Create Public Subnet on availability_zone \"3a\"\nmodule \"publicSn-a\" {\n  source            = \"../modules/sn\"\n  vpc_id            = module.vpc.vpcId \n  sn_cidr           = [\"10.0.1.0/24\", \"10.0.2.0/24\"]\n  sn_tags           = [\n    {\n      name = \"prodPublicA\"\n    },\n    {\n      name = \"prodPublicB\"\n    },\n  ] \n}\n`\n```\nAlternatively if you just want to add a `Name` tag to all the subnets and don't want more flexibility with the tags instead you could rework it like this:\n```\n`variable \"sn_names\" {\n  type = list(string)\n}\n\nresource \"aws_subnet\" \"prod_sn\" {\n  count                   = length(var.sn_cidr)\n  vpc_id                  = var.vpc_id\n  cidr_block              = var.sn_cidr[count.index]\n  availability_zone       = data.aws_availability_zones.azs.names[count.index]\n  tags                    = {\n    Name = var.sn_names[count.index]\n  }\n}\n`\n```\nand call it like so:\n```\n`# Create Public Subnet on availability_zone \"3a\"\nmodule \"publicSn-a\" {\n  source            = \"../modules/sn\"\n  vpc_id            = module.vpc.vpcId \n  sn_cidr           = [\"10.0.1.0/24\", \"10.0.2.0/24\"]\n  sn_names          = [\"prodPublicA\",\"prodPublicB\"]\n}\n`\n```",
      "question_score": 5,
      "answer_score": 4,
      "created_at": "2021-05-17T10:12:58",
      "url": "https://stackoverflow.com/questions/67566102/terraform-using-count-index-with-tags"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-aws",
      "question_id": 74611906,
      "title": "The plugin.(*GRPCProvider)",
      "problem": "I am trying to deploy a Terraform stack from a Linux EC2 and getting the following error:\n```\n` The plugin.(*GRPCProvider).ApplyResourceChange request was cancelled.\n`\n```\nand\n```\n`\u2502 The plugin.(*GRPCProvider).PlanResourceChange request was cancelled.\n`\n```\nand\n```\n`\u2502 The plugin.(*GRPCProvider).ValidateResourceConfig request was cancelled.\n`\n```\nfor random resources.\nTerraform versions: 1.3.1_linux_amd64, 1.3.5_linux_amd64.\nThe reason am noting the Linux ec2 its because when depolying from my local computer I am not getting any errors, with the same Terraform versions but for mac.\nprovider.tf:\n```\n`terraform {\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~> 4.0\"\n    }\n  }\n}\n\nprovider \"aws\" {\n  region                   = \"us-east-1\"\n}\n`\n```",
      "solution": "I fixed the version to 4.33, the last one that worked for me, and got rid of this error.\nprovider.tf:\n```\n`terraform {\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"4.33.0\"\n    }\n  }\n}\n`\n```",
      "question_score": 5,
      "answer_score": 1,
      "created_at": "2022-11-29T10:39:46",
      "url": "https://stackoverflow.com/questions/74611906/the-plugin-grpcprovider"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-aws",
      "question_id": 66868470,
      "title": "Lambda security group deletion hanging and can&#39;t be deleted in AWS console",
      "problem": "I used Terraform to create a security group and lambda, I changed the security name and wanted to apply terraform again, Terraform is trying to delete the old security group and create the one with the new name.\nBut the logs show me that it's been trying to delete the old security group for almost half an hour!!!!\nI looked for solution online, a potential solution is that I can delete the group in the console, but AWS warned me that it can't be deleted because there are network interfaces attached to it, so I tried to deleted the network interfaces, but they can't be dettached & deleted, and the status showed `in use`.\nHas anyone encountered this situation? Please help me....Thanks.",
      "solution": "Based on the comments, this is resolved by deleting the associated lambda from the console manually.\nUpdate: Issue affecting HashiCorp Terraform resource deletions after the VPC Improvements to AWS Lambda\nWhy can't I detach or delete an elastic network interface created by Lambda?",
      "question_score": 5,
      "answer_score": 3,
      "created_at": "2021-03-30T11:44:32",
      "url": "https://stackoverflow.com/questions/66868470/lambda-security-group-deletion-hanging-and-cant-be-deleted-in-aws-console"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-aws",
      "question_id": 72880636,
      "title": "VPC Peering is replaced all the time by Terraform",
      "problem": "I'm trying to create VPC Peering between two VPCs in two different accounts. One is managed by me and another one by others and I don't have access to it.\nI'm using the next snippet of Terraform script.\n```\n`resource \"aws_vpc_peering_connection\" \"a\" {\n  peer_owner_id = var.a.aws_account_id\n  peer_vpc_id   = var.a.vpc_id\n  vpc_id        = aws_vpc.main.id\n  peer_region   = \"eu-west-1\"\n\n  requester {\n    allow_remote_vpc_dns_resolution = false\n  }\n}\n`\n```\nNext, it is going to be manually accepted by those who manage that account.\nThe problem is whether Peering is accepted or not Terraform wants to `replace` that Peering connection:\n```\n`  # module.vpc.aws_vpc_peering_connection.a is tainted, so must be replaced\n-/+ resource \"aws_vpc_peering_connection\" \"a\" {\n      ~ accept_status = \"active\" -> (known after apply)\n      ~ id            = \"pcx-00000000000000000\" -> (known after apply)\n        # (5 unchanged attributes hidden)\n\n      + accepter {\n          + allow_classic_link_to_remote_vpc = (known after apply)\n          + allow_remote_vpc_dns_resolution  = (known after apply)\n          + allow_vpc_to_remote_classic_link = (known after apply)\n        }\n\n        # (1 unchanged block hidden)\n    }\n`\n```\nI have already tried to prevent the replacement by using `lifecycle`\n```\n`  lifecycle {\n    ignore_changes = all\n  }\n`\n```\nBut it doesn't help...",
      "solution": "Try to untaint the resource e.g.\n```\n`terraform untaint aws_vpc_peering_connection.a\n`\n```",
      "question_score": 5,
      "answer_score": 2,
      "created_at": "2022-07-06T11:01:26",
      "url": "https://stackoverflow.com/questions/72880636/vpc-peering-is-replaced-all-the-time-by-terraform"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-aws",
      "question_id": 68025133,
      "title": "Error configuring Terraform AWS Provider - Linux",
      "problem": "[----------------------- UPDATE --------------------------]\nI have tried a tutorial to integrate terraform with s3 now. The S3 bucket is created and I have created an IAM user, and I am using its Access key and secret key.\nNonetheless I keep getting errors regarding the providers after `terraform init`:\nbackend.tf\n```\n`terraform {\n  required_version = \">=0.12.0\"\n  backend \"s3\" {\n    region  = \"us-east-1\"\n    key     = \"terraform.tfstate\"\n    profile = \"tu\"\n    bucket  = \"terraformstatebucket3107\"\n  }\n}\n`\n```\nconfig file in .aws folder\n```\n`[tu]\nregion = us-east-1\noutput = json\n`\n```\ncredentials file in .aws folder\n```\n`[tu]\naws_access_key_id = AKIA*****************\naws_secret_access_key = nn3M1*****************\n`\n```\nError:\n```\n`Initializing the backend...\n\nError: error configuring S3 Backend: no valid credential sources for S3 Backend found.\n\nPlease see https://www.terraform.io/docs/backends/types/s3.html\nfor more information about providing credentials.\n\nError: NoCredentialProviders: no valid providers in chain. Deprecated.\n    For verbose messaging see aws.Config.CredentialsChainVerboseErrors\n`\n```",
      "solution": "So, I have tried every solution that was suggested here, but unfortunately none of them solved my issue.\nAfter some digging, I found a solution that worked for me. That was executing the `terraform init` command with the `-backend-config` option like this:\n```\n`terraform init -backend-config=\"access_key=\" -backend-config=\"secret_key=\"\n`\n```\nThis is the question where I found this solution:\nError while configuring Terraform S3 Backend",
      "question_score": 5,
      "answer_score": 2,
      "created_at": "2021-06-17T20:58:46",
      "url": "https://stackoverflow.com/questions/68025133/error-configuring-terraform-aws-provider-linux"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-aws",
      "question_id": 69311135,
      "title": "Can you tag inbound Name on inbound rules on an AWS security group using terraform?",
      "problem": "I'm trying to set individual names against each entry in my security group.\nMy example code that doesn't work below, I'd expect this to yield the result in the picture but the Name tag remains blank (i set this one as an example in the picture through the console). This tag works other places like the \"Allow-Google\" name tag against the security group. Is there a way I can achieve this ?\n```\n`resource \"aws_security_group\" \"allow-google-dns\" {\n  name        = \"allow-google-dns\"\n  description = \"Allows google dns\"\n  vpc_id      = var.vpcid\n\n  ingress = [\n    {\n      description      = \"allows google in\"\n      from_port        = 0\n      to_port          = 0\n      protocol         = \"-1\"\n      cidr_blocks = [\"8.8.8.8/32\" ]\n      ipv6_cidr_blocks = []\n      prefix_list_ids = []\n      security_groups = []\n      self = false\n      tags = {\n    Name = \"MyName\"\n           }\n    }\n  ]\n\n  tags = {\n    Name = \"Allow-Google\"\n  }\n}\n`\n```\nAWS Console",
      "solution": "Marcin is right (for the time being).\nAs of Terraform 1.0.7 with version 3.60.0 of the AWS provider, tagging rules isn't supported, neither in inline format or in external `aws_security_group_rule` resources.\nBut Terraform doesn't notice any tags that are in place, so you could conceivably add them after creation, and Terraform won't detect it as a change whenever you reapply.",
      "question_score": 5,
      "answer_score": 1,
      "created_at": "2021-09-24T09:27:42",
      "url": "https://stackoverflow.com/questions/69311135/can-you-tag-inbound-name-on-inbound-rules-on-an-aws-security-group-using-terrafo"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-aws",
      "question_id": 68098256,
      "title": "Run terraform modules conditionally",
      "problem": "I am trying to run modules conditionally. Below is the code. It works fine if the values are provided but if var.accounts[*].vpc_ids is blank, it fails saying var.vpc_id can't be empty. But that is basically the condition based on which the modules should run. If the vpc_id count is 0, then the modules should not run. Please help.\n```\n`resource \"aws_ec2_transit_gateway_vpc_attachment\" \"this\" {\n   transit_gateway_id = var.transit_gateway_id\n  vpc_id = var.vpc_id\n  subnet_ids = var.subnet_ids\n  dns_support                                     = \"disable\"\n  ipv6_support                                    = \"disable\"\n  transit_gateway_default_route_table_association = false\n  transit_gateway_default_route_table_propagation = false\n}\n\nlocals {\n  create_tgw_attach = var.accounts[*].vpc_ids != \"\" ? true : false\n}\n\nmodule \"tgw_peer2\" {\n  source = \"../modules/tgw\"\n    count = length(var.accounts[2].vpc_ids)\n  providers  = {\n    aws = aws.accepter2\n  }\n  create_tgw_attach      = local.create_tgw_attach\n  transit_gateway_id = aws_ec2_transit_gateway.this.id\n  vpc_id = var.accounts[2].vpc_ids[count.index]\n  subnet_ids = var.accounts[2].vpc_subnets[count.index].subnet_ids\n  destination_cidr_block = var.destination_cidr_block_route\n\n  share_tgw                             = true\n  create_tgw                            = false\n}\n\nmodule \"tgw_peer3\" {\n  source = \"../modules/tgw\"\n  create_tgw_attach      = local.create_tgw_attach\n  count = length(var.accounts[3].vpc_ids)\n  providers  = {\n    aws = aws.accepter3\n  }\n  transit_gateway_id = aws_ec2_transit_gateway.this.id\n  vpc_id = var.accounts[3].vpc_ids[count.index]\n  subnet_ids = var.accounts[3].vpc_subnets[count.index].subnet_ids\n\n  share_tgw                             = true\n  create_tgw                            = false  \n}\n`\n```",
      "solution": "I was trying to put the condition of vpc_id which I was using as a value in the module. I modified the condition and it worked. Below is the code.\n```\n`module \"tgw_peer1\" {\n  source = \"./modules/tgw\"\n  providers  = {\n    aws = aws.accepter1\n  }\n  count = var.accounts[1].account_id != \"\" ? length(var.accounts[1].vpc_ids) : 0\n  transit_gateway_id = aws_ec2_transit_gateway.this.id\n  vpc_id = var.accounts[1].vpc_ids[count.index]\n  subnet_ids = var.accounts[1].vpc_subnets[count.index].subnet_ids\n}\n`\n```",
      "question_score": 5,
      "answer_score": 1,
      "created_at": "2021-06-23T12:42:54",
      "url": "https://stackoverflow.com/questions/68098256/run-terraform-modules-conditionally"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-aws",
      "question_id": 67779762,
      "title": "How to create a wildcard to deny all requests from all ips in AWS WAF",
      "problem": "I got a microservice in an ECS instance in AWS behind a WAF, I want to create these rules:\n\nAllow specific IPs (done)\nAllow all connections from inside the VPN (done)\nDeny all the other requests.\n\nThe first two IP set are created, but I can't make the last one to work. I tried creating the IP set with `0.0.0.0/0` and another combinations without success.\nThis is my code, I removed ipset 1 and 2 (that are working), this is the ipset 3:\n```\n`resource \"aws_wafv2_ip_set\" \"ipset\" {\n  name = \"${var.app_name}-${var.environment_name}-whitelist-ips\"\n\n  scope              = \"REGIONAL\"\n  ip_address_version = \"IPV4\"\n\n  addresses = [\"0.0.0.0/0\"]\n}\n\nmodule \"alb_wafv2\" {\n  source = \"trussworks/wafv2/aws\"\n  version = \"~> 2.0\"\n\n  name = \"${var.app_name}-${var.environment_name}\"\n  scope = \"REGIONAL\"\n  alb_arn = aws_lb.app_lb.arn\n  associate_alb = true\n\n  ip_sets_rule = [\n    {\n      name       = \"${var.app_name}-${var.environment_name}-ip-blacklist\"\n      action     = \"deny\"\n      priority   = 1\n      ip_set_arn = aws_wafv2_ip_set.ipset.arn\n    }\n  ]\n}\n`\n```\n```\n`{\n  RespMetadata: {\n    StatusCode: 400,\n    RequestID: \"c98b2d3a-ebd0-44e0-a80a-702bc698598b\"\n  },\n  Field: \"IP_ADDRESS\",\n  Message_: \"Error reason: The parameter contains formatting that is not valid., field: IP_ADDRESS, parameter: 0.0.0.0/0\",\n  Parameter: \"0.0.0.0/0\",\n  Reason: \"The parameter contains formatting that is not valid.\"\n}\n`\n```\nTried to create an IP Set from the AWS Console with the same error:\n\nSo I got two questions, first, how can I do this? And the second one, is this the best approach?\nThanks in advance",
      "solution": "You don't need to block 0.0.0.0/0. After you created two IP rules, look \"Default web ACL action for requests that don't match any rules\" on WAF console and set Action to Block.",
      "question_score": 4,
      "answer_score": 4,
      "created_at": "2021-05-31T22:17:11",
      "url": "https://stackoverflow.com/questions/67779762/how-to-create-a-wildcard-to-deny-all-requests-from-all-ips-in-aws-waf"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-aws",
      "question_id": 65796015,
      "title": "Error creating DB instance: InvalidParameterValue: Invalid DB engine for PostgreSQL DB",
      "problem": "I am trying to create a PostgreSQL RDS instance using Terraform.\nHere is how my configuration looks:\n```\n`resource \"aws_db_subnet_group\" \"postgres\" {\n   name = \"postgres-subnets\"\n   subnet_ids = [\"mysub1\",\"mysub2\"]\n}    \nresource \"aws_db_instance\" \"myrds\" {\n   engine = \"postgresql\"\n   engine_version = \"12.4\"\n   instance_class = \"db.t2.micro\"\n   identifier = \"myrds\"\n   username = \"myuser\"\n   password = \"*******\"\n   allocated_storage = 10\n   storage_type = \"gp2\"\n   db_subnet_group_name = \"${aws_db_subnet_group.postgres.id}\"\n}\n`\n```\nIt fails with following error:\n```\n`Error: Error creating DB Instance: InvalidParameterValue: Invalid DB engine\n`\n```",
      "solution": "Terraform documentation needs to add the engine names which are supported:\n`engine = \"postgresql\"` is incorrect. Supported value is `\"postgres\"`",
      "question_score": 4,
      "answer_score": 16,
      "created_at": "2021-01-19T17:55:58",
      "url": "https://stackoverflow.com/questions/65796015/error-creating-db-instance-invalidparametervalue-invalid-db-engine-for-postgre"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-aws",
      "question_id": 68415948,
      "title": "If-else condition in terraform data source",
      "problem": "My question is very simple but unable to find any example or solution on the entire internet. I want to use the if-else condition in the `data source` of terraform and based on the value it should search the result. We have 2 different VPCs in our AWS account: 1. Prod VPC, 2. RDS VPC\nFilter should work based on boolean variable var.rds_vpc\nPseudocode:\n```\n`data \"aws_vpc\" \"vpc\" {\n  if var.rds_vpc == true:\n    tags = {\n      Name = \"rds_vpc\"\n      Type = \"database\"\n    }\n  else:\n    tags = {\n      Cluster     = \"live_vpc\"\n      Enviornment = \"production\"\n    }\n\n}\n`\n```\nIf both VPCs could have similar tags, I could simply pass the values via variables. But in the above case, the tags are also different.\nI would highly appreciate it if someone can help.",
      "solution": "There are various different ways to achieve this result depending on your detailed goals. The most direct answer would be to use a conditional expression to select one of two map values:\n```\n`  tags = (\n    var.rds_vpc ?\n    {\n      Name = \"rds_vpc\"\n      Type = \"database\"\n    } :\n    {\n      Cluster     = \"live_vpc\"\n      Enviornment = \"production\"\n    }\n  )\n`\n```\nIf you are writing a module where its abstraction makes sense to distinguish between exactly two very different kinds of tagging structures like this then this could be a reasonable way to go, but a structure like this does suggest that your module might be trying to solve too many problems at once and so I'd also consider some different designs.\n\nIf you are just trying to write a general module that works with virtual networks, which doesn't really need to \"know\" what the network really represents, then you could consider making it just take arbitrary tags as an input variable and pass them through exactly as given, and then the calling module can choose those tags differently depending on its knowledge of the purpose of the VPC:\n```\n`variables \"vpc_tags\" {\n  type    = map(string)\n}\n\ndata \"aws_vpc\" \"vpc\" {\n  tags = var.vpc_tags\n}\n`\n```\n\nBecause it seems like you are writing a module that uses a VPC rather than one that manages a VPC, you might also consider using a dependency inversion approach where the module just declares that it needs a VPC but leaves it up to the calling module to decide how to provide it:\n```\n`variables \"vpc\" {\n  # Include here the subset of VPC attributes\n  # your module actually needs.\n  type = object({\n    id         = string\n    cidr_block = string\n  })\n}\n`\n```\nElsewhere in your module you can then use `var.vpc.id` in the same situations where you would otherwise have used `data.aws_vpc.vpc.id`. The calling module can then either use a `data \"aws_vpc\"` block itself or pass in a VPC it was already managing, depending on what's appropriate for the situation:\n```\n`data \"aws_vpc\" \"rds\" {\n  tags = {\n    Name = \"rds_vpc\"\n    Type = \"database\"\n  }\n}\n\nmodule \"uses_vpc\" {\n  source = \"./uses-vpc\"\n\n  vpc = data.aws_vpc.rds\n}\n`\n```\n```\n`resource \"aws_vpc\" \"production\" {\n  cidr_block = \"10.1.0.0/16\"\n\n  tags = {\n    Cluster     = \"live_vpc\"\n    Enviornment = \"production\"\n  }\n}\n\nmodule \"uses_vpc\" {\n  source = \"./uses-vpc\"\n\n  vpc = aws_vpc.production\n}\n`\n```\nThis approach of using either `data \"aws_vpc\"` or `resource \"aws_vpc\"` with the same module can work because both the data resource type and the managed resource type have the `id` and `cidr_block` attributes that the variable's type constraint requires.\n\nI can't know what design assumptions and constraints are most important for your system, so I can't recommend any one of these approaches in particular, but I raise them because selecting two very different values depending on a boolean variable can sometimes suggest an insufficient separation of concerns.\nIn your case, for example, I'd worry if my overall system had a lot of conditions based on `var.rds_vpc` because that implies a lot of rigidity in your system design. If a later requirement calls for you to add a third VPC, how many different modules would you need to update to achieve that?\nPremature generalization can be harmful too -- there's no need to solve problems that you don't have anywhere on the horizion -- but the two alternative patterns I've suggested above are common approaches to separation of concerns in Terraform that don't tend to lead to much additional complexity.",
      "question_score": 4,
      "answer_score": 5,
      "created_at": "2021-07-17T00:41:54",
      "url": "https://stackoverflow.com/questions/68415948/if-else-condition-in-terraform-data-source"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-aws",
      "question_id": 69192628,
      "title": "How to attach multiple IAM roles to instance profile on AWS?",
      "problem": "I'm using Terraform to create IAM and EC2 as below.\nI want to attach a role named `ec2_role` to the EC2 instance profile. But it seems it only can attach one that created by `aws_iam_instance_profile`.\n```\n`resource \"aws_instance\" \"this\" {\n  # ..\n  iam_instance_profile    = aws_iam_instance_profile.this.name\n}\n\nresource \"aws_iam_instance_profile\" \"this\" {\n  name = \"ec2-profile\"\n  role = aws_iam_role.ec2_role.name\n}\n`\n```\nAbout the `ec2_role`, it uses an `ec2_role_policy`. But if I set `source_json = data.aws_iam_policy.amazon_ssm_managed_instance_core.policy` to `data \"aws_iam_policy_document\" \"ec2_role_policy\" {`, it throws an error.\n```\n`resource \"aws_iam_role\" \"ec2_role\" {\n  name               = \"ec2-role\"\n  assume_role_policy = data.aws_iam_policy_document.ec2_role_policy.json\n}\n\nresource \"aws_iam_policy\" \"ec2_policy\" {\n  name   = \"ec2-policy\"\n  policy = data.aws_iam_policy_document.ec2_use_role_policy.json\n}\n\nresource \"aws_iam_role_policy_attachment\" \"attach\" {\n  role       = aws_iam_role.ec2_role.name\n  policy_arn = aws_iam_policy.ec2_policy.arn\n}\n\ndata \"aws_iam_policy\" \"amazon_ssm_managed_instance_core\" {\n  arn = \"arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore\"\n}\n\ndata \"aws_iam_policy_document\" \"ec2_role_policy\" {\n  source_json = data.aws_iam_policy.amazon_ssm_managed_instance_core.policy\n\n  statement {                                   # Doc A\n    effect = \"Allow\"\n    principals {\n      identifiers = [\"ec2.amazonaws.com\"]\n      type        = \"Service\"\n    }\n    actions = [\"sts:AssumeRole\"]\n  }\n}\n\ndata \"aws_iam_policy_document\" \"ec2_use_role_policy\" {\n  statement {\n    effect    = \"Allow\"\n    actions   = [\"sts:AssumeRole\"]\n    resources = [\"arn:aws:iam::12313113231:role/s3-role\"]\n  }\n}\n`\n```\nThe error message is:\n```\n`Error: Error creating IAM Role ec2-role: MalformedPolicyDocument: Has prohibited field Resource\n    status code: 400, request id: 1111111-3333-2222-4444-2131331312\n\n  with aws_iam_role.ec2_role,\n  on main.tf line 10, in resource \"aws_iam_role\" \"ec2_role\":\n   10: resource \"aws_iam_role\" \"ec2_role\" {\n`\n```\nIf I remove the `source_json = data.aws_iam_policy.amazon_ssm_managed_instance_core.policy` from the `ec2_role_policy`, it works. But how to set it with `Doc A` together?",
      "solution": "As @hars34 mentioned in their answer, an instance profile can only contain one role but that role can have multiple policies attached to it. But that's not what you're doing there or what the error is complaining about.\nInstead, you seem to be confused with the `assume_role_policy` (also known as a \"trust policy\", this controls what IAM principals are allowed to use the role such as other AWS services or different AWS accounts etc) of a role and the role's permissions policy for what the role is allowed to do (eg read and write to an S3 bucket).\nIn the `assume_role_policy`/trust policy document you must specify a valid trust policy which must include a `Principal` block and cannot include a `Resource` block which is what your error message is complaining about:\n```\n`Error: Error creating IAM Role ec2-role: MalformedPolicyDocument: Has prohibited field Resource\n    status code: 400, request id: 1111111-3333-2222-4444-2131331312\n`\n```\nBecause you've concatenated your trust policy allowing EC2 instances to assume the role with a permission policy that looks like this:\n`{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"ssm:DescribeAssociation\",\n                \"ssm:GetDeployablePatchSnapshotForInstance\",\n                \"ssm:GetDocument\",\n                \"ssm:DescribeDocument\",\n                \"ssm:GetManifest\",\n                \"ssm:GetParameter\",\n                \"ssm:GetParameters\",\n                \"ssm:ListAssociations\",\n                \"ssm:ListInstanceAssociations\",\n                \"ssm:PutInventory\",\n                \"ssm:PutComplianceItems\",\n                \"ssm:PutConfigurePackageResult\",\n                \"ssm:UpdateAssociationStatus\",\n                \"ssm:UpdateInstanceAssociationStatus\",\n                \"ssm:UpdateInstanceInformation\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"ssmmessages:CreateControlChannel\",\n                \"ssmmessages:CreateDataChannel\",\n                \"ssmmessages:OpenControlChannel\",\n                \"ssmmessages:OpenDataChannel\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"ec2messages:AcknowledgeMessage\",\n                \"ec2messages:DeleteMessage\",\n                \"ec2messages:FailMessage\",\n                \"ec2messages:GetEndpoint\",\n                \"ec2messages:GetMessages\",\n                \"ec2messages:SendReply\"\n            ],\n            \"Resource\": \"*\"\n        }\n    ]\n}\n`\nwhich contains `Resource` blocks.\nIf you want the role to be able to use the `arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore` policy and also be able to assume the `arn:aws:iam::12313113231:role/s3-role` role (although it would be more normal to give the permissions directly to the role rather than use role chaining and if this involves cross account access to use the S3 bucket policy to allow that role instead) then you should instead do this:\n```\n`resource \"aws_iam_role\" \"ec2_role\" {\n  name               = \"ec2-role\"\n  assume_role_policy = data.aws_iam_policy_document.ec2_assume_role_policy.json\n}\n\nresource \"aws_iam_policy\" \"ec2_permission_policy\" {\n  name   = \"ec2-policy\"\n  policy = data.aws_iam_policy_document.ec2_permission_policy.json\n}\n\nresource \"aws_iam_role_policy_attachment\" \"attach\" {\n  role       = aws_iam_role.ec2_role.name\n  policy_arn = aws_iam_policy.ec2_permission_policy.arn\n}\n\ndata \"aws_iam_policy\" \"amazon_ssm_managed_instance_core\" {\n  arn = \"arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore\"\n}\n\ndata \"aws_iam_policy_document\" \"ec2_assume_role_policy\" {\n  statement {\n    effect = \"Allow\"\n    principals {\n      identifiers = [\"ec2.amazonaws.com\"]\n      type        = \"Service\"\n    }\n    actions = [\"sts:AssumeRole\"]\n  }\n}\n\ndata \"aws_iam_policy_document\" \"ec2_permission_policy\" {\n  source_json = data.aws_iam_policy.amazon_ssm_managed_instance_core.policy\n\n  statement {\n    effect    = \"Allow\"\n    actions   = [\"sts:AssumeRole\"]\n    resources = [\"arn:aws:iam::12313113231:role/s3-role\"]\n  }\n}\n`\n```",
      "question_score": 4,
      "answer_score": 7,
      "created_at": "2021-09-15T13:53:44",
      "url": "https://stackoverflow.com/questions/69192628/how-to-attach-multiple-iam-roles-to-instance-profile-on-aws"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-aws",
      "question_id": 67854086,
      "title": "Terraform RDS Instance monitoring_role_arn does not work",
      "problem": "I am trying to create an AWS RDS DB instance using Terraform. I'm trying to use PostgreSQL 12 as the DB. Everything seems okay except for the monitoring part where I am unable to specify the 'monitoring_role_arn' in the script.\nThe following is my Terraform script for creating the PostgreSQL DB instance:\nrds.tf\n```\n`# AWS PSQL RDS Instance \nresource \"aws_db_instance\" \"test-DB\" {\n\n  depends_on = [aws_security_group.test-PSQL-DB-SG, aws_iam_role.test-IAM-Role-RDS]\n\n  // General Configurations\n  name                 = \"testdb\"\n  identifier = \"am-poc-spoke1-db\"\n  engine               = \"postgres\"\n  engine_version       = \"12.5\"\n  instance_class       = \"db.t2.micro\" \n  parameter_group_name = \"default.postgres12\"\n  port = \"5432\"\n\n  // Authentication\n  username             = \"postgres\"\n  password             = \"postgres\"\n\n  // Storage Configurations\n  storage_type = \"gp2\"\n  allocated_storage    = 20\n  max_allocated_storage = 100\n\n  // Networking and Security \n  vpc_security_group_ids = [aws_security_group.test-PSQL-DB-SG.id]\n  availability_zone = \"ap-southeast-1a\"\n  publicly_accessible = false\n\n  // Backup Configuration\n  backup_retention_period = 7\n  backup_window = \"16:00-16:30\"\n  copy_tags_to_snapshot = true\n\n  // Monitoring and Performance Insight\n  performance_insights_enabled = true\n  performance_insights_retention_period = 7\n\n  monitoring_interval = \"60\"\n  monitoring_role_arn = aws_iam_role.test-IAM-Role-RDS.arn\n  enabled_cloudwatch_logs_exports = [\"postgresql\"]\n\n  // Other Configurations\n  auto_minor_version_upgrade = false\n  deletion_protection = false\n  skip_final_snapshot = true\n\n  tags = {\n    Name = \"test-DB\"\n  }\n}\n`\n```\nSince 'monitoring_role_arn' requires an AWS IAM Role with 'AmazonRDSEnhancedMonitoringRole' policy, I had created a script for that as well.\niam-role.tf\n```\n`# IAM Role for RDS Enhanced Monitoring\nresource \"aws_iam_role\" \"test-IAM-Role-RDS\" {\n\n  name = \"test-IAM-Role-RDS\"\n  assume_role_policy = jsonencode({\n    Version = \"2012-10-17\"\n    Statement = [\n      {\n        Action = \"sts:AssumeRole\"\n        Effect = \"Allow\"\n        Sid    = \"\"\n        Principal = {\n          Service = \"ec2.amazonaws.com\"\n        }\n      },\n    ]\n  })\n\n  tags = {\n    Name = \"test-IAM-Role-RDS\"\n  }\n}\n`\n```\nThen the policy to add to the IAM Role.\niam-role-policy.tf\n```\n`# IAM Role Policy for RDS Enhanced Monitoring\nresource \"aws_iam_role_policy\" \"test-Enhanced-Monitoring-Policy\" {\n\n  depends_on = [aws_iam_role.test-IAM-Role-RDS]\n\n  name = \"test-Enhanced-Monitoring-Policy\"\n  role = aws_iam_role.test-IAM-Role-RDS.id\n\n  policy = jsonencode({\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [{\n            \"Sid\": \"EnableCreationAndManagementOfRDSCloudwatchLogGroups\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"logs:CreateLogGroup\",\n                \"logs:PutRetentionPolicy\"\n            ],\n            \"Resource\": [\n                \"arn:aws:logs:*:*:log-group:RDS*\"\n            ]\n        },\n        {\n            \"Sid\": \"EnableCreationAndManagementOfRDSCloudwatchLogStreams\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"logs:CreateLogStream\",\n                \"logs:PutLogEvents\",\n                \"logs:DescribeLogStreams\",\n                \"logs:GetLogEvents\"\n            ],\n            \"Resource\": [\n                \"arn:aws:logs:*:*:log-group:RDS*:log-stream:*\"\n            ]\n        }\n    ]\n  })\n}\n`\n```\nWhen running `'terraform plan'` no errors are show. But once I run `'terraform apply'`, I am getting the following error.\n\nError: Error creating DB Instance: InvalidParameterValue: IAM role ARN value is invalid or does not include the required permissions for:\nENHANCED_MONITORING \u2502       status code: 400, request id:\n59e6127d-f39a-453d-885a-868e38415fc1, {\n\nDoes anyone now how to fix this?",
      "solution": "Instead of using an Inline Policy, used a managed policy which is AmazonRDSEnhancedMonitoringRole. That is, we have directly added the AWS managed policy to our IAM Role.\nAlso, I had changed the Service from `ec2.amazonaws.com` to `monitoring.rds.amazonaws.com` in the IAM role. The error is actually triggered because we don't have this change. Think it would work with the inline policy also, but we can avoid additional lines of code with just the AWS managed policy instead of creating a new inline policy.\nFull Changes:\niam.tf\n```\n`# IAM Role for RDS Enhanced Monitoring\nresource \"aws_iam_role\" \"test-IAM-Role-RDS\" {\n\n  name = \"test-IAM-Role-RDS\"\n  assume_role_policy = jsonencode({\n         Version = \"2012-10-17\"\n          Statement = [\n            {\n              Action = \"sts:AssumeRole\"\n              Effect = \"Allow\"\n              Sid    = \"\"\n              Principal = {\n                Service = \"monitoring.rds.amazonaws.com\"\n             }\n            },\n          ]\n        })\n\n  managed_policy_arns = [\"arn:aws:iam::aws:policy/service-role/AmazonRDSEnhancedMonitoringRole\"]\n\n  tags = {\n    Name = \"test-IAM-Role-RDS\"\n  }\n}\n`\n```",
      "question_score": 4,
      "answer_score": 8,
      "created_at": "2021-06-05T23:32:49",
      "url": "https://stackoverflow.com/questions/67854086/terraform-rds-instance-monitoring-role-arn-does-not-work"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-aws",
      "question_id": 69779676,
      "title": "Terraform destroys the instance inside RDS cluster when upgrading",
      "problem": "I have created a RDS cluster with 2 instances using terraform. When I am upgrading the RDS from front-end, it modifies the cluster. But when I do the same using terraform, it destroys the instance.\nWe tried create_before_destroy, and it gives error.\nWe tried with ignore_changes=engine but that didn't make any changes.\nIs there any way to prevent it?\n```\n`resource \"aws_rds_cluster\" \"rds_mysql\" {\n  cluster_identifier              = var.cluster_identifier\n  engine                          = var.engine\n  engine_version                  = var.engine_version\n  engine_mode                     = var.engine_mode\n  availability_zones              = var.availability_zones\n  database_name                   = var.database_name\n  port                            = var.db_port\n  master_username                 = var.master_username\n  master_password                 = var.master_password\n  backup_retention_period         = var.backup_retention_period\n  preferred_backup_window         = var.engine_mode == \"serverless\" ? null : var.preferred_backup_window\n  db_subnet_group_name            = var.create_db_subnet_group == \"true\" ? aws_db_subnet_group.rds_subnet_group[0].id : var.db_subnet_group_name\n  vpc_security_group_ids          = var.vpc_security_group_ids\n  db_cluster_parameter_group_name = var.create_cluster_parameter_group == \"true\" ? aws_rds_cluster_parameter_group.rds_cluster_parameter_group[0].id : var.cluster_parameter_group\n  skip_final_snapshot             = var.skip_final_snapshot\n  deletion_protection             = var.deletion_protection\n  allow_major_version_upgrade     = var.allow_major_version_upgrade\n  lifecycle {\n    create_before_destroy = false\n    ignore_changes        = [availability_zones]\n  }\n}\n\nresource \"aws_rds_cluster_instance\" \"cluster_instances\" {\n  count                      = var.engine_mode == \"serverless\" ? 0 : var.cluster_instance_count\n  identifier                 = \"${var.cluster_identifier}-${count.index}\"\n  cluster_identifier         = aws_rds_cluster.rds_mysql.id\n  instance_class             = var.instance_class\n  engine                     = var.engine\n  engine_version             = aws_rds_cluster.rds_mysql.engine_version\n  db_subnet_group_name       = var.create_db_subnet_group == \"true\" ? aws_db_subnet_group.rds_subnet_group[0].id : var.db_subnet_group_name\n  db_parameter_group_name    = var.create_db_parameter_group == \"true\" ? aws_db_parameter_group.rds_instance_parameter_group[0].id : var.db_parameter_group\n  apply_immediately          = var.apply_immediately\n  auto_minor_version_upgrade = var.auto_minor_version_upgrade\n  lifecycle {\n    create_before_destroy = false\n    ignore_changes        = [engine_version]\n  }\n}\n\n`\n```\nError:\n```\n`resource \\\"aws_rds_cluster_instance\\\" \\\"cluster_instances\\\" {\\n\\n\\n\\nError: error creating RDS Cluster (aurora-cluster-mysql) Instance: DBInstanceAlreadyExists: DB instance already exists\\n\\tstatus code: 400, request id: c6a063cc-4ffd-4710-aff2-eb0667b0774f\\n\\n on \n`\n```\nPlan output:\n```\n`Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols:\n  ~ update in-place\n+/- create replacement and then destroy\n\nTerraform will perform the following actions:\n\n  # module.rds_aurora_create[0].aws_rds_cluster.rds_mysql will be updated in-place\n  ~ resource \"aws_rds_cluster\" \"rds_mysql\" {\n      ~ allow_major_version_upgrade         = false -> true\n      ~ engine_version                      = \"5.7.mysql_aurora.2.07.1\" -> \"5.7.mysql_aurora.2.08.1\"\n        id                                  = \"aurora-cluster-mysql\"\n        tags                                = {}\n        # (33 unchanged attributes hidden)\n    }\n\n  # module.rds_aurora_create[0].aws_rds_cluster_instance.cluster_instances[0] must be replaced\n+/- resource \"aws_rds_cluster_instance\" \"cluster_instances\" {\n      ~ arn                             = \"arn:aws:rds:us-east-1:account:db:aurora-cluster-mysql-0\" -> (known after apply)\n      ~ availability_zone               = \"us-east-1a\" -> (known after apply)\n      ~ ca_cert_identifier              = \"rds-ca-\" -> (known after apply)\n      ~ dbi_resource_id                 = \"db-32432432SDF\" -> (known after apply)\n      ~ endpoint                        = \"aurora-cluster-mysql-0.jkjk.us-east-1.rds.amazonaws.com\" -> (known after apply)\n      ~ engine_version                  = \"5.7.mysql_aurora.2.07.1\" -> \"5.7.mysql_aurora.2.08.1\" # forces replacement\n      ~ id                              = \"aurora-cluster-mysql-0\" -> (known after apply)\n      + identifier_prefix               = (known after apply)\n      + kms_key_id                      = (known after apply)\n      + monitoring_role_arn             = (known after apply)\n      ~ performance_insights_enabled    = false -> (known after apply)\n      + performance_insights_kms_key_id = (known after apply)\n      ~ port                            = 3306 -> (known after apply)\n      ~ preferred_backup_window         = \"07:00-09:00\" -> (known after apply)\n      ~ preferred_maintenance_window    = \"thu:06:12-thu:06:42\" -> (known after apply)\n      ~ storage_encrypted               = false -> (known after apply)\n      - tags                            = {} -> null\n      ~ tags_all                        = {} -> (known after apply)\n      ~ writer                          = true -> (known after apply)\n        # (12 unchanged attributes hidden)\n    }\n\nPlan: 1 to add, 1 to change, 1 to destroy.\n\n`\n```",
      "solution": "I see `apply_immediately` argument not there in  `aws_rds_cluster` resource , can you add that and try.",
      "question_score": 4,
      "answer_score": 3,
      "created_at": "2021-10-30T14:36:16",
      "url": "https://stackoverflow.com/questions/69779676/terraform-destroys-the-instance-inside-rds-cluster-when-upgrading"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-aws",
      "question_id": 66848938,
      "title": "Can aws cdk provide remote state?",
      "problem": "Terraform has a remote stack via well documented plugins, i.e. terraform.backend.s3\nhttps://www.terraform.io/docs/language/settings/backends/s3.html\nCan aws cdk provide remote state for the stacks?\nI can't find in documentation.\nhttps://docs.aws.amazon.com/cdk/latest/guide/awscdk.pdfstack\nI ask about aws cdk because I  found pure documentation about aws cdktf.\nFound that cloud cloudfront generates a lot of json file as well as uses it. Does the contain state?",
      "solution": "The CDK uses CloudFormation under the hood, which manages the remote state of the infrastructure in a similar way like a Terraform state-file.\nYou get the benefit of AWS taking care of state management for you (for free) without the risks of doing it yourself and messing up your state file.\nThe drawback is that if there is drift between the state CloudFormation thinks resources are in and their actual state, things get tricky.",
      "question_score": 4,
      "answer_score": 7,
      "created_at": "2021-03-29T06:54:04",
      "url": "https://stackoverflow.com/questions/66848938/can-aws-cdk-provide-remote-state"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-aws",
      "question_id": 75728504,
      "title": "List AWS Billing Filter options or names for use in Terraform",
      "problem": "I'm looking for a hand listing some of the services available in AWS for use in filtering budgets, which in turn are created by Terraform.\nThe Terraform reference page for the resource include a cost filter, shown in the snippet below.\n```\n`resource \"aws_budgets_budget\" \"ec2\" {\n  name              = \"budget-terraform\"\n  #---\n  # some details exluded for brevity\n  #---\n\n  cost_filter {\n    name = \"Service\"\n    values = [\n      \"Amazon Elastic Compute Cloud - Compute\",\n      \"Elastic Container Services\"\n    ]\n  }\n`\n```\nThe example provides `Amazon Elastic Compute Cloud - Compute` as the filter: which in the portal/console shows up under a different (but similar) name.\nI'm trying to automate budgets across a number of different resource types (including tags) but can't seem to find where the list of potential options in the `values` field above is documented - on TF or AWS doco: Terraform references cost filter documentation under this link, which seems to me to be a dead end. My current approach of trying to use the portal and a bit of educated guess work is clunky: The images below show what I've punched into Terraform code, as well as how they appear in the portal, so there's even a bit of guesswork (or typo's your call) here, which seems a bad approach..\n\nSo - to the crux of the question:\nDoes anyone know some CLI call or reference material somewhere that one would use for getting a list of these names as they'd be used in filters?",
      "solution": "So - for those interested in the solution to this one (as AWS/Terraform doesn't seem to have it documented well if at all), my solution was to click-ops it all together: and use the CLI to pull the data from the account, and pipe to jq to get the names of services I wanted:\n`aws budgets describe-budget --account-id 12345678910 --budget-name example-budget --no-paginate | jq .Budget.CostFilters.Service`\nFor those that have run into the same issue (and don't wanna clickops): here's the output:\n```\n`[\n  \"AWS X-Ray\",\n  \"Amazon WorkSpaces\",\n  \"Amazon WorkDocs\",\n  \"AWS WAF\",\n  \"Amazon Virtual Private Cloud\",\n  \"AWS Transfer Family\",\n  \"Amazon Timestream\",\n  \"Tax\",\n  \"AWS Systems Manager\",\n  \"AWS Storage Gateway\",\n  \"AWS Step Functions\",\n  \"Amazon Simple Queue Service\",\n  \"Amazon Simple Notification Service\",\n  \"Amazon SimpleDB\",\n  \"Amazon Simple Email Service\",\n  \"AWS Service Catalog\",\n  \"AWS Network Firewall\",\n  \"Amazon OpenSearch Service\",\n  \"Amazon QuickSight\",\n  \"Amazon Redshift\",\n  \"Amazon Registrar\",\n  \"Amazon Rekognition\",\n  \"Amazon Relational Database Service\",\n  \"Amazon Route 53\",\n  \"Amazon Simple Storage Service\",\n  \"AWS Secrets Manager\",\n  \"AWS Amplify\",\n  \"Amazon API Gateway\",\n  \"AWS Application Migration Service\",\n  \"Amazon AppStream\",\n  \"Amazon Athena\",\n  \"AWS Backup\",\n  \"AWS Certificate Manager\",\n  \"AWS Cloud Map\",\n  \"Amazon CloudFront\",\n  \"AWS CloudShell\",\n  \"AWS CloudTrail\",\n  \"AmazonCloudWatch\",\n  \"CloudWatch Events\",\n  \"AWS CodeArtifact\",\n  \"CodeBuild\",\n  \"AWS CodeCommit\",\n  \"Amazon Cognito\",\n  \"AWS Config\",\n  \"AWS Cost Explorer\",\n  \"Databricks Lakehouse Platform\",\n  \"Amazon Detective\",\n  \"AWS Directory Service\",\n  \"AWS Database Migration Service\",\n  \"Amazon DynamoDB\",\n  \"Amazon EC2 Container Registry (ECR)\",\n  \"Amazon Elastic Load Balancing\",\n  \"Amazon Elastic Compute Cloud - Compute\",\n  \"Amazon Elastic Block Store\",\n  \"Amazon Elastic Container Service\",\n  \"Amazon Elastic Container Service for Kubernetes\",\n  \"AWS Elastic Disaster Recovery\",\n  \"Amazon Elastic File System\",\n  \"Amazon ElastiCache\",\n  \"Amazon FSx\",\n  \"Amazon Glacier\",\n  \"AWS Global Accelerator\",\n  \"AWS Glue\",\n  \"Amazon GuardDuty\",\n  \"Amazon Inspector\",\n  \"AWS IoT\",\n  \"AWS Key Management Service\",\n  \"Amazon Kinesis\",\n  \"Amazon Kinesis Analytics\",\n  \"Amazon Kinesis Firehose\",\n  \"AWS Lambda\",\n  \"LAMP Stack Ubuntu 20\",\n  \"Amazon Lightsail\",\n  \"Amazon Managed Grafana\",\n  \"Amazon Managed Service for Prometheus\",\n  \"Amazon Managed Streaming for Apache Kafka\",\n  \"Amazon MemoryDB\",\n  \"Amazon MQ\"\n]\n`\n```",
      "question_score": 4,
      "answer_score": 6,
      "created_at": "2023-03-14T03:06:40",
      "url": "https://stackoverflow.com/questions/75728504/list-aws-billing-filter-options-or-names-for-use-in-terraform"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-aws",
      "question_id": 69740875,
      "title": "Why do I get &quot;Error: handler and runtime must be set when PackageType is Zip&quot; when deploying a Lambda function using Terraform?",
      "problem": "I have defined a Lambda function with Terraform like this:\n```\n`resource \"aws_lambda_function\" \"this\" {\n  filename = \"${path.module}/src/existing-files-lambda.zip\"\n  function_name = \"ingest-existing-files-lambda\"\n  role = aws_iam_role.lambda.arn\n\n  runtime = \"python3.9\"\n  timeout = 900\n  environment {\n   variables = {\n      source_bucket_arn = var.source_bucket_arn  \n      destination_bucket_arn = var.destination_bucket_arn  \n    }\n  }\n}\n\nresource \"aws_iam_role\" \"lambda\" {\n  name = \"${var.prefix}-lambda-ingest\"\n  path = \"/service-role/\"\n\n  assume_role_policy = jsonencode({\n    Version = \"2012-10-17\"\n    Statement = [{\n      Effect    = \"Allow\"\n      Principal = { Service = \"lambda.amazonaws.com\" }\n      Action    = \"sts:AssumeRole\"\n    }]\n  })\n}\n`\n```\nMy python file is just this:\n```\n`import os\n\ndef lambda_handler(event, context):\n    print('Hello world from Terraform')\n    return {\n        'statusCode': 200,\n    }\n`\n```\nHowever, I am currently getting an error that:\n```\n`\u2502 Error: handler and runtime must be set when PackageType is Zip\n\u2502 \n\u2502   with module.ingest_lambda.aws_lambda_function.this,\n\u2502   on ingest_lambda/main.tf line 8, in resource \"aws_lambda_function\" \"this\":\n\u2502    8: resource \"aws_lambda_function\" \"this\" {\n`\n```\nWhat do I put as `handler` here?\nI already have `runtime` specified.",
      "solution": "You have defined the Lambda function runtime but you haven't mentioned where the entry point to the function is.\n\nThat is what the `handler` argument specifies - it is the method in your function code that processes events.\nIt should have a format similar to:\n```\n`def handler_name(event, context): \n    ...\n    return some_value\n`\n```\n\nThe value of the handler argument is comprised of the below, separated by a dot:\n\nThe name of the file in which the Lambda handler function is located\nThe name of the Python handler function.\n\ne.g.\n`ingest-existing-files-lambda.lambda_handler` calls the`lambda_handler` function defined in `ingest-existing-files-lambda.py`.\nIf your Lambda handler method is called `lambda_handler` & is inside `ingest-existing-files-lambda.py`, this should work:\n```\n`resource \"aws_lambda_function\" \"this\" {\n  filename = \"${path.module}/src/existing-files-lambda.zip\"\n  function_name = \"ingest-existing-files-lambda\"\n  handler = \"ingest-existing-files-lambda.lambda_handler\"\n  role = aws_iam_role.lambda.arn\n\n  runtime = \"python3.9\"\n  timeout = 900\n  environment {\n   variables = {\n      source_bucket_arn = var.source_bucket_arn  \n      destination_bucket_arn = var.destination_bucket_arn  \n    }\n  }\n}\n`\n```",
      "question_score": 4,
      "answer_score": 5,
      "created_at": "2021-10-27T16:50:07",
      "url": "https://stackoverflow.com/questions/69740875/why-do-i-get-error-handler-and-runtime-must-be-set-when-packagetype-is-zip-wh"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-aws",
      "question_id": 69591195,
      "title": "Why is TF build failing with &quot;Error refreshing state: HTTP remote state endpoint requires auth&quot;?",
      "problem": "My pipeline builds, plans and applies just fine for my dev branch.  When I push to my master branch, I get \"Error refreshing state: HTTP remote state endpoint requires auth\".  See pipeline logs: (and since someone will ask, the token \"$onbaord_cloud_account_into_monitoring\" has complete read/write access to the scoped project API.)...\n```\n`Running with gitlab-runner 13.4.1 (REDACTED)\n  on runner-gitlab-runner-REDACTED-REDACTED REDACTED\nResolving secrets\n00:00\nPreparing the \"kubernetes\" executor\n00:00\nUsing Kubernetes namespace: gitlab-managed-apps\nUsing Kubernetes executor with image my-gitlab.io:5005/team-cloud-platform-team/terraform-modules/root-module-deployment/python-terraform14 ...\nPreparing environment\n00:03\nWaiting for pod gitlab-managed-apps/runner-REDACTED-project-REDACTED-concurrent-REDACTED to be running, status is Pending\nRunning on runner-REDACTED-project-REDACTED-concurrent-REDACTED via runner-gitlab-runner-REDACTED-REDACTED...\nGetting source from Git repository\n00:02\nFetching changes with git depth set to 50...\nInitialized empty Git repository in /builds/team-cloud-platform-team/teamcloudv2/workers/onboard-cloud-account-into-monitoring/.git/\nCreated fresh repository.\nChecking out REDACTED as master...\nSkipping Git submodules setup\nRestoring cache\n00:00\nChecking cache for REDACTEDREDACTEDREDACTEDREDACTED...\nNo URL provided, cache will not be downloaded from shared cache server. Instead a local version of cache will be extracted. \nSuccessfully extracted cache\nExecuting \"step_script\" stage of the job script\n00:05\n$ git config --global credential.helper cache\n$ git fetch\nFrom https://my-gitlab.io/team-cloud-platform-team/teamcloudv2/workers/onboard-cloud-account-into-monitoring\n * [new branch]      dev        -> origin/dev\n$ if [ \"$CI_COMMIT_REF_NAME\" == \"master\" ]; then TF_ACCOUNT=\"REDACTED\";fi\n$ if [ \"$CI_COMMIT_REF_NAME\" == \"dev\" ]; then TF_ACCOUNT=\"REDACTED\";fi\n$ if [ \"$CI_COMMIT_REF_NAME\" == \"master\" ]; then ORG_ACCOUNT=\"REDACTED\";fi\n$ if [ \"$CI_COMMIT_REF_NAME\" == \"dev\" ]; then ORG_ACCOUNT=\"REDACTED\";fi\n$ echo ${TF_ACCOUNT}\nREDACTED\n$ echo ${TF_ADDRESS}\nhttps://my-gitlab.io/api/v4/projects/10849/terraform/state/onboard-cloud-account-into-monitoring-master\n$ echo TF_HTTP_ADDRESS=${TF_ADDRESS}\nTF_HTTP_ADDRESS=https://my-gitlab.io/api/v4/projects/10849/terraform/state/onboard-cloud-account-into-monitoring-master\n$ echo TF_HTTP_LOCK_ADDRESS=${TF_LOCK}\nTF_HTTP_LOCK_ADDRESS=https://my-gitlab.io/api/v4/projects/10849/terraform/state/onboard-cloud-account-into-monitoring-master/lock\n$ echo TF_HTTP_UNLOCK_ADDRESS=${TF_UNLOCK}\nTF_HTTP_UNLOCK_ADDRESS=https://my-gitlab.io/api/v4/projects/10849/terraform/state/onboard-cloud-account-into-monitoring-master/lock\n$ echo TF_ADDRESS=${TF_ADDRESS}\nTF_ADDRESS=https://my-gitlab.io/api/v4/projects/10849/terraform/state/onboard-cloud-account-into-monitoring-master\n$ export TF_HTTP_ADDRESS=${TF_ADDRESS}\n$ export TF_HTTP_LOCK_ADDRESS=${TF_LOCK}\n$ export TF_HTTP_UNLOCK_ADDRESS=${TF_UNLOCK}\n$ export TF_HTTP_LOCK_METHOD=\"POST\"\n$ export TF_HTTP_UNLOCK_METHOD=\"DELETE\"\n$ export TF_HTTP_RETRY_WAIT_MIN='5'\n$ export TF_HTTP_USERNAME='JOHN.DOE'\n$ export TF_HTTP_PASSWORD=${onbaord_cloud_account_into_monitoring}\n$ export TF_VAR_ACCOUNT=${TF_ACCOUNT}\n$ export TF_VAR_ORG_ACCOUNT=${ORG_ACCOUNT}\n$ export AWS_DEFAULT_REGION='us-east-1'\n$ terraform init\nInitializing modules...\nDownloading git::https://my-gitlab.io/team-cloud-platform-team/teamcloudv2/terraform/lambda-modules.git?ref=dev for lambda...\n- lambda in .terraform/modules/lambda\nDownloading git::https://my-gitlab.io/team-cloud-platform-team/teamcloudv2/terraform/iam-modules/lambda-iam.git?ref=dev for lambda_iam...\n- lambda_iam in .terraform/modules/lambda_iam\nInitializing the backend...\nSuccessfully configured the backend \"http\"! Terraform will automatically\nuse this backend unless the backend configuration changes.\nError refreshing state: HTTP remote state endpoint requires auth\nCleaning up file based variables\n00:00\nERROR: Job failed: command terminated with exit code 1\n`\n```\nA peek at my gitlab-ci.yml:\n```\n`image:\n  name: my-gitlab.io:5005/team-cloud-platform-team/terraform-modules/root-module-deployment/python-terraform14\n  entrypoint:\n    - '/usr/bin/env'\n    - 'PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'\nstages:\n  - build\n  - plan\n  - apply\n\nvariables:\n  PLAN: plan.tfplan\n  JSON_PLAN_FILE: tfplan.json\n  WORKSPACE: \"dev\"\n  TF_IN_AUTOMATION: \"true\"\n  ZIP_FILE: \"lambda_package.zip\"\n  STATE_FILE: ${CI_PROJECT_NAME}-${CI_COMMIT_BRANCH}\n  TF_ACCOUNT: \"\"\n  ORG_ACCOUNT: \"\"\n  TF_ADDRESS: ${CI_API_V4_URL}/projects/${CI_PROJECT_ID}/terraform/state/${CI_PROJECT_NAME}-${CI_COMMIT_BRANCH}\n  TF_LOCK: ${CI_API_V4_URL}/projects/${CI_PROJECT_ID}/terraform/state/${CI_PROJECT_NAME}-${CI_COMMIT_BRANCH}/lock\n  TF_UNLOCK: ${CI_API_V4_URL}/projects/${CI_PROJECT_ID}/terraform/state/${CI_PROJECT_NAME}-${CI_COMMIT_BRANCH}/lock\n\ncache:\n  key: \"$CI_COMMIT_SHA\"\n  paths:\n    - .terraform\n\nbefore_script:\n  - git config --global credential.helper cache \n  - git fetch \n  - if [ \"$CI_COMMIT_REF_NAME\" == \"master\" ]; then TF_ACCOUNT=\"REDACTED\";fi\n  - if [ \"$CI_COMMIT_REF_NAME\" == \"dev\" ]; then TF_ACCOUNT=\"REDACTED\";fi\n  - if [ \"$CI_COMMIT_REF_NAME\" == \"master\" ]; then ORG_ACCOUNT=\"REDACTED\";fi\n  - if [ \"$CI_COMMIT_REF_NAME\" == \"dev\" ]; then ORG_ACCOUNT=\"REDACTED\";fi\n  - echo ${TF_ACCOUNT}\n  - echo ${TF_ADDRESS}\n  - echo TF_HTTP_ADDRESS=${TF_ADDRESS}\n  - echo TF_HTTP_LOCK_ADDRESS=${TF_LOCK}\n  - echo TF_HTTP_UNLOCK_ADDRESS=${TF_UNLOCK}\n  - echo TF_ADDRESS=${TF_ADDRESS}\n  - export TF_HTTP_ADDRESS=${TF_ADDRESS}\n  - export TF_HTTP_LOCK_ADDRESS=${TF_LOCK}\n  - export TF_HTTP_UNLOCK_ADDRESS=${TF_UNLOCK}\n  - export TF_HTTP_LOCK_METHOD=\"POST\"\n  - export TF_HTTP_UNLOCK_METHOD=\"DELETE\"\n  - export TF_HTTP_RETRY_WAIT_MIN='5'\n  - export TF_HTTP_USERNAME='MATTHEW.FETHEROLF'\n  - export TF_HTTP_PASSWORD=${onbaord_cloud_account_into_monitoring}\n  - export TF_VAR_ACCOUNT=${TF_ACCOUNT}\n  - export TF_VAR_ORG_ACCOUNT=${ORG_ACCOUNT}\n  - export AWS_DEFAULT_REGION='us-east-1'\n  - terraform init \n\n  \n\n# -----BUILD-----\n  # 1 build job per lambda function\n\nbuildLambda:\n  stage: build\n  tags:\n    - cluster \n  script:\n    - echo \"Beginning Build\"\n    - cd lambda_code/\n    - echo \"Switched into Lambda dir\"\n    - pip3 install -r requirements.txt --target .\n    - echo \"Requirements installed\"\n    - echo $ZIP_FILE\n    - zip -r $ZIP_FILE *\n    - echo \"Zip file packaged up\"\n  artifacts:\n    paths:\n      - lambda_code/\n  only:\n    - dev\n    - master\n    - merge_requests\n\n# -----PLAN-----\n\nplanMerge:\n  stage: plan\n  tags:\n  - cluster \n  script:\n    - terraform plan\n  dependencies:\n    - buildLambda\n  only:\n    - merge_requests\n\nplanCommit:\n  stage: plan\n  tags:\n    - cluster \n  script:\n    - terraform plan\n  dependencies:\n    - buildLambda\n\n# -----APPLY-----\n  # dev branch will auto deploy\napplyDev:\n  stage: apply\n  tags:\n  - cluster \n  script:\n    - echo \"Running Terraform Apply\"\n    - terraform apply -auto-approve\n  dependencies:\n    - buildLambda\n  only:\n    - dev \n  #when: manual\n\n    # prod branch will require manual deployment approval\napplyProd:\n  stage: apply\n  tags:\n  - cluster \n  script:\n    - echo \"Running Terraform Apply\"\n    - terraform apply -auto-approve\n  when: manual\n  dependencies:\n    - buildLambda\n  only:\n    - master\n`\n```\nbackend.tf:\n```\n`terraform {\n    backend \"http\" {\n    }\n}\n`\n```\nmain.tf:\n```\n`locals {\n  common_tags = {\n    SVC_ACCOUNT_ID = \"REDACTED\",\n    CLOUD_PLATFORM_PROD_ID = \"REDACTED\"\n  }\n}\n\nmodule \"lambda\" {\n    source = \"git::https://my-gitlab.io/team-cloud-platform-team/teamcloudv2/terraform/lambda-modules.git?ref=dev\" \n    lambda_name = var.name\n    lambda_role = \"arn:aws:iam::${var.ACCOUNT}:role/${var.lambda_role}\"\n    lambda_handler = var.handler\n    lambda_runtime = var.runtime\n    default_lambda_timeout = var.timeout\n    env = local.common_tags\n    ACCOUNT = var.ACCOUNT\n}\n\nmodule \"lambda_iam\" {\n    source = \"git::https://my-gitlab.io/team-cloud-platform-team/teamcloudv2/terraform/iam-modules/lambda-iam.git?ref=dev\" \n    lambda_policy = var.lambda_policy\n    ACCOUNT = var.ACCOUNT\n    lambda_role = var.lambda_role\n}\n`\n```\ninputs.tf:\n```\n`variable \"handler\" {\n  type = string\n  default = \"handler.lambda_handler\"\n}\n\nvariable \"runtime\" {\n  type = string\n  default = \"python3.8\"\n}\n\nvariable \"name\" {\n  type = string\n  default = \"onboard-cloud-account-into-monitoring\"\n}\n\nvariable \"timeout\"{\n    type = string\n    default = \"120\"\n}\n\nvariable \"lambda_role\" {\n  type = string\n  default = \"cloud-platform-onboard-to-monitoring\"\n}\n\nvariable \"ACCOUNT\" {\n  type = string\n}\n\nvariable \"ORG_ACCOUNT\" {\n  type = string\n}\n\nvariable \"lambda_policy\" {\n  default = \"{\\\"Version\\\": \\\"2012-10-17\\\",\\\"Statement\\\": [{\\\"Sid\\\": \\\"VisualEditor0\\\",\\\"Effect\\\": \\\"Allow\\\",\\\"Action\\\": [\\\"logs:CreateLogStream\\\",\\\"logs:CreateLogGroup\\\"],\\\"Resource\\\": \\\"*\\\"},{\\\"Sid\\\": \\\"VisualEditor1\\\",\\\"Effect\\\": \\\"Allow\\\",\\\"Action\\\": \\\"logs:PutLogEvents\\\",\\\"Resource\\\": \\\"*\\\"},{\\\"Sid\\\": \\\"VisualEditor2\\\",\\\"Effect\\\": \\\"Allow\\\",\\\"Action\\\": \\\"sts:AssumeRole\\\",\\\"Resource\\\": \\\"*\\\"},{\\\"Sid\\\": \\\"VisualEditor3\\\",\\\"Effect\\\": \\\"Allow\\\",\\\"Action\\\": \\\"secretsmanager:GetSecretValue\\\",\\\"Resource\\\": \\\"*\\\"}]}\"\n}\n`\n```\nprovider.tf:\n```\n`\nprovider \"aws\" {\n    region          = \"us-east-1\" \n    assume_role {\n        role_arn    =\"arn:aws:iam::${var.ACCOUNT}:role/team-platform-onboard-to-monitoring\"\n    }\n\n    default_tags {\n            tags = {\n                owner = \"REDACTED@REDACTED.com\"\n                altowner = \"REDACTED@REDACTED.com\"\n                blc = \"REDACTED\"\n                costcenter = \"REDACTED\"\n                itemid = \"PlatformAutomation\"\n            }\n        }\n}\n`\n```\nPerhaps I also need to share the terraform modules being invoked?",
      "solution": "We found the problem:\nThe pipeline referenced a protected environment variable.  The master branch was not a protected branch.  The solution was to unprotect the environment variable or protect the branch.  Instantly solved the problem.",
      "question_score": 4,
      "answer_score": 5,
      "created_at": "2021-10-16T00:25:52",
      "url": "https://stackoverflow.com/questions/69591195/why-is-tf-build-failing-with-error-refreshing-state-http-remote-state-endpoint"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-aws",
      "question_id": 69100843,
      "title": "Access the index of a map in for_each",
      "problem": "I have a map that looks like this\n```\n`variable \"mysubnets\" {\n  type = map(string)\n  default = {\n    \"subnet1\" = \"10.1.0.0/24\"\n    \"subnet2\" = \"10.1.1.0/24\"\n  }\n}\n`\n```\nIn my module I'm trying to place subnets in different availability zones in the same vpc\n```\n`data \"aws_availability_zones\" \"azs\" {\n  state = \"available\"\n}\n\nresource \"aws_subnet\" \"test-subnets\" {\n  for_each = var.mysubnets\n  cidr_block = \"${each.value}\"\n  vpc_id = aws_vpc.myvpc.id\n  availability_zone = data.aws_availability_zones.azs.names[index(\"${each.value}\")]\n\n  tags = {\n    Name = \"${each.key}\"\n  } \n}\n`\n```\nI can get the key and value from the map no problem, but when trying to pick an availability zone I can't find how to change the value. Is there a way to get the index of a map, or create a counter for a number that increments?",
      "solution": "Your data source is called `azs`, not `available`. So it should be:\n```\n`availability_zone = data.aws_availability_zones.azs.names[index(\"${each.value}\")]\n`\n```\nUpdate:\nTo use `index` with your `var.mysubnets` you can do as follows:\n```\n`resource \"aws_subnet\" \"test-subnets\" {\n\n  for_each = {for idx, subnet in keys(var.mysubnets): \n                  idx => { \n                      name = subnet\n                      cidr = var.mysubnets[subnet]\n                  }\n             }\n             \n  cidr_block = each.value.cidr\n  vpc_id = aws_vpc.myvpc.id\n  \n  availability_zone = element(data.aws_availability_zones.azs.names, each.key)\n\n  tags = {\n    Name = each.value.name\n  } \n}\n`\n```",
      "question_score": 4,
      "answer_score": 6,
      "created_at": "2021-09-08T11:54:24",
      "url": "https://stackoverflow.com/questions/69100843/access-the-index-of-a-map-in-for-each"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-aws",
      "question_id": 72315660,
      "title": "import an existing bucket in terraform",
      "problem": "I wanted to create an event notification on an existing s3_bucket (which is not setup by me in this current terraform code).\nI came across this answer:\nterraform aws_s3_bucket_notification existing bucket\nso I tried this. Here, local.bucket_name is the name of the existing bucket.\nnotification.tf\n```\n`resource \"aws_s3_bucket\" \"trigger_pipeline\" {\n  bucket = local.bucket_name\n}\n\nterraform import aws_s3_bucket.trigger_pipeline local.bucket_name\n`\n```\nHowever, I am not sure how to use this import statement. Do I use it after the resource block? Do I use it in the beginning of the same file?\nIf I use it as it is, under the resource block, I get this error:\n```\n`Invalid block definition: Either a quoted string block label or an opening brace (\"{\") is expected here.HCL\n`\n```\nat the dot here: `aws_s3_bucket.trigger_pipeline`\nEdit:\nSo first I defined s3 resource as shown in the question above. Then I run `terraform init`. Next, I run `terraform import aws_s3_bucket.trigger_pipeline \"myoriginalbucketname\"` on the CLI. However, I still get the error that:\n```\n`Before importing this resource, please create its configuration in the root module. For example:\n\nresource \"aws_s3_bucket\" \"trigger_pipeline\" {\n  # (resource arguments)\n}\n`\n```\nI guess I am getting the sequence of events wrong",
      "solution": "`local.bucket_name` executes in your bash, not in TF. You have to actually provide the full name:\n```\n`terraform import aws_s3_bucket.trigger_pipeline \"my-bucket-name\"\n`\n```",
      "question_score": 4,
      "answer_score": 2,
      "created_at": "2022-05-20T10:17:02",
      "url": "https://stackoverflow.com/questions/72315660/import-an-existing-bucket-in-terraform"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-aws",
      "question_id": 70049176,
      "title": "Aws_acm_certificate.app_cert.domain_validation_options is a set of object, known only after apply",
      "problem": "I've been working through a Terraform (+ CI/CD) workshop which was taught in an earlier version of Terraform, but I decided to it in 1.0.11 with AWS provider 3.65.0.... just to see what sort of the difference would be. I've hit a blocker when dealing with ACM to get the certificate, and I need some advice on how to proceed.\nThe error I'm getting is during the plan stage:\n```\n`[ckerr@ck-vm-rhel8-localdomain recipe-app-api-devops]$ docker-compose -f deploy/docker-compose.yml run --rm terraform plan\nCreating deploy_terraform_run ... done\n\u2577\n\u2502 Error: Invalid for_each argument\n\u2502 \n\u2502   on dns.tf line 44, in resource \"aws_route53_record\" \"app_cert_validation_records\":\n\u2502   44:   for_each = {\n\u2502   45:     for dvo in aws_acm_certificate.app_cert.domain_validation_options : dvo.domain_name => {\n\u2502   46:       name   = dvo.resource_record_name\n\u2502   47:       type   = dvo.resource_record_type\n\u2502   48:       record = dvo.resource_record_value\n\u2502   49:     }\n\u2502   50:   }\n\u2502     \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\u2502     \u2502 aws_acm_certificate.app_cert.domain_validation_options is a set of object, known only after apply\n\u2502 \n\u2502 The \"for_each\" value depends on resource attributes that cannot be determined until apply, so Terraform cannot predict how\n\u2502 many instances will be created. To work around this, use the -target argument to first apply only the resources that the\n\u2502 for_each depends on.\n\u2575\nReleasing state lock. This may take a few moments...\nERROR: 1\n`\n```\nI have a file called dns.tf that looks after the DNS and SSL certificates, with the intention that Amazon's Certificate Manager (ACM) will acquire the certificate. That file looks like this, and I've included commentary in the hope that someone might be able to identify where my thinking has gone wrong.\n```\n`// BELIEF: this is the DNS zone we will be operating in. No issues there.\n//\ndata \"aws_route53_zone\" \"zone\" {\n  name = \"${var.dns_zone_name}.\"\n}\n\n// BELIEF: this creates a CNAME record that points to our AWS ELB instance for our application.\n// This is where it departs a little from the documentation, but I'm not sure if the documentation\n// is just being a bit sparse, or if the validation records are meant to also be part of this object.\n//\nresource \"aws_route53_record\" \"app\" {\n  zone_id = data.aws_route53_zone.zone.zone_id\n  name    = \"${lookup(var.subdomain, terraform.workspace)}.${data.aws_route53_zone.zone.name}\"\n  type    = \"CNAME\"\n  ttl     = \"300\"\n\n  records = [aws_lb.api.dns_name]\n}\n\n// BELIEF: this models the certificate for our application.\n//\nresource \"aws_acm_certificate\" \"app_cert\" {\n  domain_name       = aws_route53_record.app.fqdn\n  validation_method = \"DNS\"\n\n  tags = local.common_tags\n\n  lifecycle {\n    create_before_destroy = true\n  }\n}\n\n// BELIEF: to prove ownership of the domain we need to be able to prove that we can\n// place certain records into the domain as part of a DNS challenge method.\n// I believe this is meant to refer to just those challenge records.\n// As this is the dynamic part, its unlikely that the domain validation options would\n// be known ahead of time, and so this is where I'm coming unstuck.\n//\nresource \"aws_route53_record\" \"app_cert_validation_records\" {\n  allow_overwrite = true\n  zone_id         = data.aws_route53_zone.zone.zone_id\n  ttl             = \"60\"\n\n  for_each = {\n    for dvo in aws_acm_certificate.app_cert.domain_validation_options : dvo.domain_name => {\n      name   = dvo.resource_record_name\n      type   = dvo.resource_record_type\n      record = dvo.resource_record_value\n    }\n  }\n\n  name    = each.value.name\n  type    = each.value.type\n  records = [each.value.record]\n}\n\n// BELIEF: This doesn't create anything; its just a placeholder for the validation\n// process... presumably for dependency reasons.\n// It basically just associates the FQDN (in the certificate) with its validation records.\n//\nresource \"aws_acm_certificate_validation\" \"app_cert_validation_process\" {\n  certificate_arn         = aws_acm_certificate.app_cert.arn\n  validation_record_fqdns = [for record in aws_route53_record.app_cert_validation_records : record.fqdn]\n}\n\n// Reading https://registry.terraform.io/providers/hashicorp/aws/latest/docs/guides/version-3-upgrade#resource-aws_acm_certificate\n// I don't see where the problem is.\n`\n```\nIn case it matters, there is another resource that is depending on this, which is the ELB instance which uses the certificate:\n```\n`resource \"aws_lb_listener\" \"api_https\" {\n  load_balancer_arn = aws_lb.api.arn\n  port              = 443\n  protocol          = \"HTTPS\"\n  certificate_arn   = aws_acm_certificate_validation.app_cert_validation_process.certificate_arn\n\n  default_action {\n    type             = \"forward\"\n    target_group_arn = aws_lb_target_group.api.arn\n  }\n}\n`\n```\nI've been looking to follow https://registry.terraform.io/providers/hashicorp/aws/latest/docs/guides/version-3-upgrade#resource-aws_acm_certificate, which indicates that a plan should work and I should see the text \"(known after apply)\" in the output.\nI take it, based on my research to date, is that the root cause is that the AWS provider can't / won't predict what the domain validation options will be ahead of time.... and yet the upgrade documentation indicates that, while this is only something that is known after applying, it shouldn't cause a plan failure and I shouldn't need any ugly -target workaround.\nI tried using a depends_on, but that's not going to help with the plan stage.\nFull code is in https://gitlab.com/cameron.kerr.nz/recipe-app-api-devops/ if it helps.\nThanks for reading,\nCameron\nPS. I've asked this previously in https://discuss.hashicorp.com/t/aws-acm-certificate-app-cert-domain-validation-options-is-a-set-of-object-known-only-after-apply/31952, but so far no reply.",
      "solution": "TL;DR: This fixes your example from my testing:\n```\n` resource \"aws_acm_certificate\" \"app_cert\" {\n-  domain_name       = aws_route53_record.app.fqdn\n+  domain_name       = aws_route53_record.app.name\n   validation_method = \"DNS\"\n`\n```\nBut not so fast, this only works because for the record you are creating in `aws_route53_record.app` has the same name as the final fqdn.\nAs per the docs for the `aws_route53_record` resource, `fqdn` is built using the zone domain and the `name` parameter.\nFor further explanation, in the working example, we can see that `fqdn` is not known, but `name` is. Which the for_each in `aws_route53_record.app_cert_validation_records` doesn't like.\n```\n`  # aws_route53_record.app will be created\n  + resource \"aws_route53_record\" \"app\" {\n      + allow_overwrite = (known after apply)\n      + fqdn            = (known after apply)\n      + id              = (known after apply)\n      + name            = \"foo.example.com\"\n      + records         = [\n          + \"xxxx.alb.amazon.com\",\n        ]\n      + ttl             = 300\n      + type            = \"CNAME\"\n      + zone_id         = \"XXXXXXXXXXX\"\n    }\n`\n```",
      "question_score": 4,
      "answer_score": 5,
      "created_at": "2021-11-20T20:25:53",
      "url": "https://stackoverflow.com/questions/70049176/aws-acm-certificate-app-cert-domain-validation-options-is-a-set-of-object-known"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-aws",
      "question_id": 69662004,
      "title": "Sagemaker workforce with cognito",
      "problem": "i am trying to build the terraform for sagemaker private work force with private cognito\nFollowing : https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/sagemaker_workforce\nit working fine\nmain.tf\n```\n`resource \"aws_sagemaker_workforce\" \"workforce\" {\n  workforce_name = \"workforce\"\n\n  cognito_config {\n    client_id = aws_cognito_user_pool_client.congnito_client.id\n    user_pool = aws_cognito_user_pool_domain.domain.user_pool_id\n  }\n}\n\nresource \"aws_cognito_user_pool\" \"user_pool\" {\n  name = \"sagemaker-cognito-userpool\"\n}\n\nresource \"aws_cognito_user_pool_client\" \"congnito_client\" {\n  name            = \"congnito-client\"\n  generate_secret = true\n  user_pool_id    = aws_cognito_user_pool.user_pool.id\n}\n\nresource \"aws_cognito_user_group\" \"user_group\" {\n  name         = \"user-group\"\n  user_pool_id = aws_cognito_user_pool.user_pool.id\n}\n\nresource \"aws_cognito_user_pool_domain\" \"domain\" {\n  domain       = \"sagemaker-user-pool-ocr-domain\"\n  user_pool_id = aws_cognito_user_pool.user_pool.id\n}\n\nresource \"aws_sagemaker_workteam\" \"workteam\" {\n  workteam_name  = \"worker-team\"\n  workforce_name = aws_sagemaker_workforce.workforce.id\n  description    = \"worker-team\"\n\n  member_definition {\n    cognito_member_definition {\n      client_id  = aws_cognito_user_pool_client.congnito_client.id\n      user_pool  = aws_cognito_user_pool_domain.domain.user_pool_id\n      user_group = aws_cognito_user_group.user_group.id\n    }\n  }\n}\n\nresource \"aws_sagemaker_human_task_ui\" \"template\" {\n  human_task_ui_name = \"human-task-ui-template\"\n\n  ui_template {\n    content = file(\"${path.module}/sagemaker-human-task-ui-template.html\")\n  }\n}\n\nresource \"aws_sagemaker_flow_definition\" \"definition\" {\n  flow_definition_name = \"flow-definition\"\n  role_arn             = var.aws_iam_role\n\n  human_loop_config {\n    human_task_ui_arn                     = aws_sagemaker_human_task_ui.template.arn\n    task_availability_lifetime_in_seconds = 1\n    task_count                            = 1\n    task_description                      = \"Task description\"\n    task_title                            = \"Please review the Key Value Pairs in this document\"\n    workteam_arn                          = aws_sagemaker_workteam.workteam.arn\n  }\n\n  output_config {\n    s3_output_path = \"s3://${var.s3_output_path}\"\n  }\n}\n`\n```\nit's creating the cognito user pool with callback urls. These callback urls is coming from `aws_sagemaker_workforce.workforce.subdomain` and getting set in cognito automatically which is what i want.\nBut i also want to set config in cognito userpool like\n```\n`allowed_oauth_flows = [\"code\", \"implicit\"]\n  allowed_oauth_scopes = [\"email\", \"openid\", \"profile\"]\n`\n```\nnow when i add above two line we need to add callbackurl also which i dont want.\ni tried\n```\n`allowed_oauth_flows = [\"code\", \"implicit\"]\n  allowed_oauth_scopes = [\"email\", \"openid\", \"profile\"]\n  callback_urls = [aws_sagemaker_workforce.workforce.subdomain]\n`\n```\nwhich is giving error :\n```\n`Cycle: module.sagemaker.aws_cognito_user_pool_client.congnito_client, module.sagemaker.aws_sagemaker_workforce.workforce\n`\n```\nas both resource are dependent on each other, i want to pass those two line but it forces me to add callback url also.\nhere is the final main.tf which is failing with that three line\n```\n`resource \"aws_sagemaker_workforce\" \"workforce\" {\n  workforce_name = \"workforce\"\n\n  cognito_config {\n    client_id = aws_cognito_user_pool_client.congnito_client.id\n    user_pool = aws_cognito_user_pool_domain.domain.user_pool_id\n  }\n}\n\nresource \"aws_cognito_user_pool\" \"user_pool\" {\n  name = \"sagemaker-cognito-userpool\"\n}\n\nresource \"aws_cognito_user_pool_client\" \"congnito_client\" {\n  name            = \"congnito-client\"\n  generate_secret = true\n  user_pool_id    = aws_cognito_user_pool.user_pool.id\n\n  explicit_auth_flows                  = [\"ALLOW_REFRESH_TOKEN_AUTH\", \"ALLOW_USER_PASSWORD_AUTH\", \"ALLOW_CUSTOM_AUTH\", \"ALLOW_USER_SRP_AUTH\"]\n  allowed_oauth_flows_user_pool_client = true\n  supported_identity_providers = [\"COGNITO\"]\n\n  allowed_oauth_flows = [\"code\", \"implicit\"]\n  allowed_oauth_scopes = [\"email\", \"openid\", \"profile\"]\n  callback_urls = [aws_sagemaker_workforce.workforce.subdomain]\n}\n\nresource \"aws_cognito_user_group\" \"user_group\" {\n  name         = \"user-group\"\n  user_pool_id = aws_cognito_user_pool.user_pool.id\n}\n\nresource \"aws_cognito_user_pool_domain\" \"domain\" {\n  domain       = \"sagemaker-user-pool-ocr-domain\"\n  user_pool_id = aws_cognito_user_pool.user_pool.id\n}\n\nresource \"aws_sagemaker_workteam\" \"workteam\" {\n  workteam_name  = \"worker-team\"\n  workforce_name = aws_sagemaker_workforce.workforce.id\n  description    = \"worker-team\"\n\n  member_definition {\n    cognito_member_definition {\n      client_id  = aws_cognito_user_pool_client.congnito_client.id\n      user_pool  = aws_cognito_user_pool_domain.domain.user_pool_id\n      user_group = aws_cognito_user_group.user_group.id\n    }\n  }\n}\n\nresource \"aws_sagemaker_human_task_ui\" \"template\" {\n  human_task_ui_name = \"human-task-ui-template\"\n\n  ui_template {\n    content = file(\"${path.module}/sagemaker-human-task-ui-template.html\")\n  }\n}\n\nresource \"aws_sagemaker_flow_definition\" \"definition\" {\n  flow_definition_name = \"flow-definition\"\n  role_arn             = var.aws_iam_role\n\n  human_loop_config {\n    human_task_ui_arn                     = aws_sagemaker_human_task_ui.template.arn\n    task_availability_lifetime_in_seconds = 1\n    task_count                            = 1\n    task_description                      = \"Task description\"\n    task_title                            = \"Please review the Key Value Pairs in this document\"\n    workteam_arn                          = aws_sagemaker_workteam.workteam.arn\n  }\n\n  output_config {\n    s3_output_path = \"s3://${var.s3_output_path}\"\n  }\n}\n`\n```",
      "solution": "You do not need to specify the callback URL for the workforce. It is sufficient to specify the following in order to create the `aws_cognito_user_pool_client` resource:\n```\n`callback_urls = [\n    \"https://${aws_cognito_user_pool_domain.domain>.cloudfront_distribution_arn}\",\n]\n`\n```\nThen you reference the user pool client in your workforce definition:\n```\n`resource \"aws_sagemaker_workforce\" \"...\" {\n    workforce_name = \"...\"\n\n    cognito_config {\n        client_id = aws_cognito_user_pool_client..id\n        user_pool = aws_cognito_user_pool_domain..user_pool_id\n    }\n}\n`\n```\nExistence of the callback URLs can be proven after applying the terraform configuration by running `aws cognito-idp describe-user-pool-client --user-pool-id  --client-id `:\n```\n`\"UserPoolClient\": {\n    ...\n    \"CallbackURLs\": [\n        \"https://____.cloudfront.net\",\n        \"https://____.labeling.eu-central-1.sagemaker.aws/oauth2/idpresponse\"\n    ],\n    \"LogoutURLs\": [\n        \"https://____.labeling.eu-central-1.sagemaker.aws/logout\"\n    ],\n`\n```\nIt seems as terraform itself does not do anything special on workforce creation (see https://github.com/hashicorp/terraform-provider-aws/blob/main/internal/service/sagemaker/workforce.go). So the callback urls seem to be added by AWS SageMaker itself.\nThis means that you have to instruct terraform to ignore changes on those attributes in the `aws_cognito_user_pool_client` configuration:\n```\n`lifecycle {\n    ignore_changes = [\n        callback_urls, logout_urls\n    ]\n}\n`\n```",
      "question_score": 4,
      "answer_score": 5,
      "created_at": "2021-10-21T14:29:30",
      "url": "https://stackoverflow.com/questions/69662004/sagemaker-workforce-with-cognito"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-aws",
      "question_id": 69498813,
      "title": "How to filter aws subnets in terraform",
      "problem": "I am porting a custom made deployment infrastructure to terraform. In that custom codebase there is something that says - fetch all available subnets from default region vpc but only those that have 20 or more available IPv4 addresses.\nSo I was experimenting with this code\n`data \"aws_vpc\" \"main\" {\n  default = true\n}\n\ndata \"aws_subnets\" \"vpcsubnets\" {\n  filter {\n    name   = \"vpc-id\"\n    values = [data.aws_vpc.main.id]\n  }\n  filter {\n    name   = \"default-for-az\"\n    values = [true]\n  }\n  filter {\n    name   = \"state\"\n    values = [\"available\"]\n  }\n}\n\noutput \"ids2\" {\n  value = {\n    for k, v in data.aws_subnets.vpcsubnets : k => v if v.available_ip_address_count > 20\n  }\n}\n\n`\nBut I get errors like these\n```\n` Error: Invalid reference\n\u2502 \n\u2502   on main.tf line 51, in output \"ids2\":\n\u2502   51:     for k, vid in data.aws_subnets.vpcsubnets : k => v if v.available_ip_address_count > 20\n\u2502 \n\u2502 A reference to a resource type must be followed by at least one attribute access, specifying the resource name.\n`\n```\nUsing Terraform 1.0.8 and aws provider 3.62",
      "solution": "You need an extra intermediary step here. The full list of available subnets is available in the attribute `data.aws_subnets.vpcsubnets.ids`, but the attribute `available_ip_address_count` will only be available from the `aws_subnet` data. You need to retrieve that information for each available subnet in an intermediary data:\n```\n`data \"aws_subnet\" \"vpcsubnet\" {\n  for_each = toset(data.aws_subnets.vpcsubnets.ids)\n\n  id = each.value\n}\n`\n```\nNow the attribute is available in the namespace `data.aws_subnet.vpcsubnet[\"\"].available_ip_address_count`. You can easily make a small update to your `output` for this:\n```\n`output \"ids2\" {\n  value = {\n    for id, attributes in data.aws_subnet.vpcsubnet : id => attributes if attributes.available_ip_address_count > 20\n  }\n}\n`\n```\nwhere I also renamed the temporary lambda variables for clarity.",
      "question_score": 4,
      "answer_score": 3,
      "created_at": "2021-10-08T18:09:27",
      "url": "https://stackoverflow.com/questions/69498813/how-to-filter-aws-subnets-in-terraform"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-aws",
      "question_id": 66425619,
      "title": "Include tags on aws_iam_policy resource on Terraform",
      "problem": "I'm trying to create a policy following the Terraform documentation\n```\n`resource \"aws_iam_policy\" \"policy\" {\n  name        = \"test_policy\"\n  path        = \"/\"\n  description = \"My test policy\"\n\n  policy = jsonencode({\n    Version = \"2012-10-17\"\n    Statement = [\n      {\n        Action = [\n          \"ec2:Describe*\",\n        ]\n        Effect   = \"Allow\"\n        Resource = \"*\"\n      },\n    ]\n  })\n}\n`\n```\nUnfortunately, there is no description of how to include tags.  I'm not able to do it such as I did with the rest of the resources, even when I can provide that manually from the AWS Management Console.\nThe `tags` setting does not seem to be working. I'm receiving an error if I try to do the same I did with an IAM role, including:\n```\n`  tags = {\n    tag-key = \"tag-value\"\n  }\n`\n```",
      "solution": "Update:\nThis functionality was added in https://github.com/hashicorp/terraform-provider-aws/pull/18276 and released as part of v3.35.0 of the AWS provider.\nYou should now be able to add tags to your `aws_iam_policy` resources as you'd expect:\n```\n`resource \"aws_iam_policy\" \"policy\" {\n  name        = \"test_policy\"\n  path        = \"/\"\n  description = \"My test policy\"\n\n  policy = jsonencode({\n    Version = \"2012-10-17\"\n    Statement = [\n      {\n        Action = [\n          \"ec2:Describe*\",\n        ]\n        Effect   = \"Allow\"\n        Resource = \"*\"\n      },\n    ]\n  })\n\n  tags = {\n    tag-key = \"tag-value\"\n  }\n}\n`\n```\n\nTagging customer managed IAM policies is a new feature that was introduced on 11th February 2021. Currently there's only a feature request for this functionality on the AWS provider.\nOnce someone has added the necessary change and it has been merged and released you should expect this to work with the syntax you provided in the question.",
      "question_score": 4,
      "answer_score": 5,
      "created_at": "2021-03-01T17:11:03",
      "url": "https://stackoverflow.com/questions/66425619/include-tags-on-aws-iam-policy-resource-on-terraform"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-aws",
      "question_id": 76658472,
      "title": "Remove the SizeRestrictions_BODY size rule all together",
      "problem": "In AWS documentation rule states that it will allow only 8kb of request body.\nI have used terraform to make sure that all common rule set are created in below code.\n```\n`resource \"aws_wafv2_web_acl\" \"alb_waf_acl\" {\n  name        = \"api-alb-waf-acl-${var.usage}\"\n  scope       = \"REGIONAL\"\n  description = \"WAF ACL for the Data Refinery ALB\"\n\n  default_action {\n    allow {}\n  }\n\n  rule {\n    name     = \"base-rule\"\n    priority = 1\n\n    override_action {\n      none {}\n    }\n\n    statement {\n      managed_rule_group_statement {\n        vendor_name = \"AWS\"\n        name        = \"AWSManagedRulesCommonRuleSet\"\n      }\n    }\n\n    visibility_config {\n      cloudwatch_metrics_enabled = false\n      metric_name                = \"api-alb-waf-acl-base-rule-${var.usage}\"\n      sampled_requests_enabled   = false\n    }\n  }\n\n  visibility_config {\n    cloudwatch_metrics_enabled = false\n    metric_name                = \"api-alb-waf-acl-${var.usage}\"\n    sampled_requests_enabled   = false\n  }\n\n  tags = {\n    Name = \"api_alb_waf_acl_${var.usage}\"\n  }\n\nresource \"aws_wafv2_web_acl_association\" \"alb_waf_association\" {\n  resource_arn = aws_lb.alb.arn\n  web_acl_arn  = aws_wafv2_web_acl.alb_waf_acl.arn\n\n  timeouts {\n    create = \"10m\"\n  }\n}\n`\n```\nHow do I change the code such that I can remove the SizeRestrictions_BODY rule or make it disfunctional.",
      "solution": "I just allowed everything in Body using\n```\n` rule_action_override {\n      name = \"SizeRestrictions_BODY\"\n      action_to_use {\n        allow {}\n      }\n    }\n`\n```\nHere is my whole code\n```\n`resource \"aws_wafv2_web_acl\" \"alb_waf_acl\" {\n  name        = \"api-alb-waf-acl-${var.usage}\"\n  scope       = \"REGIONAL\"\n  description = \"WAF ACL for the Data Refinery ALB\"\n\n  default_action {\n    allow {}\n  }\n\n  rule {\n    name     = \"base-rule\"\n    priority = 1\n\n    override_action {\n      none {}\n    }\n\n    statement {\n      managed_rule_group_statement {\n        vendor_name = \"AWS\"\n        name        = \"AWSManagedRulesCommonRuleSet\"\n        rule_action_override {\n          name = \"SizeRestrictions_BODY\"\n          action_to_use {\n            allow {}\n          }\n        }\n      }\n    }\n\n    visibility_config {\n      cloudwatch_metrics_enabled = false\n      metric_name                = \"api-alb-waf-acl-base-rule-${var.usage}\"\n      sampled_requests_enabled   = false\n    }\n  }\n\n  visibility_config {\n    cloudwatch_metrics_enabled = false\n    metric_name                = \"api-alb-waf-acl-${var.usage}\"\n    sampled_requests_enabled   = false\n  }\n\n  tags = {\n    Name = \"api_alb_waf_acl_${var.usage}\"\n  }\n}\n`\n```\nNote that this will allow all sizes and won't restrict to a certain size!",
      "question_score": 4,
      "answer_score": 3,
      "created_at": "2023-07-11T05:06:00",
      "url": "https://stackoverflow.com/questions/76658472/remove-the-sizerestrictions-body-size-rule-all-together"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-aws",
      "question_id": 70018268,
      "title": "Terraform - Optional SSM parameter lookup",
      "problem": "I'm doing a lookup for an SSM parameter which may or may not exist depending on a variable passed in:\n```\n`data \"aws_ssm_parameter\" \"server_tags\" {\n  name  = \"/${var.env_number}/server_tags\"\n}\n`\n```\nI am then using it like below in my locals and passing to my module:\n```\n`locals {\n  server_tags = data.aws_ssm_parameter.server_tags != null ? jsondecode(data.aws_ssm_parameter.server_tags.value) : {}\n  instance_tags = merge(var.instance_tags, local.server_tags)\n}\n`\n```\nThis works fine when my parameter exists, but if I pass in a value where my parameter doesn't exist, I get an error:\n```\n`Error describing SSM parameter (/997/server_tags): ParameterNotFound: \n`\n```\nIs there anyway I can do a pre-check to see if the parameter exists or make it optional somehow?\nThanks",
      "solution": "Sadly you can't do this. There is no way build-on mechanism for TF to check if a data source exists or not. But you can program your own logic for that using External Data Source.\nSince you program the external data source, you can create a logic for checking if a resource exists or not.",
      "question_score": 4,
      "answer_score": 4,
      "created_at": "2021-11-18T11:33:13",
      "url": "https://stackoverflow.com/questions/70018268/terraform-optional-ssm-parameter-lookup"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-aws",
      "question_id": 66873794,
      "title": "How to create Terraform sub-module",
      "problem": "I'm using Terraform to manage AWS resources, I have a module, the source for it is the whole `terraform` folder, but now I want to create a sub-module under this module for `A.tf` and `B.tf` files, so that when I apply terraform, if I specify this submodule, Terraform doesn't have to create all the resources outside of this sub-module.\nI've tried a couple of thing but still not working, is there any example I can follow?",
      "solution": "As @luk2302 pointed it would be good to create composable modules rather than submodules.\nanyways I was able to create something as you asked.\n`\u276f\u276f tree\n.\n\u251c\u2500\u2500 main.tf\n\u251c\u2500\u2500 sub_module_1\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 main.tf\n\u251c\u2500\u2500 sub_module_2\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 main.tf\n\u2514\u2500\u2500 terraform.tfstate\n\n2 directories, 4 files\ntmp/boo/parent_module\n\u276f\u276f ls\nmain.tf            sub_module_1/      sub_module_2/      terraform.tfstate\n`\n`\u276f\u276f terraform state list\naws_iam_role_policy_attachment.lambda_logs\naws_sns_topic.user_updates\nmodule.iam_role_info.aws_iam_role.iam_for_lambda\nmodule.logging_policy.aws_iam_policy.lambda_logging\n`\n`\u276f\u276f ls\nmain.tf        parent_module/\n/private/tmp/boo\n\u276f\u276f cat main.tf\nvariable \"env\" {\n  type    = string\n  default = \"dev\"\n}\nlocals {\n  default_tags = {\n    Product     = \"wallaby\",\n    Environment = var.env,\n    Application = \"wallaby-api\"\n  }\n}\n\nmodule \"parent_module\" {\nsource = \"./parent_module\"\n}\n\noutput \"sns_info\" {\n  value = module.parent_module.sns_info\n}\n`\nin the root of the `parent_module` `main.tf`\n```\n`\u276f\u276f cat main.tf\nmodule \"iam_role_info\" {\n  source = \"./sub_module_1\"\n}\n\nmodule \"logging_policy\" {\n  source = \"./sub_module_2\"\n}\n\nresource \"aws_iam_role_policy_attachment\" \"lambda_logs\" {\n  role       = module.iam_role_info.lambda_role_name\n  policy_arn = module.logging_policy.iam_policy_arn\n}\n\nresource \"aws_sns_topic\" \"user_updates\" {\n  name = \"user-updates-topic\"\n}\n\noutput \"sns_info\" {\n  value = aws_sns_topic.user_updates.arn\n}\n`\n```\n```\n`\u276f\u276f cat sub_module_1/main.tf\nresource \"aws_iam_role\" \"iam_for_lambda\" {\n  name = \"iam_for_lambda\"\n\n  assume_role_policy = <<EOF\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Action\": \"sts:AssumeRole\",\n      \"Principal\": {\n        \"Service\": \"lambda.amazonaws.com\"\n      },\n      \"Effect\": \"Allow\",\n      \"Sid\": \"\"\n    }\n  ]\n}\nEOF\n}\n\noutput \"lambda_role_name\" {\n    value = aws_iam_role.iam_for_lambda.name\n}\n`\n```\n```\n`\u276f\u276f cat sub_module_2/main.tf\n# See also the following AWS managed policy: AWSLambdaBasicExecutionRole\nresource \"aws_iam_policy\" \"lambda_logging\" {\n  name        = \"lambda_logging\"\n  path        = \"/\"\n  description = \"IAM policy for logging from a lambda\"\n\n  policy = <<EOF\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Action\": [\n        \"logs:CreateLogGroup\",\n        \"logs:CreateLogStream\",\n        \"logs:PutLogEvents\"\n      ],\n      \"Resource\": \"arn:aws:logs:*:*:*\",\n      \"Effect\": \"Allow\"\n    }\n  ]\n}\nEOF\n}\n\noutput \"iam_policy_arn\" {\nvalue = aws_iam_policy.lambda_logging.arn\n`\n```",
      "question_score": 4,
      "answer_score": 4,
      "created_at": "2021-03-30T17:20:54",
      "url": "https://stackoverflow.com/questions/66873794/how-to-create-terraform-sub-module"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-aws",
      "question_id": 65975035,
      "title": "Cannot connect to RDS in another VPC via VPC peering",
      "problem": "I have two VPCs:\n\nVPC A\n\nRDS instance\n\nVPC B\n\nEC2 instance\n\nThere are also few subnets:\n\nVPC A\n\nPrivate A\nPrivate B\nPeer A\n\nVPC B\n\nPrivate A\nPrivate B\nPeer A\n\nThe RDS is in Private A, Private B, Peer A of VPC A.\nThe EC2 is in Peer A of VPC B.\nI want to connect to the RDS instance from the EC2.\nI have created a peering:\n```\n`resource \"aws_vpc_peering_connection\" \"a_to_b\" {\n  vpc_id      = aws_vpc.a.id\n  peer_vpc_id = aws_vpc.b.id\n  auto_accept = true\n\n  accepter {\n    allow_remote_vpc_dns_resolution = true\n  }\n\n  requester {\n    allow_remote_vpc_dns_resolution = true\n  }\n}\n\nresource \"aws_vpc_peering_connection_accepter\" \"a_to_b\" {\n  vpc_peering_connection_id = aws_vpc_peering_connection.a_to_b.id\n  auto_accept               = true\n}\n`\n```\nI also have route tables for the whole CIDR block like so:\n```\n`resource \"aws_route_table\" \"a_peer\" {\n  vpc_id = aws_vpc.a.id\n}\n\nresource \"aws_route_table_association\" \"a_peer\" {\n  route_table_id = aws_route_table.a_peer.id\n  subnet_id      = aws_subnet.a_peer.id\n}\n\nresource \"aws_route\" \"a_peer_b\" {\n  route_table_id            = aws_route_table.a_peer.id\n  destination_cidr_block    = aws_subnet.b_peer.cidr_block\n  vpc_peering_connection_id = aws_vpc_peering_connection.a_to_b.id\n}\n`\n```\n```\n`resource \"aws_route_table\" \"b_peer\" {\n  vpc_id = aws_vpc.b.id\n}\n\nresource \"aws_route_table_association\" \"b_peer\" {\n  route_table_id = aws_route_table.b_peer.id\n  subnet_id      = aws_subnet.b_peer.id\n}\n\nresource \"aws_route\" \"b_peer_a\" {\n  route_table_id            = aws_route_table.b_peer.id\n  destination_cidr_block    = aws_subnet.a_peer.cidr_block\n  vpc_peering_connection_id = aws_vpc_peering_connection.a_to_b.id\n}\n`\n```\nI have also created security groups from `ingress` and `egress` on the RDS instance to the EC2 security group.\nWhen I SSH into the EC2 I can get the DNS:\n`$ nslookup rds.xxxxxxxxxxx.eu-west-2.rds.amazonaws.com\nServer:     192.16.0.2\nAddress:    192.16.0.2#53\n\nNon-authoritative answer:\nName:   rds.xxxxxxxxxxx.eu-west-2.rds.amazonaws.com\nAddress: 10.16.192.135\n`\nHowever, `curl` cannot connect:\n`$ curl rds.xxxxxxxxxxx.eu-west-2.rds.amazonaws.com:5432\n`\nThe expected response is:\n```\n`$ curl rds.xxxxxxxxxxx.eu-west-2.rds.amazonaws.com:5432\ncurl: (52) Empty reply from server\n`\n```\nThe VPC peering is \"Active\" and the route tables match the Terraform.\nHow can I get this to connect?",
      "solution": "I did some tests on my own, and I'm pretty sure that the issue is caused by your routes, assuming that everything else in your VPC is correct as the VPCs and subnets definitions are not shown .\nSpecifically, you wrote that \"RDS is in Private A, Private B, Peer A of VPC A\".  This means that RDS master may be in any of these subnets. You have no control over it, as its up to RDS to choose which subnet to use. You can only partially control it by selecting AZs when you create your RDS. Subsequently, your peering route tables should cover all these three subnets.  The easiest way to achieve this is by using VPC CIDR range:\n```\n`# Route from instance in VPC B to any subnet in VPC A which\n# hosts your RDS in all its subnets\nresource \"aws_route\" \"b_peer_a\" {\n  route_table_id            = aws_route_table.b_peer.id\n  destination_cidr_block    = aws_vpc.a.cidr_block\n  vpc_peering_connection_id = aws_vpc_peering_connection.a_to_b.id\n}\n`\n```\nThen you also need to have a route table in VPC A associated with your peering connections for all its subnets:\n```\n`resource \"aws_route_table\" \"a_peer\" {\n  vpc_id = aws_vpc.a.id\n}\n\nresource \"aws_route_table_association\" \"a_peer\" {\n  route_table_id = aws_route_table.a_peer.id\n  subnet_id      = aws_subnet.a_peer.id\n}\n\nresource \"aws_route_table_association\" \"a_private1\" {\n  route_table_id = aws_route_table.a_peer.id\n  subnet_id      = aws_subnet.a_private1.id\n}\n\nresource \"aws_route_table_association\" \"a_private2\" {\n  route_table_id = aws_route_table.a_peer.id\n  subnet_id      = aws_subnet.a_private2.id\n}\n\nresource \"aws_route\" \"a_peer_b\" {\n  route_table_id            = aws_route_table.a_peer.id\n  destination_cidr_block    = aws_subnet.b_peer.cidr_block\n  vpc_peering_connection_id = aws_vpc_peering_connection.a_to_b.id\n}\n`\n```",
      "question_score": 4,
      "answer_score": 4,
      "created_at": "2021-01-31T02:56:26",
      "url": "https://stackoverflow.com/questions/65975035/cannot-connect-to-rds-in-another-vpc-via-vpc-peering"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-aws",
      "question_id": 74544208,
      "title": "terraform - mount EFS to ECS fargate: No such file or directory",
      "problem": "I am trying to mount a persistent volume to a container but the container doesn't start because of the error: \"No such file or directory\".\nHere the relevant configuration:\n```\n`# EFS\nresource \"aws_security_group\" \"allow_nfs_inbound\" {\n  name   = \"${local.resource_prefix}-allow-nfs-inbound\"\n  vpc_id = module.vpc.vpc_id\n\n  ingress {\n    from_port        = 2049\n    to_port          = 2049\n    protocol         = \"tcp\"\n    cidr_blocks      = [\"0.0.0.0/0\"]\n    ipv6_cidr_blocks = [\"::/0\"]\n  }\n\n  egress {\n    from_port        = 0\n    to_port          = 0\n    protocol         = \"all\"\n    cidr_blocks      = [\"0.0.0.0/0\"]\n    ipv6_cidr_blocks = [\"::/0\"]\n  }\n\n  lifecycle {\n    create_before_destroy = true\n  }\n}\n\nresource \"aws_efs_file_system\" \"persistent\" {\n  creation_token = \"${local.resource_prefix}-efs\"\n  encrypted      = true\n}\n\nresource \"aws_efs_mount_target\" \"mount_targets\" {\n  count = length(module.vpc.private_subnets)\n\n  file_system_id  = aws_efs_file_system.persistent.id\n  subnet_id       = module.vpc.private_subnets[count.index]\n  security_groups = [aws_security_group.allow_nfs_inbound.id]\n}\n\nresource \"aws_efs_access_point\" \"assets_access_point\" {\n  file_system_id = aws_efs_file_system.persistent.id\n\n  root_directory {\n    path = \"/assets\"\n\n    creation_info {\n      owner_gid   = 0\n      owner_uid   = 0\n      permissions = \"755\"\n    }\n  }\n}\n\nresource \"aws_efs_access_point\" \"shared_access_point\" {\n  file_system_id = aws_efs_file_system.persistent.id\n\n  root_directory {\n    path = \"/shared\"\n\n    creation_info {\n      owner_gid   = 0\n      owner_uid   = 0\n      permissions = \"755\"\n    }\n  }\n}\n\n# ECS\nresource \"aws_ecs_task_definition\" \"backend_task_definition\" {\n  ...\n\n  container_definitions = jsonencode(\n    [\n\n      {\n        ...\n        mountPoints = [\n          {\n            sourceVolume  = \"assets\"\n            containerPath = \"/app/assets\"\n            readOnly      = false\n          },\n          {\n            sourceVolume  = \"shared\"\n            containerPath = \"/app/shared\"\n            readOnly      = false\n          }\n        ]\n        volumesFrom = []\n        ...\n      }\n    ]\n\n  volume {\n    name = \"assets\"\n\n    efs_volume_configuration {\n      file_system_id = aws_efs_file_system.persistent.id\n      root_directory = \"/assets\"\n    }\n  }\n\n  volume {\n    name = \"shared\"\n\n    efs_volume_configuration {\n      file_system_id = aws_efs_file_system.persistent.id\n      root_directory = \"/shared\"\n    }\n  }\n  ...\n}\n\nresource \"aws_security_group\" \"allow_efs\" {\n  name   = \"${local.resource_prefix}-allow-efs\"\n  vpc_id = module.vpc.vpc_id\n\n  ingress {\n    from_port        = 2049\n    to_port          = 2049\n    protocol         = \"tcp\"\n    cidr_blocks      = [\"0.0.0.0/0\"]\n    ipv6_cidr_blocks = [\"::/0\"]\n  }\n}\n\nresource \"aws_ecs_service\" \"backend_ecs_service\" {\n\n  ...\n\n  network_configuration {\n    subnets = setunion(\n      module.vpc.public_subnets\n    )\n    security_groups = [aws_security_group.allow_efs.id]\n    assign_public_ip = true\n  }\n\n  ...\n}\n\n`\n```\nThe complete error message is:\n\nResourceinitializationerror: failed to invoke EFS utils commands to set up EFS volumes: stderr: b'mount.nfs4: mounting :/assets failed, reason given by server: No such file or directory' : unsuccessful EFS utils command execution; code: 32\n\nThe network configuration should be correct because if I remove the security groups I get a different network related error.\nEDIT:\nAdded docker `RUN`:\n```\n`RUN apt-get update && \\\n    apt-get -y install git binutils && \\\n    git clone https://github.com/aws/efs-utils && \\\n    cd efs-utils && \\\n    ./build-deb.sh && \\\n    apt-get -y install ./build/amazon-efs-utils*deb\n\nRUN wget https://bootstrap.pypa.io/pip/3.5/get-pip.py -O /tmp/get-pip.py  && \\\n    python3 /tmp/get-pip.py && \\\n    pip3 install botocore\n`\n```",
      "solution": "I'm really not sure why you are getting that specific error. You might want to make sure you have `platform_version = \"1.4.0\"` in your `aws_ecs_service` resource definition. I think `1.4.0` is the default now, but you aren't showing your entire resource definition, and if you had it set to something like `1.3.0` that could cause this issue.\nAlso makes sure you have `launch_type = \"FARGATE\"` in the `aws_ecs_service` resource definition (again I'm having to guess, because you didn't include the full code). That error really sounds to me more like an issue that would happen with EC2 deployments running a custom EC2 AMI, than with Fargate deployments. So double check you are really deploying to Fargate.\nAlso, you aren't configuring ECS to use the EFS access point you have created. You should change your `volume` blocks to look like this;\n```\n`  volume {\n    name = \"assets\"\n\n    efs_volume_configuration {\n      file_system_id = aws_efs_file_system.persistent.id\n      root_directory = \"/assets\"\n      transit_encryption = \"ENABLED\"\n\n      authorization_config {\n        access_point_id = aws_efs_access_point.assets_access_point.id\n        iam             = \"ENABLED\"\n      }\n    }\n  }\n\n  volume {\n    name = \"shared\"\n\n    efs_volume_configuration {\n      file_system_id = aws_efs_file_system.persistent.id\n      root_directory = \"/shared\"\n      transit_encryption = \"ENABLED\"\n\n      authorization_config {\n        access_point_id = aws_efs_access_point.shared_access_point.id\n        iam             = \"ENABLED\"\n      }\n    }\n  }\n`\n```\n\nAlso, why are you doing `setunion()` here?\n```\n`    subnets = setunion(\n      module.vpc.public_subnets\n    )\n`\n```\nIf your `module.vpc.public_subnets` are a list of sets, then you need to do `setunion()` everywhere you are accessing that module output variable, like in the `aws_efs_mount_target` resource. But if your `module.vpc.public_subnets` is not a list of sets then that function call is pointless.",
      "question_score": 4,
      "answer_score": 3,
      "created_at": "2022-11-23T10:09:11",
      "url": "https://stackoverflow.com/questions/74544208/terraform-mount-efs-to-ecs-fargate-no-such-file-or-directory"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-aws",
      "question_id": 67897564,
      "title": "Error: Invalid Index The given key does not identify an element in this collection value Transit gateway routes",
      "problem": "I am trying to create routes in transit gateway route table. Below is the code block.\n```\n`locals {\n  vpc_attachments_with_routes = chunklist(flatten([\n    for k, v in var.vpc_attachments : setproduct([{ key = k }], v[\"tgw_route\"]) if length(lookup(v, \"tgw_route\", {})) > 0\n  ]), 2)\n  }\n\nresource \"aws_ec2_transit_gateway_route_table\" \"route\" {\n  count = var.create_tgw ? 1 : 0\n  transit_gateway_id = aws_ec2_transit_gateway.this[0].id\n}\n\nresource \"aws_ec2_transit_gateway_route\" \"this\" {\n  count = length(local.vpc_attachments_with_routes)\n\n  destination_cidr_block = local.vpc_attachments_with_routes[count.index][1][\"destination_cidr_block\"]\n  blackhole              = lookup(local.vpc_attachments_with_routes[count.index][1], \"blackhole\", null)\n\n  transit_gateway_route_table_id = aws_ec2_transit_gateway_route_table.route[count.index].id\n  transit_gateway_attachment_id  = tobool(lookup(local.vpc_attachments_with_routes[count.index][1], \"blackhole\", false)) == false ? aws_ec2_transit_gateway_vpc_attachment.this[local.vpc_attachments_with_routes[count.index][0][\"key\"]].id : null\n   depends_on = [\n    aws_ec2_transit_gateway_route_table.route,\n  ]\n}\n`\n```\nError:\nError: Invalid index\\n\\n  on ../modules/tgw/main.tf line 85, in resource \"aws_ec2_transit_gateway_route\" \"this\":\\n  85:   transit_gateway_route_table_id = aws_ec2_transit_gateway_route_table.route[count.index].id\\n    |----------------\\n    | aws_ec2_transit_gateway_route_table.route is tuple with 1 element\\n    | count.index is 1\\n\\nThe given key does not identify an element in this collection value.\\n\\n\",",
      "solution": "You will have only 0 or 1 `aws_ec2_transit_gateway_route_table.route`, depending on the value of `create_tgw`. So it should be:\n```\n`resource \"aws_ec2_transit_gateway_route\" \"this\" {\n  count = length(local.vpc_attachments_with_routes)\n\n  destination_cidr_block = local.vpc_attachments_with_routes[count.index][1][\"destination_cidr_block\"]\n  blackhole              = lookup(local.vpc_attachments_with_routes[count.index][1], \"blackhole\", null)\n\n  transit_gateway_route_table_id = var.create_tgw ? aws_ec2_transit_gateway_route_table.route[0].id : null \n\n  transit_gateway_attachment_id  = tobool(lookup(local.vpc_attachments_with_routes[count.index][1], \"blackhole\", false)) == false ? aws_ec2_transit_gateway_vpc_attachment.this[local.vpc_attachments_with_routes[count.index][0][\"key\"]].id : null\n}\n`\n```",
      "question_score": 4,
      "answer_score": 3,
      "created_at": "2021-06-09T06:21:17",
      "url": "https://stackoverflow.com/questions/67897564/error-invalid-index-the-given-key-does-not-identify-an-element-in-this-collecti"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-aws",
      "question_id": 66521578,
      "title": "Iterate a list inside an object with Terraform",
      "problem": "I'm trying to iterate a list of objects, which each one has a list of String inside, but I'm having trouble with it.\nI tried to use dynamic blocks, but I'm getting the error Unsupported block type.\nI'm declaring my variable that way:\n```\n`variable \"test\" {\n  type = list(object({\n    name = string\n    nicknames = list(string)\n  }))\n\n  default = [\n    {\n      name = \"Phoebe\"\n      nicknames = [\"Pheebs\", \"Phalange\"]\n    },\n    {\n      name = \"Chandler\"\n      nicknames = [\"Chanchan\", \"Mr. Bing\"]\n    }\n  ]\n}\n`\n```\nAnd my resource is that way:\n```\n`resource \"aws_lambda_function\" \"test_lambda\" {\n  for_each = {for i, v in var.teste:  i => v}\n  name             = each.value.name\n  nicknames  = each.value.nicknames\n  dynamic \"nicknames_list\" {\n    for_each = [each.value.nicknames]\n    content {\n      opn = nicknames_list.value\n    }\n  }\n}\n`\n```\nHow can I iterate an object with a list inside?",
      "solution": "You can to flatten your `test` into more `for_each` friendly structure:\n```\n`\nvariable \"test\" {\n  type = list(object({\n    name = string\n    nicknames = list(string)\n  }))\n\n  default = [\n    {\n      name = \"Phoebe\"\n      nicknames = [\"Pheebs\", \"Phalange\"]\n    },\n    {\n      name = \"Chandler\"\n      nicknames = [\"Chanchan\", \"Mr. Bing\"]\n    }\n  ]\n}\n\nlocals {\n\n  test_flat = merge([\n      for idx, val in var.test:\n        {\n          for name, nickname in val.nicknames:         \n              \"${idx}-${name}-${nickname}\" => {\n                  name = name\n                  nickname = nickname\n                }\n        }          \n    ]...)\n}\n`\n```\nwhich will result in `test_flat` of\n```\n`{\n  \"0-0-Pheebs\" = {\n    \"name\" = 0\n    \"nickname\" = \"Pheebs\"\n  }\n  \"0-1-Phalange\" = {\n    \"name\" = 1\n    \"nickname\" = \"Phalange\"\n  }\n  \"1-0-Chanchan\" = {\n    \"name\" = 0\n    \"nickname\" = \"Chanchan\"\n  }\n  \"1-1-Mr. Bing\" = {\n    \"name\" = 1\n    \"nickname\" = \"Mr. Bing\"\n  }\n}\n`\n```\nI'm not sure what do you want to do in your `aws_lambda_function` as attributes such as `nicknames` or `nicknames_list` are incorrect. But you could use the flatten list (example only, as your `aws_lambda_function` is incorrect to begin with)\n```\n`resource \"aws_lambda_function\" \"test_lambda\" {\n  \n  for_each                 = local.test_flat\n\n  function_name            = each.value.name\n  # some other attribute   = each.value.nickname\n}\n`\n```",
      "question_score": 4,
      "answer_score": 3,
      "created_at": "2021-03-07T22:23:48",
      "url": "https://stackoverflow.com/questions/66521578/iterate-a-list-inside-an-object-with-terraform"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-aws",
      "question_id": 70090851,
      "title": "Terraform: how to ignore &quot;Error: Your query returned no results&quot; for data source &quot;aws_instances&quot;",
      "problem": "I'm using Terraform 0.14.4 to maintain different AWS accounts. I have one `.tf` file and multiple state files, one for each account.\nI use a data source to find all EC2 instances that have a certain tag attached to it:\n```\n`data \"aws_instances\" \"all_instances\" {\n  instance_tags = {\n    Monitoring = \"MONITOR\"\n  }\n\n  instance_state_names = [\"running\", \"pending\", \"stopped\", \"stopping\"]\n}\n`\n```\nThis data source is used to generate a few CloudWatch metrics for certain instances.\nI have instances with that tag in all accounts but one. Running `terraform plan` on that account gives me this error:\n```\n`Error: Your query returned no results. Please change your search criteria and try again.\n  on main.tf line 6, in data \"aws_instances\" \"all_instances\":\n   6: data \"aws_instances\" \"all_instances\" {\n`\n```\nI'd like to ignore that the data source doesn't find any instances in this particular account, and go ahead with all the other resources maintained in my script.\nHow do I achieve this?",
      "solution": "Apart from what you mentioned in the comment, one other possibility would be to use External Data Source. So instead of using data source given by aws provider (`aws_instances`) you could implement your own. This way you can program in any logic you want, including handling missing resources.",
      "question_score": 4,
      "answer_score": 2,
      "created_at": "2021-11-24T05:44:52",
      "url": "https://stackoverflow.com/questions/70090851/terraform-how-to-ignore-error-your-query-returned-no-results-for-data-source"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-aws",
      "question_id": 71460671,
      "title": "terraform data archive file source directory with selected files and directories",
      "problem": "I want to create a data archive_file with selected files and folders as source_dir.\nI have the folder structure as below. Within the src directory, I have lambdas directory and within that there are few folders\nand a set of files as below.\n```\n`src\n|-lambdasfolder\n  |-__init__.py\n  |-commonfolder (config.py, __init__.py)\n  |-lambda1folder (requesthandler.py, __init__.py)\n  |-lambda2folder (requesthandler.py, __init__.py)\n  |testsfolder\n  |otherfolder\n  ...\n`\n```\nI want to create a source directory for data archive file with selected folders and files. I want a create a src directory with just a single file and couple of directories matching the\nstructure below.\n```\n`src\n|-lambdasfolder\n  |-__init__.py\n  |-commonfolder (config.py, __init__.py)\n  |-lambda1folder (requesthandler.py, __init__.py)\n`\n```\nI am finding the examples as below which zips the entire directory, but how can I zip only the required ones\n```\n`data \"archive_file\" \"lambda_source\"{\n  type = \"zip\"\n\n  source_dir  = \"${path.module}/../src\"\n  output_path = \"${path.module}/temp/src.zip\"\n}\n`\n```\nI have managed to get working to some extent by creating a null reference and archive file dependant on it.\n```\n`resource \"null_resource\" \"lambda-repo\" {  \n    triggers = {\n     #not sure on this\n   }\n\n provisioner \"local-exec\" {  \n    command = \"bash lambda-repo.sh\"\n    working_dir = \"${path.module}\"\n  }\n}\n\ndata \"archive_file\" \"lambda-repo-file\" {\n  depends_on = [null_resource.lambda-repo]\n  type = \"zip\"\n\n  source_dir  = \"${path.module}/lambda_archive/lambda-repo\"\n  output_path = \"${path.module}/lambda_archive/lambda-repo.zip\"\n\n}\n`\n```\nand the shell script as follows\n```\n`#!/bin/sh\n\nmkdir -p lambda_archive/lambda-repo/lambdasfolder/common\nmkdir -p lambda_archive/lambda-repo/lambdasfolder/lambda1folder\ntouch lambda_archive/lambda-repo/lambdasfolder/__init__.py\ncp -r ../src/lambdasfolder/common/. lambda_archive/lambda-repo/lambdasfolder/common\ncp -r ../src/lambdasfolder/lambda1folder/. lambda_archive/lambda-repo/lambdasfolder/lambda1folder\n`\n```\nAnd also on the s3 object, I need to comment the etag for the initial terraform apply\n```\n`resource \"aws_s3_object\" \"lambda-repo\" {\n  bucket = aws_s3_bucket.lambda-repo.id\n\n  key    = \"lambda-repo.zip\"\n  source = data.archive_file.lambda-repo-file.output_path\n  #had to comment etag \n  #etag = filemd5(data.archive_file.lambda-repo-file.output_path)\n}\n`\n```\nNow I am left with two issues.\n\nIt is not detecting any changes in lambda functions when terraform apply is executed after the first run\nHow do I remove the folders and zip folders created through null resource?",
      "solution": "Sadly you can't do this, unless you want to use local-exec to  create the zips in a fully custom way. Otherwise, you have to re-organize your folder structure to have fully separate folders for  `archive_file`.",
      "question_score": 4,
      "answer_score": 1,
      "created_at": "2022-03-13T21:41:15",
      "url": "https://stackoverflow.com/questions/71460671/terraform-data-archive-file-source-directory-with-selected-files-and-directories"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-aws",
      "question_id": 70306803,
      "title": "Getting attribute from Terrafrom cdk deployed lambda",
      "problem": "I'm using Terraform CDK to deploy a lambda function and am trying to set a trigger to it using s3 notifications. I'm sorta new to CDK, so I'm not sure where things might be going wrong here.\nReading this example and based also on what's done using regular CDK, I thought that to access the function arn (so to add it to bucket notification setting), it'd be `my_function.arn`, but it renders the following string\n`{TfToken[TOKEN.XXX]}`.\nIt seems to me that I'd be able to fetch the arn somewhere with this value, but I couldn't find out where.\nI thought of breaking it up into two stacks, but I needed both lambda and its notification trigger to be deployed together.\nThe code is\n```\n`#!/usr/bin/env python\nfrom constructs import Construct\nfrom cdktf import App, TerraformStack, TerraformOutput\nfrom imports.aws import AwsProvider\nfrom imports.aws.lambdafunction import LambdaFunction\nfrom imports.aws.s3 import S3BucketNotification, S3BucketNotificationLambdaFunction\nimport os\n\nclass My_Stack(TerraformStack):\n  def __init__(self, scope: Construct, ns: str):\n    super().__init__(scope, ns)\n\n    AwsProvider(self, 'Aws', region='my-region')\n\n    my_lambda_function = LambdaFunction(\n      self, id='id',\n      function_name='cdk-deployment-test',\n      role='my-role',\n      memory_size=128,\n      runtime='python3.8',\n      timeout=900,\n      handler=\"lambda_handler\",\n      filename=os.path.join(os.getcwd(), 'deployment_package/package.zip')\n    )\n\n    function_to_be_triggered = S3BucketNotificationLambdaFunction(\n      lambda_function_arn= my_lambda_function.arn,\n      events = [\"s3:ObjectCreate:*\"],\n      filter_prefix = \"path\"\n    )\n\n    payment_recognition_input = S3BucketNotification(\n      self, id='s3-bucket-notification',\n      bucket = 'my-bucket',\n      lambda_function=[function_to_be_triggered]\n    )\n\napp = App()\nMy_Stack(app, \"cdktf-poc\")\n\napp.synth()\n\n`\n```",
      "solution": "This is the correct way to reference the terraform resource's ARN property, the `{TfToken[TOKEN.XXX]}` tokens resolve to Terraform language syntax in the synth output. Check out the CDK For Terraform documentation here that discusses tokens:\nhttps://github.com/hashicorp/terraform-cdk/blob/main/docs/working-with-cdk-for-terraform/tokens.md#tokens\nFor example, this CDKTF code:\n`const vpc = new Vpc(this, \"my-vpc\", {\n  name: vpcName,\n});\n\nnew Eks(this, \"EksModule\", {\n  clusterName: \"my-kubernetes-cluster\",\n  vpcId: vpc.vpcIdOutput,\n});\n`\nultimately generates (using token) this terraform:\n`{\n  \"module\": {\n    \"helloterraEksModule5DDB67AE\": {\n      \"cluster_name\": \"my-kubernetes-cluster\",\n      \"vpc_id\": \"${module.helloterraMyVpc62D94C17.vpc_id}\"\n    }\n  }\n}\n`\nSo that that reference and dependency link still exist at Terraform plan/apply time.\nFor your specific use case, have tried out the following, using the pre-built aws provider and `S3BucketNotificationLambdaFunction` for structuring the lambda function configuration:\n`from cdktf_cdktf_provider_aws.s3 import S3BucketNotificationLambdaFunction, S3BucketNotification\nfrom cdktf_cdktf_provider_aws.lambda_function import LambdaFunction\n\nmy_lambda_function = LambdaFunction(\n    self, id='id',\n    function_name='cdk-deployment-test',\n    role='my-role',\n    memory_size=128,\n    runtime='python3.8',\n    timeout=900,\n    handler=\"lambda_handler\",\n    filename=os.path.join(os.getcwd(), 'deployment_package/package.zip')\n)\n\nS3BucketNotification(\n    self,\n    id=\"s3-bucket-notification\",\n    bucket=\"my-bucket\",\n    lambda_function=[\n        S3BucketNotificationLambdaFunction(\n            lambda_function_arn=my_lambda_function.arn,\n            events=[\"s3:ObjectCreate\"],\n            filter_prefix=\"path\"\n        )\n    ]\n)\n`",
      "question_score": 4,
      "answer_score": 1,
      "created_at": "2021-12-10T16:27:22",
      "url": "https://stackoverflow.com/questions/70306803/getting-attribute-from-terrafrom-cdk-deployed-lambda"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-aws",
      "question_id": 71906029,
      "title": "Terraform error configuring AWS provider backend issue",
      "problem": "i've had this issue in terraform with backend configuration. I am getting this error when running Terraform plan.\nError: error configuring Terraform AWS Provider: no valid credential sources for Terraform AWS Provider found.\n\u2502\n\u2502 Please see https://registry.terraform.io/providers/hashicorp/aws\n\u2502 for more information about providing credentials.\n\u2502\n\u2502 Error: failed to refresh cached credentials, no EC2 IMDS role found, operation error ec2imds: GetMetadata, request send failed, Get \"http://169.254.169.254/latest/meta-data/iam/security-credentials/\": dial tcp 169.254.169.254:80: i/o timeout\n```\n` with provider[\"registry.terraform.io/hashicorp/aws\"].west,\n`\n```\n\u2502    on providers.tf line 5, in provider \"aws\":\n\u2502    5: provider \"aws\" {\n\u2502\n\u2575\nHere is the code, there are no google pages to help with this error. I will appreciate any help and I'm forever grateful thanks\n```\n`terraform {\n  `enter code here`backend \"remote\" {\norganization = \"Gnome2\"\n\nworkspaces {\n  name = \"terraform-begin\"\n}\n`\n```\n}\n```\n`required_providers {\naws = {\n  source  = \"hashicorp/aws\"\n  version = \"4.8.0\"\n}\n`\n```\n}\n}\n```\n` provider \"aws\" {\n  region = \"us-east-1\"\n  }\n\n  provider \"aws\" {\n  alias  = \"west\"\n  region = \"us-west-1\"\n  }\n  module \"vpc\" {\n  source = \"terraform-aws-modules/vpc/aws\"\n  providers = {\n  aws = aws.west\n`\n```\n}\n```\n`  name = \"my-vpc\"\n  cidr = \"10.0.0.0/16\"\n\n  azs             = [\"us-east-1a\", \"us-east-1b\", \"us-east-1c\"]\n  private_subnets = [\"10.0.1.0/24\", \"10.0.2.0/24\", \"10.0.3.0/24\"]\n  public_subnets  = [\"10.0.101.0/24\", \"10.0.102.0/24\", \"10.0.103.0/24\"]\n\n  enable_nat_gateway = true\n  enable_vpn_gateway = true\n\n  tags = {\n      Terraform   = \"true\"\n      Environment = \"dev\"\n }\n`\n```\n}",
      "solution": "Can you use your AWS CLI to connect to your AWS account? like listing your AWS s3 buckets? based on the error I think you didn't configure your AWS by doing `aws configure` on your CLI. which will require you to have an access key id and secret for the setup.",
      "question_score": 3,
      "answer_score": 8,
      "created_at": "2022-04-18T00:47:05",
      "url": "https://stackoverflow.com/questions/71906029/terraform-error-configuring-aws-provider-backend-issue"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-aws",
      "question_id": 69633803,
      "title": "Terraform Import of map resources",
      "problem": "I've got a map variable that identifies existing s3 buckets:\n```\n`resource \"aws_s3_bucket\" \"bucket\" {\n  for_each = var.s3_replication\n  bucket   = each.value.source\n  #other configuration\n}\n\nvariable \"s3_replication\" {\n  description = \"Map of buckets to replicate\"\n  type        = map\n  default = {\n    logs = {\n      source = \"logs_bucket\",\n      destination = \"central_logs_bucket\"\n    },\n    security = {\n      source = \"cloudtrail_bucket\",\n      destination = \"central_security_bucket\"\n    }\n  }\n}\n`\n```\nSince these buckets already exist, I am trying to import them and then apply the a configuration to them to update the resources. Unfortunately, I am not able to figure out how to do a terraform import on these. I've tried:\n```\n`terraform import aws_s3_bucket.bucket[\"logs\"] logs_bucket\nterraform import aws_s3_bucket.bucket[logs] logs_bucket\nterraform import aws_s3_bucket.bucket[0] logs_bucket\nterraform import aws_s3_bucket.bucket[0].source logs_bucket\nterraform import aws_s3_bucket.bucket[0[source]] logs_bucket\n`\n```\nAll failing with a different error. Any idea on how to import existing resources listed on a map?",
      "solution": "The `terraform import` subcommand relies on strings in the map key within resource namespaces that are first class expression, and this causes issues with shell interpreters where the resource is not a first class expression because they are not the Terraform DSL. You can work around this by casting the entire resource name as a literal string:\n```\n`terraform import 'aws_s3_bucket.bucket[\"logs\"]' logs_bucket\n`\n```\nand this will resolve your issue.",
      "question_score": 3,
      "answer_score": 9,
      "created_at": "2021-10-19T17:35:58",
      "url": "https://stackoverflow.com/questions/69633803/terraform-import-of-map-resources"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-aws",
      "question_id": 73296226,
      "title": "AWS App Runner - Error in assuming instance role",
      "problem": "When running my TF script to create an AWS App Runner service I'm getting this error:\n`InvalidRequestException: Error in assuming instance role arn:aws:iam::000000000000:role/MyAppRunnerServiceRole\n`\nI created the role policy trust using `AppRunnerECRAccessRole` as reference, which is auto-generated by the console, but using either that or my own below I'm getting the same issue.\nHere's my TF code:\n```\n`### IAM ###\n\nresource \"aws_iam_role\" \"app_runner\" {\n  name = \"MyAppRunnerServiceRole\"\n\n  assume_role_policy = jsonencode({\n    Version = \"2012-10-17\"\n    Statement = [\n      {\n        Action = \"sts:AssumeRole\"\n        Effect = \"Allow\"\n        Sid    = \"\"\n        Principal = {\n          Service = \"build.apprunner.amazonaws.com\"\n        }\n      },\n    ]\n  })\n}\n\nresource \"aws_iam_role_policy_attachment\" \"app_runner\" {\n  role       = aws_iam_role.app_runner.name\n  policy_arn = \"arn:aws:iam::aws:policy/service-role/AWSAppRunnerServicePolicyForECRAccess\"\n}\n\n### App Runner ###\n\nresource \"aws_apprunner_service\" \"main\" {\n  service_name = \"sandbox-service\"\n\n  source_configuration {\n    image_repository {\n      image_configuration {\n        port = \"5000\"\n      }\n      image_identifier      = \"${aws_ecr_repository.main.repository_url}:latest\"\n      image_repository_type = \"ECR\"\n    }\n  }\n\n  instance_configuration {\n    instance_role_arn = aws_iam_role.app_runner.arn\n  }\n\n}\n`\n```\n\nThis is the `AppRunnerECRAccessRole` which is auto-generated by the Console when creating a new App Runner service. I would assume this same configuration would work, but it isn't.",
      "solution": "It seems that the access and instance roles were mixed up in your code. Based on the AWS documentation [1], you need to change the trust policy to be:\n```\n`{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"Service\": \"tasks.apprunner.amazonaws.com\"\n      },\n      \"Action\": \"sts:AssumeRole\"\n    }\n  ]\n}\n`\n```\nThe permissions policy should probably remain the same, but for the sake of the completeness of the answer, it should be something along the lines:\n```\n`{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"ecr:GetDownloadUrlForLayer\",\n        \"ecr:BatchCheckLayerAvailability\",\n        \"ecr:BatchGetImage\",\n        \"ecr:DescribeImages\",\n        \"ecr:GetAuthorizationToken\"\n      ],\n      \"Resource\": \"*\"\n    }\n  ]\n}\n`\n```\nYou can of course limit everything except the `ecr:GetAuthorizationToken` to your ECR repo. The `ecr:GetAuthroziationToken` has to be set for `\"Resource\": \"*\"`.\nUpdate: Correct placement of the Access role within the source configuration section for the `\"build.apprunner.amazonaws.com\"` service\n```\n`resource \"aws_apprunner_service\" \"example\" {\n  source_configuration {\n    authentication_configuration {\n      access_role_arn = aws_iam_role.access_role.arn\n    }\n  }\n}\n`\n```\n\n[1] https://docs.aws.amazon.com/apprunner/latest/dg/security_iam_service-with-iam.html#security_iam_service-with-iam-roles-service.instance",
      "question_score": 3,
      "answer_score": 11,
      "created_at": "2022-08-09T19:57:29",
      "url": "https://stackoverflow.com/questions/73296226/aws-app-runner-error-in-assuming-instance-role"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-aws",
      "question_id": 72159273,
      "title": "using Terraform to pass a file to newly created ec2 instance without sharing the private key in &quot;connection&quot; section",
      "problem": "My setup is:\nTerraform --> AWS ec2\nusing Terraform to create the ec2 instance with SSH access.\nThe\n```\n`resource \"aws_instance\" \"inst1\" {\n  instance_type = \"t2.micro\"\n  ami           = data.aws_ami.ubuntu.id\n  key_name      = \"aws_key\"\n  subnet_id     = ...\n  user_data     = file(\"./deploy/templates/user-data.sh\")\n\n  vpc_security_group_ids = [\n    ... ,\n  ]\n  provisioner \"file\" {\n    source      = \"./deploy/templates/ec2-caller.sh\"\n    destination = \"/home/ubuntu/ec2-caller.sh\"\n  }\n\n  provisioner \"remote-exec\" {\n    inline = [\n      \"chmod +x /home/ubuntu/ec2-caller.sh\",      \n    ]\n  }\n\n  connection {\n    type        = \"ssh\"\n    host        = self.public_ip\n    user        = \"ubuntu\"\n    private_key = file(\"./keys/aws_key_enc\")\n    timeout     = \"4m\"\n  }\n}\n\n`\n```\nthe above bit works and i could see the provisioner copying and executing the 'ec2-caller.sh.\nI don't want to pass my private key in clear text to the Terraform provisioner. Is there anyway we can copy files to the newly created ec2 without using provisioner or without passing the private key on to the provisioner?\nCheers.",
      "solution": "The Terraform documentation section Provisioners are a Last Resort raises the need to provision and pass in credentials as one of the justifications for provisioners being a \"last resort\", and then goes on to suggest some other strategies for passing data into virtual machines and other compute resources.\nYou seem to already be using `user_data` to specify some other script to run, so to follow the advice in that document would require combining these all together into a single cloud-init configuration. (I'm assuming that your AMI has cloud-init installed because that's what's typically responsible for interpreting `user_data` as a shell script to execute.)\nCloud-init supports several different `user_data` formats, with the primary one being cloud-init's own YAML configuration file format, \"Cloud Config\". You can also use a multipart MIME message to pack together multiple different `user_data` payloads into a single `user_data` body, as long as the combined size of the payload fits within EC2's upper limit for `user_data` size, which is 16kiB.\nFrom your configuration it seems like you have two steps you'd need cloud-init to deal with in order to fully solve this problem with cloud-init:\n\nRun the `./deploy/templates/user-data.sh` script.\nPlace the `/home/ubuntu/ec2-caller.sh` on disk with suitable permissions.\n\nAssuming that these two steps are independent of one another, you can send `cloud-init` a multipart MIME message which includes both the user-data script you were originally using alone and a Cloud Config YAML configuration to place the `ec2-caller.sh` file on disk. The Terraform provider `hashicorp/cloudinit` has a data source `cloudinit_config` which knows how to construct multipart MIME messages for cloud-init, which you could use like this:\n```\n`data \"cloudinit_config\" \"example\" {\n  part {\n    content_type = \"text/x-shellscript\"\n    content      = file(\"${path.root}/deploy/templates/user-data.sh\")\n  }\n\n  part {\n    content_type = \"text/cloud-config\"\n    content = yamlencode({\n      write_files = [\n        {\n          encoding    = \"b64\"\n          content     = filebase64(\"${path.root}/deploy/templates/ec2-caller.sh\")\n          path        = \"/home/ubuntu/ec2-caller.sh\"\n          owner       = \"ubuntu:ubuntu\"\n          permissions = \"0755\"\n        },\n      ]\n    })\n  }\n}\n\nresource \"aws_instance\" \"inst1\" {\n  instance_type = \"t2.micro\"\n  ami           = data.aws_ami.ubuntu.id\n  key_name      = \"aws_key\"\n  subnet_id     = ...\n  user_data     = data.cloudinit_config.example.rendered\n\n  vpc_security_group_ids = [\n    ... ,\n  ]\n}\n`\n```\nThe second `part` block above includes YAML based on the cloud-init example Writing out arbitrary files, which you could refer to in order to learn what other settings are possible. Terraform's `yamlencode` function doesn't have a way to generate the special `!!binary` tag used in some of the files in that example, but setting `encoding: b64` allows passing the base64-encoded text as just a normal string.",
      "question_score": 3,
      "answer_score": 9,
      "created_at": "2022-05-08T10:28:10",
      "url": "https://stackoverflow.com/questions/72159273/using-terraform-to-pass-a-file-to-newly-created-ec2-instance-without-sharing-the"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-aws",
      "question_id": 71075125,
      "title": "Unable to trigger AWS Lambda from SNS",
      "problem": "I am trying to create trigger for AWS lambda from SNS codestar-notifications .\nWhile creating a trigger using Console it automatically adds subscription to the SNS topic. . Also, this works in alternate direction i.e. if I create a subscription for SNS as the Lambda function by explicitly adding its arn, it automatically links a trigger to Lambda function.\nBut when using terraform to create a subscription as below:\n```\n`resource \"aws_sns_topic_subscription\" \"subscribe_lambda_to_first_topic\" {\n  topic_arn = module.first_topic.sns-topic-detail.arn\n  protocol  = \"lambda\"\n  endpoint  = module.lambda_function.lambda_function.arn\n}\n`\n```\nit doesn't create a trigger in AWS Lambda.\nI tried creating a trigger using event source mapping in Terraform as below\n```\n`resource \"aws_lambda_event_source_mapping\" \"lambda_source\" {\n event_source_arn  = module.first_topic.sns-topic-detail.arn\n function_name     = module.lambda_function.lambda_function.arn\n starting_position = \"LATEST\"\n}\n`\n```\nit throws me an error saying it is possible only for\n\nError: error creating Lambda Event Source Mapping (arn:aws:sns:us-west-2:619867110810:codestar-notifications-emc-sns-to-lambda): InvalidParameterValueException: Unrecognized event source, must be kinesis, dynamodb stream or sqs. Unsupported source arn : arn:aws:sns:us-west-2:619867110810:codestar-notifications-emc-sns-to-lambda\n{\nRespMetadata: {\nStatusCode: 400,\nRequestID: \"83bf57cb-b50d-49a8-9547-72fac69778d1\"\n},\nMessage_: \"Unrecognized event source, must be kinesis, dynamodb stream or sqs. Unsupported source arn : arn:aws:sns:us-west-2:619867110810:codestar-notifications-emc-sns-to-lambda\",\nType: \"User\"\n}\nwith aws_lambda_event_source_mapping.lambda_source,\non main.tf line 43, in resource \"aws_lambda_event_source_mapping\" \"lambda_source\":\n43: resource \"aws_lambda_event_source_mapping\" \"lambda_source\" {",
      "solution": "`aws_lambda_event_source_mapping` is not for SNS, just like the error message says. Instead you use `aws_sns_topic_subscription` as you did.\nHowever, you forgot about aws_lambda_permission which should be (generic form from the docs - you need to adjust to your own setup):\n```\n`resource \"aws_lambda_permission\" \"with_sns\" {\n  statement_id  = \"AllowExecutionFromSNS\"\n  action        = \"lambda:InvokeFunction\"\n  function_name = aws_lambda_function.func.function_name\n  principal     = \"sns.amazonaws.com\"\n  source_arn    = aws_sns_topic.default.arn\n}\n`\n```",
      "question_score": 3,
      "answer_score": 9,
      "created_at": "2022-02-11T04:48:54",
      "url": "https://stackoverflow.com/questions/71075125/unable-to-trigger-aws-lambda-from-sns"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-aws",
      "question_id": 73526267,
      "title": "Terraform template file escape bash variable for user data",
      "problem": "I have a pretty simple user data script:\n`#!/bin/bash\n\n# Create script that will be executed as cron\ncat > generate_rds_password.sh \nWhat this does is create a script, and make it executable. It does some other things, but not relevant to the question.\nThe way I call generate this script with Terraform:\n```\n`resource \"aws_launch_template\" \"rds_bastion\" {\n  \n  user_data = base64encode(templatefile(\"${path.module}/bastion_userdata/startup.sh\", {\n    rds_host        = data.aws_db_instance.rds_instance[count.index].address\n    pgusername      = var.pgmonitor_user\n    region          = var.region\n    environment     = var.environment\n  }))\n}\n`\n```\nNow, for some reasons, I cannot echo the `$PGPASSWORD`.\nThe TF plan show me the correct command (`echo \"${PGPASSWORD}\"`), but when I cat the file from the instance, the echo is empty although the `PGPASSWORD` is correclty generated:\n```\n`export PGPASSWORD=\"xxxx.asdsadsad.eu-central-1.rds.amazonaws.com:5432/?Action=connect&DBUser=asdasd&X-Amz-Algorithm=....\"\necho \"\"\n`\n```\nWhat am I missing here?\nFrom this thread, it seems to be the correct way?",
      "solution": "So, after reading this blog post, it seems the variables are escaped differently if you create a script from a script.\nWhat worked for me is a combination of `\\$` and `\\$$`\n`cat > generate_rds_password.sh",
      "question_score": 3,
      "answer_score": 7,
      "created_at": "2022-08-29T10:48:58",
      "url": "https://stackoverflow.com/questions/73526267/terraform-template-file-escape-bash-variable-for-user-data"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-aws",
      "question_id": 66858898,
      "title": "terraform init step with &quot;no available releases match the given constraints ~&gt; 2.1.28&quot;",
      "problem": "Actually - received the replaced apple mac laptop (the original one crashed due to bad storage) and i am re-setting up everything with configure, i am stuck and blocked with this \"terraform init\" step,\nInstalled terraform version is 0.13.6\nInstalled AWS CLI is 2.1.32\nWhen \"terraform init\" is triggered i am getting this below error, any help is appreciated so that i can unblock my work.\n```\n`Initializing modules...\n\nInitializing the backend...\n\nInitializing provider plugins...\n- Using previously-installed hashicorp/kubernetes v1.13.3\n- Using previously-installed hashicorp/external v1.1.2\n- Finding hashicorp/aws versions matching \"~> 2.1.28\"...\n\nError: Failed to query available provider packages\n\nCould not retrieve the list of available versions for provider hashicorp/aws: no available releases match the given constraints ~>\n2.1.28\n`\n```",
      "solution": "create a config.tf file and add this line\n```\n`  required_providers {\n    aws = {\n      source  = \"registry.terraform.io/hashicorp/aws\"\n      version = \"=2.28.0\" ## or whatever version you need\n    }\n  }\n`\n```",
      "question_score": 3,
      "answer_score": 2,
      "created_at": "2021-03-29T19:33:01",
      "url": "https://stackoverflow.com/questions/66858898/terraform-init-step-with-no-available-releases-match-the-given-constraints-2"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-aws",
      "question_id": 71733644,
      "title": "Share common variables among modules",
      "problem": "We have a common set of variables in our infrastructure on AWS, planned to be used by several modules. For example subnet ids, vpc id  and so on.\nI want to avoid duplicating these variables through each module's *.tfvars files. Is that possible to make them available to all modules, while keeping the modules isolated from each other?\nI think about kind of core module, which can be imported from everywhere. But I am not sure if a module is the right way, as modules are intended to have only resources in them, and I want to simply expose a few variables instead. Is it the right way to use modules to share variables? Or how you guys cope with this problem? Think it's common or it's bad approach in terraform?",
      "solution": "If you have a set of expressions (including hard-coded literal values) that you want to reuse then it is valid to write a module which only contains input variable declarations, local values, and output values as a way to model that.\nThe simplest form of this would be a module that only contains `output` blocks whose values are hard-coded literal values, like this:\n```\n`output \"example\" {\n  value = \"result\"\n}\n`\n```\nThe official module `hashicorp/subnets/cidr` is an example of that: it doesn't declare any resources of its own, and instead it just encapsulates some logic for calculating a set of subnets based on some input variables.\nThis is a special case of Data-only Modules where the data comes from inside the module itself, rather than from data sources. A nice thing about modelling shared data in this way is that if you later decide to calculate those results automatically based on data sources then you'll be able to do so, while keeping the details encapsulated. For example, if you define a module which takes an environment name as an input variable and returns derived settings about that environment, the module could contain local logic to calculate those results today but could later determine some of those settings by fetching them from a prescribed remote location, such as AWS SSM Parameter store, if the need arises.",
      "question_score": 3,
      "answer_score": 6,
      "created_at": "2022-04-04T09:46:06",
      "url": "https://stackoverflow.com/questions/71733644/share-common-variables-among-modules"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-aws",
      "question_id": 69352118,
      "title": "How to securely allow access to AWS Secrets Manager with Terraform and cloud-init",
      "problem": "I have the situation whereby I am having Terraform create a random password and store it into AWS Secrets Manager.\nMy Terraform password and secrets manager config:\n```\n`resource \"random_password\" \"my_password\" {\n  length = 16\n  lower = true\n  upper = true\n  number = true\n  special = true\n  override_special = \"@#$%\"\n}\n\nresource \"aws_secretsmanager_secret\" \"my_password_secret\" {\n  name = \"/development/my_password\"\n}\n\nresource \"aws_secretsmanager_secret_version\" \"my_password_secret_version\" {\n  secret_id     = aws_secretsmanager_secret.my_password_secret.id\n  secret_string = random_password.my_password.result\n}\n`\n```\nThe above works well. However I am not clear on how to achieve my final goal...\nI have an AWS EC2 Instance which is also configured via Terraform, when the system boots it executes some cloud-init config which runs a setup script (Bash script). The Bash setup script needs to install some server software and set a password for that server software. I am not certain how to securely access `my_password` from that Bash script during setup.\nMy Terraform config for the instance and cloud-init config:\n```\n`resource \"aws_instance\" \"my_instance_1\" {\n  ami           = data.aws_ami.amazon_linux_2.id\n  instance_type = \"m5a.2xlarge\"\n\n  user_data = data.cloudinit_config.my_instance_1.rendered\n\n  ...\n\n}\n\ndata \"cloudinit_config\" \"my_instance_1\" {\n  gzip = true\n  base64_encode = true\n\n  part {\n    content_type = \"text/x-shellscript\"\n    filename = \"setup-script.sh\"\n    content = `  # TODO retrieve via cURL call to Secrets Manager API?\nserver_password=$my_password /opt/srv/bin/install.sh\nEOF\n  }\n}\n`\n```\nI need to be able to securely retrieve the password from the AWS Secrets Manager when the cloud-init script runs, as I have read that embedding it in the bash script is considered insecure.\nI have also read that AWS has the notion of Temporary Credentials, and that these can be associated with an EC2 instance - https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp.html\nUsing Terraform can I create temporary credentials (say 10 minutes TTL) and grant them to my AWS EC2 instance, so that when my Bash script runs during cloud-init it can retrieve the password from the AWS Secrets Manager?\nI have seen that on the Terraform `aws_instance` resource, I can associate a `iam_instance_profile` and I have started by trying something like:\n```\n`resource \"aws_iam_instance_profile\" \"my_instance_iam_instance_profile\" {\n  name = \"my_instance_iam_instance_profile\"\n  path = \"/development/\"\n  \n  role = aws_iam_role.my_instance_iam_role.name\n  \n  tags = {\n    Environment = \"dev\"\n  }\n}\n\nresource \"aws_iam_role\" \"my_instance_iam_role\" {\n  name = \"my_instance_iam_role\"\n  path = \"/development/\"\n\n  // TODO - what how to specify a temporary credential access to a specific secret in AWS Secrets Manager from EC2???\n\n  tags = {\n    Environment = \"dev\"\n  }\n}\n\nresource \"aws_instance\" \"my_instance_1\" {\n  ami           = data.aws_ami.amazon_linux_2.id\n  instance_type = \"m5a.2xlarge\"\n\n  user_data = data.cloudinit_config.my_instance_1.rendered\n\n  iam_instance_profile = join(\"\", [aws_iam_instance_profile.my_instance_iam_instance_profile.path, aws_iam_instance_profile.my_instance_iam_instance_profile.name])\n\n  ...\n\n}\n`\n```\nUnfortunately I can't seem to find any details on what I should put in the Terraform `aws_iam_role` which would allow my EC2 instance to access the Secret in the AWS Secrets Manager for a temporary period of time.\nCan anyone advise? I would also be open to alternative approaches as long as they are also secure.\nThanks",
      "solution": "You can create aws_iam_policy or an inline policy which can allow access to certain SSM parameters based on date and time.\nIn case of inline policy, this can be attached to the instance role which would look something like this:\n```\n`resource \"aws_iam_role\" \"my_instance_iam_role\" {\n  name = \"my_instance_iam_role\"\n  path = \"/development/\"\n\n  inline_policy {\n    name = \"my_inline_policy\"\n\n    policy = jsonencode({\n       \"Version\": \"2012-10-17\",\n       \"Statement\": [{\n           \"Effect\": \"Allow\",\n           \"Action\": \"ssm:GetParameters\",\n           \"Resource\": \"arn:aws:ssm:us-east-2:123456789012:parameter/development-*\",\n           \"Condition\": {\n               \"DateGreaterThan\": {\"aws:CurrentTime\": \"2020-04-01T00:00:00Z\"},\n               \"DateLessThan\": {\"aws:CurrentTime\": \"2020-06-30T23:59:59Z\"}\n           }\n       }]\n    })\n  }\n  tags = {\n    Environment = \"dev\"\n  }\n}\n`\n```",
      "question_score": 3,
      "answer_score": 4,
      "created_at": "2021-09-27T21:14:05",
      "url": "https://stackoverflow.com/questions/69352118/how-to-securely-allow-access-to-aws-secrets-manager-with-terraform-and-cloud-ini"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-aws",
      "question_id": 70574190,
      "title": "Allow lambda permission to access secretsmanager value",
      "problem": "I'm using Terraform to deploy a lambda that needs to keep secrets in the AWS SecretsManager.\nI have the following abbreviated lambda:\nLambda\n```\n`\nresource \"aws_lambda_function\" \"thisThing\" {\n  function_name = \"functionName\"\n  runtime = \"python3.8\"\n  handler = \"thisThing.handler\"\n\n  role = aws_iam_role.lambda_exec.arn\n}\n\nresource \"aws_iam_role\" \"lambda_exec\" {\n  name = \"serverless_lambda\"\n\n  assume_role_policy = jsonencode({\n    Version = \"2012-10-17\"\n    Statement = [{\n      Action = \"sts:AssumeRole\"\n      Effect = \"Allow\"\n      Sid    = \"\"\n      Principal = {\n        Service = \"lambda.amazonaws.com\"\n      }\n      }\n    ]\n  })\n}\n\nresource \"aws_iam_role_policy_attachment\" \"lambda_policy\" {\n  role       = aws_iam_role.lambda_exec.name\n  policy_arn = \"arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole\"\n}\n`\n```\nHere are the secrets\nSecrets\n```\n`# Secrets\n\nresource \"aws_secretsmanager_secret\" \"SECRET\" {\n  name = \"SECRET\"\n  recovery_window_in_days = 0\n}\n\nresource \"aws_secretsmanager_secret_version\" \"SECRET\" {\n  secret_id     = \"${aws_secretsmanager_secret.SECRET.id}\"\n  secret_string = \"${var.SECRET}\"\n}\n`\n```\nThe error I'm getting is:\n`[ERROR] ClientError: An error occurred (AccessDeniedException) when calling the GetSecretValue operation: User: arn:aws:sts::439791110569:assumed-role/serverless_lambda/thisThing is not authorized to perform: secretsmanager:GetSecretValue on resource: SECRET because no identity-based policy allows the secretsmanager:GetSecretValue action`\nThis is my first time using secrets manager, and I'm not very experienced in AWS, but I think based on the answer here, that I need to add a policy that allows my lambda exec role to have GetSecretValue rights.  I've made a few attempts, but my lack of knowledge on how to look up the different policy ARN's is shutting me down.\nHere's what I've tried adding (it's wrong, and I know it's wrong.)\n```\n`resource \"aws_iam_role_policy_attachment\" \"lambda_secretsmanager_role\" {\n  role = aws_iam_role.lambda_exec.name\n  # ? policy_arn = \"arn:aws:iam::aws:policy/SecretsManagerGetSecretValue\"\n}\n`\n```\nThat's not the correct ARN, but I'm not sure where to look to find the correct ARN.",
      "solution": "You can add the permission using aws_iam_role_policy:\n```\n`resource \"aws_iam_role_policy\" \"sm_policy\" {\n  name = \"sm_access_permissions\"\n  role = aws_iam_role.lambda_exec.id\n\n  policy = jsonencode({\n    Version = \"2012-10-17\"\n    Statement = [\n      {\n        Action = [\n          \"secretsmanager:GetSecretValue\",\n        ]\n        Effect   = \"Allow\"\n        Resource = \"*\"\n      },\n    ]\n  })\n}\n`\n```\nIf you want to follow least privileged permissions, then you can change `Resource = \"*\"` into `Resource = \"\"`.",
      "question_score": 3,
      "answer_score": 5,
      "created_at": "2022-01-04T05:46:42",
      "url": "https://stackoverflow.com/questions/70574190/allow-lambda-permission-to-access-secretsmanager-value"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-aws",
      "question_id": 69309238,
      "title": "unable to access EC2 instance created using terraform",
      "problem": "I have been following youtube guide to learn terraform and have followed each steps.\nAfter running terraform apply it was able to setup everything as expected. I have verified this on aws console. But while trying to access the public ip it is saying connection refused.\nBelow is the content of my main.tf file.\n```\n`provider \"aws\" {\n  region     = \"us-east-1\"\n  access_key = \"ACCESS-KEY\"\n  secret_key = \"SECERT-KEY\"\n}\n\n# VPC\nresource \"aws_vpc\" \"prod-vpc\" {\n  cidr_block = \"10.0.0.0/16\"\n  tags = {\n      Name = \"production\"\n  }\n}\n\n# create internet gateway \nresource \"aws_internet_gateway\" \"gw\" {\n  vpc_id = aws_vpc.prod-vpc.id\n  tags = {\n    Name : \"Prod gateway\"\n  }\n}\n\n# create custom route table \n\nresource \"aws_route_table\" \"prod-route-table\" {\n  vpc_id = aws_vpc.prod-vpc.id\n\n  route {\n      cidr_block = \"0.0.0.0/0\"\n      gateway_id = aws_internet_gateway.gw.id\n  }\n\n  route {\n      ipv6_cidr_block        = \"::/0\"\n      gateway_id = aws_internet_gateway.gw.id\n  }\n\n  tags = {\n    Name = \"Prod\"\n  }\n}\n\n# Create a subnet \n\nresource \"aws_subnet\" \"subnet-1\" {\n    vpc_id = aws_vpc.prod-vpc.id\n    cidr_block = \"10.0.1.0/24\"\n    availability_zone = \"us-east-1a\"\n    map_public_ip_on_launch = true\n\n    tags = {\n        Name = \"prod-subnet\"\n    }\n}\n\n# Associate subnet with Route Table \n\nresource \"aws_route_table_association\" \"a\" {\n  subnet_id      = aws_subnet.subnet-1.id\n  route_table_id = aws_route_table.prod-route-table.id\n}\n\n# Create Security Group to allow port 22, 80, 443\n\nresource \"aws_security_group\" \"allow_web\" {\n  name        = \"allow_web_traffic\"\n  description = \"Allow Web traffic\"\n  vpc_id      = aws_vpc.prod-vpc.id\n\n  ingress {\n      description      = \"HTTPS\"\n      from_port        = 443\n      to_port          = 443\n      protocol         = \"tcp\"\n      cidr_blocks      = [\"0.0.0.0/0\"]\n    }\n\n  ingress {\n      description      = \"HTTP\"\n      from_port        = 80\n      to_port          = 80\n      protocol         = \"tcp\"\n      cidr_blocks      = [\"0.0.0.0/0\"]\n    }\n  ingress {\n      description      = \"SSH\"\n      from_port        = 2\n      to_port          = 2\n      protocol         = \"tcp\"\n      cidr_blocks      = [\"0.0.0.0/0\"]\n    }\n\n  egress {\n      from_port        = 0\n      to_port          = 0\n      protocol         = \"-1\"\n      cidr_blocks      = [\"0.0.0.0/0\"]\n      ipv6_cidr_blocks = [\"::/0\"]\n    }\n\n  tags = {\n    Name = \"allow_web\"\n  }\n}\n\n# Create a network interface with an ip in the subnet that was created earlier \n\nresource \"aws_network_interface\" \"web-server-nic\" {\n  subnet_id       = aws_subnet.subnet-1.id\n  private_ips     = [\"10.0.1.50\"]\n  security_groups = [aws_security_group.allow_web.id]\n\n  tags = {\n    Name : \"prod-network-interface\"\n  }\n}\n\n# Assign an elastic ip to the network interface created in previous step\n\nresource \"aws_eip\" \"one\" {\n  vpc                       = true\n  network_interface         = aws_network_interface.web-server-nic.id\n  associate_with_private_ip = \"10.0.1.50\"\n  depends_on = [aws_internet_gateway.gw, aws_instance.web-server-instance]\n\n  tags = {\n    Name : \"Prod-Elastic-ip\"\n  }\n}\n\n# Create Ubuntu server and install/enable apache2\n\nresource \"aws_instance\" \"web-server-instance\" {\n    ami = \"ami-0747bdcabd34c712a\"\n    instance_type = \"t2.micro\"\n    availability_zone = \"us-east-1a\"\n    key_name = \"main-key\"\n\n    network_interface {\n        device_index = 0\n        network_interface_id = aws_network_interface.web-server-nic.id\n    }\n\n    user_data =  /var/www/html/index.html'\n        EOF\n\n    tags = {\n      Name : \"Web-Server\"\n    }    \n}\n`\n```",
      "solution": "You are missing `-y` in your user data, so your user-data will just hang for confirmation. It should be:\n```\n`sudo apt install -y apache2\n`\n```",
      "question_score": 3,
      "answer_score": 3,
      "created_at": "2021-09-24T05:36:39",
      "url": "https://stackoverflow.com/questions/69309238/unable-to-access-ec2-instance-created-using-terraform"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-aws",
      "question_id": 69162625,
      "title": "Could not refresh state. Known issue, and known workaround is not working",
      "problem": "So, if you have been working for some time with Terraform, probably you have faced this error message more than once:\n```\n`Initializing the backend...\n\nSuccessfully configured the backend \"s3\"! Terraform will automatically\nuse this backend unless the backend configuration changes.\nError refreshing state: state data in S3 does not have the expected content.\n\nThis may be caused by unusually long delays in S3 processing a previous state\nupdate.  Please wait for a minute or two and try again. If this problem\npersists, and neither S3 nor DynamoDB are experiencing an outage, you may need\nto manually verify the remote state and update the Digest value stored in the\nDynamoDB table to the following value: \n`\n```\nUsually, this is caused because something was screwed up during a destroy operation, and now there is a mismatch between the state and the lock.\nThe known solution for this is to delete the lock from DynamoDB and run terraform init again. And if it didn't resolve with that, also delete the tfstate from S3, which at this point doesn't have any data as the infrastructure was destroyed.\nSurprisingly, neither is working now, and I don't have a clue why. There is no tfstate in the bucket, in fact, I even deleted every old version stored (bucket has versioning enabled). There is no lock in DynamoDB either.\nChanging the tfstate name works without issues, but I can't change it as it would break the naming convention I'm using for all my tfstates.\nSo, any ideas what's going on here? On Friday, the infrastructure was deployed and destroyed without issues (as part of the destroy, I always check there is no lock left behind and delete the tfstate from S3). But today, I'm facing this error, and it's been a while already and can't figure it out..",
      "solution": "Holy ****!\nSo, it turns out DynamoDB was being a serious troll here.\nI searched for the key of the md5 of this tfstate, and it wasn't returned. But then I noticed there was a message about that there were more items to be returned... After clicking like 6 times on that button, it eventually returned a hidden lock for this tfstate.\nDeleted it, and everything is back to normal again.\nSo, as summary, if you ever face this issue and you can't find the lock on DynamoDB... be sure that all items are returned in the query, as it can take many attempts to return them all.",
      "question_score": 3,
      "answer_score": 5,
      "created_at": "2021-09-13T14:22:57",
      "url": "https://stackoverflow.com/questions/69162625/could-not-refresh-state-known-issue-and-known-workaround-is-not-working"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-aws",
      "question_id": 66803123,
      "title": "Terraform interpolation of locals map with key defined in a variable",
      "problem": "Terraform interpolation of locals map with key defined in a variable\nObjective:\nDefine preset sizing labels in variable, provisioning of resources uses preset values from locals map variable\n```\n`\nvar \"define_size\" {\n  description = \"Select either small, medium, large\"\n  default = \"large\"\n}\n\nlocals {\n\n  small = {\n    volume_gb = 1\n    volume_count = 1\n  }\n\n  medium = {\n    volume_gb = 20\n    volume_count = 5\n  }\n\n  large = {\n    volume_gb = 500\n    volume_count = 10\n  }\n\n}\n\nresource \"aws_ebs_volume\" \"example\" {\n  availability_zone = var.availability_zone\n  size              = ??????\n}\n`\n```\nAttempts:\n\n`size = local.$var.define_size.volume_gb`. Obvious bad syntax results in \"Error: Invalid character.\" and \"Error: Invalid attribute name\" referring to the $ character.\n`size = local.${var.define_size}.volume_gb`. Obvious bad syntax results in \"Error: Invalid character.\" and \"Error: Invalid attribute name\" referring to the $ character.\n`size = \"${local[var.define_size].volume_gb}\"`. \"Error: Invalid reference. A reference to a resource type must be followed by at least one attribute access, specifying the resource name.\"\n`size = tostring(\"local.${var.define_size}.volume_gb\")` this renders correctly but as a string and not a resource reference `\"local.large.volume_gb\"`\n`format(\"%#v\",tostring(\"local.${var.define_size}.volume_gb\"))` this renders partially correctly but as string with escape characters and not resource `\"\\\"local.large.volume_gb\\\"\"`",
      "solution": "If you want key-based access you should make the `locals` definition something that works with keys, e.g. a `map`:\n```\n`locals {\n  sizes = {\n    small = {\n      volume_gb = 1\n      volume_count = 1\n    }\n    medium = {\n      volume_gb = 20\n      volume_count = 5\n    }\n    large = {\n      volume_gb = 500\n      volume_count = 10\n    }\n  }\n}\n`\n```\nand then access that using `local.sizes[var.define_size].volume_gb`",
      "question_score": 3,
      "answer_score": 5,
      "created_at": "2021-03-25T16:52:53",
      "url": "https://stackoverflow.com/questions/66803123/terraform-interpolation-of-locals-map-with-key-defined-in-a-variable"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-aws",
      "question_id": 66585040,
      "title": "Whats the right way to create multiple elements using terraform variables?",
      "problem": "I am creating AWS SQS queues using Terraform. for each service, i need to create two queues, one normal queue and one error queue. The settings for each are mostly the same, but i need to create the error queue first so i can pass its ARN to the normal queue as part of its redrive policy. Instead of creating 10 modules there has to be a better way to loop through replacing just the names. So programming logic... foreach queue in queue_prefixes, create error module, then regular module. Im sure im just not searching right or asking the right question.\n\nsandbox/main.tf\n```\n`provider \"aws\" {\n  region = \"us-west-2\"\n}\n\nmodule \"hfd_sqs_error_sandbox\" {\n    source = \"../\"\n    for_each = var.queue_prefixes\n    name= each.key+\"_Error\"\n}\n\nmodule \"hfd_sqs_sandbox\" {\n    source = \"../\"\n\n    name=hfd_sqs_error_sandbox.name\n\n    redrive_policy = jsonencode({\n    deadLetterTargetArn = hfd_sqs_error_sandbox_this_sqs_queue_arn,\n    maxReceiveCount     = 3\n  })\n}\n`\n```\nvariables.tf\n```\n`variable \"queue_prefixes\" {\n  description = \"Create these queues with the enviroment prefixed\"\n  type = list(string)\n  default = [\n    \"Clops\",\n    \"Document\",\n    \"Ledger\",\n    \"Log\",\n    \"Underwriting\",\n    \"Wallet\",\n  ]\n}\n`\n```",
      "solution": "You may want to consider adding a wrapper module that creates both Normal Queue and Dead-Letter Queue. That would make creating resources in order much easier.\nConsider this example (with null resources for easy testing):\nRoot module creating all queues:\n```\n`# ./main.tf\n\nlocals {\n  queue_prefixes = [\n      \"Queue_Prefix_1\",\n      \"Queue_Prefix_2\",\n  ]\n}\n\nmodule queue_set {\n  source = \"./modules/queue_set\"\n\n  for_each = toset(local.queue_prefixes)\n\n  name = each.key\n}\n`\n```\nWrapper module creating a set of 2 queues: normal + dlq:\n```\n`# ./modules/queue_set/main.tf\n\nvariable \"name\" {\n  type = string\n}\n\nmodule dlq {\n  source = \"../queue\"\n\n  name = \"${var.name}_Error\"\n}\n\nmodule queue {\n  source = \"../queue\"\n\n  name = var.name\n  redrive_policy = module.dlq.id\n}\n`\n```\nIndividual queue resource suitable to create both types of queues:\n```\n`# ./modules/queue/main.tf\n\nvariable \"name\" {\n  type = string\n}\n\nvariable \"redrive_policy\" {\n  type = string\n  default =  \"\"\n}\n\nresource \"null_resource\" \"queue\" {\n  provisioner \"local-exec\" {\n    command = \"echo \\\"Created queue ${var.name}, redrive policy: ${var.redrive_policy}\\\"\"\n  }\n\n  # this is irrelevant to the question, it's just to make null resource change every time\n  triggers = {\n    always_run = timestamp()\n  }\n}\n\noutput \"id\" {\n  value = null_resource.queue.id\n}\n`\n```\nNow if we run this stack, we can see the resources created in the correct order:",
      "question_score": 3,
      "answer_score": 5,
      "created_at": "2021-03-11T15:57:07",
      "url": "https://stackoverflow.com/questions/66585040/whats-the-right-way-to-create-multiple-elements-using-terraform-variables"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-azure",
      "question_id": 71307245,
      "title": "Terraform source error - Error: Failed to query available provider packages",
      "problem": "I am trying to deploy a new infrastructure using terraform (for the first time) and I am getting the following error. I've tried everything but nothing seems to fix the issue.\nLooks like it is asking for a provider hashicorp/azure ?\nCan anyone help please??\n```\n`Initializing provider plugins...\n- Finding latest version of hashicorp/azure...\n- Finding hashicorp/azurerm versions matching \"2.98.0\"...\n- Installing hashicorp/azurerm v2.98.0...\n- Installed hashicorp/azurerm v2.98.0 (signed by HashiCorp)\n\u2577\n\u2502 Error: Failed to query available provider packages\n\u2502 \n\u2502 Could not retrieve the list of available versions for provider hashicorp/azure: provider registry registry.terraform.io does not have a provider named registry.terraform.io/hashicorp/azure\n\u2502 \n\u2502 Did you intend to use terraform-providers/azure? If so, you must specify that source address in each module which requires that provider. To see which modules are currently depending on hashicorp/azure, run the following command:\n\u2502     terraform providers\n\u2575\n\nlucas@Azure:~$ terraform providers\n\nProviders required by configuration:\n.\n\u251c\u2500\u2500 provider[registry.terraform.io/hashicorp/azurerm] 2.98.0\n\u2514\u2500\u2500 provider[registry.terraform.io/hashicorp/azure]\n`\n```\nThe code that I am using to create the infrastructure is the below:\n```\n`  required_providers {\n    azurerm = {\n      source  = \"hashicorp/azurerm\"\n      version = \"=2.98.0\"\n    }\n  }\n}\n\nprovider \"azurerm\" {\n  features {}\n  subscription_id = \"910910be-a61e-4e1f-a72a-7e43456c0836\"\n}\n\n# Create a resource group\nresource \"azurerm_resource_group\" \"rg\" {\n  name     = \"default\"\n  location = \"West Europe\"\n}\n\n# Create a virtual network\nresource \"azurerm_virtual_network\" \"vpc\" {\n  name                = \"default-network\"\n  resource_group_name = azurerm_resource_group.rg.name\n  location            = azurerm_resource_group.rg.location\n  address_space       = [\"10.0.0.0/16\"]\n}\n\n# Create frontend subnet\nresource \"azurerm_subnet\" \"subnet_frontend\" {\n  name                 = \"internal\"\n  resource_group_name  = azurerm_resource_group.rg.name\n  virtual_network_name = azurerm_virtual_network.vpc.name\n  address_prefixes     = [\"10.0.1.0/24\"]\n}\n\n# Create backend subnet\nresource \"azurerm_subnet\" \"subnet_backend\" {\n  name                 = \"internal\"\n  resource_group_name  = azurerm_resource_group.rg.name\n  virtual_network_name = azurerm_virtual_network.vpc.name\n  address_prefixes     = [\"10.0.2.0/24\"]\n}\n\n# Create frontend network interface\nresource \"azurerm_network_interface\" \"frontend_nic\" {\n  name                = \"frontend_nic\"\n  location            = azurerm_resource_group.rg.location\n  resource_group_name = azurerm_resource_group.rg.name\n\n  ip_configuration {\n    name                          = \"internal\"\n    subnet_id                     = azurerm_subnet.subnet_frontend.id\n    private_ip_address_allocation = \"Dynamic\"\n  }\n}\n\n# Create backend network interface\nresource \"azurerm_network_interface\" \"backend_nic\" {\n  name                = \"backend_nic\"\n  location            = azurerm_resource_group.rg.location\n  resource_group_name = azurerm_resource_group.rg.name\n\n  ip_configuration {\n    name                          = \"internal\"\n    subnet_id                     = azurerm_subnet.subnet_backend.id\n    private_ip_address_allocation = \"Dynamic\"\n  }\n}\n\n# Create frontend VM based on module\nresource \"azure_instance\" \"frontend\" {\n  source   = \"./vm\"\n  name     = \"frontend\"\n  rg       = module.azurerm_resource_group.rg.name\n  location = module.azurerm_resource_group.rg.location\n  nic      = module.azurerm_network_interface.frontend_nic\n}\n\n# Create backend VM based on module\nresource \"azure_instance\" \"backend\" {\n  source   = \"./vm\"\n  name     = \"backend\"\n  rg       = module.azurerm_resource_group.rg.name\n  location = module.azurerm_resource_group.rg.location\n  nic      = module.azurerm_network_interface.backend_nic\n} \n`\n```\nMy terraform version is: Terraform v1.1.5 and I am using on Azure CLI via bash.\nAny idea of what is causing this issue and how to fix it?\nThanks!",
      "solution": "This often happens when one accidentally specifies \"hashicorp/azure\" or \"hashicorp/azurem\" instead of \"hashicorp/azurerm\" in the required_providers block. Did you check the \"vm\" module referenced in the \"azure_instance\" module calls? There might be an erroneous \"hashicorp/azure\" specified there.",
      "question_score": 8,
      "answer_score": 10,
      "created_at": "2022-03-01T11:44:26",
      "url": "https://stackoverflow.com/questions/71307245/terraform-source-error-error-failed-to-query-available-provider-packages"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-azure",
      "question_id": 68957371,
      "title": "How to run multiple commands using terraform local-exec",
      "problem": "I am trying to run a few az cli commands using terraform using local-exec provisioner but I keep running into an error that says:\n```\n`Error: Invalid expression\n\nOn modules/eventgrid/main.tf line 68: Expected the start of an expression, but\nfound an invalid expression token.\n\n`\n```\nHere's my code:\n```\n`resource \"null_resource\" \"eg-role-assignment\" {\n  provisioner \"local-exec\" {\n    \n    interpreter = [\"/bin/bash\", \"-c\"]\n    command = Can anybody please guide me as to what's wrong ?",
      "solution": "With your `\nFinally, as the cause of the issue, you have a space after `EOT`.\n```\n`command = <<-EOT\n          az account set --subscription foo\n          az eventgrid topic update --resource-group $RESOURCE_GROUP --name $EVENTGRID_NAME --identity systemassigned\nEOT\n`\n```",
      "question_score": 7,
      "answer_score": 15,
      "created_at": "2021-08-27T19:30:58",
      "url": "https://stackoverflow.com/questions/68957371/how-to-run-multiple-commands-using-terraform-local-exec"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-azure",
      "question_id": 67614631,
      "title": "Terraform : How to ignore the Optional block if the Property - list of string is Empty",
      "problem": "I'm just starting with the terraform, The below is my terraform code. From the below Cors is an Optional block, where it has the property of allowed origins which is list of strings(URL or *)\n```\n`resource \"azurerm_signalr_service\" \"signalr_service\" {\n  \n  name=\"${var.signalr_name}\"\n  location = \"${var.resource_location}\"\n  resource_group_name = \"${var.resource_group_name}\"\n\n  sku {\n    name     = \"${var.sku_name}\"\n    capacity = \"${var.sku_capacity}\"\n  }\n        \n#Cors is an optional block\n  cors {\n    allowed_origins = \"${var.cors_allowed_origins}\"\n  }\n`\n```\nVarilabe.tf:\n```\n`variable \"allowed_origins\" {\n  type = \"list\"\n  description = \"A list of origins which should be able to make cross-origin calls. * can be used to allow all calls\"\n  default = []\n}\n`\n```\nUsers may/maynot provide the `allowed_origns` , if they provide there is no problem, but if they don't provide\n\nDefault as an empty list[]: when I pass the empty list the `allowed_origns`  it fails saying that not a valid URL\nDefault as an `null`: If I pass as null, `allowed_origin` alone gets null and cors is passing as an empty block i.e,`cors{}` which also fails due to property missing error\n\nNow my question is how to make the entire cors block to ignore if the user doesn't provide any values to the `allowed_origins` and what would be the default I should use ?",
      "solution": "You can use dynamic blocks to make CORS block optional:\n```\n`resource \"azurerm_signalr_service\" \"signalr_service\" {\n  \n  name=\"${var.signalr_name}\"\n  location = \"${var.resource_location}\"\n  resource_group_name = \"${var.resource_group_name}\"\n\n  sku {\n    name     = \"${var.sku_name}\"\n    capacity = \"${var.sku_capacity}\"\n  }\n        \n  dynamic \"cors\" {\n    for_each = length(var.cors_allowed_origins) > 0 ? [1] : []\n    content {\n        allowed_origins = \"${var.cors_allowed_origins}\"\n    }\n  }\n}\n`\n```\nDefault value of `[]` is fine.",
      "question_score": 7,
      "answer_score": 10,
      "created_at": "2021-05-20T07:47:45",
      "url": "https://stackoverflow.com/questions/67614631/terraform-how-to-ignore-the-optional-block-if-the-property-list-of-string-is"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-azure",
      "question_id": 67807522,
      "title": "Terraform reports error &quot;Failed to query available provider packages&quot;",
      "problem": "I have created main.tf file as below for Mongodb terraform module.\n```\n`resource \"mongodbatlas_teams\" \"test\" {\n  org_id     = null\n  name       = \"MVPAdmin_Team\"\n  usernames  = [\"user1@email.com\", \"user2@email.com\", \"user3@email.com\"]\n}\n\nresource \"mongodbatlas_project\" \"test\" {\n  name   = \"MVP_Project\"\n  org_id = null\n\n  teams {\n    team_id    = null\n    role_names = [\"GROUP_CLUSTER_MANAGER\"]\n\n  }\n  \n}\nresource \"mongodbatlas_project_ip_access_list\" \"test\" {\n  project_id = null\n  ip_address = null\n  comment    = \"IP address for MVP Dev cluster testing\"\n}\n\nresource \"mongodbatlas_cluster\" \"test\" {\n  name                = \"MVP_DevCluster\"\n  location            = azurerm_resource_group.example.location\n  resource_group_name = azurerm_resource_group.example.name\n  cluster_type        = REPLICASET\n  state_name          = var.state_name\n  replication specs {\n     num_shards= var.num_shards\n     region_config {\n       region_name = \"AU-EA\"\n       electable_nodes = var.electable_nodes\n       priority        = var.priority\n       read_only_nodes = var.read_only_nodes\n     }  \n  }\n\n  provider_backup_enabled = var.provider_backup_enabled\n  auto_scaling_disk_gb_enabled = var.auto_scaling_disk_gb_enabled\n  mongo_db_major_version = var.mongo_db_major_version\n  provider_name = \"Azure\"\n  provider_disk_type_name = var.provider_disk_type_name\n  provider_instance_size_name = var.provider_instance_size_name\n\n  mongodbatlas_database_user {\n    username = var.username\n    password = var.password\n    auth_database_name = var.auth_database_name\n    role_name = var.role_name\n    database_name = var.database_name\n  }\n  mongodbatlas_database_snapshot_backup_policy {\n    policy_item = var.policy_item\n    frequency_type = var.frequency_type\n    retention_value = var.retention_value\n  \n }\n\n advanced_configuration {\n      minimum_enabled_tls_protocol = var.minimum_enabled_tls_protocol\n   no_table_scan                  = var.no_table_scan\n   connection_string              = var.connection_string\n\n } \n}\n`\n```\nHowever, terraform init reports as below:\n```\n`$ terraform init\n\nInitializing the backend...\n\nInitializing provider plugins...\n- Finding latest version of hashicorp/mongodbatlas...\n\nError: Failed to query available provider packages\n\nCould not retrieve the list of available versions for provider\nhashicorp/mongodbatlas: provider registry registry.terraform.io does not have\na provider named registry.terraform.io/hashicorp/mongodbatlas\n\nIf you have just upgraded directly from Terraform v0.12 to Terraform v0.14\nthen please upgrade to Terraform v0.13 first and follow the upgrade guide for\nthat release, which might help you address this problem.\n\nDid you intend to use mongodb/mongodbatlas? If so, you must specify that\nsource address in each module which requires that provider. To see which\nmodules are currently depending on hashicorp/mongodbatlas, run the following\ncommand:\n    terraform providers \n\n`\n```\nAny idea as to what is going wrong?",
      "solution": "The error message explains the most likely reason for seeing this error message: you've upgraded directly from Terraform v0.12 to Terraform v0.14 without running through the Terraform v0.13 upgrade steps.\nIf you upgrade to Terraform v0.13 first and follow those instructions then the upgrade tool should be able to give more specific instructions on what to change here, and may even be able to automatically upgrade your configuration for you.\nHowever, if you wish then you can alternatively manually add the configuration block that the v0.13 upgrade tool would've inserted, to specify that you intend to use the `mongodb/mongodbatlas` provider as \"mongodbatlas\" in this module:\n```\n`terraform {\n  required_providers {\n    mongodbatlas = {\n      source = \"mongodb/mongodbatlas\"\n    }\n  }\n}\n`\n```\nThere are some other considerations in the v0.13 upgrade guide that the above doesn't address, so you may still need to perform the steps described in that upgrade guide if you see different error messages after trying what I showed above.",
      "question_score": 7,
      "answer_score": 8,
      "created_at": "2021-06-02T16:52:09",
      "url": "https://stackoverflow.com/questions/67807522/terraform-reports-error-failed-to-query-available-provider-packages"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-azure",
      "question_id": 72988987,
      "title": "Principals of type Application cannot validly be used in role assignments",
      "problem": "I am deploying a new App Registration via Terraform and then assigning a Role in my Event Hub to that App Registration.\nE.G.\nDeploy App Registration\n```\n`data \"azuread_client_config\" \"current\" {}\n\nresource \"azuread_application\" \"eventhub_auth\" {\n  display_name = \"AppReg\"\n  sign_in_audience = \"AzureADMyOrg\"\n  owners           = [data.azuread_client_config.current.object_id]\n\n    app_role {\n    allowed_member_types = [\"User\", \"Application\"]\n    description          = \"Admins can manage roles and perform all task actions\"\n    display_name         = \"Admin\"\n    enabled              = true\n    id                   = uuid()\n    value                = \"admin\"\n  }\n\n  app_role {\n    allowed_member_types = [\"User\"]\n    description          = \"ReadOnly roles have limited query access\"\n    display_name         = \"ReadOnly\"\n    enabled              = true\n    id                   = uuid()\n    value                = \"User\"\n  }\n}\n`\n```\nRole Assignment:\n```\n`resource \"azurerm_role_assignment\" \"receiver\" {\n  scope                = resource.azurerm_eventhub_namespace.hub.id\n  role_definition_name = \"Azure Event Hubs Data Receiver\"\n  principal_id         = # I have tried the Object_ID, Application_ID and the Tenant_ID here and all of them fail\n}\n`\n```\nIs there another ID/Service Principle somewhere that I am missing?",
      "solution": "I managed to work this out. Working config is:\n```\n`resource \"azuread_service_principal\" \"eventhub\" {\n  application_id = azuread_application.eventhub_auth.application_id\n}\n\nresource \"azurerm_role_assignment\" \"receiver\" {\n  scope                = resource.azurerm_eventhub_namespace.hub.id\n  role_definition_name = \"Azure Event Hubs Data Receiver\"\n  principal_id         = azuread_service_principal.eventhub.id\n}\n`\n```",
      "question_score": 6,
      "answer_score": 3,
      "created_at": "2022-07-15T06:16:40",
      "url": "https://stackoverflow.com/questions/72988987/principals-of-type-application-cannot-validly-be-used-in-role-assignments"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-azure",
      "question_id": 71587927,
      "title": "Too many command line arguments Terraform plan",
      "problem": "I am new to terraform. I am trying to create a simple storage account through azure pipeline, however when I run my pipeline I get the error  \"Too many command line arguments\". I am struck and I do not know what I am doing wrong. Can someone please help.\nthis is my plan script in pipeline:\n\r\n\r\n`- script:\n    terraform plan -out = plan.tfplan    \n  displayName: Terraform plan\n  workingDirectory: $(System.DefaultWorkingDirectory)/terraform\n  env:\n    ARM_CLIENT_ID: $(application_id)\n    ARM_CLIENT_SECRET: $(client_secret)\n    ARM_TENANT_ID: $(tenant_id)\n    ARM_SUBSCRIPTION_ID: $(subscription_id)\n    TF_VAR_client_id: $(application_id) \n    TF_VAR_tenant_id: $(tenant_id) \n    TF_VAR_subscription_id: $(subscription_id) \n    TF_VAR_client_secret: $(client_secret`\r\n\r\n\r\n\nThe error that I am getting:\n\r\n\r\n`Starting: Terraform plan\n\nGenerating script.\nScript contents:\nterraform plan -out = plan.tfplan\n========================== Starting Command Output ===========================\n/usr/bin/bash --noprofile --norc /home/vsts/work/_temp/3d07140f-ec17-4bfc-9384-a1170fae1248.sh\n\u2577\n\u2502 Error: Too many command line arguments\n\u2502 \n\u2502 To specify a working directory for the plan, use the global -chdir flag.\n\u2575\n\nFor more help on using this command, run:\n  terraform plan -help\n##[error]Bash exited with code '1'.\nFinishing: Terraform plan`",
      "solution": "This has extra spaces, which are not valid:\n```\n`terraform plan -out = plan.tfplan  \n`\n```\nIt should be like the following:\n```\n`terraform plan -out=plan.tfplan  \n`\n```",
      "question_score": 6,
      "answer_score": 6,
      "created_at": "2022-03-23T14:17:18",
      "url": "https://stackoverflow.com/questions/71587927/too-many-command-line-arguments-terraform-plan"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-azure",
      "question_id": 67807043,
      "title": "Creating Azure Front Door instance with TerraForm",
      "problem": "Having trouble creating a Azure Front Door instance with Terraform. The setup should be pretty basic, but can not find out what is wrong.\nHere is the terraform script\n```\n`resource \"azurerm_frontdoor\" \"b2cfrontdoor\" {\n  name                                         = \"fd-adpb2c-westeurope-dev\"\n  resource_group_name                          = azurerm_resource_group.b2c.name\n  enforce_backend_pools_certificate_name_check = true\n\n  routing_rule {\n    name               = \"routingrule\"\n    accepted_protocols = [\"Http\", \"Https\"]\n    patterns_to_match  = [\"/*\"]\n    frontend_endpoints = [\"b2c-frontdoor-endpoint-dev\"]\n    forwarding_configuration {\n      forwarding_protocol = \"MatchRequest\"\n      backend_pool_name   = \"b2-backend-pool-dev\"\n    }\n  }\n\n  backend_pool_load_balancing {\n    name = \"loadbalancingsettings\"\n  }\n\n  backend_pool_health_probe {\n    name    = \"healthprobesettings\"\n    enabled = false\n    probe_method = \"HEAD\"\n  }\n\n  backend_pool {\n    name = \"b2-backend-pool-dev\"\n    backend {\n      host_header = \"xyz.b2clogin.com\"\n      address     = \"xyz.b2clogin.com\"\n      http_port   = 80\n      https_port  = 443\n    }\n    load_balancing_name = \"loadbalancingsettings\"\n    health_probe_name   = \"healthprobesettings\"\n  }\n\n  frontend_endpoint {\n    name      = \"b2c-frontdoor-endpoint-dev\"\n    host_name = \"b2c-frontdoor-endpoint-dev.azurefd.net\"\n    session_affinity_enabled = false\n    session_affinity_ttl_seconds = 0\n  }\n}\n`\n```\nThe error message returned is\n```\n`Error: creating Front Door \"fd-adpb2c-westeurope-dev\" (Resource Group \"rg-adpb2c-westeurope-dev\"): frontdoor.FrontDoorsClient#CreateOrUpdate: Failure sending request: StatusCode=0 -- Original Error: Code=\"BadRequest\" Message=\"The frontend endpoint zone \\\"\\\" must only be used in the default CNAME entry.\"\n\n  on resource_frontdoor.tf line 1, in resource \"azurerm_frontdoor\" \"b2cfrontdoor\":\n   1: resource \"azurerm_frontdoor\" \"b2cfrontdoor\" {\n`\n```\nDid some sniffing on the request sende to Azure and found a PUT request to\nhttps://management.azure.com/subscriptions/*********************/resourceGroups/rg-adpb2c-westeurope-dev/providers/Microsoft.Network/frontDoors/fd-adpb2c-westeurope-dev?api-version=2020-01-01\nwith this payload\n```\n`{\n  \"location\": \"Global\",\n  \"properties\": {\n    \"backendPools\": [\n      {\n        \"id\": \"/subscriptions/*********************************/resourceGroups/rg-adpb2c-westeurope-dev/providers/Microsoft.Network/frontDoors/fd-adpb2c-westeurope-dev/backendPools/b2-backend-pool-dev\",\n        \"name\": \"b2-backend-pool-dev\",\n        \"properties\": {\n          \"backends\": [\n            {\n              \"address\": \"xyz.b2clogin.com\",\n              \"backendHostHeader\": \"xyz.b2clogin.com\",\n              \"enabledState\": \"Enabled\",\n              \"httpPort\": 80,\n              \"httpsPort\": 443,\n              \"priority\": 1,\n              \"weight\": 50\n            }\n          ],\n          \"loadBalancingSettings\": {\n            \"id\": \"/subscriptions/*********************************/resourceGroups/rg-adpb2c-westeurope-dev/providers/Microsoft.Network/frontDoors/fd-adpb2c-westeurope-dev/loadBalancingSettings/loadbalancingsettings\"\n          },\n          \"healthProbeSettings\": {\n            \"id\": \"/subscriptions/*********************************/resourceGroups/rg-adpb2c-westeurope-dev/providers/Microsoft.Network/frontDoors/fd-adpb2c-westeurope-dev/healthProbeSettings/healthprobesettings\"\n          }\n        }\n      }\n    ],\n    \"backendPoolsSettings\": {\n      \"enforceCertificateNameCheck\": \"Disabled\",\n      \"sendRecvTimeoutSeconds\": 60\n    },\n    \"enabledState\": \"Enabled\",\n    \"friendlyName\": \"\",\n    \"frontendEndpoints\": [\n      {\n        \"id\": \"/subscriptions/*********************************/resourceGroups/rg-adpb2c-westeurope-dev/providers/Microsoft.Network/frontDoors/fd-adpb2c-westeurope-dev/frontendEndpoints/b2-frontdoor-endpoint-dev\",\n        \"name\": \"b2-frontdoor-endpoint-dev\",\n        \"properties\": {\n          \"hostName\": \"b2-frontdoor-endpoint-dev.azurefd.net\",\n          \"sessionAffinityEnabledState\": \"Disabled\",\n          \"sessionAffinityTtlSeconds\": 0\n        }\n      }\n    ],\n    \"healthProbeSettings\": [\n      {\n        \"id\": \"/subscriptions/*********************************/resourceGroups/rg-adpb2c-westeurope-dev/providers/Microsoft.Network/frontDoors/fd-adpb2c-westeurope-dev/healthProbeSettings/healthprobesettings\",\n        \"name\": \"healthprobesettings\",\n        \"properties\": {\n          \"path\": \"/\",\n          \"protocol\": \"Http\",\n          \"intervalInSeconds\": 120,\n          \"healthProbeMethod\": \"GET\",\n          \"enabledState\": \"Disabled\"\n        }\n      }\n    ],\n    \"loadBalancingSettings\": [\n      {\n        \"id\": \"/subscriptions/*********************************/resourceGroups/rg-adpb2c-westeurope-dev/providers/Microsoft.Network/frontDoors/fd-adpb2c-westeurope-dev/loadBalancingSettings/loadbalancingsettings\",\n        \"name\": \"loadbalancingsettings\",\n        \"properties\": {\n          \"sampleSize\": 4,\n          \"successfulSamplesRequired\": 2,\n          \"additionalLatencyMilliseconds\": 0\n        }\n      }\n    ],\n    \"routingRules\": [\n      {\n        \"id\": \"\",\n        \"name\": \"routingrule\",\n        \"properties\": {\n          \"frontendEndpoints\": [\n            {\n              \"id\": \"/subscriptions/*********************************/resourceGroups/rg-adpb2c-westeurope-dev/providers/Microsoft.Network/frontDoors/fd-adpb2c-westeurope-dev/frontendEndpoints/b2-frontdoor-endpoint-dev\"\n            }\n          ],\n          \"acceptedProtocols\": [\n            \"Http\",\n            \"Https\"\n          ],\n          \"patternsToMatch\": [\n            \"/*\"\n          ],\n          \"enabledState\": \"Enabled\",\n          \"routeConfiguration\": {\n            \"@odata.type\": \"#Microsoft.Azure.FrontDoor.Models.FrontdoorForwardingConfiguration\",\n            \"backendPool\": {\n              \"id\": \"/subscriptions/*********************************/resourceGroups/rg-adpb2c-westeurope-dev/providers/Microsoft.Network/frontDoors/fd-adpb2c-westeurope-dev/backendPools/b2-backend-pool-dev\"\n            },\n            \"forwardingProtocol\": \"MatchRequest\"\n          }\n        }\n      }\n    ]\n  },\n  \"tags\": {}\n}\n`\n```\nand the response is\n```\n`{\n  \"error\": {\n    \"code\": \"BadRequest\",\n    \"message\": \"The frontend endpoint zone \\\"\\\" must only be used in the default CNAME entry.\"\n  }\n}\n`\n```\nThe TerraForm version is 0.14.10 and the azurerm version is v2.56.0\nAnyone knows about this problem?\nThanks",
      "solution": "Found out what was wrong (also indicated by Jim Xu). The name of the resource (\"azurerm_frontdoor\" \"b2cfrontdoor\") and the name of the frontend_endpoint must be the same. When createing a Front Door instance in the Azure Portal you are not asked for name, The Front Door instance get it's name from the name of the frontend.\n```\n`resource \"azurerm_frontdoor\" \"b2cfrontdoor\" {\n      name                                         = \"b2c-frontdoor-endpoint-dev\"\n      resource_group_name                          = azurerm_resource_group.b2c.name\n      enforce_backend_pools_certificate_name_check = true\n    \n      routing_rule {\n        name               = \"routingrule\"\n        accepted_protocols = [\"Http\", \"Https\"]\n        patterns_to_match  = [\"/*\"]\n        frontend_endpoints = [\"b2c-frontdoor-endpoint-dev\"]\n        forwarding_configuration {\n          forwarding_protocol = \"MatchRequest\"\n          backend_pool_name   = \"b2-backend-pool-dev\"\n        }\n      }\n    \n      backend_pool_load_balancing {\n        name = \"loadbalancingsettings\"\n      }\n    \n      backend_pool_health_probe {\n        name    = \"healthprobesettings\"\n        enabled = false\n        probe_method = \"HEAD\"\n      }\n    \n      backend_pool {\n        name = \"b2-backend-pool-dev\"\n        backend {\n          host_header = \"xyz.b2clogin.com\"\n          address     = \"xyz.b2clogin.com\"\n          http_port   = 80\n          https_port  = 443\n        }\n        load_balancing_name = \"loadbalancingsettings\"\n        health_probe_name   = \"healthprobesettings\"\n      }\n    \n      frontend_endpoint {\n        name      = \"b2c-frontdoor-endpoint-dev\"\n        host_name = \"b2c-frontdoor-endpoint-dev.azurefd.net\"\n        session_affinity_enabled = false\n        session_affinity_ttl_seconds = 0\n      }\n    }\n`\n```",
      "question_score": 6,
      "answer_score": 4,
      "created_at": "2021-06-02T16:24:00",
      "url": "https://stackoverflow.com/questions/67807043/creating-azure-front-door-instance-with-terraform"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-azure",
      "question_id": 69459069,
      "title": "json.Marshal(): json: error calling MarshalJSON for type msgraph.Application",
      "problem": "What specific syntax or configuration changes must be made in order to resolve the error below in which terraform is failing to create an instance of `azuread_application`?\nTHE CODE:\nThe terraform code that is triggering the error when `terraform apply` is run is as follows:\n```\n`variable \"tenantId\" { }\nvariable \"clientId\" { }\nvariable \"clientSecret\" { }\nvariable \"instanceName\" { }\n\nterraform {\n  required_providers {\n    azuread = {\n      source  = \"hashicorp/azuread\"\n      version = \"2.5.0\"\n    }\n  }\n}\n\nprovider \"azuread\" {\n  tenant_id       = var.tenantId\n  client_id       = var.clientId\n  client_secret   = var.clientSecret\n}\n\nresource \"azuread_application\" \"appRegistration\" {\n  display_name = var.instanceName\n  app_role {\n    allowed_member_types = [\"User\", \"Application\"]\n    description          = \"Admins can manage roles and perform all task actions\"\n    display_name         = \"Admin\"\n    enabled              = true\n    id                   = \"1b19509b-32b1-4e9f-b71d-4992aa991967\"\n    value                = \"admin\"\n  }\n}\n`\n```\nTHE ERROR:\nThe error and log output that result from running the above code with `terraform apply` are:\n```\n`2021/10/05 17:47:18 [DEBUG] module.ad-admin.azuread_application.appRegistration:\n apply errored, but we're indicating that via the Error pointer rather than returning it:\n Could not create application: json.Marshal():\n json: error calling MarshalJSON for type msgraph.Application:\n json: error calling MarshalJSON for type *msgraph.Owners: marshaling Owners: encountered DirectoryObject with nil ODataId\n\n2021/10/05 17:47:18 [TRACE] EvalMaybeTainted: module.ad-admin.azuread_application.appRegistration encountered an error during creation, so it is now marked as tainted\n2021/10/05 17:47:18 [TRACE] EvalWriteState: removing state object for module.ad-admin.azuread_application.appRegistration\n2021/10/05 17:47:18 [TRACE] EvalApplyProvisioners: azuread_application.appRegistration has no state, so skipping provisioners\n2021/10/05 17:47:18 [TRACE] EvalMaybeTainted: module.ad-admin.azuread_application.appRegistration encountered an error during creation, so it is now marked as tainted\n2021/10/05 17:47:18 [TRACE] EvalWriteState: removing state object for module.ad-admin.azuread_application.appRegistration\n2021/10/05 17:47:18 [TRACE] vertex \"module.ad-admin.azuread_application.appRegistration\": visit complete\n\n2021/10/05 17:47:18 [TRACE] dag/walk: upstream of \"module.ad-admin.output.application_id (expand)\" errored, so skipping\n2021/10/05 17:47:18 [TRACE] dag/walk: upstream of \"module.ad-admin.azuread_service_principal.appRegistrationSP\" errored, so skipping\n2021/10/05 17:47:18 [TRACE] dag/walk: upstream of \"output.application_id\" errored, so skipping\n2021/10/05 17:47:18 [TRACE] dag/walk: upstream of \"module.ad-admin.output.appId (expand)\" errored, so skipping\n2021/10/05 17:47:18 [TRACE] dag/walk: upstream of \"module.ad-admin.azuread_service_principal_password.appRegistrationSP_pwd\" errored, so skipping\n2021/10/05 17:47:18 [TRACE] dag/walk: upstream of \"output.appId\" errored, so skipping\n2021/10/05 17:47:18 [TRACE] dag/walk: upstream of \"module.ad-admin.azurerm_role_assignment.appRegistrationSP_role_assignment_vault\" errored, so skipping\n2021/10/05 17:47:18 [TRACE] dag/walk: upstream of \"module.ad-admin.azurerm_role_assignment.appRegistrationSP_role_assignment\" errored, so skipping\n2021/10/05 17:47:18 [TRACE] dag/walk: upstream of \"module.ad-admin.provider[\\\"registry.terraform.io/hashicorp/azuread\\\"] (close)\" errored, so skipping\n2021/10/05 17:47:18 [TRACE] dag/walk: upstream of \"module.ad-admin.provider[\\\"registry.terraform.io/hashicorp/azurerm\\\"] (close)\" errored, so skipping\n2021/10/05 17:47:18 [TRACE] dag/walk: upstream of \"module.ad-admin (close)\" errored, so skipping\n2021/10/05 17:47:18 [TRACE] dag/walk: upstream of \"meta.count-boundary (EachMode fixup)\" errored, so skipping\n2021/10/05 17:47:18 [TRACE] dag/walk: upstream of \"root\" errored, so skipping\n2021/10/05 17:47:18 [TRACE] statemgr.Filesystem: creating backup snapshot at terraform.tfstate.backup\n2021/10/05 17:47:18 [TRACE] statemgr.Filesystem: state has changed since last snapshot, so incrementing serial to 391\n2021/10/05 17:47:18 [TRACE] statemgr.Filesystem: writing snapshot at terraform.tfstate\n2021/10/05 17:47:18 [TRACE] statemgr.Filesystem: removing lock metadata file .terraform.tfstate.lock.info\n\nError: Could not create application\n\n  on ..\\..\\..\\..\\modules\\ad-admin\\active-directory.tf line 69, in resource \"azuread_application\" \"appRegistration\":\n  69: resource \"azuread_application\" \"appRegistration\" {\n\njson.Marshal(): json: error calling MarshalJSON for type msgraph.Application:\njson: error calling MarshalJSON for type *msgraph.Owners: marshaling Owners:\n2021/10/05 17:47:18 [TRACE] statemgr.Filesystem: unlocked by closing terraform.tfstate\nencountered DirectoryObject with nil ODataId\n`\n```\n`terraform -version` gives:\nTerraform v1.0.8\non windows_amd64",
      "solution": "This was a bug, reported as GitHub issue:\n\nError: ODataId was nil when creating an azuread_group resource #588 \n\nThe resolution to the problem in the OP is to upgrade the version from `2.5.0` to `2.6.0` in the `required_providers` block from the code in the OP above as follows:\n```\n`terraform {\n  required_providers {\n    azuread = {\n      source  = \"hashicorp/azuread\"\n      version = \"2.6.0\"\n    }\n  }\n}\n`\n```",
      "question_score": 6,
      "answer_score": 8,
      "created_at": "2021-10-06T04:07:35",
      "url": "https://stackoverflow.com/questions/69459069/json-marshal-json-error-calling-marshaljson-for-type-msgraph-application"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-azure",
      "question_id": 72630228,
      "title": "Issue in deploying azure function through terraform with app settings",
      "problem": "I am Following this docs page to deploy azure function with app settings https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/function_app\nMy terraform file looks like :\n```\n`terraform {\n  required_providers {\n    azurerm = {\n      source  = \"hashicorp/azurerm\"\n      version = \"3.10.0\"\n    }\n  }\n}\n\nprovider \"azurerm\" {\n}\nresource \"azurerm_resource_group\" \"example\" {\n  name     = \"azure-functions-test-rg\"\n  location = \"West Europe\"\n}\n\nresource \"azurerm_storage_account\" \"example\" {\n  name                     = \"funcdemo123shafiq\"\n  resource_group_name      = azurerm_resource_group.example.name\n  location                 = azurerm_resource_group.example.location\n  account_tier             = \"Standard\"\n  account_replication_type = \"LRS\"\n}\n\nresource \"azurerm_app_service_plan\" \"example\" {\n  name                = \"azure-functions-test-service-plan\"\n  location            = azurerm_resource_group.example.location\n  resource_group_name = azurerm_resource_group.example.name\n\n  sku {\n    tier = \"Standard\"\n    size = \"S1\"\n  }\n}\n\nresource \"azurerm_function_app\" \"example\" {\n  name                       = \"test-azure-shafiq123\"\n  location                   = azurerm_resource_group.example.location\n  resource_group_name        = azurerm_resource_group.example.name\n  app_service_plan_id        = azurerm_app_service_plan.example.id\n  storage_account_name       = azurerm_storage_account.example.name\n  storage_account_access_key = azurerm_storage_account.example.primary_access_key\n  os_type                    = \"linux\"\n  version                    = \"~4\"\n\n  app_settings {\n    FUNCTIONS_WORKER_RUNTIME = \"python\"\n    TESTING_KEY              = \"TESTING_VALUE\"\n  }\n\n  site_config {\n    linux_fx_version = \"python|3.9\"\n  }\n}\n`\n```\nWhen try to deploy this through `terraform apply` command , I am getting this error.\n```\n`\u2502 Error: Unsupported block type\n\u2502\n\u2502   on main.tf line 46, in resource \"azurerm_function_app\" \"example\":\n\u2502   46:   app_settings {\n\u2502\n\u2502 Blocks of type \"app_settings\" are not expected here. Did you mean to define argument \"app_settings\"? If so, use the equals sign to assign it a value.\n`\n```",
      "solution": "`app_setting` is supported on specific version of Terraform AzureRM provider. There is bug fixed availble for those version. I have used `3.3.0` provider version and it is working for me as expected and also you can't configure the value of `site_config`.Its value will be decide automatically based on the result of applying this configuration, same you can check in the updated document of Terraform\nmain.tf\n```\n` terraform {\n      required_providers {\n        azurerm = {\n          source  = \"hashicorp/azurerm\"\n          version = \"3.3.0\"\n        }\n      }\n    }\n    \n    provider \"azurerm\" {\n        features{}\n    }\n    data \"azurerm_resource_group\" \"example\" {\n      name     = \"v-rXXXXXree\"\n      #location = \"West Europe\"\n    }\n    \n    resource \"azurerm_storage_account\" \"example\" {\n      name                     = \"funcdemo123shafiq4535\"\n      resource_group_name      = data.azurerm_resource_group.example.name\n      location                 = data.azurerm_resource_group.example.location\n      account_tier             = \"Standard\"\n      account_replication_type = \"LRS\"\n    }\n    \n    resource \"azurerm_service_plan\" \"example\" {\n      name                = \"azure-functions-test-service-plan1\"\n      location            = data.azurerm_resource_group.example.location\n      resource_group_name = data.azurerm_resource_group.example.name\n      os_type             = \"Linux\"\n      sku_name            = \"Y1\"\n    }\n    \n    resource \"azurerm_linux_function_app\" \"example\" {\n      name                       = \"test-azure-shafi4353\"\n      location                   = data.azurerm_resource_group.example.location\n      resource_group_name        = data.azurerm_resource_group.example.name\n      service_plan_id            =  azurerm_service_plan.example.id\n      storage_account_name       = azurerm_storage_account.example.name\n      storage_account_access_key = azurerm_storage_account.example.primary_access_key\n      #os_type                    = \"linux\"\n      #version                    = \"~3\"\n    \n      app_settings={\n        FUNCTIONS_WORKER_RUNTIME = \"python\"\n        TESTING_KEY              = \"TESTING_VALUE\"\n      }\n    \n      site_config {\n        #linux_fx_version = \"python|3.9\"\n      }\n    \n    }\n`\n```",
      "question_score": 6,
      "answer_score": 6,
      "created_at": "2022-06-15T13:04:24",
      "url": "https://stackoverflow.com/questions/72630228/issue-in-deploying-azure-function-through-terraform-with-app-settings"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-azure",
      "question_id": 67228241,
      "title": "Azure DevOps Pipelines with Terraform failing on arm_xxx parameters",
      "problem": "We have several pipelines in Azure-Devops performing Terraform init-plan-apply. Until now worked fine.\nBut all of a sudden we are getting these errors in the Init phase.\n```\n`Initializing the backend...\n\u2577\n\u2502 Error: Invalid backend configuration argument\n\u2502 \n\u2502 The backend configuration argument \"arm_subscription_id\" given on the command line is not expected for the selected backend type.\n\u2575 Error: Invalid backend configuration argument\n\u2502 \n\u2502 The backend configuration argument \"arm_tenant_id\" .....\n\u2502 The backend configuration argument \"arm_client_id\" .....\n\u2502 The backend configuration argument \"arm_client_secret\" ....\n`\n```\nOn the hasicorp website I found a remark on this https://www.terraform.io/upgrade-guides/0-15.html .\nBut thegeneration of the init command is completelly done by DevOps, there is no place where I can change the arm_client_id to client_id (and the others).\nAnybody has seen this behaviour and being able to solve it.",
      "solution": "I spent about 2 hours on this issue today and I found that I had to perform the following.\n\nI had to install Terraform 0.14.11 as the new 0.15.1 was failing\n\nIn your Terraform init task, add -reconfigure to Additional command arguments\n\nAlthough my terraform code worked fine on my PC, I was getting the same errors as you are. In my main.tf file, I removed all reference to my backend since they are already defined in the init task\n\n```\n`    terraform {\n      required_providers {\n        azurerm = {\n          source  = \"hashicorp/azurerm\"\n          version = \"=2.48.0\"\n        }\n      }\n    }\n    \n    provider \"azurerm\" {\n      features {}\n    }\n    \n    terraform {\n      backend \"azurerm\" {\n      }\n    }\n`\n```",
      "question_score": 6,
      "answer_score": 3,
      "created_at": "2021-04-23T12:23:20",
      "url": "https://stackoverflow.com/questions/67228241/azure-devops-pipelines-with-terraform-failing-on-arm-xxx-parameters"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-azure",
      "question_id": 67836550,
      "title": "Terraform fails to create a 64-bit Azure App Service (Web app)",
      "problem": "I am using terraform version 0.15.5 (and also 0.15.4) to provision an Azure Web App resource. The following configuration works fine:\n` site_config {\n    http2_enabled             = true\n    always_on                 = false\n    use_32_bit_worker_process = true\n  }\n`\nBut when I use `use_32_bit_worker_process = false` to get the script provision a 64-bit web app, it fails and I get the following error message:\n```\n`2021-06-03T18:06:55.6392592Z [31m\u2502[0m [0m[1m[31mError: [0m[0m[1mError creating App Service \"gfdemogatewayapp\" (Resource Group \"MASKED\"): web.AppsClient#CreateOrUpdate: Failure sending request: StatusCode=0 -- Original Error: autorest/azure: Service returned an error. Status= [0m\n2021-06-03T18:06:55.6411094Z [31m\u2502[0m [0m\n2021-06-03T18:06:55.6426506Z [31m\u2502[0m [0m[0m  with azurerm_app_service.gfgatewayapp,\n2021-06-03T18:06:55.6427703Z [31m\u2502[0m [0m  on main.tf line 274, in resource \"azurerm_app_service\" \"gfgatewayapp\":\n2021-06-03T18:06:55.6428766Z [31m\u2502[0m [0m 274: resource \"azurerm_app_service\" \"gfgatewayapp\" [4m{[0m[0m\n2021-06-03T18:06:55.6429584Z [31m\u2502[0m [0m\n2021-06-03T18:06:55.6430461Z [31m\u2575[0m[0m\n2021-06-03T18:06:55.6534148Z ##[error]Error: The process '/opt/hostedtoolcache/terraform/0.15.4/x64/terraform' failed with exit code 1\n2021-06-03T18:06:55.6548186Z ##[section]Finishing: Terraform approve and apply\n`\n```\nIs there something that I am missing or terraform has an issue in provisioning the 64-bit web app resource on Azure?\nUPDATE: The full source code\nThe tier is Standard and the SKU size is \"F1\".\n```\n`resource \"azurerm_app_service_plan\" \"gfwebappserviceplan\" {\n  name                = var.gatewayserviceplanname\n  location            = \"${azurerm_resource_group.gf.location}\"\n  resource_group_name = \"${azurerm_resource_group.gf.name}\"\n\n  sku {\n    tier = var.gatewayserviceplanskutier\n    size = var.gatewayserviceplanskusize\n  }\n}\nresource \"azurerm_app_service\" \"gfgatewayapp\" {\n  name                = var.gatewayappname\n  location            = \"${azurerm_resource_group.gf.location}\"\n  resource_group_name = \"${azurerm_resource_group.gf.name}\"\n  app_service_plan_id = azurerm_app_service_plan.gfwebappserviceplan.id\n  app_settings = {\n    APPINSIGHTS_INSTRUMENTATIONKEY = \"${azurerm_application_insights.gfapplicationinsights.instrumentation_key}\"\n  }\n\n  site_config {\n    http2_enabled             = true\n    always_on                 = false\n    use_32_bit_worker_process = false\n  }\n}\n\noutput \"gfgatewayhostname\" {\n  value = \"${azurerm_app_service.gfgatewayapp.default_site_hostname}\"\n  description = \"Gateway default host name\"\n}\n\nresource \"azurerm_template_deployment\" \"webapp-corestack\" {\n  # This will make it .NET CORE for Stack property, and add the dotnet core logging extension\n  name                = \"AspNetCoreStack\"\n  resource_group_name = \"${azurerm_resource_group.gf.name}\"\n  template_body       = <<DEPLOY\n{\n    \"$schema\": \"https://schema.management.azure.com/schemas/2019-04-01/deploymentTemplate.json#\",\n    \"contentVersion\": \"1.0.0.0\",\n    \"parameters\": {\n        \"siteName\": {\n            \"type\": \"string\",\n            \"metadata\": {\n                \"description\": \"The Azure App Service Name\"\n            }\n        },\n        \"extensionName\": {\n            \"type\": \"string\",\n            \"metadata\": {\n                \"description\": \"The Site Extension Name.\"\n            }\n        },\n        \"extensionVersion\": {\n            \"type\": \"string\",\n            \"metadata\": {\n                \"description\": \"The Extension Version\"\n            }\n        }\n    },\n    \"resources\": [\n        {\n            \"apiVersion\": \"2018-02-01\",\n            \"name\": \"[parameters('siteName')]\",\n            \"type\": \"Microsoft.Web/sites\",\n            \"location\": \"[resourceGroup().location]\",\n            \"properties\": {\n                \"name\": \"[parameters('siteName')]\",\n                \"siteConfig\": {\n                    \"appSettings\": [],\n                    \"metadata\": [\n                        {\n                            \"name\": \"CURRENT_STACK\",\n                            \"value\": \"dotnetcore\"\n                        }\n                    ]\n                }\n            }\n        },\n        {\n            \"type\": \"Microsoft.Web/sites/siteextensions\",\n            \"name\": \"[concat(parameters('siteName'), '/', parameters('extensionName'))]\",\n            \"apiVersion\": \"2018-11-01\",\n            \"location\": \"[resourceGroup().location]\",\n            \"properties\": {\n                \"version\": \"[parameters('extensionVersion')]\"\n            }\n        }\n    ]\n}\n  DEPLOY\n  parameters = {\n    \"siteName\"         = azurerm_app_service.gfgatewayapp.name\n    \"extensionName\"    = \"Microsoft.AspNetCore.AzureAppServices.SiteExtension\"\n    \"extensionVersion\" = \"3.1.7\"\n  }\n  deployment_mode = \"Incremental\"\n  depends_on      = [azurerm_app_service.gfgatewayapp]\n}\n`\n```",
      "solution": "You are getting this error because you are using a `F1` tier `app service plan`. `Free` or `Shared` tiers do not have a 64 bit option.\nTerraform AzureRM Registry",
      "question_score": 5,
      "answer_score": 11,
      "created_at": "2021-06-04T13:36:50",
      "url": "https://stackoverflow.com/questions/67836550/terraform-fails-to-create-a-64-bit-azure-app-service-web-app"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-azure",
      "question_id": 74986447,
      "title": "Azure : GroupsClient.BaseClient.Get(): unexpected status 403 with OData error: Authorization_RequestDenied: Insufficient privileges",
      "problem": "I\u2019m trying to create the Azure AD Group using the following terraform code\n```\n`# Required Provider\nterraform {\n  required_providers {\n    azurerm = {\n      source  = \"hashicorp/azurerm\"\n      version = \"~> 3.0.2\"\n    }\n  }\n  required_version = \">= 1.1.0\"\n}\n\n# Configure the Microsoft Azure Provider\nprovider \"azurerm\" {\n  features {}\n\n  ....\n  ....\n}\n\ndata \"azuread_client_config\" \"current\" {}\n\n# Variables\nvariable \"ad_groups\" {\n  description = \"Azure AD groups to be added\"\n  type = list(object({\n    display_name = string,\n    description  = string   \n  }))\n  default = [\n    {\n      display_name = \"Group1\"\n      description  = \"some description\"\n    },\n    {\n      display_name = \"Group2\"\n      description  = \"some description\" \n    }\n  ]\n}\n\n# Create AD Groups and add the Current User\nresource \"azuread_group\" \"this\"{\n  count = length(var.ad_groups)\n  display_name =  var.ad_groups[count.index].display_name\n  description = var.ad_groups[count.index].description\n  security_enabled = true\n  prevent_duplicate_names = true  \n  owners  = [data.azuread_client_config.current.object_id]\n}\n`\n```\nand I am getting the following error\n```\n`**Error:** could not check for existing group(s): unable to list Groups with filter \"displayName eq 'Group1'\": GroupsClient.BaseClient.Get(): unexpected status 403 with OData error: Authorization_RequestDenied: Insufficient privileges to complete the operation.\n`\n```\nThis service principal has the following roles at the Management group level\n\nDoes it need both the Directory.ReadWrite.All and Group.ReadWrite.All API Permissions? If not, what access does it need?\n\nNote: If I disable the \"prevent_duplicate_names = true\" and apply the terraform, it throws the following error\n```\n`GroupsClient.BaseClient.Post(): unexpected status 403 with OData error: Authorization_RequestDenied: Insufficient privileges to\n\u2502 complete the operation.\n`\n```",
      "solution": "I tried to reproduce the same in my environment via Postman and got below results:\nBy default, newly created application will have User.Read API permission already added to it.\nI registered one new Azure AD application named `GroupSP` and has API permission like below:\n\nWithout adding any extra API permission, I generated one access token using client credentials flow via Postman like below:\n`POST https://login.microsoftonline.com//oauth2/v2.0/token\nclient_id:\ngrant_type:client_credentials\nclient_secret:\nscope: https://graph.microsoft.com/.default\n`\nResponse:\n\nWhen I used the above token to create Azure AD group with owner, I got same error as below:\n`POST https://graph.microsoft.com/v1.0/groups\nContent-Type: application/json\n\n{\n  \"description\": \"Group with designated owner\",\n  \"displayName\": \"Group1\",\n  \"groupTypes\": [ ],\n  \"mailEnabled\": false,\n  \"mailNickname\": \"srigroup\",\n  \"securityEnabled\": true,\n  \"owners@odata.bind\": [\n    \"https://graph.microsoft.com/v1.0/users/\"\n  ]\n}\n`\nResponse:\n\nTo resolve the error, I added `Directory.ReadWrite.All` API permission to the service principal like below:\n\nAfter granting admin consent to above permission, I generated access token again and ran the same query and got response successfully as below:\n`POST https://graph.microsoft.com/v1.0/groups\nContent-Type: application/json\n\n{\n  \"description\": \"Group with designated owner\",\n  \"displayName\": \"Group1\",\n  \"groupTypes\": [ ],\n  \"mailEnabled\": false,\n  \"mailNickname\": \"srigroup\",\n  \"securityEnabled\": true,\n  \"owners@odata.bind\": [\n    \"https://graph.microsoft.com/v1.0/users/\"\n  ]\n}\n`\nResponse:\n\nTo confirm that, I checked the Portal where Azure AD group is created, and owner added successfully like below:\n\nYou can also check Audit logs of that created group like below:\n\nIn your case, make sure to add `Directory.ReadWrite.All` API permission to your service principal that resolves 403 Forbidden error.\nIf Directory.ReadWrite.All permission is added to the service principal, Group.ReadWrite.All permission is not required.",
      "question_score": 5,
      "answer_score": 7,
      "created_at": "2023-01-02T20:45:34",
      "url": "https://stackoverflow.com/questions/74986447/azure-groupsclient-baseclient-get-unexpected-status-403-with-odata-error-a"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-azure",
      "question_id": 70508114,
      "title": "terraform - set a variable from commandline for module variable",
      "problem": "I have following structure\n```\n`main.tf\nmodules\n--moduleA\n----worker.tf\n----variables.tf\n`\n```\nContent of main.tf:\n```\n`module \"moduleA\" {\n  source = \"./modules/moduleA\"\n}\n`\n```\nContent of variables.tf:\n```\n`variable \"num_of_workers\" {\n  type        = number\n  description = \"This is number of workers\"\n  default     = 1\n`\n```\nI want co call `terraform apply var=\"num_of_workers=12\"`\nI am getting an error:\n```\n`Error: Value for undeclared variable\n\u2502 A variable named \"num_of_workers\" was assigned on the command line, but the root module does not declare a variable of that name. To use this value, add a \"variable\" block to the configuration.\n`\n```\nIs there any way to set variables in variables.tf in module and set them from commandline? What I am missing here?",
      "solution": "You need to also declare the variable at the parent level. Then you would pass the parent level value to the module like this:\n```\n`variable \"num_of_workers\" {\n  type = number\n}\n\nmodule \"moduleA\" {\n  source = \"./modules/moduleA\"\n  num_of_workers = var.num_of_workers\n}\n`\n```\nThen you would set the parent-level value at the command line like this:\n```\n`terraform apply -var num_of_workers=2\n`\n```\nDocumentation: https://developer.hashicorp.com/terraform/language/values/variables",
      "question_score": 5,
      "answer_score": 7,
      "created_at": "2021-12-28T15:00:46",
      "url": "https://stackoverflow.com/questions/70508114/terraform-set-a-variable-from-commandline-for-module-variable"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-azure",
      "question_id": 65789254,
      "title": "use of terraform -out parameter",
      "problem": "After running a terraform plan it said\n\nNote: You didn't specify an \"-out\" parameter to save this plan, so\nTerraform can't guarantee that exactly these actions will be performed\nif \"terraform apply\" is subsequently run.\n\nWhat does the -out mean and how and why should I use it in Azure DevOps pipeline.",
      "solution": "it means - save the output to a file (and later you can reuse that file with `terraform apply` if you want to). you don't have to use it at all in the pipelines. In fact, since its a pipeline, I don't think there is any merit to it at all, because in the pipeline, you can be sure that the plan and apply are run against the same code.",
      "question_score": 5,
      "answer_score": 6,
      "created_at": "2021-01-19T11:04:10",
      "url": "https://stackoverflow.com/questions/65789254/use-of-terraform-out-parameter"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-azure",
      "question_id": 70265157,
      "title": "Creation of Terraform Variables for Nested Objects",
      "problem": "Following is the variable declaration for my terraform module which is used in our cloud and inputs for these variables are obtained via one of the automation solutions. Now, I would like to reproduce one of the issues for which I would like to create a tfvars file from the below variable definition.\nvariables.tf:\n```\n`variable \"docker\" {\n  type = object({\n    image_name    = string\n    image_location = string\n    docker_ports = object({\n      internal = number\n      external    = number\n    })\n    rmodelling = object({\n      lang  = object({\n        version = number\n        enabled    = bool\n        policy = object({\n          identification = string\n        })\n      })\n      impl = object({\n        version   = number\n        enabled      = bool\n        policy = object({\n          identification = string\n        })\n      })\n    })\n  })\n}\n`\n```\nI have tried something like this, but for the next nested objected I am not quite sure on how those can be put down. Can someone guide or shed some pointers?\nterraform.tfvars:\n```\n`docker = {\n  image_name = \"Ubuntu 18.04\"\n  image_location = \"https://registry.jd.com/ubuntu/\"\n  docker_ports = {\n    internal = 80\n    external = 443\n  }\nrmodelling = { ??\n???\n`\n```",
      "solution": "An example of a valid value for your `var.docker` is:\n```\n`docker = {\n  image_name = \"Ubuntu 18.04\"\n  image_location = \"https://registry.jd.com/ubuntu/\"\n  docker_ports = {\n    internal = 80\n    external = 443\n  }\n  rmodelling = { \n      lang = {\n            version = 3\n            enabled = true\n            policy = {\n              identification = \"test\"\n            }\n      }\n      impl = {\n        version = 4\n        enabled = false\n        policy = {\n          identification = \"test2\"\n        }      \n     }       \n  }    \n\n}\n\n`\n```",
      "question_score": 5,
      "answer_score": 4,
      "created_at": "2021-12-07T19:22:34",
      "url": "https://stackoverflow.com/questions/70265157/creation-of-terraform-variables-for-nested-objects"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-azure",
      "question_id": 68892355,
      "title": "Unable to connect the API connection to the logic App via ARM template in terraform",
      "problem": "In my terraform I have created a logic app and its workflow with the help of a ARM Template. The 2 connections used in the logic app is also created via ARM template. But somehow even though the resources get created in AZURE. But when I got to the logic app, I always have to manually update the connection in the workflow. How can we make it automatic.\n```\n`\n//First connection\n\nresource \"azurerm_template_deployment\" \"exampleeventhub\" {\n  name                = \"acctesttemplate-44\"\n  resource_group_name = Resourcegrpname\n\n template_body = <<DEPLOY\n{\n    \"$schema\": https://schema.management.azure.com/schemas/2019-04-01/deploymentTemplate.json#,\n    \"contentVersion\": \"1.0.0.0\",\n    \"parameters\": {\n        \"connections_eventhubs_name\": {\n            \"defaultValue\": \"eventhubs\",\n            \"type\": \"String\"\n        }\n    },\n    \"variables\": {},\n    \"resources\": [\n        {\n            \"type\": \"Microsoft.Web/connections\",\n            \"apiVersion\": \"2016-06-01\",\n            \"name\": \"[parameters('connections_eventhubs_name')]\",\n            \"location\": \"qwerty\",\n            \"kind\": \"V1\",\n            \"properties\": {\n                \"displayName\": \"eventhubconnection\",\n                \"statuses\": [\n                    {\n                        \"status\": \"Connected\"\n                    }\n                ],\n                \"customParameterValues\": {},\n                \"nonSecretParameterValues\": {},\n                \"createdTime\": \"aaaaa\",\n                \"changedTime\": \"bbbb\",\n                \"api\": {\n                    \"name\": \"[parameters('connections_eventhubs_name')]\",\n                    \"displayName\": \"Event Hubs\",\n                    \"description\": \"Connect to Azure Event Hubs to send and receive events.\",\n                    \"iconUri\": \"[concat('https://connectoricons-prod.azureedge.net/releases/v1.0.1480/1.0.1480.2454/', parameters('connections_eventhubs_name'), '/icon.png')]\",\n                    \"brandColor\": \"#c4d5ff\",\n                    \"id\": \"[concat('/subscriptions/1111/providers/Microsoft.Web/locations/qwerty/managedApis/', parameters('connections_eventhubs_name'))]\",\n                    \"type\": \"Microsoft.Web/locations/managedApis\"\n                },\n                \"testLinks\": []\n            }\n        }\n    ]\n}\nDEPLOY\n    deployment_mode = \"Incremental\"\n  }\n\n`\n```\n```\n`//Second connection\nresource \"azurerm_template_deployment\" \"exampledatacollector\" {\n  name                = \"acctesttemplate-45\"\n  resource_group_name = Resourcegrpname\n template_body = <<DEPLOY\n{\n    \"$schema\": https://schema.management.azure.com/schemas/2019-04-01/deploymentTemplate.json#,\n    \"contentVersion\": \"1.0.0.0\",\n    \"parameters\": {\n        \"connections_thengadatacollector_name\": {\n            \"defaultValue\": \"thengadatacollector\",\n            \"type\": \"String\"\n        }\n    },\n    \"variables\": {},\n    \"resources\": [\n        {\n            \"type\": \"Microsoft.Web/connections\",\n            \"apiVersion\": \"2016-06-01\",\n            \"name\": \"[parameters('connections_thengadatacollector_name')]\",\n            \"location\": \"qwerty\",\n            \"kind\": \"V1\",\n            \"properties\": {\n                \"displayName\": \"azuredatacollector\",\n                \"statuses\": [\n                    {\n                        \"status\": \"Connected\"\n                    }\n                ],\n                \"customParameterValues\": {},\n                \"nonSecretParameterValues\": {\n                    \"username\": \"764a2b1e-431d-4e90-87b1-ea6a34dac48f\"\n                },\n                \"createdTime\": \"aaaa\",\n                \"changedTime\": \"bbbb\",\n                \"api\": {\n                    \"name\": \"[parameters('connections_thengadatacollector_name')]\",\n                    \"displayName\": \"Azure Log Analytics Data Collector\",\n                    \"description\": \"Azure Log Analytics Data Collector will send data to any Azure Log Analytics workspace.\",\n                    \"iconUri\": \"[concat('https://connectoricons-prod.azureedge.net/releases/v1.0.1480/1.0.1480.2454/', parameters('connections_thengadatacollector_name'), '/icon.png')]\",\n                    \"brandColor\": \"#0072C6\",\n                    \"id\": \"[concat('/subscriptions/1111/providers/Microsoft.Web/locations/qwerty/managedApis/', parameters('connections_thengadatacollector_name'))]\",\n                    \"type\": \"Microsoft.Web/locations/managedApis\"\n                },\n                \"testLinks\": []\n            }\n        }\n    ]\n}\n\nDEPLOY\n    deployment_mode = \"Incremental\"\n  }\n`\n```\n```\n`//Logic App\nresource \"azurerm_template_deployment\" \"example\" {\n  name                = \"acctesttemplate-46\"\n  resource_group_name = Resourcegrpname\n\n template_body = <<DEPLOY\n{\n    \"$schema\": https://schema.management.azure.com/schemas/2019-04-01/deploymentTemplate.json#,\n    \"contentVersion\": \"1.0.0.0\",\n    \"parameters\": {\n        \"workflows_logicapp_name\": {\n            \"defaultValue\": \"logicapp\",\n            \"type\": \"String\"\n        },\n        \"connections_thengadatacollector_externalid\": {\n            \"defaultValue\": \"/subscriptions/1111/resourceGroups/Resourcegrpname/providers/Microsoft.Web/connections/azureloganalyticsdatacollector\",\n            \"type\": \"String\"\n        },\n        \"connections_eventhubs_externalid\": {\n            \"defaultValue\": \"/subscriptions/1111/resourceGroups/Resourcegrpname/providers/Microsoft.Web/connections/eventhubs\",\n            \"type\": \"String\"\n        }\n    },\n    \"variables\": {},\n    \"resources\": [\n        {\n            \"type\": \"Microsoft.Logic/workflows\",\n            \"apiVersion\": \"2017-07-01\",\n            \"name\": \"[parameters('workflows_logicapp_name')]\",\n            \"location\": \"qwerty\",\n            \"properties\": {\n                \"state\": \"Enabled\",\n                \"definition\": {\n                    \"$schema\": https://schema.management.azure.com/providers/Microsoft.Logic/schemas/2016-06-01/workflowdefinition.json#,\n                    \"contentVersion\": \"1.0.0.0\",\n                    \"parameters\": {\n                        \"$connections\": {\n                            \"defaultValue\": {},\n                            \"type\": \"Object\"\n                        }\n                    },\n                    \"triggers\": {\n                        \"When_events_are_available_in_Event_Hub\": {\n                            \"recurrence\": {\n                                \"frequency\": \"Minute\",\n                                \"interval\": 3\n                            },\n                            \"splitOn\": \"@triggerBody()\",\n                            \"type\": \"ApiConnection\",\n                            \"inputs\": {\n                                \"host\": {\n                                    \"connection\": {\n                                        \"name\": \"@parameters('$connections')['eventhubs']['connectionId']\"\n                                    }\n                                },\n                                \"method\": \"get\",\n                                \"path\": \"/@{encodeURIComponent('thengaeventhub')}/events/batch/head\",\n                                \"queries\": {\n                                    \"contentType\": \"application/octet-stream\",\n                                    \"maximumEventsCount\": 50\n                                }\n                            }\n                        }\n                    },\n                    \"actions\": {\n                        \"Send_Data_2\": {\n                            \"runAfter\": {},\n                            \"type\": \"ApiConnection\",\n                            \"inputs\": {\n                                \"body\": \"@base64ToString(triggerBody()?['ContentData'])\",\n                                \"headers\": {\n                                    \"Log-Type\": \"testcustimlog\"\n                                },\n                                \"host\": {\n                                    \"connection\": {\n                                        \"name\": \"@parameters('$connections')['thengadatacollector_1']['connectionId']\"\n                                    }\n                                },\n                                \"method\": \"post\",\n                                \"path\": \"/api/logs\"\n                            }\n                        }\n                    }\n                },\n                \"parameters\": {\n                    \"$connections\": {\n                        \"value\": {\n                            \"thengadatacollector_1\": {\n                                \"connectionId\": \"[parameters('connections_thengadatacollector_externalid')]\",\n                                \"connectionName\": \"thengadatacollector\",\n                                \"id\": \"/subscriptions/1111/providers/Microsoft.Web/locations/qwerty/managedApis/thengadatacollector\"\n                            },\n                            \"eventhubs\": {\n                                \"connectionId\": \"[parameters('connections_eventhubs_externalid')]\",\n                                \"connectionName\": \"eventhubs\",\n                                \"id\": \"/subscriptions/1111/providers/Microsoft.Web/locations/qwerty/managedApis/eventhubs\"\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    ]\n}\nDEPLOY\n    deployment_mode = \"Incremental\"\n  }\n\n`\n```",
      "solution": "It is an expected behaviour , if you deploy the ARM Template, your both API Connections will have been created but inside logic apps you will have to update manually the connection by entering your credentials for the service. This is because for finalizing the API connection you need to give the consent but which is not possible in ARM template.\nBut if you need to finalize the API Connection creation without opening every Logic Apps then you can use PowerShell script .This script will retrieve a consent link for a connection for an OAuth Logic Apps connector. It will then open the consent link and complete authorization to enable a connection.\n```\n`Param(\n    [string] $ResourceGroupName = 'YourRG',\n    [string] $ResourceLocation = 'eastus | westus | etc.',\n    [string] $api = 'office365 | dropbox | dynamicscrmonline | etc.',\n    [string] $ConnectionName = 'YourConnectionName',\n    [string] $subscriptionId = '80d4fe69-xxxx-xxxx-a938-9250f1c8ab03',\n    [bool] $createConnection =  $true\n)\n #region mini window, made by Scripting Guy Blog\n    Function Show-OAuthWindow {\n    Add-Type -AssemblyName System.Windows.Forms\n \n    $form = New-Object -TypeName System.Windows.Forms.Form -Property @{Width=600;Height=800}\n    $web  = New-Object -TypeName System.Windows.Forms.WebBrowser -Property @{Width=580;Height=780;Url=($url -f ($Scope -join \"%20\")) }\n    $DocComp  = {\n            $Global:uri = $web.Url.AbsoluteUri\n            if ($Global:Uri -match \"error=[^&]*|code=[^&]*\") {$form.Close() }\n    }\n    $web.ScriptErrorsSuppressed = $true\n    $web.Add_DocumentCompleted($DocComp)\n    $form.Controls.Add($web)\n    $form.Add_Shown({$form.Activate()})\n    $form.ShowDialog() | Out-Null\n    }\n    #endregion\n\n#login to get an access code \n\nLogin-AzureRmAccount \n\n#select the subscription\n\n$subscription = Select-AzureRmSubscription -SubscriptionId $subscriptionId\n\n#if the connection wasn't alrady created via a deployment\nif($createConnection)\n{\n    $connection = New-AzureRmResource -Properties @{\"api\" = @{\"id\" = \"subscriptions/\" + $subscriptionId + \"/providers/Microsoft.Web/locations/\" + $ResourceLocation + \"/managedApis/\" + $api}; \"displayName\" = $ConnectionName; } -ResourceName $ConnectionName -ResourceType \"Microsoft.Web/connections\" -ResourceGroupName $ResourceGroupName -Location $ResourceLocation -Force\n}\n#else (meaning the conneciton was created via a deployment) - get the connection\nelse{\n$connection = Get-AzureRmResource -ResourceType \"Microsoft.Web/connections\" -ResourceGroupName $ResourceGroupName -ResourceName $ConnectionName\n}\nWrite-Host \"connection status: \" $connection.Properties.Statuses[0]\n\n$parameters = @{\n    \"parameters\" = ,@{\n    \"parameterName\"= \"token\";\n    \"redirectUrl\"= \"https://ema1.exp.azure.com/ema/default/authredirect\"\n    }\n}\n\n#get the links needed for consent\n$consentResponse = Invoke-AzureRmResourceAction -Action \"listConsentLinks\" -ResourceId $connection.ResourceId -Parameters $parameters -Force\n\n$url = $consentResponse.Value.Link \n\n#prompt user to login and grab the code after auth\nShow-OAuthWindow -URL $url\n\n$regex = '(code=)(.*)$'\n    $code  = ($uri | Select-string -pattern $regex).Matches[0].Groups[2].Value\n    Write-output \"Received an accessCode: $code\"\n\nif (-Not [string]::IsNullOrEmpty($code)) {\n    $parameters = @{ }\n    $parameters.Add(\"code\", $code)\n    # NOTE: errors ignored as this appears to error due to a null response\n\n    #confirm the consent code\n    Invoke-AzureRmResourceAction -Action \"confirmConsentCode\" -ResourceId $connection.ResourceId -Parameters $parameters -Force -ErrorAction Ignore\n}\n\n#retrieve the connection\n$connection = Get-AzureRmResource -ResourceType \"Microsoft.Web/connections\" -ResourceGroupName $ResourceGroupName -ResourceName $ConnectionName\nWrite-Host \"connection status now: \" $connection.Properties.Statuses[0]\n`\n```\nReference:\nDeploy Logic Apps & API Connection with ARM \u00b7 in my room (bruttin.com)",
      "question_score": 5,
      "answer_score": 2,
      "created_at": "2021-08-23T14:03:51",
      "url": "https://stackoverflow.com/questions/68892355/unable-to-connect-the-api-connection-to-the-logic-app-via-arm-template-in-terraf"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-azure",
      "question_id": 69903962,
      "title": "How to compare a variable against two strings?",
      "problem": "how to check a variable string with two values in terraform.\nI want to check a string in a variable with two string values something like the below code.\n```\n`virtual_network_type = var.vnet_type == \"External\" || \"Internal\" ? var.vnet_type : null\n\n`\n```\nBut terraform throws below error message\n```\n`Error: Invalid operand\n\u2502 \n\u2502   on ../../modules/test/test-instance.tf line 51, in resource \"azurerm_api_management\" \"apim_demo\":\n\u2502   51:  virtual_network_type = var.vnet_type == \"External\" || \"Internal\" ? var.vnet_type : null\n\u2502 \n\u2502 Unsuitable value for right operand: a bool is required.\n\u2575\n`\n```\nCan we do this in terraform?",
      "solution": "A construct like `var.vnet_type == \"External\" || \"Internal\"` is something that few languages will accept, really.\n`var.vnet_type == \"External\" || var.vnet_type == \"Internal\"\n`\nOn the other hand, is something that will give you a better result.\n\nAnother solution for those kind of use cases, if you don't want to repeat the variable name is to construct a list of the acceptable values and evaluate if the variable is contained in the list.\nSomething like:\n`contains([\"External\", \"Internal\"], var.vnet_type)\n`\n\nSo,\n\neither\n`virtual_network_type = var.vnet_type == \"External\" || var.vnet_type ==  \"Internal\" ? var.vnet_type : null\n`\n\nor\n`virtual_network_type = contains([\"External\", \"Internal\"], var.vnet_type) ? var.vnet_type : null\n`",
      "question_score": 4,
      "answer_score": 10,
      "created_at": "2021-11-09T20:27:38",
      "url": "https://stackoverflow.com/questions/69903962/how-to-compare-a-variable-against-two-strings"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-azure",
      "question_id": 66152036,
      "title": "Terraform conditional argument block",
      "problem": "I have created a module in order deploy the below `azurerm_automation_schedule` resource:\n```\n`resource \"azurerm_automation_schedule\" \"example\" {\n  name                    = var.aaname\n  resource_group_name     = azurerm_resource_group.example.name\n  automation_account_name = azurerm_automation_account.example.name\n  frequency               = var.frequency              \n  interval                = var.interval\n  timezone                = \"Australia/Perth\"\n  start_time              = \"2014-04-15T18:00:15+02:00\"\n  description             = \"This is an example schedule\"\n\n  monthly_occurrence {\n                      day = monthly_occurrence.value.day\n                      occurrence = monthly_occurrence.value.occurrence\n                  }\n }\n`\n```\nThe existence of monthly_occurrence block argument must be conditional as it is only needed when the frequency is \"Month\" otherwise it throws error.\nIs there a way to create this monthly_occurrence block argument conditionally?  I am using terraform 0.13.5 and azurerm 2.38.0\nI have tried with for_each but i couldn't find the solution.  Is there any way to do it?",
      "solution": "You can use dynamic block to make `monthly_occurrence` conditional. For example:\n```\n`resource \"azurerm_automation_schedule\" \"example\" {\n  name                    = var.aaname\n  resource_group_name     = azurerm_resource_group.example.name\n  automation_account_name = azurerm_automation_account.example.name\n  frequency               = var.frequency              \n  interval                = var.interval\n  timezone                = \"Australia/Perth\"\n  start_time              = \"2014-04-15T18:00:15+02:00\"\n  description             = \"This is an example schedule\"\n\n  dynamic \"monthly_occurrence\" {\n    for_each = var.frequency == \"Month\" ? [1] : []\n    content {\n       day = monthly_occurrence.value.day\n       occurrence = monthly_occurrence.value.occurrence\n    }\n  }\n}\n`\n```",
      "question_score": 4,
      "answer_score": 7,
      "created_at": "2021-02-11T10:31:38",
      "url": "https://stackoverflow.com/questions/66152036/terraform-conditional-argument-block"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-azure",
      "question_id": 68168693,
      "title": "Terraform Azure App Service Plan Error - The parameter SKU.Name has an invalid value",
      "problem": "Module declaration\n```\n`resource \"azurerm_app_service_plan\" \"appserviceplan\" {\n  name                = var.name\n  location            = var.location\n  resource_group_name = var.resource_group_name\n  kind                = var.kind\n  reserved            = var.reserved\n  is_xenon            = var.is_xenon\n\n  sku {\n    # name = var.sku_name\n    tier = var.sku_tier\n    size = var.sku_tier\n  }\n}\n`\n```\nCalling the above module like so (in `main.tf`) ...\n```\n`module \"appserviceplan1\" {\n  source = \"../modules/app_service_plan\"\n\n  name                = \"${var.project_name}-${var.environment}-appserviceplan\"\n  location            = var.location\n  resource_group_name = var.resource_group_name\n  kind                = var.asp_kind\n  reserved            = var.asp_reserved\n  is_xenon            = var.asp_is_xenon\n  # sku_name            = var.asp_sku_name\n  sku_tier = var.asp_sku_tier\n  sku_size = var.asp_sku_tier\n}\n`\n```\nInput variable assignment (in `main.auto.tfvars`):\n```\n`# Variable assignment for App Service Plan\nasp_kind = \"xenon\"\n# Recommended value is false\n# Ref: https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/app_service_plan\nasp_reserved = false\nasp_is_xenon = true\n# asp_sku_name = \"P3\"\nasp_sku_tier = \"PremiumV3\"\nasp_sku_size = \"P3v3\"\n`\n```\nGetting the following error with `terraform apply`:\n```\n`Error: Error creating/updating App Service Plan \"xyz-sandbox-appserviceplan\" (Resource Group \"abc-sandbox-xyz-cc-rg\"): web.AppServicePlansClient#CreateOrUpdate: Failure sending request: StatusCode=400 -- Original Error: Code=\"BadRequest\" Message=\"The parameter SKU.Name has an invalid value.\" Details=[{\"Message\":\"The parameter SKU.Name has an invalid value.\"},{\"Code\":\"BadRequest\"},{\"ErrorEntity\":{\"Code\":\"BadRequest\",\"Message\":\"The parameter SKU.Name has an invalid value.\",\"MessageTemplate\":\"The parameter {0} has an invalid value.\",\"Parameters\":[\"SKU.Name\"]}}]\n`\n```\nI have been spinning on this for hours now. Any help would be greatly appreciated!",
      "solution": "Solution: I was using incorrect tier and size.\nUsing the following solved it ..\n```\n`asp_sku_tier = \"P3v3\"\nasp_sku_size = \"P3v3\"\n`\n```",
      "question_score": 4,
      "answer_score": 4,
      "created_at": "2021-06-28T20:53:26",
      "url": "https://stackoverflow.com/questions/68168693/terraform-azure-app-service-plan-error-the-parameter-sku-name-has-an-invalid-v"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-azure",
      "question_id": 66015148,
      "title": "Terraform Unable to list provider",
      "problem": "I am trying to create azure databrick cluster but when i try to run terraform init i see the following error. How can i rectify this. So bascially how to use different provider in terraform.\nTerraform version is Terraform v0.14.5\nWhen i run terraform init i get the following error\n```\n`Initializing the backend...\n\n  Initializing provider plugins...\n        - Finding hashicorp/azurerm versions matching \"~> 2.33\"...\n        - Finding latest version of hashicorp/databricks...\n        - Installing hashicorp/azurerm v2.45.1...\n        - Installed hashicorp/azurerm v2.45.1 (signed by HashiCorp)\n\n       Error: Failed to query available provider packages\n\n           Could not retrieve the list of available versions for provider\n           hashicorp/databricks: provider registry registry.terraform.io does not have a\n           provider named registry.terraform.io/hashicorp/databricks\n\n           If you have just upgraded directly from Terraform v0.12 to Terraform v0.14\n           then please upgrade to Terraform v0.13 first and follow the upgrade guide for\n           that release, which might help you address this problem.\n`\n```",
      "solution": "if you specify provider as simply `databrics`, then it's resolved into the `hashicorp` namespace (as you can see in the message: `Finding latest version of hashicorp/databricks...`).  To resolve it correctly, you need to put its definition into the `required_providers` block of Terraform itself, like this (you can copy it from the \"USE PROVIDER\" button in documentation):\n```\n`terraform {\n  required_providers {\n    databricks = {\n      source  = \"databrickslabs/databricks\"\n      version = \"0.3.0\"\n    }\n  }\n}\n`\n```",
      "question_score": 4,
      "answer_score": 6,
      "created_at": "2021-02-02T18:59:27",
      "url": "https://stackoverflow.com/questions/66015148/terraform-unable-to-list-provider"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-azure",
      "question_id": 69019459,
      "title": "How to create alert rule in terraform for SQL DB",
      "problem": "I can't seem to find any examples and I am running into different errors depending on what I'm doing.\nI'm trying to get this to work and it's just not happening... any thoughts?\n```\n`resource \"azurerm_monitor_metric_alert\" \"example\" {\n  name                = \"example-metricalert\"\n  resource_group_name = azurerm_resource_group.example.name\n  scopes              = [azurerm_mssql_database.test.server_id]\n  description         = \"Action will be triggered when cpu is greater than 80%.\"\n\n  criteria {\n    metric_namespace = \"Microsoft.Sql/servers/databases\"\n    metric_name      = \"CPU_percentage\"\n    aggregation      = \"Average\"\n    operator         = \"GreaterThan\"\n    threshold        = 80\n}\n\n}\n`\n```",
      "solution": "You can use the below code to create an metrics alert for SQL DB. I have tested it for an existing SQL DB, so used data blocks.\nMain.tf\n```\n`   provider \"azurerm\" {\n  features {}\n}\n\ndata \"azurerm_mssql_server\" \"example\" {\n  name                = \"ztestansumanserver\"\n  resource_group_name = \"yourresourcegroup\"\n}\n\ndata \"azurerm_mssql_database\" \"dbtomonitor\" {\n  name      = \"testansumandb\"\n  server_id = data.azurerm_mssql_server.example.id\n}\n\nresource \"azurerm_monitor_action_group\" \"example\" {\n  name                = \"CriticalAlertsAction\"\n  resource_group_name = data.azurerm_mssql_server.example.resource_group_name\n  short_name          = \"p0action\"\n\n  email_receiver {\n    name                    = \"sendtoadmin\"\n    email_address           = \"youremailid\"\n    use_common_alert_schema = true\n  }\n}\n\nresource \"azurerm_monitor_metric_alert\" \"example\" {\n  name                = \"example-metricalert\"\n  resource_group_name = data.azurerm_mssql_server.example.resource_group_name\n  scopes              = [data.azurerm_mssql_database.dbtomonitor.id]\n  description         = \"Action will be triggered when cpu percent is greater than 80.\"\n\n  criteria {\n    metric_namespace = \"Microsoft.Sql/servers/databases\"\n    metric_name      = \"cpu_percent\"\n    aggregation      = \"Average\"\n    operator         = \"GreaterThan\"\n    threshold        = 80\n  }\n action {\n    action_group_id = azurerm_monitor_action_group.example.id\n  }\n}\n`\n```\noutput:\n\nNote: As per the above script alert is created successfully and it will also trigger a mail to you when the `cpu_percent > 80` .\nReference:\nAzure Monitor supported metrics by resource type - Azure Monitor | Microsoft Docs",
      "question_score": 4,
      "answer_score": 5,
      "created_at": "2021-09-01T21:17:56",
      "url": "https://stackoverflow.com/questions/69019459/how-to-create-alert-rule-in-terraform-for-sql-db"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-azure",
      "question_id": 75185362,
      "title": "AuthorizationFailed - Creating Role Assignments in Azure",
      "problem": "I keep getting the AuthorizationFailed error when I try creating managed identity and assigning role assignments.\nI have done this in the portal, but replicating in terraform has been a pain.\n```\n`# User Assigned Managed Identity\nresource \"azurerm_user_assigned_identity\" \"managed-id\" {\n  resource_group_name = var.resource_group\n  location            = var.location\n  name                = var.name\n  tags                = var.tags\n}\n\nresource \"azurerm_role_assignment\" \"rg\" {\n  scope                = data.azurerm_resource_group.rg.id\n  role_definition_name = \"Contributor\"\n  principal_id         = azurerm_user_assigned_identity.managed-id.id\n}\n\nresource \"azurerm_role_assignment\" \"vnet\" {\n  scope                = data.azurerm_virtual_network.vnet.id\n  role_definition_name = \"Network Contributor\"\n  principal_id         = azurerm_user_assigned_identity.managed-id.id\n}\n\nresource \"azurerm_role_assignment\" \"dns\" {\n  count                = \"${var.create_dns_ra ? 1 : 0}\"\n  scope                = data.azurerm_subscription.sub.id\n  role_definition_name = \"Private DNS Zone Contributor\"\n  principal_id         = azurerm_user_assigned_identity.managed-id.id\n}\n\n`\n```\nAfter the terraform apply, this is the error for the rg role assignment resource:\n```\n`Error: authorization.RoleAssignmentsClient#Create: Failure responding to request: StatusCode=403 -- Original Error: autorest/azure: Service returned an error. Status=403 Code=\"AuthorizationFailed\" Message=\"The client '9219bxxx-xxxx-xxxx-xxxx-xxxxxxxx' with object id '9219xxxx-xxxx-xxxx-xxxx-xxxxxxxx' does not have authorization to perform action 'Microsoft.Authorization/roleAssignments/write' over scope '/subscriptions/4c4xxxx-xxxx-xxxx-xxxx-xxxxxxxx/resourceGroups/test-RG/providers/Microsoft.Authorization/roleAssignments/086bxxxx-xxxx-xxxx-xxxx-xxxxxxxx' or the scope is invalid. If access was recently granted, please refresh your credentials.\"\n`\n```\nSimilar error for the vnet role assignment resource:\n```\n`Error: authorization.RoleAssignmentsClient#Create: Failure responding to request: StatusCode=403 -- Original Error: autorest/azure: Service returned an error. Status=403 Code=\"AuthorizationFailed\" Message=\"The client '9219bxxx-xxxx-xxxx-xxxx-xxxxxxxx' with object id '9219bxxx-xxxx-xxxx-xxxx-xxxxxxxx' does not have authorization to perform action 'Microsoft.Authorization/roleAssignments/write' over scope '/subscriptions/4c4xxxx-xxxx-xxxx-xxxx-xxxxxxxx/resourceGroups/test-RG/providers/Microsoft.Network/virtualNetworks/test-RG-vnet/providers/Microsoft.Authorization/roleAssignments/55adxxxx-xxxx-xxxx-xxxx-xxxxxxxx' or the scope is invalid. If access was recently granted, please refresh your credentials.\"\n`\n```\nI don't know what I need to get this going, but I'd appreciate any suggestions or solutions to this. Thanks",
      "solution": "To create role assignments, you need to assign either User Access Administrator or Owner role to your service principal that includes this permission: \"Microsoft.Authorization/roleAssignments/write\"\n\nI tried to reproduce the same in my environment via Postman and got below results:\nI used below query to create role assignments for resource group and got same error as you like this:\n`PUT https://management.azure.com/subscriptions//resourceGroups//providers/Microsoft.Authorization/roleAssignments/xxxxxxxxxxxxxxxxxxxx?api-version=2022-04-01\n{\n  \"properties\": {\n    \"roleDefinitionId\": \"/subscriptions//resourceGroups//providers/Microsoft.Authorization/roleDefinitions/b24988ac-6180-42a0-ab88-20f7382dd24c\",\n    \"principalId\": \"ca1xxx18-7561-4b98-987d-ee51xxxxd7\"\n  }\n}\n`\nResponse:\n\nI got similar error when I tried to create role assignments for `VNet` too like below:\n`PUT https://management.azure.com/subscriptions//resourceGroups//providers/Microsoft.Network/virtualNetworks/srivnet/providers/Microsoft.Authorization/roleAssignments/xxxxxxxxxxxxxxxxxxxx?api-version=2022-04-01\n{\n  \"properties\": {\n    \"roleDefinitionId\": \"/subscriptions//resourceGroups//providers/Microsoft.Network/virtualNetworks/srivnet/providers/Microsoft.Authorization/roleDefinitions/b24988ac-6180-42a0-ab88-20f7382dd24c\",\n    \"principalId\": \"ca1xxx18-7561-4b98-987d-ee51xxxxd7\"\n  }\n}\n`\nResponse:\n\nTo resolve the error, I assigned `Owner` role to the service principal under subscription like below:\n\nAfter assigning that role, role assignments created successfully on resource group when I ran below query again:\n\nIn your case, try assigning your service principal Owner role under subscription to resolve the issue.\nIf you feel Owner role is more permissive, it's better to create custom RBAC role with \"Microsoft.Authorization/roleAssignments/write\" permission as suggested in below link.\nReference:\nAuthorization failed when when writing a roleAssignment - Microsoft Q&A by AmanpreetSingh-MSFT",
      "question_score": 4,
      "answer_score": 4,
      "created_at": "2023-01-20T15:28:10",
      "url": "https://stackoverflow.com/questions/75185362/authorizationfailed-creating-role-assignments-in-azure"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-azure",
      "question_id": 73571891,
      "title": "Terraform - How To Flatten A List With In a List Of Objects",
      "problem": "I am trying to set up my DNS zones and records with in those zones and I can't get the structured collection I am looking for to pass into the tf file that will generate all the zones and records. I have a list of DNS objects and each object will have multiple lists of record objects. I am trying to create lists of just records (a, cname, txt, ...). Im trying merges and flattening and I just not sure how to get what I need. I keep trying different patterns with merge and flatten but keep getting errors in different configurations.\nErrors:\n```\n`Error: Invalid 'for' expression. Extra characters after the end of the 'for' expression.\nError: Missing attribute value. Expected an attribute value, introduced by an equals sign (\"=\").\nError: Invalid 'for' expression. Key expression is required when building an object.\nError: Missing key/value separator. Expected an equals sign (\"=\") to mark the beginning of the attribute value.\n`\n```\nVariable Definition:\n```\n`variable \"dns_target_child_zones\" {\n  type = list(object({\n    name      = string\n    a_records = list(object({\n      name        = string\n      ttl         = number\n      ip_address  = string\n    }))\n    cname_records = list(object({\n      name        = string\n      ttl         = number\n      record      = string\n    }))\n    mx_records = list(object({\n      name        = string\n      ttl         = number\n      record      = list(object({\n        preference  = number\n        exchange    = string\n      }))\n    }))\n    txt_records = list(object({\n      name        = string\n      ttl         = number\n      record      = list(object({\n        value  = string\n      }))\n    }))\n  }))\n}\n`\n```\nLocals file:\n```\n`locals {\n  a_records = flatten ([\n\n  ])\n  cname_records = flatten ([\n\n  ])\n  mx_records = flatten ([\n\n  ])\n  txt_records = flatten ([\n\n  ])        \n}\n`\n```\nThe output structure I am trying to produce so I can do something like count = length(locals.a_records)\n```\n`[\n {\n   name                = \n   zone_name           = \n   ttl                 = \n   records             = \n },\n {\n   name                = \n   zone_name           = \n   ttl                 = \n   records             = \n }\n ...\n]\n`\n```",
      "solution": "Here's an example of the local object you could create for a_records (note: I included the ip_address attribute but you have records which doesn't appear in the a_records variable object):\n```\n`  a_records = flatten([\n    for v in var.dns_target_child_zones : [\n      for record in v.a_records : {\n        name       = record.name\n        zone_name  = v.name\n        ttl        = record.ttl\n        ip_address = record.ip_address\n      }\n    ]\n  ])\n`\n```\nThis will product an output like the following:\n```\n`  + a_records = [\n  + {\n      + ip_address = \"ip1\"\n      + name       = \"a1\"\n      + ttl        = 10\n      + zone_name  = \"dns1\"\n    },\n  + {\n      + ip_address = \"ip2\"\n      + name       = \"a2\"\n      + ttl        = 10\n      + zone_name  = \"dns1\"\n    },\n  ]\n`\n```\nI used a default value and an output to test the expected result. Here's my full test, which you can run terraform plan for to show the output\n```\n`variable \"dns_target_child_zones\" {\n  type = list(object({\n    name = string\n    a_records = list(object({\n      name       = string\n      ttl        = number\n      ip_address = string\n    }))\n    cname_records = list(object({\n      name   = string\n      ttl    = number\n      record = string\n    }))\n    mx_records = list(object({\n      name = string\n      ttl  = number\n      record = list(object({\n        preference = number\n        exchange   = string\n      }))\n    }))\n    txt_records = list(object({\n      name = string\n      ttl  = number\n      record = list(object({\n        value = string\n      }))\n    }))\n  }))\n  default = [{\n    name = \"dns1\",\n    a_records = [{\n      name       = \"a1\",\n      ttl        = 10\n      ip_address = \"ip1\"\n      },\n      {\n        name       = \"a2\",\n        ttl        = 10\n        ip_address = \"ip2\"\n      }\n    ],\n    cname_records = [{\n      name   = \"cname1\",\n      ttl    = 10\n      record = \"rec1\"\n    }],\n    mx_records = [{\n      name = \"mx1\",\n      ttl  = 10,\n      record = [{\n        preference = 1,\n        exchange   = \"exchange1\"\n        }, {\n        preference = 2,\n        exchange   = \"exchange2\"\n      }]\n    }],\n    txt_records = [{\n      name = \"txt1\",\n      ttl  = 10,\n      record = [{\n        value = \"val1\"\n      }]\n    }]\n  }]\n}\n\nlocals {\n    a_records = flatten([\n        for v in var.dns_target_child_zones: [\n            for record in v.a_records: {\n                name       = record.name\n                zone_name  = v.name\n                ttl        = record.ttl\n                ip_address = record.ip_address\n            }\n        ]\n    ])\n}\n\noutput \"a_records\" {\n    value = local.a_records\n}\n`\n```\nFor further nesting extraction e.g with mx_records then you can add another for loop in your local like the below\n```\n`  mx_records = flatten([\n    for v in var.dns_target_child_zones : [\n      for mx in v.mx_records : [\n        for r in mx.record : {\n          name       = mx.name\n          zone_name  = v.name\n          ttl        = mx.ttl\n          preference = r.preference\n          exchange   = r.exchange\n        }\n      ]\n    ]\n  ])\n`\n```\nThe above local outputs this result\n```\n`  + mx_records = [\n      + {\n          + exchange   = \"exchange1\"\n          + name       = \"mx1\"\n          + preference = 1\n          + ttl        = 10\n          + zone_name  = \"dns1\"\n        },\n      + {\n          + exchange   = \"exchange2\"\n          + name       = \"mx1\"\n          + preference = 2\n          + ttl        = 10\n          + zone_name  = \"dns1\"\n        },\n    ]\n`\n```",
      "question_score": 4,
      "answer_score": 4,
      "created_at": "2022-09-01T17:23:49",
      "url": "https://stackoverflow.com/questions/73571891/terraform-how-to-flatten-a-list-with-in-a-list-of-objects"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-azure",
      "question_id": 75247192,
      "title": "What&#39;s the correct way to setup AKS cluster Static IP, Load Balancer and Ingress Controller?",
      "problem": "It's now quite a few days that I'm trying to configure the cluster on AKS but I keep jumping between parts of the docs, various questions here on SO, articles on Medium.. all to keep failing at it.\nThe goal is get a static ip with a dns that I can use to connect my apps to the server deployed on AKS.\nI have created via terraform the infrastructure which consists of a resource group in which I created a Public IP and the AKS cluster, so far so good.\n\nAfter trying to use the ingress controller that gets installed when you use the option `http_application_routing_enabled = true` on cluster creation which the docs are discouraging for production https://learn.microsoft.com/en-us/azure/aks/http-application-routing, I'm trying the recommended way and install the ingress-nginx controller via Helm https://learn.microsoft.com/en-us/azure/aks/ingress-basic?tabs=azure-cli.\nIn terraform I'm installing it all like this\nresource group and cluster\n```\n`resource \"azurerm_resource_group\" \"resource_group\" {\n  name     = var.resource_group_name\n  location = var.location\n    tags = {\n    Environment = \"Test\"\n    Team = \"DevOps\"\n  }\n}\n\nresource \"azurerm_kubernetes_cluster\" \"server_cluster\" {\n  name                = \"server_cluster\"\n  location            = azurerm_resource_group.resource_group.location\n  resource_group_name = azurerm_resource_group.resource_group.name\n  dns_prefix          = \"fixit\"\n  kubernetes_version = var.kubernetes_version\n  # sku_tier = \"Paid\"\n\n  default_node_pool {\n    name       = \"default\"\n    node_count = 1\n    min_count = 1\n    max_count = 3\n    # vm_size    = \"standard_b2s_v5\"\n    # vm_size    = \"standard_e2bs_v5\"\n    vm_size    = \"standard_b4ms\"\n    type = \"VirtualMachineScaleSets\"\n    enable_auto_scaling = true\n    enable_host_encryption = false\n    # os_disk_size_gb = 30\n\n    \n    # enable_node_public_ip = true\n\n  }\n\n  service_principal {\n    client_id = var.sp_client_id\n    client_secret = var.sp_client_secret\n  }\n\n  tags = {\n    Environment = \"Production\"\n  }\n\n  linux_profile {\n    admin_username = \"azureuser\"\n    ssh_key {\n        key_data = var.ssh_key\n    }\n  }\n  network_profile {\n      network_plugin = \"kubenet\"\n      load_balancer_sku = \"standard\"\n      # load_balancer_sku = \"basic\"\n    \n  }\n  # http_application_routing_enabled = true\n  http_application_routing_enabled = false\n\n  \n}\n\n`\n```\npublic ip\n```\n`resource \"azurerm_public_ip\" \"public-ip\" {\n  name                = \"fixit-public-ip\"\n  location            = var.location\n  resource_group_name = var.resource_group_name\n  allocation_method   = \"Static\"\n  domain_name_label = \"fixit\"\n  sku = \"Standard\"\n}\n`\n```\nload balancer\n```\n`resource \"kubernetes_service\" \"cluster-ingress\" {\n  metadata {\n    name = \"cluster-ingress-svc\"\n    annotations = {\n      \"service.beta.kubernetes.io/azure-load-balancer-resource-group\" = \"fixit-resource-group\"\n\n      # Warning  SyncLoadBalancerFailed  2m38s (x8 over 12m)  service-controller  Error syncing load balancer: \n      # failed to ensure load balancer: findMatchedPIPByLoadBalancerIP: cannot find public IP with IP address 52.157.90.236 \n      # in resource group MC_fixit-resource-group_server_cluster_westeurope\n\n      # \"service.beta.kubernetes.io/azure-load-balancer-resource-group\" = \"MC_fixit-resource-group_server_cluster_westeurope\"\n\n      # kubernetes.io/ingress.class: addon-http-application-routing\n    }\n  }\n  spec {\n    # type = \"Ingress\"\n    type = \"LoadBalancer\"\n    load_balancer_ip = var.public_ip_address\n    selector = {\n      name = \"cluster-ingress-svc\"\n    }\n    port {\n      name = \"cluster-port\"\n      protocol = \"TCP\"\n      port = 3000\n      target_port = \"80\"\n    }\n\n  }\n}\n\n`\n```\ningress controller\n```\n`resource \"helm_release\" \"nginx\" {\n  name = \"ingress-nginx\"\n  repository = \"https://kubernetes.github.io/ingress-nginx\"\n  chart = \"ingress-nginx\"\n  namespace = \"default\"\n\n  set {\n    name  = \"rbac.create\"\n    value = \"false\"\n  }\n\n  set {\n    name  = \"controller.service.externalTrafficPolicy\"\n    value = \"Local\"\n  }\n\n  set {\n    name  = \"controller.service.loadBalancerIP\"\n    value = var.public_ip_address\n  }\n\n  set {\n    name  = \"controller.service.annotations.service.beta.kubernetes.io/azure-load-balancer-internal\"\n    value = \"true\"\n  }\n  \n#   --set controller.service.annotations.\"service\\.beta\\.kubernetes\\.io/azure-load-balancer-health-probe-request-path\"=/healthz\n\n  set {\n    name  = \"controller.service.annotations.service\\\\.beta\\\\.kubernetes\\\\.io/azure-load-balancer-health-probe-request-path\"\n    value = \"/healthz\"\n  }\n} \n`\n```\nbut the installation fails with this message from terraform\n```\n`Warning: Helm release \"ingress-nginx\" was created but has a failed status. Use the `helm` command to investigate the error, correct it, then run Terraform again.\n\u2502 \n\u2502   with module.ingress_controller.helm_release.nginx,\n\u2502   on modules/ingress_controller/controller.tf line 2, in resource \"helm_release\" \"nginx\":\n\u2502    2: resource \"helm_release\" \"nginx\" {\n\u2502 \n\u2575\n\u2577\n\u2502 Error: timed out waiting for the condition\n\u2502 \n\u2502   with module.ingress_controller.helm_release.nginx,\n\u2502   on modules/ingress_controller/controller.tf line 2, in resource \"helm_release\" \"nginx\":\n\u2502    2: resource \"helm_release\" \"nginx\" {\n`\n```\nthe controller print out\n```\n`vincenzocalia@vincenzos-MacBook-Air helm_charts % kubectl describe svc ingress-nginx-controller\nName:                     ingress-nginx-controller\nNamespace:                default\nLabels:                   app.kubernetes.io/component=controller\n                          app.kubernetes.io/instance=ingress-nginx\n                          app.kubernetes.io/managed-by=Helm\n                          app.kubernetes.io/name=ingress-nginx\n                          app.kubernetes.io/part-of=ingress-nginx\n                          app.kubernetes.io/version=1.5.1\n                          helm.sh/chart=ingress-nginx-4.4.2\nAnnotations:              meta.helm.sh/release-name: ingress-nginx\n                          meta.helm.sh/release-namespace: default\n                          service: map[beta:map[kubernetes:map[io/azure-load-balancer-internal:true]]]\n                          service.beta.kubernetes.io/azure-load-balancer-health-probe-request-path: /healthz\nSelector:                 app.kubernetes.io/component=controller,app.kubernetes.io/instance=ingress-nginx,app.kubernetes.io/name=ingress-nginx\nType:                     LoadBalancer\nIP Family Policy:         SingleStack\nIP Families:              IPv4\nIP:                       10.0.173.243\nIPs:                      10.0.173.243\nIP:                       52.157.90.236\nPort:                     http  80/TCP\nTargetPort:               http/TCP\nNodePort:                 http  31709/TCP\nEndpoints:                \nPort:                     https  443/TCP\nTargetPort:               https/TCP\nNodePort:                 https  30045/TCP\nEndpoints:                \nSession Affinity:         None\nExternal Traffic Policy:  Local\nHealthCheck NodePort:     32500\nEvents:\n  Type     Reason                  Age                 From                Message\n  ----     ------                  ----                ----                -------\n  Normal   EnsuringLoadBalancer    32s (x5 over 108s)  service-controller  Ensuring load balancer\n  Warning  SyncLoadBalancerFailed  31s (x5 over 107s)  service-controller  Error syncing load balancer: failed to ensure load balancer: findMatchedPIPByLoadBalancerIP: cannot find public IP with IP address 52.157.90.236 in resource group mc_fixit-resource-group_server_cluster_westeurope\n\nvincenzocalia@vincenzos-MacBook-Air helm_charts % az aks show --resource-group fixit-resource-group --name server_cluster --query nodeResourceGroup -o tsv\nMC_fixit-resource-group_server_cluster_westeurope\n`\n```\nWhy is it looking in the `MC_fixit-resource-group_server_cluster_westeurope` resource group and not in the `fixit-resource-group` I created for the Cluster, Public IP and Load Balancer?\nIf I change the controller load balancer ip to the public ip in `MC_fixit-resource-group_server_cluster_westeurope` then terraform still outputs the same error, but the controller prints out to be correctly assigned to the ip and load balancer\n```\n`set {\n    name  = \"controller.service.loadBalancerIP\"\n    value = \"20.73.192.77\" #var.public_ip_address\n  }\n`\n```\n```\n`vincenzocalia@vincenzos-MacBook-Air helm_charts % kubectl get svc\nNAME                                 TYPE           CLUSTER-IP     EXTERNAL-IP     PORT(S)                      AGE\ncluster-ingress-svc                  LoadBalancer   10.0.110.114   52.157.90.236   3000:31863/TCP               104m\ningress-nginx-controller             LoadBalancer   10.0.106.201   20.73.192.77    80:30714/TCP,443:32737/TCP   41m\ningress-nginx-controller-admission   ClusterIP      10.0.23.188              443/TCP                      41m\nkubernetes                           ClusterIP      10.0.0.1                 443/TCP                      122m\nvincenzocalia@vincenzos-MacBook-Air helm_charts % kubectl describe svc ingress-nginx-controller\nName:                     ingress-nginx-controller\nNamespace:                default\nLabels:                   app.kubernetes.io/component=controller\n                          app.kubernetes.io/instance=ingress-nginx\n                          app.kubernetes.io/managed-by=Helm\n                          app.kubernetes.io/name=ingress-nginx\n                          app.kubernetes.io/part-of=ingress-nginx\n                          app.kubernetes.io/version=1.5.1\n                          helm.sh/chart=ingress-nginx-4.4.2\nAnnotations:              meta.helm.sh/release-name: ingress-nginx\n                          meta.helm.sh/release-namespace: default\n                          service: map[beta:map[kubernetes:map[io/azure-load-balancer-internal:true]]]\n                          service.beta.kubernetes.io/azure-load-balancer-health-probe-request-path: /healthz\nSelector:                 app.kubernetes.io/component=controller,app.kubernetes.io/instance=ingress-nginx,app.kubernetes.io/name=ingress-nginx\nType:                     LoadBalancer\nIP Family Policy:         SingleStack\nIP Families:              IPv4\nIP:                       10.0.106.201\nIPs:                      10.0.106.201\nIP:                       20.73.192.77\nLoadBalancer Ingress:     20.73.192.77\nPort:                     http  80/TCP\nTargetPort:               http/TCP\nNodePort:                 http  30714/TCP\nEndpoints:                \nPort:                     https  443/TCP\nTargetPort:               https/TCP\nNodePort:                 https  32737/TCP\nEndpoints:                \nSession Affinity:         None\nExternal Traffic Policy:  Local\nHealthCheck NodePort:     32538\nEvents:\n  Type    Reason                Age                From                Message\n  ----    ------                ----               ----                -------\n  Normal  EnsuringLoadBalancer  39m (x2 over 41m)  service-controller  Ensuring load balancer\n  Normal  EnsuredLoadBalancer   39m (x2 over 41m)  service-controller  Ensured load balancer\nvincenzocalia@vincenzos-MacBook-Air helm_charts % \n`\n```\nReading here https://learn.microsoft.com/en-us/azure/aks/faq#why-are-two-resource-groups-created-with-aks\n\nTo enable this architecture, each AKS deployment spans two resource groups:\nYou create the first resource group. This group contains only the Kubernetes service resource. The AKS resource provider automatically creates the second resource group during deployment. An example of the second resource group is MC_myResourceGroup_myAKSCluster_eastus. For information on how to specify the name of this second resource group, see the next section.\nThe second resource group, known as the node resource group, contains all of the infrastructure resources associated with the cluster. These resources include the Kubernetes node VMs, virtual networking, and storage. By default, the node resource group has a name like MC_myResourceGroup_myAKSCluster_eastus. AKS automatically deletes the node resource group whenever the cluster is deleted, so it should only be used for resources that share the cluster's lifecycle.\n\nShould I pass the first or the second group depending of what kind of resource I'm creating?\nE.g. `kubernetes_service` needs 1st rg, while `azurerm_public_ip` needs the 2nd rg?\nWhat is it that I'm missing out here?\nPlease explain it like I was 5 years old because I'm feeling like right now..\nMany thanks",
      "solution": "Finally found what the problem was.\nIndeed the `Public IP` needs to be created in the `node resource group`  because the ingress controller, with the `loadBalancerIP` assigned to the `Public IP` address, is going to look for it in the `node resource group` so if you create it in the `resource group` fails with the error I was getting.\nThe node resource group name is assigned at cluster creation eg. `MC_myResourceGroup_myAKSCluster_eastus`, but you can name it  as you wish using the parameter `node_resource_group = var.node_resource_group_name`.\nAlso, the Public IP `sku` `\"Standard\"` (to be specified) or `\"Basic\"` ( default), and the cluster `load_balancer_sku` `\"standard\"` or `\"basic\"`(no default value her, it needs to be specified) have to match.\nI also put the Public IP in the cluster module so it can depend on it, to avoid being created before it and failing as the `node resource group` has not been created yet, couldn't set that dependency correctly in `main.tf` file.\nSo the working configuration is now:\nmain\n```\n`terraform {\n  required_version = \">=1.1.0\"\n  required_providers {\n    azurerm = {\n      source = \"hashicorp/azurerm\"\n       version = \"~> 3.0.2\"\n    }\n  }\n}\n\nprovider \"azurerm\" {\n  features {\n    resource_group {\n      prevent_deletion_if_contains_resources = false\n    }\n  }\n  subscription_id   = var.azure_subscription_id\n  tenant_id         = var.azure_subscription_tenant_id\n  client_id         = var.service_principal_appid\n  client_secret     = var.service_principal_password\n}\n\nprovider \"kubernetes\" {\n  host = \"${module.cluster.host}\"\n  client_certificate = \"${base64decode(module.cluster.client_certificate)}\"\n  client_key = \"${base64decode(module.cluster.client_key)}\"\n  cluster_ca_certificate = \"${base64decode(module.cluster.cluster_ca_certificate)}\"\n}\n\nprovider \"helm\" {\n  kubernetes {\n    host                   = \"${module.cluster.host}\"\n    client_certificate     = \"${base64decode(module.cluster.client_certificate)}\"\n    client_key             = \"${base64decode(module.cluster.client_key)}\"\n    cluster_ca_certificate = \"${base64decode(module.cluster.cluster_ca_certificate)}\"\n  }\n}\n\nmodule \"cluster\" {\n  source = \"./modules/cluster\"\n  location = var.location\n  vm_size = var.vm_size\n  resource_group_name = var.resource_group_name\n  node_resource_group_name = var.node_resource_group_name\n  kubernetes_version = var.kubernetes_version\n  ssh_key = var.ssh_key\n  sp_client_id = var.service_principal_appid\n  sp_client_secret = var.service_principal_password\n}\n\nmodule \"ingress-controller\" {\n  source = \"./modules/ingress-controller\"\n  public_ip_address = module.cluster.public_ip_address\n  depends_on = [\n    module.cluster.public_ip_address\n  ]\n}\n`\n```\ncluster\n```\n`resource \"azurerm_resource_group\" \"resource_group\" {\n  name     = var.resource_group_name\n  location = var.location\n    tags = {\n    Environment = \"test\"\n    Team = \"DevOps\"\n  }\n}\nresource \"azurerm_kubernetes_cluster\" \"server_cluster\" {\n  name                = \"server_cluster\"\n  ### choose the resource goup to use for the cluster\n  location            = azurerm_resource_group.resource_group.location\n  resource_group_name = azurerm_resource_group.resource_group.name\n  ### decide the name of the cluster \"node\" resource group, if unset will be named automatically \n  node_resource_group = var.node_resource_group_name\n  dns_prefix          = \"fixit\"\n  kubernetes_version = var.kubernetes_version\n  # sku_tier = \"Paid\"\n\n  default_node_pool {\n    name       = \"default\"\n    node_count = 1\n    min_count = 1\n    max_count = 3\n    vm_size = var.vm_size\n\n    type = \"VirtualMachineScaleSets\"\n    enable_auto_scaling = true\n    enable_host_encryption = false\n    # os_disk_size_gb = 30\n  }\n\n  service_principal {\n    client_id = var.sp_client_id\n    client_secret = var.sp_client_secret\n  }\n\n  tags = {\n    Environment = \"Production\"\n  }\n\n  linux_profile {\n    admin_username = \"azureuser\"\n    ssh_key {\n        key_data = var.ssh_key\n    }\n  }\n  network_profile {\n      network_plugin = \"kubenet\"\n      load_balancer_sku = \"basic\" \n    \n  }\n  http_application_routing_enabled = false\n  depends_on = [\n    azurerm_resource_group.resource_group\n  ]\n}\n\nresource \"azurerm_public_ip\" \"public-ip\" {\n  name                = \"fixit-public-ip\"\n  location            = var.location\n  # resource_group_name = var.resource_group_name\n  resource_group_name = var.node_resource_group_name\n  allocation_method   = \"Static\"\n  domain_name_label = \"fixit\"\n  # sku = \"Standard\"\n\ndepends_on = [\n  azurerm_kubernetes_cluster.server_cluster\n]\n}\n`\n```\ningress controller\n```\n`resource \"helm_release\" \"nginx\" {\n  name      = \"ingress-nginx\"\n  repository = \"ingress-nginx\"\n  chart     = \"ingress-nginx/ingress-nginx\"\n  namespace = \"default\"\n\n  set {\n    name  = \"controller.service.externalTrafficPolicy\"\n    value = \"Local\"\n  }\n\n  set {\n    name = \"controller.service.annotations.service.beta.kubernetes.io/azure-load-balancer-internal\"\n    value = \"true\"\n  }\n\n  set {\n    name  = \"controller.service.loadBalancerIP\"\n    value = var.public_ip_address\n  }\n\n  set {\n    name  = \"controller.service.annotations.service.beta.kubernetes.io/azure-load-balancer-health-probe-request-path\"\n    value = \"/healthz\"\n  }\n} \n`\n```\ningress service\n```\n`apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: ingress-service\n  # namespace: default\n  annotations:\n    nginx.ingress.kubernetes.io/ssl-redirect: \"false\"\n    nginx.ingress.kubernetes.io/use-regex: \"true\"\n    nginx.ingress.kubernetes.io/rewrite-target: /$2$3$4\nspec:\n  ingressClassName: nginx\n  rules:\n    # - host: fixit.westeurope.cloudapp.azure.com #dns from Azure PublicIP\n\n### Node.js server\n  - http:\n      paths:\n      - path: /(/|$)(.*)\n        pathType: Prefix\n        backend:\n          service:\n            name: server-clusterip-service\n            port:\n              number: 80 \n\n  - http:\n      paths:\n      - path: /server(/|$)(.*)\n        pathType: Prefix\n        backend:\n          service:\n            name: server-clusterip-service\n            port:\n              number: 80\n...\n\nother services omitted\n`\n```\nHope this can help others having difficulties in getting the setup right.\nCheers.",
      "question_score": 4,
      "answer_score": 3,
      "created_at": "2023-01-26T15:04:00",
      "url": "https://stackoverflow.com/questions/75247192/whats-the-correct-way-to-setup-aks-cluster-static-ip-load-balancer-and-ingress"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-azure",
      "question_id": 74206482,
      "title": "Permissions to purge Azure Key Vault on delete",
      "problem": "I'm building a Terraform infra with Azure DevOps, and I have a key vault in my infra. when trying to destroy the environment (locally or in the pipeline) terraform returns the following error:\n```\n`Error: keyvault.VaultsClient#PurgeDeleted: Failure sending request: StatusCode=403\n     -- Original Error: Code=\"AuthorizationFailed\" Message=\"The client 'my-email' with\n     object id 'my-object-id' does not have authorization to perform action\n     'Microsoft.KeyVault/locations/deletedVaults/purge/action' over scope\n     '/subscriptions/subscription-id' or the scope is invalid.\n     If access was recently granted, please refresh your credentials.\"\n`\n```\nThis error shows although I have `owner/contributor roles over the resource group where this kv is provisioned`, and I have `Key Vault adminitrator/contributor in the subscription level`. Can someone enlighten me on what role (more restricted is better) is needed to avoid this issue in the future?\nThanks",
      "solution": "I tried to reproduce the same in my environment and got the below results:\nI created one test user and assigned same roles as you like below:\n\nNow I logged in to Azure Portal using test user's credentials and tried to purge the deleted key vault as below:\nGo to Azure Portal -> Key vaults -> Manage deleted vaults -> Select Subscription -> Select key vault -> Purge -> Delete\n\nWhen I clicked on Delete, it gave me same error as you like below:\n\nTo purge a soft deleted key vault, user requires `role` that\nincludes permission like\nMicrosoft.KeyVault/locations/deletedVaults/purge/action. Please note that, only `Subscription Owner` will have that permission.\n\nTo resolve the error, you need to assign Owner role to the user at subscription level.\n\nWhen I tried the same after getting `Subscription Owner` role, I'm able to purge that deleted key vault successfully like below:\n\nIf you don't want to assign `Subscription Owner` role, you can create a custom RBAC role by including required permissions and assign it to the user based on your requirement.\nReferences:\nAzure Key Vault recovery overview | Microsoft \nAzure custom roles - Azure RBAC | Microsoft",
      "question_score": 4,
      "answer_score": 3,
      "created_at": "2022-10-26T12:44:30",
      "url": "https://stackoverflow.com/questions/74206482/permissions-to-purge-azure-key-vault-on-delete"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-azure",
      "question_id": 71645084,
      "title": "Terraform: how can we update a tag only if modification is detected",
      "problem": "I implemented tags in my terraform files with a field Lastupdate like below\n\nThe problem is that whenever i apply my tf file, this field is always updated even if there is no change. I would like this tag beeing updated only if a change is detected.\nDo you know how can i achieve this ?\nMy code looks like\n```\n`resource \"azurerm_app_service_plan\" \"terra_app_plan\" {\n  resource_group_name = azurerm_resource_group.terra_resource_group.name\n  location            = azurerm_resource_group.terra_resource_group.location\n  name                = local.app_serviceplan_name\n\n  sku {\n    size = var.app_serviceplan_size\n    tier = var.app_serviceplan_tiers\n  }\n  tags = local.common_tags\n}\n`\n```\nAnd also here\n```\n`locals {\n  common_tags = {\n    costcenter  = var.cost_center\n    environment = terraform.workspace\n    lastupdate  = formatdate(\"DD-MMM-YY hh:mm:ss ZZZ\", timestamp())\n  }\n`\n```\nMaybe we could add a conditional item for parameter lastupdate but i have no idea on how do it\nThanks for all",
      "solution": "EDIT/CAUTION: I found issues in this approach using remote environments for execution, such as Terraform Cloud. This works locally, but in situations where the targeted file is being created during runtime for apply, the last modified date is based on the file being created during that runtime.\nA best practice is for tags to have meaningful key/values, as suggested in other answers. However, if you are faced with this being a requirement, one approach is to use the Terraform `external` data resource to run a shell script, capture the date, and use that in your Terraform as a value for your tags.\nFirst, create a bash script in the Terraform module:\n`#!/bin/bash\n\nLASTUPDATED=$(date -r $1)\n\necho '{\"lastupdated\":\"'$LASTUPDATED'\"}' | jq .\n`\nThen, within your Terraform you can use the result to apply your tag:\n```\n`# so_71645084.tf\nvariable \"costcenter\" { default = \"foo\" }\n\ndata \"external\" \"file_last_updated\" {\n  program = [\"bash\", \"${path.module}/so_71645084.sh\", \"${path.module}/so_71645084.tf\"]\n}\n\nlocals {\n  common_tags = {\n    costcenter  = var.costcenter\n    environment = terraform.workspace\n    lastupdate  = data.external.file_last_updated.result.lastupdated\n  }\n}\n\noutput \"common_tags\" { value = local.common_tags }\n`\n```\nYour resulting `common_tags` output would be:\n```\n`...\n\n + costcenter  = \"foo\"\n + environment = \"dev\"\n + lastupdate  = \"Mon Mar 28 13:32:15 UTC 2022\"\n`\n```\nIn summary, the `external` data resource runs a `date` command against the targeted file you are tracking changes for. We are using the result as a data resource in our Terraform to apply a date value to our tags.",
      "question_score": 4,
      "answer_score": 2,
      "created_at": "2022-03-28T11:21:47",
      "url": "https://stackoverflow.com/questions/71645084/terraform-how-can-we-update-a-tag-only-if-modification-is-detected"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-azure",
      "question_id": 74781372,
      "title": "Terraform Azure Key Vault - context deadline exceeded",
      "problem": "I am trying to provision a simple `keyvault` in `azure` using `terraform` and I get the following error:\n\nError: retrieving `contact` for KeyVault: keyvault.BaseClient#GetCertificateContacts: Failure sending request: StatusCode=0 -- Original Error: context deadline exceeded\n\nAnd here's my terraform snippet:\n```\n`# data \"azurerm_client_config\" \"current\" {}\ndata \"azuread_client_config\" \"current\" {}\n\nresource \"azurerm_key_vault\" \"aks_key_vault\" {\n  name                        = var.aks_kv_name\n  location                    = var.location\n  resource_group_name         = var.rg_name\n  enabled_for_disk_encryption = true\n  tenant_id                   = data.azuread_client_config.current.tenant_id\n#   soft_delete_retention_days  = var.aks_kv_soft_delete_retention_days\n#   purge_protection_enabled    = false\n\n  sku_name = var.aks_kv_sku\n\n  access_policy {\n    tenant_id = data.azuread_client_config.current.tenant_id\n    object_id = data.azuread_client_config.current.object_id\n\n    key_permissions     = var.aks_kv_key_permissions\n    secret_permissions  = var.aks_kv_secret_permissions\n    storage_permissions = var.aks_kv_storage_permissions\n  }\n  tags = merge(var.common_tags)\n}\n\n`\n```\nand here's the `variables.tf`\n```\n`# Key Vault related variables\naks_kv_name                       = \"jana-azure-csi-kv\"\naks_kv_soft_delete_retention_days = 7\naks_kv_sku                        = \"standard\"\naks_kv_key_permissions            = [\"Get\"]\naks_kv_secret_permissions         = [\"Get\"]\naks_kv_storage_permissions        = [\"Get\"]\n`\n```\nCan someone help me understand what is the issue and how to fix it?\n`terraform` version:\n```\n`Terraform v1.3.6\non darwin_amd64\n`\n```\n`azurerm` version:\n```\n`version = \"=3.0.0\"\n`\n```",
      "solution": "'context deadline exceeded' means the respected action won't be done with in timeframe.  Mostly it was caused because of network connection issue.\nHere the issue was caused by the `azurerm` resource provider version, as part of solution please upgrade or keep > on provider:\nFrom:\n`version = \"3.0.0\"\n`\nTo:\n`version = \">=3.0.0\"\n#          ^^\n`\nafter that please run below command\n```\n`terraform init -upgrade\n`\n```\n\nReplicate the same issue via below code. Here is the code snippet used to create keyvault\nMain tf file as follows:\n```\n`data \"azurerm_resource_group\" \"example\" {\n  name     = \"**********\"\n}\n\nprovider \"azurerm\" {\n  features {\n    key_vault {\n      purge_soft_delete_on_destroy    = true\n      recover_soft_deleted_key_vaults = true\n    }\n  }\n}\n\ndata \"azurerm_client_config\" \"current\" {}\n\nresource \"azurerm_key_vault\" \"example\" {\n  name                        = \"swarnademokeyvault\"\n  location            = data.azurerm_resource_group.example.location\n  resource_group_name = data.azurerm_resource_group.example.name\n  enabled_for_disk_encryption = true\n  tenant_id                   = data.azurerm_client_config.current.tenant_id\n  soft_delete_retention_days  = 7\n  purge_protection_enabled    = false\n\n  sku_name = \"standard\"\n\n  access_policy {\n    tenant_id = data.azurerm_client_config.current.tenant_id\n    object_id = data.azurerm_client_config.current.object_id\n\n    key_permissions = [\n      \"Get\",\n    ]\n\n    secret_permissions = [\n      \"Get\",\n    ]\n\n    storage_permissions = [\n      \"Get\",\n    ]\n  }\n}\n`\n```\nProvider tf file as follows:\nterraform {\n```\n`  required_version = \"~>1.3.3\"\n  required_providers {\n    azurerm = {\n       source = \"hashicorp/azurerm\"\n       version = \">=3.0.0\"\n         }\n       }\n`\n```\n}\nWhen we ran below command\n```\n`terraform plan \n`\n```\n\nWhile run apply\n```\n`terraform apply -auto-approve\n`\n```\n\nVerification:",
      "question_score": 3,
      "answer_score": 7,
      "created_at": "2022-12-13T08:34:30",
      "url": "https://stackoverflow.com/questions/74781372/terraform-azure-key-vault-context-deadline-exceeded"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-azure",
      "question_id": 69273569,
      "title": "Issue using third party providers in terraform for azure-devops and mysql provider",
      "problem": "While doing terraform init getting error, following the terraform official documentation, I am trying to create azure-pipeline via terraform and have created modules for it, but unable to initialize, its working fine if i am passing it directly in .tf file, but when adding to module, the terraform init command itself is failing.\n```\n`\u2577\n\u2502 Error: Failed to query available provider packages\n\u2502\n\u2502 Could not retrieve the list of available versions for provider hashicorp/mysql: provider registry registry.terraform.io does not have a provider named       \n\u2502 registry.terraform.io/hashicorp/mysql\n\u2502\n\u2502 Did you intend to use terraform-providers/mysql? If so, you must specify that source address in each module which requires that provider. To see which       \n\u2502 modules are currently depending on hashicorp/mysql, run the following command:\n\u2502     terraform providers\n\u2575\n\n\u2577\n\u2502 Error: Failed to query available provider packages\n\u2502\n\u2502 Could not retrieve the list of available versions for provider hashicorp/azuredevops: provider registry registry.terraform.io does not have a provider named \n\u2502 registry.terraform.io/hashicorp/azuredevops\n\u2502\n\u2502 Did you intend to use microsoft/azuredevops? If so, you must specify that source address in each module which requires that provider. To see which modules   \n\u2502 are currently depending on hashicorp/azuredevops, run the following command:\n\u2502     terraform providers\n`\n```",
      "solution": "Anyone here looking for answer, For any third party provider we need to add the source in modules as well. Like I had to add the azure-devops source in the module\n```\n`terraform {\n  required_providers {\n    azuredevops = {\n      source = \"microsoft/azuredevops\"\n      version = \"0.1.7\"\n    }\n  }\n}\n\n#Create Azure Repo and Azure Pipeline\ndata \"azuredevops_project\" \"project\" {\n  name = \"Test\"\n}\n\n#Create New Repo\nresource \"azuredevops_git_repository\" \"repo\" {\n  project_id = data.azuredevops_project.project.id\n  name       = var.name\n  initialization {\n    init_type   = \"Import\"\n    source_type = \"Git\"\n    source_url  = lookup(var.template_map,var.template)\n  }\n}\n`\n```",
      "question_score": 3,
      "answer_score": 9,
      "created_at": "2021-09-21T20:02:35",
      "url": "https://stackoverflow.com/questions/69273569/issue-using-third-party-providers-in-terraform-for-azure-devops-and-mysql-provid"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-azure",
      "question_id": 75054516,
      "title": "ACR purge task using terraform not working as expected",
      "problem": "Creating ACR like below\n```\n`resource \"azurerm_container_registry\" \"acr\" {\n  name                = var.azure_container_registry_name\n  resource_group_name = var.resource_group.name\n  location            = var.location\n}\n`\n```\nAnd purge task like below\n```\n`resource \"azurerm_container_registry_task\" \"acr_purge_task\" {\n  name                  = \"scheduledAcrPurgeTask\"\n  container_registry_id = azurerm_container_registry.acr.id\n  platform {\n    os           = \"Linux\"\n  }\n  encoded_step {\n    task_content = The task is getting creating successfully but when ran I am getting below error\n```\n`Unable to find image 'acr:latest' locally\ndocker: Error response from daemon: pull access denied for acr, repository does not exist or may require 'docker login': denied: requested access to the resource is denied.\n`\n```\nI am not sure why this is trying to find image in acr:latest as I am maintaining REPO as my repository.",
      "solution": "Silly mistake\nI removed the empty spaces before lines\n```\n`version: v1.1.0\nsteps:\n  - cmd: acr purge --filter 'REPO:TEST.*' --untagged --ago 10m\n    disableWorkingDirectoryOverride: true\n    timeout: 3600\n`\n```\nearlier\n```\n`    version: v1.1.0\n    steps:\n      - cmd: acr purge --filter 'REPO:TEST.*' --untagged --ago 10m\n        disableWorkingDirectoryOverride: true\n        timeout: 3600\n`\n```\nI don't why terraform or either the Azure didn't provide better error response.",
      "question_score": 3,
      "answer_score": 8,
      "created_at": "2023-01-09T09:04:24",
      "url": "https://stackoverflow.com/questions/75054516/acr-purge-task-using-terraform-not-working-as-expected"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-azure",
      "question_id": 73809505,
      "title": "How to use output of one child module as an input to another child module in Terraform",
      "problem": "I have the below directory structure\n```\n`\n\u251c\u2500\u2500 main.tf\n\u251c\u2500\u2500 output.tf\n\u251c\u2500\u2500 variables.tf\n\u251c\u2500\u2500 modules\n\u2502   \u251c\u2500\u2500 ServicePrincipal\n\u2502   \u2502   \u251c\u2500\u2500 variables.tf\n\u2502   \u2502   \u251c\u2500\u2500 outputs.tf\n\u2502   \u2502   \u251c\u2500\u2500 main.tf\n\u2502   \u251c\u2500\u2500 aks\n\u2502   \u2502   \u251c\u2500\u2500 main.tf\n\u2502   \u2502   \u251c\u2500\u2500 output.tf\n\u2502   \u2502   \u2514\u2500\u2500 variables.tf\n...\n\n`\n```\nIssue:\nI want to use client_id and client_secret generated from service principal module as an input to create my aks cluster. I am able to reference the below output variables from my root main.tf by module.modulename.outputvarname however, I cannot access it in another child module(aks) as var.client_id or module.serviceprincipal.client_id\nmain.tf of root module where I am able to use client_id and client_secret\n```\n`module \"ServicePrincipal\" {\n  source                 = \"./modules/ServicePrincipal\"\n  service_principal_name = var.service_principal_name\n  redirect_uris          = var.redirect_uris\n\n}\n\nmodule \"aks\" {\n  source                 = \"./modules/aks/\"\n  service_principal_name = var.service_principal_name\n  serviceprinciple_id    = module.ServicePrincipal.service_principal_object_id\n  serviceprinciple_key   = module.ServicePrincipal.client_secret\n  location            = var.location\n  resource_group_name = var.rgname\n  depends_on = [\n    module.ServicePrincipal\n  ]\n\n}\n\n`\n```\nmain.tf of aks module\n```\n`service_principal  {\n    client_id = var.client_id\n    client_secret = var.client_secret\n  }\n\n`\n```\noutput.tf for my ServicePrincipal module\n```\n`output \"client_id\" {\n  description = \"The application id of AzureAD application created.\"\n  value       = azuread_application.main.application_id\n\n}\n\noutput \"client_secret\" {\n  description = \"Password for service principal.\"\n  value       = azuread_service_principal_password.main.*.value\n  \n}\n\n`\n```\nBelow is the error I am getting:\n```\n`\nError: Missing required argument\n\n  on main.tf line 136, in module \"aks\":\n 136: module \"aks\" {\n\nThe argument \"client_id\" is required, but no definition was found.\n\nError: Missing required argument\n\n  on main.tf line 136, in module \"aks\":\n 136: module \"aks\" {\n\nThe argument \"client_secret\" is required, but no definition was found.\n\n`\n```\nI already defined those as variables in aks module and root module, am I missing something here?\nThanks in advance!\nPiyush",
      "solution": "Child modules can't reference each others outputs. You have to explicitly pass them in the root module from one module to the second, e.g.\nin root:\n```\n`module \"ServicePrincipal\" {\n}\n\nmodule \"aks\" {\n   client_id = module.ServicePrincipal.client_id\n}\n`\n```",
      "question_score": 3,
      "answer_score": 5,
      "created_at": "2022-09-22T06:52:17",
      "url": "https://stackoverflow.com/questions/73809505/how-to-use-output-of-one-child-module-as-an-input-to-another-child-module-in-ter"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-azure",
      "question_id": 77102159,
      "title": "Change Azure Databricks cluster&#39;s runtime version to 13.3 LTS by terraform",
      "problem": "I have created a Databricks cluster in an Azure Databricks workspace by using terraform. I am using terraform azurerm and databricks providers. The runtime version of my cluster is 11.3 LTS. Now I would like to change it to 13.3 LTS. All my changes to the cluster must be done by terraform and not manually. This is our policy at our organization. Here is the terraform code I use to create the cluster: I have made a module and I call the module in my code which worked very well for ruuntime 11.3:\nmod_databricks_cluster.tf file is as follows:\n```\n`module \"mod_cluster_job_run\" {\n  source                    = \"git::https://dev.azure.com/MyOrganization/MyProject/_git/tf-module-databricks-cluster?ref=2.0.0\"\n  cluster_name              = \"tf-spark-job-cluster\"\n  autoscale                 = false\n  autotermination_minutes   = 60\n  spark_version             = \"11.3.x-scala2.12\" #\"13.0-lts\"#\"13.0.x-scala2.12\"\n  spark_conf = local.spark_conf_test\n  node_type_id = var.cluster_node_type\n  driver_node_type_id = var.cluster_node_type\n  num_workers = var.num_workers\n}\n`\n```\nThis works perfectly well. But when I try to change spark_version to any of these values \"3.12.0-scala2.12\", \"3.12.1-scala2.12\", \"3.12.2-scala2.12\", \"13.0-lts\" or \"13.0.x-scala2.12\", it fails with the following error: (terraform apply is running in an Azure DevOps pipeline)\nError: cannot update cluster: Invalid spark version 3.12.0-scala2.12.\nwith module.mod_cluster_job_run.databricks_cluster.cluster,\non .terraform/modules/mod_cluster_job_run/main.tf line 1, in resource \"databricks_cluster\" \"cluster\":\n1: resource \"databricks_cluster\" \"cluster\"\nHere is the screen shot of the error:\n\nI get the exact same error with other values that I wrote above.\nMy question is what spark version should be in used in terraform to change the runtime to 13.3 LTS?\nIn other words to change from the existing runtime shown below:\n\nTo the runtime 13.3 LTS shown below:\n\nWhat spark version value is acceptable for terraform?",
      "solution": "You can get all valid Spark version from Databricks Cluster API or CLI:\n```\n`$ databricks clusters spark-versions | jq -r '.versions[].key' | sort\n(...)\n13.3.x-cpu-ml-scala2.12\n13.3.x-gpu-ml-scala2.12\n13.3.x-photon-scala2.12\n13.3.x-scala2.12\n(...)\n`\n```\n`13.3.x-scala2.12` and `13.0.x-scala2.12` are valid.\n`3.12.0-scala2.12`, `3.12.1-scala2.12`, `3.12.2-scala2.12`, `13.0-lts` are not.",
      "question_score": 3,
      "answer_score": 4,
      "created_at": "2023-09-14T08:03:11",
      "url": "https://stackoverflow.com/questions/77102159/change-azure-databricks-clusters-runtime-version-to-13-3-lts-by-terraform"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-azure",
      "question_id": 75495510,
      "title": "How can I create an Azure CosmosDB Serverless using Terraform?",
      "problem": "I've been trying to create the serverless offer of the cosmosdb database using Terraform, but haven't been able to do so.\nAfter reading Microsoft's documentations and the Azurerm terraform registry documentations, I could code this resource:\n```\n`resource \"azurerm_cosmosdb_account\" \"resume-challenge-cosmosdb\" {\nname                      = var.cosmosdb-name\n  location                  = var.region\n  resource_group_name       = azurerm_resource_group.cloud-resume-rg.name\n  offer_type                = \"Standard\"\n  kind                      = \"GlobalDocumentDB\"\n  enable_automatic_failover = false\n  enable_free_tier          = true\n  consistency_policy {\n    consistency_level       = \"BoundedStaleness\"\n    max_interval_in_seconds = 300\n    max_staleness_prefix    = 100000\n  }\n  geo_location {\n    location          = \"brazilsouth\"\n    failover_priority = 0\n  }\n}\n`\n```\nbut it creates the regular version of the cosmosdb.",
      "solution": "To make a CosmosDB account serverless using the AzureRM Terraform provider you need to enable the `EnableServerless` capability on the `azurerm_cosmosdb_account`.  To do this you must add a `capabilities` block with `name = \"EnableServerless\"`.  Applying this to your above example would look like so:\n```\n`resource \"azurerm_cosmosdb_account\" \"resume-challenge-cosmosdb\" {\nname                      = var.cosmosdb-name\n  location                  = var.region\n  resource_group_name       = azurerm_resource_group.cloud-resume-rg.name\n  offer_type                = \"Standard\"\n  kind                      = \"GlobalDocumentDB\"\n  enable_automatic_failover = false\n  enable_free_tier          = true\n  consistency_policy {\n    consistency_level       = \"BoundedStaleness\"\n    max_interval_in_seconds = 300\n    max_staleness_prefix    = 100000\n  }\n  geo_location {\n    location          = \"brazilsouth\"\n    failover_priority = 0\n  }\n  capabilities {\n    name = \"EnableServerless\"\n  }\n}\n`\n```\nI found this by searching for the word \"serverless\" on the azurerm_cosmosdb_account resource docs.",
      "question_score": 3,
      "answer_score": 5,
      "created_at": "2023-02-18T19:32:54",
      "url": "https://stackoverflow.com/questions/75495510/how-can-i-create-an-azure-cosmosdb-serverless-using-terraform"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-azure",
      "question_id": 71695811,
      "title": "Terraform bind SSL Certificate to Azure WebApp",
      "problem": "I have recently been trying to bind a domain and an SSL certificate to a web app using Terraform in Azure.\nI am having no luck in doing this and the documentation is a bit confusing / light on the ground.\nThe error I am getting when just doing a plan is:\n\r\n\r\n`Error: parsing \"/subscriptions//resourceGroups/Testing_Prod_KeyVault_JC/providers/Microsoft.KeyVault/vaults/secrets-testingprodjc\": KeyVault Nested Item should contain 2 or 3 segments, got 8 from \"subscriptions//resourceGroups/Testing_Prod_KeyVault_JC/providers/Microsoft.KeyVault/vaults/secrets-testingprodjc\"`\r\n\r\n\r\n\nI was wondering if anyone had been able to do this so far?\nHere is my code for the Certificate and Domain bind:\n\r\n\r\n`//First Read the External Key Vault\ndata \"azurerm_key_vault\" \"production_keyvault\" {\n  name                = \"secrets-testingprodjc\"\n  resource_group_name = \"Testing_Prod_KeyVault_JC\"\n}\n\n// Now Read the Certificate\ndata \"azurerm_key_vault_certificate\" \"prod_certificate\" {\n  name         = \"testing-certificate-for-cic\"\n  key_vault_id = data.azurerm_key_vault.production_keyvault.id\n}\n\n// Now bind the webapp to the domain and look for certificate. \nresource \"azurerm_app_service_custom_hostname_binding\" \"website_app_hostname_bind\" {\n  hostname            = \"portal-staging-westeurope.jasoncontenttestingdomain.com\"\n  app_service_name    = azurerm_app_service.website_app.name\n  resource_group_name = azurerm_resource_group.Terraform.name\n  ssl_state = \"SniEnabled\"\n  thumbprint = azurerm_app_service_certificate.cert.thumbprint\n}\n\n/* // Following block NOT BEING USED\nresource \"azurerm_app_service_certificate_binding\" \"bind_certificate_to_webapp\" {\n  hostname_binding_id = azurerm_app_service_custom_hostname_binding.website_app_hostname_bind.id\n  ssl_state           = \"SniEnabled\"\n  thumbprint = azurerm_app_service_certificate.cert.thumbprint \n}\n*/\n\n// Get Certificate from External KeyVault\nresource \"azurerm_app_service_certificate\" \"cert\" {\n  name                = \"testing-certificate-for-cic\"\n  resource_group_name = azurerm_resource_group.Terraform.name\n  location            = azurerm_resource_group.Terraform.location \n  key_vault_secret_id = data.azurerm_key_vault.production_keyvault.id\n}`\r\n\r\n\r\n\nI am just for now doing this with my logged-in user account, not a service principle I am aware of the service principal part but for now I am just testing this. My logged-in account does have access to the external keyvault with full rights.",
      "solution": "I actually fixed this myself the other day with the following code, I found my answer on a GitHub repo for HashiCorp but I cant find the link now. It has to do with the resource `azurerm_app_service_certificate` if you use the `key_vault_secret_id` part it doesn't work you need to use `pfx_blob`.\nHere is the code for anyone's ref:\n\r\n\r\n`//First Read the External Key Vault\ndata \"azurerm_key_vault\" \"production_keyvault\" {\n  name                = \"testingkeyvault2022\"\n  resource_group_name = \"KeyVaultWestEuropeBackend\"\n}\n\n// Now Read the Certificate\ndata \"azurerm_key_vault_secret\" \"prod_certificate\" {\n  name         = \"testcert\"\n  key_vault_id = data.azurerm_key_vault.production_keyvault.id\n}\n\n// Now bind the webapp to the domain and look for certificate. \nresource \"azurerm_app_service_custom_hostname_binding\" \"website_app_hostname_bind\" { //Website App\n  depends_on = [\n    azurerm_app_service_certificate.cert,\n  ]\n  hostname            = var.websiteurlbind\n  app_service_name    = data.azurerm_app_service.read_website_app.name\n  resource_group_name = data.azurerm_resource_group.Terraform.name\n  ssl_state           = \"SniEnabled\"\n  thumbprint          = azurerm_app_service_certificate.cert.thumbprint\n}\n\n// Get Certificate from External KeyVault\nresource \"azurerm_app_service_certificate\" \"cert\" {\n  name                = \"testingcert\"\n  resource_group_name = data.azurerm_resource_group.Terraform.name\n  location            = data.azurerm_resource_group.Terraform.location\n  pfx_blob            = data.azurerm_key_vault_secret.prod_certificate.value\n}`\r\n\r\n\r\n\nWhat I also noticed in my testing is you have to put the cert resource as a depends on on the bind. Its in my code but for clarity here is this piece of code:\n\r\n\r\n`depends_on = [\n        azurerm_app_service_certificate.cert,\n      ]`",
      "question_score": 3,
      "answer_score": 1,
      "created_at": "2022-03-31T18:06:13",
      "url": "https://stackoverflow.com/questions/71695811/terraform-bind-ssl-certificate-to-azure-webapp"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-azure",
      "question_id": 71028835,
      "title": "Error while trying to run Terraform Apply , Web App Authentication Error",
      "problem": "I get the below error while trying to run Terraform Apply.\n```\n`Error: updating Authentication Settings for App Service \"app-cont-sa-fe-predev-cus-bb2e\": web.AppsClient#UpdateAuthSettings: Failure responding to request: StatusCode=400 -- Original Error: autorest/azure: Service returned an error. Status=400 Code=\"BadRequest\" Message=\"Cannot execute the request for site app-cont-sa-fe-predev-cus-bb2e because the site is running on auth version v2.\" Details=[{\"Message\":\"Cannot execute the request for site app-cont-sa-fe-predev-cus-bb2e because the site is running on auth version v2.\"},{\"Code\":\"BadRequest\"},{\"ErrorEntity\":{\"Code\":\"BadRequest\",\"ExtendedCode\":\"04534\",\"Message\":\"Cannot execute the request for site app-cont-sa-fe-predev-cus-bb2e because the site is running on auth version v2.\",\"MessageTemplate\":\"Cannot execute the request for site {0} because the site is running on auth version {1}.\",\"Parameters\":[\"app-cont-sa-fe-predev-cus-bb2e\",\"v2\"]}}]\n    \u2502 \n    \u2502   with azurerm_app_service.fe,\n    \u2502   on resources.app.tf line 59, in resource \"azurerm_app_service\" \"fe\":\n    \u2502   59: resource \"azurerm_app_service\" \"fe\" {\n`\n```\nCan anyone tell me what do i need to change in my below resource block so that i don't get the error. Thanks\n```\n`resource \"azurerm_app_service\" \"fe\" {\n  location            = module.resourcegroup.resource_group.location\n  resource_group_name = module.resourcegroup.resource_group.name\n  tags                = module.resourcegroup.resource_group.tags\n  app_service_plan_id = azurerm_app_service_plan.default.id\n  name                = module.names-web-app-fe.location.app_service.name_unique\n  identity { type = \"SystemAssigned\" }\n  auth_settings {\n    enabled                       = true\n    default_provider              = \"AzureActiveDirectory\"\n    issuer                        = format(\"https://sts.windows.net/%s/\", data.azurerm_client_config.default.tenant_id)\n    runtime_version               = \"~1\"\n    token_store_enabled           = true\n    unauthenticated_client_action = \"RedirectToLoginPage\"\n    additional_login_params = {\n      \"response_type\" = \"code id_token\",\n      \"resource\"      = azuread_application.app-fe.application_id\n    }\n    active_directory {\n      client_id         = azuread_application.app-fe.object_id\n      client_secret     = azuread_application_password.fe-app-sp-secret.application_object_id\n      allowed_audiences = [format(\"https://%s.azurewebsites.net\", module.names-web-app-fe.location.app_service.name_unique)]\n    }\n  }\n  site_config {\n    always_on                = true\n    app_command_line         = \"\"\n    default_documents        = []\n    dotnet_framework_version = \"v4.0\"\n    ftps_state               = \"Disabled\"\n    health_check_path        = \"\"\n    http2_enabled            = true\n    linux_fx_version         = \"STATICSITE|1.0\"\n    local_mysql_enabled      = false\n    managed_pipeline_mode    = \"Integrated\"\n    min_tls_version          = \"1.2\"\n    #pre_warmed_instance_count = 0\n    python_version            = \"3.4\"\n    remote_debugging_enabled  = false\n    remote_debugging_version  = \"VS2019\"\n    use_32_bit_worker_process = false\n    websockets_enabled        = false\n    windows_fx_version        = \"\"\n    cors {\n      allowed_origins     = []\n      support_credentials = false\n    }\n  }\n  app_settings = {\n    \"WEBSITE_DNS_SERVER\"     = \"168.63.129.16\"\n    \"WEBSITE_VNET_ROUTE_ALL\" = \"1\"\n  }\n}\n`\n```\nI guess there were changes from Azure side wrt authentication and because of which i am getting this error.",
      "solution": "Terraform uses `Auth V1 Settings` instead of using `Auth V2 setting` for the Web App. Currently only `Azure CLI cmdlet` and `ARM Templates` allow the `auth_settings_v2` to be configured. This might be available in the `upcoming version of azurerm provider i.e. v3.0.0` as mentioned under Feature details: New Data Sources / Resources for App Service & Function Apps .\nAs for the error which you are getting , I tried creating a App Service in Azure using similar code as yours and it didn't provide any error in the initial creation but after I go to portal and under authentication setting , I upgrade the authentication settings to v2 . I start receiving the same error while trying to update the application from terraform like below :\n\nIn order to avoid the error , if you are using terraform to create and manage the web app , then please don't upgrade Web Authentication Settings .",
      "question_score": 3,
      "answer_score": 5,
      "created_at": "2022-02-08T06:03:57",
      "url": "https://stackoverflow.com/questions/71028835/error-while-trying-to-run-terraform-apply-web-app-authentication-error"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-azure",
      "question_id": 66523796,
      "title": "two frontend ports of application gateway are using the same port 443 - Azure application gateway in terraform",
      "problem": "I am configuring azure application gateway using terraform.\nFollowing is the module that i wrote:\n```\n`locals {\n  backend_address_pool_name      = format(\"appgwbeap-%[1]s-%[2]s%[3]sweb-gw\",var.project_code,var.env,var.zone)\n  frontend_port_name             = format(\"appgwfeport-%[1]s-%[2]s%[3]sweb-gw\",var.project_code,var.env,var.zone)\n  frontend_ip_configuration_name = format(\"appgwfeip-%[1]s-%[2]s%[3]sweb-gw\",var.project_code,var.env,var.zone)\n  http_setting_name              = format(\"appgwhtst-%[1]s-%[2]s%[3]sweb-gw\",var.project_code,var.env,var.zone)\n  listener_name                  = format(\"appgwhttplstnr-%[1]s-%[2]s%[3]sweb-gw\",var.project_code,var.env,var.zone)\n  request_routing_rule_name      = format(\"appgwrqrt-%[1]s-%[2]s%[3]sweb-gw\",var.project_code,var.env,var.zone)\n  redirect_configuration_name    = format(\"appgwrdrcfg-%[1]s-%[2]s%[3]sweb-gw\",var.project_code,var.env,var.zone)\n}\n\nresource \"azurerm_application_gateway\" \"appgw\" {\n  name                = format(\"appgw-%[1]s-%[2]s%[3]sweb-gw\",var.project_code,var.env,var.zone)\n  resource_group_name = var.rg_name\n  location            = var.location\n\n  sku {\n    name     = var.sku_name\n    tier     = var.sku_tier\n    capacity = var.sku_capacity\n  }\n\n  gateway_ip_configuration {\n    name      = format(\"appgwipcfg-%[1]s-%[2]s%[3]sweb-gw\",var.project_code,var.env,var.zone)\n    subnet_id = var.subnet_id\n  }\n\n  frontend_port {\n    name = \"appgwfeport-app1-uatizweb-gw\"\n    port = \"443\"\n  }\n\n  frontend_port {\n    name = \"appgwfeport-app2-uatizweb-gw\"\n    port = \"443\"\n  }\n\n  ssl_certificate {\n    name     = \"UAT-APP1-APPGW-SSL-CERT-SGCORE-12Jan21-12Jan23\"\n    data     = filebase64(\"./certificates/web.app1.sso.gwwu.xxx.com.de-12Jan2021.pfx\")\n    password = \"${var.app1_pfx_password}\"\n  }\n  authentication_certificate {\n    name = \"UAT-APP1-APPGW-SSL-CERT-SGCORE-12Jan21-12Jan23\"\n    data = file(\"./certificates/web_app1_sso_gwwu_xxx_com_de-12Jan21.cer\")\n  }  \n\n  ssl_certificate {\n    name     = \"UAT-APP2-APPGW-SSL-CERT-01Mar21\"\n    data     = filebase64(\"./certificates/selfsigned-app2-uat-01Mar21.pfx\")\n    password = \"${var.app1_pfx_password}\"\n  }\n  authentication_certificate {\n    name = \"UAT-APP2-APPGW-SSL-CERT-01Mar21\"\n    data = file(\"./certificates/selfsigned-app2-uat-01Mar21.cer\")\n  }  \n\n  frontend_ip_configuration {\n    name                 = \"${local.frontend_ip_configuration_name}\"\n    subnet_id            = var.subnet_id\n    private_ip_address   = var.frontend_private_ip\n    private_ip_address_allocation = \"Static\"\n  }\n\n  backend_address_pool {\n    name = \"beap-path-app1-app\"\n    #fqdns     = var.fqdn_list\n    ip_addresses = [\"10.xxx.xxx.36\"]\n  }\n\n  backend_address_pool {\n    name = \"beap-path-app2-app\"\n    #fqdns     = var.fqdn_list\n    ip_addresses = [\"10.xxx.xxx.37\"]\n  }\n\n  backend_http_settings {\n    name                  = \"behs-path-app1-app\"\n    cookie_based_affinity = var.backend_cookie_based_affinity\n    affinity_cookie_name  = \"ApplicationGatewayAffinity\"\n    path                  = var.backend_path\n    port                  = \"443\"\n    #probe_name            = \"probe-app1\"\n    protocol              = \"Https\"\n    request_timeout       = var.backend_request_timeout\n    authentication_certificate {\n      name = \"UAT-APP1-APPGW-SSL-CERT-SGCORE-12Jan21-12Jan23\"\n    }\n  }\n\n  backend_http_settings {\n    name                  = \"behs-path-app2-app\"\n    cookie_based_affinity = var.backend_cookie_based_affinity\n    affinity_cookie_name  = \"ApplicationGatewayAffinity\"\n    path                  = var.backend_path\n    port                  = \"443\"\n    #probe_name            = \"probe-app2\"\n    protocol              = \"Https\"\n    request_timeout       = var.backend_request_timeout\n    authentication_certificate {\n      name = \"UAT-APP2-APPGW-SSL-CERT-01Mar21\"\n    }\n  }\n\n  http_listener {\n    name                           = \"appgwhttplsnr-app1-uatizweb-gw\"\n    frontend_ip_configuration_name = \"${local.frontend_ip_configuration_name}\"\n    frontend_port_name             = \"appgwfeport-app1-uatizweb-gw\"\n    protocol                       = \"Https\"\n    ssl_certificate_name           = \"UAT-APP1-APPGW-SSL-CERT-SGCORE-12Jan21-12Jan23\"\n    require_sni                    = true\n    host_name                      = \"web.app1.sso.gwwu.xxx.com.de\"\n  }\n\n  http_listener {\n    name                           = \"appgwhttplsnr-app2-uatizweb-gw\"\n    frontend_ip_configuration_name = \"${local.frontend_ip_configuration_name}\"\n    frontend_port_name             = \"appgwfeport-app2-uatizweb-gw\"\n    ssl_certificate_name           = \"UAT-APP2-APPGW-SSL-CERT-01Mar21\"\n    require_sni                    = true\n    protocol                       = \"Https\"\n    host_name                      = \"web.app2.sso.gwwu.xxx.com.de\"\n  }\n\n  request_routing_rule {\n    name                       = \"appgwrqrt-app2-uatizweb-gw\"\n    rule_type                  = var.backend_rule_type\n    http_listener_name         = \"appgwhttplsnr-app2-uatizweb-gw\"\n    backend_address_pool_name  = \"beap-path-app2-app\"\n    backend_http_settings_name = \"behs-path-app2-app\"\n  }\n\n  request_routing_rule {\n    name                       = \"appgwrqrt-app1-uatizweb-gw\"\n    rule_type                  = var.backend_rule_type\n    http_listener_name         = \"appgwhttplsnr-app1-uatizweb-gw\"\n    backend_address_pool_name  = \"beap-path-app1-app\"\n    backend_http_settings_name = \"behs-path-app1-app\"\n  }\n}\n`\n```\nBelow is the `main.tf` that calls the module:\n```\n`module \"app_gateway\" {\n  source                     = \"../../../modules/appgateway\"\n  rg_name                    = var.rg_name\n  agency                     = local.agency\n  project_code               = local.project_code\n  env                        = var.env\n  zone                       = var.zone\n  tier                       = \"appgw\"\n  location                   = local.location\n  vnet_name                  = var.vnet_name\n  subnet_id                  = module.agw_subnet.subnet_id\n  sku_name                   = var.appgw_sku_name\n  sku_capacity               = var.appgw_sku_capacity\n  frontend_private_ip        = var.appgw_frontend_ip\n  frontend_port              = var.frontend_port\n  frontend_protocol          = var.frontend_protocol\n  app1_pfx_password          = \"${var.app1_pfx_password}\"\n  backend_protocol           = var.backend_protocol\n  backend_port               = var.backend_port\n  backend_path               = \"/\"\n  providers = {\n    azurerm = azurerm.corpapps\n  }\n}\n`\n```\nI have used Multi-site, However when i deploy -i get the following error:\n`two frontend ports of application gateway are using the same port number 443`.\nWhen i change one of my port to 5443 - it does get deployed and works from terraform.\nAlso, i can create two frontend port with 443 (multi-site) from portal.Can't do this from terraform.\nWhat am i missing from terraform.\nAny light on this will help!",
      "solution": "We could use the same frontend configuration(frontend IP, protocol, port or name) for multi-sites listener instead of creating two `frontend_port` names.\nFor example, change the related codes:\n```\n` resource \"azurerm_application_gateway\" \"appgw\" {\n    #..\n    \n      frontend_port {\n        name = \"appgwfeport-app1-uatizweb-gw\"\n        port = \"443\"\n      }\n    \n    \n    #  frontend_port {\n    #    name = \"appgwfeport-app2-uatizweb-gw\"\n    #    port = \"443\"\n    #  }\n    \n    #..\n    \n    \n      http_listener {\n        name                           = \"appgwhttplsnr-app1-uatizweb-gw\"\n        frontend_ip_configuration_name = \"${local.frontend_ip_configuration_name}\"\n        frontend_port_name             = \"appgwfeport-app1-uatizweb-gw\"\n        protocol                       = \"Https\"\n        ssl_certificate_name           = \"UAT-APP1-APPGW-SSL-CERT-SGCORE-12Jan21-12Jan23\"\n        require_sni                    = true\n        host_name                      = \"web.app1.sso.gwwu.xxx.com.de\"\n      }\n    \n      http_listener {\n        name                           = \"appgwhttplsnr-app2-uatizweb-gw\"\n        frontend_ip_configuration_name = \"${local.frontend_ip_configuration_name}\"\n        frontend_port_name             = \"appgwfeport-app1-uatizweb-gw\"      #change here\n        ssl_certificate_name           = \"UAT-APP2-APPGW-SSL-CERT-01Mar21\"\n        require_sni                    = true\n        protocol                       = \"Https\"\n        host_name                      = \"web.app2.sso.gwwu.xxx.com.de\"\n      }    \n    }\n`\n```\nFor more information, read https://learn.microsoft.com/en-us/azure/application-gateway/tutorial-multiple-sites-powershell and https://learn.microsoft.com/en-us/azure/application-gateway/create-multiple-sites-portal#configuration-tab",
      "question_score": 3,
      "answer_score": 1,
      "created_at": "2021-03-08T03:59:23",
      "url": "https://stackoverflow.com/questions/66523796/two-frontend-ports-of-application-gateway-are-using-the-same-port-443-azure-ap"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-azure",
      "question_id": 73162569,
      "title": "Using variables inside terraform.tfvars file",
      "problem": "I need to create VPC subnet tags using map variable.\nGetting below error while passing value from locals tag in the terraform.tfvars file.\n```\n`\"Variables may not be used here\"\n`\n```\nI want to pass values from tfvars file instead of setting a default value for subnet_tags variable in the variables.tf file.\nBelow are the details.\n```\n`variables.tf:\n\nvariable \"project_name\" {   \n  type        = string\n}\n\nvariable \"subnet_tags\" {  \n  description = \"public subnets\"\n  type        = map\n} \n\n---------------------------------------------------------\n\nterraform.tfvars:\n\nsubnet_tags={\n    \"project\"                                   = \"${local.project_name}\"\n    \"environment\"                               = \"development\"\n    \"managed-by\"                                = \"terraform\"\n    \"role\"                                      = \"public\"    \n    }\n\n--------------------------------------------------------------------------------\n\nmain.tf:\n\nlocals {\n  project_name = var.project_name \n\n}\n\nmodule \"vpc\" {\n  source      = \"../modules/vpc \n  subnet_tags = var.subnet_tags \n  ...\n  \n}\n`\n```",
      "solution": "You can use the `merge` [1] built-in function. Variables cannot be used inside tfvars file as you can see. So in your case, to combine two different sets of tags, you could drop the `project` tag from the `subnet_tags` variable:\n```\n`subnet_tags={\n    \"environment\"                               = \"development\"\n    \"managed-by\"                                = \"terraform\"\n    \"role\"                                      = \"public\"    \n    }\n`\n```\nAnd then in the module call, just use `merge` and the `project` tag:\n```\n`module \"vpc\" {\n  source      = \"../modules/vpc \n  subnet_tags = merge(\n    { \"project\" = local.project_name },\n    var.subnet_tags\n   )\n  ...\n  \n}\n`\n```\n\n[1] https://www.terraform.io/language/functions/merge",
      "question_score": 3,
      "answer_score": 4,
      "created_at": "2022-07-29T08:42:21",
      "url": "https://stackoverflow.com/questions/73162569/using-variables-inside-terraform-tfvars-file"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-azure",
      "question_id": 70369099,
      "title": "How to add Azure User Assigned Managed Identity to Azure AD group in Terraform?",
      "problem": "I am trying to assign User Assigned Managed identity to AAD group. I have following Terraform code:\n```\n`resource \"azurerm_user_assigned_identity\" \"myid\" {\n  name                = \"my_identity\"\n  resource_group_name = azurerm_resource_group.somerg.name\n  location            = azurerm_resource_group.somerg.location\n}\n\ndata \"azuread_group\" \"existinggroup\" {\n  display_name     = \"existing_group\"\n  security_enabled = true\n}\n\nresource \"azuread_group_member\" \"mygrpmember\" {\n  group_object_id  = data.azuread_group.existinggroup.id\n  member_object_id = azurerm_user_assigned_identity.myid.id\n}\n`\n```\nDuring `plan` operation, I get following error:\n```\n`Error: Value must be a valid UUID\n`\n```\nWhen I change `myid.id` to `myid.principal_id` in last line of above code, I get an error during `apply` operation:\n```\n`Error: Could not retrieve member principal object \"4e83cd6b-d984-4484-8fb2-3ae6e1667ef9\"\nODataId was nil\n`\n```\nWhen I try with `myid.client_id` I get this during `apply`:\n```\n`Error: Could not retrieve principal object \"838c2662-5fe2-484c-bb52-f70994fa1d8b\"\nDirectoryObjects.BaseClient.Get(): Get \"https://graph.microsoft.com/v1.0/5989ece0-f90e-40bf-9c79-1a7beccdb861/directoryObjects/838c2662-5fe2-484c-bb52-f70994fa1d8b\": GET https://graph.microsoft.com/v1.0/5989ece0-f90e-40bf-9c79-1a7beccdb861/directoryObjects/838c2662-5fe2-484c-bb52-f70994fa1d8b giving up after 9 attempt(s)\n`\n```\nWhat am I doing wrong?",
      "solution": "It will work if you give `myid.principal_id` only . Please use the latest versions i.e. terraform Version `v1.1.0` , azuread version `v2.13.0` and azurerm version `v2.89.0`   :\n\nI tested the same code in my environment like below :\n```\n`provider \"azuread\"{}\n\nprovider \"azurerm\"{\n  features {}\n}\ndata \"azurerm_resource_group\" \"somerg\"{\n  name = \"ansuman-resourcegroup\"\n}\nresource \"azurerm_user_assigned_identity\" \"myid\" {\n  name                = \"ansuman-identity\"\n  resource_group_name = data.azurerm_resource_group.somerg.name\n  location            = data.azurerm_resource_group.somerg.location\n}\n\ndata \"azuread_group\" \"existinggroup\" {\n  display_name     = \"TestQA\"\n  security_enabled = true\n}\n\nresource \"azuread_group_member\" \"mygrpmember\" {\n  group_object_id  = data.azuread_group.existinggroup.id\n  member_object_id = azurerm_user_assigned_identity.myid.principal_id\n}\n`\n```\nOutput:",
      "question_score": 3,
      "answer_score": 4,
      "created_at": "2021-12-15T20:08:18",
      "url": "https://stackoverflow.com/questions/70369099/how-to-add-azure-user-assigned-managed-identity-to-azure-ad-group-in-terraform"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-azure",
      "question_id": 69740400,
      "title": "Unexpected error updating Function App plan from consumption to standard. But only for some",
      "problem": "While updating Function Apps using Terraform to modify existing resources, I ran into the following. Had 7 function apps. 1 was using a standard, non-consumption plan. The other 6 were on consumption plans. Updated the plan to set all to standard, non-consumption plans.\nDuring terraform apply, for just 2 of the 6 being changed from consumption to non-consumption, and those 2 failures were repeatable. I don't see any differences in the resources that would account for it.\n```\n`Error: updating Function App \"FunctionAppThatFailed\" (Resource Group \"rg-group-dev\"): \nweb.AppsClient#CreateOrUpdate: Failure sending request: StatusCode=400 -- Original Error: \nCode=\"BadRequest\" Message=\"Number of Web workers must be greater than zero.\" Details=[{\"Message\":\"Number of Web workers must be greater than zero.\"},{\"Code\":\"BadRequest\"},{\"ErrorEntity\":{\"Code\":\"BadRequest\",\"ExtendedCode\":\"04004\",\"Message\":\"Number of Web workers must be greater than zero.\",\"MessageTemplate\":\"Number of Web workers must be greater than zero.\",\"Parameters\":[]}}]\n`\n```\nAny hints, things to check that I may have missed, solutions, etc. are definitely welcome.",
      "solution": "I found what was causing those two to fail by comparing against an environment created more recently.\nIf you call the Azure REST API endpoint to get the function app definition, you'll find a property \"numberOfWorkers\" which is set to -1. On the newer function apps it was set to 1. I think the Azure API used to create the consumption based function apps with -1. It currently creates them with a 1 so this should only be a problem on Function Apps created, say, somewhere around early 2021 and before.\n\nUsing the REST API I updated that property to 1 on the offending function apps. After that change, the switch to a standard plan worked fine.\nAzure REST API Endpoints used:\nGet site\nUpdate site",
      "question_score": 3,
      "answer_score": 4,
      "created_at": "2021-10-27T16:21:55",
      "url": "https://stackoverflow.com/questions/69740400/unexpected-error-updating-function-app-plan-from-consumption-to-standard-but-on"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-azure",
      "question_id": 68902609,
      "title": "Create Azure AD Application using terraform(Invalid UUID)",
      "problem": "I am using terraform to create an Azure AD application, I have tried the default example from the terraform samples https://registry.terraform.io/providers/hashicorp/azuread/latest/docs/resources/application also I have customized the code below from the one I created manually(basically, I have created an application manually in AD and got the details from the data resource using terraform for the created application). both the code throws same error\n\nError: Value must be a valid UUID \u2502 \u2502   with\nazuread_application.example, \u2502   on adapp.tf line 3, in resource\n\"azuread_application\" \"example\": \u2502    3: resource\n\"azuread_application\" \"example\" {\n\nThis is the code I have customized from the original example\n```\n`data \"azuread_client_config\" \"current\" {}\n\nresource \"azuread_application\" \"example\" {\n  display_name     = \"example\"\n  identifier_uris  = [\"api://example-app\"]\n  owners           = [data.azuread_client_config.current.object_id]\n  sign_in_audience = \"AzureADMultipleOrgs\"\n\n  required_resource_access {\n    resource_app_id = \"00000003-0000-0000-c000-000000000000\"\n    resource_access {\n      id   = \"...\"\n      type = \"Scope\"\n    }\n  }\n\n  web {\n    redirect_uris = [\"https://app.example.net/account\"]\n\n    implicit_grant {\n      access_token_issuance_enabled = false\n    }\n  }\n}\n`\n```\nI have validated the \"data.azuread_client_config.current.object_id\", its not null and it producing the value.\nTerraform Config:\n\nTerraform v0.15.4 on windows_amd64\n\nprovider registry.terraform.io/hashicorp/azuread v1.6.0",
      "solution": "As you are using the resource app id of `\"Microsoft Graph\"` (00000003-0000-0000-c000-000000000000) , so you have to provide what delegated permissions you need for your app to have in Microsoft graph like User.read etc.\nSome CLI commands that will help you to get the Microsoft Graph resource App Id's and Delegated Permissions Id's:\n```\n` - az ad sp list --display-name \"Microsoft Graph\" --query\n   '[].{appDisplayName:appDisplayName, appId:appId}' \n           --output table\n - az ad sp show --id 00000003-0000-0000-c000-000000000000 --query\n   \"oauth2Permissions[].{Value:value, Id:id}\" --output table\n`\n```\n\nSo as you are already using the default Microsoft Graph App Id , we need to get the delegated permission ID's to provide in resource access id.\n\nThen your terraform code will be as below :\n```\n`data \"azuread_client_config\" \"current\" {}\n\nresource \"azuread_application\" \"example\" {\n  display_name     = \"example\"\n  identifier_uris  = [\"api://example-app\"]\n  owners           = [data.azuread_client_config.current.object_id]\n  sign_in_audience = \"AzureADMultipleOrgs\"\n\n  required_resource_access {\n    resource_app_id = \"00000003-0000-0000-c000-000000000000\"# resourceid of microsoft graph\n    resource_access {\n      id   = \"e1fe6dd8-ba31-4d61-89e7-88639da4683d\"  # User.Read\n      type = \"Scope\"\n    }\n  }\n\n  web {\n    redirect_uris = [\"https://app.example.net/account\"]\n\n    implicit_grant {\n      access_token_issuance_enabled = false\n    }\n  }\n}\n`\n```\nDoing a terraform plan :\n\nNote : Default Microsoft Graph App ID is `\"00000003-0000-0000-c000-000000000000\"` and Default Windows Active Directory App ID (Azure AD Graph) is `\"00000002-0000-0000-c000-000000000000\"`. Based on your requirement you can use Microsoft Graph or Azure AD Graph .",
      "question_score": 3,
      "answer_score": 3,
      "created_at": "2021-08-24T08:19:22",
      "url": "https://stackoverflow.com/questions/68902609/create-azure-ad-application-using-terraforminvalid-uuid"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-azure",
      "question_id": 77630758,
      "title": "How to loop two different maps using for_each loop in Terraform",
      "problem": "I am trying to attach network interface to backendend pool as per the below requirement\napp-poc-1-nic & app-poc-2-nic to app-lb backendpool\ndb-nic-1-1-nic & db-nic-1-2-nic to db-lb backendpool\nBelow is the local block with nested map of objects\n```\n`locals {\n  vms = {\n    nodes = {\n      app_node1 = {\n        \"vm_name\" = \"app-poc\"\n        \"vm_num\"  = \"1\"\n        networks = {\n          nic1 = {\n            \"vm_name\" = \"app-poc\"\n            \"subnet\"  = \"/subscriptions/*****/subnets/app\"\n          },\n        }\n      },\n      app_node2 = {\n        \"vm_name\" = \"app-poc\"\n        \"vm_num\"  = \"2\"\n        networks = {\n          nic1 = {\n            \"vm_name\" = \"app-poc\"\n            \"subnet\"  = \"/subscriptions/*****/subnets/app\"\n          },\n        }\n      },\n      service_node1 = {\n        \"vm_name\" = \"service-poc\"\n        \"vm_num\"  = \"1\"\n        networks = {\n          nic1 = {\n            \"vm_name\" = \"service-poc\"\n            \"subnet\"  = \"/subscriptions/*****/subnets/app\"\n          },\n        }\n      },\n      service_node2 = {\n        \"vm_name\" = \"service-poc\"\n        \"vm_num\"  = \"2\"\n        networks = {\n          nic1 = {\n            \"vm_name\" = \"service-poc\"\n            \"subnet\"  = \"/subscriptions/*****/subnets/app\"\n          },\n        }\n      },\n      db_node1 = {\n        \"vm_name\" = \"db-poc\"\n        \"vm_num\"  = \"1\"\n        networks = {\n          nic1 = {\n            \"vm_name\" = \"db-nic-1\"\n            \"subnet\"  = \"/subscriptions/*****/subnets/app\"\n          },\n          nic2 = {\n            \"vm_name\" = \"db-nic-2\"\n            \"subnet\"  = \"/subscriptions/*****/subnets/db\"\n          }\n        }\n      },\n      db_node2 = {\n        \"vm_name\" = \"db-poc\"\n        \"vm_num\"  = \"2\"\n        networks = {\n          nic1 = {\n            \"vm_name\" = \"db-nic-1\"\n            \"subnet\"  = \"/subscriptions/*****/subnets/app\"\n          },\n          nic2 = {\n            \"vm_name\" = \"db-nic-2\"\n            \"subnet\"  = \"/subscriptions/*****/subnets/app\"\n          },\n        }\n      },\n    },\n  }\n  lbs = {\n    tiers = {\n        app-lb = {\n          lb_name           = \"app-lb\"\n          fip_name          = \"app-fip\"\n          subnet_id         = \"/subscriptions/*****/subnets/app\"\n          private_ip_type   = \"Dynamic\"\n          address_pool_name = \"app-address-pool\"\n          lb_probes = {\n            ssh_probe = {\n              protocol = \"Tcp\"\n              port     = \"22\"\n            }\n          }\n          lb_rules = {\n            ssh_rule = {\n              frontend_port           = \"22\"\n              protocol                = \"Tcp\"\n              backend_port            = \"22\"\n              enable_floating_ip      = true\n              frontend_ip_config_name = \"app-fip\"\n            }\n          }\n        },\n        db-lb = {\n          lb_name           = \"db-lb\"\n          fip_name          = \"db-fip\"\n          subnet_id         = \"/subscriptions/*****/subnets/app\"\n          private_ip_type   = \"Dynamic\"\n          address_pool_name = \"db-address-pool\"\n          lb_probes = {\n            ssh_probe = {\n              protocol = \"Tcp\"\n              port     = \"22\"\n            }\n          }\n          lb_rules = {\n            ssh_rule = {\n              frontend_port           = \"22\"\n              protocol                = \"Tcp\"\n              backend_port            = \"22\"\n              enable_floating_ip      = false\n              frontend_ip_config_name = \"db-fip\"\n            }\n          }\n        }\n    }\n  }\n}\n`\n```\nBelow are terraform resources that creates  Network Interfaces, Virtual Machines, load balancer, probe, lb rules\n```\n`data \"azurerm_resource_group\" \"rg\" {\n  name = \"test-rg\"\n}\nresource \"azurerm_network_interface\" \"nic-poc\" {\n  for_each = {\n    for vm in flatten([\n      for vm_name, vm in local.vms.nodes : [\n        for nic_name, nic in vm.networks : {\n          vm_number    = vm.vm_num,\n          vm_name      = vm_name,\n          nic_value    = nic.vm_name,\n          subnet_value = nic.subnet\n          nic_name     = nic_name\n        }\n      ]\n      ]\n    ) : \"${vm.vm_name}-${vm.nic_name}\" => vm\n  }\n  name                = \"${each.value.nic_value}-${each.value.vm_number}-nic\"\n  location            = data.azurerm_resource_group.rg.location\n  resource_group_name = data.azurerm_resource_group.rg.name\n  ip_configuration {\n    name                          = \"${each.value.nic_name}-${each.value.vm_number}-ipconfig\"\n    subnet_id                     = each.value.subnet_value\n    private_ip_address_allocation = \"Dynamic\"\n  }\n}\n\nresource \"azurerm_linux_virtual_machine\" \"vm-poc\" {\n  depends_on                      = [ azurerm_network_interface.nic-poc]\n  for_each                        = local.vms.nodes\n  name                            = \"${each.value.vm_name}-${each.value.vm_num}\"\n  admin_username                  = \"test-admin\"\n  admin_password                  = \"password@29\"\n  disable_password_authentication = false\n  location                        = data.azurerm_resource_group.rg.location\n  resource_group_name             = data.azurerm_resource_group.rg.name\n  network_interface_ids           = [for nic_key, nic in azurerm_network_interface.nic-poc : nic.id if startswith(nic_key, \"${each.key}-\")]\n\n  size                = \"Standard_B2ms\"\n  identity {\n    type = \"SystemAssigned\"\n  }\n  os_disk {\n    name                 = \"${each.value.vm_name}-${each.value.vm_num}-OSdisk\"\n    caching              = \"ReadWrite\"\n    storage_account_type = \"Standard_LRS\"\n  }\n  source_image_reference {\n    publisher = \"RedHat\"\n    offer     = \"RHEL\"\n    sku       = \"82gen2\"\n    version   = \"latest\"\n  }\n}\n\nresource \"azurerm_lb\" \"lb\" {\n  for_each            = local.lbs.tiers\n  name                = each.value.lb_name\n  location            = data.azurerm_resource_group.rg.location\n  resource_group_name = data.azurerm_resource_group.rg.name\n  sku                 = \"Standard\"\n\n  frontend_ip_configuration {\n    name                          = each.value.fip_name\n    subnet_id                     = each.value.subnet_id\n    private_ip_address_allocation = each.value.private_ip_type\n  }\n}\n\nresource \"azurerm_lb_backend_address_pool\" \"bepool\" {\n  for_each        = local.lbs.tiers\n  loadbalancer_id = azurerm_lb.lb[each.key].id\n  name            = each.value.address_pool_name\n}\n\nresource \"azurerm_lb_probe\" \"probe\" {\n  for_each = { for lb, details in local.lbs.tiers : lb => details.lb_probes }\n\n  loadbalancer_id = azurerm_lb.lb[each.key].id\n  name            = \"ssh-probe\"\n  protocol        = each.value[\"ssh_probe\"].protocol\n  port            = each.value[\"ssh_probe\"].port\n\n}\n\nresource \"azurerm_lb_rule\" \"rule\" {\n  for_each = { for lb, details in local.lbs.tiers : lb => details.lb_rules }\n\n  loadbalancer_id                = azurerm_lb.lb[each.key].id\n  name                           = \"ssh-rule\"\n  protocol                       = each.value[\"ssh_rule\"].protocol\n  frontend_port                  = each.value[\"ssh_rule\"].frontend_port\n  backend_port                   = each.value[\"ssh_rule\"].backend_port\n  frontend_ip_configuration_name = azurerm_lb.lb[each.key].frontend_ip_configuration[0].name\n  enable_floating_ip             = each.value[\"ssh_rule\"].enable_floating_ip\n  backend_address_pool_ids       = [azurerm_lb_backend_address_pool.bepool[each.key].id]\n  probe_id                       = azurerm_lb_probe.probe[each.key].id\n}\n`\n```\nBelow is the code I have problem to handle the two different maps using for_loop\n```\n`   resource \"azurerm_network_interface_backend_address_pool_association\" \"pool1-1\" {\n      for_each                    = local.vms.nodes\n      network_interface_id        = [for nic_key, nic in azurerm_network_interface.nic-poc : nic.id if startswith(nic_key, \"${each.key}-\")]\n      ip_configuration_name       = [for nic_key, nic in azurerm_network_interface.nic-poc : nic.ip_configuration.name if startswith(nic_key, \"${each.key}-\")]\n      backend_address_pool_id     = azurerm_lb_backend_address_pool.bepool[each.key].id\n    }\n`\n```\nError :\n```\n`Error: Invalid index\n\u2502\n\u2502   on main.tf line 238, in resource \"azurerm_network_interface_backend_address_pool_association\" \"pool1-1\":\n\u2502  238:   backend_address_pool_id     = azurerm_lb_backend_address_pool.bepool[each.key].id\n\u2502     \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\u2502     \u2502 azurerm_lb_backend_address_pool.bepool is object with 2 attributes\n\u2502     \u2502 each.key is \"db_node2\"\n\u2502\n\u2502 The given key does not identify an element in this collection value.\n`\n```\nOther errors for the azurerm_network_interface_backend_address_pool_association resource\n```\n`  Error: Incorrect attribute value type\n\u2502\n\u2502   on main.tf line 340, in resource \"azurerm_network_interface_backend_address_pool_association\" \"pool1-1\":\n\u2502  340:   network_interface_id        = [for nic_key, nic in azurerm_network_interface.nic-poc : nic.id if startswith(nic_key, \"${each.key}-\")]       \n\u2502     \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\u2502     \u2502 azurerm_network_interface.nic-poc is object with 8 attributes\n\u2502     \u2502 each.key is \"service_node1\"\n\u2502\n\u2502 Inappropriate value for attribute \"network_interface_id\": string required.\n\nError: Incorrect attribute value type\n\u2502\n\u2502   on main.tf line 341, in resource \"azurerm_network_interface_backend_address_pool_association\" \"pool1-1\":\n\u2502  341:   ip_configuration_name       = [for nic_key, nic in azurerm_network_interface.nic-poc : nic.ip_configuration.name if startswith(nic_key, \"${each.key}-\")]\n\u2502     \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\u2502     \u2502 azurerm_network_interface.nic-poc is object with 8 attributes\n\u2502     \u2502 each.key is \"service_node2\"\n\u2502\n\u2502 Inappropriate value for attribute \"ip_configuration_name\": string required.\n`\n```\nCould someone throw someone light on this ? Thank you in advance.\nHelder Sepulveda suggestion error :\n```\n`\u2502 Error: Invalid index\n\u2502\n\u2502   on main.tf line 199, in resource \"azurerm_network_interface_backend_address_pool_association\" \"nic_lb_association\":\n\u2502  199:   ip_configuration_name   = azurerm_network_interface.nic-poc[each.key].ip_configuration[0].name\n\u2502     \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\u2502     \u2502 azurerm_network_interface.nic-poc is object with 8 attributes\n\u2502     \u2502 each.key is \"db_node1\"\n\u2502\n\u2502 The given key does not identify an element in this collection value.\n`\n```",
      "solution": "From the first line in your question you have:\n\n... requirement\napp-poc-1-nic & app-poc-2-nic to app-lb backendpool\ndb-nic-1-1-nic & db-nic-1-2-nic to db-lb backendpool\n\nas suggested by @VonC you need a map that creates that relation, I like his approach but I do not like hardcoding that map, instead we can get it dynamically ...\nWe can see that the vm nodes have `app` and `db` as a prefix same with the lb tiers, I'm going to assume that pattern remains the same in larger dataset, so we can use that prefix to build the map, see my sample code below:\n```\n`locals {\n  vms = {\n    nodes = {\n      app_node1 = {},\n      app_node2 = {},\n      service_node1 = {},\n      service_node2 = {},\n      db_node1 = {},\n      db_node2 = {},\n    },\n  }\n  lbs = {\n    tiers = {\n        app-lb = {},\n        db-lb = {}\n    }\n  }\n\n  net_backend = {\n    for vm in keys(local.vms.nodes) :\n        vm => [\n            for lb in keys(local.lbs.tiers) : lb\n            if split(\"-\", lb)[0] == split(\"_\", vm)[0]\n        ]\n  }\n  vm_to_lb_map = {\n    for k, v in local.net_backend : k => v[0]\n    if length(v) > 0\n  }\n}\n\noutput \"vm_to_lb_map\" {\n  value = local.vm_to_lb_map\n} \n`\n```\nI'm simplifying your data to keep the code short, for our purposes we really do not care about the values so I used just `{}` hopefully that does not confuse anyone.\nLet's break it down...\nI added two new local variables\n```\n`  net_backend = {\n    for vm in keys(local.vms.nodes) :\n        vm => [\n            for lb in keys(local.lbs.tiers) : lb\n            if split(\"-\", lb)[0] == split(\"_\", vm)[0]\n        ]\n  }\n`\n```\nThe `net_backend` loops over the vm nodes and the lb tiers looking for matches in the prefix, (notice you are splitting by underscore in one but dashes on the other, would be nice to stick to one and keep it standard) at the end of this we end up with some records that do not have a match...\n```\n`  vm_to_lb_map = {\n    for k, v in local.net_backend : k => v[0]\n    if length(v) > 0\n  }\n`\n```\nThe `vm_to_lb_map` cleans up those that did not not get a match\n\n...and a terraform plan on that code will give us:\n`terraform plan\n\nChanges to Outputs:\n  + vm_to_lb_map = {\n      + app_node1 = \"app-lb\"\n      + app_node2 = \"app-lb\"\n      + db_node1  = \"db-lb\"\n      + db_node2  = \"db-lb\"\n    }\n`\nthen you can do the same as suggested by @VonC\n```\n`resource \"azurerm_network_interface_backend_address_pool_association\" \"nic_lb_association\" {\n  for_each = local.vm_to_lb_map\n\n  network_interface_id    = azurerm_network_interface.nic-poc[each.key].id\n  ip_configuration_name   = azurerm_network_interface.nic-poc[each.key].ip_configuration[0].name\n  backend_address_pool_id = azurerm_lb_backend_address_pool.bepool[each.value].id\n}\n`\n```",
      "question_score": 3,
      "answer_score": 3,
      "created_at": "2023-12-09T10:22:10",
      "url": "https://stackoverflow.com/questions/77630758/how-to-loop-two-different-maps-using-for-each-loop-in-terraform"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-azure",
      "question_id": 72812811,
      "title": "Terraform how to create a block of code based on a conditions",
      "problem": "I'm trying to create a resource with a creation condition for the github_configuration block, but I don't find such an option.\nDepending on the environment I need or don't need git config.\nI can't figure out how I can set up such a dependency\n```\n`resource \"azurerm_data_factory\" \"myDataFactory\" {\n  name                = var.datafactortyName\n  location            = azurerm_resource_group.myResourceGroup.location\n  resource_group_name = azurerm_resource_group.myResourceGroup.name\n\n  dynamic \"github_configuration\" {\n  #count = var.Environment == \"dev\" ? 1 : 0\n  for_each = var.Environment == \"dev\"\n\n    content {\n      account_name    = var.Environment == \"dev\" ? var.AccountName : null\n      branch_name     = var.Environment == \"dev\" ? var.Branch : null\n      git_url         = var.Environment == \"dev\" ? var.RepoUrl : null\n      repository_name = var.Environment == \"dev\" ? var.RepoName : null\n      root_folder     = var.Environment == \"dev\" ? var.RepoFolder : null\n    }\n  }\n}\n`\n```",
      "solution": "To make `github_configuration` block optional, you can do:\n```\n`  dynamic \"github_configuration\" {\n\n    for_each = var.Environment == \"dev\" ? [1] : []\n\n    content {\n      account_name    = var.Environment == \"dev\" ? var.AccountName : null\n      branch_name     = var.Environment == \"dev\" ? var.Branch : null\n      git_url         = var.Environment == \"dev\" ? var.RepoUrl : null\n      repository_name = var.Environment == \"dev\" ? var.RepoName : null\n      root_folder     = var.Environment == \"dev\" ? var.RepoFolder : null\n    }\n  }\n}\n`\n```\n`[1]` ensures that `for_each` executes ones. Actual value of `1` is irrelevant. It can be any value, as long as the list have one element.",
      "question_score": 3,
      "answer_score": 2,
      "created_at": "2022-06-30T10:46:00",
      "url": "https://stackoverflow.com/questions/72812811/terraform-how-to-create-a-block-of-code-based-on-a-conditions"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-azure",
      "question_id": 67800978,
      "title": "How to correct this error in creating Azure NSG with Terraform?",
      "problem": "I am trying  to create a NSG in Azure with Terraform.\nTerraform Version is v0.15.2 with provider version azurerm v2.61.0\nHere's the piece of code in my TF file.\n```\n`  resource \"azurerm_network_security_group\" \"nsg\" {\n  name                = \"SG\"\n  location            = azurerm_resource_group.rg.location\n  resource_group_name = azurerm_resource_group.rg.name\n\n  security_rule = [{\n    access                                     = \"Allow\"\n    description                                = \"SSH Rule\"\n    destination_address_prefix                 = \"*\"\n    destination_address_prefixes               = [\"*\"]\n    destination_port_range                     = \"*\"\n    destination_port_ranges                    = [\"22\"]\n    direction                                  = \"Inbound\"\n    name                                       = \"SSH Rule\"\n    priority                                   = 100\n    protocol                                   = \"Tcp\"\n    source_address_prefix                      = \"*\"\n    source_address_prefixes                    = [\"*\"]\n    source_port_range                          = \"22\"\n    source_port_ranges                         = [\"22\"]\n    source_application_security_group_ids      = [\"\"]\n    destination_application_security_group_ids = [\"\"]\n\n  }]\n}\n`\n```\nNow when I run `terraform plan` & `terraform apply`, I get the expected output as:\n```\n`* only one of \"source_port_range\" and \"source_port_ranges\" can be used per security rule\n* only one of \"destination_port_range\" and \"destination_port_ranges\" can be used per security rule\n* only one of \"source_address_prefix\" and \"source_address_prefixes\" can be used per security rule\n* only one of \"destination_address_prefix\" and \"destination_address_prefixes\" can be used per security rule\n`\n```\nNow when I keep only `source_port_range, destination_port_range, source_address_prefix, destination_address_prefix` fields & run `terraform plan` again, it gives me following error:\n```\n`Inappropriate value for attribute \"security_rule\": element 0: attributes \"destination_address_prefixes\",\n\u2502 \"destination_port_ranges\", \"source_address_prefixes\", and \"source_port_ranges\" are required.\n`\n```\nIf I add those & remove the earlier ones, I get:\n```\n`\u2502 Inappropriate value for attribute \"security_rule\": element 0: attributes \"destination_address_prefix\", \"destination_port_range\",  \n\u2502 \"source_address_prefix\", and \"source_port_range\" are required.\n`\n```\nWhy is this happening & how to get around with this issue?\nUpdate\nTook into consideration the point mentioned in the comment & made few changes to get it working.\nPoints to note:\n\nEven when it's mentioned Optional in the doc, Terraform needs all the keys in the security_rule block.\nAlso, it doesn't allow Source/destination address_prefixes = [\"*\"] like this. Source/Destination address_prefix did the job.\nAnd where values are not mentioned, [] is expected instead of [\"\"]\n\n```\n`resource \"azurerm_network_security_group\" \"nsg\" {\n  name                = \"SG\"\n  location            = azurerm_resource_group.rg.location\n  resource_group_name = azurerm_resource_group.rg.name\n\n  security_rule = [{\n    access                                     = \"Allow\"\n    description                                = \"SSH Rule\"\n    destination_address_prefix                 = \"*\"\n    destination_address_prefixes               = []\n    destination_port_range                     = \"\"\n    destination_port_ranges                    = [\"22\"]\n    direction                                  = \"Inbound\"\n    name                                       = \"SSH Rule\"\n    priority                                   = 100\n    protocol                                   = \"Tcp\"\n    source_address_prefix                      = \"*\"\n    source_address_prefixes                    = []\n    source_port_range                          = \"*\"\n    source_port_ranges                         = []\n    source_application_security_group_ids      = []\n    destination_application_security_group_ids = []\n\n  }]\n}\n`\n```",
      "solution": "Can you try this:\n```\n`  security_rule = [{\n    access                                     = \"Allow\"\n    description                                = \"SSH Rule\"\n    destination_address_prefix                 = \"\"\n    destination_address_prefixes               = [\"*\"]\n    destination_port_range                     = \"\"\n    destination_port_ranges                    = [\"22\"]\n    direction                                  = \"Inbound\"\n    name                                       = \"SSH Rule\"\n    priority                                   = 100\n    protocol                                   = \"Tcp\"\n    source_address_prefix                      = \"\"\n    source_address_prefixes                    = [\"*\"]\n    source_port_range                          = \"\"\n    source_port_ranges                         = [\"22\"]\n    source_application_security_group_ids      = []\n    destination_application_security_group_ids = []\n\n  }]\n}\n`\n```\nYou're defining values in both fields - i.e. Terraform will only accept one of either `destination_port_range` or `destination_port_ranges` and not both. Same goes for the other attributes.",
      "question_score": 3,
      "answer_score": 2,
      "created_at": "2021-06-02T09:49:54",
      "url": "https://stackoverflow.com/questions/67800978/how-to-correct-this-error-in-creating-azure-nsg-with-terraform"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-azure",
      "question_id": 65928717,
      "title": "How to deploy a Windows VM with Terraform Azure CAF?",
      "problem": "I want to deploy a Windows VM with Azure Cloud Adoption Framework (CAF) using Terraform. In the example of configuration.tfvars, all the configuration is done.But I cannot find the correct terraform code to deploy this tfvars configuration.\nThe windows vm module is here.\nSo far, i have written the code below:\n```\n`module \"caf_virtual_machine\" {\n  source  = \"aztfmod/caf/azurerm//modules/compute/virtual_machine\"\n  version = \"5.0.0\"\n  # belows are the 7 required variables \n\n  base_tags = var.tags\n  client_config = \n  global_settings = var.global_settings\n  location = var.location\n  resource_group_name = var.resource_group_name\n  settings = \n  vnets =  var.vnets  \n}\n`\n```\nSo the `vnets, global_settings, resource_group_name` variables already exists in the configuration.tfvars. I have added `tags` and `location` variables to the configuration.tfvars.\nBut what should i enter to `settings` and `client_config` variables?",
      "solution": "The virtual machine is a private module. You should use it by calling the base CAF module.\nThe Readme of the terraform registry explains how to leverage the core CAF module - https://registry.terraform.io/modules/aztfmod/caf/azurerm/latest/submodules/virtual_machine\nSource code of an example:\nhttps://github.com/aztfmod/terraform-azurerm-caf/tree/master/examples/compute/virtual_machine/211-vm-bastion-winrm-agents/registry\nYou have a library of configuration files examples showing how to deploy virtual machines\nhttps://github.com/aztfmod/terraform-azurerm-caf/tree/master/examples/compute/virtual_machine\n```\n`   module \"caf\" {\n    source  = \"aztfmod/caf/azurerm\"\n    version = \"5.0.0\"\n    \n    global_settings    = var.global_settings\n    tags               = var.tags\n    resource_groups    = var.resource_groups\n    storage_accounts   = var.storage_accounts\n    keyvaults          = var.keyvaults\n    managed_identities = var.managed_identities\n    role_mapping       = var.role_mapping\n    \n    diagnostics = {\n      # Get the diagnostics settings of services to create\n      diagnostic_log_analytics    = var.diagnostic_log_analytics\n      diagnostic_storage_accounts = var.diagnostic_storage_accounts\n    }\n    \n    compute = {\n      virtual_machines = var.virtual_machines\n    }\n    \n    networking = {\n      vnets                             = var.vnets\n      network_security_group_definition = var.network_security_group_definition\n      public_ip_addresses               = var.public_ip_addresses\n    }\n    \n    security = {\n      dynamic_keyvault_secrets = var.dynamic_keyvault_secrets\n    }\n  }\n`\n```\nNote - it is recommended to leverage the VScode devcontainer provided in the source repository to execute the terraform deployment. The devcontainer includes the tooling required to deploy Azure solutions.",
      "question_score": 3,
      "answer_score": 2,
      "created_at": "2021-01-28T00:04:38",
      "url": "https://stackoverflow.com/questions/65928717/how-to-deploy-a-windows-vm-with-terraform-azure-caf"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-azure",
      "question_id": 75249627,
      "title": "The &quot;count&quot; value depends on resource attributes that cannot be determined until apply, so Terraform cannot predict how many instances will be created",
      "problem": "I want to exempt certain policies for an Azure VM. I have the following terraform code to exempt the policies.\nIt uses locals to identify the scope on which  policies should be exempt.\n```\n`locals {\n  exemption_scope = try({\n    mg       = length(regexall(\"(\\\\/managementGroups\\\\/)\", var.scope)) > 0 ? 1 : 0,\n    sub      = length(split(\"/\", var.scope)) == 3 ? 1 : 0,\n    rg       = length(regexall(\"(\\\\/managementGroups\\\\/)\", var.scope)) = 6 ? 1 : 0,\n  })\n\n  expires_on = var.expires_on != null ? \"${var.expires_on}T23:00:00Z\" : null\n\n  metadata = var.metadata != null ? jsonencode(var.metadata) : null\n\n  # generate reference Ids when unknown, assumes the set was created with the initiative module\n  policy_definition_reference_ids = length(var.member_definition_names) > 0 ? [for name in var.member_definition_names :\n    replace(substr(title(replace(name, \"/-|_|\\\\s/\", \" \")), 0, 64), \"/\\\\s/\", \"\")\n  ] : var.policy_definition_reference_ids\n\n  exemption_id = try(\n    azurerm_management_group_policy_exemption.management_group_exemption[0].id,\n    azurerm_subscription_policy_exemption.subscription_exemption[0].id,\n    azurerm_resource_group_policy_exemption.resource_group_exemption[0].id,\n    azurerm_resource_policy_exemption.resource_exemption[0].id,\n  \"\")\n}\n`\n```\nand the above local is used like mentioned below\n```\n`resource \"azurerm_management_group_policy_exemption\" \"management_group_exemption\" {\n  count                           = local.exemption_scope.mg\n  name                            = var.name\n  display_name                    = var.display_name\n  description                     = var.description\n  management_group_id             = var.scope\n  policy_assignment_id            = var.policy_assignment_id\n  exemption_category              = var.exemption_category\n  expires_on                      = local.expires_on\n  policy_definition_reference_ids = local.policy_definition_reference_ids\n  metadata                        = local.metadata\n}\n`\n```\nBoth the locals and azurerm_management_group_policy_exemption are part of the same module file. And Policy exemption is applied like mentioned below\n```\n`module exemption_jumpbox_sql_vulnerability_assessment {\n  count                           = var.enable_jumpbox == true ? 1 : 0  \n  source                          = \"../policy_exemption\"\n  name                            = \"Exemption - SQL servers on machines should have vulnerability\"\n  display_name                    = \"Exemption - SQL servers on machines should have vulnerability\"\n  description                     = \"Not required for Jumpbox\"\n  scope                           = module.create_jumbox_vm[0].virtual_machine_id\n  policy_assignment_id            = module.security_center.azurerm_subscription_policy_assignment_id\n  policy_definition_reference_ids = var.exemption_policy_definition_ids\n  exemption_category              = \"Waiver\"\n  depends_on                      = [module.create_jumbox_vm,module.security_center]\n}\n`\n```\nIt works for an existing Azure VM. However it throws the following error while trying to provision the Azure VM and apply the policy exemption on this Azure VM.\nIdeally, module.exemption_jumpbox_sql_vulnerability_assessment should get executed only after [module.create_jumbox_vm as it is defined as a dependent. But not sure why it is throwing the error\n```\n`\u2502 The \"count\" value depends on resource attributes that cannot be determined\n\u2502 until apply, so Terraform cannot predict how many instances will be\n\u2502 created. To work around this, use the -target argument to first apply only\n\u2502 the resources that the count depends on.\n`\n```",
      "solution": "I tried to reproduce the scenario in my environment.\n```\n`resource \"azurerm_management_group_policy_exemption\" \"management_group_exemption\" {\n  count                           = local.exemption_scope.mg\n  name                            = var.name\n  display_name                    = var.display_name\n  description                     = var.description\n  management_group_id             = var.scope\n  policy_assignment_id            = var.policy_assignment_id\n  exemption_category              = var.exemption_category\n  expires_on                      = local.expires_on\n  policy_definition_reference_ids = local.policy_definition_reference_ids\n  metadata                        = local.metadata\n}\n\nlocals {\n  exemption_scope = try({\n        ...\n  })\n`\n```\nReceived the same error:\n```\n`The \"count\" value depends on resource attributes that cannot be determined\n\u2502 until apply, so Terraform cannot predict how many instances will be\n\u2502 created. To work around this, use the -target argument to first apply only\n\u2502 the resources that the count depends on.\n`\n```\nReferring to local values, the values will be known on the apply time only, and not during the apply time. So if it is not dependent on other sources, it will exempt policies but it is dependent on the VM which may be still in process of creation.\nSo target only the resource that is dependent on first, as only when vm is created is when the exemption policy can be assigned to that vm.\nCheck count:using-expressions-in-count | Terraform | HashiCorp Developer\n\nAlso note that while using terraform count argument with Azure Virtual Machines, NIC resource also has to be created for each Virtual Machine resource.\n```\n`resource \"azurerm_network_interface\" \"nic\" {\n  count               = var.vm_count\n  name                = \"${var.vm_name_pfx}-${count.index}-nic\"\n  location            = data.azurerm_resource_group.example.location\n  resource_group_name = data.azurerm_resource_group.example.name\n  //tags = var.tags\n \n\n  ip_configuration {\n    name                          = \"internal\"\n    subnet_id                     = azurerm_subnet.internal.id\n    private_ip_address_allocation = \"Dynamic\"\n  }\n}\n`\n```\n\nReference: terraform-azurerm-policy-exemptions/examples/count at main \u00b7 AnsumanBal-MT/terraform-azurerm-policy-exemptions \u00b7 GitHub",
      "question_score": 3,
      "answer_score": 1,
      "created_at": "2023-01-26T18:23:12",
      "url": "https://stackoverflow.com/questions/75249627/the-count-value-depends-on-resource-attributes-that-cannot-be-determined-until"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-azure",
      "question_id": 69512359,
      "title": "Terraform data source not able to fetch existing resource",
      "problem": "Unable to get existing resource using Terraform Data source. I am getting below error,\nIt can run successfully if it is for one storage account.\nBut what I need to achieve is, for first storage account create  Private DNS Zone that is \"privatelink.blob.core.windows.net\", subsquent storage account use existing Private DNS Zone that is \"privatelink.blob.core.windows.net\".\nFor that, I am using lookup variable, if it is true , do not create just look for existing resource, if false create it.\nCan somebody please help, what I am doing mistake in below code. All required details shared below. If additional information is required, please do comment.\n```\n`Error: Private DNS Zone \"privatelink.blob.core.windows.net\" (Resource Group \"rg1\") was not found\n\u2502\n\u2502   with module.storage[1].data.azurerm_private_dns_zone.lookup[0],\n\u2502   on ../../../../modules/storage/main.tf line 57, in data \"azurerm_private_dns_zone\" \"lookup\":\n\u2502   57: data \"azurerm_private_dns_zone\" \"lookup\" {\n`\n```\nBelow is the module main.tf\n```\n`resource \"azurerm_storage_account\" \"main\" {\n  name                          = var.storage_name\n  resource_group_name           = var.storage_resource_group_name\n  location                      = var.storage_location\n  account_tier                  = var.account_tier\n  account_kind                  = var.account_kind\n  account_replication_type      = var.account_replication_type\n  enable_https_traffic_only     = var.enable_https_traffic_only\n  min_tls_version               = var.min_tls_version\n  allow_blob_public_access      = var.allow_blob_public_access\n  tags                          = var.tags\n  depends_on = [\n    azurerm_private_dns_zone.main\n  ]\n}\n\nresource \"azurerm_storage_container\" \"main\" {\n  count                 = length(var.container_names)\n  name                  = var.container_names[count.index]\n  storage_account_name  = azurerm_storage_account.main.name\n  container_access_type = var.container_access_type\n}\n\nresource \"azurerm_storage_account_network_rules\" \"main\" {\n  storage_account_id = azurerm_storage_account.main.id\n\n  default_action             = var.default_action\n  ip_rules                   = var.ip_rules\n  virtual_network_subnet_ids = var.virtual_network_subnet_ids\n  bypass                     = var.bypass\n}\n\nresource \"azurerm_private_endpoint\" \"main\" {\n  name                = var.pep_name\n  location            = var.pep_location\n  resource_group_name = var.pep_resource_group_name\n  subnet_id           = var.pep_subnet_id\n\n  private_service_connection {\n    name                           = var.psc_name\n    private_connection_resource_id = azurerm_storage_account.main.id\n    subresource_names              = var.subresource_names\n    is_manual_connection           = var.is_manual_connection\n  }\n  private_dns_zone_group {\n       name                  = var.private_dns_group_name\n       private_dns_zone_ids  = var.lookup_private_dns_zone_name ? [data.azurerm_private_dns_zone.lookup[0].id] : [azurerm_private_dns_zone.main[0].id]\n  }\n}\n\nresource \"azurerm_private_dns_zone\" \"main\" {\n  count               = var.lookup_private_dns_zone_name ? 0 : 1\n  name                = var.private_dns_zone_name\n  resource_group_name = var.private_dns_zone_resource_group_name\n}\n\ndata \"azurerm_private_dns_zone\" \"lookup\" {\n  count               = var.lookup_private_dns_zone_name ? 1 : 0\n  name                = azurerm_private_dns_zone.main[0].name\n  resource_group_name = azurerm_private_dns_zone.main[0].resource_group_name\n\n  depends_on = [\n    azurerm_private_dns_zone.main\n  ]\n}\n\nresource \"azurerm_private_dns_zone_virtual_network_link\" \"main\" {\n  name                  = var.private_dns_zone_virtual_network_link_name\n  resource_group_name   = var.private_dns_zone_virtual_network_link_resource_group_name\n  private_dns_zone_name = var.lookup_private_dns_zone_name ? data.azurerm_private_dns_zone.lookup[0].name : azurerm_private_dns_zone.main[0].name\n  virtual_network_id    = var.private_dns_zone_virtual_nevtwork_id\n}\n`\n```\ncalling root module:\n```\n`module \"storage\" {\n  count                                                     = length(var.storage)\n  source                                                    = \"../../../../modules/storage\"\n  storage_name                                              = join(\"\", [local.prefix, \"store\", var.storage[count.index].id])\n  lookup_private_dns_zone_name                              = try(var.storage[count.index].lookup_private_dns_zone_name, false)\n  storage_resource_group_name                               = var.storage[count.index].resource_group_name\n  storage_location                                          = var.storage[count.index].location\n  account_replication_type                                  = try(var.storage[count.index].account_replication_type, \"GRS\")\n  tags                                                      = merge(try(var.storage[count.index].tags, {}), local.tags)\n  container_names                                           = try(var.storage[count.index].container_names, [])\n  virtual_network_subnet_ids                                = try(var.storage[count.index].virtual_network_subnet_ids, [])\n  default_action                                            = try(var.storage[count.index].default_action, \"Deny\")\n  pep_name                                                  = join(\"\", [local.prefix, \"pepstore\", var.storage[count.index].id])\n  pep_location                                              = var.storage[count.index].location\n  pep_resource_group_name                                   = var.storage[count.index].resource_group_name\n  pep_subnet_id                                             = var.storage[count.index].pep_subnet_id\n  psc_name                                                  = join(\"\", [local.prefix, \"pscstore\", var.storage[count.index].id])\n  is_manual_connection                                      = false\n  private_dns_group_name                                    = join(\"\", [local.prefix, \"dnsgroupstore\", var.storage[count.index].id])\n  private_dns_zone_name                                     = \"privatelink.blob.core.windows.net\"\n  private_dns_zone_resource_group_name                      = var.storage[count.index].resource_group_name\n  private_dns_zone_virtual_network_link_name                = join(\"\", [local.prefix, \"pdzvnlstore\", var.storage[count.index].id])\n  private_dns_zone_virtual_network_link_resource_group_name = var.storage[count.index].resource_group_name\n  private_dns_zone_virtual_network_id                       = var.storage[count.index].private_dns_zone_virtual_network_id\n  depends_on                                                = [module.resource_group]\n}\n`\n```\ninput file poc.tfvars.json:\n```\n`{\"storage\": [\n        {\n            \"id\": \"04\",\n            \"resource_group_name\": \"rg1\",\n            \"location\": \"westus2\",\n            \"pep_subnet_id\": \"sub_net_resource_id\",\n            \"private_dns_zone_virtual_network_id\": \"virtual_network_id\",\n            \"container_names\": [\"containerinfratfswsu2ctedev\"]\n        },\n        {\n            \"id\": \"05\",\n            \"lookup_private_dns_zone_name\": true,\n            \"resource_group_name\": \"WUS2-DEV-PE-CTE-CCI-TF-REPO-RG\",\n            \"location\": \"westus2\",\n            \"pep_subnet_id\": \"subnet_resource_id\",\n            \"private_dns_zone_virtual_network_id\": \"virtual_network_id\"\n        }\n    ]}\n`\n```",
      "solution": "Since you are using:\n```\n`module \"storage\" {\n  count                                                     = length(var.storage)\n`\n```\nthe two instances of your module will be created concurrently, not in succession. So obviously, second instance of the module fails, because it is created at the same time (not after) the first instance. So at this very moment, there is no `azurerm_private_dns_zone`.\nYou either have to run the modules manually, one after the other with `depends_on`, or somehow extract the `lookup_private_dns_zone_name` functionality to its own module, that is run before everything else.\nOther alternative is to use External Data Source that activates for second instance module, and artificially halts its till the `azurerm_private_dns_zone` exists. Buts its very hacky to relay on that.",
      "question_score": 3,
      "answer_score": 1,
      "created_at": "2021-10-10T06:39:56",
      "url": "https://stackoverflow.com/questions/69512359/terraform-data-source-not-able-to-fetch-existing-resource"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-azure",
      "question_id": 68985139,
      "title": "Trying to create azure storage account &amp; use the same to store terraform state file",
      "problem": "on the way to create aks via terraform, here i want to create azure storage account & want to use the same same account to store the terraform state file.\nhowever getting below error\n\u2502 Error: Error loading state: Error retrieving keys for Storage Account \"azurerm_resource_group.aks_rg.name\": storage.AccountsClient#ListKeys: Invalid input: autorest/validation: validation failed: parameter=accountName constraint=MaxLength value=\"azurerm_resource_group.aks_rg.name\" details: value length must be less than or equal to 24\n\u2502\n```\n`#Create Resource Group\nresource \"azurerm_resource_group\" \"aks_rg\" {\n  location = \"${var.location}\"\n  name     = \"${var.global-prefix}-${var.cluster-id}-${var.environment}-azwe-aks-rg\"\n}\n\n#Create Storage Account & Container\nresource \"azurerm_storage_account\" \"storage_acc\" {\n  name                     = \"${var.cluster-id}-storage-account\"\n  resource_group_name      = azurerm_resource_group.aks_rg.name\n  location                 = azurerm_resource_group.aks_rg.location\n  account_tier             = \"Standard\"\n  account_replication_type = \"LRS\" \n}\nresource \"azurerm_storage_container\" \"storage_container\" {\n  name                  = \"${var.cluster-id}-storage-account-container\"\n  storage_account_name  = azurerm_storage_account.storage_acc.name\n  container_access_type = \"private\"\n}\n\n#store terraform state in remote container\nterraform {\n  # Configure Terraform State Storage\n  backend \"azurerm\" {\n    resource_group_name  = \"azurerm_resource_group.aks_rg.name\"\n    storage_account_name = \"azurerm_storage_container.storage_acc.name\"\n    container_name       = \"azurerm_storage_container.storage_container.name\"\n    key                  = \"terraform.tfstate\"\n  }\n}\n\n`\n```",
      "solution": "You need to first create the storage account and container then while creating the aks cluster you need to give the below:\n```\n`terraform {\n  # Configure Terraform State Storage\n  backend \"azurerm\" {\n    resource_group_name  = \"azurerm_resource_group.aks_rg.name\"\n    storage_account_name = \"azurerm_resource_group.aks_rg.name\"\n    container_name       = \"powermeprodtfstate\"\n    key                  = \"terraform.tfstate\"\n  }\n}\n`\n```\nInstead of creating the storage account and container ins the same file while storing the terraform tfstate.\nExample:\nCreate storage account and container:\n```\n`provider \"azurerm\" { \n  features {}\n}\n\ndata \"azurerm_resource_group\" \"example\" {\n  name     = \"resourcegroupname\"\n}\n\nresource \"azurerm_storage_account\" \"example\" {\n  name                     = \"yourstorageaccountname\"\n  resource_group_name      = data.azurerm_resource_group.example.name\n  location                 = data.azurerm_resource_group.example.location\n  account_tier             = \"Standard\"\n  account_replication_type = \"LRS\" \n}\nresource \"azurerm_storage_container\" \"example\" {\n  name                  = \"terraform\"\n  storage_account_name  = azurerm_storage_account.example.name\n  container_access_type = \"private\"\n}\n`\n```\n\nThen create the aks resource group and store the tfstate in container.\n```\n`provider \"azurerm\" { \n  features {}\n}\nterraform {\n  # Configure Terraform State Storage\n  backend \"azurerm\" {\n    resource_group_name  = \"resourcegroup\"\n    storage_account_name = \"storageaccountnameearliercreated\"\n    container_name       = \"terraform\"\n    key                  = \"terraform.tfstate\"\n  }\n}\n\nresource \"azurerm_resource_group\" \"aks_rg\" {\n name = \"aks-rg\"\n location = \"west us\"\n}\n`\n```\n\nReference:\nHow to store the Terraform state file in Azure Storage. \u00bb Jorge Bernhardt",
      "question_score": 3,
      "answer_score": 1,
      "created_at": "2021-08-30T15:40:09",
      "url": "https://stackoverflow.com/questions/68985139/trying-to-create-azure-storage-account-use-the-same-to-store-terraform-state-f"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-azure",
      "question_id": 68981358,
      "title": "Adding Azure Diagnostic Settings for VMSS",
      "problem": "I am currently running a Linux VMSS with Ubuntu 20.04 VMs created using terraform. I wish to add the Linux Azure Diagnostic (LAD) extension to enable Diagnostic Logs to the VMs. Here is my current terraform resources for this purpose\n```\n`resource \"time_offset\" \"linux_oms_sas_start\" {\n  offset_days = -1\n}\n\nresource \"time_offset\" \"linux_oms_sas_expiry\" {\n  offset_years = 5\n}\n\ndata \"azurerm_storage_account_sas\" \"linux_oms\" {\n  connection_string = var.storage_account_primary_connection_string\n  https_only        = true\n\n  resource_types {\n    service   = true\n    container = true\n    object    = true\n  }\n\n  services {\n    blob  = true\n    table = true\n    queue = false\n    file = false\n  }\n\n  start  = time_offset.linux_oms_sas_start.rfc3339\n  expiry = time_offset.linux_oms_sas_expiry.rfc3339\n\n  permissions {\n    read    = true\n    write   = true\n    delete  = true\n    list    = true\n    add     = true\n    create  = true\n    update  = true\n    process = true\n  }\n  depends_on = [time_offset.linux_oms_sas_start,time_offset.linux_oms_sas_expiry]\n}\n\nresource \"azurerm_virtual_machine_scale_set_extension\" \"da_extension\" {\n  name                       = \"DAExtension\"\n  virtual_machine_scale_set_id         = var.vmss_id\n  publisher                  = \"Microsoft.Azure.Monitoring.DependencyAgent\"\n  type                       = \"DependencyAgentLinux\"\n  type_handler_version       = \"9.5\"\n  auto_upgrade_minor_version = false\n}\n\nresource \"azurerm_virtual_machine_scale_set_extension\" \"diagnostics_extension\" {\n  name = \"StorageExtension\"\n  virtual_machine_scale_set_id =  var.vmss_id\n  publisher            = \"Microsoft.Azure.Diagnostics\"\n  type                 = \"LinuxDiagnostic\"\n  type_handler_version = \"4.0\"\n  auto_upgrade_minor_version = false\n\n  settings = However when applying the above terraform code, I am getting an error from the portal as below\n```\n`Enable failed:'NoneType' object has no attribute 'get_fluentd_syslog_src_config' \n`\n```\nAny help regarding on what the issue is would be greatly appreciated.\nP.S. I have attached the `azure_extension_diagnostics_linux_performancecounters.json` file and `azure_extension_diagnostics_linux_syslogevents.json` file used within the code for further reference if required.\n`azure_extension_diagnostics_linux_performancecounters.json` file\n```\n`{\n  \"performanceCounterConfiguration\": []\n}\n`\n```\nand the `azure_extension_diagnostics_linux_syslogevents.json` file\n```\n`{\n  \"syslogEventConfiguration\": {\n    \"LOG_AUTH\": \"LOG_DEBUG\",\n    \"LOG_AUTHPRIV\": \"LOG_DEBUG\",\n    \"LOG_CRON\": \"LOG_DEBUG\",\n    \"LOG_DAEMON\": \"LOG_DEBUG\",\n    \"LOG_FTP\": \"LOG_DEBUG\",\n    \"LOG_KERN\": \"LOG_DEBUG\",\n    \"LOG_LOCAL0\": \"LOG_DEBUG\",\n    \"LOG_LOCAL1\": \"LOG_DEBUG\",\n    \"LOG_LOCAL2\": \"LOG_DEBUG\",\n    \"LOG_LOCAL3\": \"LOG_DEBUG\",\n    \"LOG_LOCAL4\": \"LOG_DEBUG\",\n    \"LOG_LOCAL5\": \"LOG_DEBUG\",\n    \"LOG_LOCAL6\": \"LOG_DEBUG\",\n    \"LOG_LOCAL7\": \"LOG_DEBUG\",\n    \"LOG_LPR\": \"LOG_DEBUG\",\n    \"LOG_MAIL\": \"LOG_DEBUG\",\n    \"LOG_NEWS\": \"LOG_DEBUG\",\n    \"LOG_SYSLOG\": \"LOG_DEBUG\",\n    \"LOG_USER\": \"LOG_DEBUG\",\n    \"LOG_UUCP\": \"LOG_DEBUG\"\n  }\n}\n`\n```",
      "solution": "It's not supported to install Diagnostics Agent For Ubuntu 20.04. Only Azure Monitor Agent or Log Analytics Agent and Dependency Agent is possible .\nReference:\nOverview of the Azure monitoring agents - Azure Monitor | Microsoft Docs\nAzure Compute - Linux diagnostic extension 4.0 - Azure Virtual Machines | Microsoft Docs",
      "question_score": 3,
      "answer_score": 1,
      "created_at": "2021-08-30T10:55:42",
      "url": "https://stackoverflow.com/questions/68981358/adding-azure-diagnostic-settings-for-vmss"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-azure",
      "question_id": 71680143,
      "title": "Terraform Plan stuck at Terraform:Plan task in ADO pipeline",
      "problem": "I am trying to follow the below link to deploy an App Service using Terraform via Azure Dev Ops Pipeline\nAzureDevOpsLabs\nMy terraform files are given below :\nwebapp.tf\n```\n`terraform {\n  required_version = \"~> 1.0\" \n  backend \"azurerm\" {\n    storage_account_name = \"__terraformstorageaccount__\"\n    container_name       = \"terraform\"\n    key                  = \"terraform.tfstate\"\n    }\n}\n\nprovider \"azurerm\" {\n  tenant_id       = var.tenant_id\n  client_id       = var.client_id\n  client_secret   = var.client_secret\n  subscription_id = var.subscription_id\n  features {}\n}\n\nresource \"azurerm_resource_group\" \"dev\" {\n  name     = var.resource_group_name\n  location = var.resource_group_location\n}\n\nresource \"azurerm_app_service_plan\" \"dev\" {\n  name                = var.appserviceplan\n  location            = var.resource_group_location\n  resource_group_name = var.resource_group_name\n\n  sku {\n    tier = \"Free\"\n    size = \"F1\"\n  }\n  depends_on = [\n    azurerm_resource_group.dev\n  ]\n}\n\nresource \"azurerm_app_service\" \"dev\" {\n  name                = var.appservicename\n  location            = azurerm_app_service_plan.dev.location \n  resource_group_name = azurerm_app_service_plan.dev.location\n  app_service_plan_id = azurerm_app_service_plan.dev.id\n}\n`\n```\nvar.tf\n```\n`variable \"client_id\" {}\n\nvariable \"client_secret\" {}\n\nvariable \"tenant_id\" {}\n\nvariable \"subscription_id\" {}\n\nvariable \"appserviceplan\" {}\n\nvariable \"appservicename\" {}\n\nvariable \"resource_group_name\" {}\n\nvariable \"resource_group_location\" {}\n`\n```\nThe actual values are given in the pipeline variables\nI have deployed the same app service directly using open source terraform and it worked fine.\nBut the \"Terraform:Plan\" step is stuck in the Release Pipeline as shown in the screenshot.\nAny idea why this is happening and the plan is not finishing properly\n\nI have disabled Terraform Init and enabled debugging. But it still fails at Terraform Apply and i see the below logs. The task never finishes\nTerraform Apply Task\n```\n`Exit code 0 received from tool 'C:\\hostedtoolcache\\windows\\terraform\\1.1.5\\x64\\terraform.exe'\nSTDIO streams have closed for tool 'C:\\hostedtoolcache\\windows\\terraform\\1.1.5\\x64\\terraform.exe'\nprovider=azurerm\ncommandOptions=-auto-approve\nworkingDirectory=C:\\hostedtoolcache\\windows\\terraform\nenvironmentServiceNameAzureRM=a8ee372e-0734-4e50-aa5a-e19d9e5f2a62\nwhich 'terraform'\nfound: 'C:\\hostedtoolcache\\windows\\terraform\\1.1.5\\x64\\terraform.exe'\nwhich 'C:\\hostedtoolcache\\windows\\terraform\\1.1.5\\x64\\terraform.exe'\nfound: 'C:\\hostedtoolcache\\windows\\terraform\\1.1.5\\x64\\terraform.exe'\nC:\\hostedtoolcache\\windows\\terraform\\1.1.5\\x64\\terraform.exe arg: apply\nC:\\hostedtoolcache\\windows\\terraform\\1.1.5\\x64\\terraform.exe arg: -auto-approve\na8ee372e-0734-4e50-aa5a-e19d9e5f2a62 auth param serviceprincipalid = ***\na8ee372e-0734-4e50-aa5a-e19d9e5f2a62 auth param serviceprincipalkey = ***\na8ee372e-0734-4e50-aa5a-e19d9e5f2a62 data subscriptionid = xxxx-xxxx-xxxx\na8ee372e-0734-4e50-aa5a-e19d9e5f2a62 auth param tenantid = xxxx-xxxxxx\na8ee372e-0734-4e50-aa5a-e19d9e5f2a62 auth param serviceprincipalid = ***\na8ee372e-0734-4e50-aa5a-e19d9e5f2a62 auth param serviceprincipalkey = ***\nexec tool: C:\\hostedtoolcache\\windows\\terraform\\1.1.5\\x64\\terraform.exe\narguments:\n`\n```",
      "solution": "When running Terraform from within a non-interactive pipeline you must add the flag `-input=false` , otherwise Terraform will hang expecting user input.\nSee documentation here: https://www.terraform.io/cli/commands/plan#input-false",
      "question_score": 2,
      "answer_score": 6,
      "created_at": "2022-03-30T17:35:18",
      "url": "https://stackoverflow.com/questions/71680143/terraform-plan-stuck-at-terraformplan-task-in-ado-pipeline"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-azure",
      "question_id": 71440779,
      "title": "Terraform azurerm read current signed in user?",
      "problem": "Looking at the documentation I am unable to find a data source which gives me\nthe current user (preferably the email) logged in to `az` when using the azurerm provider in terraform.\nThis information is available when I run `az ad signed-in-user` and I would like to use it to tag the resources created by terraform in azure.\nIs this not possible right now?",
      "solution": "You can use azurerm_client_config to get the AD object ID for the current user and then look up the returned object id with azuread_user to get the user principal name (UPN).  Then, the UPN can be assigned to a tag.  In the code below, outputs are not necessary but are helpful for validation because their values appear in the plan.\n```\n`data \"azurerm_client_config\" \"current\" { }\n\ndata \"azuread_user\" \"current_user\" {\n  object_id = data.azurerm_client_config.current.object_id\n}\n\nresource \"azurerm_resource_group\" \"example-rg\" {\n  name     = \"example-rg\"\n  location = \"westus\"\n  tags = {\n    userCreated = data.azuread_user.current_user.user_principal_name\n  }\n}\n\noutput \"object_id\" {\n  value = data.azurerm_client_config.current.object_id\n}\n\noutput \"user_principal_name\" {\n  value = data.azuread_user.current_user.user_principal_name\n}\n`\n```",
      "question_score": 2,
      "answer_score": 6,
      "created_at": "2022-03-11T16:20:40",
      "url": "https://stackoverflow.com/questions/71440779/terraform-azurerm-read-current-signed-in-user"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-azure",
      "question_id": 69827443,
      "title": "Azure Terraform Web App private Endpoint virtual network",
      "problem": "I am trying to automate the deployment of an azure virtual network and azure web app.\nDuring the deployment of those resources, everything went just fine and no errors. So I wanted to try to activate the `private endpoint` on the web app. This is my configuration on terraform.\n```\n`resource \"azurerm_virtual_network\" \"demo-vnet\" {\n  name                = \"virtual-network-test\"\n  address_space       = [\"10.100.0.0/16\"]\n  location            = var.location\n  resource_group_name = azurerm_resource_group.rg-testing-env.name\n}\n\nresource \"azurerm_subnet\" \"front_end\" {\n  name                 = \"Front_End-Subnet\"\n  address_prefixes     = [\"10.100.5.0/28\"]\n  virtual_network_name = azurerm_virtual_network.demo-vnet.name\n  resource_group_name  = azurerm_resource_group.rg-testing-env.name\n  delegation {\n    name = \"testing-frontend\"\n    service_delegation {\n      name    = \"Microsoft.Web/serverFarms\"\n      actions = [\"Microsoft.Network/virtualNetworks/subnets/action\"]\n    }\n  }\n}\n`\n```\nAnd on the web app itself, I set this configuration\n```\n`resource \"azurerm_app_service_virtual_network_swift_connection\" \"web-app-vnet\" {\n  app_service_id = azurerm_app_service.app-test.example.id\n  subnet_id      = azurerm_subnet.front_end.id\n}\n`\n```\nNOTE: On my first deployment, the swift failed because I had not delegation on the virtual network, so I had to add the delegation on the subnet to be able to run terraform.\nAfter setting in place all the configuration, I run my terraform, everything run just smoothly, no errors.\nAfter the completion, I checked my web app `Private Endpoint` and that was just off.\n\nCan please anyone explain me what am I doing wrong here?. I thought that the `swift` connection was the block of code to activate the `Private endpoint` but apparently I am missing something else.\nJust to confirm my logic workflow, I tried to do the manual steps in the portal. But surprisingly I was not able because I have the delegation on the subnet, as you can see.\n\nThank you so much for any help and/or explanation you can offer to solve this issue",
      "solution": "I have used the below code to test the creation of VNET and Web app with private endpoint.\n```\n`provider \"azurerm\" {\n    features{}\n}\n\ndata \"azurerm_resource_group\" \"rg\" {\n  name     = \"ansumantest\"\n}\n\n# Virtual Network\nresource \"azurerm_virtual_network\" \"vnet\" {\n  name                = \"ansumanapp-vnet\"\n  location            = data.azurerm_resource_group.rg.location\n  resource_group_name = data.azurerm_resource_group.rg.name\n  address_space       = [\"10.4.0.0/16\"]\n}\n\n# Subnets for App Service instances\nresource \"azurerm_subnet\" \"appserv\" {\n  name                 = \"frontend-app\"\n  resource_group_name  = data.azurerm_resource_group.rg.name\n  virtual_network_name = azurerm_virtual_network.vnet.name\n  address_prefixes     = [\"10.4.1.0/24\"]\n  enforce_private_link_endpoint_network_policies = true\n  }\n\n \n# App Service Plan\nresource \"azurerm_app_service_plan\" \"frontend\" {\n  name                = \"ansuman-frontend-asp\"\n  location            = data.azurerm_resource_group.rg.location\n  resource_group_name = data.azurerm_resource_group.rg.name\n  kind                = \"Linux\"\n  reserved            = true\n\n  sku {\n    tier = \"Premium\"\n    size = \"P1V2\"\n  }\n}\n\n# App Service\nresource \"azurerm_app_service\" \"frontend\" {\n  name                = \"ansuman-frontend-app\"\n  location            = data.azurerm_resource_group.rg.location\n  resource_group_name = data.azurerm_resource_group.rg.name\n  app_service_plan_id = azurerm_app_service_plan.frontend.id\n\n}\n#private endpoint\n\nresource \"azurerm_private_endpoint\" \"example\" {\n  name                = \"${azurerm_app_service.frontend.name}-endpoint\"\n  location            = data.azurerm_resource_group.rg.location\n  resource_group_name = data.azurerm_resource_group.rg.name\n  subnet_id           = azurerm_subnet.appserv.id\n  \n\n  private_service_connection {\n    name                           = \"${azurerm_app_service.frontend.name}-privateconnection\"\n    private_connection_resource_id = azurerm_app_service.frontend.id\n    subresource_names = [\"sites\"]\n    is_manual_connection = false\n  }\n}\n\n# private DNS\nresource \"azurerm_private_dns_zone\" \"example\" {\n  name                = \"privatelink.azurewebsites.net\"\n  resource_group_name = data.azurerm_resource_group.rg.name\n}\n\n#private DNS Link\nresource \"azurerm_private_dns_zone_virtual_network_link\" \"example\" {\n  name                  = \"${azurerm_app_service.frontend.name}-dnslink\"\n  resource_group_name   = data.azurerm_resource_group.rg.name\n  private_dns_zone_name = azurerm_private_dns_zone.example.name\n  virtual_network_id    = azurerm_virtual_network.vnet.id\n  registration_enabled = false\n}\n`\n```\nRequirements:\n\nAs you can see from the above code the Private Endpoint , Private DNS and Private DNS Link block are required for creating the private endpoint and enabling it for the app service.\nThe `App service Plan` needs to have Premium Plan for having Private\nendpoint.\nThe Subnet to be used by `Private Endpoint` should have\n`enforce_private_link_endpoint_network_policies = true` set other\nwise it will error giving message as `subnet has private endpoint network policies enabled , it should be disabled to be used by Private endpoint`.\nDNS zone name should only be `privatelink.azurewebsites.net` as you are creating a private endpoint for webapp.\n\nOutputs:",
      "question_score": 2,
      "answer_score": 6,
      "created_at": "2021-11-03T16:22:14",
      "url": "https://stackoverflow.com/questions/69827443/azure-terraform-web-app-private-endpoint-virtual-network"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-azure",
      "question_id": 76200207,
      "title": "Is it possible to disable Databricks Managed Storage Account public access?",
      "problem": "We have a problem with public access on the managed storage account from Databricks in Azure.\nDatabricks has a default managed resource group, with a managed storage account that Databricks uses for the standard hive metastore (we do not use this metastore, since we are using unity catalog).\nThis storage account has public access enabled, but our Azure policies do not allow public access on storage accounts.\nWe are deploying using Terraform and as far as we can see there are no options to close the public access for this storage account.\nAs a sidenote: we have Databricks deployed with VNET injection.\nOne hacky solution is to always run an `az cli` command after our Databricks deployment to close the access, but we would rather not do this (edit: this is also not possibly due to `deny assignment` on the st).\nDoes anyone know if this access can be configured?",
      "solution": "May 2024: it's now possible to make this storage account private: https://learn.microsoft.com/en-us/azure/databricks/security/network/storage/firewall-support\nNo, it's not possible to do this right now, and really you can't make this change because of how managed resource group is configured.  And the public access is required for Databricks to work as many things, such as, logs, models, etc. are stored on DBFS Root (managed storage account) and needs to be accessed by the Azure Databricks control plane.\nBut if it comes to the data security, here are the relevant information.  The storage account has the deny policy that prevents from making changes & accessing the data for anyone except the Databricks application. So even it has public access, you can't generate SAS, use storage account key, etc.",
      "question_score": 2,
      "answer_score": 5,
      "created_at": "2023-05-08T13:35:05",
      "url": "https://stackoverflow.com/questions/76200207/is-it-possible-to-disable-databricks-managed-storage-account-public-access"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-azure",
      "question_id": 72920731,
      "title": "How to fix the &quot;count&quot; object can only be used in &quot;module&quot;, &quot;resource&quot;, and &quot;data&quot; blocks, and only when the &quot;count&quot; argument",
      "problem": "Objective: I am trying to create azure resources with `Terraform`\nCode I used in `main.tf`:\n```\n`resource \"azurerm_subnet\" \"clientdata_snet\" {\n  count                = var.clientdata_subnet_address_space != null ? 1 : 0\n  name                 = \"ClientDataSubnet\"\n  resource_group_name  = var.resource_group_name\n  virtual_network_name = azurerm_virtual_network.vnet.name\n  address_prefixes     = [\"${var.clientdata_subnet_address_space}\"]\n  service_endpoints    = var.service_endpoints\n}\n`\n```\nand now subsequently want to use `client_snet.id` for creating storage endpoint\n```\n`resource \"azurerm_private_endpoint\" \"sa_pe_blob\" {\n  name                = \"pe-stdlorpcbcntldevwe-blob-${random_string.postfix.result}\"\n  location            = var.location\n  resource_group_name = var.resource_group_name\n  subnet_id           = azurerm_subnet.clientdata_snet.id\n`\n```\nError I get is:\n```\n` Error: Missing resource instance key\n    \u2502 \n    \u2502   on main.tf line 470, in resource \"azurerm_private_endpoint\" \"sa_pe_blob\":\n    \u2502  470:   subnet_id           = azurerm_subnet.clientdata_snet.id\n    \u2502 \n    \u2502 Because azurerm_subnet.clientdata_snet has \"count\" set, its attributes must be accessed on specific instances.\n    \u2502 \n    \u2502 For example, to correlate with indices of a referring resource, use:\n    \u2502     azurerm_subnet.clientdata_snet[count.index]\n`\n```\nThen I referred to some posts here.. where I need to use like below:\n```\n`  subnet_id           = azurerm_subnet.clientdata_snet[count.index].id\n`\n```\nthen its giving me this error:\n```\n`Error: Reference to \"count\" in non-counted context\n    \u2502 \n    \u2502   on main.tf line 470, in resource \"azurerm_private_endpoint\" \"sa_pe_blob\":\n    \u2502  470:   subnet_id           = azurerm_subnet.clientdata_snet[count.index].id\n\u2502 \n\u2502 The \"count\" object can only be used in \"module\", \"resource\", and \"data\" blocks, and only when the \"count\" argument is set.\n`\n```\nReally confused, both ways its giving me error. I have only root module, I dont have any other modules.Can someone suggest what is correct way to do it ?",
      "solution": "If you are using `count` meta-argument, you have to use either the right index or use the same way of creating the second resource, by referencing the same variable to decide what the count will be. So the options are:\n```\n`resource \"azurerm_private_endpoint\" \"sa_pe_blob\" {\n  name                = \"pe-stdlorpcbcntldevwe-blob-${random_string.postfix.result}\"\n  location            = var.location\n  resource_group_name = var.resource_group_name\n  subnet_id           = azurerm_subnet.clientdata_snet[0].id # exact index\n}\n`\n```\nAs you can see, the `subnet_id` is now referencing a previously created resource with index of `0`. To understand how references to instances work when `count` is used, look in [1].\nThe second way you could do it is like this:\n```\n`resource \"azurerm_private_endpoint\" \"sa_pe_blob\" {\n  count               = var.clientdata_subnet_address_space != null ? 1 : 0\n  name                = \"pe-stdlorpcbcntldevwe-blob-${random_string.postfix.result}\"\n  location            = var.location\n  resource_group_name = var.resource_group_name\n  subnet_id           = azurerm_subnet.clientdata_snet[count.index].id # using count.index\n}\n`\n```\nThis way you will create a dependency between the subnet and the endpoint resources.\nAs you can see here, the resources created with `count` can be referenced either by specifying the exact index which is fine when there is only one resource, but much harder when there are more and the code would have to be repeated. The other way is to use the same variable with the `count` meta-argument.\nI strongly suggest going through the documentation to understand the `count` meta-argument better.\n\n[1] https://www.terraform.io/language/meta-arguments/count#referring-to-instances",
      "question_score": 2,
      "answer_score": 5,
      "created_at": "2022-07-09T12:48:46",
      "url": "https://stackoverflow.com/questions/72920731/how-to-fix-the-count-object-can-only-be-used-in-module-resource-and-dat"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-azure",
      "question_id": 70193492,
      "title": "Issue with Application ID URI in Azure AD App registration , while using Terraform",
      "problem": "I am trying to modify my application identifier URI from the format :\n\"`https://app-contoso.api-qa.contoso.onmicrosoft.com`\" to the format \"`api://(app id)`\"\nI am using Terraform to do this.\nSo if i just use :\n```\n`application_identifier_uris   = [format(\"api://%s\", random_uuid.contoso-api-app.result)]\n`\n```\ni get a random id and not the actual app ID.\nHow do i ensure that i get the actual App ID in the format \"api://(app ID)\"\nI have to use a random uuid as i get a cyclic error in Terraform if i refer the output of the app ID from my application module.\nI am using our own application module to do app registration",
      "solution": "i get a random id and not the actual app ID. How do i ensure that i\nget the actual App ID in the format \"api://(app ID)\" I have to use a\nrandom uuid as i get a cyclic error in Terraform if i refer the output\nof the app ID from my application module.\n\nYou are generating a random GUID and assigning the value there , so for that reason you will be getting random ID and not the actual app ID. As for the Cyclic Error you will be receiving error something like below image if you use a reference to AppID as the application is getting created the same time when you referencing it so after creation only the application id will be usable or identified.\n\nSo, the above is not possible from terraform for now ,as you already know there has been a Feature flag or Enhancement raised for the same in this Github Issue.\n\nFor solution you can use other management tools like `Powershell/Azure-CLI` to update the `identifier uri's`.\nAfter Azure AD application is created from terraform you can use AzureAD Modules or az ad app CLI module to update the application programatically.\nFor your use case Powershell script will be something like below:\n```\n`Connect-AzureAD\n$app= Get-AzureADApplication -Filter \"DisplayName eq 'ansumanterraformtest'\"\n$appobjectid = $app.ObjectId\n$appId = $app.AppId\nSet-AzureADApplication -ObjectId $app.ObjectId -IdentifierUris \"api://$appId\"\n`\n```\nOutputs:",
      "question_score": 2,
      "answer_score": 1,
      "created_at": "2021-12-02T03:22:05",
      "url": "https://stackoverflow.com/questions/70193492/issue-with-application-id-uri-in-azure-ad-app-registration-while-using-terrafo"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-azure",
      "question_id": 68887263,
      "title": "mutually exclusive attributes to one resource in terraform",
      "problem": "I need some help here, I'm trying to write a module for azure ACI container instance.\nI found two attributes are mutually exclusive, dns_name_label and network_profile_id.\nIf I set ip_address_type to public, I would like to use dns_name_label, but network_profile_id can't come to the same script\nvice versa, If I set ip_address_type to Private, I have to define network_profile_id, but dns_name_label can't come to script\nIf there anyway to contain both dns_name_label and network_profile_id, judging on ip_address_type?\n```\n`resource \"azurerm_container_group\" \"this\" {\n  name                = var.name\n  location            = var.location\n  resource_group_name = var.resource_group_name\n  ip_address_type     = var.ip_address_type\n  dns_name_label = var.ip_address_type ==\"Public\"&&length(var.dns_name_label)> 0 ? var.dns_name_label :\"\"\n   os_type            = var.os_type\n  restart_policy     = var.restart_policy\n  network_profile_id = var.ip_address_type == \"Private\" ? azurerm_network_profile.this[0].id : \"\"\n\n}\n`\n```\nnote above code doesn't work, I get error\n```\n`\"network_profile_id\": conflicts with dns_name_label\n`\n```",
      "solution": "You can use null instead of `\"\"`, which will eliminate the attribute from the resource is `null` is set as its value:\n```\n`resource \"azurerm_container_group\" \"this\" {\n  name                = var.name\n  location            = var.location\n  resource_group_name = var.resource_group_name\n  ip_address_type     = var.ip_address_type\n  dns_name_label = var.ip_address_type ==\"Public\"&&length(var.dns_name_label)> 0 ? var.dns_name_label : null\n   os_type            = var.os_type\n  restart_policy     = var.restart_policy\n  network_profile_id = var.ip_address_type == \"Private\" ? azurerm_network_profile.this[0].id : null\n}\n`\n```",
      "question_score": 2,
      "answer_score": 5,
      "created_at": "2021-08-23T06:20:33",
      "url": "https://stackoverflow.com/questions/68887263/mutually-exclusive-attributes-to-one-resource-in-terraform"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-azure",
      "question_id": 66866692,
      "title": "Terraform Azure VM Extension Custom Script from Local Script",
      "problem": "I'm trying to deploy Azure Windows VM with VM extension by Terraform that will install ADDS role for the Windows VM.\nI have one Powershell script file for the installation named install_adds.ps1 from my local machine\nThe Terraform file of VM Extension as below:\n```\n`resource \"azurerm_virtual_machine_extension\" \"main\" {\n  name                 = \"extensionTest\"\n  virtual_machine_id   = azurerm_virtual_machine.main.id\n  publisher            = \"Microsoft.Azure.Extensions\"\n  type                 = \"CustomScript\"\n  type_handler_version = \"2.0\"\n\n  settings = I have deployed the VM Extension Terraform file but not working\nI think my Terraform file syntax is wrong and the Windows does not run my PowerShell script.\nAny way can run my local PowerShell Script by Terraform VM Extension file?",
      "solution": "Based on the schemas you posted, you might be trying to deploy the wrong customer script extension (CSE for Linux VM) on Windows based VM.\nBelow extensions schemas what you could use depending on the OS:\nLinux:\n```\n`\"publisher\": \"Microsoft.Azure.Extensions\",\n\"type\": \"CustomScript\",\n\"typeHandlerVersion\": \"2.1\",\n`\n```\nWindows:\n```\n`    \"publisher\": \"Microsoft.Compute\",\n    \"type\": \"CustomScriptExtension\",\n    \"typeHandlerVersion\": \"1.10\",\n`\n```\nYou used on Windows:\n```\n`publisher            = \"Microsoft.Azure.Extensions\"\ntype                 = \"CustomScript\"\ntype_handler_version = \"2.0\"\n`\n```",
      "question_score": 2,
      "answer_score": 5,
      "created_at": "2021-03-30T09:45:06",
      "url": "https://stackoverflow.com/questions/66866692/terraform-azure-vm-extension-custom-script-from-local-script"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-azure",
      "question_id": 66678399,
      "title": "Terraform - Static ip addresses on Azure",
      "problem": "We have a requirement to configure static private ip's for the vm's that get deployed in Azure via terraform. Tjhe reason is that we then need to use these in Ansible via an ansible pipeline.\nOne solution I found here was to create a nic with a \"dynamic\" address first and then convert that to a \"static\" ip in the next step in Terraform.\n```\n`# Create network interfaces with Private IP's\nresource \"azurerm_network_interface\" \"nic\" {\n  for_each = { for vm in var.vms : vm.hostname => vm }\n  name                = \"${each.value.hostname}-NIC\"\n  location            = var.network_location\n  resource_group_name = var.vm_resource_group\n  ip_configuration {\n    name                          = \"monitoringConfg\"\n    subnet_id                     = data.azurerm_subnet.vm_subnet.id\n    private_ip_address_allocation = \"dynamic\"\n  }\n  tags = each.value.extra_tag\n}\n\n#Convert Dynamic Private IP's to Static\nresource \"azurerm_network_interface\" \"staticnic\" {\n  for_each = { for vm in var.vms : vm.hostname => vm }\n  name                = \"${each.value.hostname}-NIC\"\n  location            = var.network_location\n  resource_group_name = var.vm_resource_group\n  ip_configuration {\n    name                          = \"monitoringConfg\"\n    subnet_id                     = data.azurerm_subnet.vm_subnet.id\n    private_ip_address_allocation = \"static\"\n    private_ip_address            = azurerm_network_interface.nic[each.key].private_ip_address    \n  }\n  tags = each.value.extra_tag\n`\n```\nBut when I run this, I get the following error:\nA resource with the ID \"/subscriptions/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx/resourceGroups/xxxxxxxxxxxxxxxx/providers/Microsoft.Network/networkInterfaces/xxxxxxxxxxxxxxxxxxx-NIC\" already exists - to be managed via Terraform this resource needs to be imported into the State. Please see the resource documentation for \"azurerm_network_interface\" for more information.\non ../../modules/main.tf line 58, in resource \"azurerm_network_interface\" \"staticnic\":\n58: resource \"azurerm_network_interface\" \"staticnic\" {\nDoes anyone have any idea what i am doing wrong or a better way to handle this?\nKind Regards,\nRB",
      "solution": "Azure does not assign a Dynamic IP Address until the Network Interface is attached to a running Virtual Machine (or other resource), refer to this. So I think that we can't convert the Dynamic IP to the Static one before the VM created because the IP address does not exist for that time being.\nInstead, we could directly associate some static IP addresses to the Azure VM by assigning some IP address in that subnet range. Read private IP allocation method.\n\nAzure reserves the first four addresses in each subnet address range.\nThe addresses can't be assigned to resources. For example, if the\nsubnet's address range is 10.0.0.0/16, addresses 10.0.0.0-10.0.0.3 and\n10.0.255.255 are unavailable.\n\nFor example, you may refer this template to configure static private ip's for the vms:\n```\n`variable \"vmlist\" {\n  type = map(object({\n    hostname = string\n    IP_address = string\n  }))\n  default = {\n    vm1 ={\n    hostname = \"vma\"\n    IP_address = \"10.0.2.4\"\n    },\n    vm2 = {\n    hostname = \"vmb\"\n    IP_address = \"10.0.2.5\"\n    }\n  }\n}\n\n#...\n\nresource \"azurerm_network_interface\" \"staticnic\" {\n  for_each = var.vmlist\n  name                = \"${each.value.hostname}-nic\"\n  location            = azurerm_resource_group.main.location\n  resource_group_name = azurerm_resource_group.main.name\n\n  ip_configuration {\n    name                          = \"testconfiguration1\"\n    subnet_id                     = azurerm_subnet.internal.id\n    private_ip_address_allocation = \"Static\"\n    private_ip_address            = each.value.IP_address\n  }\n}\n\n #...\n\nresource \"azurerm_virtual_machine\" \"main\" {\n  for_each = var.vmlist\n  name                  = each.value.hostname\n  location              = azurerm_resource_group.main.location\n  resource_group_name   = azurerm_resource_group.main.name\n  network_interface_ids = [azurerm_network_interface.staticnic[each.key].id]\n  vm_size               = \"Standard_DS1_v2\"\n\n  # Uncomment this line to delete the OS disk automatically when deleting the VM\n  # delete_os_disk_on_termination = true\n\n  # Uncomment this line to delete the data disks automatically when deleting the VM\n  # delete_data_disks_on_termination = true\n\n  storage_image_reference {\n    publisher = \"MicrosoftWindowsServer\"\n    offer     = \"WindowsServer\"\n    sku       = \"2016-Datacenter\"\n    version   = \"latest\"\n  }\n\n  storage_os_disk {\n    name              = \"${each.value.hostname}-osdisk\"\n    caching           = \"ReadWrite\"\n    create_option     = \"FromImage\"\n    managed_disk_type = \"Standard_LRS\"\n  }\n  os_profile {\n    computer_name  = each.value.hostname\n    admin_username = \"testadmin\"\n    admin_password = \"Password1234!\"\n  }\n\n   os_profile_windows_config {\n    provision_vm_agent = \"true\"\n  }\n\n}\n`\n```\nI am using\n```\n`Terraform v0.14.7\n+ provider registry.terraform.io/hashicorp/azurerm v2.52.0\n`\n```\n\nUpdate\nIf you want to let Azure assign the dynamic IP and then convert it to a static one, you can use local-exec Provisioner to invoke a local executable after a resource is created.\n```\n`resource \"null_resource\" \"example\" {\n\n  for_each = var.vmlist\n    provisioner \"local-exec\" {\n\n   command = <<EOT\n\n      $Nic = Get-AzNetworkInterface -ResourceGroupName ${azurerm_resource_group.main.name} -Name ${azurerm_network_interface.nic[each.key].name}\n      $Nic.IpConfigurations[0].PrivateIpAllocationMethod = \"Static\"\n      Set-AzNetworkInterface -NetworkInterface $Nic\n   EOT\n   \n   interpreter = [\"PowerShell\", \"-Command\"]\n  \n  }\n}\n`\n```",
      "question_score": 2,
      "answer_score": 4,
      "created_at": "2021-03-17T18:38:37",
      "url": "https://stackoverflow.com/questions/66678399/terraform-static-ip-addresses-on-azure"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-azure",
      "question_id": 66041169,
      "title": "Terraform: How to install PS Modules from Gallery into Azure Automation?",
      "problem": "how can I install a PowerShel module from the Gallery into my azure Automation Account using Terraform?\nI tried using powershellgallery API url:\n```\n`resource \"azurerm_automation_account\" \"aac\" {\n  name                = var.azure_automation_account_name\n  location            = var.location\n  tags                = var.tags\n  resource_group_name = var.resource_group\n  sku_name = \"Basic\"\n}\n\nresource \"azurerm_automation_module\" \"az_accounts\" {\n  name                    = \"az_accounts\"\n  resource_group_name     = var.resource_group\n  automation_account_name = azurerm_automation_account.aac.name\n\n  module_link {\n    uri = \"https://www.powershellgallery.com/api/v2/package/az.accounts/2.2.4\"\n  }\n}\n`\n```\nThis allways gets me an error (I tried several modules with different versions, made no difference):\n```\n`Error: Error waiting for Module \"az_accounts\" (Automation Account \"XXX\" / Resource Group \"YYY\") to finish provisioning: Orchestrator.Shared.AsyncModuleImport.ModuleImportException: Cannot import the module of name az_accounts, as the module structure was invalid.\n`\n```\nWhat am I doing wrong here?\nJan",
      "solution": "The module name should be `\"Az.Accounts\"`. This works for me.\n```\n`resource \"azurerm_automation_module\" \"example\" {\n  name                    = \"Az.Accounts\"\n  resource_group_name     = azurerm_resource_group.example.name\n  automation_account_name = azurerm_automation_account.example.name\n\n  module_link {\n    uri = \"https://www.powershellgallery.com/api/v2/package/az.accounts/2.2.4\"\n  }\n}\n`\n```",
      "question_score": 2,
      "answer_score": 5,
      "created_at": "2021-02-04T08:35:19",
      "url": "https://stackoverflow.com/questions/66041169/terraform-how-to-install-ps-modules-from-gallery-into-azure-automation"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-azure",
      "question_id": 69664157,
      "title": "Terraform Azure AKS Cluster not exporting kubeconfig file",
      "problem": "I am trying to provision a kubernetes cluster on Azure (AKS) with Terraform. The provisioning works quite well but I can't get the kubeconfig from `kube_config_raw` exported to a file.\nBelow is my `main.tf` and `outputs.tf`. I supressed the `resource_group` and `user_assigned_identity` resources.\nThis is a resource I used for creating the configuration: https://learnk8s.io/terraform-aks\n\n`main.tf`\n\n```\n`terraform {\n  required_providers {\n    azurerm = {\n      source  = \"hashicorp/azurerm\"\n      version = \">=2.79.1\"\n    }\n  }\n}\n\nprovider \"azurerm\" {\n  features {}\n  subscription_id = \"...\"\n}\n\nresource \"azurerm_kubernetes_cluster\" \"aks\" {\n  name                = \"myCluster\"\n  location            = azurerm_resource_group.rg.location\n  resource_group_name = azurerm_resource_group.rg.name\n  dns_prefix          = \"my-cluster-dns\"\n\n  default_node_pool {\n    name            = \"agentpool\"\n    node_count      = 1\n    os_disk_size_gb = 64\n    vm_size         = \"Standard_B2ms\"\n  }\n\n  identity {\n    type                      = \"UserAssigned\"\n    user_assigned_identity_id = azurerm_user_assigned_identity.user_assigned_identity.id\n  }\n\n  depends_on = [\n    azurerm_user_assigned_identity.user_assigned_identity\n  ]\n}\n`\n```\n\n`outputs.tf` - I've tried \"./kubeconfig\" and \"kubeconfig\" in the filename but nothing gets exported anywhere\n\n```\n`resource \"local_file\" \"kubeconfig\" {\n  depends_on   = [azurerm_kubernetes_cluster.aks]\n  filename     = \"./kubeconfig\"\n  content      = azurerm_kubernetes_cluster.aks.kube_config_raw\n}\n`\n```\nBonus: is it possible to export it directly to the existing `~/.kube/config` file? Like the `az aks get-credentials` command does?",
      "solution": "outputs.tf - I've tried \"./kubeconfig\" and \"kubeconfig\" in the\nfilename but nothing gets exported anywhere\n\nI tested the same code that you have and did `terraform-apply` , it saved the local file to the location where the apply was performed.\nFor example:\nIf I ran the `main.tf` file from `C:\\Users\\user\\terraform\\aksconfig>` as its present there then the `kubeconfig` file gets saved in the same path .\nOutput:\n\nBonus: is it possible to export it directly to the existing ~/.kube/config file? Like the az aks get-credentials command does?\n\nPath where the `az aks get-credentials --resource-group myresourcegroup --name myCluster` stores the config file:\n\nCode to save the script in the same path as az command:\n```\n`resource \"local_file\" \"kubeconfig\" {\n  depends_on   = [azurerm_kubernetes_cluster.aks]\n  filename     = \"C:/Users/user/.kube/config\" this is where the config file gets stored\n  content      = azurerm_kubernetes_cluster.aks.kube_config_raw\n}\n`\n```\nOutput:\nExisitng config file in `/.kube/config`\n\nNew File overwrites the existing file :\n\nNote: Using `local_file` block here will completely overwrite the file not appending the context to the previous one . If you are looking for merging the content in a single file like az command does , then its not possible from terraform.",
      "question_score": 2,
      "answer_score": 4,
      "created_at": "2021-10-21T16:50:12",
      "url": "https://stackoverflow.com/questions/69664157/terraform-azure-aks-cluster-not-exporting-kubeconfig-file"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-azure",
      "question_id": 68746665,
      "title": "Setup VNet integration for an Azure App Service (web app) via Terraform",
      "problem": "I am trying to setup VNet integration for an Azure App Service (web app) via terraform, and the below is the code I have been using:\n```\n`resource \"azurerm_subnet\" \"network_app_wtier_subnet\" {\n  name                 = \"App-Web-tier\"\n  resource_group_name  = azurerm_resource_group.network_rg.name\n  virtual_network_name = local.vnet_name\n  address_prefixes     = [\"10.1.1.0/27\"]\n\n  delegation {\n    name = \"delegation\"\n\n    service_delegation {\n        actions = [\n            \"Microsoft.Network/virtualNetworks/subnets/action\",\n            \"Microsoft.Network/virtualNetworks/subnets/join/action\"\n          ]\n        name    = \"Microsoft.Web/serverfarms\"\n      }\n  }\n}\n`\n```\n... and I am getting the following error:\n```\n`> Error: expected delegation.0.service_delegation.0.name to be one of \n> [Microsoft.ApiManagement/service \n> Microsoft.AzureCosmosDB/clusters\n> Microsoft.BareMetal/AzureVMware Microsoft.BareMetal/CrayServers\n> Microsoft.Batch/batchAccounts\n> Microsoft.ContainerInstance/containerGroups\n> Microsoft.Databricks/workspaces Microsoft.DBforMySQL/flexibleServers\n> Microsoft.DBforMySQL/serversv2\n> Microsoft.DBforPostgreSQL/flexibleServers\n> Microsoft.DBforPostgreSQL/serversv2\n> Microsoft.DBforPostgreSQL/singleServers\n> Microsoft.HardwareSecurityModules/dedicatedHSMs\n> Microsoft.Kusto/clusters\n> Microsoft.Logic/integrationServiceEnvironments\n> Microsoft.MachineLearningServices/workspaces Microsoft.Netapp/volumes\n> Microsoft.Network/managedResolvers\n> Microsoft.PowerPlatform/vnetaccesslinks\n> Microsoft.ServiceFabricMesh/networks Microsoft.Sql/managedInstances\n> Microsoft.Sql/servers Microsoft.StreamAnalytics/streamingJobs\n> Microsoft.Synapse/workspaces Microsoft.Web/hostingEnvironments\n> Microsoft.Web/serverFarms], got Microsoft.Web/serverfarms\n>     \u2502\n>     \u2502   with module.paired_regions_network.azurerm_subnet.network_app_wtier_subnet,\n>     \u2502   on modules/network/main.tf line 49, in resource \"azurerm_subnet\" \"network_app_wtier_subnet\":\n>     \u2502   49:         name    = \"Microsoft.Web/serverfarms\"\n`\n```\nThe error itself is contradictive : so, it expects `Microsoft.Web/serverFarms`, gets `Microsoft.Web/serverFarms`, but still errors?\nAny idea how can I fix this?\n\nEDIT\nThe configuration I was using when I initially wrote the post is the following:\n```\n`terraform {\n  backend \"azurerm\" { }\n\n  required_version = \">= 0.14\"\n  required_providers {\n    azurerm = {\n      source  = \"hashicorp/azurerm\"\n      version = \">=2.50.0\"\n    }\n  }\n}\n`\n```\nWhich should accommodate for any updates, I guess. Right?\nUpdating it to more recent versions (below) resulted in the same error.\n```\n`terraform {\n  backend \"azurerm\" { }\n\n  required_version = \">= 0.15\"\n  required_providers {\n    azurerm = {\n      source  = \"hashicorp/azurerm\"\n      version = \">=2.72.0\"\n    }\n  }\n}\n`\n```",
      "solution": "Ah, I could repro it - and fix it. It's case-sensitive... Use `\"Microsoft.Web/serverFarms\"` instead (uppercase F)",
      "question_score": 2,
      "answer_score": 4,
      "created_at": "2021-08-11T19:40:20",
      "url": "https://stackoverflow.com/questions/68746665/setup-vnet-integration-for-an-azure-app-service-web-app-via-terraform"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-azure",
      "question_id": 67788413,
      "title": "How to config backups of Azure Managed Disks from Terraform?",
      "problem": "I'd like to create a backup from an Azure managed disk, as this article says, but with terraform.\nI can create the managed disk as:\n```\n`resource \"azurerm_managed_disk\" \"mongodb_disk\" {\n  name = \"${var.env_name}-mongodb-disk\"\n  location = var.resource_group.location\n  resource_group_name = var.resource_group.name\n  create_option = \"Empty\"\n  storage_account_type = \"Premium_LRS\"\n  disk_size_gb = var.mongo_db_disk_size_in_gb\n}\n`\n```\nBut I'm unable to find the following resources:\n\nbackup vault\nthe backup policy in the vault\nthe backup itself in the vault\n\nCould you help me with the correct terraform resources, to create the backup? Or is there an example anywhere, how to create a scheduled backup of an Azure Managed Disk?\nThanks,",
      "solution": "EDIT: Just realised that Azure Backup Vault is not yet supported in Terraform, you can upvote the issue.\nFor VMs backups it's just Azure Recovery Services Vault. A workaround could be for you to use an `external` terraform object that will run\neither a PowerShell or Bash script (using Az Cli), to create the backup. It's not 100% IaC but better than nothing.",
      "question_score": 2,
      "answer_score": 3,
      "created_at": "2021-06-01T13:54:23",
      "url": "https://stackoverflow.com/questions/67788413/how-to-config-backups-of-azure-managed-disks-from-terraform"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-azure",
      "question_id": 68046936,
      "title": "Experiencing an error when try to output content of a csv file using terraform",
      "problem": "I'm trying to use terraform variable data (CSV file) to create a resource group and the name of the resource group is added into the CSV file. I'm currently experiencing the below error\n\u2502 Error: Unsupported attribute\n\u2502\n\u2502   on testtf.tf line 11, in resource \"azurerm_resource_group\" \"Main\":\n\u2502   11:       name     =  local.resource_groupname[count.index].groupname\n\u2502     \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\u2502     \u2502 count.index is a number, known only after apply\n\u2502     \u2502 local.resource_groupname is list of object with 3 elements\n\u2502\n\u2502 This object does not have an attribute named \"groupname\".\nCode\n```\n`provider \"azurerm\" {\n    features{}\n}\n\nlocals {\n      resource_groupname = csvdecode(file(\"./test.csv\"))\n    }\n\n    resource \"azurerm_resource_group\" \"Main\" {\n      count    = length(local.resource_groupname)\n      name     =  local.resource_groupname[count.index].groupname   \n      location = \"North europe\"\n    }\n`\n```\n./test.csv content\nhttps://drive.google.com/file/d/1ituKDzaMVXnyynkjLBZRzMdWK9tnkL14/view?usp=sharing",
      "solution": "I think the file you provided has UTF BOM (Byte Order Mark) bytes that cause TF and/or Azure to choke.\nI recreated the csv file as plain ascii and your HCL worked ok\nI found out about the extra characters by using `terraform console`. It is a very simple and quick way to troubleshoot TF errors.\nI used this really basic .tf file to check the `cvsdecode()` behavior.  (test0.csv below is your original file and test.csv my created from scratch text file):\n```\n`locals {\n      resource_groupname0 = csvdecode(file(\"./test0.csv\"))\n      resource_groupname = csvdecode(file(\"./test.csv\"))\n }\n\n`\n```\nRun terraform console and inspect the local variables. Note the BOM characters before \"groupname\" (test0.csv):\n```\n`$ terraform console\n> local.resource_groupname0\ntolist([\n  {\n    \"\\ufeffgroupname\" = \"test11\"\n  },\n  {\n    \"\\ufeffgroupname\" = \"test12\"\n  },\n  {\n    \"\\ufeffgroupname\" = \"test13\"\n  },\n])\n> local.resource_groupname\ntolist([\n  {\n    \"groupname\" = \"test11\"\n  },\n  {\n    \"groupname\" = \"test12\"\n  },\n  {\n    \"groupname\" = \"test13\"\n  },\n])\n\n`\n```\nAlso using the unix file command:\n```\n`## Your file\n$ file test0.csv\ntest0.csv: UTF-8 Unicode (with BOM) text, with CRLF line terminators\n\n## Created by hand with text editor\n$ file test.csv\ntest.csv: ASCII text\n`\n```",
      "question_score": 2,
      "answer_score": 3,
      "created_at": "2021-06-19T14:49:47",
      "url": "https://stackoverflow.com/questions/68046936/experiencing-an-error-when-try-to-output-content-of-a-csv-file-using-terraform"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-azure",
      "question_id": 67985330,
      "title": "Terraform: Error: Error parsing App Service Resource ID",
      "problem": "This is a continuation to this question\n```\n`resource \"azurerm_app_service_slot\" \"app_service_slot_template\"   {\n  for_each = {for sl in \"${var.app_service_slots}\" : sl.name => sl }\n  \n  app_service_name = \"${var.mandatory_prefix}-${var.app_service_name}\"\n  resource_group_name = \"${var.resource_group_name}\"\n  app_service_plan_id = \"${data.azurerm_app_service_plan.service_plan.id}\"\n  location = \"${var.resource_location}\"\n  https_only = true\n\n \n      \n        \n  name =  \"${each.value[\"name\"]}\"\n  \n  dynamic \"site_config\" {\n    for_each = \"${var.site_config}\"\n    content {\n      min_tls_version = lookup(site_config.value, \"min_tls_version\", null)\n      python_version = lookup(site_config.value, \"python_version\", null)\n      java_version = lookup(site_config.value, \"java_version\", null)\n      always_on = lookup(site_config.value, \"always_on\", null)\n      app_command_line = lookup(site_config.value, \"app_command_line\", null)\n      dotnet_framework_version = lookup(site_config.value, \"dotnet_framework_version\", null)          \n    }\n  }\n  \n  dynamic \"connection_string\" {\n    for_each =  \"${each.value[\"connection_strings\"]}\"\n    content {\n      name = \"${connection_string.value[\"name\"]}\"\n      type = \"${connection_string.value[\"type\"]}\"\n      value = \"${connection_string.value[\"value\"]}\"\n    }\n  }\n\n  app_settings = \"${merge(each.value[\"app_settings\"], local.additional_app_settings)}\"\n}\n\n  # Get the Id of the subnet\ndata \"azurerm_subnet\" \"azurerm_subnet_template\" {\n  name                 = \"${var.subnet_name}\"\n  virtual_network_name = \"${var.virtual_network_name}\"\n  resource_group_name  = \"${var.vnet_resource_group_name}\"\n}\n\nresource \"azurerm_app_service_virtual_network_swift_connection\" \"azureapp_vnet_integration_for_slot\" {\n  for_each = {for sl in \"${azurerm_app_service_slot.app_service_slot_template}\" : sl.name => sl }\n\n  app_service_id = \"${each.value.id}\"\n  subnet_id      = \"${data.azurerm_subnet.azurerm_subnet_template.id}\"\n  depends_on = [azurerm_app_service_slot.app_service_slot_template]\n}\n`\n```\nThis works perfectly on the Terraform plan, however during the apply it failed with the below error\n\nError: Error parsing App Service Resource ID\non .terraform/modules/generic_app_service/main.tf line 235, in\nresource \"azurerm_app_service_virtual_network_swift_connection\"\n\"azureapp_vnet_integration_for_slot\": 235: resource\n\"azurerm_app_service_virtual_network_swift_connection\"\n\"azureapp_vnet_integration_for_slot\" {\n\nI can confirm from the plan output the app_service_id is getting/passing the correct resource id for the azure slot but don't know why it is complaining about App Service Resource ID",
      "solution": "The docs for app_service_id write:\n\nThe ID of the App Service or Function App to associate to the VNet. Changing this forces a new resource to be created.\n\nSo it should be either ID of `azurerm_app_service` or `azurerm_function_app`. But you are trying to use `azurerm_app_service_slot`, which is a different resource.",
      "question_score": 2,
      "answer_score": 3,
      "created_at": "2021-06-15T13:24:14",
      "url": "https://stackoverflow.com/questions/67985330/terraform-error-error-parsing-app-service-resource-id"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-azure",
      "question_id": 65600886,
      "title": "Terraform v0.13+ : Using count in modules",
      "problem": "I am using Terraform version v0.14.3.\nI am using count in modules to create multiple Azure resources (network interface card, VM) of the same type.\nBelow is the parent module, calling child modules NIC and VM :\n```\n`module \"NIC\" {\n  source = \"./NIC\"\n  count  = 2\n\n  nic_name      =  \"vm-nic-${count.index + 1}\" \n  nic_location  = \"eastus2\"\n  rg_name       = \"abc-test-rg\"\n  ipconfig_name = \"vm-nic-ipconfig-${count.index + 1}\" \n  subnet_id     = \"/subscriptions/***********/resourceGroups/abc-test-rg/providers/Microsoft.Network/virtualNetworks/abc-test-vnet/subnets/abc-test-vnet\"\n  \n}\noutput \"nic_id\" {\n  value = module.NIC[*].nic_id\n}\nmodule \"VM\" {\n  source = \"./VM\"\n  count = 2\n\n  vm_name        = \"test-vm\"\n  rg_name        = \"abc-test-rg\"\n  location       = \"eastus2\"\n  admin_password = var.admin_password\n  nic_id         = [module.NIC[*].nic_id]\n  \n}\n`\n```\nI am getting below error during terraform plan :\n```\n`Error: Incorrect attribute value type\n\n  on VM\\main.tf line 8, in resource \"azurerm_linux_virtual_machine\" \"vm\":\n   8:   network_interface_ids           = var.nic_id\n    |----------------\n    | var.nic_id is tuple with 1 element\n\nInappropriate value for attribute \"network_interface_ids\": element 0: string\nrequired.\n`\n```\nHow do I loop around the two NIC ids generated and pass them to the two VMs in the VM module?\nThanks in advance!",
      "solution": "Use `count.index` to reference a specific value of your output in relation to the the number of VMs you are provisioning in your second module call.\n```\n`  nic_id         = [module.NIC[count.index].nic_id]\n`\n```",
      "question_score": 2,
      "answer_score": 3,
      "created_at": "2021-01-06T18:52:21",
      "url": "https://stackoverflow.com/questions/65600886/terraform-v0-13-using-count-in-modules"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-azure",
      "question_id": 76206678,
      "title": "Terraform Kubernetes Provider - cannot load Kubernetes client config",
      "problem": "In my terraform project, I have my own module which creates an AKS cluster with RBAC enabled. This module has an output aks_public_fqdn which holds the FQDN of the cluster.\n```\n`module \"aks\" {\n  source = \"./aks\"\n\n  environment        = local.environment\n  region             = local.region\n  instance_id        = local.workload_id\n  application        = local.workload_name\n  resource_group     = local.rg_name\n  kubernetes_version = local.kubernetes_version\n\n}\n`\n```\nOnce the cluster is up and running, I would like to create some K8S resources through Terraform with the Kubernetes provider.\nTo authenticate I am using the following code snippet:\n```\n`provider \"kubernetes\" {\n  host                   = \"https://${module.aks.aks_public_fqdn}\"\n  insecure = true\n\n  exec {\n    api_version = \"client.authentication.k8s.io/v1beta1\"\n    command     = \"kubelogin\"\n    args = [\n      \"get-token\",\n      \"--environment\",\n      \"AzurePublicCloud\",\n      \"--server-id\",\n      \"3333333-3333333-3333333\",\n      \"--login\",\n      \"azurecli\"\n    ]\n  }\n}\n`\n```\nWhen I try to run terraform apply I get:\n```\n`Error: Provider configuration: cannot load Kubernetes client config\ninvalid configuration: default cluster has no server defined\n`\n```\nThe only entry I have in my kubeconfig file is the context for my local kind cluster. What is missing here?\nAm I hit by the following github issue?\nProvider Issue",
      "solution": "The `kubernetes` provider configuration contains an output from the module declared as `aks`. Prior to version `2.4.0` of the Kubernetes provider it was possible to simultaneously manage a Kubernetes cluster backing infrastructure and the cluster itself at initial provisioning (or subsequent Delete/Create) by configuring the provider with either resource attributes, or with `data` attributes. At version `2.4.0` of the Kubernetes provider, the new experimental Kubernetes provider with the latest Terraform SDK and Kubernetes Go SDK bindings (and consequently Kubernetes API) stabilized and replaced the former Kubernetes provider (ergo why some resources are marked `v1` and `v2` as the provider still contained some of the legacy code for backwards support, and only the minor version of the provider was iterated and not the major version according to semantic versioning rules). With this new provider and its use of the modern Kubernetes API it became no longer possible for this simultaneous management at initial provisioning. Therefore the error in the question is observed as Terraform's Kubernetes provider is attempting to configure with a non-existent cluster instead of charting a dependency hierarchy that would imply the provider is dependent upon the `aks` module.\nWith all this in mind it becomes clear there are two workarounds. The first is to downgrade the Kubernetes provider to the last version of the old provider:\n```\n`terraform {\n  required_providers {\n    kubernetes = {\n      source  = \"hashicorp/kubernetes\"\n      version = \">= 2.3.2\"\n    }\n  }\n}\n`\n```\nThe other solution would be to `-target` the `aks` module first to manually enforce the dependency and populate the outputs, and then subsequently manage the entire Terraform config:\n```\n`terraform plan -target=module.aks\n`\n```",
      "question_score": 2,
      "answer_score": 2,
      "created_at": "2023-05-09T09:06:36",
      "url": "https://stackoverflow.com/questions/76206678/terraform-kubernetes-provider-cannot-load-kubernetes-client-config"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-azure",
      "question_id": 75128701,
      "title": "Can&#39;t make feature prevent_deletion_if_contains_resources working in Terraform",
      "problem": "Context\nTrying to understand how is working feature prevent_deletion_if_contains_resources in AzureRm on Terraform:\n```\n`provider \"azurerm\" {\n  features {\n    resource_group {\n      prevent_deletion_if_contains_resources = true\n    }\n  }\n}\n`\n```\nThe documentation:\nhttps://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/guides/features-block#resource_group\nSays:\n\nShould the azurerm_resource_group resource check that there are no\nResources within the Resource Group during deletion? This means that\nall Resources within the Resource Group must be deleted prior to\ndeleting the Resource Group. Defaults to true.\n\nMy issue\nWhatever the value of prevent_deletion_if_contains_resources this never happens.\n\nTerraform destroy work as fine\nI can delete the Resource Group from the portal\n\nWhat I did\nThis is the full script:\n```\n`provider \"azurerm\" {\n  features {\n    resource_group {\n      prevent_deletion_if_contains_resources = true\n    }\n  }\n}\n\nprovider \"azurerm\" {\n  alias = \"autreChoix\"\n  features {\n    resource_group {\n      prevent_deletion_if_contains_resources = false\n    }\n  }\n}\n\nresource \"azurerm_resource_group\" \"rg2\" {\n  name     = \"rg2\"\n  location = \"northeurope\"\n\n  provider = azurerm.autreChoix\n}\n\nresource \"azurerm_resource_group\" \"rg\" {\n  name     = \"rg1\"\n  location = \"westeurope\"\n}\n\nresource \"azurerm_storage_account\" \"sa\" {\n  name                     = \"mystor1\"\n  resource_group_name      = azurerm_resource_group.rg.name\n  location                 = azurerm_resource_group.rg.location\n  account_tier             = \"Standard\"\n  account_replication_type = \"LRS\"\n}\n\nresource \"azurerm_storage_account\" \"sa2\" {\n  name                     = \"mystor2\"\n  resource_group_name      = azurerm_resource_group.rg2.name\n  location                 = azurerm_resource_group.rg2.location\n  account_tier             = \"Standard\"\n  account_replication_type = \"LRS\"\n}\n`\n```\nCreates 2 RG. On with each value of prevent_deletion_if_contains_resources.\nWhat I need\nDoes anybody tells me what I am missing?\nThanks",
      "solution": "The main problem prevent_deletion_if_contains_resources solves is throwing a warning to the Terraform user when there are additional Resources within a Resource Group that it is trying to remove that it does not manage, since those Resources will also be deleted when the Resource Group is deleted by Terraform.\nHere is the original issue: https://github.com/hashicorp/terraform-provider-azurerm/issues/1608, and later the default behavior in the provider was changed to be true as a result of this other issue: https://github.com/hashicorp/terraform-provider-azurerm/issues/13777.\nThat setting only applies to using Terraform. It does not prevent users in the Azure portal from deleting the Resource Group. However, I suspect that if you create the Resource Group using Terraform, then add a new Resource within that Resource Group using the Azure portal, and then finally perform a `terraform destroy` on that Resource Group, you should see an Terraform error if you have prevent_deletion_if_contains_resources set to `true`.",
      "question_score": 2,
      "answer_score": 2,
      "created_at": "2023-01-15T22:59:55",
      "url": "https://stackoverflow.com/questions/75128701/cant-make-feature-prevent-deletion-if-contains-resources-working-in-terraform"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-azure",
      "question_id": 74054066,
      "title": "Azure Container Apps environment creation fails due to error ManagedEnvironmentResourceGroupDisallowedByPolicy when adding a VNET integration",
      "problem": "I'm trying to create an Azure Container Apps Environment through the AzAPI provider on Terraform.\nThe configuration I'm using is the following:\n```\n`resource \"azapi_resource\" \"aca_env\" {\n  type      = \"Microsoft.App/managedEnvironments@2022-03-01\"\n  parent_id = azurerm_resource_group.rg.id\n  location  = azurerm_resource_group.rg.location\n  name      = var.ACA_ENV_NAME\n  body = jsonencode({\n    properties = {\n      appLogsConfiguration = {\n        destination               = \"log-analytics\"\n        logAnalyticsConfiguration = {\n          customerId = azurerm_log_analytics_workspace.log.workspace_id\n          sharedKey  = azurerm_log_analytics_workspace.log.primary_shared_key\n        }\n      }\n      daprAIConnectionString = azurerm_application_insights.insights.connection_string\n      vnetConfiguration = {\n        \"internal\" = true\n        \"infrastructureSubnetId\" = azurerm_subnet.aca_subnet.id\n        \"dockerBridgeCidr\" = var.ACA_ENV_BRIDGE_CIDR\n        \"platformReservedCidr\" = var.ACA_ENV_RESERVED_CIDR\n        \"platformReservedDnsIP\" = var.ACA_ENV_RESERVED_DNS_IP\n      }\n    }\n  })\n  depends_on = [\n    azurerm_subnet.aca_subnet\n  ]\n  response_export_values  = [\"properties.defaultDomain\", \"properties.staticIp\"]\n  ignore_missing_property = true\n}\n`\n```\nWhen I try to execute this, I get the following error:\n```\n`ErrorCode: ManagedEnvironmentResourceGroupDisallowedByPolicy, Message: Fail to create managed environment because resource group creation is disallowed by policy, refer to https://go.microsoft.com/fwlink/?linkid=2198255 for more detail.\n`\n```\nMy guess is that it's trying to create a resource group somehow. However, we require certain tags to be present on a resource group, which is probably failing.\nThe weird part is that even though this error happens, the Azure Container Apps environment is still created. Also, if I remove the VNET configuration, the environment is created without any errors.\nThe question is, why is it trying to create a resource group? I referenced one already in the `parent_id` attribute.",
      "solution": "This is a known issue tracked here:\n\nFeature Request: Allow managed resource group created with tag when deploying a Container App in a custom vnet\n\nFor the moment, the proposed workaround is to add a policy assignment exception for resource group that have the `MC_` prefix and `_{region}` suffix.",
      "question_score": 2,
      "answer_score": 2,
      "created_at": "2022-10-13T12:15:43",
      "url": "https://stackoverflow.com/questions/74054066/azure-container-apps-environment-creation-fails-due-to-error-managedenvironmentr"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-azure",
      "question_id": 73971345,
      "title": "An argument active_directory is not expected here in terraform Azure app",
      "problem": "i want to add `auth_settings` into my Azure App Service. I am using this provider:\nhttps://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/windows_function_app\nAnd this is my code. Module:\n```\n`resource \"azurerm_windows_function_app\" \"function_app\" {\n  name                 = var.name\n  resource_group_name  = var.resource_group_name\n  location             = var.location\n  storage_account_name = var.storage_account_name\n  service_plan_id      = var.service_plan_id\n  app_settings         = var.app_settings\n\n  auth_settings {\n    enabled          = var.auth_settings_enabled\n    active_directory = var.auth_active_directory\n  }\n}\n`\n```\nAnd `variables.tf` file:\n```\n`// ommited the rest\nvariable \"auth_settings_enabled\" {\n    type = bool\n    default = false\n}\n\nvariable \"auth_active_directory\" {\n    default = null\n    type = object({\n        client_id         = optional(string)\n        client_secret     = optional(string)\n        allowed_audiences = optional(list(string))\n    })\n}\n`\n```\nThen, i declare my module in `main.tf`\n```\n`/// \nmodule \"function_app_1\" {\n  source = \"./function-app-module\"\n\n  // standard vars like name etc here...\n\n  auth_settings_enabled = true\n  auth_active_directory = {\n    client_id         = var.clientid\n    client_secret     = var.clientsecret\n    allowed_audiences = [ var.audience ]\n  }\n}\n\nmodule \"function_app_2\" {\n  source = \"./function-app-module\"\n\n  // standard vars like name etc here...\n \n  auth_active_directory = {}\n}\n`\n```\nAnd after `terraform plan` commend i am getting this error:\n```\n`\u2502 Error: Unsupported argument\n\u2502\n\u2502   on function-app\\main.tf line 28, in resource \"azurerm_windows_function_app\" \"function_app\":\n\u2502   28:     active_directory = var.auth_active_directory\n\u2502\n\u2502 An argument named \"active_directory\" is not expected here. Did you mean to define a block of type \"active_directory\"?\n\u2575\n\u2577\n\u2502 Error: Unsupported argument\n\u2502\n\u2502   on function-app\\main.tf line 28, in resource \"azurerm_windows_function_app\" \"function_app\":\n\u2502   28:     active_directory = var.auth_active_directory\n\u2502\n\u2502 An argument named \"active_directory\" is not expected here. Did you mean to define a block of type \"active_directory\"?\n`\n```\nMy question is, how to init `active_directory` in `auth_settings` object properly?",
      "solution": "Since `active_directory` is a block and not an argument, you cannot define it the way you are currently trying to. So, there are a couple of things to consider:\n\nIf the `enabled` value is set to `true` the `active_directory` block should be used\nVariable value assignment to a block rather than an argument\n\nBased on the two assumptions, you could refactor the code block in question like this:\n```\n`  auth_settings {\n    enabled          = var.auth_settings_enabled\n    dynamic \"active_directory\" {\n      for_each = auth_settings_enabled ? [1] : []\n      content {\n        client_id         = var.auth_active_directory.client_id\n        client_secret     = var.auth_active_directory.client_secret\n        allowed_audiences = var.auth_active_directory.allowed_audiences\n      }\n    }\n  }\n`\n```\nIn this case, Terraform `dynamic` block is used [1] to make sure the `active_directory` block is optional and used only when the `auth_settings_enabled` variable is equal to `true`.\n\n[1] https://developer.hashicorp.com/terraform/language/expressions/dynamic-blocks",
      "question_score": 2,
      "answer_score": 2,
      "created_at": "2022-10-06T11:15:46",
      "url": "https://stackoverflow.com/questions/73971345/an-argument-active-directory-is-not-expected-here-in-terraform-azure-app"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-azure",
      "question_id": 71856069,
      "title": "Terraform Azuread provider Authorizer Error",
      "problem": "My GitLab CI/CD pipeline is throwing a Terraform Azuread provider authoriser error which has become a major blocker for me and I simply can't find a way round it.\nMy Terraform configuration includes a data.tf file which has the following single line entry:\n```\n`data \"azuread_client_config\" \"current\" {}\n`\n```\nI also have a provider.tf file, the content of which includes the following azuread provider block:\n```\n`provider \"azuread\" {\n   tenant_id = \"#TenantID#\"\n   use_cli = \"false\"\n}\n`\n```\nRunning the GitLab CI/CD pipeline, it throws the below error:\n```\n`Error: no Authorizer could be configured, please check your configuration\n\nwith provider [\"registry.terraform.io/hashicorp/azuread\"],\non provider.tf line 29, in provider \"azuread\":\n29: provider \"azuread\" {\n`\n```\nIf I exclude the data.tf file from my terraform configuration or comment out its single line entry, the pipeline runs without throwing any errors. What am I doing wrong, or what do I need to do to get the pipeline run successfully, upon inclusion of the data.tf file?",
      "solution": "Data Source: azuread_client_config\nUse this data source to access the configuration of the AzureAD provider.\n#This is while Terraform authenticating via the Azure CLI\n```\n`data \"azuread_client_config\" \"current\" {}\n\noutput \"object_id\" {\n  value = data.azuread_client_config.current.object_id\n}\n`\n```\n#Configure the Azure Active Directory Provider\n```\n`provider \"azuread\" {\n\n  # NOTE: Environment Variables can also be used for Service Principal authentication\n\n  # client_id     = \"...\"\n  # client_secret = \"...\"\n  # tenant_id     = \"...\"\n}\n`\n```\nSo would suggest you remove `data \"azuread_client_config\" \"current\" {}` line from `data.tf` file if you are using `provider azuread {}` in provider.tf file. Because you are already authenticating with Service Principle so there is no point of using data source of azuread.\nYou can also refer this Documention regarding the `Data Sources` and `Resources` supported by the Azure Active Directory Provider",
      "question_score": 2,
      "answer_score": 2,
      "created_at": "2022-04-13T12:37:27",
      "url": "https://stackoverflow.com/questions/71856069/terraform-azuread-provider-authorizer-error"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-azure",
      "question_id": 71744596,
      "title": "Terraform error: Error: creating Subnet: Original Error: Code=&quot;NetcfgInvalidSubnet&quot; Message=&quot;Subnet &#39;internal&#39; is not valid in virtual network",
      "problem": "I am trying to create resources in azure using terraform, a SQL server database and also a virtual machine.I get the error.\n`\u2502 Error: creating Subnet: (Name \"db_subnetn\" / Virtual Network Name \"tf_dev-network\" / Resource Group \"terraform_youtube\"): network.SubnetsClient#CreateOrUpdate: Failure sending request: StatusCode=400 -- Original Error: Code=\"NetcfgInvalidSubnet\" Message=\"Subnet 'db_subnetn' is not valid in virtual network 'tf_dev-network'.\" Details=[]`\nWhat have I done ?\nfollowed the link here Error while provisioning Terraform subnet using azurerm\nI deleted other network resources using thesame IP range.\nMy network understanding is pretty basic, however from my research it appears that 10.0.0.0/16 is quite a large IP range and can lead to overlaps. So what did I do, I changed the virtual network IP range from 10.0.0.0/16 to 10.0.1.0/24 to restrict the range, what simply happened is that the error changed to\n`\u2502 Error: creating Subnet: (Name \"internal\" / Virtual Network Name \"tf_dev-network\" / Resource Group \"terraform_youtube\"): network.SubnetsClient#CreateOrUpdate: Failure sending request: StatusCode=400 -- Original Error: Code=\"NetcfgInvalidSubnet\" Message=\"Subnet 'internal' is not valid in virtual network 'tf_dev-network'.\" Details=[]`\nAt this stage, I would be grateful if someone can explain what is going wrong here and what needs to be done. Thanks in advance\nMy files are as follows.\ndbcode.tf\n```\n`resource \"azurerm_sql_server\" \"sqlserver\" {\n  name                         = \"tom556sqlserver\"\n  resource_group_name          = azurerm_resource_group.resource_gp.name\n  location                     = azurerm_resource_group.resource_gp.location\n  version                      = \"12.0\"\n  administrator_login          = \"khdfd9898rerer\"\n  administrator_login_password = \"4-v3ry-jlhdfdf89-p455w0rd\"\n\n  tags = {\n    environment = \"production\"\n  }\n}\n\nresource \"azurerm_sql_virtual_network_rule\" \"sqlvnetrule\" {\n    name = \"sql_vnet_rule\"\n    resource_group_name          = azurerm_resource_group.resource_gp.name\n    server_name = azurerm_sql_server.sqlserver.name\n    subnet_id = azurerm_subnet.db_subnet.id\n  \n}\n\nresource \"azurerm_subnet\" \"db_subnet\" {\n    name = \"db_subnetn\"\n    resource_group_name     = azurerm_resource_group.resource_gp.name\n    virtual_network_name    = azurerm_virtual_network.main.name\n    address_prefixes        = [\"10.0.2.0/24\"]\n    service_endpoints       = [\"Microsoft.Sql\"]\n  \n}\n`\n```\nmain.tf\n```\n`resource \"azurerm_resource_group\" \"resource_gp\" {\n  name=\"terraform_youtube\"\n  location = \"UK South\"\n\n  tags = {\n    \"owner\" = \"Rahman\"\n    \"purpose\" = \"Practice terraform\"\n  }\n}\n\nvariable \"prefix\" {\n  default = \"tf_dev\"\n}\n\nresource \"azurerm_virtual_network\" \"main\" {\n  name                = \"${var.prefix}-network\"\n  address_space       = [\"10.0.0.0/16\"]\n  location            = azurerm_resource_group.resource_gp.location\n  resource_group_name = azurerm_resource_group.resource_gp.name\n}\n\nresource \"azurerm_subnet\" \"internal\" {\n  name                 = \"internal\"\n  resource_group_name  = azurerm_resource_group.resource_gp.name\n  virtual_network_name = azurerm_virtual_network.main.name\n  address_prefixes     = [\"10.0.2.0/24\"]\n}\n\nresource \"azurerm_network_interface\" \"main\" {\n  name                = \"${var.prefix}-nic\"\n  location            = azurerm_resource_group.resource_gp.location\n  resource_group_name = azurerm_resource_group.resource_gp.name\n\n  ip_configuration {\n    name                          = \"testconfiguration1\"\n    subnet_id                     = azurerm_subnet.internal.id\n    private_ip_address_allocation = \"Dynamic\"\n  }\n}\n\nresource \"azurerm_virtual_machine\" \"main\" {\n  name                  = \"${var.prefix}-vm\"\n  location              = azurerm_resource_group.resource_gp.location\n  resource_group_name   = azurerm_resource_group.resource_gp.name\n  network_interface_ids = [azurerm_network_interface.main.id]\n  vm_size               = \"Standard_B1ls\"\n\n  # Uncomment this line to delete the OS disk automatically when deleting the VM\n  delete_os_disk_on_termination = true\n\n  # Uncomment this line to delete the data disks automatically when deleting the VM\n  delete_data_disks_on_termination = true\n\n  storage_image_reference {\n    publisher = \"Canonical\"\n    offer     = \"UbuntuServer\"\n    sku       = \"16.04-LTS\"\n    version   = \"latest\"\n  }\n  storage_os_disk {\n    name              = \"myosdisk1\"\n    caching           = \"ReadWrite\"\n    create_option     = \"FromImage\"\n    managed_disk_type = \"Standard_LRS\"\n  }\n  os_profile {\n    computer_name  = \"hostname\"\n    admin_username = \"testadmin\"\n    admin_password = \"Password1234!\"\n  }\n  os_profile_linux_config {\n    disable_password_authentication = false\n  }\n  tags = {\n    environment = \"staging\"\n  }\n}\n`\n```",
      "solution": "Tested with your code in my environment was getting the same error.\n\nTo fix the issue you need to change `address_prefixes` for `db_subnet` to `[\"10.0.3.0/24\"]` as `[\"10.0.2.0/24\"]` address range is already using by `internal` subnet in your `main.tf` and also check update for `sqlvnetrule` and do the changes in your `dbcode.tf` file.\n```\n`resource \"azurerm_mssql_server\" \"sqlserver\" {\n  name                         = \"tom556sqlserver\"\n  resource_group_name          = azurerm_resource_group.resource_gp.name\n  location                     = azurerm_resource_group.resource_gp.location\n  version                      = \"12.0\"\n  administrator_login          = \"khdfd9898rerer\"\n  administrator_login_password = \"4-v3ry-jlhdfdf89-p455w0rd\"\n\n  tags = {\n    environment = \"production\"\n  }\n}\n\nresource \"azurerm_subnet\" \"db_subnet\" {\n    name = \"db_subnetn\"\n    resource_group_name     = azurerm_resource_group.resource_gp.name\n    virtual_network_name    = azurerm_virtual_network.main.name\n    address_prefixes        = [\"10.0.3.0/24\"]\n    service_endpoints       = [\"Microsoft.Sql\"]\n  \n}\n\nresource \"azurerm_mssql_virtual_network_rule\" \"sqlvnetrule\" {\n    name = \"sql_vnet_rule\"\n    #resource_group_name          = azurerm_resource_group.resource_gp.name\n    #server_name = azurerm_sql_server.sqlserver.name\n    server_id = azurerm_mssql_server.sqlserver.id\n    subnet_id = azurerm_subnet.db_subnet.id\n  \n}\n`\n```",
      "question_score": 2,
      "answer_score": 2,
      "created_at": "2022-04-05T00:53:22",
      "url": "https://stackoverflow.com/questions/71744596/terraform-error-error-creating-subnet-original-error-code-netcfginvalidsubn"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-azure",
      "question_id": 71171249,
      "title": "terraform network_interface_ids - Inappropriate value for attribute",
      "problem": "I'm trying to set the \"network_interface_ids\" within \"azurerm_virtual_machine\" to the output of one of my modules, but am receiving the following error during the \"terraform plan\" and can't figure out where I'm going wrong:\n```\n`\u2502 Error: Incorrect attribute value type\n\u2502 \n\u2502   on modules/virtualmachine/main.tf line 6, in resource \"azurerm_virtual_machine\" \"vm\":\n\u2502    6:   network_interface_ids             = [var.nicid]\n\u2502     \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\u2502     \u2502 var.nicid is a list of string, known only after apply\n\u2502 \n\u2502 Inappropriate value for attribute \"network_interface_ids\": element 0:\n\u2502 string required.\n\u2575\n##[error]Bash exited with code '1'.\n`\n```\nI'm sure this is something simple, I have it working without being split out into modules just can't quite figure it out. I've attached all of the relevant code below (obviously removed the bulk of the code to help with brevity):\nmain.tf\n```\n`  module \"virtualmachine\" {\n  source                   = \"./modules/virtualmachine\"\n  nicid                    = module.networking.nicidoutput\n  }\n`\n```\nmodules/networking/main.tf\n```\n`resource \"azurerm_network_interface\" \"nic\" {\n  name                = var.nicname\n  location            = azurerm_resource_group.rg.location\n  resource_group_name = azurerm_resource_group.rg.name\n}\n`\n```\nmodules/networking/outputs.tf\n```\n`output \"nicidoutput\" {\n    value = azurerm_network_interface.nic.id\n}\n`\n```\nmodules/virtualmachine/main.tf\n```\n`  resource \"azurerm_virtual_machine\" \"vm\" {\n  network_interface_ids             = [var.nicid]\n  }\n`\n```\nmodules/virtualmachine/variables.tf\n```\n`variable \"nicid\" {\n    type = list(string)\n    description = \"network interface id\"\n}\n`\n```",
      "solution": "Your `var.nicid` is already a list. So it should be:\n```\n`network_interface_ids             = var.nicid\n`\n```\nupdate:\nalso the following should be changed in `virtualmachine`\n```\n`nicid                    = [module.networking.nicidoutput] for virtualmachine\n`\n```",
      "question_score": 2,
      "answer_score": 2,
      "created_at": "2022-02-18T10:38:54",
      "url": "https://stackoverflow.com/questions/71171249/terraform-network-interface-ids-inappropriate-value-for-attribute"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-azure",
      "question_id": 71147530,
      "title": "For_each loop with for expression based on value in map",
      "problem": "Since the title is not descriptive enough let me introduce my problem.\nI'm creating DRY module code for CDN that contains profile/endpoint/custom_domain.\nVariable cdn_config would hold all necessary/optional parameters and these are created based on the for_each loop.\nVariable looks like this:\n```\n`variable \"cdn_config\" {\n  profiles = {\n    \"profile_1\" = {}\n }\n \n endpoints = {\n    \"endpoint_1\" = {\n       custom_domain = {\n    }\n  }\n }\n}\n`\n```\nCore of this module is working - in the means that it would create cdn_profile \"profile_1\" then cdn_endpoint \"endpoint_1\" will be created and assigned to this profile then cdn_custom_domain will be created and assigned to \"endpoint_1\" since it's the part of \"endpoint_1\" map.\nThen I realize, what in case I want to create \"cdn_custom_domain\" only and specify resource ID manually?\nI was thinking that adding the optional parameter \"standalone\" could help, so it would look like this:\n```\n`variable \"cdn_config\" {\n  profiles = {\n    \"profile_1\" = {}\n }\n\n endpoints = {\n    \"endpoint_1\" = {\n       custom_domain = {\n    }\n  }\n    \"endpoint_standalone\" = {\n       custom_domain = {\n         standalone = true\n         cdn_endpoint_id = \"xxxxx\"\n   }\n  } \n }\n}\n`\n```\nHaving this \"standalone\" parameter eq true \"endpoint_standalone\" map should be totally ignored from looping in the azurerm_cdn_endpoint resource creation.\nSo far this direction is my only guess, clearly, it's not working - if I add \"endpoint_standalone\" it complains that not all required parameters are specified so it's surely finding it.\n```\n`resource \"azurerm_cdn_endpoint\" \"this\" {\n\nfor_each = {for k in keys(var.cdn_config.endpoints) : k => var.cdn_config.endpoints[k] if lookup(var.cdn_config.endpoints[k],\"standalone\",null) != \"true\"}\n`\n```\nI would be grateful if you have a solution for this problem.",
      "solution": "You are comparing a bool type to a string type, so the logical comparison will always return false:\n```\n`for_each = {for k in keys(var.cdn_config.endpoints) : k => var.cdn_config.endpoints[k] if lookup(var.cdn_config.endpoints[k],\"standalone\",null) != true }\n`\n```\nWhile we are here, we can also improve this `for` expression:\n```\n`for_each = { for endpoint, params in var.cdn_config.endpoints : endpoint => params if lookup(params.custom_domain, \"standalone\", null) != true }\n`\n```",
      "question_score": 2,
      "answer_score": 2,
      "created_at": "2022-02-16T19:38:56",
      "url": "https://stackoverflow.com/questions/71147530/for-each-loop-with-for-expression-based-on-value-in-map"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-azure",
      "question_id": 70851465,
      "title": "Azure AD Group- Authorization_RequestDenied - Insufficient privileges to complete the operation",
      "problem": "I\u2019m trying to create the Azure AD Group using the following terraform code through the Azure DevOps.\n```\n`# Create Azure AD Group in Active Directory for AKS Admins\nresource \"azuread_group\" \"aks_administrators\" {\n  #name        = \"${azurerm_resource_group.aks_rg.name}-administrators\"\n  display_name        = \"${azurerm_resource_group.aks_rg.name}-${var.environment}-administrators\"\n  description = \"Azure AKS Kubernetes administrators for the ${azurerm_resource_group.aks_rg.name}-${var.environment} cluster.\"\n  security_enabled = true\n}\n`\n```\nI have followed these steps to provide the permission to create Azure AD Groups through the Azure DevOps ARM service connection (Service Principle).\n\u2022   Provide permission for Service connection created in previous step to create Azure AD Groups\n\u2022   Go to -> Azure DevOps -> Select Organization -> Select project terraform-azure-aks\n\u2022   Go to Project Settings -> Pipelines -> Service Connections\n\u2022   Open terraform-aks-azurerm-svc-con\n\u2022   Click on Manage Service Principal, new tab will be opened\n\u2022   Click on View API Permissions\n\u2022   Click on Add Permission\n\u2022   Select an API: Microsoft APIs\n\u2022   Commonly used Microsoft APIs: Supported legacy APIs: Azure Active Directory Graph-DEPRECATING Use Microsoft Graph\n\u2022   Click on Application Permissions\n\u2022   Check Directory.ReadWrite.All and click on Add Permission\n\u2022   Click on Grant Admin consent for Default Directory\nBut I\u2019m getting the following error:\n\nError: Creating group \"xxxxxxxxxx-administrators\"\n\u2502\n\u2502   with azuread_group.aks_administrators,\n\u2502   on 06-aks-administrators-azure-ad.tf line 2, in resource \"azuread_group\" \"aks_administrators\":\n\u2502    2: resource \"azuread_group\" \"aks_administrators\" {\n\u2502\n\u2502 graphrbac.GroupsClient#Create: Failure responding to request:\n\u2502 StatusCode=403 -- Original Error: autorest/azure: Service returned an\n\u2502 error. Status=403 Code=\"Unknown\" Message=\"Unknown service error\"\n\u2502 Details=[{\"odata.error\":{\"code\":\"Authorization_RequestDenied\",\"date\":\"2022-01-25T04:06:31\",\"message\":{\"lang\":\"en\",\"value\":\"Insufficient\n\u2502 privileges to complete the\n\u2502 operation.\"}}}]",
      "solution": "Please check the Microsoft Graph permission `Directory.ReadWrite.All` has been provided to the service connection and it has been granted the admin consent.\nI tested the same in my environment where I gave the permission to my service principal but didn't grant admin consent like below :\n\nWhen deploying the below code, it gave me error :\n```\n`provider \"azuread\" {}\n# Create Azure AD Group in Active Directory for AKS Admins\nresource \"azuread_group\" \"aks_administrators\" {\n  #name        = \"ans-aks-administrators\"\n  display_name        = \"ans-aks-test-administrators\"\n  description = \"Azure AKS Kubernetes administrators for the ans-aks-test cluster.\"\n  security_enabled = true\n}\n`\n```\n\nAfter granting the permission admin consent it gets resolved :\n\nIf the issue still occurs then please add a new secret for the service connection service principal and use the below code :\n```\n`provider \"azuread\" {\nclient_id = \"ClientID of the service principal\"\nclient_secret = \"ClientSecret\"\ntenant_id = \"\"\n}\n\n# Create Azure AD Group in Active Directory for AKS Admins\nresource \"azuread_group\" \"aks_administrators\" {\n  #name        = \"ans-aks-administrators\"\n  display_name        = \"ans-aks-test-administrators\"\n  description = \"Azure AKS Kubernetes administrators for the ans-aks-test cluster.\"\n  security_enabled = true\n}\n`\n```",
      "question_score": 2,
      "answer_score": 2,
      "created_at": "2022-01-25T16:36:43",
      "url": "https://stackoverflow.com/questions/70851465/azure-ad-group-authorization-requestdenied-insufficient-privileges-to-complet"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-azure",
      "question_id": 69222060,
      "title": "Grant read access to a user assigned identity to a storage account with Azure",
      "problem": "I would like to create a User assigned identity with Terraform, that have the read permission on an already existing azure storage account. How can I grant that with Terraform? I am creating the whole infrastructure with the service principal I'm using right now. So, it's unlikely that it's a permission issue.\n```\n`# My user assigned identity:\n\nresource \"azurerm_user_assigned_identity\" \"user_assigned_identity\" {\n  name                = \"${var.resource_prefix}useridentity\"\n  location            = var.location\n  resource_group_name = var.resource_group_name\n}\n\n# and the role assignment to this identity\nresource \"azurerm_role_assignment\" \"example\" {\n  scope              = \"/subscriptions/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx/resourceGroups/resource-group-name\"\n  role_definition_id = \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxx\"\n  principal_id       = azurerm_user_assigned_identity.user_assigned_identity.principal_id\n}\n`\n```\nBut I am having this error:\n\nError: authorization.RoleAssignmentsClient#Create: Failure responding\nto request: StatusCode=403 -- Original Error: autorest/azure: Service\nreturned an error. Status=403 Code=\"AuthorizationFailed\" Message=\"The\nclient 'xxxxxxxxxxxxxxxxxxx' with object id 'xxxxxxxxxxxxxxxxxxx' does\nnot have authorization to perform action\n'Microsoft.Authorization/roleAssignments/write' over scope\n'/subscriptions/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx/resourceGroups/resource-group-name/providers/Microsoft.Authorization/roleAssignments/\"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxx\"'\nor the scope is invalid. If access was recently granted, please\nrefresh your credentials.\"\n\nAny idea what I'm doing wrong?",
      "solution": "For assigning roles to the some user assigned identity using your Service Principal from terraform you need to give the service principal \"Owner\" permission to to subscription. It is not possible to do from \"Contributor\" permission. Using contributor access you can create or manage the resources for the subscription but not assign roles.\n\nTesting:\nMy service principal which I will be using to authenticate from terraform.\n\nProviding Owner access to the above service principal in the subscription.\n\nMy Terraform Script:\n```\n`    provider \"azurerm\"{\n  client_id = \"f6a2f33d-xxxx-xxxxx-xxxxx-xxxx\"\n  subscription_id = \"948d4068--xxxx-xxxx-xxxxx-xxxx\"\n  client_secret = \"KEa7Q~2673QY.uN.xxxxxxxxxxxx\"\n  tenant_id = \"72f988bf-xxxxx-xxxxx-xxxxx-xxxxx\"\n    features{}\n}\n\nresource \"azurerm_user_assigned_identity\" \"user_assigned_identity\" {\n  name                = \"myuseridentity\"\n  location            = \"East US\"\n  resource_group_name = \"ansumantest\"\n}\n\n# and the role assignment to this identity\nresource \"azurerm_role_assignment\" \"example\" {\n  scope              = \"/subscriptions/948d4068--xxxx-xxxx-xxxxx-xxxxx/resourceGroups/ansumantest\"\n  role_definition_name = \"Storage Blob Data Reader\"\n  principal_id       = azurerm_user_assigned_identity.user_assigned_identity.principal_id\n}\n`\n```\nOutput:\n\nValidating from Azure Portal:\n\nNote:\nYour Service principal must be having the contributor access, giving it owner access should resolve the issue.",
      "question_score": 2,
      "answer_score": 1,
      "created_at": "2021-09-17T12:43:12",
      "url": "https://stackoverflow.com/questions/69222060/grant-read-access-to-a-user-assigned-identity-to-a-storage-account-with-azure"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-azure",
      "question_id": 69158576,
      "title": "Is there a way to configure git for synapse using terraform or arm templates?",
      "problem": "'''\n```\n`data \"azurerm_client_config\" \"current\" {}\nresource \"azurerm_synapse_workspace\" \"example\" {\n  name                                 = \"example\"\n  resource_group_name                  = azurerm_resource_group.example.name\n  location                             = azurerm_resource_group.example.location\n  storage_data_lake_gen2_filesystem_id = data.azurerm_storage_container.example\n  sql_administrator_login              = \"sqladminuser\"\n  sql_administrator_login_password     = \"admin@123\"\n\n  azure_devops_repo {\n      account_name = \"organizationaz440\"\n      branch_name = \"Development\"\n      project_name = \"TestProject\"\n      repository_name = \"DataOps\"\n      root_folder = \"/SynapseNew\"\n      tenant_id = data.azurerm_client_config.current.tenant_id\n  }\n  depends_on = [resource.azurerm_synapse_workspace.example]\n}\n`\n```\n'''\nhttps://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/synapse_workspace\nError: Blocks of type \"azure_devops_repo\" are not expected here.",
      "solution": "There are couple of things which needs to be fixed:\n\nYou are giving `storage_data_lake_gen2_filesystem_id = data.azurerm_storage_container.example` instead you have to give\n`storage_data_lake_gen2_filesystem_id = azurerm_storage_data_lake_gen2_filesystem.example.id`. You have to\ncreate a datalake gen 2 filesystem using your exisitng storage\naccount as data source for `azurerm_storage_data_lake_gen2_filesystem` is not supported.\nYou can remove this `tenant_id = data.azurerm_client_config.current.tenant_id` as its optional and\nkeeping it will give an error that `An argument named \"tenant_id\" is not expected here.`\nYou can remove `depends_on = [resource.azurerm_synapse_workspace.example]` as its not required.\n\nSo, after the above changes are made the .tf file will be like below:\n```\n`    provider \"azurerm\" {\n        features{}\n    }\n    \n    data \"azurerm_storage_account\" \"name\" {\n      name = \"ansumanstorageacc\"\n      resource_group_name = \"yourresourcegroupname\"\n    }\n    resource \"azurerm_storage_data_lake_gen2_filesystem\" \"example\" {\n      name               = \"example\"\n      storage_account_id = data.azurerm_storage_account.name.id\n    }\n    resource \"azurerm_synapse_workspace\" \"example\" {\n      name                                 = \"example\"\n      resource_group_name                  = \"yourresourcegroupname\"\n      location                             = \"West US 2\"\n      storage_data_lake_gen2_filesystem_id = azurerm_storage_data_lake_gen2_filesystem.example.id\n      sql_administrator_login              = \"sqladminuser\"\n      sql_administrator_login_password     = \"admin@123\"\n    \n      azure_devops_repo {\n          account_name = \"organizationaz440\"\n          branch_name = \"Development\"\n          project_name = \"TestProject\"\n          repository_name = \"DataOps\"\n          root_folder = \"/SynapseNew\"\n      }\n    }\n`\n```\nOutputs:\n\nterraform apply -auto-approve:",
      "question_score": 2,
      "answer_score": 2,
      "created_at": "2021-09-13T09:03:13",
      "url": "https://stackoverflow.com/questions/69158576/is-there-a-way-to-configure-git-for-synapse-using-terraform-or-arm-templates"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-azure",
      "question_id": 68665984,
      "title": "Terraform aks module - get cluster name and resource group name via remote state",
      "problem": "Hi I am trying to follow this offical guide to manage aks resources. There terraform_remote_state is used to get the `resource_group_name` and `kubernetes_cluster_name`.\n`data \"terraform_remote_state\" \"aks\" {\n  backend = \"local\"\n\n  config = {\n    path = \"/path/to/base/project/terraform.tfstate\"\n  }\n}\n\n# Retrieve AKS cluster information\nprovider \"azurerm\" {\n  features {}\n}\n\ndata \"azurerm_kubernetes_cluster\" \"cluster\" {\n  name                = data.terraform_remote_state.aks.outputs.kubernetes_cluster_name\n  resource_group_name = data.terraform_remote_state.aks.outputs.resource_group_name\n}\n`\nI have created the inital aks cluster with the aks module. Looking at its output in the documentation, it doesnt export the resource group name or cluster name.\nNow I wonder how I can get the information. I have tried the below in the base project.\n`module \"aks\" {\n  ...\n}\n\noutput \"resource_group_name\" {\n  value = module.aks.resource_group_name\n}\n\noutput \"kubernetes_cluster_name\" {\n  value = module.aks.cluster_name\n}\n`\nBut I get erros when trying `terraform plan`\n`Error: Unsupported attribute\n\u2502 \n\u2502   on main.tf line 59, in output \"resource_group_name\":\n\u2502   59:   value = module.aks.resource_group_name\n\u2502     \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\u2502     \u2502 module.aks is a object, known only after apply\n\u2502 \n\u2502 This object does not have an attribute named \"resource_group_name\".\n\u2575\n\u2577\n\u2502 Error: Unsupported attribute\n\u2502 \n\u2502   on main.tf line 63, in output \"kubernetes_cluster_name\":\n\u2502   63:   value = module.aks.cluster_name\n\u2502     \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\u2502     \u2502 module.aks is a object, known only after apply\n\u2502 \n\u2502 This object does not have an attribute named \"cluster_name\".\n`\nThose are listed under inputs for that module though. Now I dont have an idea know how to get those values from the `terraform_remote_state`.",
      "solution": "As the module itself doesn\u2019t have name and resource group as output , we have to declare outputs there first and then call it while deploying or in remote state as well.\nSo we have to add 2 outputs in output.tf for aks module after doing terraform init.\n```\n`output \"kubernetes_cluster_name\" {\n  value = azurerm_kubernetes_cluster.main.name\n}\n\noutput \"resource_group_name\" {\n  value = azurerm_kubernetes_cluster.main.resource_group_name\n}\n`\n```\n\nThen call outputs in main.tf after defining the modules i.e. network and aks , you can see your Kubernetes cluster name in plan as well and after applying it.\n```\n`output \"kuberneteclustername\" {\n  value = module.aks.kubernetes_cluster_name\n}\n\noutput \"resourcegroupname\" {\n  value = module.aks.resource_group_name\n}\n`\n```\n\nNow lets test it from the remote state :\n```\n`data \"terraform_remote_state\" \"aks\" {\n  backend = \"local\"\n\n  config = {\n    path = \"path/to/terraform/aksmodule/terraform.tfstate\"\n  }\n}\n\n# Retrieve AKS cluster information\nprovider \"azurerm\" {\n  features {}\n}\n\ndata \"azurerm_kubernetes_cluster\" \"cluster\" {\n  name                = data.terraform_remote_state.aks.outputs.kuberneteclustername\n  resource_group_name = data.terraform_remote_state.aks.outputs.resourcegroupname\n}\n\noutput \"aks\" {\n  value = data.azurerm_kubernetes_cluster.cluster.name\n}\noutput \"rg\" {\n  value = data.azurerm_kubernetes_cluster.cluster.resource_group_name\n}\n`\n```",
      "question_score": 2,
      "answer_score": 2,
      "created_at": "2021-08-05T13:45:53",
      "url": "https://stackoverflow.com/questions/68665984/terraform-aks-module-get-cluster-name-and-resource-group-name-via-remote-state"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-azure",
      "question_id": 68002358,
      "title": "How not to destroy the imported resource group in Terraform with Azure",
      "problem": "I am currently writing a Terraform file which should create all required resources in Azure in a specific resource group. As I do not have the permissions to create resource groups I imported an existing resource group following this link.\nNow the problem is that when I run `terraform destroy`, the system also wants to delete the resource group as it was included as a resource.\nIs there a simple way I can tell Terraform to destroy all created resources except the imported resource group? Or another way to approach this problem?\nI tried the lifecycle variable `prevent_destroy` but it prevents any resource from being destroyed.\nThe solution presented here does not really work for me as I do not want to use multiple commands as this adds the risk of human error. What I am basically looking for is a way to label the resource group in the Terraform file in a way that it is not deleted.",
      "solution": "If you want to refer to a resource but not have Terraform manage the lifecycle of it (either at all or just in that specific Terraform workspace) then you can use data sources.\nIn your case you could replace the `azure_resource_group` resource with the `azure_resource_group` data source.",
      "question_score": 2,
      "answer_score": 2,
      "created_at": "2021-06-16T14:07:19",
      "url": "https://stackoverflow.com/questions/68002358/how-not-to-destroy-the-imported-resource-group-in-terraform-with-azure"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-azure",
      "question_id": 67808307,
      "title": "Terraform Azure Container Instance Dynamic Volume - share_name loop over azurerm_storage_share",
      "problem": "I have the following terraform code creating azure storage file shares.\n```\n`resource \"azurerm_storage_share\" \"jms-sftp-share\" {\n  for_each             = toset([\"one\", \"two\", \"three\"])\n  name                 = each.key\n  quota                = 5120\n  storage_account_name = azurerm_storage_account.working-storage_account.name\n\n  acl {\n    id = \"${each.key}_this_is_my_id\"\n\n    access_policy {\n      permissions = \"rwl\"\n    }\n  }\n}\n`\n```\nI am trying to then create an azurerm_container_instance with a dynamic volume block that gets it's share name by looping over the azurerm_storage_share.jms-sftp-share.\n```\n`resource \"azurerm_container_group\" \"jms-sftp\" {\n  dns_name_label = \"doccji-dts-dev-jms-sftp\"\n  exposed_port = [\n    {\n      port     = 22\n      protocol = \"TCP\"\n    },\n  ]\n  location            = var.resource-location\n  name                = \"${local.resource-name-prefix}-sftp-1\"\n  os_type             = \"Linux\"\n  resource_group_name = local.resource-group-name\n  restart_policy      = \"Always\"\n  tags                = merge(local.common_tags, tomap({ \"type\" = \"docker-sftp-server\" }))\n\n  container {\n    commands = []\n    cpu      = 1\n    image    = \"atmoz/sftp:latest\"\n    memory   = 1.5\n    name     = \"jms-sftp-1\"\n\n    ports {\n      port     = 22\n      protocol = \"TCP\"\n    }\n    dynamic \"volume\" {\n      for_each = [for v in azurerm_storage_share.jms-sftp-share : {\n        name = v.name\n      }]\n      content {\n        empty_dir            = false\n        mount_path           = \"/home/${volume.value.name}\"\n        name                 = \"${volume.value.name}-home-folder\"\n        read_only            = false\n        share_name           = azurerm_storage_share.jms-sftp-share[volume.value.name]\n        storage_account_key  = azurerm_storage_account.working-storage_account.primary_access_key\n        storage_account_name = azurerm_storage_account.working-storage_account.name\n      }\n\n    }\n    volume {\n      empty_dir            = false\n      mount_path           = \"/etc/sftp\"\n      name                 = \"sftp-users-conf\"\n      read_only            = true\n      share_name           = azurerm_storage_share.jms-sftp-users-share.name\n      storage_account_key  = azurerm_storage_account.working-storage_account.primary_access_key\n      storage_account_name = azurerm_storage_account.working-storage_account.name\n    }\n  }\n\n  depends_on = [\n    azurerm_storage_share.jms-sftp-share,\n    azurerm_storage_share.jms-sftp-users-share\n  ]\n}\n`\n```\nI'm stuck on the following error:\n```\n`Error: Incorrect attribute value type\n\n  on container-instance.tf line 36, in resource \"azurerm_container_group\" \"jms-sftp\":\n  36:         share_name           = azurerm_storage_share.jms-sftp-share[volume.value.name]\n    |----------------\n    | azurerm_storage_share.jms-sftp-share is object with 3 attributes\n\nInappropriate value for attribute \"share_name\": string required.\n\nError: Incorrect attribute value type\n\n  on container-instance.tf line 36, in resource \"azurerm_container_group\" \"jms-sftp\":\n  36:         share_name           = azurerm_storage_share.jms-sftp-share[volume.value.name]\n    |----------------\n    | azurerm_storage_share.jms-sftp-share is object with 3 attributes\n\nInappropriate value for attribute \"share_name\": string required.\n\nError: Incorrect attribute value type\n\n  on container-instance.tf line 36, in resource \"azurerm_container_group\" \"jms-sftp\":\n  36:         share_name           = azurerm_storage_share.jms-sftp-share[volume.value.name]\n    |----------------\n    | azurerm_storage_share.jms-sftp-share is object with 3 attributes\n\nInappropriate value for attribute \"share_name\": string required.\n`\n```\nI believe I get what it's telling me, but I don't know how to setup the\n```\n`share_name           = azurerm_storage_share.jms-sftp-share[volume.value.name]\n`\n```\nto correctly reference the associated share.\nIf I change the share_name to be\n```\n`share_name           = azurerm_storage_share.jms-sftp-share[volume.value.name.name]\n`\n```\nI get output that suggests my previous notation was correct, but I'm just not sure where to go.\n```\n`Error: Unsupported attribute\n\n  on container-instance.tf line 36, in resource \"azurerm_container_group\" \"jms-sftp\":\n  36:         share_name           = azurerm_storage_share.jms-sftp-share[volume.value.name.name]\n    |----------------\n    | volume.value.name is \"one\"\n\nThis value does not have any attributes.\n\nError: Unsupported attribute\n\n  on container-instance.tf line 36, in resource \"azurerm_container_group\" \"jms-sftp\":\n  36:         share_name           = azurerm_storage_share.jms-sftp-share[volume.value.name.name]\n    |----------------\n    | volume.value.name is \"two\"\n\nThis value does not have any attributes.\n\nError: Unsupported attribute\n\n  on container-instance.tf line 36, in resource \"azurerm_container_group\" \"jms-sftp\":\n  36:         share_name           = azurerm_storage_share.jms-sftp-share[volume.value.name.name]\n    |----------------\n    | volume.value.name is \"three\"\n\nThis value does not have any attributes.\n`\n```\nAny ideas?",
      "solution": "You need to reference the specific value in the exported resource attribute object. The error message states:\n\nazurerm_storage_share.jms-sftp-share is object with 3 attributes\n\nindicating you need to reference the specific element in the object. The three attributes are denoted with the `one` `two` and `three` Strings you use as keys to iterate over in the question. You then access the specific element like:\n```\n`share_name = azurerm_storage_share.jms-sftp-share[\"one\"].name\n`\n```\nwhich accesses the `one` element of the `azurerm_storage_share.jms-sftp-users-share` object from the exported resource attributes.",
      "question_score": 2,
      "answer_score": 2,
      "created_at": "2021-06-02T17:39:45",
      "url": "https://stackoverflow.com/questions/67808307/terraform-azure-container-instance-dynamic-volume-share-name-loop-over-azurerm"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-azure",
      "question_id": 67490398,
      "title": "Terraform nested for_each",
      "problem": "I'm quite new to Terraform and might be I want too much ... but here is the case\nthis is what I have in my .tfvars (part of)\n```\n`     st_resources = {\n            \"steu1edwdas001common\" = {\n                    st_name_site_region   = \"eu1\"\n                    st_name_application   = \"edw\"\n                    st_name_role          = \"das\"\n                    st_name_seqnr         = \"001\"\n                    st_name_purpose       = \"commfs\"\n...\n                    st_kind                      = \"StorageV2\"\n                    st_tier                      = \"Premium\"\n...\n                    st_blob_contributor_role_aadgroups = [\n                                                          { display_name = \"LG_GLB_AzureLZSolutionLeadersAdmin\", role = \"Storage Blob Data Contributor\" }\n                                                         ]\n    ...\n            }\n    }\n`\n```\nIn a template I have this code\n```\n`module \"st_create\" {\n      for_each = var.resources_st\n      source = \"../../_modules/general/st_create\"\n    \n      st_name_site_region   = each.value[\"st_name_site_region\"]\n      st_name_application   = each.value[\"st_name_application\"]\n      st_name_role          = each.value[\"st_name_role\"]\n      st_name_seqnr         = each.value[\"st_name_seqnr\"]\n      st_name_purpose       = each.value[\"st_name_purpose\"]\n    ...\n      st_blob_contributor_role_aadgroups = each.value[\"st_blob_contributor_role_aadgroups\"]\n    ...\n    }\n`\n```\nThis works fine, but now I would like to create the roles and assign aad groups, so in my pseudo logic i see following steps\nSo I'm adding\n```\n`locals {\n  st_blob_contributor_role_aadgroups = flatten([\n    for st_key, st in var.resources_st : [\n      for rbac_key, rbac in st.st_blob_contributor_role_aadgroups : {\n        st_key            = st_key\n        rbac_key          = rbac_key\n        role_display_name = rbac.display_name\n        role_role         = rbac.role \n      }\n    ]\n  ])\n}\n`\n```\nBut I have no idea how to continue.\nI could do a second module and loop the flattened structure, but will I get in this structure the ID of the storage account ...\nAfraid I'm mixing this but can't find a sample representing a bit what I have in mind ....",
      "solution": "The two main requirements for `for_each` are:\n\nYou have a collection with one element per resource instance you want to declare.\nYou can derive some sort of unique string key for each element which will be fully known at planning time.\n\nIt seems like your list in `locals.st_blob_contributor_roleaadgroups` meets these requirements, and so it's suitable to use as the basis for a `for_each` but will require a little additional transformation to turn this into a map where the unique per-element strings are the keys:\n```\n`module \"example\" {\n  source = \"../../_modules/example\"\n  for_each = {\n    for obj in locals.st_blob_contributor_roleaadgroups : \"${obj.st_key}:${obj.rbac_key}\" => obj\n  }\n\n  # ...\n}\n`\n```\nInside this `module` block you can use `each.value` to refer to the current object, such as `each.value.display_name` to get the display name. Each instance of the module will be identified by that concatenation of the `st_key` and `rbac_key` attributes, which must be unique because they were originally taken from the keys of two different maps.",
      "question_score": 2,
      "answer_score": 2,
      "created_at": "2021-05-11T18:01:18",
      "url": "https://stackoverflow.com/questions/67490398/terraform-nested-for-each"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-azure",
      "question_id": 66320461,
      "title": "Referencing terraform_remote_state from within module",
      "problem": "Using Terraform v0.13.5\nI have a module which has some outputs derived from some sub modules within it, e.g.:\n```\n`module \"egressvnet\" {\n  source = \"../modules/vnet/egress\"\n}\n\noutput \"subnet\" {\n  value = module.egressvnet.subnet\n}\n`\n```\n`terraform output` confirms that what I expect to be outputted is.\nWithin another terraform setup I would like to reference the outputs from the above.\nSo I have this in my terraform config:\n```\n`data \"terraform_remote_state\" \"network\" {\n  backend = \"azurerm\"\n\n  config = {\n    resource_group_name  = \"xxx\"\n    storage_account_name = \"xxx\"\n    container_name       = \"terraform\"\n    key                  = \"network.tfstate\"\n  }\n}\n\nmodule \"web\" {\n  source = \"../modules/web\"\n\n  subnet_id = terraform_remote_state.network.outputs.subnet\n\n}\nIs what I'm trying to do possible?\n`\n```\nBut when I do a plan I get this error:\n```\n`Error: Reference to undeclared resource\n\n  on base.tf line 111, in module \"web\":\n 111:   subnet_id = terraform_remote_state.network.outputs.subnet\n\nA managed resource \"terraform_remote_state\" \"network\" has not been declared in\nthe root module.\n`\n```",
      "solution": "Since your `terraform_remote_state` is a data source, you should refer to it using `data.`:\n```\n`subnet_id = data.terraform_remote_state.network.outputs.subnet\n`\n```",
      "question_score": 2,
      "answer_score": 2,
      "created_at": "2021-02-22T18:24:20",
      "url": "https://stackoverflow.com/questions/66320461/referencing-terraform-remote-state-from-within-module"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-azure",
      "question_id": 77572029,
      "title": "Getting error when creating app configuration key with dynamic block in for_each",
      "problem": "I need to create Azure App Configurations with using Terraform. In Terraform documentation, it gives the structure to create is:\n\nIn my problem, I have configurations in this schema as a sample data:\n```\n`locals {\n  configurations = {\n    label1 = {\n      key1 = \"value1\",\n      key2 = \"value2\",\n    },\n    label2 = {\n      key1 = \"value2\",\n      key3 = \"value3\",\n      key4 = \"value4\",\n    }\n  }\n}\n`\n```\nGenerating \"label\", \"key\", and \"value\" fields need to use these configurations value. So, I have created a code to implement it as you see in below:\n```\n`resource \"azurerm_app_configuration_key\" \"example\" {\n  for_each                = local.configurations\n  configuration_store_id = example.id\n  label                  = each.key\n  dynamic \"key\" {\n    for_each = each.value\n    content {\n      name  = key\n      value = value\n    }\n  }\n}\n`\n```\nBut it gives 'Required attribute \"key\" not specified: An attribute named \"key\" is required here' error.\nHow to solve this problem?",
      "solution": "The documentation indicates that the resource indeed does not allow a key to be configured with a dynamic block.\nInstead, you must iterate over each key-label pair, as that uniquely defines a configuration key. You can achieve that by some Terraform list and map comprehension magic as follows:\n```\n`locals {\n  resource_group_name = \"...\"\n  location            = \"...\"\n  configurations = {\n    label1 = {\n      key1 = \"value1\",\n      key2 = \"value2\",\n    },\n    label2 = {\n      key1 = \"value2\",\n      key3 = \"value3\",\n      key4 = \"value4\",\n    }\n  }\n  key_labels = merge(\n    [for label, config in local.configurations :\n      { for key, value in config :\n        \"${label}-${key}\" =>\n        {\n          \"key\"   = key,\n          \"value\" = value,\n          \"label\" = label\n        }\n      }\n    ]...\n  )\n}\nresource \"azurerm_app_configuration\" \"test\" {\n  name                = \"testconfig12345678\"\n  resource_group_name = local.resource_group_name\n  location            = local.location\n}\n\nresource \"azurerm_app_configuration_key\" \"test\" {\n  for_each               = local.key_labels\n  configuration_store_id = azurerm_app_configuration.test.id\n  key                    = each.value.key\n  label                  = each.value.label\n  value                  = each.value.value\n}\n`\n```",
      "question_score": 2,
      "answer_score": 1,
      "created_at": "2023-11-29T15:26:38",
      "url": "https://stackoverflow.com/questions/77572029/getting-error-when-creating-app-configuration-key-with-dynamic-block-in-for-each"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-azure",
      "question_id": 74741331,
      "title": "Azure Terraform import bacpac into SQL Server with public network access disabled",
      "problem": "I have a SQL Server in Azure with public network access disabled\n```\n`resource \"azurerm_mssql_server\" \"sql_server\" {\n    name                          = var.db-server-name\n    resource_group_name           = var.resource_group\n    location                      = var.location\n    version                       = \"12.0\"\n    administrator_login           = local.login\n    administrator_login_password  = local.password\n    minimum_tls_version           = \"1.2\"\n    public_network_access_enabled = false\n    tags = var.tags\n}\n`\n```\nFor accessing the server, I am creating a private endpoint:\n```\n`resource \"azurerm_private_endpoint\" \"sqlserver_private_endpoint\" {\n    name                = \"sqlserver-private-endpoint\"\n    location            = var.location\n    resource_group_name = var.resource_group\n    subnet_id           = azurerm_subnet.db_subnet.id\n    private_service_connection {\n        name                           = \"sqlserver-psc\"\n        is_manual_connection           = false\n        private_connection_resource_id = azurerm_mssql_server.sql_server.id\n        subresource_names              = [\"sqlServer\"]\n    }\n    tags = var.tags\n}\n`\n```\nI am then trying to create/import a database from Blob-Storage\n```\n`resource \"azurerm_mssql_database\" \"sql_server_database\" {\n    name                        = var.db-name\n    server_id                   = azurerm_mssql_server.sql_server.id\n    collation                   = \"SQL_Latin1_General_CP1_CI_AS\"\n    auto_pause_delay_in_minutes = 60\n    max_size_gb                 = 32\n    min_capacity                = 0.5\n    read_replica_count          = 0\n    read_scale                  = false\n    sku_name                    = \"GP_S_Gen5_1\"\n    zone_redundant              = false\n    import {\n        storage_uri                  = var.storage-url\n        storage_key                  = var.storage-key\n        storage_key_type             = \"StorageAccessKey\"\n        administrator_login          = azurerm_mssql_server.sql_server.administrator_login\n        administrator_login_password = azurerm_mssql_server.sql_server.administrator_login_password\n        authentication_type          = \"Sql\"\n    }\n}\n`\n```\nWith this setup, I get the following error\n\nError: while import bacpac into the new database test-db (Resource Group Test-dev): Code=\"ImportExportJobError\" Message=\"The ImportExport operation with Request Id '1b005b56-bccd-4484-a5e0-c2495834798a' failed due to 'The SQL instance is inaccessible because the public network interface is denied (Error 47073). Please enable public network access on the SQL Server or configure Import/Export to use Private Link per https://docs.microsoft.com/en-us/azure/azure-sql/database/database-import-export-private-link.'.\"\nwith module.sql_server.azurerm_mssql_database.sql_server_database, on Modules\\SqlServer\\main.tf line 69, in resource \"azurerm_mssql_database\" \"sql_server_database\":\n69: resource \"azurerm_mssql_database\" \"sql_server_database\" {\n\nThis error makes sense as I have set `public_network_access_enabled = false` on my SQL Server.\nFor security reasons I would not like to set\n```\n`public_network_access_enabled = true\n`\n```\nso my question would be: is there a possibility to import the database without enabling network access on the server?\nHere I found a way to import the database using PowerShell, which should create a Privatelink for importing, but using this the Database would not be created using Terraform and would not be managed through the Terraform-State...\nSo does someone know of a way to import the database using Terraform with `public_network_access_enabled = false`?\n(AzureRM Provider Version: 3.31)",
      "solution": "Using Private Link for importing seems to be still in Preview. Preview features are rarely (or never) supported with Terraform so I would search alternatives for now.\nI personally would create the database with Terraform and then manually (or with some CI/CD magic) import the required data. Generally Terraform is a bit clunky tool in managing what happens inside a database and I personally like using other tools for it.",
      "question_score": 2,
      "answer_score": 1,
      "created_at": "2022-12-09T10:45:31",
      "url": "https://stackoverflow.com/questions/74741331/azure-terraform-import-bacpac-into-sql-server-with-public-network-access-disable"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "terraform-provider-azure",
      "question_id": 73105751,
      "title": "terraform Invalid value for &quot;seqs&quot; parameter: all arguments must be lists or tuples; got string",
      "problem": "Why i'm not using the ID directly:\n\nI have multiple datalake's where the filesystem is deployed. It throws error \"resource not found\"  during the deployment.\n\nWhat i'm trying to achieve now:\n\ni am trying to use concat function and create the ID's. which is throwing an error.\n\n```\n`module.adlsfs[\"adlsfilesystem1\"].time_sleep.wait_few_mins_fs: Refreshing state... [id=2022-07-23T21:45:55Z]\n\u2577\n\u2502 Error: Invalid function argument\n\u2502 \n\u2502   on ../../../tf-core-module/adls/fs/filesystem.tf line 20, in resource \"azurerm_storage_data_lake_gen2_filesystem\" \"storagedlsgen2fs\":\n\u2502   20:   storage_account_id = concat(\"/subscriptions/\",data.azurerm_subscription.current.id,\"/resourceGroups/rsg-test/providers/Microsoft.Storage/storageAccounts/\",each.value.staname)\n\u2502 \n\u2502 Invalid value for \"seqs\" parameter: all arguments must be lists or tuples; got string.\n\u2575\n\u2577\n\u2502 Error: Invalid function argument\n\u2502 \n\u2502   on ../../../tf-core-module/adls/fs/filesystem.tf line 20, in resource \"azurerm_storage_data_lake_gen2_filesystem\" \"storagedlsgen2fs\":\n\u2502   20:   storage_account_id = concat(\"/subscriptions/\",data.azurerm_subscription.current.id,\"/resourceGroups/rsg-test/providers/Microsoft.Storage/storageAccounts/\",each.value.staname)\n\u2502 \n\u2502 Invalid value for \"seqs\" parameter: all arguments must be lists or tuples; got string.\n\n`\n```\n```\n`data \"azurerm_subscription\" \"current\" {\n}\n\nlocals {\n  staname = toset([\n    for pair in sort(var.sta_name) : {\n      staname  = pair\n    }\n  ])\n}\n\n//**********************************************************\n//  Create File System in Datalake\n//**********************************************************\nresource \"azurerm_storage_data_lake_gen2_filesystem\" \"storagedlsgen2fs\" {\n  for_each = { for p in local.staname : jsonencode(p) => p }\n  name               = var.adlsfilesystems\n  storage_account_id = concat(\"/subscriptions/\",data.azurerm_subscription.current.id,\"/resourceGroups/resourcegroup/providers/Microsoft.Storage/storageAccounts/\",each.value.staname)\n}\n`\n```\nIs it even possible to use the function here? and how can i solve this.\nthank you",
      "solution": "I think that instead of `concat`, you want `join`:\n```\n`storage_account_id = join(\"\",[\"/subscriptions/\",data.azurerm_subscription.current.id,\"/resourceGroups/resourcegroup/providers/Microsoft.Storage/storageAccounts/\",each.value.staname])\n`\n```",
      "question_score": 2,
      "answer_score": 1,
      "created_at": "2022-07-25T09:54:56",
      "url": "https://stackoverflow.com/questions/73105751/terraform-invalid-value-for-seqs-parameter-all-arguments-must-be-lists-or-tup"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "hcl",
      "question_id": 68592817,
      "title": "Conditionally enable or disable modules in root main.tf terraform",
      "problem": "How do we select or disable modules in root main.tf\nExample:\n```\n`module \"foo\" {\n  source = \"bar\"\n  count   = \"${var.include_module ? 1 : 0}\"\n`\n```\n}\nAbove one does not work, as per terraform issue discussion link\nAny alternate method ?",
      "solution": "What version of Terraform are you using? `count` and `for_each` for modules were introduced in Terraform version 0.13.0.\nNote that the interpolation syntax you're using is deprecated. Use:\n```\n`count = var.include_module ? 1 : 0\n`\n```",
      "question_score": 6,
      "answer_score": 14,
      "created_at": "2021-07-30T16:37:55",
      "url": "https://stackoverflow.com/questions/68592817/conditionally-enable-or-disable-modules-in-root-main-tf-terraform"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "hcl",
      "question_id": 66620096,
      "title": "Unable to read terraform variables.tf files into may go program",
      "problem": "I am attempting to write a go program that reads in a terraform `variables.tf` and populates a struct for later manipulation. However, I am getting errors when attempting to \"parse\" the file.  I Am hoping someone can tell me what I am doing wrong:\nCode:\n`package main\n\nimport (\n    \"fmt\"\n    \"io/ioutil\"\n    \"log\"\n    \"os\"\n\n    \"github.com/hashicorp/hcl/v2\"\n    \"github.com/hashicorp/hcl/v2/gohcl\"\n    \"github.com/hashicorp/hcl/v2/hclsyntax\"\n)\n\ntype Config struct {\n    Upstreams []*TfVariable `hcl:\"variable,block\"`\n}\n\ntype TfVariable struct {\n    Name string `hcl:\",label\"`\n    // Default     string `hcl:\"default,optional\"`\n    Type        string `hcl:\"type\"`\n    Description string `hcl:\"description,attr\"`\n    // validation block\n    Sensitive bool `hcl:\"sensitive,optional\"`\n}\n\nfunc main() {\n    readHCLFile(\"examples/string.tf\")\n}\n\n// Exits program by sending error message to standard error and specified error code.\nfunc abort(errorMessage string, exitcode int) {\n    fmt.Fprintln(os.Stderr, errorMessage)\n    os.Exit(exitcode)\n}\n\nfunc readHCLFile(filePath string) {\n    content, err := ioutil.ReadFile(filePath)\n    if err != nil {\n        log.Fatal(err)\n    }\n\n    fmt.Printf(\"File contents: %s\", content) // TODO: Remove me\n\n    file, diags := hclsyntax.ParseConfig(content, filePath, hcl.Pos{Line: 1, Column: 1})\n    if diags.HasErrors() {\n        log.Fatal(fmt.Errorf(\"ParseConfig: %w\", diags))\n    }\n\n    c := &Config{}\n    diags = gohcl.DecodeBody(file.Body, nil, c)\n    if diags.HasErrors() {\n        log.Fatal(fmt.Errorf(\"DecodeBody: %w\", diags))\n    }\n\n    fmt.Println(c) // TODO: Remove me\n}\n`\nERROR\n```\n`File contents: variable \"image_id\" {\n  type        = string\n  description = \"The id of the machine image (AMI) to use for the server.\"\n  sensitive   = false\n}\n\nvariable \"other_id\" {\n  type        = string\n  description = \"The id of the machine image (AMI) to use for the server.\"\n  sensitive   = true\n}\n2021/03/13 19:55:49 DecodeBody: examples/string.tf:2,17-23: Variables not allowed; Variables may not be used here., and 3 other diagnostic(s)\nexit status 1\n`\n```\nStack driver question is sadly for hcl1\nBlog post I am referencing.",
      "solution": "It looks like it's a bug/feature of the library, since as soon as you change `string` to `\"string\"`, e.g.,\n```\n`variable \"image_id\" {\n  type        = string\n  ...\n`\n```\nto\n```\n`variable \"image_id\" {\n  type        = \"string\"\n  ...\n`\n```\n`gohcl.DecodeBody` succeeds.\n--- UPDATE ---\nSo, they do use this package in Terraform, BUT they custom-parse configs, i.e., they don't use `gohcl.DecodeBody`. They also custom-treat `type` attributes by using `hcl.ExprAsKeyword` (compare with `description`). As you assumed, they do use a custom type for `type`, but with custom parsing you don't have to.\nBelow is a working example:\n`package main\n\nimport (\n    \"fmt\"\n    \"log\"\n    \"os\"\n\n    \"github.com/hashicorp/hcl/v2\"\n    \"github.com/hashicorp/hcl/v2/gohcl\"\n    \"github.com/hashicorp/hcl/v2/hclsyntax\"\n)\n\nvar (\n    configFileSchema = &hcl.BodySchema{\n        Blocks: []hcl.BlockHeaderSchema{\n            {\n                Type:       \"variable\",\n                LabelNames: []string{\"name\"},\n            },\n        },\n    }\n\n    variableBlockSchema = &hcl.BodySchema{\n        Attributes: []hcl.AttributeSchema{\n            {\n                Name: \"description\",\n            },\n            {\n                Name: \"type\",\n            },\n            {\n                Name: \"sensitive\",\n            },\n        },\n    }\n)\n\ntype Config struct {\n    Variables []*Variable\n}\n\ntype Variable struct {\n    Name        string\n    Description string\n    Type        string\n    Sensitive   bool\n}\n\nfunc main() {\n    config := configFromFile(\"examples/string.tf\")\n    for _, v := range config.Variables {\n        fmt.Printf(\"%+v\\n\", v)\n    }\n}\n\nfunc configFromFile(filePath string) *Config {\n    content, err := os.ReadFile(filePath) // go 1.16\n    if err != nil {\n        log.Fatal(err)\n    }\n\n    file, diags := hclsyntax.ParseConfig(content, filePath, hcl.Pos{Line: 1, Column: 1})\n    if diags.HasErrors() {\n        log.Fatal(\"ParseConfig\", diags)\n    }\n\n    bodyCont, diags := file.Body.Content(configFileSchema)\n    if diags.HasErrors() {\n        log.Fatal(\"file content\", diags)\n    }\n\n    res := &Config{}\n\n    for _, block := range bodyCont.Blocks {\n        v := &Variable{\n            Name: block.Labels[0],\n        }\n\n        blockCont, diags := block.Body.Content(variableBlockSchema)\n        if diags.HasErrors() {\n            log.Fatal(\"block content\", diags)\n        }\n\n        if attr, exists := blockCont.Attributes[\"description\"]; exists {\n            diags := gohcl.DecodeExpression(attr.Expr, nil, &v.Description)\n            if diags.HasErrors() {\n                log.Fatal(\"description attr\", diags)\n            }\n        }\n\n        if attr, exists := blockCont.Attributes[\"sensitive\"]; exists {\n            diags := gohcl.DecodeExpression(attr.Expr, nil, &v.Sensitive)\n            if diags.HasErrors() {\n                log.Fatal(\"sensitive attr\", diags)\n            }\n        }\n\n        if attr, exists := blockCont.Attributes[\"type\"]; exists {\n            v.Type = hcl.ExprAsKeyword(attr.Expr)\n            if v.Type == \"\" {\n                log.Fatal(\"type attr\", \"invalid value\")\n            }\n        }\n\n        res.Variables = append(res.Variables, v)\n    }\n    return res\n}\n`\nAdd for completeness, `example/string.tf`:\n```\n`variable \"image_id\" {\n  type        = string\n  description = \"The id of the machine image (AMI) to use for the server.\"\n  sensitive   = false\n}\n\nvariable \"other_id\" {\n  type        = string\n  description = \"The id of the machine image (AMI) to use for the server.\"\n  sensitive   = true\n}\n`\n```",
      "question_score": 3,
      "answer_score": 5,
      "created_at": "2021-03-14T02:02:52",
      "url": "https://stackoverflow.com/questions/66620096/unable-to-read-terraform-variables-tf-files-into-may-go-program"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "hcl",
      "question_id": 66711903,
      "title": "AWS IP address to use in terraform IPSec tunnels (via Transit Gateway)",
      "problem": "I'm trying to build an AWS terraform IPSec VPN config.  However, I can't remember where to find the AWS IPSec IP address; the terraform cgw documentation says the `ip_address` field is required.\nThe answer should assume the VPN will be attached to my AWS Transit Gateway.\nMy terraform:\n```\n`resource \"aws_customer_gateway\" \"cgw-abbv-for-local-and-remote\" {\n  bgp_asn    = 65001\n  ip_address = \"A.B.C.D\"   #<-- I need this IP before terraform apply\n  type       = \"ipsec.1\"\n\n  tags = {\n    Name        = \"insert-cgw-name-here\"\n  }\n}\n\nresource \"aws_vpn_connection\" \"vpn-abbv-for-local-and-remote\" {\n  customer_gateway_id = aws_customer_gateway.cgw-abbv-for-local-and-remote.id\n  transit_gateway_id  = aws_ec2_transit_gateway.my-tgw-name.id\n  type                = aws_customer_gateway.cgw-abbv-for-local-and-remote.type\n\n  tags = {\n    Name        = \"insert-vpn-name-here\"\n  }\n}\n`\n```",
      "solution": "Seems like OP already found the answer, but let me add my two cents since I spent a lot of time figuring things out when it comes to AWS VPN two years ago in order to pass the AWS Advanced Networking cert. This could potentially turn out useful for folks that are new to VPN - especially in the AWS ecosystem:\nThere is a fantastic book called AWS Certified Advanced Networking Official Study Guide which I would recommend everyone in a cloud network engineer role to read. [1]\nIt points out the following:\n\nAfter you create a VPN connection, the VPN tunnel activates when traffic is generated\nfrom your side of the VPN connection. The VGW is not the initiator; your customer gateway must initiate the tunnels. If your VPN connection experiences a period of idle time (usually\n10 seconds, depending on your configuration), the tunnel may go down. This is because\nAWS uses an on-demand DPD mechanism. If AWS receives no traffic from a VPN peer for\n10 seconds, AWS sends a DPD \u201cR-U-THERE\u201d message. If the VPN peer does not respond\nto three successive DPDs, the VPN peer is considered dead and AWS closes the tunnel. [pp. 100, 101]\n\nAt the non-AWS end of a VPN connection, the VPN is terminated on a customer gateway.\nA customer gateway is the AWS term for the VPN termination device at the customer\u2019s onpremises end. A customer gateway can also be hosted in AWS as an EC2 instance running\nVPN software that meets the requirements given in the next section.\nMost customers don\u2019t require the purchase of an additional device and can reuse an\nexisting on-premises VPN termination device to create a tunnel to a VPC. [p. 110]\n\nYou can use any third-party VPN device that supports Layer 3 VPN technologies. AWS\ndoes not support Layer 2 VPN technologies.\nIPsec is used for the VGW at the AWS end of VPN termination, and so the IPsec protocol must be supported by your VPN device. You will set up two VPN tunnels per VGW.\nSupport for BGP routing protocol is optional but recommended for advanced routing capabilities. Other routing protocols like Open Shortest Path First (OSPF) are not supported by\nAWS. You must ensure that you have opened the right ports in your on-premises firewall\nfor the IPsec traffic to flow. [p. 111]\nThat is in particular: both ends of the VPN connection must possess a public IP address!\n\nIf you didn't already, I really really recommend skipping through these pages to be aware of best-practices and the AWS-way of thinking when it comes to (hybrid) cloud architectures. You avoid getting confused afterwards if things didn't go the way you wanted to. IPSec (i.e. Layer-3) VPNs are harder to get right then most people think. One should be aware of all the routing and security relevant stuff such as: IKE, SAs, Policy-based routing, NAT-Traversal, ISAKMP etc. [see also p. 97: VPN Features -> Security & Routing sections].\nAnother good reference is the AWS Site-to-Site VPN guide (PDF). [2]\nAlso good to know: Many terraform attributes can also be found in the AWS CloudFormation docs. The docs for the AWS::EC2::CustomerGateway resource's IpAddress attribute state [3]:\n\nThe Internet-routable IP address for the customer gateway's outside interface. The address must be static.\n\n[1] https://www.programmer-books.com/wp-content/uploads/2019/04/AWS-Certified-Advanced-Networking-Official-Study-Guide.pdf\n[2] https://docs.aws.amazon.com/vpn/latest/s2svpn/s2s-vpn-user-guide.pdf\n[3] https://docs.aws.amazon.com/de_de/AWSCloudFormation/latest/UserGuide/aws-resource-ec2-customer-gateway.html#cfn-ec2-customergateway-ipaddress",
      "question_score": 3,
      "answer_score": 2,
      "created_at": "2021-03-19T17:21:58",
      "url": "https://stackoverflow.com/questions/66711903/aws-ip-address-to-use-in-terraform-ipsec-tunnels-via-transit-gateway"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "hcl",
      "question_id": 71769671,
      "title": "Add resource argument in loop from list of strings",
      "problem": "I'm trying to set up a Digital Ocean Database Firewall, which uses the below syntax:\n```\n`resource \"digitalocean_database_firewall\" \"example-fw\" {\n  cluster_id = digitalocean_database_cluster.app.id\n\n  rule {\n    type  = \"ip_addr\"\n    value = \"192.168.1.1\"\n  }\n\n  rule {\n    type  = \"ip_addr\"\n    value = \"192.0.2.0\"\n  }\n}\n`\n```\nI have a variable which is a list of whitelisted IPs that should be added to the firewall, along with the VPC IP block. I first tried to add these using `for_each`:\n```\n`# Postgres firewall (only allow connection inside VPC)\nresource \"digitalocean_database_firewall\" \"vpc-fw\" {\n  cluster_id = digitalocean_database_cluster.app.id\n\n  rule {\n    type  = \"ip_addr\"\n    value = digitalocean_vpc.app_vpc.ip_range\n  }\n}\n\n# Postgres firewall (allow connections from whitelisted IPs)\nresource \"digitalocean_database_firewall\" \"whitelisted-fw\" {\n  for_each = toset(var.db_allowed_ips)\n\n  cluster_id = digitalocean_database_cluster.app.id\n\n  rule {\n    type  = \"ip_addr\"\n    value = each.key\n  }\n}\n`\n```\nHowever it seems you can only have one firewall resource per cluster as only the last IP is saved and shows on the dashboard.\nI also tried using `for_each` in the `rule` block but this throws an error that it can only appear in module or resource blocks.\nI've also tried passing the list to `value` directly, but it only supports strings and not lists.\nHow can I add a `rule { }` block for each IP in `var.db_allowed_ips` and `digitalocean_vpc.app_vpc.ip_range`?",
      "solution": "You can achieve this with a dynamic block:\n```\n`resource \"digitalocean_database_firewall\" \"whitelisted-fw\" {\n  cluster_id = digitalocean_database_cluster.app.id\n\n  dynamic \"rule\" {\n    for_each = toset(var.db_allowed_ips)\n\n    content {\n      type  = \"ip_addr\"\n      value = each.value\n    }\n  }\n}\n`\n```\nI believe the `for_each` meta-argument at the dynamic block scope accepts values of `list(string)`, but nothing is lost by being safe and using the normal type conversion to `set(string)`.",
      "question_score": 3,
      "answer_score": 3,
      "created_at": "2022-04-06T17:35:47",
      "url": "https://stackoverflow.com/questions/71769671/add-resource-argument-in-loop-from-list-of-strings"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "hcl",
      "question_id": 72197201,
      "title": "HCL Decoding: Blocks with multiple labels",
      "problem": "My goal is to parse a HCL configuration (Terraform Configuration) and then write the collected data about variables, outputs, resources blocks and data blocks into a Markdown file.\nVariables and outputs are no problem, however, as soon as I trying to decode resource blocks, which have multiple labels.\nWorks:\n```\n`variable \"foo\" {\n  type = \"bar\"\n}\n`\n```\nDoesn't Work:\n```\n`resource \"foo\" \"bar\" {\n name = \"biz\"\n}\n`\n```\nError: `Extraneous label for resource; Only 1 labels (name) are expected for resource blocks. `\nType declaration Code:\n```\n`import (\n    \"log\"\n    \"os\"\n    \"strconv\"\n\n    \"github.com/hashicorp/hcl/v2\"\n    \"github.com/hashicorp/hcl/v2/gohcl\"\n    \"github.com/hashicorp/hcl/v2/hclsyntax\"\n)\n\ntype Variable struct {\n    Name        string         `hcl:\",label\"`\n    Description string         `hcl:\"description,optional\"`\n    Sensitive   bool           `hcl:\"sensitive,optional\"`\n    Type        *hcl.Attribute `hcl:\"type,optional\"`\n    Default     *hcl.Attribute `hcl:\"default,optional\"`\n    Options     hcl.Body       `hcl:\",remain\"`\n}\n\ntype Output struct {\n    Name        string   `hcl:\",label\"`\n    Description string   `hcl:\"description,optional\"`\n    Sensitive   bool     `hcl:\"sensitive,optional\"`\n    Value       string   `hcl:\"value,optional\"`\n    Options     hcl.Body `hcl:\",remain\"`\n}\n\ntype Resource struct {\n    Name    string   `hcl:\"name,label\"`\n    Options hcl.Body `hcl:\",remain\"`\n}\n\ntype Data struct {\n    Name    string   `hcl:\"name,label\"`\n    Options hcl.Body `hcl:\",remain\"`\n}\n\ntype Config struct {\n    Outputs   []*Output   `hcl:\"output,block\"`\n    Variables []*Variable `hcl:\"variable,block\"`\n    Resources []*Resource `hcl:\"resource,block\"`\n    Data      []*Data     `hcl:\"data,block\"`\n}\n`\n```\nDecoding Code:\n```\n`func createDocs(hclPath string) map[string][]map[string]string {\n    var variables, outputs []map[string]string\n\n    parsedConfig := make(map[string][]map[string]string)\n    hclConfig := make(map[string][]byte)\n\n    c := &Config{}\n\n    // Iterate all Terraform files and safe the contents in the hclConfig map\n    for _, file := range filesInDirectory(hclPath, \".tf\") {\n        fileContent, err := os.ReadFile(hclPath + \"/\" + file.Name())\n        if err != nil {\n            log.Fatal(err)\n        }\n        hclConfig[file.Name()] = fileContent\n    }\n\n    // Iterate all file contents\n    for k, v := range hclConfig {\n        parsedConfig, diags := hclsyntax.ParseConfig(v, k, hcl.Pos{Line: 1, Column: 1})\n        if diags.HasErrors() {\n            log.Fatal(diags)\n        }\n\n        diags = gohcl.DecodeBody(parsedConfig.Body, nil, c)\n        if diags.HasErrors() {\n            log.Fatal(diags)\n        }\n    }\n\n    for _, v := range c.Variables {\n        var variableType string\n        var variableDefault string\n\n        if v.Type != nil {\n            variableType = (v.Type.Expr).Variables()[0].RootName()\n        }\n\n        if v.Default != nil {\n            variableDefault = (v.Default.Expr).Variables()[0].RootName()\n        }\n\n        variables = append(variables, map[string]string{\"name\": v.Name, \"description\": v.Description,\n            \"sensitive\": strconv.FormatBool(v.Sensitive), \"type\": variableType, \"default\": variableDefault})\n    }\n\n    for _, v := range c.Outputs {\n        outputs = append(outputs, map[string]string{\"name\": v.Name, \"description\": v.Description,\n            \"sensitive\": strconv.FormatBool(v.Sensitive), \"value\": v.Value})\n    }\n\n    parsedConfig[\"variables\"], parsedConfig[\"outputs\"] = variables, outputs\n\n    return parsedConfig\n}\n`\n```\nQuestion: How can I parse multiple labels from resource blocks?",
      "solution": "The error you shared is due to the definition of `type Resource`. `resource` blocks (and `data` blocks) in Terraform expect two labels, indicating the resource type and name. To match that in the schema you're implying with these struct types, you'll need to define to fields that are tagged as `label`:\n```\n`type Resource struct {\n    Type    string   `hcl:\"type,label\"`\n    Name    string   `hcl:\"name,label\"`\n    Options hcl.Body `hcl:\",remain\"`\n}\n\ntype Data struct {\n    Type    string   `hcl:\"type,label\"`\n    Name    string   `hcl:\"name,label\"`\n    Options hcl.Body `hcl:\",remain\"`\n}\n`\n```\n\nAlthough this should work for the limited input you showed here, I want to caution that you are using the higher-level `gohcl` API which can decode only a subset of HCL that maps well onto Go's struct types. Terraform itself uses the lower-level APIs of `hcl.Body` and `hcl.Expression` directly, which allows the Terraform language to include some HCL features that the `gohcl` API cannot directly represent.\nDepending on what your goal is, you may find it better to use the official library `terraform-config-inspect`, which can parse, decode, and describe a subset of the Terraform language at a higher level of abstraction than the HCL API itself. It also supports modules written for Terraform versions going all the way back to Terraform v0.11, and is the implementation that backs the analysis of modules done by Terraform Registry.",
      "question_score": 3,
      "answer_score": 1,
      "created_at": "2022-05-11T09:42:33",
      "url": "https://stackoverflow.com/questions/72197201/hcl-decoding-blocks-with-multiple-labels"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "hcl",
      "question_id": 75981253,
      "title": "Terraform: Handling locals that are conditional due to feature flags",
      "problem": "I'm building a Terraform module that uses some variables for feature flags along with locals for storing some computed values. I'm bumping into some errors while a flag is true.\nThe flags (booleans saved as variables) are on every resource and use this convention which seems to be standard in Terraform:\n```\n`resource \"provider_resource_id\" \"resource_name\" {\n    ...\n    count = var.disable_resource ? 0 : 1\n    ...\n}\n`\n```\nThe provider outputs IDs when making these resources and because `count` forces me to put an index on them, I'm saving them as locals in a locals.tf file to be less verbose:\n```\n`locals {\n    resource_name_sid = provider_resource_id.resource_name[0].sid\n}\n`\n```\nI'm now running `terraform apply` when `disable_resource = true` and get this error: `Invalid index: provider_resource_id.resource_name[0].sid (provider_resource_id.resource_name is empty tuple)`. I see that defining the local when the resource isn't created is a problem. So I commented out the local. Now I get other errors on all resources expecting the local: `Reference to undeclared local value: (resource_name_sid has not been declared)` These resources wouldn't actually be built due to the flag, but they still expect the local (which I can't define because the resource isn't being built).\nI bet I can put a ternary on every local to say, for example:\n```\n`locals {\n    resource_name_sid = var.disable_resource ? \"\" : provider_resource_id.resource_name[0].sid\n}\n`\n```\nBut that is getting verbose again. Maybe I can't externalize these locals and use feature flags at the same time. (I did try moving locals into the resources file but got the same result.) Maybe I need to abandon the use of locals for storing these and just put them inline in the resources. Or is there something I am missing?",
      "solution": "There is no way to avoid explaining to Terraform what should happen in the case where the object doesn't exist, but there are some shorter ways to express the idea of using a fallback value as a placeholder when there are zero instances of the resource.\n\nOne concise option is to use `one`, which is a function intended to deal with the common situation of turning a list of zero or one elements into a value that might be `null`:\n```\n`locals {\n  resource_name_sid = one(provider_resource_id.resource_name[*].sid)\n}\n`\n```\n`provider_resource_id.resource_name[*].sid` produces a list of length matching the count of `provider_resource_id.resource_name`. In your configuration the count can only be either zero or one, which matches the expectations of `one`.\nTherefore `local.resource_name_sid` will either be a single `sid` value or it will be `null`.\n\nAnother possibility is to use `try` to let the element lookup `[0]` fail and provider a fallback value to use if it does:\n```\n`locals {\n  resource_name_sid = try(provider_resource_id.resource_name[0].sid, null)\n}\n`\n```\nThis option lets you choose a different fallback value to use instead of `null` if you like, although `null` is the typical way to represent the absense of a value in Terraform so I would suggest using that unless you have some other working SID value to use as a fallback.\nUsing `null` has the advantage that you can then assign `local.resource_name_sid` directly to an argument of another resource and then in the case where its `null` it will be completely indistinguishable to the provider from having omitted that argument entirely, because `null` also represents the absence of an argument.\n\nA final option is to directly test the length of `provider_resource_id.resource_name` to see if there is a zeroth index:\n```\n`locals {\n  resource_name_sid = (\n    length(provider_resource_id.resource_name) > 0 ?\n    provider_resource_id.resource_name[0].sid :\n    null\n}\n`\n```\nThis is similar to the conditional you included in your question but it directly tests whether there's a `provider_resource_id.resource_name[0]` rather than repeating the reference to `var.disable_resource`.\nTesting the resource directly means that if you change the `count` definition in future then you won't need to update this expression too, as long as your new `count` expression still chooses between either zero or one elements.\nHowever, this is the most verbose option and requires repeating the long expression `provider_resource_id.resource_name` in two places, so I'd typically use the `try` option above if I needed to have a non-null fallback value, and the `one` option if `null` is a sufficient fallback value.\nThe `one` function also has the advantage over the others that it will fail if there is ever more than one instance of `provider_resource_id.resource_name`, and so if you update this module to have multiple instances of that resource in future then you'll be reminded by the error to update your other expressions to deal with two or more SID values. The other expressions will just silently ignore the other SIDs.",
      "question_score": 2,
      "answer_score": 4,
      "created_at": "2023-04-11T00:41:17",
      "url": "https://stackoverflow.com/questions/75981253/terraform-handling-locals-that-are-conditional-due-to-feature-flags"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "hcl",
      "question_id": 66788291,
      "title": "Jenkinsfile print parameters nicely",
      "problem": "We have a Jenkinsfile with parameters declared as follows:\n```\n`def params = [string(name: 'ENVIRONMENT', value: environment),\n              string(name: 'VERSION',     value: version),\n              string(name: 'REGION',      value: region)]\n`\n```\nI'd like to echo these, for example\n```\n`DEPLOYING: ENVIRONMENT=STAGING, VERSION=1.3.0, REGION=EU\n`\n```\nHowever calling `echo \"$params\"` prints:\n```\n`[@string(name=ENVIRONMENT,value=STAGING), @string(name=VERSION,value=1.3.0), @string(name=REGION,value=EU)]\n`\n```\nI tried iterating the array - e.g. :\n`params.each { echo it }` throws `UnsupportedOperationException: no known implementation of class java.lang.String is using symbol \u2018string\u2019`\n`params.each { echo it.name }`  throws `RejectedAccessException: No such field found: field org.jenkinsci.plugins.structs.describable.UninstantiatedDescribable name`\nHow can I print the `params` array nicely?\nEDIT - from Matt Schuchard's response:\n```\n`def params = [string(name: 'ENVIRONMENT', value: \"STAGING\"),\n              string(name: 'VERSION',     value: \"1.3.0\"),\n              string(name: 'REGION',      value: \"EU\")]\nprint \"$params\"\n\nparams.each() { param, value ->\n    print \"Parameter: ${param}, Value: ${value}\"\n}\n`\n```\nreturns (i.e. Value is all null):\n```\n`[@string(name=ENVIRONMENT,value=STAGING), @string(name=VERSION,value=1.3.0), @string(name=REGION,value=EU)]\nParameter: @string(name=ENVIRONMENT,value=STAGING), Value: null\nParameter: @string(name=VERSION,value=1.3.0), Value: null\nParameter: @string(name=REGION,value=EU), Value: null\n`\n```",
      "solution": "You can use a Groovy map iterator lambda method (such as `each`) to iterate over the `params` map. An example follows:\n```\n`params.each() { param, value ->\n  print \"Parameter: ${param}, Value: ${value}\"\n}\n`\n```\nIf you decide to use this directly inside a `pipeline` block, then this will need to be placed inside a `script` block when using declarative DSL.",
      "question_score": 2,
      "answer_score": 4,
      "created_at": "2021-03-24T20:46:38",
      "url": "https://stackoverflow.com/questions/66788291/jenkinsfile-print-parameters-nicely"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "hcl",
      "question_id": 66460117,
      "title": "parse simple terraform file using go",
      "problem": "I now tried, everything but cant get this simple thing to work.\nI got the following `test_file.hcl`:\n```\n`variable \"value\" {\n  test = \"ok\"\n}\n`\n```\nI want to parse it using the following code:\n```\n`package hcl\n\nimport (\n    \"github.com/hashicorp/hcl/v2/hclsimple\"\n)\n\ntype Config struct {\n    Variable string `hcl:\"test\"`\n}\n\nfunc HclToStruct(path string) (*Config, error) {\n    var config Config\n\n    return &config, hclsimple.DecodeFile(path, nil, &config)\n \n`\n```\nBut I receive:\n```\n`test_file.hcl:1,1-1: Missing required argument; The argument \"test\" is required, but no definition was found., and 1 other diagnostic(s)\n`\n```\nI checked with other projects using the same library but I cannot find my mistake .. I just dont know anymore. Can someone guide me into the right direction?",
      "solution": "The Go struct type you wrote here doesn't correspond with the shape of the file you want to parse. Notice that the input file has a `variable` block with a `test` argument inside it, but the Go type you wrote has only the `test` argument. For that reason, the HCL parser is expecting to find a file with just the `test` argument at the top-level, like this:\n```\n`test = \"ok\"\n`\n```\nTo parse this Terraform-like structure with `hclsimple` will require you to write two struct types: one to represent the top level body containing `variable` blocks and another to represent the content of each of the blocks. For example:\n```\n`type Config struct {\n  Variables []*Variable `hcl:\"variable,block\"`\n}\n\ntype Variable struct {\n  Test *string `hcl:\"test\"`\n}\n`\n```\nWith that said, I'll note that the Terraform language uses some features of HCL that `hclsimple` can't support, or at least can't support with direct decoding like this. For example, the `type` argument inside `variable` blocks is a special sort of expression called a type constraint which `hclsimple` doesn't support directly, and so parsing it will require using the lower-level HCL API. (That's what Terraform is doing internally.)",
      "question_score": 2,
      "answer_score": 3,
      "created_at": "2021-03-03T16:31:40",
      "url": "https://stackoverflow.com/questions/66460117/parse-simple-terraform-file-using-go"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "hcl",
      "question_id": 75376845,
      "title": "Nomad job: interpolating within Docker args",
      "problem": "I'm writing a Thanos sidecar job for Prometheus.  I have looked and can't figure out how to specify this argument in a config file, so my job looks like this.  This is a templatefile for Terraform, so if you try to reference `${var}`, Terraform tries to evaluate that as a templatefile variable.\n```\n`  task \"thanos-sidecar\" {\n      driver = \"docker\"\n\n      config {\n        image = \"quay.io/thanos/thanos:v0.30.2\"\n        args = [\n          \"sidecar\",\n          \"--tsdb.path=/alloc/data/tsdb\",\n# XXX BUSTED\n# variants using single or double curlies break the job. HALP.\n#          \"--prometheus.url=http://$NOMAD_ADDR_prometheus_ui\",  # doesn't evaluate\n          \"--grpc-address=0.0.0.0:10901\",\n          \"--http-address=0.0.0.0:10902\",\n          \"--objstore.config-file=/local/thanos.yml\"\n        ]\n      } # end config\n\n      template {\n        change_mode = \"restart\"\n        destination = \"local/thanos.yml\"\n\n        data = I've tried a number of different syntax options.  I'm almost at the point of using Terraform to ask Consul where the job is and pass in the address to be evaluated in the templatefile.  That's less than ideal for a few reasons, not least of which is that the job might get rescheduled but Terraform might not know that, so the address could change without being reformulated in the config file.\nI looked also at using template sequences, but no dice.",
      "solution": "And...the solution is easy enough.  Use the format `$${ NOMAD_ADDR_prometheus_ui }` - note the double dollar sign, so that Terraform's `templatefile()` won't interpret it at the first stage.",
      "question_score": 2,
      "answer_score": 2,
      "created_at": "2023-02-07T18:19:03",
      "url": "https://stackoverflow.com/questions/75376845/nomad-job-interpolating-within-docker-args"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "hcl",
      "question_id": 71247793,
      "title": "terraform temporary/intermediate variables in loops?",
      "problem": "I would like to know if it is possible to use temporary variables inside a for loop for intermediate computation. If yes, what is the syntax to use that, the example is simplified just to illustrate the problem. This example code works. I've commented the portion which doesn't work, but would like to know how to use intermediate variable. Thanks!\n```\n`\nlocals {\n  nestedmap =  fx}\n}\n`\n```",
      "solution": "There are no intermediate variables in TF. You have to just repeat `trimprefix(ux_key, \"user-\")`:\n```\n`  flatlist = flatten([\n    for ux_key, ux_val in yamldecode(local.nestedmap): [\n      for tx_key, tx_val in ux_val.tables: {\n        pfx = trimprefix(ux_key, \"user-\")\n        key = \"${trimprefix(ux_key, \"user-\")}-${tx_key}\"\n        db = ux_val.db\n        rows = tx_val.rows\n      }\n    ]\n  ])\n`\n```",
      "question_score": 2,
      "answer_score": 1,
      "created_at": "2022-02-24T07:04:50",
      "url": "https://stackoverflow.com/questions/71247793/terraform-temporary-intermediate-variables-in-loops"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "hcl",
      "question_id": 77865781,
      "title": "How to set Packer variables in HCL2",
      "problem": "I am new to packer and need help to understand the `variables` usage\nso I have a main `build.pck.hcl` file with `source` and other sections where I want to use variables defined in another file like this:\n```\n`...\nclient_id = var.client_id\n...\n`\n```\nI created another file for variables only:\n`account.pkr.hcl`\n```\n`variable \"client_id\" {\n    type = string\n}\n`\n```\nthe I start `packer` like this:\n```\n`packer build -color=false -debug  -var \"client_id=123\" -var-file account.pkr.hcl build.pck.hcl\n`\n```\nbut I always get an error while running:\n```\n`Error: Variable declaration in a .pkrvar file\n\n  on account.pkr.hcl line 14, in variable \"client_id\":\n  14: variable \"client_id\" {\n\nA .pkrvar file is used to assign values to variables that have already been\ndeclared in .pkr files, not to declare new variables. To declare variable\n\"client_id\", place this block in one of your .pkr files, such as\nvariables.pkr.hcl\n\nTo set a value for this variable in account.pkr.hcl, use the definition syntax\ninstead:\n    client_id = \n`\n```\nWhat I am missing ? How to fix that ?\nI want to some variables have `default` value and some of them will be passed on command line\nP.S. If I changed `packer build` command and pass a folder path where files are I get an error:\n```\n`Error: Undefined -var variable\n\nA \"client_id\" variable was passed in the command line but was not found in known\nvariables. To declare variable \"client_id\", place this block in one of your .pkr\nfiles, such as variables.pkr.hcl\n\n`\n```",
      "solution": "It looks like you are using `-var-file` because you think you need to tell packer where to find that file. In reality, you don't need to do that and you simply need to use `.` as a context when running `packer build`, as packer will load all files ending in `.pkr.hcl`.\nThe following should work for you:\n```\n`$ packer build -color=false -debug -var \"client_id=123\" .\n`\n```",
      "question_score": 2,
      "answer_score": 1,
      "created_at": "2024-01-23T12:00:31",
      "url": "https://stackoverflow.com/questions/77865781/how-to-set-packer-variables-in-hcl2"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "hcl",
      "question_id": 74085827,
      "title": "Terraform variable from gitlab CI/CD variables",
      "problem": "I understand that CI/CD variables can be used in HCL by counting on the fact that having them declared them with a TF_VAR_ prefix in the environment will enable me to look them up as input variables, then use them in the .tf file where I need them.\nI did:\n\nset my variable via the UI in the GitLab project, as `TF_VAR_ibm_api_key`, then masked it.\nwrite a variable block for it in `main.tf`\ncall it where I need it in the same file `main.tf`\ntried including the variable in `variables.tf`, same result\nread the documentation from gitlab and from terraform, but I'm not getting this right.\n\nThis is my `main.tf` file:\n```\n`variable ibm_api_key {\n}\n\nterraform {\n  required_version = \">= 0.13\"\nrequired_providers {\n    ibm = {\n    source = \"IBM-Cloud/ibm\"\n    }\n }\n}\n\nprovider \"ibm\" {\n  ibmcloud_api_key = var.ibm_api_key\n}\n`\n```\nExpected behavior: the variable is passed from the CI/CD and added to the HCL code.\nCurrent behavior: during \u00b4plan\u00b4, the job falls with error code 1\n```\n`$ terraform plan\nvar.ibm_api_key\n  Enter a value: \u2577\n\u2502 Error: No value for required variable\n\u2502 \n\u2502   on main.tf line 1:\n\u2502    1: variable ibm_api_key {\n\u2502 \n\u2502 The root module input variable \"ibm_api_key\" is not set, and has no default\n\u2502 value. Use a -var or -var-file command line argument to provide a value for\n\u2502 this variable.\n\u2575\n`\n```\n\nalthough logically it can't seem to be the issue, I tried formatting the variable call as string interpolation, like:\nprovider \"ibm\" {\nibmcloud_api_key = \"${var.ibm_api_key}\"\n}\nnaturally to no avail.\n\nalthough logically it can't seem to be the issue, I tried defining a type for the variable:\nvariable ibm_api_key {\ntype = string\n}\nnaturally to no avail.\n\nIn order to check if variables are passed from the CI/CD settings to the gitlab runner's environment, I added a variable that is neither protected nor masked, and assigned string  inserted a double check:\n\necho ${output_check}\necho ${TF_VAR_ibm_api_key}\n\nwhich does not result in an error, but are not being printed either. Only the \"echo\" commands appear in the output.\n```\n`$ echo ${output_check}\n$ echo ${TF_VAR_ibm_api_key}\nCleaning up project directory and file based variables 00:01\nJob succeeded\n`\n```",
      "solution": "The error was in the CI/CD settings.\nThe variables were set to be exclusively passed to protected branches. I was pushing my code to an unprotected one, which prevented variables being passed. When merging the code to a protected branch, the variables showed up correctly. Variables are also correctly imported to Terraform, with the expected exclusion of the TF_VAR_ prefix.\nTL;DR If you're having this issue in GitLab's CI/CD check your CICD variables' setting for protected branches, and if the branch you're pushing to corresponds to that setting.",
      "question_score": 1,
      "answer_score": 3,
      "created_at": "2022-10-16T11:08:49",
      "url": "https://stackoverflow.com/questions/74085827/terraform-variable-from-gitlab-ci-cd-variables"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "hcl",
      "question_id": 70246154,
      "title": "Terraform match multiple conditions in a single conditional",
      "problem": "I have 4 envs, qa and dev use one ID, uat and prod use another. I'm trying to do an if else, basically, if env is dev or qa, use id1, else use id2. This is what I tried:\n```\n`locals{\n  endpoint_id = \"${var.env == \"dev\" || \"qa\" ? \"id1\" : \"id2\"}\"\n}\n`\n```\nAnd this is what I get:\n```\n`Error: Invalid operand\n\u2502 \n\u2502   on ssm-parameters.tf line 2, in locals:\n\u2502    2:   endpoint_id = \"${var.env == \"dev\" || \"qa\" ? \"id1\" : \"id2\"}\"\n\u2502 \n\u2502 Unsuitable value for right operand: a bool is required.\n`\n```\nApparently I can't do an \"OR\" here. How would I go about this? Thank you.",
      "solution": "How about:\n```\n`locals{\n  endpoint_id = length(regexall(\"dev|qa\", var.env)) > 0 ? \"id1\" : \"id2\"\n}\n`\n```\nThis will check if `var.env` matches either `dev` or `qa`, the output is a list, if there's some match the list will have at least one element and zero otherwise.",
      "question_score": 1,
      "answer_score": 5,
      "created_at": "2021-12-06T14:17:41",
      "url": "https://stackoverflow.com/questions/70246154/terraform-match-multiple-conditions-in-a-single-conditional"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "hcl",
      "question_id": 74821218,
      "title": "Terraform not declaring tfvars",
      "problem": "I am new to Terraform and I am writing a script. Following is my directory structure\n```\n`folder\n---.terraform\n---..terraform.lock.hcl\n---main.tf\n---terraform.tfvars\n---variables.tf\n`\n```\nFollowing is my content on `terraform.tfvars`.\n```\n`environment    = \"development\"\n`\n```\nFollowing is my content on `main.tf`.\n```\n`tags = {\n  environment = var.environment\n}\n`\n```\nBut the values are not updating. Following is the error:\n```\n`\u2577\n\u2502 Warning: Value for undeclared variable\n\u2502\n\u2502 The root module does not declare a variable named \"environment\" but a value was found in file \"terraform.tfvars\". If you meant to use this value, add a \"variable\" block to the configuration.\n\u2502\n\u2502 To silence these warnings, use TF_VAR_... environment variables to provide certain \"global\" settings to all configurations in your organization. To reduce the verbosity of these warnings, use the      \n\u2502 -compact-warnings option.\n\u2575\n\u2577\n\u2502 Warning: Value for undeclared variable\n\u2502\n\u2502 The root module does not declare a variable named \"admin_username\" but a value was found in file \"terraform.tfvars\". If you meant to use this value, add a \"variable\" block to the configuration.        \n\u2502\n\u2502 To silence these warnings, use TF_VAR_... environment variables to provide certain \"global\" settings to all configurations in your organization. To reduce the verbosity of these warnings, use the      \n\u2502 -compact-warnings option.\n\u2575\n\u2577\n\u2502 Warning: Values for undeclared variables\n\u2502\n\u2502 In addition to the other similar warnings shown, 1 other variable(s) defined without being declared.\n\u2575\n\u2577\n\u2502 Error: Reference to undeclared input variable\n\u2502\n\u2502   on main.tf line 22, in resource \"azurerm_resource_group\" \"tf_example_rg\":\n\u2502   22:     environment = var.environment\n\u2502\n\u2502 An input variable with the name \"environment\" has not been declared. This variable can be declared with a variable \"environment\" {} block.\n`\n```\nAs I am using `terraform.tfvars` I don't need to give the filename on CLI. I think I am doing everything right but it's yet not working.",
      "solution": "You have to actually declare your variable using variable block. For example:\n```\n`variable \"environment\" {}\n`\n```\nIf you have such declarations, you have to double check the spelling and locations of them.",
      "question_score": 1,
      "answer_score": 4,
      "created_at": "2022-12-16T08:13:48",
      "url": "https://stackoverflow.com/questions/74821218/terraform-not-declaring-tfvars"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "hcl",
      "question_id": 74047054,
      "title": "How to use Bash Regex Lookbehind across whitespace in Terraform file?",
      "problem": "I'm writing a bash script to parse a bunch (a dozen or more) massive Terraform files that contain a large number of google_bigquery_dataset resources and their associated IAM access blocks. The script should take each dataset resource and copy it to another file, named for the dataset itself.\nAll of this is fine, except extracting the name of the dataset from the resource's \"dataset_id\" field. This would be easy enough, if not for the fact that some of these dataset resources have authorized view blocks that also contain \"dataset_id\" values.\nHere is an example of such a resource:\n```\n`resource \"google_bigquery_dataset\" \"project-bigquery-dataset-RESOURCE_NAME\" {\n  access {\n    role          = \"WRITER\"\n    special_group = \"projectWriters\"\n  }\n\n  access {\n    role          = \"READER\"\n    special_group = \"projectReaders\"\n  }\n\n  access {\n    role          = \"WRITER\"\n    user_by_email = \"user1@project.iam.gserviceaccount.com\"\n  }\n\n  access {\n    role          = \"OWNER\"\n    special_group = \"projectOwners\"\n  }\n\n  access {\n    view {\n      dataset_id = \"DO_NOT_WANT\"\n      project_id = \"project\"\n      table_id   = \"table1\"\n    }\n  }\n\n  access {\n    view {\n      dataset_id = \"DO_NOT_WANT\"\n      project_id = \"project\"\n      table_id   = \"table2\"\n    }\n  }\n\n  access {\n    view {\n      dataset_id = \"DO_NOT_WANT\"\n      project_id = \"project\"\n      table_id   = \"table3\"\n    }\n  }\n\n  dataset_id                      = \"THIS_IS_WHAT_I_WANT\"\n  default_partition_expiration_ms = \"0\"\n  delete_contents_on_destroy      = \"false\"\n\n  labels = {\n    application-name = \"app-name\"\n  }\n\n  location = \"US\"\n  project  = \"project\"\n}\n`\n```\nBefore I realized that the authorized view blocks also had a `dataset_id` field, I was using this to try to grab the value I wanted, assuming `startIndex` and `endIndex` are just the start and end line numbers representing a complete dataset resource block as above:\n```\n`fileName=$( sed -n ${startIndex},${endIndex}p $bigFile | grep \"dataset_id\" | cut -d\\\" -f2)\n`\n```\nWhich works only when there are not Authorized View blocks contained other `dataset_id` values.\nI then tried to use a Negative Lookbehind:\n```\n`fileName=$( sed -n ${startIndex},${endIndex}p $bigFile | grep '(?That doesn't work. I'm not sure if it's because of the newline or because of the whitespace between the end of `view {` and the start of `dataset_id = \"DO_NOT_WANT\"`.\nI've tried variations on it, such as `(?\nIs there any way to capture only the `dataset_id` that isn't in a view block?\nA couple notes:\n\nI can guarantee that `view {` will always precede the `dataset_id` in a block, without a line break.\nI cannot guarantee the order. It's possible the `dataset_id` I'm trying to capture could be present before the `view` blocks, after them, or even somewhere between them.\nDesired output for the above example would simply be `THIS_IS_WHAT_I_WANT`\nAny help would be appreciated.",
      "solution": "With your shown samples only, please try following `awk` code. Written and tested in GNU `awk`.\n`awk -v RS= -v FS=\"\\n\" '\n/^[[:space:]]+dataset_id[[:space:]]+/{\n  split($1,arr,\"\\\"\")\n  print arr[2]\n}\n'  Input_file\n`\nExplanation: Simple explanation for complete code would be:\n\nSetting `RS`(Record separator) as paragraph mode in `awk` program.\nThen setting `FS`(Field separator) as new line.\nThen in main block checking condition if line starts from 1 or more spaces followed by `dataset_id` followed by again 1 or more spaces, if this condition is TRUE then:\nUsing `split` function of `awk` to split $1(first field) into an array named `arr` with delimiter of `\"`. This basically creates an array named `arr` with index of 1 2 3 4 and so on depending upon how many elements it splits based on delimiter.\nThen printing array `arr`'s 2nd element which is required output by OP.",
      "question_score": 1,
      "answer_score": 3,
      "created_at": "2022-10-12T21:22:17",
      "url": "https://stackoverflow.com/questions/74047054/how-to-use-bash-regex-lookbehind-across-whitespace-in-terraform-file"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "hcl",
      "question_id": 73273661,
      "title": "Use attribute KEY and VALUE of a map inside a list element",
      "problem": "I have defined a List called data, inside the list, i use a map with a key and a value. I now need to access the KEY and VALUE inside each element of the data list.\n```\n`locals {\n      data = [\n        { \"secret1\" = 1 },\n        { \"secret2\" = 1 },\n        { \"secret3\" = 1 },\n        { \"secret4\" = 1 },\n        { \"secret5\" = 1 }\n      ]\n    }\n`\n```\nThe goal is to use the KEY and VALUE inside a google secret resource,\nthe value should then be used inside secret and version attribute.  Something like this:\n```\n`data \"google_secret_manager_secret_version\" \"secret_datas\" {\n  count   = length(local.data)\n  secret  = local.data[count.index].key\n  project = \"myproject\"\n  version = local.data[count.index].value\n}\n`\n```\nMy Current Error Message\n```\n`\u2502 Error: Unsupported attribute\n\u2502 \n\u2502   on dependabot.tf line 38, in data \"google_secret_manager_secret_version\" \"secret_data\":\n\u2502   38:   version = local.data[count.index].value\n\u2502     \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\u2502     \u2502 count.index is 1\n\u2502     \u2502 local.data is tuple with 5 elements\n\u2502 \n\u2502 This object does not have an attribute named \"value\".\n`\n```",
      "solution": "This would be much easier with the modern for_each meta-argument. After optimizing the structure of the `data` in the `locals`:\n```\n`locals {\n  data = {\n    \"secret1\" = 1,\n    \"secret2\" = 1,\n    \"secret3\" = 1,\n    \"secret4\" = 1,\n    \"secret5\" = 1\n  }\n}\n`\n```\nwe can easily use it in the resource.\n```\n`data \"google_secret_manager_secret_version\" \"secret_datas\" {\n  for_each = local.data\n  \n  secret  = each.key\n  project = \"myproject\"\n  version = each.value\n}\n`\n```",
      "question_score": 1,
      "answer_score": 4,
      "created_at": "2022-08-08T08:31:20",
      "url": "https://stackoverflow.com/questions/73273661/use-attribute-key-and-value-of-a-map-inside-a-list-element"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "hcl",
      "question_id": 72871906,
      "title": "Convert terraform resourceData of type map[string]interface{} to struct",
      "problem": "I'm creating a custom terraform provider and I came across this issue.\nI was trying to convert a `schema.TypeList` field into a struct, the TypeList looks something like this:\n```\n`\"template\": {\n                Type:     schema.TypeList,\n                Required: true,\n                ForceNew: false,\n                Elem: &schema.Resource{\n                    Schema: map[string]*schema.Schema{\n                        \"lists_test\": {\n                            Type:     schema.TypeSet,\n                            Required: true,\n                            ForceNew: false,\n                            Elem: &schema.Schema{\n                                Type: schema.TypeString,\n                            },\n                        },\n                        \"name_test\": {\n                            Type:     schema.TypeString,\n                            Required: true,\n                            ForceNew: false,\n                        },\n},},\n`\n```\nand the struct that I'm trying to align to looks something like this:\n```\n`type TestStruct struct {\n    NameTest string   `json:\"name_test\"`\n    ListsTests   []string `json:\"lists_test\"`\n}\n`\n```\nI tried a couple of solutions, for instance I tried unmarshalling it to json. Something like below:\n```\n`template := d.Get(\"template\").([]interface{})[0].(map[string]interface{})\ntemplateStr, err := json.Marshal(template)\ntemplateConverted := &TestStruct{}\njson.Unmarshal(template, templateConverted)\n`\n```\nhowever, I'm getting an error `json: unsupported type: SchemaSetFunc`, which is probably because it's trying to marshal a `schema.Schema` type instead of map[string]interface{} type, which confuses me. I also tried to use `gohcl.DecodeBody` but I abandoned the idea since it's usage seems more inclined into reading direct tf files rather than `*schema.ResourceData` types.\nDoes anyone had the same experience dealing with this type of scenario? Any help or suggestion is appreciated. Thank you!",
      "solution": "Terraform's older SDK (SDKv2) is not designed around the paradigm of decoding into a tagged structure, and instead expects you to use `d.Get` and manually type-assert individual values, which in your case would perhaps look something like this:\n```\n`  raw := d.Get(\"template\").([]interface{})[0].(map[string]interface{})\n  t := &TestStruct{\n      NameTest: raw[\"name_test\"].(string),\n      ListsTests: make([]string, len(raw[\"lists_test\"].([]interface{})),\n  }\n  for i, itemRaw := range raw[\"lists_test\"].([]interface{}) {\n    t.ListsTests[i] = itemRaw.(string)\n  }\n`\n```\nThe idiomatic style for most Terraform providers is to write logic like this in separate functions for each complex-typed attribute, where each returns an object of the appropriate type in the target platform's SDK. There would typically also be a matching function for going in the opposite direction: given an object from the target platform's SDK, return a `map[string]interface{}` that can be assigned to this attribute using `d.Set`.\n\nHowever, just because there isn't something built in to the SDK to handle this, that doesn't mean you can't use other libraries that are more general utilities for use in any Go programs.\nOne example library is `github.com/mitchellh/mapstructure`, which is designed for exactly the goal you have in mind: to take a value of some interface type and try to use reflection to fit it onto a tagged structure type.\nIf you want to use that library then you would need to annotate your structure with `mapstructure:`, instead of the `json:` ones, and then pass your `raw` value to the `mapstructure.Decode` function:\n```\n`  raw := d.Get(\"template\").([]interface{})[0].(map[string]interface{})\n  var t TestStruct\n  err := mapstructure.Decode(raw, &t)\n`\n```\nSince the `schema.ResourceData` abstraction in SDKv2 guarantees to return specific data types based on the schema you defined, you should not typically get errors from `mapstructure.Decode` as long as your schema and your target type match, but still a good idea to check for errors anyway because otherwise your `t` value may not be completely populated, causing confusing broken behavior downstream.\nThis is not a typical implementation style used in the official providers, but there's no real harm in writing your provider in this way if you find this style more convenient, or easier to maintain.\n\nAlternatively, if you are not already deeply invested in SDKv2 then you may wish to consider using Terraform Plugin Framework instead. As well as being designed around the type system of modern Terraform (whereas SDKv2 was designed for Terraform v0.11 and earlier), it also supports a programming style more like what you are aiming for, with methods like `tfsdk.Plan.Get` and `tfsdk.Plan.GetAttribute` that can decode directly into an appropriately-shaped and appropriately tagged \"normal\" Go value.\nI can't easily show an example of that because it would presume a provider written in quite a different way, but hopefully you can see from the signature of those two functions how they might be used. There's some more commentary and examples in Accessing State, Config, and Plan.",
      "question_score": 1,
      "answer_score": 4,
      "created_at": "2022-07-05T17:23:22",
      "url": "https://stackoverflow.com/questions/72871906/convert-terraform-resourcedata-of-type-mapstringinterface-to-struct"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "hcl",
      "question_id": 69441924,
      "title": "convert terraform HCL variable type=map(any) to JSON",
      "problem": "I am trying to convert an terraform variable written in HCL to a dynamically generated `tf.json` file containing the variable, but I am running into errors.\nHCL version I am trying to convert:\n```\n`variable \"accounts\" {\n  type        = map(any)\n\n  default = {\n    acct1     = [\"000000000001\"]\n    acct2     = [\"000000000002\"]\n  }\n}\n`\n```\nI have tried the following format:\n```\n`{\n  \"variable\": {\n    \"accounts\": {\n      \"type\": \"map(any)\",\n\n      \"default\": [\n        { \"acct1\": \"000000000001\" },\n        { \"acct2\": \"000000000002\"}\n      ]\n    }\n  }\n}\n`\n```\nand\n```\n`{\n  \"variable\": {\n    \"accounts\": {\n      \"type\": \"map(any)\",\n      \"default\": [\n        {\n          \"acct1\": [\"000000000001\"],\n          \"acct2\": [\"000000000002\"]\n        }\n      ]\n    }\n  }\n}\n`\n```\nI get the following error:\n```\n`\u2502 Error: Invalid default value for variable\n\u2502 \n\u2502   on accounts.tf.json line 6, in variable.accounts:\n\u2502    6:       \"default\": [\nThis default value is not compatible with the variable's type constraint: map of any single type required.\n`\n```\nIs there a tool that will convert HCL to valid `.tf.json` configurations? Or what am I missing on the formatting here?",
      "solution": "Your specified type for the variable is a `map(any)`, so your default value for the variable must also be a `map(any)`, and cannot be a `list(map(list(string)))`.\n```\n`{\n  \"variable\": {\n    \"accounts\": {\n      \"type\": \"map(any)\",\n      \"default\": {\n        \"acct1\": [\"000000000001\"],\n        \"acct2\": [\"000000000002\"]\n      }\n    }\n  }\n}\n`\n```\nThat would assign a default value of type `object(list(string))` which matches the same `object(list(string))` type structure in your HCL2, and also would be a subset of the specified `map(any)`.",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2021-10-04T22:14:44",
      "url": "https://stackoverflow.com/questions/69441924/convert-terraform-hcl-variable-type-mapany-to-json"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "hcl",
      "question_id": 78485634,
      "title": "Parsing terraform tfvars file with Golang",
      "problem": "I am trying the dynamically manipulate the tfvars file using golang.\nHere is my code\n`package main\n\nimport (\n    \"fmt\"\n    \"io\"\n    \"log\"\n    \"os\"\n\n    \"github.com/hashicorp/hcl/v2\"\n    \"github.com/hashicorp/hcl/v2/gohcl\"\n    \"github.com/hashicorp/hcl/v2/hclsyntax\"\n    \"github.com/hashicorp/hcl/v2/hclwrite\"\n\n)\n\ntype RawDatabase struct {\n    Name                string   `hcl:\"name\"`\n    BusinessVerticalID  string   `hcl:\"business_vertical_id\"`\n\n    Readers             []string `hcl:\"readers\"`\n    Contributors        []string `hcl:\"contributors\"`\n\n}\n\ntype Config struct {\n    Environment string  `hcl:\"environment\"`\n    BusinessEntity *string `hcl:\"business_entity\"`\n    EntitlementLookup map[string]string `hcl:\"entitlement_lookup\"`\n    RawDatabases []RawDatabase `hcl:\"raw_databases\"`\n\n}\n\nfunc readFile(filePath string) ([]byte, error) {\n    // Open the file for reading\n    file, err := os.Open(filePath)\n    if err != nil {\n        return nil, err\n    }\n    defer file.Close()\n\n    // Read the file content into a byte slice\n    content, err := io.ReadAll(file)\n    if err != nil {\n        return nil, err\n    }\n\n    return content, nil\n}\n\nfunc writeFile(data []byte, filePath string) ( error) {\n    // Open the file for reading\n    file, err := os.OpenFile(filePath, os.O_WRONLY|os.O_CREATE, 0644)\n    if err != nil {\n        return err\n    }\n    defer file.Close()\n\n    // Read the file content into a byte slice\n    _, err = file.Write(data)\n    if err != nil {\n        return err\n    }\n\n    return nil\n}\n\nfunc main() {\n    // Read the tfvar file content\n    content, err := readFile(\"demo.tfvars\")\n    if err != nil {\n        log.Fatalf(\"Error reading tfvars file: %v\", err)\n    }\n\n    // Parse tfvars content\n    parsedObject, diags := hclsyntax.ParseConfig(content, \"\", hcl.Pos{})\n    if diags.HasErrors() {\n        log.Fatalf(\"Failed to parse tfvars content: %s\", diags)\n    }\n// Print parsedObject.Body to debug\nfmt.Println(\"Parsed Body:\", parsedObject.Body)\n    // Decode HCL into Go struct\n    var config Config\n    diags = gohcl.DecodeBody(parsedObject.Body, nil, &config)\n    if diags.HasErrors() {\n        log.Fatalf(\"Failed to decode tfvars content: %s\", diags)\n    }\n\n    // Print Go struct\n    fmt.Printf(\"%+v\\n\", config)\n\n    // Convert Go struct back to HCL\n    f := hclwrite.NewEmptyFile()\n    gohcl.EncodeIntoBody(&config, f.Body())\n\n    // Write HCL content to a file\n    err = writeFile(f.Bytes(), \"../../../terraform/project1/dev_out.tfvars\")\n    if err != nil {\n        log.Fatalf(\"Failed to write HCL to file: %v\", err)\n    }\n}\n`\nand the `demo.tfvars` file content is\n```\n`environment = \"dev\"\n\nentitlement_lookup = {\n  \"obj1\"     = \"obj_id1\",\n  \"obj2\"     = \"obj_id2\",\n}\n\nraw_databases = [\n  {\n    name = \"db1\"\n    business_vertical_id = \"fin\"\n    readers = []\n    contributors = [\"obj1\", \"obj2\"]\n  },\n  {\n    name = \"db2\"\n    business_vertical_id = \"fin\"\n    readers = []\n    contributors = [\"obj2\"]\n  },\n  {\n    name = \"db3\"\n    business_vertical_id = \"fin\"\n    readers = []\n    contributors = [\"obj1\"]\n  }\n]\n\n`\n```\nMy code is working fine if i try to the above code without raw_database.\nHelp me to solve the error\n`panic: unsuitable DecodeExpression target: no cty.Type for main.RawDatabase (no cty field tags)\n\ngoroutine 1 [running]:\ngithub.com/hashicorp/hcl/v2/gohcl.DecodeExpression({0x7fdab078cc68, 0xc0000fa8c0}, 0x5f54e0?, {0xc0000bbc80, 0xc0000bb9a0})\n    /go/pkg/mod/github.com/hashicorp/hcl/v2@v2.20.1/gohcl/decode.go:296 +0x7f4\ngithub.com/hashicorp/hcl/v2/gohcl.decodeBodyToStruct({0x68cb58, 0xc000136420}, 0x0, {0x615b80?, 0xc0000bb980?, 0xc00019fe00?})\n    /go/pkg/mod/github.com/hashicorp/hcl/v2@v2.20.1/gohcl/decode.go:127 +0xd38\ngithub.com/hashicorp/hcl/v2/gohcl.decodeBodyToValue({0x68cb58, 0xc000136420}, 0x0, {0x615b80?, 0xc0000bb980?, 0xc00019fe78?})\n    /go/pkg/mod/github.com/hashicorp/hcl/v2@v2.20.1/gohcl/decode.go:46 +0xba\ngithub.com/hashicorp/hcl/v2/gohcl.DecodeBody({0x68cb58, 0xc000136420}, 0x0, {0x5f2060?, 0xc0000bb980?})\n    /go/pkg/mod/github.com/hashicorp/hcl/v2@v2.20.1/gohcl/decode.go:39 +0xc5\nmain.main()\n    /workspaces/cdassp/go/cmd/go_hcl/main.go:106 +0x1bd\nexit status 2\n`\n\nI have verified below\n\nDouble-check tag placement: Ensure each hcl:\"...\" tag is placed directly above the corresponding struct field. Incorrect placement can lead to the error.\n\nVerify tag syntax: Make sure the tag format is correct: hcl:\"key_name\". The key name should match the exact name used in your HCL configuration file for that field.\n\nDoes this mean the struct objects can not be parsed with HCL?",
      "solution": "When using the `gohcl` abstraction to declare HCL schema using Go struct tags, the HCL tags can only represent concepts that can map to HCL's \"body schema\" model. That means:\n\nA set of attribute names, some of which are required.\nA set of nested block type names, each of which requires zero or more labels.\n\nHCL itself does not model attribute types or values. Instead, it delegates that to an upstream library called `cty`. You can see that in HCL's low-level API in that `hcl.Expression`'s `Value` method returns `cty.Value`, not an HCL-specific type. (Disclosure: I am the primary author and maintainer of `cty`.)\nTo support the `gohcl` abstraction, HCL delegates attribute value decoding to `cty`'s corresponding package `gocty`, which has its own rules for mapping `cty.Value` to \"normal\" Go types, including its own struct tags for describing object types.\nThat means that if you want to decode an object-typed value into an instance of a Go struct type then you'll need to declare that struct type with `gocty`'s struct tags, rather than `gohcl`'s struct tags. `gohcl`'s struct tags are only for decoding HCL's own concepts: attributes and nested blocks.\nThe following pair of types should achieve the effect you wanted:\n```\n`type RawDatabase struct {\n    Name                string   `cty:\"name\"`\n    BusinessVerticalID  string   `cty:\"business_vertical_id\"`\n    Readers             []string `cty:\"readers\"`\n    Contributors        []string `cty:\"contributors\"`\n}\n\ntype Config struct {\n    Environment       string            `hcl:\"environment\"`\n    BusinessEntity    *string           `hcl:\"business_entity\"`\n    EntitlementLookup map[string]string `hcl:\"entitlement_lookup\"`\n    RawDatabases      []RawDatabase     `hcl:\"raw_databases\"`\n}\n`\n```\nNotice that `RawDatabase` now has `cty:` struct tags, instead of `hcl:` struct tags. The tags in `Config` tell `gohcl` which attributes to request in the `hcl.BodySchema` object it generates. The type of `RawDatabases`, and the field tags in `RawDatabase`, tell `gocty` to expect a list of objects that each have the four attributes you specified.\n\nTerraform itself does not use the `gohcl` abstraction to implement its `.tfvars` format. Instead, it works directly with the low-level HCL API.\nTerraform's interpretation of such a file works roughly like this:\n```\n`    file, diags := hclsyntax.ParseConfig([]byte(src), \"filename.tfvars\", hcl.InitialPos)\n    if diags.HasErrors() {\n        // (handle the errors)\n    }\n\n    attrs, diags := file.Body.JustAttributes()\n    if diags.HasErrors() {\n        // (handle the errors)\n    }\n\n    vals := make(map[string]cty.Value, len(attrs))\n    for name, attr := range attrs {\n        vals[name], diags = attr.Expr.Value(nil)\n        if diags.HasErrors() {\n            // (handle the errors)\n        }\n    }\n`\n```\nThe result in `vals` is a map with an element for each of the defined variables, where each one is represented as `cty.Value`.\nTerraform never needs to translate those values into Go string, slice, or struct types because it does all of its work using the `cty.Value` API, because that represents the Terraform language type system.\nHowever, if you need to do that for your own purposes then you can still use `gocty` with the values if you like, passing each value into `gocty.FromCtyValue` with an appropriate target type.\nHowever, if your goals allow for your interpretation of the file to be stricter than Terraform would be then your original approach of using `gohcl` along with `gocty`, with the modifications I proposed above, are a reasonable shortcut. I mention this other detail just to avoid presenting the misleading impression that Terraform's `.tfvars` format is decoded using the `gohcl` helper. (`gohcl` is intended for applications with simpler needs than Terraform.)",
      "question_score": 1,
      "answer_score": 3,
      "created_at": "2024-05-15T19:40:49",
      "url": "https://stackoverflow.com/questions/78485634/parsing-terraform-tfvars-file-with-golang"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "hcl",
      "question_id": 76088198,
      "title": "Use S3 Object tag from terraform",
      "problem": "I have a split code deploy / infrastructure deploy that I'm working with. I currently have it working by uploading a separate file containing the codes' sha256 code_hash like so:\n```\n`data \"archive_file\" \"source\" {\n  type        = \"zip\"\n  source_file = \"../../code/lambda.py\"\n  output_path = \"../../code/lambda.zip\"\n}\n\nresource \"local_file\" \"source_hash\" {\n  content  = data.archive_file.source.output_base64sha256\n  filename = \"../../code/source_code_hash\"\n}\n\nresource \"aws_s3_object\" \"file_upload\" {\n  bucket = \"deployment_bucket\"\n  key    = \"deployment_folder/lambda.zip\"\n  source = data.archive_file.source.output_path\n  tags   = { sha256 = \"${data.archive_file.source.output_base64sha256}\" }\n}\n\nresource \"aws_s3_object\" \"source_hash_upload\" {\n  bucket = \"deployment_bucket\"\n  key    = \"deployment_folder/source_code_hash\"\n  source = resource.local_file.source_hash.filename\n}\n`\n```\nThis is fine but I feel like I can use the tag I've attached to the s3 bucket to skip the local file creation, uploading and later downloading.\nI can't find anything about reading tags from s3 objects from terraform, is it possible to implement something like the following?\n```\n`data \"aws_s3_object\" \"source_code\" {\n  bucket = \"deployment_bucket\"\n  key    = \"deployment_folder/lambda.zip\"\n}\n\nresource \"aws_lambda_function\" \"relaysecret\" {\n  function_name = \"lambda-function\"\n\n  s3_bucket        = \"deployment_bucket\"\n  s3_key           = \"deployment_folder/lambda.zip\"\n  source_code_hash = chomp(data.aws_s3_object.source_code.tag.sha256)\n}\n`\n```",
      "solution": "`aws_s3_object` has tags attribute (not `tag` as you have), so you can get the `sha256` as follows:\n```\n`source_code_hash = chomp(data.aws_s3_object.source_code.tags[\"sha256\"])\n`\n```",
      "question_score": 1,
      "answer_score": 3,
      "created_at": "2023-04-24T04:14:10",
      "url": "https://stackoverflow.com/questions/76088198/use-s3-object-tag-from-terraform"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "hcl",
      "question_id": 74723928,
      "title": "TERRAFORM: Modify Values for ZipMap while Transforming Map to new Keys",
      "problem": "I thought this would be easy :frowning:\nGoal is to transform this map:\n```\n````\naccounts = {\n  \"acct-key-1\" = {\n    \"billingcode\" = \"sys\"\n    \"future-key\" = \"SOME-UNIQUE-VALUE-1\"\n    \"private-attribute-1\" = \"fee\"\n    \"private-attribute-2\" = \"foe\"\n  }\n  \"acct-key-2\" = {\n    \"billingcode\" = \"sys\"\n    \"future-key\" = \"SOME-UNIQUE-VALUE-2\"\n    \"private-attribute-1\" = \"fie\"\n    \"private-attribute-2\" = \"fum\"\n  }\n}\n```  \n`\n```\ninto this map:\n```\n````\ngoodness = {\n  \"SOME-UNIQUE-VALUE-1\" = {\n      \"billingcode\" = \"sys\"\n      \"acct-key\" = \"acct-key-1\" \n  }\n  \"SOME-UNIQUE-VALUE-2\" = {\n      \"billingcode\" = \"sys\"\n      \"acct-key\" = \"acct-key-2\"\n  }\n}\n```\n`\n```\nAs you can see, there are three tasks going on:\n\nCreate the new map using a guaranteed-unique attribute (future-key) as the key for the new map.  That's not a problem, see code below.\nInsert the old key as an attribute into the new map\nRemove some named attributes (private-attribute-n) from the new map\n\nPer code below, ZipMap seems to be the way to go partway there.  Using ZipMap I get a partial result like this:\n```\n````\npartial = {\n  \"SOME-UNIQUE-VALUE-1\" = {\n    \"billingcode\" = \"sys\"\n    \"future-key\" = \"SOME-UNIQUE-VALUE-1\"\n    \"private-attribute-1\" = \"fee\"\n    \"private-attribute-2\" = \"foe\"\n  }\n  \"SOME-UNIQUE-VALUE-2\" = {\n    \"billingcode\" = \"sys\"\n    \"future-key\" = \"SOME-UNIQUE-VALUE-2\"\n    \"private-attribute-1\" = \"fie\"\n    \"private-attribute-2\" = \"fum\"\n  }\n}\n```\n`\n```\nwith a few things wrong with it:\n\nold key not inserted as an attribute\nprivate attributes not removed\nold key still present.  Don't really care about that one!\n\nIt seems that \"all\" I need to do is to modify `local.newvalues` to:\n\nInsert the old key as an attribute, and\nRemove the unwanted values\n\nAnd I've tried nearly every variant of nested for..in loops I could think of and find on the web, with no success at all, not even enough to show.\nThe problem seems to be that `local.newvalues` is a tuple, and methods to modify tuples are few and far between.\nThe logic I've tried to implement would go like:\nfor  each object in local.newvalues\nget the corresponding old key from keys(accts) and insert it\nlook at other attributes and skip or remove them if they match one of the private keys\nThe code is simple and goes like this:\n```\n````\nlocals {\n    # get source map\n    accts = jsondecode(file(\"${path.module}/question.json\"))\n    newkeys = values(local.accts)[*].future-key\n    newvalues = values(local.accts)\n    partial = zipmap(\n      local.newkeys, local.newvalues\n    )\n}\n\noutput \"accounts\" {\n    value = local.accts\n}\n\noutput \"newkeys\" {\n    value = local.newkeys\n}\n\noutput \"newvalues\" {\n    value = local.newvalues\n}\n\noutput \"partial\" {\n    value = local.partial\n}```\n`\n```\nAnd the unfinished output like this:\n```\n````\nOutputs:\n\naccounts = {\n  \"acct-key-1\" = {\n    \"billingcode\" = \"sys\"\n    \"future-key\" = \"SOME-UNIQUE-VALUE-1\"\n    \"private-attribute-1\" = \"fee\"\n    \"private-attribute-2\" = \"foe\"\n  }\n  \"acct-key-2\" = {\n    \"billingcode\" = \"sys\"\n    \"future-key\" = \"SOME-UNIQUE-VALUE-2\"\n    \"private-attribute-1\" = \"fie\"\n    \"private-attribute-2\" = \"fum\"\n  }\n}\nnewkeys = [\n  \"SOME-UNIQUE-VALUE-1\",\n  \"SOME-UNIQUE-VALUE-2\",\n]\nnewvalues = [\n  {\n    \"billingcode\" = \"sys\"\n    \"future-key\" = \"SOME-UNIQUE-VALUE-1\"\n    \"private-attribute-1\" = \"fee\"\n    \"private-attribute-2\" = \"foe\"\n  },\n  {\n    \"billingcode\" = \"sys\"\n    \"future-key\" = \"SOME-UNIQUE-VALUE-2\"\n    \"private-attribute-1\" = \"fie\"\n    \"private-attribute-2\" = \"fum\"\n  },\n]\npartial = {\n  \"SOME-UNIQUE-VALUE-1\" = {\n    \"billingcode\" = \"sys\"\n    \"future-key\" = \"SOME-UNIQUE-VALUE-1\"\n    \"private-attribute-1\" = \"fee\"\n    \"private-attribute-2\" = \"foe\"\n  }\n  \"SOME-UNIQUE-VALUE-2\" = {\n    \"billingcode\" = \"sys\"\n    \"future-key\" = \"SOME-UNIQUE-VALUE-2\"\n    \"private-attribute-1\" = \"fie\"\n    \"private-attribute-2\" = \"fum\"\n  }\n}\n`\n```\nTo make code easy to reproduce, I put the initial map in a json file that the code reads and decodes; here it is as question.json:\n```\n````\n{\n    \"acct-key-1\": {\n        \"future-key\": \"SOME-UNIQUE-VALUE-1\",\n        \"billingcode\": \"sys\",\n        \"private-attribute-1\": \"fee\",\n        \"private-attribute-2\": \"foe\"\n    },\n    \"acct-key-2\": {\n        \"future-key\": \"SOME-UNIQUE-VALUE-2\",\n        \"billingcode\": \"sys\",\n        \"private-attribute-1\": \"fie\",\n        \"private-attribute-2\": \"fum\"\n    }  \n}\n```\n`\n```\nIdeas much appreciated",
      "solution": "You do not need zip for that. Just a single for loop is enough:\n```\n`\nlocals {\n  goodness = {\n    for acc_key, acc_details in local.accts:\n      acc_details.future-key => {\n        acct-key = acc_key\n        billingcode = acc_details.billingcode\n      }\n  }\n}\n`\n```\nwhich gives:\n```\n`{\n  \"SOME-UNIQUE-VALUE-1\" = {\n    \"acct-key\" = \"acct-key-1\"\n    \"billingcode\" = \"sys\"\n  }\n  \"SOME-UNIQUE-VALUE-2\" = {\n    \"acct-key\" = \"acct-key-2\"\n    \"billingcode\" = \"sys\"\n  }\n}\n`\n```",
      "question_score": 1,
      "answer_score": 3,
      "created_at": "2022-12-08T00:25:37",
      "url": "https://stackoverflow.com/questions/74723928/terraform-modify-values-for-zipmap-while-transforming-map-to-new-keys"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "hcl",
      "question_id": 74350957,
      "title": "Get the type of value using cty in hclwrite",
      "problem": "am looking for a way to find the type of variable using go-cty package in hclwrite.\nMy aim is to generate a variables file like below\n```\n`variable \"test_var\" {\n  val1 = bool\n  val2 = string\n  val3 = number\n}\n`\n```\nreference: https://developer.hashicorp.com/terraform/language/values/variables\nI am using the below code to generate this.\n```\n`    vars := hclwrite.NewEmptyFile()\n    vars_root_body := vars.Body()\n    vars_file, vars_create_err := os.Create(\"variables.tf\")\n    logErrors(vars_create_err)\n    vars_block := vars_root_body.AppendNewBlock(\"variable\",[]string{\"test_var\"})\n    vars_block_body := vars_block.Body()\n\n    vars_block_body.SetAttributeValue(\"val\", cty.Value{})\n\n    _, vars_write_err := vars_file.Write(vars.Bytes())\n    logErrors(vars_write_err)\n    defer vars_file.Close()\n`\n```\nthe above code generates this\n```\n`variable \"test_var\" {\n  val = null\n}\n`\n```\nI want to fetch the type of that variable and set the attribute value based on that type, as show in the reference link above. I tried lot of ways but didn't get anything. Can someone please help me on this?\nI tried the above code and lot of other ways like\n```\n`cty.SetValEmpty(cty.Bool)\n`\n```\nbut it didn't work.",
      "solution": "The expected syntax for a `variable` block in Terraform includes an argument named `type`, not an argument named `val`. From your example I assume that you are intending to populate `type`.\nThe type constraint syntax that Terraform uses is not directly part of HCL and so there isn't any built-in way to generate that syntax in only one step. However, type constraint are built from HCL's identifier and function call syntaxes, and `hclwrite` does have some functions for helping to generate those as individual parts:\n\n`TokensForIdentifier`\n`TokensForFunctionCall`\n\n```\n`    f := hclwrite.NewEmptyFile()\n    rootBody := f.Body()\n    varBlock := rootBody.AppendNewBlock(\"variable\", []string{\"example\"})\n    varBody := varBlock.Body()\n    varBody.SetAttributeRaw(\n        \"type\",\n        hclwrite.TokensForFunctionCall(\n            \"set\",\n            hclwrite.TokensForIdentifier(\"string\"),\n        ),\n    )\n    fmt.Printf(\"%s\", f.Bytes())\n`\n```\nThe above will generate the following:\n```\n`variable \"example\" {\n  type = set(string)\n}\n`\n```\nIf you already have a `cty.Value` value then you can obtain its type using the `Type` method. However, as mentioned above there isn't any ready-to-use function for converting a type into a type expression, so if you want to be able to generate a type constraint for any value then you'd need to write a function for this yourself, wrapping the `TokensForFunctionCall` and `TokensForIdentifier` functions. For example:\n```\n`package main\n\nimport (\n    \"fmt\"\n    \"sort\"\n\n    \"github.com/hashicorp/hcl/v2/hclwrite\"\n    \"github.com/zclconf/go-cty/cty\"\n)\n\nfunc main() {\n    f := hclwrite.NewEmptyFile()\n    rootBody := f.Body()\n    varBlock := rootBody.AppendNewBlock(\"variable\", []string{\"example\"})\n    varBody := varBlock.Body()\n    varBody.SetAttributeRaw(\n        \"type\",\n        typeExprTokens(cty.Set(cty.String)),\n    )\n    fmt.Printf(\"%s\", f.Bytes())\n}\n\nfunc typeExprTokens(ty cty.Type) hclwrite.Tokens {\n    switch ty {\n    case cty.String:\n        return hclwrite.TokensForIdentifier(\"string\")\n    case cty.Bool:\n        return hclwrite.TokensForIdentifier(\"bool\")\n    case cty.Number:\n        return hclwrite.TokensForIdentifier(\"number\")\n    case cty.DynamicPseudoType:\n        return hclwrite.TokensForIdentifier(\"any\")\n    }\n\n    if ty.IsCollectionType() {\n        etyTokens := typeExprTokens(ty.ElementType())\n        switch {\n        case ty.IsListType():\n            return hclwrite.TokensForFunctionCall(\"list\", etyTokens)\n        case ty.IsSetType():\n            return hclwrite.TokensForFunctionCall(\"set\", etyTokens)\n        case ty.IsMapType():\n            return hclwrite.TokensForFunctionCall(\"map\", etyTokens)\n        default:\n            // Should never happen because the above is exhaustive\n            panic(\"unsupported collection type\")\n        }\n    }\n\n    if ty.IsObjectType() {\n        atys := ty.AttributeTypes()\n        names := make([]string, 0, len(atys))\n        for name := range atys {\n            names = append(names, name)\n        }\n        sort.Strings(names)\n\n        items := make([]hclwrite.ObjectAttrTokens, len(names))\n        for i, name := range names {\n            items[i] = hclwrite.ObjectAttrTokens{\n                Name:  hclwrite.TokensForIdentifier(name),\n                Value: typeExprTokens(atys[name]),\n            }\n        }\n\n        return hclwrite.TokensForObject(items)\n    }\n\n    if ty.IsTupleType() {\n        etys := ty.TupleElementTypes()\n        items := make([]hclwrite.Tokens, len(etys))\n        for i, ety := range etys {\n            items[i] = typeExprTokens(ety)\n        }\n        return hclwrite.TokensForTuple(items)\n    }\n\n    panic(fmt.Errorf(\"unsupported type %#v\", ty))\n}\n`\n```\nThis program will generate the same output as the previous example. You can change `func main` to pass a different type to `typeExprTokens` to see how it behaves with some different types.",
      "question_score": 1,
      "answer_score": 3,
      "created_at": "2022-11-07T19:00:55",
      "url": "https://stackoverflow.com/questions/74350957/get-the-type-of-value-using-cty-in-hclwrite"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "hcl",
      "question_id": 70739101,
      "title": "Have optional sub-variables in required Terraform variable",
      "problem": "I have created a module corresponding to the Azure Firewall Network Rule Collection. The module looks like this:\n```\n`resource \"azurerm_firewall_network_rule_collection\" \"fwnrc\" {\n  name                = \"fwnrc-${var.name}\"\n  resource_group_name = var.resource_group_name\n\n  azure_firewall_name = var.azure_firewall_name\n  priority            = var.priority\n  action              = var.action\n\n  dynamic \"rule\" {\n    for_each = var.rule != null ? [true] : []\n    content {\n      name                  = var.rule.name\n      description           = var.rule.description\n      source_addresses      = var.rule.source_addresses\n      source_ip_groups      = var.rule.source_ip_groups\n      destination_addresses = var.rule.destination_addresses\n      destination_ip_groups = var.rule.destination_ip_groups\n      destination_fqdns     = var.rule.destination_fqdns\n      destination_ports     = var.rule.destination_ports\n      protocols             = var.rule.protocols\n    }\n  }\n}\n`\n```\nThe section of interest right now is the `dynamic \"rule\"`, which has a corresponding variable defined like this:\n```\n`variable \"rule\" {\n  type = object({\n    name                  = string\n    description           = string\n    source_addresses      = list(string)\n    source_ip_groups      = list(string)\n    destination_addresses = list(string)\n    destination_ip_groups = list(string)\n    destination_fqdns     = list(string)\n    destination_ports     = list(string)\n    protocols             = list(string)\n  })\n}\n`\n```\nI know that it is possible to make the `rule` variable \"Optional\" by setting its default value to `null`. I want to go one step deeper and make the sub-variables* optional/required. For instance, in the resource documentation it is written that one must specify either `*_addresses` or `*_ip_groups`. The docs also says `destination_fqdns` is optional.\n* Is there an actual name for these?\nSince the `rule` variable is required by my module I get an error if I do not give explicit values to all sub-variables. My solution for now is to do the following:\n```\n`module \"firewall_network_rule_collection\" {\n  source = \"/path/to/module\"\n\n  name                = \"fwrc\"\n  azure_firewall_name = \"afw\"\n  resource_group_name = \"rg\"\n  priority            = 110\n  action              = \"Allow\"\n\n  rule = {\n    description = \"rule\"\n    name        = \"rule\"\n\n    source_addresses = [\"*\"]\n    source_ip_groups = null\n\n    destination_ports = [\"*\"]\n    destination_addresses = [\n      \"AzureContainerRegistry\",\n      \"MicrosoftContainerRegistry\",\n      \"AzureActiveDirectory\"\n    ]\n    destination_fqdns     = null\n    destination_ip_groups = null\n\n    protocols = [\"Any\"]\n\n  }\n\n}\n`\n```\nNote the `null` values. Can I get rid of these somehow?\n--\nI am using the following provider settings:\n```\n`terraform {\n  required_version = \">=1.0.11\"\n  required_providers {\n    azurerm = {\n      source  = \"hashicorp/azurerm\"\n      version = \">=2.90.0\"\n    }\n  }\n}\n`\n```",
      "solution": "The experimental \"optional\" object type is currently available if one opts in.\nAdd the following to your module:\n```\n`terraform {\n  experiments = [module_variable_optional_attrs]\n}\n`\n```\nThis allows the following:\n```\n`variable \"rule\" {\n  type = object({\n    name                  = string\n    description           = optional(string)\n    source_addresses      = optional(list(string))\n    source_ip_groups      = optional(list(string))\n    destination_addresses = optional(list(string))\n    destination_ip_groups = optional(list(string))\n    destination_fqdns     = optional(list(string))\n    destination_ports     = optional(list(string))\n    protocols             = optional(list(string))\n  })\n}\n`\n```\nHopefully this feature makes it to a fully supported part of the next release.",
      "question_score": 1,
      "answer_score": 3,
      "created_at": "2022-01-17T10:29:20",
      "url": "https://stackoverflow.com/questions/70739101/have-optional-sub-variables-in-required-terraform-variable"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "hcl",
      "question_id": 78927212,
      "title": "Create a nested (2 level) map in Terraform output",
      "problem": "I have a Terraform module that create EC2 instances \"for me\" grouped by \"cluster\" with the following output (here in a `cluster_module_output` local for easy debug/testing):\n```\n`locals {\n  cluster_module_output = {\n    # API cluster on eu-west-1a:\n    some_unique_index_1 = {\n      cluster_az_name = \"eu-west-1a\"\n      cluster_name = \"api\"\n      instance_names = [\n        \"api-1a-node_1\",\n        \"api-1a-node_2\",\n      ]\n      instances_ids = [\u2026]\n    }\n    # Web cluster on eu-west-1a:\n    some_unique_index_2 = {\n      cluster_az_name = \"eu-west-1a\"\n      cluster_name = \"web\"\n      instance_names = [\n        \"web-1a-node_1\",\n        \"web-1a-node_2\",\n        \"web-1a-node_3\",\n      ]\n      instances_ids = [\u2026]\n    }\n    # API cluster on eu-west-1b:\n    some_unique_index_3 = {\n      cluster_az_name = \"eu-west-1b\"\n      cluster_name = \"api\"\n      instance_names = [\n        \"api-1b-node_1\",\n        \"api-1b-node_2\",\n      ]\n      instances_ids = [\u2026]\n    }\n  }\n}\n`\n```\nIn my Terraform root module I would like to output various informations, such as:\n\nAll instances names (no matter the cluster)\nAll instances names, grouped by cluster and by AZ\n\nFor the former (raw list of instances), I can do:\n```\n`output \"instances_names\" {\n  value = flatten([for cluster in local.cluster_module_output : cluster.instance_names])\n}\n`\n```\nWhich gives:\n```\n`instances_names = [\n  \"api-1a-node_1\",\n  \"api-1a-node_2\",\n  \"api-1b-node_1\",\n  \"api-1b-node_2\",\n  \"web-1a-node_1\",\n  \"web-1a-node_2\",\n  \"web-1a-node_3\",\n]\n`\n```\nBut for the latter (instances grouped by cluster and by AZ) I fail to find an expression.\nDesired output:\n```\n`instances_names_by_cluster = {\n  \"api\" = {\n    \"eu-west-1a\" = [\n      \"api-1a-node_1\",\n      \"api-1a-node_2\",\n    ]\n    \"eu-west-1b\" = [\n      \"api-1b-node_1\",\n      \"api-1b-node_2\",\n    ]\n  }\n  \"web\" = {\n    \"eu-west-1a\" = [\n      \"web-1a-node_1\",\n      \"web-1a-node_2\",\n      \"web-1a-node_3\",\n    ]\n  }\n}\n`\n```\nMy failing attempts are:\n```\n`output \"attempt1\" {\n  value = tomap({\n    for item in (\n      flatten([\n        for cluster in local.cluster_module_output : {\n          cluster_name = cluster.cluster_name\n          cluster_az = cluster.cluster_az_name\n          instance_ids = cluster.instances_ids\n        }\n      ])\n    ) : item.cluster_name => {\n      (item.cluster_az) = item.instance_ids\n    }\n  })\n}\n`\n```\nFails with:\n\nError: Duplicate object key:\nTwo different items produced the key \"api\" in this 'for' expression. If duplicates are expected, use the ellipsis (...) after the value expression to enable grouping by key\n\nObviously I need a way to merge first-level elements in the loop.\nAdding the ellipsis (`...`) like advised:\n```\n`output \"attempt2\" {\n  value = tomap({\n    for item in (\n      flatten([\n        for cluster in local.cluster_module_output : {\n          cluster_name = cluster.cluster_name\n          cluster_az = cluster.cluster_az_name\n          instance_ids = cluster.instances_ids\n        }\n      ])\n    ) : item.cluster_name => {\n      (item.cluster_az) = item.instance_ids\n    }... # ellipsis added here\n  })\n}\n`\n```\nRaises no more error but don't have the desired output:\n```\n`attempt2 = {\n  \"api\" = [\n    {\n      eu-west-1a = [\n        \"api-1a-node_1\",\n        \"api-1a-node_2\",\n      ]\n    },\n    {\n      eu-west-1b = [\n        \"api-1b-node_1\",\n        \"api-1b-node_2\",\n      ]\n    },\n  ]\n  \"web\" = [\n    {\n      eu-west-1a = [\n        \"web-1a-node_1\",\n        \"web-1a-node_2\",\n        \"web-1a-node_3\",\n      ]\n    },\n  ]\n}\n`\n```\nHow can I do this? Do I have to use intermediate locals?",
      "solution": "Your last attempt is very close: you only need to merge a list of objects into a single object. `merge` does exactly that. You have a list of values, `merge` expects separate arguments, so splat comes to rescue.\nHowever, you can also get rid of that convoluted `flatten`:\n`desired_output = {\n    for k, v in {\n        for item in values(local.cluster_module_output):\n        # Group by cluster_name\n        item.cluster_name => {\n            (item.cluster_az_name): item.instance_names\n        }...\n    }: k => merge(v...)\n}\n`\nNote: this expects that `(cluster_name, cluster_az_name)` pairs are unique in your input - there should be no values with both components equal.",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2024-08-29T12:00:27",
      "url": "https://stackoverflow.com/questions/78927212/create-a-nested-2-level-map-in-terraform-output"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "hcl",
      "question_id": 78717482,
      "title": "How to convert (write) hcl files as tfvars?",
      "problem": "We use `Terraformer` to acquire state and `HCL` files, which we then utilize in `Terraform` modules to manage our infrastructure and prevent drift. As `tfvars` files are fed as variables to the module, we need `tfvars` output to automate the entire process. Here is a simple illustration:\n```\n`infrastructure -> terraformer -> (HCl, statefile) -> tfvars -> terraform module -> infrastructure\n`\n```\nSince `Terraformer` does not support outputting `tfvars`, I am attempting to write a Go module to convert `HCL` files (or `statefile`) to `.tfvars` files. Searching through the internet, I came across this Stack Overflow question and solution; however, it was about parsing a `tfvars` file. The accepted answer suggested using the `cty` module as a low-level parser.\nConsidering that, I tried the following:\n```\n`// ...\nparser := hclparse.NewParser()\n// filePath point to a hclfile in JOSON format\nhclFile, diags := parser.ParseJSONFile(filePath)\nattrs, diags := hclFile.Body.JustAttributes()\nif diags.HasErrors() {\n    log.Fatal(diags.Error())\n}\nvals := make(map[string]cty.Value, len(attrs))\n\nfor name, attr := range attrs {\n    vals[name], diags = attr.Expr.Value(nil)\n    if diags.HasErrors() {\n        log.Fatal(diags.Error())\n    }\n}\n`\n```\nUp until now, I managed to parse the HCL to cty types. However, as cty does not support serializing to a tfvars file, I tried writing my own serializer.\nIs this a correct workaround? How could I achieve this goal?\n\nHere is the serializer:\n`func serializeValue(value cty.Value) string {\n    switch {\n    case value.Type().IsPrimitiveType():\n        switch value.Type() {\n        case cty.String:\n            return fmt.Sprintf(\"\\\"%s\\\"\", value.AsString())\n        case cty.Number:\n            return value.AsBigFloat().Text('f', -1)\n        case cty.Bool:\n            return fmt.Sprintf(\"%t\", value.True())\n        }\n    case value.Type().IsListType() || value.Type().IsTupleType():\n        return serializeList(value)\n    case value.Type().IsMapType() || value.Type().IsObjectType():\n        return serializeMapOrObject(value)\n    default:\n        panic(\"Unhandled type\")\n    }\n    return \"\"\n}\n\nfunc serializeMapOrObject(value cty.Value) string {\n    var elements []string\n    for key, val := range value.AsValueMap() {\n        elements = append(elements, fmt.Sprintf(\"\\\"%s\\\" = %s\\n\", key, serializeValue(val)))\n    }\n    return fmt.Sprintf(\"{%s}\", strings.Join(elements, \"\\n\"))\n}\n\nfunc serializeList(value cty.Value) string {\n    var elements []string\n    for _, elem := range value.AsValueSlice() {\n        elements = append(elements, serializeValue(elem))\n    }\n    return fmt.Sprintf(\"[%s]\", strings.Join(elements, \", \"))\n}\n`",
      "solution": "If you have a `cty.Value` that you know is of an object or map type and that all of the attribute/key names are valid HCL identifiers then you can use HCL's `hclwrite` package to generate something that Terraform would accept as the content a `.tfvars` file.\nTerraform's `.tfvars` file format is a relatively-simple application of HCL where the root body is interpreted as \"just attributes\" (in HCL's terminology) and then each attribute has its expression evaluated with no variables or functions available.\nYou can generate such a file like this, with a suitable `cty.Value` stored in variable `obj`:\n```\n`f := hclwrite.NewEmptyFile()\nbody := f.Body()\nfor it := obj.ElementIterator(); it.Next(); {\n    k, v := it.Element()\n    name := k.AsString()\n    body.SetAttributeValue(name, v)\n}\nresult := f.Bytes()\n`\n```\nAfter executing the above, `result` is a `[]byte` containing the content you could write to your `.tfvars` file.\n`cty` does not iterate map elements or object attributes in any predictable order, so the attributes in the result will also be in an unpredictable order. If you want to guarantee the order then you'll need to do something a little more elaborate than this, but I'll leave that as an exercise since the above shows the basic mechanic of generating a HCL file that contains only attributes whose values are constants.",
      "question_score": 1,
      "answer_score": 1,
      "created_at": "2024-07-07T15:52:52",
      "url": "https://stackoverflow.com/questions/78717482/how-to-convert-write-hcl-files-as-tfvars"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "hcl",
      "question_id": 77390356,
      "title": "Packer HCL2 validation returns &quot;There is no function named formatdate&quot;",
      "problem": "I'm trying to assign a variable in a Packer v1.9.4 HCL2 template:\n```\n`variable \"timestamp\" {\n  type = string\n  default = formatdate(\"YYMMDDhhmmss\", timestamp())\n}\n`\n```\nWhen I validate it I get the error in the subject:\n```\n`Error: Call to unknown function\n\n  on C:\\Users\\mathe\\Documents\\Azure Files\\Packer Templates\\avd-packer-win10-proto.pkr.hcl line 19, in variable \"timestamp\":\n19:   default = formatdate(\"YYMMDDhhmmss\", timestamp())\n\nThere is no function named \"formatdate\".\n`\n```\nI'm confused because the Packer documentation specifically calls out the `formatdate` function.\nHas the `formatdate` function been removed or replaced? Am I missing a plugin or something?\nAside, is there a better way to assign a variable like this without using `default`?",
      "solution": "Short answer is you can't call methods for Default values, or any kind of calculation / interpolation to result in the value passed.  Defaults must be constants.  Only local variables allow calculated values.  It's a business rule set by Hashicorp for security reasons, I believe.\nThis thread might help for context:\nhttps://github.com/hashicorp/packer/issues/9430",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2023-10-30T17:26:48",
      "url": "https://stackoverflow.com/questions/77390356/packer-hcl2-validation-returns-there-is-no-function-named-formatdate"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "hcl",
      "question_id": 71037498,
      "title": "Terraform | Change a resource created by a public module",
      "problem": "Let's say that I'm creating an AWS ECS cluster with the help of a public or internal module that I cannot directly change. That module is creating, besides the ECS, let's say a couple of SGs, some alb, maybe an ACM cert, etc. It's all good BUT, for the sake of this problem, I don't like the SGs configs and would like to change them but the developer did not provide a var for that.\nIs there a way for me to reference a particular SG(or any other resource) created by that module and then overwrite/change/replace its config?\nOr do I have to create a PR in the repo of that module to add the necessary vars/changes?",
      "solution": "In Terraform it's the responsibility of a module author to decide the scope of that module and tailor its input variables and output values to meet that scope. There is no way to add additional customization capabilities to a module from the perspective of a caller, because that would then cause your customizations to be effectively a compatibility constraint on future changes to the module which the module author has no awareness of, and therefore cannot predict.\nIf a particular module doesn't meet your needs then your options are, unfortunately, much the same as they are for third-party libraries in most language ecosystems:\n\nSend a feature request to the maintainers (and possibly also a pull request to implement that feature) and see if the maintainers are willing to accept your new requirement into their scope.\nAssuming that the module uses a suitable open source license, use it as the basis for your own module which meets a similar need as the original module but that is extended to meet your specific requirements.",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2022-02-08T17:38:03",
      "url": "https://stackoverflow.com/questions/71037498/terraform-change-a-resource-created-by-a-public-module"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "hcl",
      "question_id": 71016889,
      "title": "Creating multiple SecurityGroups / Rules in AWS using Terraform",
      "problem": "I am trying to create multiple Security Groups and rules within this group at the same time in a module for AWS.\nI have a variable type like this below\n```\n`variable \"security_rules\" {\n\n      type = map(map(object({\n        type        = string\n        description = string\n        from_port   = number\n        to_port     = number\n        protocol    = string\n        cidr_blocks = list(string)\n      })))\n    }\n`\n```\nAnd i am passing the values like this\n```\n`security_rules = {\n  internal_sg = {\n    \"rule1\" = { type = \"ingress\", from_port = 22, to_port = 22, protocol = \"tcp\", cidr_blocks = [\"0.0.0.0/0\"], description = \"For SSH\" },\n    \"rule2\" = { type = \"ingress\", from_port = 22, to_port = 22, protocol = \"tcp\", cidr_blocks = [\"0.0.0.0/0\"], description = \"For SSH\" }\n  external_sg = {\n    \"rule1\" = { type = \"ingress\", from_port = 22, to_port = 22, protocol = \"tcp\"\n  }\n}\n`\n```\nWhere `internal_sg` and `external_sg` would be security group name and there corresponding  rules.\nI am able to create the security groups but failing to add rules on top of that.\n```\n`locals {\n  name = var.security_rules\n}\n\nresource \"aws_security_group\" \"ec2_security_groups\" {\n  for_each = local.name\n  name   = each.key\n  vpc_id = data.aws_vpc.selected.id\n}\n \n`\n```\nBut i am just unable to make the logic for the security group rules\n```\n`resource \"aws_security_group_rule\" \"rules\" {\n  for_each          = { for k, v in local.name : k => v }\n  type              = each.value.type\n  from_port         = each.value.from_port\n  to_port           = each.value.to_port\n  protocol          = each.value.protocol\n  cidr_blocks       = each.value.cidr_blocks\n  description       = each.value.description\n  security_group_id = aws_security_group.ec2_security_groups[each.key]\n}\n`\n```\nError:\n```\n`\u2577\n\u2502 Error: Missing map element\n\u2502 \n\u2502   on ../../terraform-stacks/Stacks/security-group/main.tf line 19, in resource \"aws_security_group_rule\" \"rules\":\n\u2502   19:   type              = each.value.type\n\u2502     \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\u2502     \u2502 each.value is map of object with 3 elements\n\u2502 \n\u2502 This map does not have an element with the key \"type\".\n\u2575\n`\n```",
      "solution": "You have to flatten your variable first:\n```\n`\nlocals {\n  flat_security_rules = merge([\n      for sg, rules in var.security_rules:\n         {\n           for rule, vals in rules:\n             \"${sg}-${rule}\" => merge(vals, {sg_name = sg})\n         }\n    ]...) # please, do NOT remove the dots\n}\n`\n```\nthen\n```\n`resource \"aws_security_group_rule\" \"rules\" {\n  for_each          = local.flat_security_rules\n  type              = each.value.type\n  from_port         = each.value.from_port\n  to_port           = each.value.to_port\n  protocol          = each.value.protocol\n  cidr_blocks       = each.value.cidr_blocks\n  description       = each.value.description\n  security_group_id = aws_security_group.ec2_security_groups[each.value.sg_name].id\n}\n`\n```",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2022-02-07T11:21:49",
      "url": "https://stackoverflow.com/questions/71016889/creating-multiple-securitygroups-rules-in-aws-using-terraform"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "hcl",
      "question_id": 70053276,
      "title": "Create dynamic block in terraform",
      "problem": "I wanted to create `google_compute_health_check` in terraform and I'm thinking about how to make them the most versatile. My code atm looks like that\n`application.hcl`\n`inputs = {\n  health_checks = {\n    tcp-health-check = {\n      name                = \"tcp-health-check\"\n      desc                = \"Health check via tcp\"\n      port                = 80\n      timeout_sec         = 4\n      check_interval_sec  = 30\n    }\n  }\n`\nmain `terragrunt.hcl`\n`include {\n  path = find_in_parent_folders()\n}\n\nterraform { source ...}\n\nlocals {\n  app_vars    = read_terragrunt_config(find_in_parent_folders(\"application.hcl\"))\n}\n\ninputs = {\n  # and my idea was that every invocation of the module, picks it's own set\n  # of health checks that it wants to use\n  health_checks = [local.app_vars.inputs.health_checks.tcp-health-check]\n}\n`\nnow the module `main.tf` looks like so\n`locals {\n  checks = { for check in var.health_checks: check.name => check }\n}\n\nresource \"google_compute_health_check\" \"main\" {\n  for_each           = local.checks\n  name               = each.value.name\n\n  timeout_sec        = each.value.timeout_sec\n  check_interval_sec = each.value.check_interval_sec\n\n  dynamic tcp_health_check {\n    #for_each             = each.value.name == \"tcp_health_check\" ? each.value : []\n    #for_each             = lookup(each.value, \"tcp_health_check\", [])\n    for_each             = contains(keys(each.value), \"tcp_health_check\") != null ? each.value : {}\n      content {\n        port               = 80\n        #        port               = each.value.port\n        #        port_name          = each.value.name\n      }\n  }\n`\nand I'm stuck in the dynamic block - how to make it work so that it only is applied when I pass the `tcp` health_check, and when I pass, `ssh` it creates dynamic `ssh` block (I know there is no ssh block in the code atm, but in future I'll expand the module by whichever healht-check I'll need)\n\nThe errors I get are as followed with `contains`\n`Error: List longer than MaxItems\n\n  on main.tf line 30, in resource \"google_compute_health_check\" \"main\":\n  30: resource \"google_compute_health_check\" \"main\" {\n\nAttribute supports 1 item maximum, config has 7 declared\n\nERRO[0011] 1 error occurred:\n    * exit status 1\n`\nwith `lookup`\n`Error: ExactlyOne\n\n  on main.tf line 30, in resource \"google_compute_health_check\" \"main\":\n  30: resource \"google_compute_health_check\" \"main\" {\n\n\"ssl_health_check\": one of\n`grpc_health_check,http2_health_check,http_health_check,https_health_check,ssl_health_check,tcp_health_check`\nmust be specified\n\nERRO[0005] 1 error occurred:\n    * exit status 1\n`\nand with `==` comparison\n`Error: Inconsistent conditional result types\n\n  on main.tf line 44, in resource \"google_compute_health_check\" \"main\":\n  44:     for_each             = each.value.name == \"tcp_health_check\" ? each.value : []\n    |----------------\n    | each.value is object with 7 attributes\n    | each.value.name is \"tcp-health-check\"\n\nThe true and false result expressions must have consistent types. The given\nexpressions are object and tuple, respectively.\n\nERRO[0005] 1 error occurred:\n    * exit status 1\n`\n\nOk solved it doing it the other way, but thanks for the answer Marcin\n`terragrunt.hcl`\n`  inputs = {\n    name                = \"nat-health-check\"\n    used_for            = \"used for NATs\"\n    check_interval_sec  = 30\n    timeout_sec         = 5\n    healthy_threshold   = 1\n    unhealthy_threshold = 5\n    http_checks         = local.app_vars.inputs.health_checks.nat-http\n  }\n`\nand modules `main.tf`\n`resource \"google_compute_health_check\" \"main\" {\n  name               = var.name\n  timeout_sec        = var.timeout_sec\n  check_interval_sec = var.check_interval_sec\n  description        = \"${var.name} - ${var.used_for}\"\n\n  dynamic \"http_health_check\" {\n    for_each = var.http_checks != null ? [1] : []\n    content {\n      port               = var.http_checks.port\n      request_path       = var.http_checks.request_path\n      port_specification = var.http_checks.port_specification\n    }\n  }\n}\n`",
      "solution": "The following\n```\n`contains(keys(each.value), \"tcp_health_check\") != null ? each.value : {}\n`\n```\nfails because when this is true, your dynamic block `tcp_health_check` will be executed more then once, since your `each.value` is just a map of values. You can't have more then one `tcp_health_check` block.\nThe second\n```\n`for_each             = lookup(each.value, \"tcp_health_check\", [])\n`\n```\ndoes not fail and is correct. But since it results in `false`, your  `tcp_health_check` block is not created. This fails as you are not providing any of the alternative blocks `grpc_health_check,http2_health_check,http_health_check,https_health_check,ssl_health_check,tcp_health_check`.\nThe last attempt:\n```\n`for_each             = each.value.name == \"tcp_health_check\" ? each.value : []\n`\n```\nfails because it should be `for_each             = each.value.name == \"tcp_health_check\" ? each.value : {}`. But if you fix this issue, `each.value` will again fail as before.\nTo sum up, you are always doomed fot fail, since when you eliminate your block `tcp_health_check` as in the second case, you are not providing any alternatives to it. And if your condition is `true`, `each.value` will try to create multiple blocks.\nThe closes solution would be (does not account for false case, where you need to provide alternatives):\n```\n`  dynamic tcp_health_check {\n    for_each           = each.value.name == \"tcp_health_check\" ? [1] : []\n    content {\n         port          = each.value.port\n         port_name     = each.value.name\n      }\n  }\n`\n```",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2021-11-21T10:29:49",
      "url": "https://stackoverflow.com/questions/70053276/create-dynamic-block-in-terraform"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "hcl",
      "question_id": 79766313,
      "title": "How can I get access to the exact image used by packer when I&#39;ve specified an image family for it use",
      "problem": "I'm building GCE images using packer. My hcl contains something like this\n```\n`source \"googlecompute\" \"my-image\" {\n  project_id          = var.project_id\n  ...\n  source_image_family = local.source_image_family\n  image_labels = {\n    \"my_label\" : \"my_value\",\n    \"label2\" : var.label2_value\n  }\n  ...\n}\n`\n```\nI would like to include the exact source image name resolved from the specified family as a label. Something like \"base_image\" but I can't seem to find a way of doing this.\nWhat I tried so far was to try and resolve the image name from the family in a data block and then I can specify that resolved image name to my source as `source_image` and also use it in labels and anywhere else I want. E.G.\n```\n`data \"google_compute_image\" \"base_image\" {\n  family  = var.source_image_family \n  project = var.project_id\n}\n\nsource \"googlecompute\" \"my-image\" {\n  project_id          = var.project_id\n  ...\n  source_image = data.google_compute_image.base_image.self_link\n  image_labels = {\n    \"my_label\" : \"my_value\",\n    \"label2\" : var.label2_value,\n    \"base_image\" : data.google_compute_image.base_image.name\n  }\n  ...\n}\n`\n```\nHowever that appeared to not be the right approach, it seems the googe_compute_image plugin isn't for use in packer and I get this error `executing packer validate Error: Unknown data type google_compute_image`\nFor packer I found https://developer.hashicorp.com/packer/integrations/hashicorp/googlecompute\nbut this doesn't seem to provide any data components.\nI did the obligatory asking of AI tooling but it suggested\n```\n`data \"googlecompute\" \"base_image\" {\n  family  = local.source_image_family\n  project = var.project_id\n}\n`\n```\nwhich when ran just gives me this error `executing packer validate Error: Unknown data type googlecompute`.\nIt suggested I hadn't imported the plugin... but I am successfully using that plugin's builder component (my `source` block) so the plugin is clearly available.\nAm I missing some obvious step/approach or is this just something that isn't possible in packer?\nThanks",
      "solution": "The correct name for the `data` block is `googlecompute-image` as indicated in the documentation.\nAccording to the content in the question, your `data` block should appear like:\n```\n`data \"googlecompute-image\" \"basic-example\" {\n  project_id  = var.project_id\n  filters     = \"family=${var.source_image_family}\"\n  most_recent = true\n}\n`\n```\nNote: you may be under the assumption that separators are generally `_` instead of `-`, and this may be due to experience with Terraform. The Packer ecosystem generally uses `-` instead of `_` for their nomenclature.",
      "question_score": 1,
      "answer_score": 1,
      "created_at": "2025-09-16T15:54:34",
      "url": "https://stackoverflow.com/questions/79766313/how-can-i-get-access-to-the-exact-image-used-by-packer-when-ive-specified-an-im"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "hcl",
      "question_id": 75565234,
      "title": "Terraform how to use for_each for ssh_keys when creating multiple digitalocean droplets",
      "problem": "Background\nI am using the digitalocean_droplet resource. I would like to create multiple digitalocean droplets, each with their own SSH key.\nThe droplets will be created by reading `locals` variable named `droplets` at the top of the script for ease of modification, and each droplet must have its own SSH key. I am using the cloudposse/terraform-tls-ssh-key-pair module to create the key pair.\nProblem\nThe `digitalocean_droplet` resource requires an array of IDs under the `ssh_keys` key. I know how to provide a single key here, but I am using `for_each` for both the module and for the `digitalocean_ssh_key` resource.\nI don't know how to tell the `digitalocean_droplet` resource to, for `ssh_keys`, use multiple values from the `digitalocean_ssh_key` resource.\nTo use a single key, `ssh_keys` would look like this:\n```\n`ssh_keys = [digitalocean_ssh_key.ssh_key.id]\n`\n```\nTo use multiple, I think that I need to tell `ssh_keys` to \"use each of the values generated by the digitalocean_ssh_key resource\". But I don't know how to do this.\nI expect (and hope) this to simply be a lack of knowledge on HCL.\nWhat I have tried\nI have tried what I think should be telling `ssh_keys` to use each `.id` from the `digitalocean_ssh_key` resource:\n```\n`ssh_keys = {\n  for index, v in digitalocean_ssh_key.ssh_key:\n    v => v.id\n}\n`\n```\nHowever this results in a cycle error on running `terraform plan`:\n\nError: Cycle: module.ssh_key_pair.output.private_key (expand), module.ssh_key_pair.var.chmod_command (expand), module.ssh_key_pair.null_resource.chmod, module.ssh_key_pair.output.public_key (expand), module.ssh_key_pair.var.private_key_extension (expand) [etc etc]\n\nI have also tried using `module.ssh_key_pair`, but this does not provide me with an `id`, which is what `ssh_keys` requires.\nConclusion\nWhat do I need to do to tell `digitalocean_droplet` to, for each of the configuration in my `locals` variable named \"droplets\", generate a new SSH key using my module and to assign that SSH key to each individual droplet?\nCode\n```\n`locals {\n  // An example of a single droplet. I will add multiple here in the future.\n  droplets = {\n    \"DROPLET_NAME_AS_KEY\" : {\n      image = \"distro image here\"\n      size  = \"droplet size here\"\n    }\n  }\n}\n\n// For each droplet above, create a new digitalocean droplet\nresource \"digitalocean_droplet\" \"droplets\" {\n  for_each = local.droplets\n  name     = each.key\n  region   = \"fra1\"\n  image    = each.value.image\n  size     = each.value.size\n  tags     = [each.key]\n  // Next is my problem, what goes for ssh_keys??\n  ssh_keys = ???\n}\n\n// Create a new key pair for each droplet.\nmodule \"ssh_key_pair\" {\n  for_each = digitalocean_droplet.droplets\n\n  source              = \"git::https://github.com/cloudposse/terraform-tls-ssh-key-pair.git?ref=master\"\n  ssh_public_key_path = \"/users/me/.ssh\"\n  name                = \"${each.value.name}\"\n}\n\n// The link between the key pair resource and the ssh_key for digital ocean.\nresource \"digitalocean_ssh_key\" \"ssh_key\" {\n  for_each = module.ssh_key_pair\n\n  name       = each.value.key_name\n  public_key = each.value.public_key\n}\n`\n```\nBonus question if the above gets solved:\nI actually use variables for all the values in the `locals` array. These come from environment variables from docker-compose, as I'm running terraform in a container. Is there a simple way to provide an array in docker-compose which can become this array here without me having to update both the `environment` key in docker-compose.yml and this droplets array, and instead just update an array in docker-compose.yml?",
      "solution": "You should use `local.droplets` in all three `for_each` loops to avoid cycles.\nYou can then access `module.ssh_key_pair` module in `digitalocean_ssh_key` resource with `module.ssh_key_pair[each.key]`.\nThe `ssh_keys` field in `digitalocean_droplet` resource would be `ssh_keys = [digitalocean_ssh_key.ssh_key[each.key].fingerprint]`.\n\nRegarding your bonus question - you could declare this variable:\n```\n`variable \"droplets\" {\n  type = map(object({\n    image = string\n    size = string\n  }))\n}\n`\n```\nAnd then in `for_each` use `var.droplets` instead of `locals.droplets`.\nYou can use env variables to set Terraform variables, however:\n\nFor readability, and to avoid the need to worry about shell escaping,\nwe recommend always setting complex variable values via variable\ndefinitions files.\n\nSee details here - https://developer.hashicorp.com/terraform/language/values/variables#assigning-values-to-root-module-variables.",
      "question_score": 1,
      "answer_score": 1,
      "created_at": "2023-02-25T12:55:49",
      "url": "https://stackoverflow.com/questions/75565234/terraform-how-to-use-for-each-for-ssh-keys-when-creating-multiple-digitalocean-d"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "hcl",
      "question_id": 75353050,
      "title": "Applying varying lifecycle policies to list of s3 buckets",
      "problem": "I have a list of s3 buckets on which i want to apply different lifecycle policies.\nvariables.tf\n```\n`variable \"bucket_name\" {    \n    type    = list(any)    \n    default = [\"in\", \"out\", \"in-archive\", \"out-archive\"]  \n}\n`\n```\nFor the first 2 items in the list I want to have their contents deleted after 180 days. And the remaining 2 buckets to move their contents to GLACIER class and then remove them after 600 days.\nI have declared two different resource blocks for varying policies, but the problem is how do I make terraform to start counting index from 3rd element instead of 1st element.\nresource block\n```\n`resource \"aws_s3_bucket\" \"bucket\" {\n    count  = length(var.bucket_name)\n    bucket = \"${var.bucket_name[count.index]}\"\n}\n\nresource \"aws_s3_bucket_lifecycle_configuration\" \"bucket_lifecycle_rule\" {\n    count  = length(aws_s3_bucket.bucket)\n    bucket = aws_s3_bucket.bucket[count.index].id  ///Want this index to stop at 2nd element\n    rule {\n        status = \"Enabled\"\n        id     = \"bucket-lifecycle-rule\"\n        expiration {\n            days = 180\n        }\n    }\n}\n\nresource \"aws_s3_bucket_lifecycle_configuration\" \"archive_bucket_lifecycle_rule\" {\n    count  = length(aws_s3_bucket.bucket)\n    bucket = aws_s3_bucket.bucket[count.index + 4].id   ///Want this index to begin from 3rd and end\n    rule {                                              ///at 4th element\n        status = \"Enabled\"\n        id     = \"archive-bucket-lifecycle-rule\"\n        transition {\n            days          = 181\n            storage_class = \"GLACIER\"\n        }\n        expiration {\n            days = 600\n        }\n    }\n}\n\n`\n```\nWhile I approach this rule, i get an error :\n```\n`in resource \"aws_s3_bucket_lifecycle_configuration\" \"archive_bucket_lifecycle_rule\":\n31:   bucket = aws_s3_bucket.bucket[count.index + 2].id\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\u2502 aws_s3_bucket.bucket is tuple with 4 elements\n\u2502 count.index is 2\n\nThe given key does not identify an element in this collection value.\n`\n```",
      "solution": "How about making the input variable a bit more complex to accommodate what you need...\nHere is a quick example:\n`provider \"aws\" { region = \"us-east-1\" }\n\nvariable \"buckets\" {\n  type = map(any)\n  default = {\n    \"in\" : { expiration : 180, transition : 0 },\n    \"out\" : { expiration : 120, transition : 0 },\n    \"in-archive\" : { expiration : 200, transition : 180 },\n    \"out-archive\" : { expiration : 360, transition : 180 }\n  }\n}\n\nresource \"aws_s3_bucket\" \"bucket\" {\n  for_each = var.buckets\n  bucket   = each.key\n}\n\nresource \"aws_s3_bucket_lifecycle_configuration\" \"lifecycle\" {\n  for_each = var.buckets\n  bucket   = aws_s3_bucket.bucket[each.key].id\n  rule {\n    status = \"Enabled\"\n    id     = \"bucket-lifecycle-rule\"\n    expiration {\n      days = each.value.expiration\n    }\n  }\n  rule {\n    status = each.value.transition > 0 ? \"Enabled\" : \"Disabled\"\n    id     = \"archive-bucket-lifecycle-rule\"\n    transition {\n      days          = each.value.transition\n      storage_class = \"GLACIER\"\n    }\n  }\n}\n\n`\nNow our variable is `type = map(any)` we can create a more complex object there and pass the lifecycle expiration, you can make that as complex as you need to fit more complex rules",
      "question_score": 1,
      "answer_score": 1,
      "created_at": "2023-02-05T15:36:49",
      "url": "https://stackoverflow.com/questions/75353050/applying-varying-lifecycle-policies-to-list-of-s3-buckets"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "hcl",
      "question_id": 74759739,
      "title": "How to pass key of for_each loop in a data source to the resource configuration?",
      "problem": "I have been wanting to create a \"azurerm_virtual_network\" by passing location and name of the resource group attribute using data source: \"azurerm_resource_group\". My data source itself is created using \"for_each\". The idea here is, if I want to provision more VNETs in different groups, I just add more resource group names into the variable list, \"var.network_rg\". However, I am not able to figure out how to pass this list of values as an input to the attributes of vnet resource which is also created using for_each. The loop seems to be taking the value from resource config \"for_each\" instead of data source. Can anyone please help identify what is wrong here ? Is this even the correct way to achieve what I'm trying to do ? Below is my code:\nchild main.tf\n```\n`data \"azurerm_resource_group\" \"network_rg\" {\n  for_each = toset(var.network_rg)\n  name     = \"${var.owner}_${var.env}_${each.key}_${var.location_short}\"\n}\n\nresource \"azurerm_virtual_network\" \"vnet\" {\n  for_each            = var.vnet\n  name                = \"${var.owner}_${var.env}_${each.value[\"name\"]}_${var.location_short}}\"\n  location            = data.azurerm_resource_group.network_rg[each.key].location\n  resource_group_name = data.azurerm_resource_group.network_rg[each.key].name\n  address_space       = each.value[\"address_space\"]\n  lifecycle {\n    ignore_changes = [\n      tags[\"Created\"]\n    ]\n  }\n  tags = {\n    \"Purpose\" = var.purpose_tag #\"Test\"\n    }\n}\n`\n```\nchild variable.tf\n```\n`variable \"owner\" {\n  type        = string\n  description = \"Owner of the resource\"\n}\nvariable \"network_rg\" {\n  type        = list(string)\n  description = \"RG in which VNET is to be created\"\n}\nvariable \"location\" {\n  type        = string\n  description = \"Location in which resource group needs to be created\"\n}\nvariable \"env\" {\n  type        = string\n  description = \"Environment to be deployed in\"\n}\nvariable \"location_short\" {\n  type        = string\n  description = \"Short name for location\"\n}\nvariable \"vnet\" {\n  type = map\n  description = \"Map of VNET attributes\"\n}\nvariable \"purpose_tag\" {\n  type        = string\n  description = \"Purpose Tag\"\n}\n`\n```\n.tfvars\n```\n`owner = \"rrb\"\nlocation = \"australiaeast\"\nenv = \"dev\"\nlocation_short = \"aue\"\npurpose_tag = \"Test\"\n####################### Resource Group Module ####################################\nrg_name = [\"platform\"] #add rg name here to create more rg\n\n###################### Network Module ############################################\n\nnetwork_rg = [\"platform\"]\nvnet = {\n    hub_vnet = {\n        name = \"hub\"\n        address_space = [\"10.200.0.0/16\"]\n    }\n    spoke_vnet = {\n        name = \"spoke\"\n        address_space = [\"10.201.0.0/16\"]\n    }\n}\n`\n```\nRoot main.tf\n```\n`module \"rg\" {\n    source = \"./modules/resourceGroup\"\n    owner = var.owner\n    env  = var.env\n    rg_name = var.rg_name\n    location = var.location\n    location_short = var.location_short\n    purpose_tag = var.purpose_tag\n}\n\nmodule \"network\" {\n    source = \"./modules/network\"\n    network_rg = var.network_rg\n    vnet = var.vnet\n    owner = var.owner\n    env  = var.env\n    location = var.location\n    location_short = var.location_short\n    purpose_tag = var.purpose_tag\n}\n`\n```\nError Message\n```\n`\u2502 Error: Invalid index\n\u2502 \n\u2502   on modules/network/main.tf line 10, in resource \"azurerm_virtual_network\" \"vnet\":\n\u2502   10:   location            = data.azurerm_resource_group.network_rg[each.key].location\n\u2502     \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\u2502     \u2502 data.azurerm_resource_group.network_rg is object with 1 attribute \"platform\"\n\u2502     \u2502 each.key is \"spoke_vnet\"\n\u2502 \n\u2502 The given key does not identify an element in this collection value.\n\u2575\n\u2577\n\u2502 Error: Invalid index\n\u2502 \n\u2502   on modules/network/main.tf line 10, in resource \"azurerm_virtual_network\" \"vnet\":\n\u2502   10:   location            = data.azurerm_resource_group.network_rg[each.key].location\n\u2502     \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\u2502     \u2502 data.azurerm_resource_group.network_rg is object with 1 attribute \"platform\"\n\u2502     \u2502 each.key is \"hub_vnet\"\n\u2502 \n\u2502 The given key does not identify an element in this collection value.\n\n`\n```\nOutput of for_each loop as per below suggestion\n```\n`> {for idx, val in setproduct(keys(var.vnet), var.network_rg): idx => val}\n{\n  \"0\" = [\n    \"hub_vnet\",\n    \"platform\",\n  ]\n  \"1\" = [\n    \"spoke_vnet\",\n    \"platform\",\n  ]\n}\n`\n```",
      "solution": "You have to iterate over vents and network_rgs. Normally this would be done using double for loop, but in you case you could use setproduct as well.\n```\n`resource \"azurerm_virtual_network\" \"vnet\" {\n  for_each            = {for idx, val in setproduct(keys(var.vnet), var.network_rg): idx => val}\n  name                = \"${var.owner}_${var.env}_${var.vnet[each.value[0]].name}_${var.location_short}}\"\n\n  location            = data.azurerm_resource_group.network_rg[each.value[1]].location\n\n  resource_group_name = data.azurerm_resource_group.network_rg[each.value[1]].name\n\n  address_space       = var.vnet[each.value[0]].address_space\n  lifecycle {\n    ignore_changes = [\n      tags[\"Created\"]\n    ]\n  }\n  tags = {\n    \"Purpose\" = var.purpose_tag #\"Test\"\n    }\n}\n`\n```",
      "question_score": 1,
      "answer_score": 1,
      "created_at": "2022-12-11T10:46:47",
      "url": "https://stackoverflow.com/questions/74759739/how-to-pass-key-of-for-each-loop-in-a-data-source-to-the-resource-configuration"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "hcl",
      "question_id": 72075996,
      "title": "How to run TFLint Docker - Passing multiple Args",
      "problem": "Running TFLint through their Docker Image Here - I have to pass tflint multiple commands one after the other to initialize and run the tool which is where I'm running into issues\nI have ran it locally using the following commands, which returns what I want:\n```\n`tflint --init --config 'path/to/config/'\ntflint --config 'path/to/config/config.tflint.hcl' 'path/to/terraform/code'\n\n# AND\n\ntflint --init --config 'path/to/config/config.tflint.hcl' && tflint --config 'path/to/config/' 'path/to/terraform/code'\n`\n```\nHere are the commands I use to run the docker image:\n```\n`docker run -it -v \"$(pwd):/tflint\" ghcr.io/terraform-linters/tflint --init --config '/tflint/path/to/config/config.tflint.hcl'   \n\ndocker run -it -v \"$(pwd):/tflint\" ghcr.io/terraform-linters/tflint --config '/tflint/path/to/config/config.tflint.hcl' '/tflint/path/to/terraform/code'\n\n`\n```\nWhich outputs:\n```\n`Installing `azurerm` plugin...\nInstalled `azurerm` (source: github.com/terraform-linters/tflint-ruleset-azurerm, version: 0.15.0)\nFailed to initialize plugins; Plugin `azurerm` not found. Did you run `tflint --init`?\n\n`\n```\nI know this is creating a new container on each run which is why it's not detecting its been initialized already - My question is how can I reuse this container to pass the extra args it requires after initializing it? Or is there a better way of doing so? Any input/feedback would be appreciated :) Thank you!\nNote: Here is the Dockerfile TFLint uses\n```\n`FROM golang:1.18.1-alpine3.15 as builder\n\nRUN apk add --no-cache make\n\nWORKDIR /tflint\nCOPY . /tflint\nRUN make build\n\nFROM alpine:3.15.4 as prod\n\nLABEL maintainer=terraform-linters\n\nRUN apk add --no-cache ca-certificates\n\nCOPY --from=builder /tflint/dist/tflint /usr/local/bin\n\nENTRYPOINT [\"tflint\"]\nWORKDIR /data\n`\n```",
      "solution": "You can change entrypoint to `sh` and pass multiple commands\n`docker run -it -v \"$(pwd):/tflint\" --entrypoint=/bin/sh ghcr.io/terraform-linters/tflint -c \"tflint --init --config '/tflint/path/to/config/config.tflint.hcl'; tflint --config '/tflint/path/to/config/config.tflint.hcl' '/tflint/path/to/terraform/code'\"`",
      "question_score": 1,
      "answer_score": 1,
      "created_at": "2022-05-01T12:45:33",
      "url": "https://stackoverflow.com/questions/72075996/how-to-run-tflint-docker-passing-multiple-args"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "hcl",
      "question_id": 71566019,
      "title": "Unable to reference an AWS API Gateway Usage Plan as a data source in Terraform",
      "problem": "I have two Terraform projects that are deployed at different times. One of them needs to reference an API Gateway Usage plan in the other project, which should be deployed first. The code looks like this:\n`data \"aws_api_gateway_usage_plan\" \"usage_plan\" {\n  name = \"name-of-usage-plan\"\n}\n`\nHowever, when I execute `terraform plan`, I get the following:\n```\n`data \"aws_api_gateway_usage_plan\" \"usage_plan\" {\n\nThe provider hashicorp/aws does not support data source \"aws_api_gateway_usage_plan\".\n\nDid you intend to use the managed resource type \"aws_api_gateway_usage_plan\"? If so, declare this using a \"resource\" block instead of a \"data\" block.\n`\n```\nI think that if I use a \"resource\" block it will just create a new usage plan, which I don't want to do.\nI'm using terraform 1.1.0 with the aws provider v4.6.0.\nWhat other options do I have? Can I use the AWS CLI to get a reference to the resource? I only need to get its ARN to add it to an IAM statement.",
      "solution": "There is no data source called `aws_api_gateway_usage_plan`. TF does not support that. Please check docs for list of existing data sources.\nYou have to implement your own custom data source to query API stage details.",
      "question_score": 1,
      "answer_score": 1,
      "created_at": "2022-03-22T02:38:02",
      "url": "https://stackoverflow.com/questions/71566019/unable-to-reference-an-aws-api-gateway-usage-plan-as-a-data-source-in-terraform"
    },
    {
      "tech": "terraform",
      "source": "stackoverflow",
      "tag": "hcl",
      "question_id": 68590996,
      "title": "Using a single vars file for all modules",
      "problem": "I want to arrange my resources in separate directories like modules e.g.\n```\n`.\n\u251c\u2500\u2500 alb/\n\u251c\u2500\u2500 networking/\n\u251c\u2500\u2500 variables.tf\n\u2514\u2500\u2500 servers/\n`\n```\nWhen I run the `terraform validate` command from within any of the directory, there is error saying that the variable(s) not declared.\nI don't want to keep variables in every directory. What should I do?",
      "solution": "You don't run `terraform validate` in each of those directories (unless you're doing this from your workstation only). You run it at the top level directory, which has passed the appropriate variables to the `module` blocks.\nGenerally speaking (and best practice) is to publish your modules to their own repos (or some other module hosting, e.g.: tf enterprise) and set up a pipeline to run `terraform validate` as a step/stage/job/whatever. Once it has completed a successful pipeline run, then you can merge and tag it.\nThen you can rest assured that your calling terraform can call those modules, (hopefully by specific versions) knowing there aren't any errors.\nIf you're just learning terraform, then the chances you're doing this on your local workstation seems high - if I were you, I'd do something like `validate.sh`:\n```\n`#!/bin/bash\n\nfor dir in $(ls -d */); do\n    pushd $dir\n    echo -e \"\\n\\n$dir\\n\\n\"\n    terraform validate\n    popd\ndone\n\necho \"*****\"\npwd\necho -e \"\\n\"\nterraform validate\n`\n```",
      "question_score": 1,
      "answer_score": 1,
      "created_at": "2021-07-30T14:30:05",
      "url": "https://stackoverflow.com/questions/68590996/using-a-single-vars-file-for-all-modules"
    }
  ]
}