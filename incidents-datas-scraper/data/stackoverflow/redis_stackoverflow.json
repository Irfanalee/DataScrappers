{
  "tech": "redis",
  "count": 126,
  "examples": [
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 67904609,
      "title": "How do you perform a HEALTHCHECK in the Redis Docker image?",
      "problem": "Recently, we had an outage due to Redis being unable to write to a file system (not sure why it's Amazon EFS) anyway I noted that there was no actual HEALTHCHECK set up for the Docker service to make sure it is running correctly, Redis is up so I can't simply use `nc -z` to check if the port is open.\nIs there a command I can execute in the `redis:6-alpine` (or non-alpine) image that I can put in the `healthcheck` block of the `docker-compose.yml` file.\nNote I am looking for command that is available internally in the image.  Not an external healthcheck.",
      "solution": "Although the `ping` operation from @nitrin0 answer generally works.  It does not handle the case where the write operation will actually fail.  So instead I perform a change that will just increment a value to a key I don't plan to use.\n```\n`image: redis:6\nhealthcheck:\n  test: [ \"CMD\", \"redis-cli\", \"--raw\", \"incr\", \"ping\" ]\n`\n```\nNote this MUST NOT be performed on a cluster that is initialized by Docker.  Since this health check will prevent the cluster from being formed as the Redis are not empty.",
      "question_score": 81,
      "answer_score": 79,
      "created_at": "2021-06-09T14:56:41",
      "url": "https://stackoverflow.com/questions/67904609/how-do-you-perform-a-healthcheck-in-the-redis-docker-image"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 70164076,
      "title": "Celery unable to use redis",
      "problem": "Trying to start Celery first time but issues error as below,\ni have installed redis and its starting fine , but still somehow django seems to have issues with it ,\n```\n`File \"\", line 848, in exec_module\n  File \"\", line 219, in _call_with_frames_removed\n  File \"/home/atif/Documents/celery_test/celery-env/lib/python3.8/site-packages/kombu/transport/redis.py\", line 263, in \n    class PrefixedStrictRedis(GlobalKeyPrefixMixin, redis.Redis):\nAttributeError: 'NoneType' object has no attribute 'Redis'\n`\n```\nCelery.py\n```\n`from django.conf import settings\nos.environ.setdefault('DJANGO_SETTINGS_MODULE', 'celery_test.settings')\n\n    app = Celery('celery_test',)\n    \n    app.config_from_object('django.conf:settings')\n    \n    # Load task modules from all registered Django apps.\n    app.autodiscover_tasks(lambda: settings.INSTALLED_APPS)\n    \n    \n    @app.task(bind=True)\n    def debug_task(self):\n        print(f'Request: {self.request!r}')\n`\n```\nSettings\n```\n`#celery stuff ---------------\nBROKER_URL = 'redis://localhost:6379'\nCELERY_RESULT_BACKEND = 'redis://localhost:6379'\nCELERY_ACCEPT_CONTENT = ['application/json']\nCELERY_TASK_SERIALIZER = 'json'\nCELERY_RESULT_SERIALIZER = 'json'\nCELERY_TIMEZONE = 'Asia/Kolkata'\n`\n```\ncelery_module/tasks.py\n```\n`from celery import Celery\n\napp = Celery('tasks',)\n\n@app.task\ndef add(x, y):\n    return x + y\n`\n```",
      "solution": "Try to install `Redis` as in your virtual environment as well:\n```\n`pip install Redis\n`\n```",
      "question_score": 70,
      "answer_score": 143,
      "created_at": "2021-11-30T04:54:02",
      "url": "https://stackoverflow.com/questions/70164076/celery-unable-to-use-redis"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 66246528,
      "title": "Herkou Redis - certificate verify failed (self signed certificate in certificate chain)",
      "problem": "I have been using heroku redis for a while now on one of my side projects. I currently use it for 3 things\n\nIt serves as a place for me to store firebase certificates\nIt is used for caching data on the site\nIt is used for rails sidekiq jobs\n\nRecently, my heroku usage went up and I had to change it to use heroku redis premium plan. Ever since then I have been getting `error: SSL_connect returned=1 errno=0 state=error: certificate verify failed (self signed certificate in certificate chain)` somehow. Everything stayed the same yet the error started popping out of nowhere.\nDoes heroku-redis premium plan work fundamentally different than a basic heroku-redis plan?\nI am using ruby on rails, deployed on heroku with heroku redis if that helps.",
      "solution": "Actually, when you install the Heroku Redis on your heroku app, it will create for you 2 Config Vars : REDIS_TLS_URL and REDIS_URL.\nThe docs are actually incorrect, you have to set SSL to verify_none because TLS happens automatically.\nFrom Heroku support:\n\n\"Our data infrastructure uses self-signed certificates so certificates\ncan be cycled regularly... you need to set the verify_mode\nconfiguration variable to OpenSSL::SSL::VERIFY_NONE\"",
      "question_score": 34,
      "answer_score": 19,
      "created_at": "2021-02-17T17:57:35",
      "url": "https://stackoverflow.com/questions/66246528/herkou-redis-certificate-verify-failed-self-signed-certificate-in-certificate"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 78380867,
      "title": "docker-compose run issue 2024: Error: &#39;ContainerConfig&#39;",
      "problem": "I have what seems like a very strange issue that I hope someone has hit before.\nI have a docker-compose file that houses a service for redis. Nothing special, I just grab the latest redis from docker hub. I went in to redeploy today and I normally run `--force-recreate` to down/up the containers, but when I attempt to run `--force-recreate` today, I am getting weird errors I have not seen before (and this worked fine yesterday).\nStrangely enough though, running normal down/up commands works and there is no issue. Am I missing something?\nHere are the commands that work to down/up my system without errors:\n`docker-compose -f docker-compose.prod.yml down \ndocker-compose -f docker-compose.prod.yml up -d\n`\nHere is the command that should be fine, but it fails with 'ContainerConfig' errors for redis' docker-compose:\n`docker-compose -f docker-compose.prod.yml up -d --force-recreate\n`\nOutput...\n`docker-compose -f docker-compose.prod.yml up -d --force-recreate\nRecreating app_redis_1 ...\n\nERROR: for app_redis_1  'ContainerConfig'\n\nTraceback (most recent call last):\n  File \"docker-compose\", line 3, in \n  File \"compose/cli/main.py\", line 81, in main\n  File \"compose/cli/main.py\", line 203, in perform_command\n  File \"compose/metrics/decorator.py\", line 18, in wrapper\n  File \"compose/cli/main.py\", line 1186, in up\n  File \"compose/cli/main.py\", line 1182, in up\n  File \"compose/project.py\", line 702, in up\n  File \"compose/parallel.py\", line 108, in parallel_execute\n  File \"compose/parallel.py\", line 206, in producer\n  File \"compose/project.py\", line 688, in do\n  File \"compose/service.py\", line 581, in execute_convergence_plan\n  File \"compose/service.py\", line 503, in _execute_convergence_recreate\n  File \"compose/parallel.py\", line 108, in parallel_execute\n  File \"compose/parallel.py\", line 206, in producer\n  File \"compose/service.py\", line 496, in recreate\n  File \"compose/service.py\", line 615, in recreate_container\n  File \"compose/service.py\", line 334, in create_container\n  File \"compose/service.py\", line 922, in _get_container_create_options\n  File \"compose/service.py\", line 962, in _build_container_volume_options\n  File \"compose/service.py\", line 1549, in merge_volume_bindings\n  File \"compose/service.py\", line 1579, in get_container_data_volumes\nKeyError: 'ContainerConfig'\n[88001] Failed to execute script docker-compose\n`\nHere is the simple `docker-compose` config for the redis service:\n```\n`version: '3.8'\n\nservices:\n\n  redis:\n    image: redis:latest\n    restart: always\n    ports:\n      - \"6379\"\n`\n```",
      "solution": "As noted in countless posts here, and as noted in the comments by @Chris Becke this was a cause of depreciated commands in Docker. It is now 2024 and things have updated.\nFor whatever reason, `--force-recreate` with `docker-compose` now fails on my production system after system updates, while `docker compose up -d --force-recreate` works as expected. (Notice the removal of `-`)\nWeird thing is I host on DigitalOcean and did run updates the other day via `apt-get...` and I did notice docker being updated...but this error was not easy to figure out the cause which is why I asked here. It also hasn't effected my staging env so not sure what the cause for the original commands not working is...",
      "question_score": 30,
      "answer_score": 32,
      "created_at": "2024-04-24T21:41:59",
      "url": "https://stackoverflow.com/questions/78380867/docker-compose-run-issue-2024-error-containerconfig"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 65834575,
      "title": "How to enable TLS for Redis 6 on Sidekiq?",
      "problem": "Problem\nOn my Ruby on Rails app, I keep getting the error below for the Heroku Redis Premium 0 add-on:\n\nOpenSSL::SSL::SSLError: SSL_connect returned=1 errno=0 state=error: certificate verify failed (self signed certificate in certificate chain)\n\nHeroku Redis documentation mentions that I need to enable TLS in my Redis client's configuration in order to connect to a Redis 6 database. To achive this, I have read SSL/TLS Support documentation on redis-rb. My understanding from it is; I need to assign `ca_file`, `cert` and `key` for `Redis.new#ssl_params`. The question is how to set these for Redis or through Sidekiq on Heroku?\nUpdates\nUpdate 3: Heroku support provided an answer which solved the problem.\nUpdate 2: Created Heroku support ticket and waiting response.\nUpdate 1: Asked on Sidekiq's Github issues and was adviced go write Heroku support. Will update this question, when I do get an answer.\n\nRelated Info\nI have verified the app does work when the add-on is either one of the below:\n\nhobby-dev for Redis 6\npremium 0 for Redis 5\n\nVersions:\n\nRuby \u2013 3.0.0p0\nRuby on Rails \u2013 6.1.1\nRedis \u2013 6.0\nredis-rb \u2013 4.2.5\nSidekiq \u2013 6.2.1\nHeroku Stack \u2013 20\n\nSome links that helped me to narrow down the issue:\n\nhttps://bibwild.wordpress.com/2020/11/24/are-you-talking-to-heroku-redis-in-cleartext-or-ssl/\nhttps://mislav.net/2013/07/ruby-openssl/",
      "solution": "Solution\nUse `OpenSSL::SSL::VERIFY_NONE` for your Redis client.\nSidekiq\n`# config/initializers/sidekiq.rb\nSidekiq.configure_server do |config|\n  config.redis = { ssl_params: { verify_mode: OpenSSL::SSL::VERIFY_NONE } }\nend\n\nSidekiq.configure_client do |config|\n  config.redis = { ssl_params: { verify_mode: OpenSSL::SSL::VERIFY_NONE } }\nend\n`\nRedis\n`Redis.new(url: 'url', driver: :ruby, ssl_params: { verify_mode: OpenSSL::SSL::VERIFY_NONE })\n`\nReason\nRedis 6 requires TLS to connect. However, Heroku support explained that they manage requests from the router level to the application level involving Self Signed Certs. Turns out, Heroku terminates SSL at the router level and requests are forwarded from there to the application via HTTP while everything is behind Heroku's Firewall and security measures.\n\nSources\n\nhttps://ogirginc.github.io/en/heroku-redis-ssl-error\nhttps://devcenter.heroku.com/articles/securing-heroku-redis#connecting-directly-to-stunnel",
      "question_score": 28,
      "answer_score": 38,
      "created_at": "2021-01-21T20:36:13",
      "url": "https://stackoverflow.com/questions/65834575/how-to-enable-tls-for-redis-6-on-sidekiq"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 67668213,
      "title": "ModuleNotFoundError: No module named &#39;grp&#39; on windows",
      "problem": "As i was going through the Celery implementation from the Celery documentation,\n```\n`celery -A tasks worker --loglevel=INFO\n`\n```\ngave output unexpected that from the documentation,\n\nFile\n\"d:\\101_all_projects\\celery-testing\\venv\\lib\\site-packages\\celery\\platforms.py\",\nline 9, in \nimport grp ModuleNotFoundError: No module named 'grp'\n\nIs this because i am on windows?",
      "solution": "If you're using the PyPi package django-celery-beat it looks like it installs the most recent version of the required package celery rather than installing a compatible version (by the time I'm posting this, 25th of May 2021 this would be v5.1.0), which seems to have compatibility issues with django-celery-beat version 2.2.0 (the most recent) as well as Windows OS.\nI suggest you try\n```\n`pip uninstall celery\npip install celery==5.0.5\n`\n```",
      "question_score": 20,
      "answer_score": 31,
      "created_at": "2021-05-24T09:31:59",
      "url": "https://stackoverflow.com/questions/67668213/modulenotfounderror-no-module-named-grp-on-windows"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 73400719,
      "title": "Django + Celery + Redis: kombu.exceptions.OperationalError: [Errno 111] Connection refused",
      "problem": "Although celery reports no problems at start and says it successfully connected to redis (see log), I get this error running `celery inspect ping`\n```\n`Traceback (most recent call last):\n  File \"/usr/local/lib/python3.8/site-packages/kombu/connection.py\", line 446, in _reraise_as_library_errors\n    yield\n  File \"/usr/local/lib/python3.8/site-packages/kombu/connection.py\", line 433, in _ensure_connection\n    return retry_over_time(\n  File \"/usr/local/lib/python3.8/site-packages/kombu/utils/functional.py\", line 312, in retry_over_time\n    return fun(*args, **kwargs)\n  File \"/usr/local/lib/python3.8/site-packages/kombu/connection.py\", line 877, in _connection_factory\n    self._connection = self._establish_connection()\n  File \"/usr/local/lib/python3.8/site-packages/kombu/connection.py\", line 812, in _establish_connection\n    conn = self.transport.establish_connection()\n  File \"/usr/local/lib/python3.8/site-packages/kombu/transport/pyamqp.py\", line 201, in establish_connection\n    conn.connect()\n  File \"/usr/local/lib/python3.8/site-packages/amqp/connection.py\", line 323, in connect\n    self.transport.connect()\n  File \"/usr/local/lib/python3.8/site-packages/amqp/transport.py\", line 129, in connect\n    self._connect(self.host, self.port, self.connect_timeout)\n  File \"/usr/local/lib/python3.8/site-packages/amqp/transport.py\", line 184, in _connect\n    self.sock.connect(sa)\nConnectionRefusedError: [Errno 111] Connection refused\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/usr/local/bin/celery\", line 8, in \n    sys.exit(main())\n  File \"/usr/local/lib/python3.8/site-packages/celery/__main__.py\", line 15, in main\n    sys.exit(_main())\n  File \"/usr/local/lib/python3.8/site-packages/celery/bin/celery.py\", line 217, in main\n    return celery(auto_envvar_prefix=\"CELERY\")\n  File \"/usr/local/lib/python3.8/site-packages/click/core.py\", line 1130, in __call__\n    return self.main(*args, **kwargs)\n  File \"/usr/local/lib/python3.8/site-packages/click/core.py\", line 1055, in main\n    rv = self.invoke(ctx)\n  File \"/usr/local/lib/python3.8/site-packages/click/core.py\", line 1657, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"/usr/local/lib/python3.8/site-packages/click/core.py\", line 1404, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"/usr/local/lib/python3.8/site-packages/click/core.py\", line 760, in invoke\n    return __callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.8/site-packages/click/decorators.py\", line 26, in new_func\n    return f(get_current_context(), *args, **kwargs)\n  File \"/usr/local/lib/python3.8/site-packages/celery/bin/base.py\", line 134, in caller\n    return f(ctx, *args, **kwargs)\n  File \"/usr/local/lib/python3.8/site-packages/celery/bin/control.py\", line 136, in inspect\n    replies = inspect._request(action,\n  File \"/usr/local/lib/python3.8/site-packages/celery/app/control.py\", line 106, in _request\n    return self._prepare(self.app.control.broadcast(\n  File \"/usr/local/lib/python3.8/site-packages/celery/app/control.py\", line 741, in broadcast\n    return self.mailbox(conn)._broadcast(\n  File \"/usr/local/lib/python3.8/site-packages/kombu/pidbox.py\", line 328, in _broadcast\n    chan = channel or self.connection.default_channel\n  File \"/usr/local/lib/python3.8/site-packages/kombu/connection.py\", line 895, in default_channel\n    self._ensure_connection(**conn_opts)\n  File \"/usr/local/lib/python3.8/site-packages/kombu/connection.py\", line 433, in _ensure_connection\n    return retry_over_time(\n  File \"/usr/local/lib/python3.8/contextlib.py\", line 131, in __exit__\n    self.gen.throw(type, value, traceback)\n  File \"/usr/local/lib/python3.8/site-packages/kombu/connection.py\", line 450, in _reraise_as_library_errors\n    raise ConnectionError(str(exc)) from exc\nkombu.exceptions.OperationalError: [Errno 111] Connection refused\n`\n```\nIt is a docker-compose app with the redis initiated in docker-compose.yml, so we can see the logs from all containers. Also we can see the celery-beat tasks run successfully, but I want to call task.delay() in one of the views, and there it fails with the same error. Logs:\n```\n`db_1           | \ndb_1           | PostgreSQL Database directory appears to contain a database; Skipping initialization\ndb_1           | \ndb_1           | 2022-08-18 09:16:02.923 UTC [1] LOG:  starting PostgreSQL 13.0 on x86_64-pc-linux-musl, compiled by gcc (Alpine 9.3.0) 9.3.0, 64-bit\ndb_1           | 2022-08-18 09:16:02.923 UTC [1] LOG:  listening on IPv4 address \"0.0.0.0\", port 5432\ndb_1           | 2022-08-18 09:16:02.923 UTC [1] LOG:  listening on IPv6 address \"::\", port 5432\ndb_1           | 2022-08-18 09:16:02.930 UTC [1] LOG:  listening on Unix socket \"/var/run/postgresql/.s.PGSQL.5432\"\nfrontend_1     | /docker-entrypoint.sh: /docker-entrypoint.d/ is not empty, will attempt to perform configuration\nfrontend_1     | /docker-entrypoint.sh: Looking for shell scripts in /docker-entrypoint.d/\nredis_1        | 1:C 18 Aug 2022 09:16:02.869 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo\nredis_1        | 1:C 18 Aug 2022 09:16:02.869 # Redis version=7.0.0, bits=64, commit=00000000, modified=0, pid=1, just started\nredis_1        | 1:C 18 Aug 2022 09:16:02.869 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf\ndb_1           | 2022-08-18 09:16:02.942 UTC [21] LOG:  database system was shut down at 2022-08-18 09:13:45 UTC\nfrontend_1     | /docker-entrypoint.sh: Launching /docker-entrypoint.d/10-listen-on-ipv6-by-default.sh\nredis_1        | 1:M 18 Aug 2022 09:16:02.870 * monotonic clock: POSIX clock_gettime\ndb_1           | 2022-08-18 09:16:02.947 UTC [1] LOG:  database system is ready to accept connections\nfrontend_1     | 10-listen-on-ipv6-by-default.sh: info: /etc/nginx/conf.d/default.conf is not a file or does not exist\nredis_1        | 1:M 18 Aug 2022 09:16:02.871 * Running mode=standalone, port=6379.\nredis_1        | 1:M 18 Aug 2022 09:16:02.871 # Server initialized\nredis_1        | 1:M 18 Aug 2022 09:16:02.871 # WARNING overcommit_memory is set to 0! Background save may fail under low memory condition. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect.\nfrontend_1     | /docker-entrypoint.sh: Launching /docker-entrypoint.d/20-envsubst-on-templates.sh\nredis_1        | 1:M 18 Aug 2022 09:16:02.872 * The AOF directory appendonlydir doesn't exist\nfrontend_1     | /docker-entrypoint.sh: Launching /docker-entrypoint.d/30-tune-worker-processes.sh\nredis_1        | 1:M 18 Aug 2022 09:16:02.872 * Loading RDB produced by version 7.0.0\nredis_1        | 1:M 18 Aug 2022 09:16:02.872 * RDB age 134 seconds\nfrontend_1     | /docker-entrypoint.sh: Configuration complete; ready for start up\nredis_1        | 1:M 18 Aug 2022 09:16:02.872 * RDB memory usage when created 1.18 Mb\nredis_1        | 1:M 18 Aug 2022 09:16:02.872 * Done loading RDB, keys loaded: 2, keys expired: 0.\nredis_1        | 1:M 18 Aug 2022 09:16:02.872 * DB loaded from disk: 0.000 seconds\nfrontend_1     | 2022/08/18 09:16:04 [notice] 1#1: using the \"epoll\" event method\nfrontend_1     | 2022/08/18 09:16:04 [notice] 1#1: nginx/1.22.0\nfrontend_1     | 2022/08/18 09:16:04 [notice] 1#1: built by gcc 11.2.1 20220219 (Alpine 11.2.1_git20220219) \nfrontend_1     | 2022/08/18 09:16:04 [notice] 1#1: OS: Linux 5.15.0-46-generic\nfrontend_1     | 2022/08/18 09:16:04 [notice] 1#1: getrlimit(RLIMIT_NOFILE): 1048576:1048576\nredis_1        | 1:M 18 Aug 2022 09:16:02.872 * Ready to accept connections\nfrontend_1     | 2022/08/18 09:16:04 [notice] 1#1: start worker processes\nfrontend_1     | 2022/08/18 09:16:04 [notice] 1#1: start worker process 22\nfrontend_1     | 2022/08/18 09:16:04 [notice] 1#1: start worker process 23\nfrontend_1     | 2022/08/18 09:16:04 [notice] 1#1: start worker process 24\nfrontend_1     | 2022/08/18 09:16:04 [notice] 1#1: start worker process 25\nfrontend_1     | 2022/08/18 09:16:04 [notice] 1#1: start worker process 26\nfrontend_1     | 2022/08/18 09:16:04 [notice] 1#1: start worker process 27\nfrontend_1     | 2022/08/18 09:16:04 [notice] 1#1: start worker process 28\nfrontend_1     | 2022/08/18 09:16:04 [notice] 1#1: start worker process 29\ncelery-beat_1  | [2022-08-18 09:16:05,012: INFO/MainProcess] beat: Starting...\ncelery_1       |  \ncelery_1       |  -------------- celery@934c4c8d4628 v5.2.7 (dawn-chorus)\ncelery_1       | --- ***** ----- \ncelery_1       | -- ******* ---- Linux-5.15.0-46-generic-x86_64-with-glibc2.2.5 2022-08-18 09:16:05\ncelery_1       | - *** --- * --- \ncelery_1       | - ** ---------- [config]\ncelery_1       | - ** ---------- .> app:         MyFavouriteSite:0x7f2188204a00\ncelery_1       | - ** ---------- .> transport:   redis://redis:6379//\ncelery_1       | - ** ---------- .> results:     redis://redis:6379/\ncelery_1       | - *** --- * --- .> concurrency: 8 (prefork)\ncelery_1       | -- ******* ---- .> task events: OFF (enable -E to monitor tasks in this worker)\ncelery_1       | --- ***** ----- \ncelery_1       |  -------------- [queues]\ncelery_1       |                 .> celery           exchange=celery(direct) key=celery\ncelery_1       |                 \ncelery_1       | \ncelery_1       | [tasks]\ncelery_1       |   . app1.tasks.task1\ncelery_1       |   . app2.tasks.task1\ncelery_1       | \ncelery_1       | [2022-08-18 09:16:05,516: INFO/MainProcess] Connected to redis://redis:6379//\ncelery_1       | [2022-08-18 09:16:05,519: INFO/MainProcess] mingle: searching for neighbors\ncelery_1       | [2022-08-18 09:16:06,530: INFO/MainProcess] mingle: all alone\ncelery_1       | [2022-08-18 09:16:06,614: WARNING/MainProcess] /usr/local/lib/python3.8/site-packages/celery/fixups/django.py:203: UserWarning: Using settings.DEBUG leads to a memory\ncelery_1       |             leak, never use this setting in production environments!\ncelery_1       |   warnings.warn('''Using settings.DEBUG leads to a memory\ncelery_1       | \ncelery_1       | [2022-08-18 09:16:06,614: INFO/MainProcess] celery@934c4c8d4628 ready.\ncelery-beat_1  | [2022-08-18 09:30:00,104: INFO/MainProcess] Scheduler: Sending due task task1_name_in_settings (app1.tasks.task1)\ncelery_1       | [2022-08-18 09:30:00,123: INFO/MainProcess] Task app1.tasks.task1[09c3c690-a379-4a30-a7b4-25cfa57d679a] received\ncelery_1       | [2022-08-18 09:30:00,143: INFO/ForkPoolWorker-7] Task app1.tasks.task1[09c3c690-a379-4a30-a7b4-25cfa57d679a] succeeded in 0.0180537269989145s: None\n`\n```",
      "solution": "I didn't have the following code in `MySiteName/__init__.py` (MySiteName is a folder that also contains settings.py and celery.py)\n```\n`from .celery import app as celery_app\n\n__all__ = ['celery_app']\n`\n```\nAdding it fixed my issue. (celery inspect ping still produces it, probably, because of misconfig, but I can run the task I want in my view)",
      "question_score": 17,
      "answer_score": 27,
      "created_at": "2022-08-18T11:37:44",
      "url": "https://stackoverflow.com/questions/73400719/django-celery-redis-kombu-exceptions-operationalerror-errno-111-connecti"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 71574823,
      "title": "Reusing Redis Connection: Socket Closed Unexpectedly - node-redis",
      "problem": "First, let me tell you how I'm using Redis connection in my NodeJS application:\n\nI'm re-using one connection throughout the app using a singleton class.\n\n```\n`class RDB {\n\n    static async getClient() {\n        if (this.client) {\n            return this.client\n        }\n\n        let startTime = Date.now();\n\n        this.client = createClient({\n            url: config.redis.uri\n        });\n\n        await this.client.connect();\n\n        return this.client;\n    }\n\n}\n`\n```\nFor some reason - that I don't know - time to time my application crashes giving an error without any reason - this happens about once or twice a week:\n```\n`Error: Socket closed unexpectedly\n`\n```\nNow, my questions:\n\nIs using Redis connections like this alright? Is there something wrong with my approach?\nWhy does this happen? Why is my socket closing unexpectedly?\nIs there a way to catch this error (using my approach) or any other good practice for implementing Redis connections?",
      "solution": "I solved this using the 'error' listener. Just listening to it - saves the node application from crashing.\n```\n`client.on(\"error\", function(error) {\n   console.error(error);\n   // I report it onto a logging service like Sentry. \n});\n`\n```",
      "question_score": 15,
      "answer_score": 13,
      "created_at": "2022-03-22T16:34:28",
      "url": "https://stackoverflow.com/questions/71574823/reusing-redis-connection-socket-closed-unexpectedly-node-redis"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 69107860,
      "title": "Celery: what is the reason to have acks_late=true without setting task_reject_on_worker_lost=true",
      "problem": "After playing with some \"defect\" scenarios with celery (Redis being a broker for whatever it worth) we came to understanding that there is effectively no sense in setting `acks_late=true` without simultaneous setting of `task_reject_on_worker_lost=true` because the task won't be rescheduled (again, in our tests) -- task stays in the \"unacked\" category forever.\nAt the same time everybody says that `acks_late` will make the task being subject for rescheduling on the same / another worker, so the question is: when does it happen?\nThe official docs say that\n\nNote that the worker will acknowledge the message if the child process\nexecuting the task is terminated (either by the task calling\nsys.exit(), or by signal) even when acks_late is enabled. This\nbehavior is intentional as\u2026\n\nWe don\u2019t want to rerun tasks that forces the kernel to send a SIGSEGV (segmentation fault) or similar signals to the process.\n\nWe assume that a system administrator deliberately killing the task does not want it to automatically restart.\n\nA task that allocates too much memory is in danger of triggering the kernel OOM killer, the same may happen again.\n\nA task that always fails when redelivered may cause a high-frequency message loop taking down the system.\n\nIf you really want a task to be redelivered in these scenarios you\nshould consider enabling the task_reject_on_worker_lost setting.\n\nWhat are possible examples of \"something went wrong\" that don't fall into the \"worker terminated deliberately or due to a signal caught\" category?",
      "solution": "Reboot, power outage, hardware failure.  n.b., all of your examples assume that the prefetch multiplier is 1.",
      "question_score": 14,
      "answer_score": 1,
      "created_at": "2021-09-08T20:13:03",
      "url": "https://stackoverflow.com/questions/69107860/celery-what-is-the-reason-to-have-acks-late-true-without-setting-task-reject-on"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 69109109,
      "title": "ClassNotFoundException while JedisClient initialization in Spring Boot 2.5.4 application",
      "problem": "I have a Spring Boot 2.5.4 application in which I would like to add Redis and access it via Spring Data Redis. My current configuration looks like this:\n`pom.xml`\n```\n`\n\n  4.0.0\n  \n    org.springframework.boot\n    spring-boot-starter-parent\n    2.5.4\n    \n  \n  com.application\n  ApiGateway\n  0.0.1-SNAPSHOT\n  ApiGateway\n  ApiGateway\n  \n    15\n    2.5.0\n    3.1.0\n  \n  \n\n    \n      org.springframework.boot\n      spring-boot-starter-web\n    \n\n    \n      org.springframework.boot\n      spring-boot-starter-data-jpa\n    \n\n    \n      org.springframework.boot\n      spring-boot-starter-data-redis\n      ${spring-boot-starter-redis.version}\n    \n\n    \n    \n    \n      redis.clients\n      jedis\n      ${redis.version}\n      jar\n    \n\n    \n      com.amazonaws\n      aws-java-sdk-cognitoidp\n      ${aws.sdk.version}\n    \n\n    \n      com.amazonaws\n      aws-java-sdk\n      ${aws.sdk.version}\n    \n\n    \n      com.amazonaws\n      aws-java-sdk-core\n      ${aws.sdk.version}\n    \n\n    \n\n  \n  \n    \n      \n        org.springframework.cloud\n        spring-cloud-dependencies\n        ${spring-cloud.version}\n        pom\n        import\n      \n    \n  \n\n  \n    \n      \n        org.springframework.boot\n        spring-boot-maven-plugin\n        \n          \n            \n              org.projectlombok\n              lombok\n            \n          \n        \n      \n    \n  \n\n`\n```\n`RedisConfiguration.java`\n```\n`@Configuration\n@PropertySource(\"classpath:redis.properties\")\npublic class RedisConfiguration {\n\n  @Value(\"${redis.host}\")\n  private String host;\n\n  @Value(\"${redis.port}\")\n  private int port;\n\n  @Value(\"${redis.database}\")\n  private int database;\n\n  @Value(\"${redis.password}\")\n  private String password;\n\n  @Value(\"${redis.timeout}\")\n  private String timeout;\n\n  @Bean\n  public JedisConnectionFactory jedisConnectionFactory() {\n    RedisStandaloneConfiguration redisConfiguration = new RedisStandaloneConfiguration();\n    redisConfiguration.setHostName(host);\n    redisConfiguration.setPort(port);\n    redisConfiguration.setDatabase(database);\n    redisConfiguration.setPassword(RedisPassword.of(password));\n\n    JedisClientConfigurationBuilder jedisClientConfiguration = JedisClientConfiguration.builder();\n    jedisClientConfiguration.connectTimeout(Duration.ofMillis(Long.parseLong(timeout)));\n\n    return new JedisConnectionFactory(redisConfiguration, jedisClientConfiguration.build());\n  }\n\n  @Bean\n  public RedisTemplate redisTemplate() {\n    RedisTemplate template = new RedisTemplate<>();\n    template.setConnectionFactory(jedisConnectionFactory());\n    return template;\n  }\n}\n`\n```\nfor current configuration while application start I am receiving\n```\n`Caused by: org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.springframework.data.redis.core.RedisTemplate]: Factory method 'redisTemplate' threw exception; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'jedisConnectionFactory' defined in class path resource [com/application/apigateway/intrastructure/cache/RedisConfiguration.class]: Bean instantiation via factory method failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.springframework.data.redis.connection.jedis.JedisConnectionFactory]: Factory method 'jedisConnectionFactory' threw exception; nested exception is java.lang.NoClassDefFoundError: redis/clients/jedis/DefaultJedisClientConfig\n    at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:185)\n    at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:653)\n    ... 33 common frames omitted\nCaused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'jedisConnectionFactory' defined in class path resource [com/application/apigateway/intrastructure/cache/RedisConfiguration.class]: Bean instantiation via factory method failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.springframework.data.redis.connection.jedis.JedisConnectionFactory]: Factory method 'jedisConnectionFactory' threw exception; nested exception is java.lang.NoClassDefFoundError: redis/clients/jedis/DefaultJedisClientConfig\n    at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:658)\n    at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:486)\n    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1334)\n    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1177)\n    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:564)\n    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:524)\n    at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:335)\n    at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)\n    at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:333)\n    at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:208)\n    at org.springframework.context.annotation.ConfigurationClassEnhancer$BeanMethodInterceptor.resolveBeanReference(ConfigurationClassEnhancer.java:362)\n    at org.springframework.context.annotation.ConfigurationClassEnhancer$BeanMethodInterceptor.intercept(ConfigurationClassEnhancer.java:334)\n    at com.application.apigateway.intrastructure.cache.RedisConfiguration$$EnhancerBySpringCGLIB$$63c8a244.jedisConnectionFactory()\n    at com.application.apigateway.intrastructure.cache.RedisConfiguration.redisTemplate(RedisConfiguration.java:51)\n    at com.application.apigateway.intrastructure.cache.RedisConfiguration$$EnhancerBySpringCGLIB$$63c8a244.CGLIB$redisTemplate$0()\n    at com.application.apigateway.intrastructure.cache.RedisConfiguration$$EnhancerBySpringCGLIB$$63c8a244$$FastClassBySpringCGLIB$$c9ce6595.invoke()\n    at org.springframework.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:244)\n    at org.springframework.context.annotation.ConfigurationClassEnhancer$BeanMethodInterceptor.intercept(ConfigurationClassEnhancer.java:331)\n    at com.application.apigateway.intrastructure.cache.RedisConfiguration$$EnhancerBySpringCGLIB$$63c8a244.redisTemplate()\n    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:64)\n    at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.base/java.lang.reflect.Method.invoke(Method.java:564)\n    at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:154)\n    ... 34 common frames omitted\nCaused by: org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.springframework.data.redis.connection.jedis.JedisConnectionFactory]: Factory method 'jedisConnectionFactory' threw exception; nested exception is java.lang.NoClassDefFoundError: redis/clients/jedis/DefaultJedisClientConfig\n    at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:185)\n    at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:653)\n    ... 57 common frames omitted\nCaused by: java.lang.NoClassDefFoundError: redis/clients/jedis/DefaultJedisClientConfig\n    at org.springframework.data.redis.connection.jedis.JedisConnectionFactory.(JedisConnectionFactory.java:97)\n    at org.springframework.data.redis.connection.jedis.JedisConnectionFactory.(JedisConnectionFactory.java:232)\n    at com.application.apigateway.intrastructure.cache.RedisConfiguration.jedisConnectionFactory(RedisConfiguration.java:45)\n    at com.application.apigateway.intrastructure.cache.RedisConfiguration$$EnhancerBySpringCGLIB$$63c8a244.CGLIB$jedisConnectionFactory$1()\n    at com.application.apigateway.intrastructure.cache.RedisConfiguration$$EnhancerBySpringCGLIB$$63c8a244$$FastClassBySpringCGLIB$$c9ce6595.invoke()\n    at org.springframework.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:244)\n    at org.springframework.context.annotation.ConfigurationClassEnhancer$BeanMethodInterceptor.intercept(ConfigurationClassEnhancer.java:331)\n    at com.application.apigateway.intrastructure.cache.RedisConfiguration$$EnhancerBySpringCGLIB$$63c8a244.jedisConnectionFactory()\n    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:64)\n    at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.base/java.lang.reflect.Method.invoke(Method.java:564)\n    at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:154)\n    ... 58 common frames omitted\nCaused by: java.lang.ClassNotFoundException: redis.clients.jedis.DefaultJedisClientConfig\n    at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:606)\n    at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:168)\n    at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522)\n    ... 71 common frames omitted\n`\n```\nSo far I have tried manipulating the `redis.clients` and `spring-boot-starter-data-redis` versions by decreasing and increasing it but always problems with initialization occur. Every single tutorial which I found doesn't contain configuration for the new Spring Boot application version. I Will be grateful for suggestions on how to manipulate the pom.xml version of `redis.clients` and `spring-boot-starter-data-redis` to make the application starts again.",
      "solution": "`spring-boot-starter-data-redis` already includes `jedis` as a dependency so you shouldn't add it to your pom.xml with a version that might not be compatible with your `spring-boot-starter-data-redis`.\nIn the case of `spring-boot-starter-data-redis` version 2.5.0, it includes `jedis` version 3.6.3 but you override this with version 3.1.0, which might not be compatible with `spring-boot-starter-data-redis` version 2.5.0.\nAlways check the dependencies already included in the Spring Boot Starters because incompatibilities are exactly what they try to avoid.\nHaving said all that, I am not completely sure that this will solve the issue, but it is a good place to start.\n\nUpdate 26/01/2022\nThe `spring-boot-starter-data-redis` dependency `pom.xml` includes the following dependency:\n`\n  org.springframework.data\n  spring-data-redis\n  2.5.1\n  compile\n\n`\nThis `spring-data-redis` dependency `pom.xml` already includes `jedis` version:\n`\n  (...)\n  3.6.0\n  (...)\n\n`\nAnd the dependency itself:\n`\n  redis.clients\n  jedis\n  ${jedis}\n  true\n\n`\nThis means that you should only add the dependency to your `pom.xml` but set no version so that the one set in `spring-data-redis` is used as follows:\n`\n  (...)\n  \n  \n  \n    redis.clients\n    jedis\n  \n  \n  (...)\n  \n\n`",
      "question_score": 13,
      "answer_score": 26,
      "created_at": "2021-09-08T22:09:47",
      "url": "https://stackoverflow.com/questions/69109109/classnotfoundexception-while-jedisclient-initialization-in-spring-boot-2-5-4-app"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 74314906,
      "title": "heartbeat: Unsupported command argument type: FalseClass - Redis",
      "problem": "I have rails application which is using sidekiq for background jobs. Recently upgraded ruby from 2.7.0 to 3.1.2 and respectively redis gem automatically updated to '5.0.5'. After this sidekiq(6.4.1) logs throws below error, if i revert to old redis version (4.6.0) in my gemlock, there is no error. Any idea what is the reason for this?\nError message:",
      "solution": "The Redis client for Ruby was recently updated and now checks command argument types strictly. Sidekiq as of 6.4.2 was passing a boolean (in your case `FalseClass`) in the heartbeat code, which the new Redis client rejected, hence the error. Booleans are invalid because Redis hashes don't support type hints; Redis 4.6 and older would just quietly convert to string.\nSidekiq has been updated to work with the new Redis client as of 6.5.x. I'm using 6.5.5 and the error is gone:\n`gem 'sidekiq', '~> 6.5.5'`\nHere's the PR that introduced the fix for reference: https://github.com/mperham/sidekiq/pull/5298",
      "question_score": 13,
      "answer_score": 21,
      "created_at": "2022-11-04T10:38:34",
      "url": "https://stackoverflow.com/questions/74314906/heartbeat-unsupported-command-argument-type-falseclass-redis"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 67095815,
      "title": "Packer giving me SCP permission denied when moving a redis.conf file over to an AMI",
      "problem": "I am extremely confused on why this occurs when using Packer. What I want to do is move a local redis.conf file over to my AMI; however, it gives me an error as I do this. My Packer provisioner is like so:\n```\n`  {\n    \"type\": \"file\",\n    \"source\": \"../../path/to/file/redis.conf\",\n    \"destination\": \"/etc/redis/redis.conf\"\n  }\n`\n```\nAnd then it returns an error saying: `scp /etc/redis/: Permission denied`",
      "solution": "What you need to do is copy the file to a location where you have write access (/tmp for example) and then use an inline provisioner to move it somewhere else.\nI have something like:\n```\n`provisioner \"file\" {\n  source      = \"some/path/to/file.conf\"\n  destination = \"/tmp/file.conf\n}\n`\n```\nAnd then:\n```\n`provisioner \"shell\" {\n  inline = [\"sudo mv /tmp/file.conf /etc/some_service/file.conf\"]\n}\n`\n```",
      "question_score": 13,
      "answer_score": 19,
      "created_at": "2021-04-14T18:44:02",
      "url": "https://stackoverflow.com/questions/67095815/packer-giving-me-scp-permission-denied-when-moving-a-redis-conf-file-over-to-an"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 74465674,
      "title": "How do I add the method AddStackExchangeRedisCache to IServiceCollection in C#?",
      "problem": "I have this startup.cs file that adds the different services to my application. One of the services I need to add is AddStackExchangeRedisCache but I am getting an error that says\n\"IServiceCollection does not contain a definition for AddStackExchangeRedisCache and no accessible extension method AddStackExchangeRedisCache accepting a first argument of type IServiceCollection could be found (are you missing a using directive or an asssembly reference?)?\"\nI have the following stack exchange redis refernce in my CSPROJ\n```\n`    \n\n`\n```\nAnd the method where I am trying to add the service is as follows.\n```\n`public void ConfigureServices(IServiceCollection services) {\n    services.AddStackExchangeRedisCache(options =>\n        {\n           // some code here\n        }\n    }\n`\n```",
      "solution": "Install `Microsoft.Extensions.Caching.StackExchangeRedis` nuget package.",
      "question_score": 12,
      "answer_score": 21,
      "created_at": "2022-11-16T19:46:17",
      "url": "https://stackoverflow.com/questions/74465674/how-do-i-add-the-method-addstackexchangerediscache-to-iservicecollection-in-c"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 71717395,
      "title": "Getting Error: connect ECONNREFUSED 127.0.0.1:6379 in docker-compose while connecting redis",
      "problem": "I am getting connection  Error: connect ECONNREFUSED 127.0.0.1:6379 while working with docker-compose to use Redis with node js.\nI used the same host name and service name in Redis but still got an error.\nMy node js code is:\n```\n`const express = require('express');\nconst redis = require('redis');\n\nconst client = redis.createClient({\n    port: 6379,\n    host: 'redis'\n});\nclient.connect();\nclient.on('connect', (err)=>{\n    if(err) throw err;\n    else console.log('Redis Connected..!');\n});\n\nconst app = express();\napp.get('/',async (req,res)=>{\n    let key = req.query['name'];\n    if(key){\n        let value = await client.get(key);\n        if(value){\n            value++;\n            client.set(key,value);\n            res.send(`Hello ${key}, ${value}!`);\n            console.log(`${key}, ${value}`);\n        }\n        else{\n            value = 1;\n            client.set(key,value); \n            res.send(`Hello ${key}, ${value}!`);\n            console.log(`${key}, ${value}`);\n        }\n    }\n    else{\n        console.log(\"Name not passed!\");\n        res.send(\"Hello World!\");\n    }\n});\nconst port = 3000;\napp.listen(port,()=>{\n    console.log(`App is listening at http://localhost:${port}`);\n});\n`\n```\nMy docker-compose.yml file is:\n```\n`version: \"3\"\nservices:\n  redis: \n    image: redis:latest\n    container_name: client\n    restart: unless-stopped\n    expose:\n      - 6379\n  app:\n    depends_on:\n      - redis\n    build:\n      context: .\n      dockerfile: Dockerfile\n    container_name: app\n    restart: on-failure\n    ports:\n      - \"3000:3000\"\n    volumes:\n      - .:/app\n`\n```\nand what I am getting on console is\n```\n`C:\\Users\\ashok\\Desktop\\Practice>docker-compose up\n[+] Running 3/3\n - Network practice_default  Created                                                                                                        0.1s\n - Container client          Created                                                                                                        1.1s\n - Container app             Created                                                                                                        0.3s\nAttaching to app, client\nclient  | 1:C 02 Apr 2022 11:08:28.885 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo\nclient  | 1:C 02 Apr 2022 11:08:28.886 # Redis version=6.2.6, bits=64, commit=00000000, modified=0, pid=1, just started\nclient  | 1:C 02 Apr 2022 11:08:28.886 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf\nclient  | 1:M 02 Apr 2022 11:08:28.889 * monotonic clock: POSIX clock_gettime\nclient  | 1:M 02 Apr 2022 11:08:28.890 * Running mode=standalone, port=6379.\nclient  | 1:M 02 Apr 2022 11:08:28.891 # Server initialized\nclient  | 1:M 02 Apr 2022 11:08:28.891 # WARNING overcommit_memory is set to 0! Background save may fail under low memory condition. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect.\nclient  | 1:M 02 Apr 2022 11:08:28.893 * Ready to accept connections\napp     |\napp     | > practice@1.0.0 start\napp     | > node app.js\napp     |\napp     | Server is live at port: 3000\napp     | node:internal/process/promises:279\napp exited with code 1\napp     |             triggerUncaughtException(err, true /* fromPromise */);\napp     |             ^\napp     |\napp     | Error: connect ECONNREFUSED 127.0.0.1:6379\napp     |     at TCPConnectWrap.afterConnect [as oncomplete] (node:net:1187:16)\napp     | Emitted 'error' event on Commander instance at:\napp     |     at RedisSocket. (/app/node_modules/@node-redis/client/dist/lib/client/index.js:339:14)\napp     |     at RedisSocket.emit (node:events:527:28)\napp     |     at RedisSocket._RedisSocket_connect (/app/node_modules/@node-redis/client/dist/lib/client/socket.js:117:14)\napp     |     at processTicksAndRejections (node:internal/process/task_queues:96:5)\napp     |     at async Commander.connect (/app/node_modules/@node-redis/client/dist/lib/client/index.js:162:9) {\napp     |   errno: -111,\napp     |   code: 'ECONNREFUSED',\napp     |   syscall: 'connect',\napp     |   address: '127.0.0.1',\napp     |   port: 6379\napp     | }\n`\n```\nwhat can I do to resolve it?",
      "solution": "I had tried specifying host and port in the client creation too, and that didn't work.\nI found that using\n```\n`const client = redis.createClient({\nurl: 'redis://redis:6379'\n});\n`\n```\nfixed the issue for me.",
      "question_score": 10,
      "answer_score": 19,
      "created_at": "2022-04-02T14:06:03",
      "url": "https://stackoverflow.com/questions/71717395/getting-error-connect-econnrefused-127-0-0-16379-in-docker-compose-while-conne"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 79844786,
      "title": "Ruby on Rails Redis connection_pool wrong number of arguments (given 1, expected 0)",
      "problem": "I recently encountered an issue deploying a Ruby on Rails app to production (on Render) whereby the app would not build and rake tasks would not run. The application would build and launch fine in development and was previously running fine in production too.\nThe error suggested some kind of Redis issue:\n```\n`rake aborted!\nArgumentError: wrong number of arguments (given 1, expected 0) (ArgumentError)\n/opt/render/project/.gems/ruby/3.4.0/gems/connection_pool-3.0.2/lib/connection_pool.rb:48:in 'initialize'\n/opt/render/project/.gems/ruby/3.4.0/gems/activesupport-8.1.1/lib/active_support/cache/redis_cache_store.rb:163:in 'Class#new'\n`\n```\nI tried (and failed) to debug the commit (which was a new feature so ~90 files changed) and couldn't find anything by searching, so I wanted to share the final solution in case anyone else has similar issues.",
      "solution": "The issue turned out to be a result of the Sidekiq gem being updated to `8.1.0` which in turn requires `connection_pool` to be at version `>= 3.0.0`. This then introduced a compatibility issue between `connection_pool` and Rails 8.1 (specifically `ActiveSupport`).\nThe issue\nThe `connection_pool` gem released version 3.0 recently, which removed support for passing configuration as a positional hash (a legacy Ruby behavior). However, the `RedisCacheStore` in Rails 8.1.1 is still instantiating the pool using the old style, causing the `ArgumentError`.\nThe fix\nDowngrading `connection_pool` to v2 solved the issue. It wasn't specifically referenced in my Gemfile, so I added the following line:\n`gem 'connection_pool', '~> 2.4.1'`\nAnd then running:\n`bundle update connection_pool`\nThis also results in Sidekiq being downgraded to `7.3.10`.\n(Additionally, I needed to update the Dependabot config to prevent it from automatically upgrading `connection_pool` and `sidekiq` using the following:\n```\n`version: 2\nupdates:\n  - package-ecosystem: bundler\n    directory: '/'\n    schedule:\n      interval: daily\n    open-pull-requests-limit: 10\n    ignore:\n      - dependency-name: 'connection_pool'\n        versions: ['>= 3.0']\n      - dependency-name: 'sidekiq'\n        versions: ['>= 8.1']\n\n  - package-ecosystem: github-actions\n    directory: '/'\n    schedule:\n      interval: daily\n    open-pull-requests-limit: 10\n`\n```\nUpdate\nThe above fix rolled back Sidekiq further than I would have liked it to. Instead, I have now removed the following line:\n`gem \"connection_pool\", \"\nAnd instead pinned Sidekiq it at the previously working 8.0 version:\n`gem \"sidekiq\", \"8.0.10\"`\nSee GitHub issue: Error after updating to 3.0.2: wrong number of arguments\u00a0#212.",
      "question_score": 10,
      "answer_score": 15,
      "created_at": "2025-12-12T11:36:19",
      "url": "https://stackoverflow.com/questions/79844786/ruby-on-rails-redis-connection-pool-wrong-number-of-arguments-given-1-expected"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 67613484,
      "title": "Redis: ERR wrong number of arguments for &#39;auth&#39; command",
      "problem": "I'm using `redis` with nodejs. Version: `\"redis\": \"^3.1.2\"`\nWhen my server connects to redis, I get the following error:\n```\n`ERR wrong number of arguments for 'auth' command\n`\n```\nI'm guessing it has something to do with the URL, which looks like this:\n```\n`redis://h:@:\n`\n```\nMy redis is hosted by Heroku and I'm unable to change the URL. And have no idea how can I make it work. Any ideas/solutions much appreciated.",
      "solution": "Redis Version node-redis >=3.1.0  , `redis://h:@:` will not work and throw `ERR wrong number of arguments for 'auth' command`.\nSolution: `redis://:@:` works ie: remove username from the URL.\nNote:- The \":\" before the ``\nThis should fix the problem that you are facing.\nFor other versions:\nRedis Version node-redis @:` works.\nRedis Version >= 6.0.0 and node-redis(any version), both `redis://:@:`(when username in redis ACL is set to custom username) and  `redis://@:` will work.\nThe reason is:\n\nnode-redis made the changes to support Redis-6, as per releasenote.\n\nRedis-6 supports username in ALC. Before v6, Redis did not include the notion of users, and the only authentication strategy was a single login password. Reference\n\nWhenever you attach Redis addon to Heroku container, the environment variable REDIS_URL is set and the value is Connection URL of format: `redis://h:@:`. This \"h\" is a fake/dummy/placeholder username because some clients(eg: node-redis) could not correctly handle a blank username in the URL.   After the release of ACLs in Redis 6, clients began to support the new AUTH command that uses 2 arguments (username and password). Clients that attempt to pass the h username to the AUTH will result in an above error on Redis versions 4 and 5. Reference",
      "question_score": 9,
      "answer_score": 27,
      "created_at": "2021-05-20T05:16:40",
      "url": "https://stackoverflow.com/questions/67613484/redis-err-wrong-number-of-arguments-for-auth-command"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 70322115,
      "title": "log4shell exploit for Redis server",
      "problem": "We are running redis server on EC2 instance.\ni can see in many publications that Redis Server is vulnerable to the log4shell exploit, but can't see any documentation or any official about that.\nWhat should I do in order to protect my redis server instance to be in-vulnerable for this exploit?",
      "solution": "Redis Server does not use Java and is therefore not impacted by this vulnerability.\nSee more here: https://redis.com/security/notice-apache-log4j2-cve-2021-44228/",
      "question_score": 9,
      "answer_score": 8,
      "created_at": "2021-12-12T09:42:41",
      "url": "https://stackoverflow.com/questions/70322115/log4shell-exploit-for-redis-server"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 70145795,
      "title": "Node Redis does not work on my windows computer even though the server is up and running",
      "problem": "```\n`const express = require(\"express\");\nconst redis = require(\"redis\");\nconst app = express();\n\nconst client = redis.createClient({\n  url: \"redis://admin123@ec2-35-182-15-126.ca-central-1.compute.amazonaws.com\",\n});\n\nclient.on(\"connect\", function () {\n  console.log(\"redis connected\");\n  console.log(`connected ${redisClient.connected}`);\n});\n\nclient.on(\"error\", (err) => {\n  console.log(err);\n});\n\napp.listen(process.env.PORT || 3000, () => {\n  console.log(\"Node server started\");\n});\n`\n```\nThe above code does not show any connection to redis server even though I have checked the EC2 redis instance by connecting using Redsmin.\nhosting details in Redsmin\nThis is a very simple thing to do but the error that I get cannot be googled.\n\nNode server started C:\\Users\\Sithira_105661\\Desktop\\Projects\\Learning\\Redis\\node_modules@node-redis\\client\\dist\\lib\\client\\index.js:387\nreturn Promise.reject(new errors_1.ClientClosedError());\n^\n\nClientClosedError: The client is closed\nat Commander._RedisClient_sendCommand (C:\\Users\\Sithira_105661\\Desktop\\Projects\\Learning\\Redis\\node_modules@node-redis\\client\\dist\\lib\\client\\index.js:387:31)\nat Commander.commandsExecutor (C:\\Users\\Sithira_105661\\Desktop\\Projects\\Learning\\Redis\\node_modules@node-redis\\client\\dist\\lib\\client\\index.js:160:154)\nat Commander.BaseClass. [as set] (C:\\Users\\Sithira_105661\\Desktop\\Projects\\Learning\\Redis\\node_modules@node-redis\\client\\dist\\lib\\commander.js:8:29)\nat Object. (C:\\Users\\Sithira_105661\\Desktop\\Projects\\Learning\\Redis\\redis.js:19:8)\nat Module._compile (node:internal/modules/cjs/loader:1101:14)\nat Object.Module._extensions..js (node:internal/modules/cjs/loader:1153:10)\nat Module.load (node:internal/modules/cjs/loader:981:32)\nat Function.Module._load (node:internal/modules/cjs/loader:822:12)\nat Function.executeUserEntryPoint [as runMain] (node:internal/modules/run_main:81:12)\nat node:internal/main/run_main_module:17:47\n\nHelp me understand the issue. Thanks in advance.",
      "solution": "finally found the solution. I used node-redis 3.0.0 rather than 4 and the code works fine. I do not know why it does not work in latest node-redis . If any of you guys are getting this issue use node-redis 3",
      "question_score": 8,
      "answer_score": 28,
      "created_at": "2021-11-28T18:23:27",
      "url": "https://stackoverflow.com/questions/70145795/node-redis-does-not-work-on-my-windows-computer-even-though-the-server-is-up-and"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 75157428,
      "title": "redis.exceptions.DataError: Invalid input of type: &#39;dict&#39;. Convert to a bytes, string, int or float first",
      "problem": "Goal: store a `dict()` or `{}` as the value for a key-value pair, to `set()` onto Redis.\nCode\n```\n`import redis\n\nr = redis.Redis()\n\nvalue = 180\n\nmy_dict = dict(bar=value)\n\nr.set('foo', my_dict)\n`\n```\n```\n`redis.exceptions.DataError: Invalid input of type: 'dict'. Convert to a bytes, string, int or float first.\n`\n```",
      "solution": "You cannot pass a dictionary object as a value in the `set()` operation to Redis.\nHowever, we can use either `pickle` or `json` to get the `Bytes` of an object.\nWhichever you already have imported would be optimal, imho.\n\nPickle\nSerialize to pickle (with `pickle.dumps`) pre-`set()`\n```\n`import pickle\n\nmy_dict = {'a': 1, 'b': 2}\n\ndict_bytes = pickle.dumps(my_dict)\n\nr.set('my_key', dict_bytes)\n`\n```\nDeserialize the object (dict) (with `pickle.loads`) post-`get()`:\n```\n`dict_bytes = r.get('my_key')\n\nmy_dict = pickle.loads(dict_bytes)\n`\n```\n\nJSON string\nSerialize to JSON string (with `json.dumps`) pre-`set()`\n```\n`import json\n\nmy_dict = {'a': 1, 'b': 2}\n\ndict_str = json.dumps(my_dict)\n\nr.set('my_key', dict_str)\n`\n```\nDeserialize the object (dict) (with `json.loads`) post-`get()`:\n```\n`dict_str = r.get('my_key')\n\nmy_dict = json.loads(dict_str)\n`\n```",
      "question_score": 8,
      "answer_score": 17,
      "created_at": "2023-01-18T10:58:14",
      "url": "https://stackoverflow.com/questions/75157428/redis-exceptions-dataerror-invalid-input-of-type-dict-convert-to-a-bytes-s"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 70128573,
      "title": "Django Celery with Redis Issues on Digital Ocean App Platform",
      "problem": "After quite a bit of trial and error and a step by step attempt to find solutions I thought I share the problems here and answer them myself according to what I've found. There is not too much documentation on this anywhere except small bits and pieces and this will hopefully help others in the future.\nPlease note that this is specific to Django, Celery, Redis and the Digital Ocean App Platform.\nThis is mostly about the below errors and further resulting implications:\n\nOSError: [Errno 38] Function not implemented\n\nand\n\nCannot connect to redis://......\n\nThe first error happens when you try run the celery command `celery -A your_app worker --beat -l info`\nor similar on the App Platform. It appears that this is currently not supported on digital ocean. The second error occurs when you make a number of potential mistakes.",
      "solution": "PART 1:\nWhile Digital Ocean might remedy this in the future here is an approach that will offer a workaround. The problem is the not supported execution pool. Google \"celery execution pools\" if you want to know more and how they work. The default one is `prefork`. But what you need is either `gevent` or `eventlet`. I went with the former for my purposes.\nWhichever you pick you will have to install it as it doesn't come with celery by default. In my case it was: `pip install gevent` (and don't forget adding it to your requirements as well).\nOnce you have that you can re-run the celery command but note that `gevent` and `beat` are not supported within a single command (will result in an error). Instead do the following:\n`celery -A your_app worker --pool=gevent -l info`\nand then separately (if you want to run beat that is) in another terminal/console\n`celery -A your_app beat -l info`\nIn the first line you can also specify the `concurrency` like so: `--concurrency=100`. This is not required but useful. Read up on it what it does as that goes beyond the solution here.\nPART 2:\nIn my specific case I've tested the above locally (development) first to make sure they work. The next issue was getting this into production. I use Redis as the db/broker.\nIn my specific setup I have most of my celery configuration in `the_main_app/celery/__init__.py` file but sometimes people put it directly into `the_main_app/celery.py`. Whichever it is you do make sure that the REDIS_URL is set correctly. For development it usually looks something like this:\n`YOUR_VAR_NAME = os.environ.get('REDIS_URL', 'redis://localhost:6379')` where `YOUR_VAR_NAME` is then set to the broker with everything as below:\n```\n`YOUR_VAR_NAME = os.environ.get('REDIS_URL', 'redis://localhost:6379')\napp = Celery('the_main_app')\napp.conf.broker_url = YOUR_VAR_NAME\n`\n```\nThe remaining settings are all documented on the \"celery first steps with django\" help page but are not relevant for what I am showing here.\nPART 3:\nWhen you setup your Redis Database on the App Platform (which is very simple) you will see the connection details as 'public network' and 'VPC network'.\nThe celery documentation says to use the following URL format for production: `redis://:password@hostname:port/db_number`. This didn't work. If you are not using a yaml file then you can simply copy paste the entire connection string (select from the dropdown!) from the Redis DB connection details and then setup an App-Level environment variable in your Digital Ocean project named `REDIS_URL` and paste in that entire string (and also encrypt it!).\nThe string should look like something like this (redis with 2 s!)\n`rediss://USER:PASS@URL.db.ondigitialocean.com:PORT`.\nYou are almost done. The last step is to setup the workers. It was fine for me to run the PART 1 commands as console commands on the App Platform to test them but eventually I've setup a small worker (+ Add Component) for each line pasted them into the Run Command.\nThat is basically the process step by step. Good luck!",
      "question_score": 8,
      "answer_score": 13,
      "created_at": "2021-11-26T19:13:47",
      "url": "https://stackoverflow.com/questions/70128573/django-celery-with-redis-issues-on-digital-ocean-app-platform"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 73353675,
      "title": "Two levels of cache (Redis + Caffeine)",
      "problem": "When profiling an application it came up that Redis is impacting the execution times because there are many sleeps in threads. I need to implement two levels of cache or think about solution of this problem.\nI would like to have two levels of caches:\n\nL1 - local for each instance of deployment,\nL2 - cache global for all instances of same deployment,\n\nThe solution that I came up with is:\n\nCreate two CacheManagers (CaffeineCacheManager and RedisCacheManager),\nInitialize same caches for each cache manager,\nUse annotation @Caching with cacheable={} to use two caches,\n\n`\n    @Caching(cacheable = {\n            @Cacheable(cacheNames = CacheConfiguration.HELLO_WORLD),\n            @Cacheable(cacheNames = CacheConfiguration.HELLO_WORLD, cacheManager = \"cacheManagerRedis\")\n    })\n    public String generate(String name)\n    {\n        log.info(\"  Cached method call...\");\n        return helloWorldService.generate(name);\n    }\n\n`\nThe structure of classes is similar to: CachedService (annotations here) -> NonCachedService\nThe problem I am facing:\nI wanted to have it working in flow (yes - works/n - not working):\n\n[ y ] data is fetched and then cached to both caches Redis and local - this works\n[ y ] if data exists in local cache do not move it to redis - this works\n[ y ] if any of caches contains the data it will be fetched from cache\n[ n ] if data exists in Redis, move it to local - this does not work\n\nModification of @Caching annotation to have put={} where it would put values into local cache is making whole cache not working.\n`\n    @Caching(cacheable = {\n            @Cacheable(cacheNames = CacheConfiguration.HELLO_WORLD),\n            @Cacheable(cacheNames = CacheConfiguration.HELLO_WORLD, cacheManager = \"cacheManagerRedis\")\n    }, put = {\n            @CachePut(cacheNames = CacheConfiguration.HELLO_WORLD),\n    })\n    public String generate(String name)\n    {\n        log.info(\"  Cached method call...\");\n        return helloWorldService.generate(name);\n    }\n\n`\n\nDo you know any spring-ready solutions to work with two levels of cache?\nI've read about local caching with Redis but it does not mean anything similar to my case (it's just the standard redis use case),\nI am left only with double-layered services structure to achieve this goal? Similar to CachedLocal -> CachedRedis -> NonCached",
      "solution": "For anyone else looking for this, I was able to implement the CacheInterceptor suggested by Ankit.\nExample:\n```\n` public class RedisMultiCacheInterceptor extends CacheInterceptor {\n\n    @Autowired\n    private CacheManager caffeineCacheManager;\n\n    @Override\n    protected Cache.ValueWrapper doGet(Cache cache, Object key) {\n        //Get item from cache\n        var superGetResult = super.doGet(cache, key);\n\n        if (superGetResult == null) {\n            return superGetResult;\n        }\n\n        //If retrieved from Redis, check if it's missing from caffeine on local and add it\n        if (cache.getClass() == RedisCache.class) {\n            var caffeineCache = caffeineCacheManager.getCache(cache.getName());\n\n            if (caffeineCache != null) {\n                caffeineCache.putIfAbsent(key, superGetResult.get());\n            }\n        }\n\n        return superGetResult;\n    }\n}\n`\n```",
      "question_score": 8,
      "answer_score": 6,
      "created_at": "2022-08-14T19:26:40",
      "url": "https://stackoverflow.com/questions/73353675/two-levels-of-cache-redis-caffeine"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 72092382,
      "title": "Does Redis cache have advantage over Spring cache if used only for simple cache?",
      "problem": "I am new to caching thing and learning some different solutions for my spring boot app. I was looking at Spring Cache and it is simple caching mechanism (that was what I look for) than I saw redis cache as well. And there are so many resources like \"spring+redis cache\". When I look at simple usage, I saw no difference. Even the annotations are same(Cacheable, CacheEvict, CachePut etc.), and I could not see a difference of usage except extra redis configuration and redis docker container etc... And none of these spring+redis cache articles tell what is the difference between just spring cache and spring+redis cache.\nWhat is the advantage of redis cache over spring cache ? Or can you tell a simple use case that definitely I need to use redis cache and I cannot achieve it with spring cache ?",
      "solution": "There is no such thing as a \"Spring cache\" from an implementation perspective. However, there is Spring's Cache Abstraction providing general caching facilities, and it originated in the core Spring Framework.\nThe Cache Abstraction is fundamentally an Adapter enabling different caching providers to be plugged into the framework and used generically for caching purposes. Caching can be applied in any tier of your Spring [Boot] application, for example, in the data access layer, or service layer, and so on. Caching can even be applied in multiple tiers simultaneously. Caching is, after all, a cross-cutting concern, and is, not coincidently, implemented with Spring AOP.\nThe abstraction provides a facade including an API along with Annotations for the most common caching operations (for example: `Cache.put(key, value)`, `Cache.get(key)`, `Cache.evict(key)`, etc).  These caching operations are fairly standard and common across most caching provider implementations (e.g. Redis, Hazelcast, Apache Geode, and even Java's `Map` and `ConcurrentMap` implementations; a cache really is a Map-like data structure).\nEvery caching provider must provide an implementation of the `Cache` and `CacheManager` interfaces defined by Spring's Cache Abstraction in order to be able to be plugged into the framework for caching purposes. In this way, every caching provider can be treated generically using Spring's caching API (or Annotations, and even the JCache annotations) so that you are 1) not tied to any specific caching provider, which then 2) allows you to interchange caching providers as your application use cases or requirements change.\nMake sense so far?\nOut-of-the-box, Spring (Boot) configures the `ConcurrentMap` caching implementation (see here, then here (source) and finally, here followed by this) as the caching provider plugged into the framework when no other caching provider (e.g. Redis) is present on the application classpath.\nHowever, there is a BIG difference between the `ConcurrentMap` caching implementation and the Redis provider implementation.  The later plugs in a Redis client which can remotely connect to a cluster of Redis servers. This means, in essence, you can have a \"distributed\" cache, where the load can be uniformly distributed, assuming you configure your caching provider, like Redis, correctly.\nNot all caching providers are equal. Some offer many additional services, and some, like Apache Geode, can even function as the system of record (SoR) for your entire application, and maybe even eventually replace your RDBMS.\nEven though most caching providers are capable of complex eviction (LRU, LFU) and expiration, Spring's Cache Abstraction does not specify the contract for these capabilities. That is left for you to decide and configure based on the cache provider you choose.\nFully distributed caching providers like Apache Geode and Hazelcast also provide memory management capabilities (after all, keys/values are stored in-memory). They can even be configured for persistence (hence the SoR UC), support different topologies: client/server, WAN (multi-site), embedded, and have support for different caching patterns (Cache-Aside, Inline Caching, Near Caching, etc) as well as different caching UC (e.g. (HTTP) Session state caching).\nDISCLAIMER: I am the Spring Data engineer behind the Spring for Apache Geode offerings, so I will leave you with a chapter I devoted to \"caching\" in the Spring Boot for Apache Geode reference guide.\nThis post should hopefully give you enough to chew on for awhile.\nIf you have additional questions, please ask in the comments.",
      "question_score": 7,
      "answer_score": 28,
      "created_at": "2022-05-02T23:02:27",
      "url": "https://stackoverflow.com/questions/72092382/does-redis-cache-have-advantage-over-spring-cache-if-used-only-for-simple-cache"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 70274342,
      "title": "Unable to connect to remote redis host [nodeJS]",
      "problem": "`const redis = require('redis');\nrequire('dotenv').config();\nconsole.log(process.env.redisHost, ':', process.env.redisPort);\nconst redisClient = redis.createClient({\n    host: process.env.redisHost,\n    port: process.env.redisPort,\n    password: process.env.redisKey\n});\nredisClient.connect();\nredisClient.on('error', err => console.log('Redis error: ', err.message));\nredisClient.on('connect', () => console.log('Connected to redis server'));\nmodule.exports = redisClient;\n`\nI tried this sample from redis docs but still I'm getting an error stating:\n`Redis error:  connect ECONNREFUSED 127.0.0.1:6379`\nI logged the environment host and port variables to the console and I got the remote host ipv4 address, but still the client is trying to connect to localhost instead of remote host (I purposely uninstalled redis from my local device to check if the client is working as it is supposed to). I also confirmed that the remote redis host is working perfectly.\nI also tried different methods like :\nhttps://cloud.google.com/community/tutorials/nodejs-redis-on-appengine\n```\n`redis.createClient(port, host, {auth_pass: password});\n`\n```\nBut still, I got the same error.\nI am able to connect to the redis host via commandline:\n```\n`redis-cli.exe -h XX.XX.XX.XXX -a Password\nXX.XX.XX.XXX:6379> set name dhruv\nOK\nXX.XX.XX.XXX:6379> get name\n\"dhruv\"\nXX.XX.XX.XXX:6379> exit\n`\n```\nI'm trying to use redis on nodejs for the first time, so don't have a proper idea but I think I am doing everything right.\nAny solution/workaround will be helpful :D",
      "solution": "It worked with this code:\n`const url = `redis://${process.env.redisHost}:${process.env.redisPort}`;\nconst redisClient = redis.createClient({\n    url,\n    password: process.env.redisKey\n});\nredisClient.connect();\n`",
      "question_score": 7,
      "answer_score": 16,
      "created_at": "2021-12-08T12:36:22",
      "url": "https://stackoverflow.com/questions/70274342/unable-to-connect-to-remote-redis-host-nodejs"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 73808301,
      "title": "Rails error: can&#39;t activate redis (&gt;= 3, &lt; 5), already activated redis-5.0.4",
      "problem": "I tried to deploy my Rails app, and got the error\n```\n`can't activate redis (>= 3, I've tried deleting the gemfile, running bundle install/update but no joy. I've also tried specifying redis 4.8.0 in my gemfile but still got the same error.\nMy gemfile:\n```\n`source 'https://rubygems.org'\ngit_source(:github) { |repo| \"https://github.com/#{repo}.git\" }\n\nruby '2.7.0'\n\n# Bundle edge Rails instead: gem 'rails', github: 'rails/rails'\ngem 'rails', '~> 6.0.4'\n# Use postgresql as the database for Active Record\ngem 'pg', '>= 0.18', ' 3.11'\n# Use SCSS for stylesheets\ngem 'sass-rails', '~> 5.0'\n# Use Uglifier as compgiffnressor for JavaScript assets\ngem 'uglifier', '>= 1.3.0'\n\ngem 'execjs'\n# See https://github.com/rails/execjs#readme for more supported runtimes\ngem 'csv'\ngem 'i18n', '1.8.11'\ngem 'coffee-rails', '~> 4.2'\n# Build JSON APIs with ease. Read more: https://github.com/rails/jbuilder\ngem 'jbuilder', '~> 2.5'\ngem 'jquery-rails'\ngem 'rails_12factor'\ngem 'mimemagic', github: 'mimemagicrb/mimemagic', ref: '3543363026121ee28d98dfce4cb6366980c055ee'\ngem 'by_star', git: 'https://github.com/radar/by_star'\ngem 'forest_liana'\ngem 'faker', :git => 'https://github.com/faker-ruby/faker.git', :branch => 'master'\ngem 'pagy', '~> 5.10'\ngem 'will_paginate', '~> 3.1.0'\ngem 'will_paginate_infinite'\ngem 'devise'\ngem 'friendly_id', '~> 5.4.0'\ngem 'cloudinary'\ngem 'attachinary', git: 'https://github.com/ThomasConnolly/attachinary.git'\ngem 'carrierwave'\ngem 'alphabetical_paginate'\n\ngem 'stripe', '~> 5.14.0'\ngem 'omniauth-stripe-connect'\ngem 'stripe_event'\ngem 'httparty'\ngem \"banktools-gb\"\ngem \"numbers_and_words\"\n\ngem 'fcm'\n\ngem 'resque'\ngem 'resque_mailer'\ngem 'resque-scheduler'\n\ngem 'resque-heroku-signals'\n\ngem 'rails_admin', '~> 2.0'\ngem 'cancancan'\n\ngem \"administrate\"\n\ngem 'country_select', '~> 4.0'\n\ngem \"font-awesome-rails\"\n\ngem 'devise-bootstrap-views'\ngem \"twitter-bootstrap-rails\"\ngem 'bootstrap-sass', '~> 3.3.7'\n# Use Redis adapter to run Action Cable in production\ngem 'redis'\ngem 'gon'\ngem 'sendgrid'\ngem 'add_event'\n\ngem \"serviceworker-rails\"\ngem \"simple_calendar\", \"~> 2.4\"\n\n# Use ActiveModel has_secure_password\n# gem 'bcrypt', '~> 3.1.7'\n\n# Use ActiveStorage variant\n# gem 'mini_magick', '~> 4.8'\n\n# Use Capistrano for deployment\n# gem 'capistrano-rails', group: :development\ngem 'listen'\n\n# Reduces boot times through caching; required in config/boot.rb\ngem 'bootsnap', '>= 1.1.0', require: false\n\ngroup :development, :test do\n  # Call 'byebug' anywhere in the code to stop execution and get a debugger console\n  gem 'byebug', platforms: [:mri, :mingw, :x64_mingw]\nend\n\ngroup :development do\n  # Access an interactive console on exception pages or by calling 'console' anywhere in the code.\n  gem 'web-console', '>= 3.3.0'\n  # Spring speeds up development by keeping your application running in the background. Read more: https://github.com/rails/spring\n  gem 'spring'\n  gem 'spring-watcher-listen', '~> 2.0.0'\nend\n\ngroup :test do\n  # Adds support for Capybara system testing and selenium driver\n  gem 'capybara', '>= 2.15'\n  gem 'selenium-webdriver'\n  # Easy installation and use of chromedriver to run system tests with Chrome\n  gem 'chromedriver-helper'\nend\n\n# Windows does not include zoneinfo files, so bundle the tzinfo-data gem\ngem 'tzinfo-data', platforms: [:mingw, :mswin, :x64_mingw, :jruby]\n`\n```\ngemfile.lock:\n```\n`GIT\n  remote: https://github.com/ThomasConnolly/attachinary.git\n  revision: c7bbc9d5d5ea6aee7584e381c82d0b5bea73e49b\n  specs:\n    attachinary (1.3.1)\n      cloudinary (~> 1.1)\n      rails (>= 3.2)\n\nGIT\n  remote: https://github.com/faker-ruby/faker.git\n  revision: 4f4c1a330963de84ea188f97de12e6895217eac3\n  branch: master\n  specs:\n    faker (2.23.0)\n      i18n (>= 1.8.11, = 3.2.0)\n\nGEM\n  remote: https://rubygems.org/\n  specs:\n    actioncable (6.0.6)\n      actionpack (= 6.0.6)\n      nio4r (~> 2.0)\n      websocket-driver (>= 0.6.1)\n    actionmailbox (6.0.6)\n      actionpack (= 6.0.6)\n      activejob (= 6.0.6)\n      activerecord (= 6.0.6)\n      activestorage (= 6.0.6)\n      activesupport (= 6.0.6)\n      mail (>= 2.7.1)\n    actionmailer (6.0.6)\n      actionpack (= 6.0.6)\n      actionview (= 6.0.6)\n      activejob (= 6.0.6)\n      mail (~> 2.5, >= 2.5.4)\n      rails-dom-testing (~> 2.0)\n    actionpack (6.0.6)\n      actionview (= 6.0.6)\n      activesupport (= 6.0.6)\n      rack (~> 2.0, >= 2.0.8)\n      rack-test (>= 0.6.3)\n      rails-dom-testing (~> 2.0)\n      rails-html-sanitizer (~> 1.0, >= 1.2.0)\n    actiontext (6.0.6)\n      actionpack (= 6.0.6)\n      activerecord (= 6.0.6)\n      activestorage (= 6.0.6)\n      activesupport (= 6.0.6)\n      nokogiri (>= 1.8.5)\n    actionview (6.0.6)\n      activesupport (= 6.0.6)\n      builder (~> 3.1)\n      erubi (~> 1.4)\n      rails-dom-testing (~> 2.0)\n      rails-html-sanitizer (~> 1.1, >= 1.2.0)\n    activejob (6.0.6)\n      activesupport (= 6.0.6)\n      globalid (>= 0.3.6)\n    activemodel (6.0.6)\n      activesupport (= 6.0.6)\n    activemodel-serializers-xml (1.0.2)\n      activemodel (> 5.x)\n      activesupport (> 5.x)\n      builder (~> 3.1)\n    activerecord (6.0.6)\n      activemodel (= 6.0.6)\n      activesupport (= 6.0.6)\n    activestorage (6.0.6)\n      actionpack (= 6.0.6)\n      activejob (= 6.0.6)\n      activerecord (= 6.0.6)\n      marcel (~> 1.0)\n    activesupport (6.0.6)\n      concurrent-ruby (~> 1.0, >= 1.0.2)\n      i18n (>= 0.7,  5.1)\n      tzinfo (~> 1.1)\n      zeitwerk (~> 2.2, >= 2.2.2)\n    add_event (0.1.0)\n      addressable (~> 2.3)\n    addressable (2.8.1)\n      public_suffix (>= 2.0.2, = 5.0)\n      actionview (>= 5.0)\n      activerecord (>= 5.0)\n      jquery-rails (>= 4.0)\n      kaminari (>= 1.0)\n      sassc-rails (~> 2.1)\n      selectize-rails (~> 0.6)\n    aes_key_wrap (1.1.0)\n    alphabetical_paginate (2.3.4)\n    archive-zip (0.12.0)\n      io-like (~> 0.3.0)\n    arel-helpers (2.14.0)\n      activerecord (>= 3.1.0,  2)\n    aws_cf_signer (0.1.3)\n    banktools-gb (0.13.1)\n      attr_extras\n    bcrypt (3.1.18)\n    bindata (2.4.10)\n    bindex (0.8.1)\n    bootsnap (1.13.0)\n      msgpack (~> 1.2)\n    bootstrap-sass (3.3.7)\n      autoprefixer-rails (>= 5.2.1)\n      sass (>= 3.3.4)\n    builder (3.2.4)\n    byebug (11.1.3)\n    cancancan (3.4.0)\n    capybara (3.37.1)\n      addressable\n      matrix\n      mini_mime (>= 0.1.3)\n      nokogiri (~> 1.8)\n      rack (>= 1.6.0)\n      rack-test (>= 0.6.3)\n      regexp_parser (>= 1.5,  3.2)\n    carrierwave (2.2.2)\n      activemodel (>= 5.0.0)\n      activesupport (>= 5.0.0)\n      addressable (~> 2.6)\n      image_processing (~> 1.1)\n      marcel (~> 1.0.0)\n      mini_mime (>= 0.1.3)\n      ssrf_filter (~> 1.0)\n    childprocess (4.1.0)\n    chromedriver-helper (2.1.1)\n      archive-zip (~> 0.10)\n      nokogiri (~> 1.8)\n    cloudinary (1.23.0)\n      aws_cf_signer\n      rest-client (>= 2.0.0)\n    coffee-rails (4.2.2)\n      coffee-script (>= 2.2.0)\n      railties (>= 4.0.0)\n    coffee-script (2.4.1)\n      coffee-script-source\n      execjs\n    coffee-script-source (1.12.2)\n    commonjs (0.2.7)\n    concurrent-ruby (1.1.10)\n    connection_pool (2.3.0)\n    countries (3.1.0)\n      i18n_data (~> 0.11.0)\n      sixarm_ruby_unaccent (~> 1.1)\n      unicode_utils (~> 1.4)\n    country_select (4.0.0)\n      countries (~> 3.0)\n      sort_alphabetical (~> 1.0)\n    crass (1.0.6)\n    csv (3.2.5)\n    devise (4.8.1)\n      bcrypt (~> 3.0)\n      orm_adapter (~> 0.1)\n      railties (>= 4.1.0)\n      responders\n      warden (~> 1.2.3)\n    devise-bootstrap-views (1.1.0)\n    digest (3.1.0)\n    domain_name (0.5.20190701)\n      unf (>= 0.0.5, = 2.0, = 0.0.4)\n    faraday-net_http (3.0.0)\n    fcm (1.0.8)\n      faraday (>= 1.0.0,  1)\n    ffi (1.15.5)\n    font-awesome-rails (4.7.0.8)\n      railties (>= 3.2, = 0.14.0)\n      groupdate (>= 5.0.0)\n      httparty\n      ipaddress\n      json\n      json-jwt\n      jwt\n      openid_connect\n      rack-cors\n      rails (>= 4.0)\n      useragent\n    forestadmin-jsonapi-serializers (2.0.0.pre.beta.2)\n      activesupport\n    friendly_id (5.4.2)\n      activerecord (>= 4.0.0)\n    globalid (1.0.0)\n      activesupport (>= 5.0)\n    gon (6.4.0)\n      actionpack (>= 3.0.20)\n      i18n (>= 0.7)\n      multi_json\n      request_store (>= 1.0)\n    googleauth (1.2.0)\n      faraday (>= 0.17.3, = 1.4,  0.16)\n      multi_json (~> 1.11)\n      os (>= 0.9, = 0.16, = 5.2)\n    haml (5.2.2)\n      temple (>= 0.8.0)\n      tilt\n    hashie (5.0.0)\n    http-accept (1.7.0)\n    http-cookie (1.0.5)\n      domain_name (~> 0.5)\n    httparty (0.20.0)\n      mime-types (~> 3.0)\n      multi_xml (>= 0.5.2)\n    httpclient (2.8.3)\n    i18n (1.8.11)\n      concurrent-ruby (~> 1.0)\n    i18n_data (0.11.0)\n    image_processing (1.12.2)\n      mini_magick (>= 4.9.5, = 2.0.17, = 5.0.0)\n      activesupport (>= 5.0.0)\n    jquery-rails (4.5.0)\n      rails-dom-testing (>= 1, = 4.2.0)\n      thor (>= 0.14, = 3.2.16)\n    json (2.6.2)\n    json-jwt (1.15.3)\n      activesupport (>= 4.2)\n      aes_key_wrap\n      bindata\n      httpclient\n    jwt (2.5.0)\n    kaminari (1.2.2)\n      activesupport (>= 4.1.0)\n      kaminari-actionview (= 1.2.2)\n      kaminari-activerecord (= 1.2.2)\n      kaminari-core (= 1.2.2)\n    kaminari-actionview (1.2.2)\n      actionview\n      kaminari-core (= 1.2.2)\n    kaminari-activerecord (1.2.2)\n      activerecord\n      kaminari-core (= 1.2.2)\n    kaminari-core (1.2.2)\n    less (2.6.0)\n      commonjs (~> 0.2.7)\n    less-rails (4.0.0)\n      actionpack (>= 4)\n      less (~> 2.6.0)\n      sprockets (>= 2)\n    listen (3.7.1)\n      rb-fsevent (~> 0.10, >= 0.10.3)\n      rb-inotify (~> 0.9, >= 0.9.10)\n    loofah (2.19.0)\n      crass (~> 1.0.2)\n      nokogiri (>= 1.5.9)\n    mail (2.7.1)\n      mini_mime (>= 0.1.1)\n    marcel (1.0.2)\n    matrix (0.4.2)\n    memoist (0.16.2)\n    method_source (1.0.0)\n    mime-types (3.4.1)\n      mime-types-data (~> 3.2015)\n    mime-types-data (3.2022.0105)\n    mini_magick (4.11.0)\n    mini_mime (1.1.2)\n    minitest (5.16.3)\n    msgpack (1.5.6)\n    multi_json (1.15.0)\n    multi_xml (0.6.0)\n    nested_form (0.3.2)\n    net-protocol (0.1.3)\n      timeout\n    net-smtp (0.3.1)\n      digest\n      net-protocol\n      timeout\n    netrc (0.11.0)\n    nio4r (2.5.8)\n    nokogiri (1.13.8-x86_64-darwin)\n      racc (~> 1.4)\n    numbers_and_words (0.11.11)\n      i18n (= 0.17.3, = 1.0,  0.5)\n      rack (>= 1.2,  2.0)\n      version_gem (~> 1.1)\n    omniauth (1.9.2)\n      hashie (>= 3.4.6)\n      rack (>= 1.6.2, = 1.4, = 1.9,  1.3)\n      omniauth-oauth2 (~> 1.4)\n    openid_connect (1.3.1)\n      activemodel\n      attr_required (>= 1.0.0)\n      json-jwt (>= 1.5.0)\n      net-smtp\n      rack-oauth2 (>= 1.6.1)\n      swd (>= 1.0.0)\n      tzinfo\n      validate_email\n      validate_url\n      webfinger (>= 1.0.1)\n    orm_adapter (0.5.0)\n    os (1.1.4)\n    pagy (5.10.1)\n      activesupport\n    pg (1.4.3)\n    public_suffix (5.0.0)\n    puma (3.12.6)\n    racc (1.6.0)\n    rack (2.2.4)\n    rack-cors (1.1.1)\n      rack (>= 2.0.0)\n    rack-oauth2 (1.21.3)\n      activesupport\n      attr_required\n      httpclient\n      json-jwt (>= 1.11.0)\n      rack (>= 2.1.0)\n    rack-pjax (1.1.0)\n      nokogiri (~> 1.5)\n      rack (>= 1.1)\n    rack-test (2.0.2)\n      rack (>= 1.3)\n    rails (6.0.6)\n      actioncable (= 6.0.6)\n      actionmailbox (= 6.0.6)\n      actionmailer (= 6.0.6)\n      actionpack (= 6.0.6)\n      actiontext (= 6.0.6)\n      actionview (= 6.0.6)\n      activejob (= 6.0.6)\n      activemodel (= 6.0.6)\n      activerecord (= 6.0.6)\n      activestorage (= 6.0.6)\n      activesupport (= 6.0.6)\n      bundler (>= 1.3.0)\n      railties (= 6.0.6)\n      sprockets-rails (>= 2.0.0)\n    rails-dom-testing (2.0.3)\n      activesupport (>= 4.2.0)\n      nokogiri (>= 1.6)\n    rails-html-sanitizer (1.4.3)\n      loofah (~> 2.3)\n    rails_12factor (0.0.3)\n      rails_serve_static_assets\n      rails_stdout_logging\n    rails_admin (2.2.1)\n      activemodel-serializers-xml (>= 1.0)\n      builder (~> 3.1)\n      haml (>= 4.0, = 3.0, = 5.0, = 0.14,  0.3)\n      rack-pjax (>= 0.7)\n      rails (>= 5.0,  1.3)\n      sassc-rails (>= 1.3, = 0.8.7)\n      thor (>= 0.20.3,  1.0)\n    redis (5.0.5)\n      redis-client (>= 0.9.0)\n    redis-client (0.9.0)\n      connection_pool\n    regexp_parser (2.5.0)\n    remotipart (1.4.4)\n    request_store (1.5.1)\n      rack (>= 1.4)\n    responders (3.0.1)\n      actionpack (>= 5.0)\n      railties (>= 5.0)\n    rest-client (2.1.0)\n      http-accept (>= 1.7.0, = 1.0.2, = 1.16,  0.8)\n    rexml (3.2.5)\n    ruby-vips (2.1.4)\n      ffi (~> 1.12)\n    ruby2_keywords (0.0.5)\n    rubyzip (2.3.2)\n    sass (3.7.4)\n      sass-listen (~> 4.0.0)\n    sass-listen (4.0.0)\n      rb-fsevent (~> 0.9, >= 0.9.4)\n      rb-inotify (~> 0.9, >= 0.9.7)\n    sass-rails (5.1.0)\n      railties (>= 5.2.0)\n      sass (~> 3.1)\n      sprockets (>= 2.8, = 2.0, = 1.1,  1.9)\n    sassc-rails (2.1.2)\n      railties (>= 4.0.0)\n      sassc (>= 2.0)\n      sprockets (> 3.0)\n      sprockets-rails\n      tilt\n    selectize-rails (0.12.6)\n    selenium-webdriver (4.4.0)\n      childprocess (>= 0.5,  3.2, >= 3.2.5)\n      rubyzip (>= 1.2.2,  1.0)\n    sendgrid (1.2.4)\n      json\n    serviceworker-rails (0.6.0)\n      railties (>= 3.1)\n    signet (0.17.0)\n      addressable (~> 2.8)\n      faraday (>= 0.17.5, = 1.5,  1.10)\n    simple_calendar (2.4.3)\n      rails (>= 3.0)\n    sixarm_ruby_unaccent (1.2.0)\n    snaky_hash (2.0.0)\n      hashie\n      version_gem (~> 1.1)\n    sort_alphabetical (1.1.0)\n      unicode_utils (>= 1.2.2)\n    spring (2.1.1)\n    spring-watcher-listen (2.0.1)\n      listen (>= 2.7, = 1.2,  1.0)\n      rack (> 1, = 5.2)\n      activesupport (>= 5.2)\n      sprockets (>= 3.0.0)\n    ssrf_filter (1.1.1)\n    stripe (5.14.0)\n    stripe_event (2.6.0)\n      activesupport (>= 3.1)\n      stripe (>= 2.8, = 3)\n      attr_required (>= 0.0.5)\n      httpclient (>= 2.4)\n    temple (0.8.2)\n    thor (1.2.1)\n    thread_safe (0.3.6)\n    tilt (2.0.11)\n    timeout (0.3.0)\n    twitter-bootstrap-rails (5.0.0)\n      actionpack (>= 5.0,  2.7)\n      less-rails (>= 3.0, = 5.0,  0.1)\n    uglifier (4.2.0)\n      execjs (>= 0.3.0, = 3.0)\n      mail (>= 2.2.5)\n    validate_url (1.0.15)\n      activemodel (>= 3.0.0)\n      public_suffix\n    version_gem (1.1.1)\n    warden (1.2.9)\n      rack (>= 2.0.9)\n    web-console (4.2.0)\n      actionview (>= 6.0.0)\n      activemodel (>= 6.0.0)\n      bindex (>= 0.4.0)\n      railties (>= 6.0.0)\n    webfinger (1.2.0)\n      activesupport\n      httpclient (>= 2.4)\n    websocket (1.2.9)\n    websocket-driver (0.7.5)\n      websocket-extensions (>= 0.1.0)\n    websocket-extensions (0.1.5)\n    will_paginate (3.1.8)\n    will_paginate_infinite (0.1.3)\n      will_paginate (~> 3.0, >= 3.0.3)\n    xpath (3.2.0)\n      nokogiri (~> 1.8)\n    zeitwerk (2.6.0)\n\nPLATFORMS\n  x86_64-darwin-20\n\nDEPENDENCIES\n  add_event\n  administrate\n  alphabetical_paginate\n  attachinary!\n  banktools-gb\n  bootsnap (>= 1.1.0)\n  bootstrap-sass (~> 3.3.7)\n  by_star!\n  byebug\n  cancancan\n  capybara (>= 2.15)\n  carrierwave\n  chromedriver-helper\n  cloudinary\n  coffee-rails (~> 4.2)\n  country_select (~> 4.0)\n  csv\n  devise\n  devise-bootstrap-views\n  execjs\n  faker!\n  fcm\n  font-awesome-rails\n  forest_liana\n  friendly_id (~> 5.4.0)\n  gon\n  httparty\n  i18n (= 1.8.11)\n  jbuilder (~> 2.5)\n  jquery-rails\n  listen\n  mimemagic!\n  numbers_and_words\n  omniauth-stripe-connect\n  pagy (~> 5.10)\n  pg (>= 0.18,  3.11)\n  rails (~> 6.0.4)\n  rails_12factor\n  rails_admin (~> 2.0)\n  redis\n  sass-rails (~> 5.0)\n  selenium-webdriver\n  sendgrid\n  serviceworker-rails\n  simple_calendar (~> 2.4)\n  spring\n  spring-watcher-listen (~> 2.0.0)\n  stripe (~> 5.14.0)\n  stripe_event\n  twitter-bootstrap-rails\n  tzinfo-data\n  uglifier (>= 1.3.0)\n  web-console (>= 3.3.0)\n  will_paginate (~> 3.1.0)\n  will_paginate_infinite\n\nRUBY VERSION\n   ruby 2.7.0p0\n\nBUNDLED WITH\n   2.3.21\n`\n```\nAny help would be massively appreciated, I have no idea why this error has started happening.",
      "solution": "Okay, I figured it out. Redis >= 3, \nSee https://github.com/rails/rails/blob/v6.1.7/actioncable/lib/action_cable/subscription_adapter/redis.rb at the top of the file.\nI wonder why so little people seem to encounter this issue. Anyway, the solution should be to either upgrade to Rails >= 7.0.4 (which is rather drastic) or to downgrade the redis gem to \nYou said you already did the latter, but I guess something went wrong there.",
      "question_score": 7,
      "answer_score": 15,
      "created_at": "2022-09-22T03:00:51",
      "url": "https://stackoverflow.com/questions/73808301/rails-error-cant-activate-redis-3-5-already-activated-redis-5-0-4"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 66932042,
      "title": "Heroku Redis gets a wrong version number error in logs",
      "problem": "I am using Heroku's Redis add-on. I upgraded yesterday to a higher tier and now I'm getting this when I send requests to my server. Any suggestions on what this error log means?\n```\n`Apr 03 07:00:24 myapp app/redis-flexible-99415 Error accepting a client connection: error:1408F10B:SSL routines:ssl3_get_record:wrong version number (conn: fd=12)\n`\n```\nI am connecting to Redis as so:\n```\n`import Redis from \"ioredis\";\nimport { Job, Queue, Worker } from \"bullmq\";\n\nconst client = new Redis(process.env.REDIS_URL, {\n    connectTimeout: 30000,\n    tls: {\n        rejectUnauthorized: false,\n    },\n});\n...\n`\n```",
      "solution": "Heroku shipped new updates related to Redis as outlined in their changelog here: https://devcenter.heroku.com/changelog-items/1952.\nThe New Redis addons without a version specified will now default to 6.0 version, as it is secured with the self-signed certificate because of its built-in TLS for production plans. You'll need to add `ssl_params: { verify_mode: OpenSSL::SSL::VERIFY_NONE }` to handle the self-signed certificate. Please refer here for additional details: https://devcenter.heroku.com/articles/securing-heroku-redis\nYou may also work around this behavior by downgrading to Redis 5 version. You can do it by running(using --version flag): `heroku addons:create heroku-redis:premium-2 --version 5 -a `. More details here: https://devcenter.heroku.com/articles/heroku-redis#version-support-and-legacy-infrastructure.",
      "question_score": 7,
      "answer_score": 14,
      "created_at": "2021-04-03T16:03:49",
      "url": "https://stackoverflow.com/questions/66932042/heroku-redis-gets-a-wrong-version-number-error-in-logs"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 65983358,
      "title": "Delete Redis keys matching a pattern",
      "problem": "I am using Redis hash set to store data in the following format:\n```\n`hset b1.b2.b3 name test\n`\n```\nNow I want to delete this key so I am using the following format:\n```\n`del b1.b2.*\n`\n```\nBut it not working so how I delete the Redis key using a pattern?",
      "solution": "Redis does not provide any method to delete bulk keys. But redis-cli along with xargs can be used to achieve what you are trying to do. See the commands below:\n```\n`127.0.0.1:6379> hset b1.b2.b3 name test\n(integer) 1\n127.0.0.1:6379> hgetall b1.b2.b3\n1) \"name\"\n2) \"test\"\n$ redis-cli --scan --pattern b1.b2.* | xargs redis-cli del\n(integer) 1\n$ redis-cli\n127.0.0.1:6379> hgetall b1.b2.b3\n(empty list or set)\n`\n```\nWe are scanning redis for a pattern using '--scan' and the output is given to redis-cli again using the xargs method whcih combines all the keys in the scan result and finally we delete all of them using 'del' command.",
      "question_score": 7,
      "answer_score": 13,
      "created_at": "2021-01-31T20:32:37",
      "url": "https://stackoverflow.com/questions/65983358/delete-redis-keys-matching-a-pattern"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 71785724,
      "title": "GCP Memorystore Redis: Protocol error, got &quot;\\x15&quot; as reply type byte",
      "problem": "I have been researching this Redis error for days now...\nI created a GCP Memorystore Redis instance and received the following internal IP endpoint:\n```\n`10.xxx.xxx.xxx:6378\n`\n```\nI created a small GCE instance and made sure that zone matched the Redis instance:\n```\n`us-central1-f\n`\n```\n\nHowever, I when I ssh into the VM, connect to Redis, and issue a PING, I get the following error response\n```\n`Protocol error, got \"\\x15\" as reply type byte\n`\n```\n\nAny idea why I am getting this error?",
      "solution": "If the Redis instance was configured with an AUTH string and/or TLS encryption, you would need to pass these credentials when connecting to it.\nI received the same error when using your command to connect to my encrypted instance. This can be done in two ways as far as I tested (from a GCE instance in the same VPC as Redis):\n1. Using the redis-cli, you could use the following command to authenticate (see here for information on the flags used in the command):\n```\n`redis-cli -h  -p  -a  --tls --cacert \n`\n```\nNote: the certificate file would need to be installed to your VM.\n2. The GCP Memorystore documentation recommends using telnet and Stunnel to connect to a secured and encrypted instance.\nSomething not included in the documentation is that after running `telnet localhost 6378` in step 4, you would need to pass the AUTH string in the telnet console:\n```\n`AUTH \n+OK\nPING\n+PONG\n`\n```\nAfter that, you can PING the instance in step 5, skipping sending the AUTH string will return this error: `-NOAUTH Authentication required`. Besides that, you should follow as documented the rest of the steps.",
      "question_score": 7,
      "answer_score": 9,
      "created_at": "2022-04-07T18:22:51",
      "url": "https://stackoverflow.com/questions/71785724/gcp-memorystore-redis-protocol-error-got-x15-as-reply-type-byte"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 71074098,
      "title": "ioredis connection keeps resetting when connecting to local redis cluster from docker container",
      "problem": "I have a docker compose containerized client/server node app that is failing to create a stable connection to a redis cluster I have running on my local environment. The redis cluster has 6 nodes (3 master, 3 replica configuration) running on my local machine. Every time I start my app and attempt to connect to redis, the `connect` event is spammed and I get the following error on my client:\n```\n`Proxy error: Could not proxy request /check-login from localhost:3000 to http://server.\nSee https://nodejs.org/api/errors.html#errors_common_system_errors for more information (ECONNRESET)\n`\n```\nI have made sure to configure the redis cluster to have the settings `protected-mode` to `no` and `bind` to `0.0.0.0` to allow remote access to the cluster. I have confirmed that I can access the cluster locally by pinging one of the cluster nodes `redis-cli -h 127.0.0.1 -p 30001`:\n```\n`127.0.0.1:30001> ping\nPONG\n127.0.0.1:30001> exit\n`\n```\nI am setting my `REDIS_HOSTS` environment to be `\"host.docker.internal:30001,host.docker.internal:30005,host.docker.internal:30006\"`. This should allow me to connect to my redis cluster running at `127.0.0.1` on my host machine. My node application code:\n```\n`const Redis = require(\"ioredis\");\n\nconst hosts = process.env.REDIS_HOSTS.split(\",\").map((connection) => {\n  const [host, port] = connection.split(\":\");\n\n  return {\n    host,\n    port,\n  };\n});\n\nconst client = new Redis.Cluster(hosts, {\n  enableAutoPipelining: true,\n  slotsRefreshTimeout: 100000,\n});\n\nclient.on(\"error\", (error) => {\n  console.log(\"redis connection failed: \", error);\n});\n\nclient.on(\"connect\", () => {\n  console.log(\"redis connection established\");\n});\n\nmodule.exports = client;\n`\n```\nMy ioredis logs:\n```\n`2022-02-11T23:52:09.970Z ioredis:cluster status: [empty] -> connecting\ninfo: Listening on port 80\n2022-02-11T23:52:15.449Z ioredis:cluster resolved hostname host.docker.internal to IP 192.168.65.2\n2022-02-11T23:52:15.476Z ioredis:cluster:connectionPool Reset with [\n  { host: '192.168.65.2', port: 30001 },\n  { host: '192.168.65.2', port: 30002 },\n  { host: '192.168.65.2', port: 30003 }\n]\n2022-02-11T23:52:15.482Z ioredis:cluster:connectionPool Connecting to 192.168.65.2:30001 as master\n2022-02-11T23:52:15.504Z ioredis:redis status[192.168.65.2:30001]: [empty] -> wait\n2022-02-11T23:52:15.511Z ioredis:cluster:connectionPool Connecting to 192.168.65.2:30002 as master\n2022-02-11T23:52:15.517Z ioredis:redis status[192.168.65.2:30002]: [empty] -> wait\n2022-02-11T23:52:15.519Z ioredis:cluster:connectionPool Connecting to 192.168.65.2:30003 as master\n2022-02-11T23:52:15.521Z ioredis:redis status[192.168.65.2:30003]: [empty] -> wait\n2022-02-11T23:52:15.530Z ioredis:cluster getting slot cache from 192.168.65.2:30002\n2022-02-11T23:52:15.541Z ioredis:redis status[192.168.65.2:30002 (ioredis-cluster(refresher))]: [empty] -> wait\n2022-02-11T23:52:15.590Z ioredis:redis status[192.168.65.2:30002 (ioredis-cluster(refresher))]: wait -> connecting\n2022-02-11T23:52:15.603Z ioredis:redis queue command[192.168.65.2:30002 (ioredis-cluster(refresher))]: 0 -> cluster([ 'slots' ])\n2022-02-11T23:52:15.614Z ioredis:cluster:subscriber selected a subscriber 192.168.65.2:30001\n2022-02-11T23:52:15.621Z ioredis:redis status[192.168.65.2:30001 (ioredis-cluster(subscriber))]: [empty] -> wait\n2022-02-11T23:52:15.622Z ioredis:cluster:subscriber started\n2022-02-11T23:52:15.734Z ioredis:redis status[192.168.65.2:30002 (ioredis-cluster(refresher))]: connecting -> connect\n2022-02-11T23:52:15.737Z ioredis:redis status[192.168.65.2:30002 (ioredis-cluster(refresher))]: connect -> ready\n2022-02-11T23:52:15.739Z ioredis:connection set the connection name [ioredis-cluster(refresher)]\n2022-02-11T23:52:15.742Z ioredis:redis write command[192.168.65.2:30002 (ioredis-cluster(refresher))]: 0 -> client([ 'setname', 'ioredis-cluster(refresher)' ])\n2022-02-11T23:52:15.749Z ioredis:connection send 1 commands in offline queue\n2022-02-11T23:52:15.750Z ioredis:redis write command[192.168.65.2:30002 (ioredis-cluster(refresher))]: 0 -> cluster([ 'slots' ])\n2022-02-11T23:52:15.781Z ioredis:cluster cluster slots result count: 3\n2022-02-11T23:52:15.783Z ioredis:cluster cluster slots result [0]: slots 0~5460 served by [ '127.0.0.1:30001', '127.0.0.1:30004' ]\n2022-02-11T23:52:15.788Z ioredis:cluster cluster slots result [1]: slots 5461~10922 served by [ '127.0.0.1:30002', '127.0.0.1:30005' ]\n2022-02-11T23:52:15.792Z ioredis:cluster cluster slots result [2]: slots 10923~16383 served by [ '127.0.0.1:30003', '127.0.0.1:30006' ]\n2022-02-11T23:52:15.849Z ioredis:cluster:connectionPool Reset with [\n  { host: '127.0.0.1', port: 30001, readOnly: false },\n  { host: '127.0.0.1', port: 30004, readOnly: true },\n  { host: '127.0.0.1', port: 30002, readOnly: false },\n  { host: '127.0.0.1', port: 30005, readOnly: true },\n  { host: '127.0.0.1', port: 30003, readOnly: false },\n  { host: '127.0.0.1', port: 30006, readOnly: true }\n]\n2022-02-11T23:52:15.850Z ioredis:cluster:connectionPool Disconnect 192.168.65.2:30001 because the node does not hold any slot\n2022-02-11T23:52:15.851Z ioredis:redis status[192.168.65.2:30001]: wait -> close\n2022-02-11T23:52:15.851Z ioredis:connection skip reconnecting since the connection is manually closed.\n2022-02-11T23:52:15.852Z ioredis:redis status[192.168.65.2:30001]: close -> end\n2022-02-11T23:52:15.857Z ioredis:cluster:connectionPool Remove 192.168.65.2:30001 from the pool\n2022-02-11T23:52:15.858Z ioredis:cluster:connectionPool Disconnect 192.168.65.2:30002 because the node does not hold any slot\n2022-02-11T23:52:15.858Z ioredis:redis status[192.168.65.2:30002]: wait -> close\n2022-02-11T23:52:15.859Z ioredis:connection skip reconnecting since the connection is manually closed.\n2022-02-11T23:52:15.859Z ioredis:redis status[192.168.65.2:30002]: close -> end\n2022-02-11T23:52:15.861Z ioredis:cluster:connectionPool Remove 192.168.65.2:30002 from the pool\n2022-02-11T23:52:15.861Z ioredis:cluster:connectionPool Disconnect 192.168.65.2:30003 because the node does not hold any slot\n2022-02-11T23:52:15.861Z ioredis:redis status[192.168.65.2:30003]: wait -> close\n2022-02-11T23:52:15.865Z ioredis:connection skip reconnecting since the connection is manually closed.\n2022-02-11T23:52:15.866Z ioredis:redis status[192.168.65.2:30003]: close -> end\n2022-02-11T23:52:15.866Z ioredis:cluster:connectionPool Remove 192.168.65.2:30003 from the pool\n2022-02-11T23:52:15.867Z ioredis:cluster:connectionPool Connecting to 127.0.0.1:30001 as master\n2022-02-11T23:52:15.869Z ioredis:redis status[127.0.0.1:30001]: [empty] -> wait\n2022-02-11T23:52:15.871Z ioredis:cluster:connectionPool Connecting to 127.0.0.1:30004 as slave\n2022-02-11T23:52:15.873Z ioredis:redis status[127.0.0.1:30004]: [empty] -> wait\n2022-02-11T23:52:15.874Z ioredis:cluster:connectionPool Connecting to 127.0.0.1:30002 as master\n2022-02-11T23:52:15.877Z ioredis:redis status[127.0.0.1:30002]: [empty] -> wait\n2022-02-11T23:52:15.877Z ioredis:cluster:connectionPool Connecting to 127.0.0.1:30005 as slave\n2022-02-11T23:52:15.882Z ioredis:redis status[127.0.0.1:30005]: [empty] -> wait\n2022-02-11T23:52:15.883Z ioredis:cluster:connectionPool Connecting to 127.0.0.1:30003 as master\n2022-02-11T23:52:15.885Z ioredis:redis status[127.0.0.1:30003]: [empty] -> wait\n2022-02-11T23:52:15.886Z ioredis:cluster:connectionPool Connecting to 127.0.0.1:30006 as slave\n2022-02-11T23:52:15.887Z ioredis:redis status[127.0.0.1:30006]: [empty] -> wait\n2022-02-11T23:52:15.893Z ioredis:cluster status: connecting -> connect\n2022-02-11T23:52:15.904Z ioredis:redis status[127.0.0.1:30002]: wait -> connecting\n2022-02-11T23:52:15.906Z ioredis:redis queue command[127.0.0.1:30002]: 0 -> cluster([ 'info' ])\n2022-02-11T23:52:15.916Z ioredis:cluster:subscriber subscriber has left, selecting a new one...\n2022-02-11T23:52:15.917Z ioredis:redis status[192.168.65.2:30001 (ioredis-cluster(subscriber))]: wait -> close\n2022-02-11T23:52:15.918Z ioredis:connection skip reconnecting since the connection is manually closed.\n2022-02-11T23:52:15.918Z ioredis:redis status[192.168.65.2:30001 (ioredis-cluster(subscriber))]: close -> end\n2022-02-11T23:52:15.919Z ioredis:cluster:subscriber selected a subscriber 127.0.0.1:30004\n2022-02-11T23:52:15.921Z ioredis:redis status[127.0.0.1:30004 (ioredis-cluster(subscriber))]: [empty] -> wait\n2022-02-11T23:52:16.000Z ioredis:connection error: { Error: connect ECONNREFUSED 127.0.0.1:30002\n    at TCPConnectWrap.afterConnect [as oncomplete] (net.js:1148:16)\n  errno: -111,\n  code: 'ECONNREFUSED',\n  syscall: 'connect',\n  address: '127.0.0.1',\n  port: 30002 }\n2022-02-11T23:52:16.030Z ioredis:redis status[127.0.0.1:30002]: connecting -> close\n2022-02-11T23:52:16.031Z ioredis:connection skip reconnecting because `retryStrategy` is not a function\n2022-02-11T23:52:16.032Z ioredis:redis status[127.0.0.1:30002]: close -> end\n2022-02-11T23:52:16.034Z ioredis:cluster:connectionPool Remove 127.0.0.1:30002 from the pool\n2022-02-11T23:52:16.036Z ioredis:cluster Ready check failed (Error: Connection is closed.\n    at close (/usr/src/app/node_modules/ioredis/built/redis/event_handler.js:184:25)\n    at Socket. (/usr/src/app/node_modules/ioredis/built/redis/event_handler.js:155:20)\n    at Object.onceWrapper (events.js:520:26)\n    at Socket.emit (events.js:400:28)\n    at Socket.emit (domain.js:470:12)\n    at TCP. (net.js:675:12)). Reconnecting...\n2022-02-11T23:52:16.042Z ioredis:cluster status: connect -> disconnecting\n...\n`\n```",
      "solution": "The clue to the solution was found in the following log snippet:\n```\n`2022-02-11T23:52:15.750Z ioredis:redis write command[192.168.65.2:30002 (ioredis-cluster(refresher))]: 0 -> cluster([ 'slots' ])\n2022-02-11T23:52:15.781Z ioredis:cluster cluster slots result count: 3\n2022-02-11T23:52:15.783Z ioredis:cluster cluster slots result [0]: slots 0~5460 served by [ '127.0.0.1:30001', '127.0.0.1:30004' ]\n2022-02-11T23:52:15.788Z ioredis:cluster cluster slots result [1]: slots 5461~10922 served by [ '127.0.0.1:30002', '127.0.0.1:30005' ]\n2022-02-11T23:52:15.792Z ioredis:cluster cluster slots result [2]: slots 10923~16383 served by [ '127.0.0.1:30003', '127.0.0.1:30006' ]\n`\n```\nThe internal redis cluster network was still communicating between nodes on network address `127.0.0.1`, creating the `connect ECONNREFUSED` errors when the `ioredis` client attempted to use those network mappings to establish the cluster connection.\nI had to use the `natMap` option in the `ioredis` client to remap the internal cluster network connections to the network address of the docker container:\n```\n`let natMap = {};\nconst localHost = \"127.0.0.1\";\n\nconst hosts = process.env.REDIS_HOSTS.split(\",\").map((connection) => {\n  const [host, port] = connection.split(\":\");\n\n  // assign nat host address mappings\n  // when accessing local redis cluster from containerized network\n  natMap[`${localHost}:${port}`] = { host, port };\n\n  return {\n    host,\n    port,\n  };\n});\n\n// natMap output\n// {\n//   \"127.0.0.1:30001\": { host: \"docker.internal.network\", port: 30001 },\n//   \"127.0.0.1:30002\": { host: \"docker.internal.network\", port: 30002 },\n//   \"127.0.0.1:30003\": { host: \"docker.internal.network\", port: 30003 },\n// }\n\n// create redis cluster client\nconst client = new Redis.Cluster(hosts, {\n  enableAutoPipelining: true,\n  slotsRefreshTimeout: 100000,\n  natMap,\n});\n`\n```",
      "question_score": 7,
      "answer_score": 9,
      "created_at": "2022-02-11T01:48:53",
      "url": "https://stackoverflow.com/questions/71074098/ioredis-connection-keeps-resetting-when-connecting-to-local-redis-cluster-from-d"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 69984241,
      "title": "what is the redis headless service used for in kubernetes cluster",
      "problem": "Today I am facing a problem that the redis in the kubernetes cluster shows error like this:\n```\n`2021-11-16T05:32:18 [INFO] - increment delete failed...,MobcError(RedisCMDError(An error was signalled by the server: You can't write against a read only replica.))\n`\n```\nI check my redis config and found that the config url like this:\n```\n`redisConnectionStr=\"redis://default:password@cruise-redis-headless.reddwarf-cache.svc.cluster.local:6379/3\"\n`\n```\nI check the servcie and found it mapped to the redis cluster master and replica, I was wonder the headness service user for what situation? why not only keep the master servcie and replica servcie? If read operate, use any of the servcie would be fine. If write, only use the master service. when should i use the headness service?",
      "solution": "why not only keep the master service and replica servcie?\n\nYou can do it, if you could implement it by your own.\nAs redis helm maybe dont use specific labels to run Read/Write replicas so it's hard to divert and create the different services for read & write.\nIf you are deploying the Redis using the helm it will create the Two services into the K8s cluster\n\nHeadless service\n\nClusterIP service\n\nIn the your application code side, you should be using the normal clusterIP service.\nSo idea is headless return the pod replicas IPs which will be used further by the Redis internally to manage the cluster if you are using the Redis sentinel or Redis cluster.\nIf you are using the ClusterIP service it will auto route the traffic to Read/Write replicas based on the condition.\nIf you are not using sentinel or cluster simply use ClusterIP and you are good to go.\nSentinel details\nIf you are using the Sentinel feature, you can ping the sentinel service and it will return master and read replicas IP just like API response, then connect to specific.\nInside application code you can use as per requirement now as you are getting both write and read IP with sentinel.\nInside the helm chart you can see one configuration\n```\n`sentinel:\n  enabled: true\n`\n```\nIn the bitnami docs they explicitly mention This command will return the address of the current master, which can be accessed from inside the cluster..\nRead more at : https://github.com/bitnami/charts/tree/master/bitnami/redis#master-replicas-with-sentinel",
      "question_score": 7,
      "answer_score": 9,
      "created_at": "2021-11-16T06:39:27",
      "url": "https://stackoverflow.com/questions/69984241/what-is-the-redis-headless-service-used-for-in-kubernetes-cluster"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 67484542,
      "title": "Redis command timed out; nested exception is io.lettuce.core.RedisCommandTimeoutException: Command timed out after 1 minute(s)",
      "problem": "Today when I using Java redis client(`spring-data-redis-2.3.9.RELEASE.jar`) to consume Redis stream message encount this error:\n```\n`2021-05-11 17:49:42.134 ERROR 26301 --- [-post-service-1] c.d.s.p.common.mq.StreamConsumerRunner   : pull message error\n\norg.springframework.dao.QueryTimeoutException: Redis command timed out; nested exception is io.lettuce.core.RedisCommandTimeoutException: Command timed out after 1 minute(s)\n    at org.springframework.data.redis.connection.lettuce.LettuceExceptionConverter.convert(LettuceExceptionConverter.java:70) ~[spring-data-redis-2.3.9.RELEASE.jar!/:2.3.9.RELEASE]\n    at org.springframework.data.redis.connection.lettuce.LettuceExceptionConverter.convert(LettuceExceptionConverter.java:41) ~[spring-data-redis-2.3.9.RELEASE.jar!/:2.3.9.RELEASE]\n    at org.springframework.data.redis.PassThroughExceptionTranslationStrategy.translate(PassThroughExceptionTranslationStrategy.java:44) ~[spring-data-redis-2.3.9.RELEASE.jar!/:2.3.9.RELEASE]\n    at org.springframework.data.redis.FallbackExceptionTranslationStrategy.translate(FallbackExceptionTranslationStrategy.java:42) ~[spring-data-redis-2.3.9.RELEASE.jar!/:2.3.9.RELEASE]\n    at org.springframework.data.redis.connection.lettuce.LettuceConnection.convertLettuceAccessException(LettuceConnection.java:275) ~[spring-data-redis-2.3.9.RELEASE.jar!/:2.3.9.RELEASE]\n    at org.springframework.data.redis.connection.lettuce.LettuceStreamCommands.convertLettuceAccessException(LettuceStreamCommands.java:712) ~[spring-data-redis-2.3.9.RELEASE.jar!/:2.3.9.RELEASE]\n    at org.springframework.data.redis.connection.lettuce.LettuceStreamCommands.xReadGroup(LettuceStreamCommands.java:602) ~[spring-data-redis-2.3.9.RELEASE.jar!/:2.3.9.RELEASE]\n    at org.springframework.data.redis.connection.DefaultedRedisConnection.xReadGroup(DefaultedRedisConnection.java:591) ~[spring-data-redis-2.3.9.RELEASE.jar!/:2.3.9.RELEASE]\n    at org.springframework.data.redis.core.DefaultStreamOperations$4.inRedis(DefaultStreamOperations.java:310) ~[spring-data-redis-2.3.9.RELEASE.jar!/:2.3.9.RELEASE]\n    at org.springframework.data.redis.core.DefaultStreamOperations$RecordDeserializingRedisCallback.doInRedis(DefaultStreamOperations.java:376) ~[spring-data-redis-2.3.9.RELEASE.jar!/:2.3.9.RELEASE]\n    at org.springframework.data.redis.core.DefaultStreamOperations$RecordDeserializingRedisCallback.doInRedis(DefaultStreamOperations.java:371) ~[spring-data-redis-2.3.9.RELEASE.jar!/:2.3.9.RELEASE]\n    at org.springframework.data.redis.core.RedisTemplate.execute(RedisTemplate.java:228) ~[spring-data-redis-2.3.9.RELEASE.jar!/:2.3.9.RELEASE]\n    at org.springframework.data.redis.core.RedisTemplate.execute(RedisTemplate.java:188) ~[spring-data-redis-2.3.9.RELEASE.jar!/:2.3.9.RELEASE]\n    at org.springframework.data.redis.core.AbstractOperations.execute(AbstractOperations.java:96) ~[spring-data-redis-2.3.9.RELEASE.jar!/:2.3.9.RELEASE]\n    at org.springframework.data.redis.core.DefaultStreamOperations.read(DefaultStreamOperations.java:305) ~[spring-data-redis-2.3.9.RELEASE.jar!/:2.3.9.RELEASE]\n    at org.springframework.data.redis.stream.DefaultStreamMessageListenerContainer.lambda$getReadFunction$3(DefaultStreamMessageListenerContainer.java:236) ~[spring-data-redis-2.3.9.RELEASE.jar!/:2.3.9.RELEASE]\n    at org.springframework.data.redis.stream.StreamPollTask.doLoop(StreamPollTask.java:138) ~[spring-data-redis-2.3.9.RELEASE.jar!/:2.3.9.RELEASE]\n    at org.springframework.data.redis.stream.StreamPollTask.run(StreamPollTask.java:123) ~[spring-data-redis-2.3.9.RELEASE.jar!/:2.3.9.RELEASE]\n    at misc.config.async.pool.MdcTaskDecorator.lambda$decorate$0(MdcTaskDecorator.java:29) ~[dolphin-common-1.0.0-SNAPSHOT.jar!/:na]\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) ~[na:na]\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) ~[na:na]\n    at java.base/java.lang.Thread.run(Thread.java:834) ~[na:na]\nCaused by: io.lettuce.core.RedisCommandTimeoutException: Command timed out after 1 minute(s)\n    at io.lettuce.core.internal.ExceptionFactory.createTimeoutException(ExceptionFactory.java:53) ~[lettuce-core-6.1.1.RELEASE.jar!/:6.1.1.RELEASE]\n    at io.lettuce.core.internal.Futures.awaitOrCancel(Futures.java:246) ~[lettuce-core-6.1.1.RELEASE.jar!/:6.1.1.RELEASE]\n    at io.lettuce.core.FutureSyncInvocationHandler.handleInvocation(FutureSyncInvocationHandler.java:75) ~[lettuce-core-6.1.1.RELEASE.jar!/:6.1.1.RELEASE]\n    at io.lettuce.core.internal.AbstractInvocationHandler.invoke(AbstractInvocationHandler.java:80) ~[lettuce-core-6.1.1.RELEASE.jar!/:6.1.1.RELEASE]\n    at com.sun.proxy.$Proxy149.xreadgroup(Unknown Source) ~[na:na]\n    at org.springframework.data.redis.connection.lettuce.LettuceStreamCommands.xReadGroup(LettuceStreamCommands.java:600) ~[spring-data-redis-2.3.9.RELEASE.jar!/:2.3.9.RELEASE]\n    ... 15 common frames omitted\n`\n```\nFor the beginning the message could consume by the client successfully, some message could be consumed and handle correctly, after a while it will encount the time out problem. this is my code:\n```\n`package com.dolphin.soa.post.common.mq;\n\nimport lombok.extern.slf4j.Slf4j;\nimport org.springframework.beans.factory.DisposableBean;\nimport org.springframework.beans.factory.annotation.Value;\nimport org.springframework.boot.ApplicationArguments;\nimport org.springframework.boot.ApplicationRunner;\nimport org.springframework.data.redis.connection.RedisConnectionFactory;\nimport org.springframework.data.redis.connection.stream.Consumer;\nimport org.springframework.data.redis.connection.stream.MapRecord;\nimport org.springframework.data.redis.connection.stream.ReadOffset;\nimport org.springframework.data.redis.connection.stream.StreamOffset;\nimport org.springframework.data.redis.core.StringRedisTemplate;\nimport org.springframework.data.redis.serializer.StringRedisSerializer;\nimport org.springframework.data.redis.stream.StreamMessageListenerContainer;\nimport org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor;\nimport org.springframework.stereotype.Component;\nimport org.springframework.util.ErrorHandler;\n\nimport java.time.Duration;\n\n/**\n * @author dolphin\n */\n@Component\n@Slf4j\npublic class StreamConsumerRunner implements ApplicationRunner, DisposableBean {\n\n    @Value(\"${dolphin.redis.stream.consumer}\")\n    private String consumer;\n\n    @Value(\"${dolphin.redis.stream.group}\")\n    private String groupName;\n\n    private final RedisConnectionFactory redisConnectionFactory;\n\n    private final ThreadPoolTaskExecutor threadPoolTaskExecutor;\n\n    private final StreamMessageListener streamMessageListener;\n\n    private final StringRedisTemplate stringRedisTemplate;\n\n    private StreamMessageListenerContainer> streamMessageListenerContainer;\n\n    public StreamConsumerRunner(RedisConnectionFactory redisConnectionFactory,\n                                ThreadPoolTaskExecutor threadPoolTaskExecutor,\n                                StreamMessageListener streamMessageListener,\n                                StringRedisTemplate stringRedisTemplate) {\n        this.redisConnectionFactory = redisConnectionFactory;\n        this.threadPoolTaskExecutor = threadPoolTaskExecutor;\n        this.streamMessageListener = streamMessageListener;\n        this.stringRedisTemplate = stringRedisTemplate;\n    }\n\n    @Override\n    public void run(ApplicationArguments args) {\n        StreamMessageListenerContainer.StreamMessageListenerContainerOptions> streamMessageListenerContainerOptions = StreamMessageListenerContainer.StreamMessageListenerContainerOptions\n                .builder()\n                .batchSize(1)\n                .executor(this.threadPoolTaskExecutor)\n                .errorHandler(new ErrorHandler() {\n                    @Override\n                    public void handleError(Throwable t) {\n                        log.error(\"pull message error\",t);\n                    }\n                })\n                .pollTimeout(Duration.ofMinutes(1L))\n                .serializer(new StringRedisSerializer())\n                .build();\n        StreamMessageListenerContainer> streamMessageListenerContainer = StreamMessageListenerContainer\n                .create(this.redisConnectionFactory, streamMessageListenerContainerOptions);\n        streamMessageListenerContainer.receive(Consumer.from(groupName, consumer),\n                StreamOffset.create(consumer, ReadOffset.lastConsumed()), this.streamMessageListener);\n        this.streamMessageListenerContainer = streamMessageListenerContainer;\n        this.streamMessageListenerContainer.start();\n    }\n\n    @Override\n    public void destroy() throws Exception {\n        this.streamMessageListenerContainer.stop();\n    }\n}\n`\n```\nand this is the `StreamMessageListener`:\n```\n`package com.dolphin.soa.post.common.mq;\n\nimport com.dolphin.soa.post.contract.request.ArticleRequest;\nimport com.dolphin.soa.post.model.entity.SubRelation;\nimport com.dolphin.soa.post.service.IArticleService;\nimport com.dolphin.soa.post.service.ISubRelationService;\nimport lombok.extern.slf4j.Slf4j;\nimport org.apache.commons.collections4.CollectionUtils;\nimport org.apache.commons.collections4.MapUtils;\nimport org.springframework.beans.factory.annotation.Qualifier;\nimport org.springframework.beans.factory.annotation.Value;\nimport org.springframework.data.redis.connection.stream.MapRecord;\nimport org.springframework.data.redis.connection.stream.RecordId;\nimport org.springframework.data.redis.core.DefaultTypedTuple;\nimport org.springframework.data.redis.core.RedisTemplate;\nimport org.springframework.data.redis.core.StringRedisTemplate;\nimport org.springframework.data.redis.core.ZSetOperations;\nimport org.springframework.data.redis.stream.StreamListener;\nimport org.springframework.stereotype.Component;\n\nimport java.util.Arrays;\nimport java.util.HashSet;\nimport java.util.List;\nimport java.util.Map;\n\n/**\n * @author dolphin\n */\n@Component\n@Slf4j\npublic class StreamMessageListener implements StreamListener> {\n\n    @Value(\"${dolphin.redis.stream.group}\")\n    private String groupName;\n\n    @Value(\"${dolphin.redis.user.sub.article.key}\")\n    private String subArticleKey;\n\n    private final StringRedisTemplate stringRedisTemplate;\n\n    private final RedisTemplate articleRedisTemplate;\n\n    private final RedisTemplate redisLongTemplate;\n\n    private final ISubRelationService subRelationService;\n\n    private final IArticleService articleService;\n\n    public StreamMessageListener(StringRedisTemplate stringRedisTemplate,\n                                 @Qualifier(\"redisObjectTemplate\") RedisTemplate articleRedisTemplate,\n                                 ISubRelationService subRelationService,\n                                 @Qualifier(\"redisLongTemplate\") RedisTemplate redisLongTemplate,\n                                 IArticleService articleService) {\n        this.stringRedisTemplate = stringRedisTemplate;\n        this.articleRedisTemplate = articleRedisTemplate;\n        this.subRelationService = subRelationService;\n        this.redisLongTemplate = redisLongTemplate;\n        this.articleService = articleService;\n    }\n\n    @Override\n    public void onMessage(MapRecord message) {\n        try {\n            RecordId messageId = message.getId();\n            Map body = message.getValue();\n            log.info(\"stream message\u3002messageId={}, stream={}, body={}\", messageId, message.getStream(), body);\n            handleArticle(body);\n            this.stringRedisTemplate.opsForStream().acknowledge(groupName, message);\n        } catch (Exception e) {\n            log.error(\"error\", e);\n        }\n    }\n}\n`\n```\nI am searching from internet and find no useful suggestions. I have tried to upgrade the lettuce package version to newest `6.1.1.RELEASE` version but still not fix it. Where is the problem and  What should I do to make it work as expect? I also tried replace `lettuce` with `jedis`:\n```\n`api (\"org.springframework.boot:spring-boot-starter-data-redis\") {\n            exclude group: \"io.lettuce\", module: \"lettuce-core\"\n}\napi \"redis.clients:jedis:3.6.0\"\n`\n```\nseems it did not work.",
      "solution": "I tried to using jedis not using lettuce solve this problem.\nstep 1: exclude lettuce:\n```\n`api (\"org.springframework.boot:spring-boot-starter-data-redis\") {\n            exclude group: \"io.lettuce\", module: \"lettuce-core\"\n            exclude group: \"org.springframework.data\", module: \"spring-data-redis\"\n        }\n`\n```\nbecause the spring-boot-starter-data-redis contains both `jedis` and `lettuce`, if you exclude `lettuce`, the default connection should use `jedis`.",
      "question_score": 7,
      "answer_score": 1,
      "created_at": "2021-05-11T12:02:57",
      "url": "https://stackoverflow.com/questions/67484542/redis-command-timed-out-nested-exception-is-io-lettuce-core-rediscommandtimeout"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 70799637,
      "title": "AWS Redis Cluster MOVED Error using redis node library",
      "problem": "I have created a Redis MemoryDB cluster with 2 nodes in AWS:\n\nI connect to it using redis node library v4.0.0 like this:\n```\n`import { createCluster } from 'redis';\n(async () => {\n    const REDIS_USERNAME = 'test-username';\n    const REDIS_PASSWORD = 'test-pass';\n    const cluster = createCluster({\n        rootNodes: [\n            {\n                url: `rediss://node1.amazonaws.com:6379`,\n            },\n            {\n                url: `rediss://node2.amazonaws.com:6379`,\n            },\n        ],\n        defaults: {\n            url: `rediss://cluster.amazonaws.com:6379`,\n            username: REDIS_USERNAME,\n            password: REDIS_PASSWORD,\n        }\n    });\n    cluster.on('error', (err) => console.log('Redis Cluster Error', err));\n    await cluster.connect();\n    console.log('connected to cluster...');\n    await cluster.set('key', 'value');\n    const value = await cluster.get('key');\n    console.log('Value', value);\n    await cluster.disconnect();\n})();\n`\n```\nBut sometimes I get the error `ReplyError: MOVED 12539 rediss://node2.amazonaws.com:6379` and I cannot get the value from the key.\nDo you have any idea if there is something wrong with the configuration of the cluster or with the code using redis node library?\nEdit:\nI tried it with ioredis library and it works, so it's something wrong with the redis library.\nNode.js Version: 16\nRedis Server Version: 6",
      "solution": "I had created an issue to redis library, so it's going to be solved soon with this PR.",
      "question_score": 7,
      "answer_score": 2,
      "created_at": "2022-01-21T11:13:12",
      "url": "https://stackoverflow.com/questions/70799637/aws-redis-cluster-moved-error-using-redis-node-library"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 72803152,
      "title": "Django Celery Redis",
      "problem": "I have this error `A rediss:// URL must have parameter ssl_cert_reqs and this must be set to CERT_REQUIRED, CERT_OPTIONAL, or CERT_NONE`\nsettings\n```\n`CELERY_BROKER_URL = os.environ.get('REDIS_URL', \"redis://localhost:6379\")\nCELERY_RESULT_BACKEND = os.environ.get('REDIS_URL', \"redis://localhost:6379\")\nCELERY_ACCEPT_CONTENT = ['application/json']\nCELERY_TASK_SERIALIZER = 'json'\nCELERY_RESULT_SERIALIZER = 'json'\n`\n```\ncelery.py\n```\n`from __future__ import absolute_import, unicode_literals\nfrom celery import Celery\nimport os, ssl\n\nos.environ.setdefault('DJANGO_SETTINGS_MODULE', 'myproj.settings')\n\napp = Celery(\n'myproj',\nbroker_use_ssl = {\n    'ssl_cert_reqs': ssl.CERT_NONE\n})\n\napp.config_from_object('django.conf:settings', namespace=\"CELERY\")\n\napp.autodiscover_tasks()\n`\n```\nI changed the value of ssl_cert_reqs to different value 'none', 'cert_required', ..., but nothing, always the same error when I use rediss:// instead of redis://.",
      "solution": "i never used option `broker_use_ssl`, can you try delete this option and try again\n`app = Celery('myproj')\n`\nor\nupdate\n` Celery('myproj',\n     broker_use_ssl = {\n        'ssl_cert_reqs': ssl.CERT_NONE\n     },\n     redis_backend_use_ssl = {\n        'ssl_cert_reqs': ssl.CERT_NONE\n     }\n)\n`",
      "question_score": 6,
      "answer_score": 16,
      "created_at": "2022-06-29T16:30:47",
      "url": "https://stackoverflow.com/questions/72803152/django-celery-redis"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 75399597,
      "title": "Redis Docker compose Can&#39;t handle RDB format version 10",
      "problem": "I can't start redis container in my docker-compose file. I know that docker-compose file is OK, because my colleagues can start the project successfully. I read that there is a solution to delete dump.rdb file. But I can't find it. I use Windows machine. Any suggestions will be very helpful.\nError\n2023-02-09 16:41:28 1:M 09 Feb 2023 13:41:28.699 # Can't handle RDB format version 10\nRedis in docker_compose:\nredis:\n```\n`container_name: redis\nhostname: redis\nimage: redis:5.0\nports:\n  - \"6379:6379\"\nvolumes:\n  - redis:/data\nrestart: always\n`\n```",
      "solution": "I would have to specify something there. I also faced such an issue, and if the solution to remove the volume is working, you can't delete a volume in use, which means you are to remove the container using the volume first...for most container / volumes, that's not an issue, but regarding to redis, if for example you are trying to downgrade your version, then it will force you to do the same operation with your database container leading you to dump your database as you will have to rebuild it again.\nSo my point is do not run these commands blindly and prepare yourself a dumpp of your database first.\nEven better if you need to work on multiple version of and environment for the same project, instead of rebuilding your docker all the time, consider taking 2 differents projets so as your database will keep sage.",
      "question_score": 6,
      "answer_score": 1,
      "created_at": "2023-02-09T14:53:00",
      "url": "https://stackoverflow.com/questions/75399597/redis-docker-compose-cant-handle-rdb-format-version-10"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 70084903,
      "title": "Redis LPOP wrong number of arguments in Python",
      "problem": "I have a simple Redis command that does the following:\n```\n`redis_conn.lpop(queue_name, batch_size)\n`\n```\nAccording to the Redis documentation and their Python SDK documentation, this should be a valid request. And, yet, I get the following error:\n\nredis.exceptions.ResponseError: wrong number of arguments for 'lpop' command\n\nMaybe I'm being obtuse and making a noobie mistake because it's 2:00AM but, this should work. So why doesn't it?",
      "solution": "Well, I was being obtuse. The documentation I linked states that the `count` argument is available from version 6.2. However, since I'm running Windows I don't get the newest version, ergo the failure.",
      "question_score": 6,
      "answer_score": 13,
      "created_at": "2021-11-23T18:02:27",
      "url": "https://stackoverflow.com/questions/70084903/redis-lpop-wrong-number-of-arguments-in-python"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 72187735,
      "title": "class file for redis.clients.jedis.JedisShardInfo not found",
      "problem": "when I upgrade the jedis to version 4.2.3 in gradle.build:\n```\n`    api \"redis.clients:jedis:4.2.3\"\n`\n```\nshow error:\n```\n`/Users/xiaoqiangjiang/source/reddwarf/backend/retire/dolphin-common/src/main/java/misc/config/redis/RedisConfig.java:77: error: cannot access JedisShardInfo\n        return new JedisConnectionFactory(redisConfig);\n               ^\n  class file for redis.clients.jedis.JedisShardInfo not found\n`\n```\nthis is the redis config:\n```\n`    @Bean\n    public JedisConnectionFactory redisConnectionFactory() {\n        var redisConfig = new RedisStandaloneConfiguration(redisHost, redisPort);\n        redisConfig.setPassword(redisPwd);\n        return new JedisConnectionFactory(redisConfig);\n    }\n`\n```\nwhy did this error happen? what should I do to fix it? I have searched from google seems no one facing this problem.",
      "solution": "The class `JedisShardInfo` is removed since Jedis 4.\n\nThe ShardedJedisPool, Sharded, ShardedJedis, BinaryShardedJedis, ShardInfo, JedisShardInfo classes have been removed.\n\nHere is the list of all breaking changes between Jedis 3.x and Jedis 4.x.",
      "question_score": 6,
      "answer_score": 5,
      "created_at": "2022-05-10T15:58:33",
      "url": "https://stackoverflow.com/questions/72187735/class-file-for-redis-clients-jedis-jedisshardinfo-not-found"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 68960083,
      "title": "Bitnami Redis on Kubernetes Authentication Failure with Existing Secret",
      "problem": "I'm trying to install Redis on Kubernetes environment with Bitnami Redis HELM Chart. I want to use a defined password rather than randomly generated one. But i'm getting error below when i want to connect to redis master or replicas with redis-cli.\n```\n`I have no name!@redis-client:/$ redis-cli -h redis-master -a $REDIS_PASSWORD \nWarning: Using a password with '-a' or '-u' option on the command line interface may not be safe.\nWarning: AUTH failed\n`\n```\nI created a Kubernetes secret like this.\n```\n`---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: redis-secret\n  namespace: redis\ntype: Opaque\ndata:\n  redis-password: YWRtaW4xMjM0Cg==\n`\n```\nAnd in values.yaml file i updated auth spec like below.\n```\n`auth:\n  enabled: true\n  sentinel: false\n  existingSecret: \"redis-secret\"\n  existingSecretPasswordKey: \"redis-password\"\n  usePasswordFiles: false\n`\n```\nIf i don't define `existingSecret` field and use randomly generated password then i can connect without an issue. I also tried `AUTH admin1234` after `Warning: AUTH failed` error but it didn't work either.",
      "solution": "The issue was about how i encoded password with echo command. There was a newline character at the end of my password. I tried with printf command rather than echo and it created a different result.\n```\n`printf admin1234 | base64\n`\n```",
      "question_score": 6,
      "answer_score": 3,
      "created_at": "2021-08-28T00:58:00",
      "url": "https://stackoverflow.com/questions/68960083/bitnami-redis-on-kubernetes-authentication-failure-with-existing-secret"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 77717029,
      "title": "Redisearch full text index not working with Python client",
      "problem": "I am trying to follow this Redis documentation link to create a small DB of notable people searchable in real time (using Python client).\nI tried a similar code, but the final line, which queries by \"s\", should return two documents, instead, it returns a blank set. Can anybody help me find out the mistake I am making?\n```\n`import redis\nfrom redis.commands.json.path import Path\nimport redis.commands.search.aggregation as aggregations\nimport redis.commands.search.reducers as reducers\nfrom redis.commands.search.field import TextField, NumericField, TagField\nfrom redis.commands.search.indexDefinition import IndexDefinition, IndexType\nfrom redis.commands.search.query import NumericFilter, Query\n\nd1 = {\"key\": \"shahrukh khan\", \"pl\": '{\"d\": \"mvtv\", \"id\": \"1234-a\", \"img\": \"foo.jpg\", \"t: \"act\", \"tme\": \"1965-\"}', \"org\": \"1\", \"p\": 100}\nd2 = {\"key\": \"salman khan\", \"pl\": '{\"d\": \"mvtv\", \"id\": \"1236-a\", \"img\": \"fool.jpg\", \"t: \"act\", \"tme\": \"1965-\"}', \"org\": \"1\", \"p\": 100}\nd3 = {\"key\": \"aamir khan\", \"pl\": '{\"d\": \"mvtv\", \"id\": \"1237-a\", \"img\": \"fooler.jpg\", \"t: \"act\", \"tme\": \"1965-\"}', \"org\": \"1\", \"p\": 100}\n\nschema = ( \n    TextField(\"$.key\", as_name=\"key\"),  \n    NumericField(\"$.p\", as_name=\"p\"),  \n) \n\nr = redis.Redis(host='localhost', port=6379)\nrs = r.ft(\"idx:au\") \nrs.create_index(     \n    schema,     \n    definition=IndexDefinition(     \n        prefix=[\"au:\"], index_type=IndexType.JSON   \n    )    \n)\n\nr.json().set(\"au:mvtv-1234-a\", Path.root_path(), d1)  \nr.json().set(\"au:mvtv-1236-a\", Path.root_path(), d2)  \nr.json().set(\"au:mvtv-1237-a\", Path.root_path(), d3)  \n\nrs.search(Query(\"s\"))\n`\n```",
      "solution": "When executing a query from the redis-py client, it will transmit the `FT.SEARCH` command to the redis server. You can observe it by using the command `MONITOR` from a redis-client for example.\nAccording to the documentation, when providing a single word for the research, the matching is full. That's why the result of your query is the empty set. If you want to search by prefix, you need to use the expression `prefix*`.\nHowever, documentation says:\n\nThe prefix needs to be at least two characters long.\n\nHence, you cannot search by word starting only by s. What you could do:\n`rs.search(Query(\"sa*\"))\n#Result{1 total, docs: [Document {'id': 'au:mvtv-1236-a', 'payload': None, 'json': '{\"key\":\"salman khan\",\"pl\":\"{\\\\\"d\\\\\": \\\\\"mvtv\\\\\", \\\\\"id\\\\\": \\\\\"1236-a\\\\\", \\\\\"img\\\\\": \\\\\"fool.jpg\\\\\", \\\\\"t: \\\\\"act\\\\\", \\\\\"tme\\\\\": \\\\\"1965-\\\\\"}\",\"org\":\"1\",\"p\":100}'}]}\n`\nAside note\nIf you want to scope your search on a specific field, the syntax is:\n`Query(\"@field_name: word\") # Query(\"@key: sa*\")\n`\nwhere `@field_name` is the schema field's name. Otherwise, the search will look up for all `TextField` attributes.",
      "question_score": 6,
      "answer_score": 2,
      "created_at": "2023-12-26T12:33:29",
      "url": "https://stackoverflow.com/questions/77717029/redisearch-full-text-index-not-working-with-python-client"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 77511864,
      "title": "Redis with nextjs 13.4 (TypeError: url_1.URL is not a constructor)",
      "problem": "I am facing an issue with connecting to redis\nNote : I am using secure connection i.e `rediss`.\nIt works when I do the following\n```\n`// src/redis-client.js\nconst { createClient } = require('redis');\n\nconst redisClient = createClient({\n  url: REDIS_URL, // can be found in Vercel KV store\n});\n\nredisClient.on('error', (error) => {\n  console.error('Redis Error:', error);\n});\n\nredisClient.connect();\n\nmodule.exports = redisClient;\n`\n```\n```\n`// test-redis.js\nconst redisClient = require('./src/redis-client')\n\nconst testRedis = async () => {\n    const key = 'foo';\n    const value = 'bar';\n    const result = await redisClient.set(key, value, { EX: 60 });\n    console.log(result);\n    const result2 = await redisClient.get(key);\n    console.log(result2);\n    await redisClient.quit()\n};\n\ntestRedis();\n`\n```\nAnd then run following command in terminal\n`node test-redis.js`\nBut when I try to do this in nextjs environment\n```\n`import { createClient } from 'redis';\n\nconsole.log(process.env.REDIS_URL) // This prints the correct URL\n\nconst redisClient = createClient({ url: process.env.REDIS_URL }); // this is where it fails\n\nredisClient.on('error', (error) => {\n  console.error('Redis Error:', error);\n});\n\nredisClient.connect();\n\nexport default redisClient; \n`\n```\nEnd up getting following error\n```\n`- event compiled client and server successfully in 2.7s (3788 modules)\n- error Error [TypeError]: url_1.URL is not a constructor\n    at  (webpack-internal:///(middleware)/./node_modules/@redis/client/dist/lib/client/index.js:86)\n    at Function.parseURL (webpack-internal:///(middleware)/./node_modules/@redis/client/dist/lib/client/index.js:86:76)\n    at Commander._RedisClient_initiateOptions (webpack-internal:///(middleware)/./node_modules/@redis/client/dist/lib/client/index.js:394:27)\n    at new RedisClient (webpack-internal:///(middleware)/./node_modules/@redis/client/dist/lib/client/index.js:202:148)\n    at new Commander (webpack-internal:///(middleware)/./node_modules/@redis/client/dist/lib/commander.js:46:13)\n    at create (webpack-internal:///(middleware)/./node_modules/@redis/client/dist/lib/client/index.js:82:16)\n    at createClient (webpack-internal:///(middleware)/./node_modules/redis/dist/index.js:78:38)\n    at eval (webpack-internal:///(middleware)/./src/redis-client.ts:8:72)\n    at Module.(middleware)/./src/redis-client.ts (file:///Users/shreyas/Desktop/planet-webapp/.next/server/middleware.js:6408:1)\n    at __webpack_require__ (file:///Users/shreyas/Desktop/planet-webapp/.next/server/edge-runtime-webpack.js:37:33)\n    at fn (file:///Users/shreyas/Desktop/planet-webapp/.next/server/edge-runtime-webpack.js:268:21) {\n  digest: undefined\n}\n`\n```\nDoes this have to do with nextjs running the code in edge-runtime?\nI faced the similar issue when I used dotenv package with nextjs, where it said process.cwd is not a function, but when it is run in nodejs env it worked.",
      "solution": "So this is related to the runtime environment.\nNextjs code runs in edge-runtime, thus we get following towards the end of the error\n```\n`(file:///Users/shreyas/Desktop/planet-webapp/.next/server/edge-runtime-webpack.js:268:21) {\n  digest: undefined\n}\n`\n```\nFrom what I realized, `redis` as a package is not designed to run on edge-runtime environment, more suitable packages to run in order to connect to redis in this environment would be @upstash/redis and @vercel/kv",
      "question_score": 6,
      "answer_score": 1,
      "created_at": "2023-11-19T18:56:36",
      "url": "https://stackoverflow.com/questions/77511864/redis-with-nextjs-13-4-typeerror-url-1-url-is-not-a-constructor"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 68224424,
      "title": "Accessing AWS ElastiCache (Redis CLUSTER mode) from different AWS accounts via AWS PrivateLink",
      "problem": "I have a business case where I want to access a clustered Redis cache from one account (let's say account A) to an account B.\nI have used the solution mentioned in the below link and for the most part, it works Base Solution\nThe base solution works fine if I am trying to access the clustered Redis via `redis-py` however if I try to use it with `redis-py-cluster` it fails.\nI am testing all this in a staging environment where the Redis cluster has only one node but in the production environment, it has two nodes, so the `redis-py` approach will not work for me.\nBelow is my sample code\n```\n`redis = \"3.5.3\"\nredis-py-cluster = \"2.1.3\"\n==============================\n\nfrom redis import Redis\nfrom rediscluster import RedisCluster\n\nrespCluster = 'error'\nrespRegular = 'error'\n\nhost = \"vpce-XXX.us-east-1.vpce.amazonaws.com\"\nport = \"6379\"\n\ntry:\n    ru = RedisCluster(startup_nodes=[{\"host\": host, \"port\": port}], decode_responses=True, skip_full_coverage_check=True)\n    respCluster = ru.get('ABC')\nexcept Exception as e:\n    print(e)\n\ntry:\n    ru = Redis(host=host, port=port, decode_responses=True)\n    respRegular = ru.get('ABC')\nexcept Exception as e:\n    print(e)\n\nreturn {\"respCluster\": respCluster, \"respRegular\": respRegular}\n`\n```\nThe above code works perfectly in account A but in account B the output that I got was\n```\n`{'respCluster': 'error', 'respRegular': '123456789'}\n`\n```\nAnd the error that I am getting is\n```\n`rediscluster.exceptions.ClusterError: TTL exhausted\n`\n```\nIn account A we are using AWS ECS + EC2 + docker to run this and\nIn account B we are running the code in an AWS EKS Kubernetes pod.\nWhat should I do to make the `redis-py-cluster` work in this case? or is there an alternative to `redis-py-cluster` in python to access a multinode Redis cluster?\nI know this is a highly specific case, any help is appreciated.\nEDIT 1: Upon further research, it seems that TTL exhaust is a general error, in the logs the initial error is\n```\n`redis.exceptions.ConnectionError: \nError 101 connecting to XX.XXX.XX.XXX:6379. Network is unreachable\n`\n```\nHere the XXXX is the IP of the Redus cluster in Account A.\nThis is strange since the `redis-py` also connects to the same IP and port,\nthis error should not exist.",
      "solution": "So turns out the issue was due to how `redis-py-cluster` manages host and port.\nWhen a new `redis-py-cluster` object is created it gets a list of host IPs from the Redis server(i.e. Redis cluster host IPs form account A), after which the client tries to connect to the new host and ports.\nIn normal cases, it works as the initial host and the IP from the response are one and the same.(i.e. the host and port added at the time of object creation)\nIn our case, the object creation host and port are obtained from the DNS name from the Endpoint service of Account B.\nIt leads to the code trying to access the actual IP from account A instead of the DNS name from account B.\nThe issue was resolved using Host port remapping, here we bound the IP returned from the Redis server from Account A with IP Of Account B's endpoints services DNA name.",
      "question_score": 6,
      "answer_score": 1,
      "created_at": "2021-07-02T13:29:14",
      "url": "https://stackoverflow.com/questions/68224424/accessing-aws-elasticache-redis-cluster-mode-from-different-aws-accounts-via-a"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 66117290,
      "title": "redis-cli how to AUTH using password and issue a command?",
      "problem": "When no password is set, we can issue for instance;\n```\n`>> redis-cli keys * \n`\n```\nor\n```\n`>> redis-cli config set requirepass \"aaaaaa\"\n`\n```\nHowever, after we have have issued the latter, the first no longer works and results in:\n```\n`>> redis-cli keys *\n  (error) NOAUTH Authentication required.  \n`\n```\nWe need to authenticate. Sure.\n```\n`>> redis-cli AUTH aaaaaa\n   OK\n>> redis-cli keys *\n   (error) NOAUTH Authentication required. \n`\n```\nHow do we authenticate and then able to execute a command?\nIs this not possible? Heredocs only?\nI've tried:\n```\n`>> redis-cli AUTH aaaaaa && config set requirepass \"aaaaaa\"\n`\n```\nBut did not work. Also semicolon after aaaaaa. Not work.\nHow?",
      "solution": "The `AUTH` commands only last for the duration of the tcp connection. Each new invocation of `redis-cli` creates a new connection, thus you have to authenticate at each invocation.\nIt is possible to execute several redis commands on one invocation of `redis-cli`: they must be separated by `\\n`\nThus this would work:\n```\n`echo -e 'AUTH aaaaaa\\nkeys *' | redis-cli  \n`\n```\nNote: The other answer also provides a way to pass arguments separated by `\\n` to `redis-cli`",
      "question_score": 5,
      "answer_score": 10,
      "created_at": "2021-02-09T11:38:54",
      "url": "https://stackoverflow.com/questions/66117290/redis-cli-how-to-auth-using-password-and-issue-a-command"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 70178133,
      "title": "redis freezes node server when connected",
      "problem": "I am trying to use redis to store sessions with express-session. Here is the code:\n```\n`//Imports\nconst express = require('express');\nconst app = express();\nconst bodyParser = require('body-parser');\nconst session = require('express-session');\nconst logger = require('morgan');\n\n//Connect to Redis\nconst redis = require('redis');\nlet RedisStore = require('connect-redis')(session);\nlet redisClient = redis.createClient();\nredisClient.connect();\nredisClient.on('connect', () => console.log('Connected to Redis..'));\n\napp.use(\n  session({\n    store: new RedisStore({ client: redisClient }),\n    saveUninitialized: false,\n    secret: 'keyboard cat',\n    resave: false,\n  })\n);\n\napp.use(bodyParser.urlencoded({ extended: false }));\napp.use(bodyParser.json());\napp.use(logger('dev'));\n\napp.get('/', function (req, res) {\n  var body = '';\n  console.log(req.session);\n  if (req.session.views) {\n    ++req.session.views;\n  } else {\n    req.session.views = 1;\n    body += 'First time visiting? view this page in several browsers :)';\n  }\n  res.send(\n    body + 'viewed ' + req.session.views + ' times.'\n  );\n});\n\napp.listen(3000, () => console.log('App is listening on port 3000'));\n`\n```\nWhenever I start my application, it freezes. I have checked the redis-cli and pinged it with the response 'PONG'. redis-server is starting just fine. Whenever I remove the following lines:\n```\n`redisClient.connect();\nredisClient.on('connect', () => console.log('Connected to Redis..'));\n`\n```\nthe app crashes whenever I hit the \"/\" route. But if I had those lines in, the app just hangs there and doesn't do anything. I checked the docs here and they said you need to connect before any command:\nhttps://github.com/redis/node-redis/blob/HEAD/docs/v3-to-v4.md#no-auto-connect\nAnd the code I'm running is right from the connect-redis npm page:\nhttps://www.npmjs.com/package/connect-redis\nI'm not sure why the server is freezing. It works fine if I remove store from the session so it's definitely something to do with redis but on the node side. The redis server is running fine on the localhost. Any suggestions?\n**UPDATE\nI checked the connect-redis repo issues and I found that someone else is having the same problem as me. The problem will be resolved in the next commit:\nhttps://github.com/tj/connect-redis/issues/336",
      "solution": "This is currently a known issue. Currently connect-redis is not compatible with the latest version of node redis.\nhttps://github.com/tj/connect-redis/issues/336\nAdd the following to your client to fix this issue until patched:\n```\n`const client = createClient({\n    legacyMode: true\n});\n`\n```",
      "question_score": 5,
      "answer_score": 11,
      "created_at": "2021-12-01T02:35:22",
      "url": "https://stackoverflow.com/questions/70178133/redis-freezes-node-server-when-connected"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 73135157,
      "title": "Redis timeseries with python : ResponseError: unknown command &#39;TS.CREATE&#39;",
      "problem": "I am trying to create a timeseries in Redis using python like so:\n```\n`    import redis\n    connection_redis = redis.Redis(host='127.0.0.1', port=6379)\n    connection_redis.ts().create('ts', retention_msecs=0)\n`\n```\nbut I get the following error: ResponseError: unknown command 'TS.CREATE'.\nI have been searching for a way to solve this problem but I haven't found anything.\nI am running redis in a docker.\nThank you :)!",
      "solution": "The Redis docker image does not contain any Redis module.\nYou can use the Redis Stack docker image.\n`redis/redis-stack-server` contains the RediSearch, RedisJSON, RedisGraph, RedisTimeSeries, and RedisBloom modules. `redis/redis-stack` also contains\nRedisInsight.\nUpdate, October 2024\nFrom a Redis Blog Post:\n\nRedis 8 introduces seven new data structures \u2014JSON, time series, and five probabilistic types\u2014 along with the fastest and most scalable Redis query engine to date. These capabilities, previously only available separately through Redis Stack or our Software and Cloud offerings, are now built natively into Redis Community Edition.\n\nYou can now simply use Redis docker image (version 8.0-M01 or later).",
      "question_score": 5,
      "answer_score": 8,
      "created_at": "2022-07-27T11:08:58",
      "url": "https://stackoverflow.com/questions/73135157/redis-timeseries-with-python-responseerror-unknown-command-ts-create"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 67327413,
      "title": "AWS Lambda Timeout when connecting to Redis Elasticache in same VPC",
      "problem": "Trying to publish from a Lambda function to a Redis Elasticache, but I just continue to get `502 Bad Gateway` responses with the Lambda function timing out.\nI have successfully connected to the Elasticache instance using an ECS in the same VPC which leads me to think that the VPC settings for my Lambda are not correct. I tried following this tutorial (https://docs.aws.amazon.com/lambda/latest/dg/services-elasticache-tutorial.html) and have looked at several StackOverflow threads to no avail.\nThe Lambda Function:\n`export const publisher = redis.createClient({\n  url: XXXXXX, // env var containing the URL which is also used in the ECS server to successfully connect\n});\n\nexport const handler = async (\n  event: AWSLambda.APIGatewayProxyWithCognitoAuthorizerEvent\n): Promise => {\n  try {\n    if (!event.body || !event.pathParameters || !event.pathParameters.channelId)\n      return ApiResponse.error(400, {}, new InvalidRequestError());\n\n    const { action, payload } = JSON.parse(event.body) as {\n      action: string;\n      payload?: { [key: string]: string };\n    };\n\n    const { channelId } = event.pathParameters;\n\n    const publishAsync = promisify(publisher.publish).bind(publisher);\n\n    await publishAsync(\n      channelId,\n      JSON.stringify({\n        action,\n        payload: payload || {},\n      })\n    );\n\n    return ApiResponse.success(204);\n  } catch (e) {\n    Logger.error(e);\n    return ApiResponse.error();\n  }\n};\n`\nIn my troubleshooting, I have verified the following in the Lambda functions console:\n\nThe correct role is showing in `Configuration > Permissions`\nThe lambda function has access to the VPC (`Configuration > VPCs`), Subnets, and the same SG as the Elasticache instance.\nThe SG is allowing all traffic from anywhere.\nIt is indeed the Redis connection. Using `console.log` the code stops at this line: `await publishAsync()`\n\nI am sure it is something small, but it is racking my brain!\nUpdate 1:\nTried adding an error handler to log any issues with the publish in addition to the main `try/catch` block, but it's not logging a thing.\n```\n`publisher.on('error', (e) => {\n    Logger.error(e, 'evses-service', 'message-publisher');\n  });\n`\n```\nAlso have copied my Elasticache setup:\n\nAnd my Elasticache Subnet Group:\n\nAnd my Lambda VPC settings:\n\nAnd that my Lambda has the right access:\n\nUpdate 2:\nTried to follow the tutorial here (https://docs.aws.amazon.com/lambda/latest/dg/services-elasticache-tutorial.html) word for word, but getting the same issue. No logs, just a timeout after 30 seconds. Here is the test code:\n```\n`const crypto = require('crypto');\nconst redis = require('redis');\nconst util = require('util');\n\nconst client = redis.createClient({\n  url: 'rediss://clusterforlambdatest.9nxhfd.0001.use1.cache.amazonaws.com',\n});\n\nclient.on('error', (e) => {\n  console.log(e);\n});\n\nexports.handler = async (event) => {\n  try {\n    const len = 24;\n\n    const randomString = crypto\n      .randomBytes(Math.ceil(len / 2))\n      .toString('hex') // convert to hexadecimal format\n      .slice(0, len)\n      .toUpperCase();\n\n    const setAsync = util.promisify(client.set).bind(client);\n    const getAsync = util.promisify(client.get).bind(client);\n\n    await setAsync(randomString, 'We set this string bruh!');\n    const doc = await getAsync(randomString);\n\n    console.log(`Successfully receieved document ${randomString} with contents: ${doc}`);\n\n    return;\n  } catch (e) {\n    console.log(e);\n\n    return {\n      statusCode: 500,\n    };\n  }\n};\n`\n```",
      "solution": "If you have timeout, assuming the lambda network is well configured, you should check the following:\n\nredis SSL configuration: check diffs between redisS connection url and cluster configuration (in-transit encryption and client configuration with `tls: {}`)\nconfigure the client with a specific retry strategy to avoid lambda timeout and catch connection issue\ncheck VPC acl and security groups",
      "question_score": 5,
      "answer_score": 4,
      "created_at": "2021-04-30T04:24:10",
      "url": "https://stackoverflow.com/questions/67327413/aws-lambda-timeout-when-connecting-to-redis-elasticache-in-same-vpc"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 69922363,
      "title": "Flushall in redis leads to loading the dataset in memory error",
      "problem": "How do I \"FLUSHALL\" in redis in this situation?\nRunning redis via docker on PopOs 21.0.4 as shown in the following docker-compose.yml\n```\n`version: \"2.4\"\nservices:\n  redis:\n    image: redis:5-alpine\n    command: redis-server --save \"\" --appendonly yes\n    restart: always\n    volumes:\n      - \"${PWD}//redis/data:/data\"\n    ports:\n      - \"6379:6379\"\n`\n```\nConnecting to redis-cli and issuing a FLUSHALL (or FLUSHDB) command and I get the error:\n```\n`127.0.0.1:6379[1]> FLUSHALL\n(error) LOADING Redis is loading the dataset in memory\n`\n```\nHere is docker version:\n```\n`Client: Docker Engine - Community\n Version:           20.10.10\n API version:       1.41\n Go version:        go1.16.9\n Git commit:        b485636\n Built:             Mon Oct 25 07:43:13 2021\n OS/Arch:           linux/amd64\n Context:           default\n Experimental:      true\n\nServer: Docker Engine - Community\n Engine:\n  Version:          20.10.10\n  API version:      1.41 (minimum version 1.12)\n  Go version:       go1.16.9\n  Git commit:       e2f740d\n  Built:            Mon Oct 25 07:41:20 2021\n  OS/Arch:          linux/amd64\n  Experimental:     false\n containerd:\n  Version:          1.4.11\n  GitCommit:        5b46e404f6b9f661a205e28d59c982d3634148f8\n runc:\n  Version:          1.0.2\n  GitCommit:        v1.0.2-0-g52b36a2\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\n`\n```",
      "solution": "The error message means that Redis is still loading data, i.e. in your case, the AOF file. You cannot run `FLUSHALL` until the loading finishes.\nIf you don't need the data to be loaded, you can delete the AOF file before starting Redis.",
      "question_score": 5,
      "answer_score": 6,
      "created_at": "2021-11-11T02:58:09",
      "url": "https://stackoverflow.com/questions/69922363/flushall-in-redis-leads-to-loading-the-dataset-in-memory-error"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 67983070,
      "title": "Deserializing redis&#39;s Value in Rust (FromRedisValue)",
      "problem": "I'm implementing a simple task queue using redis in Rust, but am struggling to deserialize the returned values from redis into my custom types.\nIn total I thought of 3 approches:\n\nDeserializing using serde-redis\nManually implementing the `FromRedisValue` trait\nSerializing to String using serde-json > sending as string > then deserializing from string\n\nThe 3rd approach worked but feels artificial. I'd like to figure out either 1 or 2, both of which I'm failing at.\nApproach 1 - serde-redis\nI have a simple Task definition:\n```\n`#[derive(Debug, PartialEq, serde::Deserialize, serde::Serialize)]\npub struct Task {\n    pub id: u32,\n    pub desc: String,\n}\n`\n```\nwhich I'm trying to receive into a worker:\n```\n`use serde_redis::RedisDeserialize;\n\npub fn execute_task(conn: &mut Connection) {\n    let task: Task = redis::cmd(\"LPOP\")\n        .arg(\"task_q\")\n        .query::(conn)\n        .unwrap()\n        .deserialize()\n        .unwrap();\n\n    println!(\"... executing task {} ...\", task.id);\n}\n`\n```\nbut I'm getting the following error:\n```\n`error[E0277]: the trait bound `Task: FromRedisValue` is not satisfied\n  --> src/bin/worker.rs:38:10\n   |\n38 |         .query::(conn)\n   |          ^^^^^ the trait `FromRedisValue` is not implemented for `Task`\n\nerror[E0599]: no method named `deserialize` found for struct `Task` in the current scope\n  --> src/bin/worker.rs:40:10\n   |\n40 |         .deserialize()\n   |          ^^^^^^^^^^^ method not found in `Task`\n`\n```\nSo clearly I integrated the crate wrong, as it's not working. The documentation is super brief and the source code way over my head as a beginner - what could I be missing?\nApproach 2 - manually implementing FromRedisValue\nMy naive approach:\n```\n`impl FromRedisValue for Task {\n    fn from_redis_value(v: &Value) -> RedisResult {\n        let t = Task {\n            id: v.id,\n            desc: v.desc,\n        };\n        RedisResult::Ok(t)\n    }\n\n    fn from_redis_values(items: &[Value]) -> RedisResult> {\n        let tasks = items\n            .into_iter()\n            .map(|v| Task {\n                id: v.id,\n                desc: v.desc,\n            })\n            .collect();\n        RedisResult::Ok(tasks)\n    }\n}\n`\n```\nThe errors I'm getting:\n```\n`error[E0609]: no field `id` on type `&Value`\n   --> src/redis_tut.rs:203:19\n    |\n203 |             id: v.id,\n    |                   ^^\n\nerror[E0609]: no field `desc` on type `&Value`\n   --> src/redis_tut.rs:204:21\n    |\n204 |             desc: v.desc,\n    |                     ^^^^\n\n// ...the same for the vector implementation\n`\n```\nSo clearly redis's Value doesn't have / know of the fields I want for Task. What's the right way to do this?",
      "solution": "Redis doesn't define structured serialization formats. It mostly store strings and integers. So you have to choose or define your format for your struct.\nA popular one is JSON, as you noticed, but if you just want to (de)serialize simple pairs of (id, description), it's not very readable nor convenient.\nIn such a case, you can define your own format, for example the id and the description with a dash in between:\n`#[derive(Debug, PartialEq)]\npub struct Task {\n    pub id: u32,\n    pub desc: String,\n}\n// assume a task is defined as \"-\"\nimpl FromRedisValue for Task {\n    fn from_redis_value(v: &Value) -> RedisResult {\n        let v: String = from_redis_value(v)?;\n        if let Some((id, desc)) = v.split_once('-') {\n            if let Ok(id) = id.parse() {\n                Ok(Task {\n                    id,\n                    desc: desc.to_string(),\n                })\n            } else {\n                Err((ErrorKind::TypeError, \"bad first token\").into())\n            }\n        } else {\n            Err((ErrorKind::TypeError, \"missing dash\").into())\n        }\n    }\n}\n`",
      "question_score": 5,
      "answer_score": 4,
      "created_at": "2021-06-15T10:54:14",
      "url": "https://stackoverflow.com/questions/67983070/deserializing-rediss-value-in-rust-fromredisvalue"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 73148706,
      "title": "Unable to ping redis server port 6379 from Windows Command Prompt",
      "problem": "I have installed Redis in Ubuntu distro in Windows using WSL and I am trying to connect to the server from Windows command prompt.\nWhen I ping the local IP (172.29.0.1) of Ubuntu, I get the ping result but when I ping the same IP with the port number (172.29.0.1:6379) then I am getting this error message.\n```\n`Ping request could not find host 172.29.0.1:6379. Please check the name and try again.\n`\n```\nI have started the redis server on Ubuntu using\n`sudo service redis-server start`\nAnd when I ping the server on Ubuntu's terminal, I am getting the response back.\n```\n`:~$ redis-cli \n127.0.0.1:6379> ping\nPONG\n`\n```\nI have also tried executing `sudo ufw allow 6379` command to expose 6379 port but it has not helped.\nHow can I connect to the Redis Server that is running on Ubuntu from my Windows command prompt or any other application running in Windows?",
      "solution": "I doubt Redis responds to regular `ping` requests which use ICMP packets .\nIf you want to check your Redis server is up, install the `redis-cli` tools on Windows and send a Redis-style \"ping\" with:\n```\n`redis-cli -h 172.29.0.1 PING\n`\n```",
      "question_score": 5,
      "answer_score": 4,
      "created_at": "2022-07-28T09:15:44",
      "url": "https://stackoverflow.com/questions/73148706/unable-to-ping-redis-server-port-6379-from-windows-command-prompt"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 68854213,
      "title": "Spring Data Redis issue with CRUDRepository",
      "problem": "My team and I are using a CRUD repository with Redis to perform some operations on it. The problem is that there is an entry in Redis that gets generated as an index which stores the keys associated to the Redis entries, and this entry never removes entries that were already flushed when TTL reached 0.\nHere is an example of the code we use.\n```\n`@RedisHash(\"rate\")\npublic class RateRedisEntry implements Serializable {\n    @Id\n    private String tenantEndpointByBlock; // A HTTP end point\n    ...\n}\n\n// CRUD repository.\n@Repository\npublic interface RateRepository extends CrudRepository {}\n`\n```\nThis generates the entry `rate` in Redis which is the Set object I mentioned before.\nWhen we check the memory usage on it, it just keeps growing all the time until the memory usage reaches 100% from the available in Redis.'\n```\n`> MEMORY USAGE \"rate\"\n(integer) 153034\n.\n.\n> MEMORY USAGE \"rate\"\n(integer) 153876\n.\n.\n> MEMORY USAGE \"rate\"\n(integer) 163492\n`\n```\nIs there a way to prevent this index from being created or for the values stored to be removed once the entries' TTL reaches 0?\nAny assistance is appreciated.",
      "solution": "I found a possible solution to the problem. You could set the repository options to track the event where an entry's TTL reaches 0.\n```\n`@EnableRedisRepositories(enableKeyspaceEvents \n    = EnableKeyspaceEvents.ON_STARTUP)\n`\n```\nThis will make Spring Data Redis to track the entries that are flushed, so it updates the entries that are indexes.\nHowever, this will introduced some extra processing on your system, so you should be aware of it and evaluate if working with `RedisRepository` makes sense.\nIn our case, we decided that we were gonna work with the `RedisTemplate` class directly, to avoid any extra over head or uncontrolled processing.\nIf you are interested in seeing all the details of how we fixed it, you can read this article I wrote about it.\nhttps://engineering.salesforce.com/lessons-learned-using-spring-data-redis-f3121f89bff9",
      "question_score": 5,
      "answer_score": 4,
      "created_at": "2021-08-19T22:29:34",
      "url": "https://stackoverflow.com/questions/68854213/spring-data-redis-issue-with-crudrepository"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 67479940,
      "title": "the redis stream length limit not work when using python redis",
      "problem": "Now I am using python(`Python 3`) redis client to add element into redis stream, this is the dependencies of the lib:\n```\n`redis~=5.0.3\n`\n```\nand then this is my code to write element into redis using Python:\n```\n`def push_message_to_stream(article):\n    try:\n        message = {\n            \"id\": article.id,\n            \"sub_source_id\": article.sub_source_id\n        }\n        #\n        # Redis did not remove the ack message automatic\n        # when the element is full, it remove the oldest\n        #\n        redis_client.xadd(name=article_stream_name, fields=message, maxlen=10)\n        # redis_client.xgroup_create(name=article_stream_name, groupname=article_group_name)\n        # redis_client.publish(channel=article_stream_name, message=\"hello world!\")\n    except Exception as e:\n        logger.error(\"write stream error:\", e)\n`\n```\nthe element could write into redis successfully, but after I add parameter of `maxlen` to 10, when I check the stream element in redis, it still has more than 90+ element in the stream, I gusses maybe the redis using old creator config,then I tried delete the stream and recreated it but still in this situation, where am I doing wrong? why the maxlen parameter did not take any effect?",
      "solution": "by default the `approximate` parameter to `xadd` in `redis-py` is set to `True`.  This means that you won't get exact length trimming and gives you a bit better performance.  If you want exact length, try:\n```\n`redis_client.xadd(name=article_stream_name, fields=message, maxlen=10, approximate=False)\n`\n```\nSource code for `xadd`: https://redis-py.readthedocs.io/en/stable/_modules/redis/client.html#Redis.xadd",
      "question_score": 5,
      "answer_score": 4,
      "created_at": "2021-05-11T04:50:21",
      "url": "https://stackoverflow.com/questions/67479940/the-redis-stream-length-limit-not-work-when-using-python-redis"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 65800101,
      "title": "Why my Redis Docker container shows &quot;(empty array)&quot; for KEYS * while I&#39;m absolutely sure it has cached data and it works OK",
      "problem": "This is chunk of my `docker-compose.yml`:\n```\n`  redis:\n    image: redis:alpine\n    container_name: foo_redis\n    volumes:\n      - ./docker/volumes/redis:/data\n    restart: unless-stopped\n    ports:\n      - 6379:6379\n    networks:\n      - local_network\n`\n```\nI can guarantee that data persists between `ups` and `downs` because I've defined `volumes`.\nI can prove that data is being stored physically because I see `dump.rdb` within `./docker/volumes/redis` on my host machine.\n\nWhen I enable debugbar I can see that route where I cache stuff has no queries.\nWhen I clear Laravel's cache and refresh page I see number of DB queries, second refresh - no queries. Clearly it works with no issues.\n\nThen from my host machine I do:\n`docker exec -it foo_redis redis-cli`\nI get a prompt:\n`127.0.0.1:6379>`\nI type `KEYS *` and I get:\n```\n`127.0.0.1:6379> KEYS *\n(empty array)\n127.0.0.1:6379>\n`\n```\nWhy? What am I doing wrong? Redis seems to works fine. My Laravel app has `predis` and caching works with no issues.\nedit:\nMy `.env` looks like:\n```\n`CACHE_DRIVER=redis\nREDIS_CLIENT=predis\nREDIS_HOST=redis\nREDIS_PASSWORD=null\nREDIS_PORT=6379\n`\n```\nedit 2:\nWhen I manually put something into redis, it shows only that thing.\n```\n`127.0.0.1:6379> set hello world\nOK\n127.0.0.1:6379> get hello\n\"world\"\n127.0.0.1:6379> KEYS *\n1) \"hello\"\n127.0.0.1:6379>\n`\n```",
      "solution": "The old answer, but maybe will be useful for someone,\ncheck the using Redis database number.\nIf your data contains in the Redis database with a number different from 0 (default number), first set the DB number in CLI mode, using the command SELECT \nExample: https://redis.io/docs/manual/cli/#interactive-mode",
      "question_score": 5,
      "answer_score": 4,
      "created_at": "2021-01-19T22:56:18",
      "url": "https://stackoverflow.com/questions/65800101/why-my-redis-docker-container-shows-empty-array-for-keys-while-im-absolut"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 75287120,
      "title": "StackExchange.Redis sync-ops and conn-sec",
      "problem": "Every now and then we received a large set of timeouts (around peak time for website traffic) with lots of logs in the following form:\n```\n`Timeout performing GET (5000ms)\nnext: GET ObjectPageView.120.633.0\ninst: 21\nqu: 0\nqs: 0\naw: False\nbw: SpinningDown\nrs: ReadAsync\nws: Idle\nin: 0\nlast-in: 0\ncur-in: 0\nsync-ops: 456703\nasync-ops: 1\nconn-sec: 72340.11\nmc: 1/1/0\nmgr: 10 of 10 available\nIOCP: (Busy=0 Free=1800 Min=600 Max=1800)\nWORKER: (Busy=720 Free=1080 Min=600 Max=1800)\nv: 2.6.90.64945\n`\n```\nWhat do `sync-ops` and `conn-sec` stand for?  The rest of the numbers seem fine, but these seem high and I'm not entirely sure what they are describing.",
      "solution": "These are statistic about the current connection:\n\n\"sync-ops\" a count of synchronous operations (as opposed to async-ops for asynchronous operations) performed on the current connection.\n\n\"conn-sec\" is the current duration of the current connection (from connected until now)",
      "question_score": 5,
      "answer_score": 2,
      "created_at": "2023-01-30T17:12:04",
      "url": "https://stackoverflow.com/questions/75287120/stackexchange-redis-sync-ops-and-conn-sec"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 71246992,
      "title": "Does the Redis PHP extension installed through PECL work on Mac M1?",
      "problem": "The Redis server is running successfully using Homebrew with `brew services start redis`.\nThe PECL Redis installer appears to work with `sudo pecl install redis `, giving the following output:\n```\n`Build process completed successfully\nInstalling '/opt/homebrew/Cellar/php@7.4/7.4.28/pecl/20190902/redis.so'\ninstall ok: channel://pecl.php.net/redis-5.3.7\nExtension redis enabled in php.ini\n`\n```\nIf I use `php --ini`, this is the output:\n```\n`Warning: PHP Startup: Unable to load dynamic library 'redis.so' (tried: /opt/homebrew/lib/php/pecl/20190902/redis.so (dlopen(/opt/homebrew/lib/php/pecl/20190902/redis.so, 0x0009): tried: '/opt/homebrew/lib/php/pecl/20190902/redis.so' (mach-o file, but is an incompatible architecture (have 'x86_64', need 'arm64e')), '/usr/local/lib/redis.so' (no such file), '/usr/lib/redis.so' (no such file)), /opt/homebrew/lib/php/pecl/20190902/redis.so.so (dlopen(/opt/homebrew/lib/php/pecl/20190902/redis.so.so, 0x0009): tried: '/opt/homebrew/lib/php/pecl/20190902/redis.so.so' (no such file), '/usr/local/lib/redis.so.so' (no such file), '/usr/lib/redis.so.so' (no such file))) in Unknown on line 0\n`\n```\nThe redis.so library is in /opt/homebrew/lib/php/pecl/20190902.\nIs there any way to get this library working on Mac M1?",
      "solution": "You can run `pecl` with `arch` to ensure that the architecture is `arm64`.\n```\n`arch -arm64 sudo pecl install redis\n`\n```\nAlternatively, you can use a brew tap I maintain (shivammathur/extensions).\n```\n`brew tap shivammathur/extensions\nbrew install redis@7.4\n`\n```",
      "question_score": 4,
      "answer_score": 15,
      "created_at": "2022-02-24T05:06:56",
      "url": "https://stackoverflow.com/questions/71246992/does-the-redis-php-extension-installed-through-pecl-work-on-mac-m1"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 67886898,
      "title": "Redis sentinel HA on Kubernetes",
      "problem": "I am trying to have 1 redis master with 2 redis replicas tied to a 3 Quorum Sentinel on Kubernetes. I am very new to Kubernetes.\nMy initial plan was to have the master running on a pod tied to 1 Kubernetes SVC and the 2 replicas running on their own pods tied to another Kubernetes SVC. Finally, the 3 Sentinel pods will be tied to their own SVC. The replicas will be tied to the master SVC (because without svc, ip will change). The sentinel will also be configured and tied to master and replica SVCs. But I'm not sure if this is feasible because when master pod crashes, how will one of the replica pods move to the master SVC and become the master? Is that possible?\nThe second approach I had was to wrap redis pods in a replication controller and the same for sentinel as well. However, I'm not sure how to make one of the pods master and the others replicas with a replication controller.\nWould any of the two approaches work? If not, is there a better design that I can adopt? Any leads would be appreciated.",
      "solution": "You can deploy Redis Sentinel using the Helm package manager and the Redis Helm Chart.\nIf you don't have `Helm3` installed yet, you can use this documentation to install it.\nI will provide a few explanations to illustrate how it works.\n\nFirst we need to get the `values.yaml` file from the Redis Helm Chart to customize our installation:\n```\n`$ wget https://raw.githubusercontent.com/bitnami/charts/master/bitnami/redis/values.yaml\n`\n```\nWe can configure a lot of parameters in the `values.yaml` file , but for demonstration purposes I only enabled Sentinel and set the redis password:\nNOTE: For a list of parameters that can be configured during installation, see the Redis Helm Chart Parameters documentation.\n```\n`# values.yaml\n\nglobal:\n  redis:\n    password: redispassword\n...\nreplica:\n  replicaCount: 3\n...\nsentinel:\n  enabled: true\n...\n`\n```\nThen we can deploy Redis using the configuration from the  `values.yaml` file:\nNOTE: It will deploy a three Pod cluster (one master and two slaves) managed by the StatefulSets with a `sentinel` container running inside each Pod.\n```\n`$ helm install redis-sentinel bitnami/redis --values values.yaml\n`\n```\nBe sure to carefully read the NOTES section of the chart installation output. It contains many useful information (e.g. how to connect to your database from outside the cluster)\nAfter installation, check redis `StatefulSet`, `Pods` and `Services` (headless service can be used for internal access):\n```\n`$ kubectl get pods -o wide\nNAME                    READY   STATUS    RESTARTS   AGE     IP\nredis-sentinel-node-0   2/2     Running   0          2m13s   10.4.2.21\nredis-sentinel-node-1   2/2     Running   0          86s     10.4.0.10\nredis-sentinel-node-2   2/2     Running   0          47s     10.4.1.10\n\n$ kubectl get sts\nNAME                  READY   AGE\nredis-sentinel-node   3/3     2m41s\n\n$ kubectl get svc\nNAME                      TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)              AGE\nredis-sentinel            ClusterIP   10.8.15.252           6379/TCP,26379/TCP   2m\nredis-sentinel-headless   ClusterIP   None                  6379/TCP,26379/TCP   2m\n`\n```\nAs you can see, each `redis-sentinel-node` Pod contains the `redis` and `sentinel` containers:\n```\n`$ kubectl get pods redis-sentinel-node-0 -o jsonpath={.spec.containers[*].name}\nredis sentinel\n`\n```\nWe can check the `sentinel` container logs to find out which `redis-sentinel-node` is the master:\n```\n`$ kubectl logs -f redis-sentinel-node-0 sentinel\n...\n1:X 09 Jun 2021 09:52:01.017 # Configuration loaded\n1:X 09 Jun 2021 09:52:01.019 * monotonic clock: POSIX clock_gettime\n1:X 09 Jun 2021 09:52:01.019 * Running mode=sentinel, port=26379.\n1:X 09 Jun 2021 09:52:01.026 # Sentinel ID is 1bad9439401e44e749e2bf5868ad9ec7787e914e\n1:X 09 Jun 2021 09:52:01.026 # +monitor master mymaster 10.4.2.21 6379 quorum 2\n...\n1:X 09 Jun 2021 09:53:21.429 * +slave slave 10.4.0.10:6379 10.4.0.10 6379 @ mymaster 10.4.2.21 6379\n1:X 09 Jun 2021 09:53:21.435 * +slave slave 10.4.1.10:6379 10.4.1.10 6379 @ mymaster 10.4.2.21 6379\n...\n`\n```\nAs you can see from the logs above, the `redis-sentinel-node-0` Pod is the master and the `redis-sentinel-node-1` & `redis-sentinel-node-2` Pods are slaves.\nFor testing, let's delete the master and check if sentinel will switch the master role to one of the slaves:\n```\n`    $ kubectl delete pod redis-sentinel-node-0\n    pod \"redis-sentinel-node-0\" deleted\n    \n    $ kubectl logs -f redis-sentinel-node-1 sentinel\n    ...                                                                                           \n    1:X 09 Jun 2021 09:55:20.902 # Executing user requested FAILOVER of 'mymaster'\n    ...\n    1:X 09 Jun 2021 09:55:22.666 # +switch-master mymaster 10.4.2.21 6379 10.4.1.10 6379\n    ...\n    1:X 09 Jun 2021 09:55:50.626 * +slave slave 10.4.0.10:6379 10.4.0.10 6379 @ mymaster 10.4.1.10 6379\n    1:X 09 Jun 2021 09:55:50.632 * +slave slave 10.4.2.22:6379 10.4.2.22 6379 @ mymaster 10.4.1.10 6379\n`\n```\nA new master (`redis-sentinel-node-2` `10.4.1.10`) has been selected, so everything works as expected.\nAdditionally, we can display more information by connecting to one of the Redis nodes:\n```\n`$ kubectl run --namespace default redis-client --restart='Never' --env REDIS_PASSWORD=redispassword --image docker.io/bitnami/redis:6.2.1-debian-10-r47 --command -- sleep infinity\npod/redis-client created\n$ kubectl exec --tty -i redis-client --namespace default -- bash\nI have no name!@redis-client:/$ redis-cli -h redis-sentinel-node-1.redis-sentinel-headless -p 6379 -a $REDIS_PASSWORD\nWarning: Using a password with '-a' or '-u' option on the command line interface may not be safe.\nredis-sentinel-node-1.redis-sentinel-headless:6379> info replication\n# Replication\nrole:slave\nmaster_host:10.4.1.10\nmaster_port:6379\nmaster_link_status:up\n...\n`\n```",
      "question_score": 4,
      "answer_score": 16,
      "created_at": "2021-06-08T14:25:27",
      "url": "https://stackoverflow.com/questions/67886898/redis-sentinel-ha-on-kubernetes"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 70402383,
      "title": "Redis Cloud and connect-redis: the client is closed",
      "problem": "I currently build a website using Express and want to use redis cloud database to save userID in session. The redisClient is created in redisClient.js and after that i pass it to redisStore in session in app.js. Here is the code:\nredisCLient.js\n```\n`const redis = require(\"redis\");\nlet redisClient = redis.createClient({\n    host: process.env.REDIS_HOSTNAME,\n    port: parseInt(process.env.REDIS_PORT),\n    password: process.env.REDIS_PASSWORD\n});\n\nredisClient.on('error', function(err) {\n    console.log('*Redis Client Error: ' + err.message);\n});\nredisClient.on('connect', function(){\n   console.log('Connected to redis instance');\n});\n\n(async () => {\n    await redisClient.auth(process.env.REDIS_PASSWORD)\n        .catch(err => {console.log('Redis auth error: ' + err.message)});\n    await redisClient.connect()\n        .catch(err => {console.log('Redis connect error: ' + err.message)});\n})();\n\nmodule.exports = redisClient;\n`\n```\napp.js\n```\n`const session = require(\"express-session\");\nconst redisStore = require('connect-redis')(session);\nconst redisClient = require('./session-store/redisClient');\n\n...\n\napp.use(cookieParser());\napp.use(session({\n  store: new redisStore({client: redisClient, ttl: 3600 * 24 * 30}),\n  saveUninitialized: false,\n  secret: process.env.SESSION_SECRET,\n  resave: false\n}));\n\n`\n```\nThe problem is: upon starting the server i got error messages log in console like this:\n```\n`Redis auth error: The client is closed\n*Redis Client Error: connect ECONNREFUSED 127.0.0.1:6379\n*Redis Client Error: connect ECONNREFUSED 127.0.0.1:6379\n*Redis Client Error: connect ECONNREFUSED 127.0.0.1:6379\n*Redis Client Error: connect ECONNREFUSED 127.0.0.1:6379\n...\n`\n```\nI used this guide to set up redis cloud and assign dotenv variables (host, port and password). I have debugged and the dotenv is working fine and I have host, port and password variables correct.\nBut the problem still remains. I still get The client is closed and connect ECONNREFUSED 127.0.0.1:6379 error as in console log above. How can i fix this?",
      "solution": "was stuck on same issue and found some luck. have used 'ioredis' module instead of redis which worked seamlessly.\n```\n`const redis = require('ioredis');\nconst redisClient = redis.createClient({host:'your host address',port:your port,username:'',password:''});\n\nredisClient.on('connect',() => {\n    console.log('connected to redis successfully!');\n})\n\nredisClient.on('error',(error) => {\n    console.log('Redis connection error :', error);\n})\n\nmodule.exports = redisClient;\n`\n```",
      "question_score": 4,
      "answer_score": 9,
      "created_at": "2021-12-18T10:45:38",
      "url": "https://stackoverflow.com/questions/70402383/redis-cloud-and-connect-redis-the-client-is-closed"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 68021351,
      "title": ".NET service can&#39;t connect to Redis",
      "problem": "I'm trying to get a .NET API to be able to connect to a Redis instance on my docker network. Here's what I've got:\n\nRedis container: up and running on custom docker network `my-network` at `localhost:6379`\n.NET service: trying to get up and running on custom docker network `my-network`\n\n(Maybe?) Relevant versioning:\n\nAbp.RedisCache 5.4.0\nStackExchange.Redis 2.1.58\n\nI've tried everything, `network inspect`, adding and removing both containers to the network, even with the special connection string flag `resolveDns=true` in my .NET code. No matter what I do I get:\n```\n`StackExchange.Redis.RedisConnectionException: It was not possible to connect to the redis server(s). UnableToConnect on localhost:6379/Interactive, Initializing/NotStarted, last: NONE, origin: BeginConnectAsync, outstanding: 0, last-read: 0s ago, last-write: 0s ago, keep-alive: 60s, state: Connecting, mgr: 10 of 10 available, last-heartbeat: never, global: 0s ago, v: 2.1.58.34321\n`\n```\nI'm losing my mind here, Redis is up and running at `localhost:6379` on the docker network, why can't .NET connect?\nYES I've passed a custom redis.conf which has `bind 0.0.0.0` as well.",
      "solution": "Thanks to Hans Kilian's comment, I was able to finally connect!\nDespite what seemingly infinite Redis + Docker tutorials say on the using `localhost:6379`, this is in fact not the case when Redis is running in a Docker network. To find that IP, for example with a container named `redis`, issue:\n`docker inspect -f '{{range.NetworkSettings.Networks}}{{.IPAddress}}{{end}}' redis\n`\nIt is then the resulting IP address, with port 6379, that you would need to provide as your connection string.\ntl;dr;\nnot:\n`localhost:6379`\nbut the Docker assigned IP for the container, ex.:\n`172.18.0.7:6379`\nEDIT: You can also use the container name instead of the IP, i.e.:\n`redis:6379`\nHope this helps anyone who may stumble upon this.",
      "question_score": 4,
      "answer_score": 9,
      "created_at": "2021-06-17T16:27:41",
      "url": "https://stackoverflow.com/questions/68021351/net-service-cant-connect-to-redis"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 74514478,
      "title": "How to use BloomFilter with Redis",
      "problem": "I faced an issue with Redis Penetration. Too many meaningless queries from the clients go to the database because no keys are matched in Redis.\nNow I hope to use a BloomFilter to filter some meaningless queries, but I don't know how to use it with Redis or DB.\nWhen will the key be added in Bloom Filter ?  just before the key is added in Redis cache ?\nAre Keys in Bloom Filter deleted if the key in Redis expires?\nOr read all the keys from Database and put them in Blomm filter?\nbut if the key is deleted from DB,  can we delete the key from BloomFilter?",
      "solution": "How to avoid cache penetration\n\nYour best strategy of course would be to implement some logic (e.g., IP range filtering) before checking the cache.\nOn top of this, if the same unacceptable addresses are queried repeatedly, you may consider storing these addresses in Redis with an empty string value.\nIf you need to store millions of invalid keys, indeed, you may consider using a Bloom filter.\nRedisBloom is a Redis module, developed by Redis Inc., that adds probabilistic data structures (including a Bloom filter) to Redis. You can install RedisBloom on top of an existing Redis, or switch to Redis Stack, which includes RedisBloom to start with.\nRedisBloom commands are documented here.\nSimply create a Bloom filter using BF.RESERVE and add invalid addresses using BF.ADD. To determine if an invalid address has been seen before, use BF.EXISTS (the answer \"1\" means that, with high probability, the value has been seen before, and a \"0\" means that it definitely wasn't seen before).\n\nHow to handle incoming requests\n\nSince false positive matches are possible with a Bloom Filter (BF), you have several options:\n\nStore all valid keys in a BF upfront\n\nAdd all valid keys to the BF\nWhen a request is received, search in the Bloom filter.\nIf found in the BF - it is, with high probability, a valid key. Try to fetch it from the DB. If not found in the DB (low probability) it was a false positive.\nIf not found in the BF: it is necessarily an invalid key.\n\nStore valid keys in a BF on-the-fly\n\nWhen a request is received, search in the Bloom filter.\nIf found in the BF - it is, with high probability, a valid key that was already seen. Try to fetch it from the DB. If not found in the DB (low probability) it was a false positive.\nIf not found in the BF - it is either a first-time valid key or an invalid key. Check, and if valid - add to the BF.\n\nStore invalid keys in a BF\n\nWhen a request is received, search in the Bloom filter.\nIf found in the BF - it is, with high probability, an invalid key. Note that it may be a valid key (low probability) and you will ignore it, but that's a price you should be ready to pay if you go this way.\nIf not found in the BF - it is either a valid key or a first-time invalid key. Check, and if invalid - add to the BF.\n\nSome notes:\n\nYou don't need to add an item to a BF more than once (there is no benefit, but also no harm).\nYou cannot delete keys from a BF, but you can use a Cuckoo filter instead, which supports deletions (but has some disadvantages compared to BF). RedisBloom supports Cuckoo filters as well.",
      "question_score": 4,
      "answer_score": 8,
      "created_at": "2022-11-21T06:09:34",
      "url": "https://stackoverflow.com/questions/74514478/how-to-use-bloomfilter-with-redis"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 70160820,
      "title": "Cannot connect to Redis using node js module",
      "problem": "I have redis up and running on port 6379, connecting via telnet works fine.\nI'm trying to connect to it on node.js, but i'm getting no response from the event listeners.\nIf i make a call any function like client.set() I get: \"ClientClosedError: The client is closed\".\nThis is the code im running:\n`const redis = require('redis');\nconst client = redis.createClient(6379);\n\nclient.on('connect', () => {\n    console.log('connected');\n});\nclient.on('end', () => {\n    console.log('disconnected');\n});\nclient.on('reconnecting', () => {\n    console.log('reconnecting');\n});\nclient.on('error', (err) => {\n    console.log('error', { err });\n});\n\nsetTimeout(() => {console.log(\"goodbye\") }, 20*1000 );\n\n`\nNothing happens for 20 seconds, and then it closes",
      "solution": "Starting from v4 of `node-redis` library, you need to call `client.connect()` after initializing a client. See this migration guide.\n`const redis = require('redis');\nconst client = redis.createClient({ socket: { port: 6379 } });\n\nclient.connect();\n\nclient.on('connect', () => {\n    console.log('connected');\n});\n`\nYou might also want to consider running the client connect method with `await` in an asynchronous function. So you don't have to worry about event listeners.\n`const redis = require('redis');\n\n(async () => {\n  try {\n    const client = redis.createClient({ socket: { port: 6379 } });\n    await client.connect();\n    console.log('connected');\n  } catch (err) {\n    console.error(err)\n  }\n})()\n`",
      "question_score": 4,
      "answer_score": 8,
      "created_at": "2021-11-29T21:17:21",
      "url": "https://stackoverflow.com/questions/70160820/cannot-connect-to-redis-using-node-js-module"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 73309475,
      "title": "How do I run the config command on aws elasticache (redis)",
      "problem": "I am trying to setup a node client to subscribe to expired key events, doing something like this\n```\n`  export async function redisSubscriber(): Promise {\n    await redis.config(\"SET\", \"notify-keyspace-events\", \"Ex\");\n\n    const subscribe = redisServer.duplicate();\n\n    subscribe.subscribe(\"__keyevent@0__:expired\", async (key): Promise => {\n      logger.info(\"key expired=> \" + key);\n    });\n}\n\nredisSubscriber().catch((err: unknown) => {\n  logger.error(\n    `error in redis subscriber: ${\n      err instanceof Error ? err.message : inspect(err)\n    }`\n  );\n});\n`\n```\nHowever I am getting this error\n```\n`error in redis subscriber: ERR unknown command `config`, with args beginning with: `SET`, `notify-keyspace-events`, `Ex`\n`\n```\nI am assuming this is due to config being a restricted command.\nHow do I execute the config command from a node script to enable expired key events when redis runs as elasticache in aws?\nI have tried adding a Parameter Group in aws and assigned it to my instance and I can receive expired events using the redis-cli tool, however it does not seem to work inside a node script\n```\n`redis-cli -h  -p 6379 -a  --tls --csv subscribe '__keyevent@0__:expired'\n`\n```",
      "solution": "I was able to get it working by adding a Parameter Group with notify-keyspace-events set to Ex and tie it to the redis instance. For some reason I could not get the ioredis.subscribe to work, however it works using the default redis library.",
      "question_score": 4,
      "answer_score": 3,
      "created_at": "2022-08-10T18:07:01",
      "url": "https://stackoverflow.com/questions/73309475/how-do-i-run-the-config-command-on-aws-elasticache-redis"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 69397679,
      "title": "Dash/Plotly - long_callback fails with celery/redis backend",
      "problem": "Summary\nI\u2019ve been developing a dash app that uses a `long_callback`, and for development I\u2019ve been using a `diskcache` backend for my `long_callback_manager`, as recommended by the guide I found here: https://dash.plotly.com/long-callbacks\nWhen I tried running my app using gunicorn, it failed to start because of something apparently wrong with `diskcache`. As such, I decided to switch to a celery/redis backend since that\u2019s recommended for production anyway.\nI got a redis server running (responds properly to `redis-cli ping` with `PONG`), and then started the app again. This time it started up fine, and all of the normal callbacks work, but the `long_callback` does not work.\nDetails:\n\nThe page more or less hangs, with the page title flashing between the normal title and the `Updating...` title, indicating that the app thinks it\u2019s \u201cwaiting\u201d for a response/update from the `long_callback`.\nThe values set by the running argument of the `long_callback` are set to their starting value, indicating that the app recognizes that the `long_callback` ought to run.\nBy placing a print statement as the first line within the `long_callback` function and seeing that it does not print, I\u2019ve determined that the function never starts.\nThe failure happens both with gunicorn and without gunicorn.\n\nThese details all point to the problem being the celery/redis backend. No errors are shown, neither on the client/browser nor on the server\u2019s stdout/sterr.\nHow do I get a celery/redis backend working?\nUPDATE: After realizing that the `__name__` variable is being used and that its value changes depending on the file from which it is referenced, I've also tried moving the code which creates `celery_app` and `LONG_CALLBACK_MANAGER` into `app.py`, to no avail. The exact same thing happens.\nCode\napp.py\n`import dash\nimport dash_bootstrap_components as dbc\n\nfrom website.layout_main import define_callbacks, layout\nfrom website.long_callback_manager import LONG_CALLBACK_MANAGER\n\napp = dash.Dash(\n    __name__,\n    update_title=\"Loading...\",\n    external_stylesheets=[\n        dbc.themes.BOOTSTRAP,\n        \"https://codepen.io/chriddyp/pen/bWLwgP.css\"\n    ],\n    long_callback_manager=LONG_CALLBACK_MANAGER\n)\n\napp.title = \"CS 236 | Project Submissions\"\napp.layout = layout\ndefine_callbacks(app)\nserver = app.server  # expose for gunicorn\n\nif __name__ == \"__main__\":\n    app.run_server(debug=True, host=\"0.0.0.0\")\n`\nwebsite/long_callback_manager.py with diskcache (functional)\n`import os\nimport shutil\n\nimport diskcache\nfrom dash.long_callback import DiskcacheLongCallbackManager\n\nfrom util import RUN_DIR\n\ncache_dir = os.path.join(RUN_DIR, \"callback_cache\")\nshutil.rmtree(cache_dir, ignore_errors=True)  # ok if it didn't exist\n\ncache = diskcache.Cache(cache_dir)\n\nLONG_CALLBACK_MANAGER = DiskcacheLongCallbackManager(cache)\n`\nwebsite/long_callback_manager.py with celery/redis (not functional)\n`from dash.long_callback import CeleryLongCallbackManager\nfrom celery import Celery\n\ncelery_app = Celery(\n    __name__,\n    broker=\"redis://localhost:6379/0\",\n    backend=\"redis://localhost:6379/1\"\n)\n\nLONG_CALLBACK_MANAGER = CeleryLongCallbackManager(celery_app)\n`\nwebsite/layout_main.py\n`from typing import Union\n\nimport dash\nimport dash_bootstrap_components as dbc\nfrom dash import dcc, html\nfrom dash.dependencies import Input, Output, State\n\nfrom util.authenticator import authenticate\nfrom website import ID_LOGIN_STORE, NET_ID, PASSWORD\nfrom website.tabs.config import define_config_callbacks, layout as config_layout\nfrom website.tabs.log import define_log_callbacks, layout as log_layout\nfrom website.tabs.submit import define_submit_callbacks, layout as submit_layout\nfrom website.util import AUTH_FAILED_MESSAGE, STYLE_RED\n\n# cache\nLOGIN_INFO_EMPTY = {NET_ID: None, PASSWORD: None}\n# button display modes\nVISIBLE = \"inline-block\"\nHIDDEN = \"none\"\n\n# header\nID_LOGIN_BUTTON = \"login-button\"\nID_LOGGED_IN_AS = \"logged-in-as\"\nID_LOGOUT_BUTTON = \"logout-button\"\n# tabs\nID_TAB_SELECTOR = \"tab-selector\"\nID_SUBMIT_TAB = \"submit-tab\"\nID_LOG_TAB = \"log-tab\"\nID_CONFIG_TAB = \"config-tab\"\n# login modal\nID_LOGIN_MODAL = \"login-modal\"\nID_LOGIN_MODAL_NET_ID = \"login-modal-net-id\"\nID_LOGIN_MODAL_PASSWORD = \"login-modal-password\"\nID_LOGIN_MODAL_MESSAGE = \"login-modal-message\"\nID_LOGIN_MODAL_CANCEL = \"login-modal-cancel\"\nID_LOGIN_MODAL_ACCEPT = \"login-modal-accept\"\n# logout modal\nID_LOGOUT_MODAL = \"logout-modal\"\nID_LOGOUT_MODAL_CANCEL = \"logout-modal-cancel\"\nID_LOGOUT_MODAL_ACCEPT = \"logout-modal-accept\"\n\nlayout = html.Div([\n    dcc.Store(id=ID_LOGIN_STORE, storage_type=\"session\", data=LOGIN_INFO_EMPTY),\n    html.Div(\n        [\n            html.H2(\"BYU CS 236 - Project Submission Website\", style={\"marginLeft\": \"10px\"}),\n            html.Div(\n                [\n                    html.Div(id=ID_LOGGED_IN_AS, style={\"display\": HIDDEN, \"marginRight\": \"10px\"}),\n                    html.Button(\"Log in\", id=ID_LOGIN_BUTTON, style={\"display\": VISIBLE}),\n                    html.Button(\"Log out\", id=ID_LOGOUT_BUTTON, style={\"display\": HIDDEN})\n                ],\n                style={\n                    \"marginRight\": \"25px\",\n                    \"display\": \"flex\",\n                    \"alignItems\": \"center\"\n                }\n            )\n        ],\n        style={\n            \"height\": \"100px\",\n            \"marginLeft\": \"10px\",\n            \"marginRight\": \"10px\",\n            \"display\": \"flex\",\n            \"alignItems\": \"center\",\n            \"justifyContent\": \"space-between\"\n        }\n    ),\n    dcc.Tabs(id=ID_TAB_SELECTOR, value=ID_SUBMIT_TAB, children=[\n        dcc.Tab(submit_layout, label=\"New Submission\", value=ID_SUBMIT_TAB),\n        dcc.Tab(log_layout, label=\"Submission Logs\", value=ID_LOG_TAB),\n        dcc.Tab(config_layout, label=\"View Configuration\", value=ID_CONFIG_TAB)\n    ]),\n    dbc.Modal(\n        [\n            dbc.ModalHeader(\"Log In\"),\n            dbc.ModalBody([\n                html.Div(\n                    [\n                        html.Label(\"BYU Net ID:\", style={\"marginRight\": \"10px\"}),\n                        dcc.Input(\n                            id=ID_LOGIN_MODAL_NET_ID,\n                            type=\"text\",\n                            autoComplete=\"username\",\n                            value=\"\",\n                            style={\"marginRight\": \"30px\"}\n                        )\n                    ],\n                    style={\n                        \"marginBottom\": \"5px\",\n                        \"display\": \"flex\",\n                        \"alignItems\": \"center\",\n                        \"justifyContent\": \"flex-end\"\n                    }\n                ),\n                html.Div(\n                    [\n                        html.Label(\"Submission Password:\", style={\"marginRight\": \"10px\"}),\n                        dcc.Input(\n                            id=ID_LOGIN_MODAL_PASSWORD,\n                            type=\"password\",\n                            autoComplete=\"current-password\",\n                            value=\"\",\n                            style={\"marginRight\": \"30px\"}\n                        )\n                    ],\n                    style={\n                        \"display\": \"flex\",\n                        \"alignItems\": \"center\",\n                        \"justifyContent\": \"flex-end\"\n                    }\n                ),\n                html.Div(id=ID_LOGIN_MODAL_MESSAGE, style={\"textAlign\": \"center\", \"marginTop\": \"10px\"})\n            ]),\n            dbc.ModalFooter([\n                html.Button(\"Cancel\", id=ID_LOGIN_MODAL_CANCEL),\n                html.Button(\"Log In\", id=ID_LOGIN_MODAL_ACCEPT)\n            ])\n        ],\n        id=ID_LOGIN_MODAL,\n        is_open=False\n    ),\n    dbc.Modal(\n        [\n            dbc.ModalHeader(\"Log Out\"),\n            dbc.ModalBody(\"Are you sure you want to log out?\"),\n            dbc.ModalFooter([\n                html.Button(\"Stay Logged In\", id=ID_LOGOUT_MODAL_CANCEL),\n                html.Button(\"Log Out\", id=ID_LOGOUT_MODAL_ACCEPT)\n            ])\n        ],\n        id=ID_LOGOUT_MODAL,\n        is_open=False\n    )\n])\n\ndef on_click_login_modal_accept(net_id: Union[str, None], password: Union[str, None]) -> Union[str, None]:\n    # validate\n    if net_id is None or net_id == \"\":\n        return \"BYU Net ID is required.\"\n    if password is None or password == \"\":\n        return \"Submission Password is required.\"\n    # authenticate\n    auth_success = authenticate(net_id, password)\n    if auth_success:\n        return None\n    else:\n        return AUTH_FAILED_MESSAGE\n\ndef define_callbacks(app: dash.Dash):\n    @app.callback(Output(ID_LOGIN_MODAL, \"is_open\"),\n                  Output(ID_LOGIN_MODAL_MESSAGE, \"children\"),\n                  Output(ID_LOGOUT_MODAL, \"is_open\"),\n                  Output(ID_LOGIN_STORE, \"data\"),\n                  Input(ID_LOGIN_BUTTON, \"n_clicks\"),\n                  Input(ID_LOGIN_MODAL_CANCEL, \"n_clicks\"),\n                  Input(ID_LOGIN_MODAL_ACCEPT, \"n_clicks\"),\n                  Input(ID_LOGOUT_BUTTON, \"n_clicks\"),\n                  Input(ID_LOGOUT_MODAL_CANCEL, \"n_clicks\"),\n                  Input(ID_LOGOUT_MODAL_ACCEPT, \"n_clicks\"),\n                  State(ID_LOGIN_MODAL_NET_ID, \"value\"),\n                  State(ID_LOGIN_MODAL_PASSWORD, \"value\"),\n                  prevent_initial_call=True)\n    def on_login_logout_clicked(\n            n_login_clicks: int,\n            n_login_cancel_clicks: int,\n            n_login_accept_clicks: int,\n            n_logout_clicks: int,\n            n_logout_cancel_clicks: int,\n            n_logout_accept_clicks: int,\n            net_id: str,\n            password: str):\n        ctx = dash.callback_context\n        btn_id = ctx.triggered[0][\"prop_id\"].split(\".\")[0]\n        if btn_id == ID_LOGIN_BUTTON:\n            # show the login modal (with no message)\n            return True, None, dash.no_update, dash.no_update\n        elif btn_id == ID_LOGIN_MODAL_CANCEL:\n            # hide the login modal\n            return False, dash.no_update, dash.no_update, dash.no_update\n        elif btn_id == ID_LOGIN_MODAL_ACCEPT:\n            # try to actually log in\n            error_message = on_click_login_modal_accept(net_id, password)\n            if error_message is None:  # login success!\n                # hide the modal, update the login store\n                return False, dash.no_update, dash.no_update, {NET_ID: net_id, PASSWORD: password}\n            else:  # login failed\n                # show the message and keep the modal open\n                return dash.no_update, html.Span(error_message, style=STYLE_RED), dash.no_update, dash.no_update\n        elif btn_id == ID_LOGOUT_BUTTON:\n            # show the logout modal\n            return dash.no_update, dash.no_update, True, dash.no_update\n        elif btn_id == ID_LOGOUT_MODAL_CANCEL:\n            # hide the logout modal\n            return dash.no_update, dash.no_update, False, dash.no_update\n        elif btn_id == ID_LOGOUT_MODAL_ACCEPT:\n            # hide the logout modal and clear the login store\n            return dash.no_update, dash.no_update, False, LOGIN_INFO_EMPTY\n        else:  # error\n            print(f\"unknown button id: {btn_id}\")  # TODO: better logging\n            return [dash.no_update] * 4  # one for each Output\n\n    @app.callback(Output(ID_LOGIN_BUTTON, \"style\"),\n                  Output(ID_LOGGED_IN_AS, \"children\"),\n                  Output(ID_LOGGED_IN_AS, \"style\"),\n                  Output(ID_LOGOUT_BUTTON, \"style\"),\n                  Input(ID_LOGIN_STORE, \"data\"),\n                  State(ID_LOGIN_BUTTON, \"style\"),\n                  State(ID_LOGGED_IN_AS, \"style\"),\n                  State(ID_LOGOUT_BUTTON, \"style\"))\n    def on_login_data_changed(login_store, login_style, logged_in_as_style, logout_style):\n        # just in case no style is provided\n        if login_style is None:\n            login_style = dict()\n        if logged_in_as_style is None:\n            logged_in_as_style = dict()\n        if logout_style is None:\n            logout_style = dict()\n        # are they logged in or not?\n        if login_store[NET_ID] is None or login_store[PASSWORD] is None:\n            # not logged in\n            login_style[\"display\"] = VISIBLE\n            logged_in_as_style[\"display\"] = HIDDEN\n            logout_style[\"display\"] = HIDDEN\n            return login_style, None, logged_in_as_style, logout_style\n        else:  # yes logged in\n            login_style[\"display\"] = HIDDEN\n            logged_in_as_style[\"display\"] = VISIBLE\n            logout_style[\"display\"] = VISIBLE\n            return login_style, f\"Logged in as '{login_store[NET_ID]}'\", logged_in_as_style, logout_style\n\n    # define callbacks for all of the tabs\n    define_submit_callbacks(app)\n    define_log_callbacks(app)\n    define_config_callbacks(app)\n`\nwebsite/tabs/submit.py\n`import os\nimport time\nfrom io import StringIO\nfrom typing import Callable, Dict, Union\n\nimport dash\nimport dash_bootstrap_components as dbc\nimport dash_gif_component as gif\nfrom dash import dcc, html\nfrom dash.dependencies import Input, Output, State\nfrom dash.exceptions import PreventUpdate\n\nfrom config.loaded_config import CONFIG\nfrom driver.passoff_driver import PassoffDriver\nfrom util.authenticator import authenticate\nfrom website import ID_LOGIN_STORE, NET_ID, PASSWORD\nfrom website.util import AUTH_FAILED_MESSAGE, save_to_submit, STYLE_DIV_VISIBLE, STYLE_DIV_VISIBLE_TOP_MARGIN, STYLE_HIDDEN, text_html_colorizer\n\n# submit tab IDs\nID_SUBMISSION_ROOT_DIV = \"submission-root-div\"\nID_SUBMIT_PROJECT_NUMBER_RADIO = \"submit-project-number-radio\"\nID_UPLOAD_BUTTON = \"upload-button\"\nID_UPLOAD_CONTENTS = \"upload-contents\"\nID_FILE_NAME_DISPLAY = \"file-name-display\"\nID_SUBMISSION_SUBMIT_BUTTON = \"submission-submit-button\"\nID_SUBMISSION_OUTPUT = \"submission-output\"\nID_SUBMISSION_LOADING = \"submission-loading\"\n# clear/refresh to submit again\nID_SUBMISSION_REFRESH_BUTTON = \"submission-refresh-button\"\nID_SUBMISSION_REFRESH_DIV = \"submission-refresh-div\"\nID_SUBMISSION_RESETTING_STORE = \"submission-resetting-store\"\n# info modal\nID_SUBMISSION_INFO_MODAL = \"submission-info-modal\"\nID_SUBMISSION_INFO_MODAL_MESSAGE = \"submission-info-modal-message\"\nID_SUBMISSION_INFO_MODAL_ACCEPT = \"submission-info-modal-accept\"\n# submission confirmation modal\nID_SUBMISSION_CONFIRMATION_MODAL = \"submission-confirmation-modal\"\nID_SUBMISSION_CONFIRMATION_MODAL_CANCEL = \"submission-confirmation-modal-cancel\"\nID_SUBMISSION_CONFIRMATION_MODAL_ACCEPT = \"submission-confirmation-modal-accept\"\n# store to trigger submission\nID_SUBMISSION_TRIGGER_STORE = \"submission-trigger-store\"\n\nLAYOUT_DEFAULT_CONTENTS = [\n    html.H3(\"Upload New Submission\"),\n    html.P(\"Which project are you submitting?\"),\n    dcc.RadioItems(\n        id=ID_SUBMIT_PROJECT_NUMBER_RADIO,\n        options=[{\n            \"label\": f\" Project {proj_num}\",\n            \"value\": proj_num\n        } for proj_num in range(1, CONFIG.n_projects + 1)]\n    ),\n    html.Br(),\n    html.P(\"Upload your .zip file here:\"),\n    html.Div(\n        [\n            dcc.Upload(\n                html.Button(\"Select File\", id=ID_UPLOAD_BUTTON),\n                id=ID_UPLOAD_CONTENTS,\n                multiple=False\n            ),\n            html.Pre(\"No File Selected\", id=ID_FILE_NAME_DISPLAY, style={\"marginLeft\": \"10px\"})\n        ],\n        style={\n            \"display\": \"flex\",\n            \"justifyContent\": \"flex-start\",\n            \"alignItems\": \"center\"\n        }\n    ),\n    html.Button(\"Submit\", id=ID_SUBMISSION_SUBMIT_BUTTON, style={\"marginTop\": \"20px\"}),\n    html.Div(id=ID_SUBMISSION_OUTPUT, style=STYLE_HIDDEN),\n    html.Div(\n        html.Div(\n            gif.GifPlayer(\n                gif=os.path.join(\"assets\", \"loading.gif\"),\n                still=os.path.join(\"assets\", \"loading.png\"),\n                alt=\"loading symbol\",\n                autoplay=True\n            ),\n            style={\"zoom\": \"0.2\"}\n        ),\n        id=ID_SUBMISSION_LOADING,\n        style=STYLE_HIDDEN\n    ),\n    html.Div(\n        [\n            html.P(\"Reset the page to submit again:\"),\n            html.Button(\"Reset\", id=ID_SUBMISSION_REFRESH_BUTTON),\n        ],\n        id=ID_SUBMISSION_REFRESH_DIV,\n        style=STYLE_HIDDEN\n    ),\n    dbc.Modal(\n        [\n            dbc.ModalHeader(\"Try Again\"),\n            dbc.ModalBody(id=ID_SUBMISSION_INFO_MODAL_MESSAGE),\n            dbc.ModalFooter([\n                html.Button(\"OK\", id=ID_SUBMISSION_INFO_MODAL_ACCEPT)\n            ])\n        ],\n        id=ID_SUBMISSION_INFO_MODAL,\n        is_open=False\n    ),\n    dbc.Modal(\n        [\n            dbc.ModalHeader(\"Confirm Submission\"),\n            dbc.ModalBody(\"Are you sure you want to officially submit?\"),\n            dbc.ModalFooter([\n                html.Button(\"Cancel\", id=ID_SUBMISSION_CONFIRMATION_MODAL_CANCEL),\n                html.Button(\"Submit\", id=ID_SUBMISSION_CONFIRMATION_MODAL_ACCEPT)\n            ])\n        ],\n        id=ID_SUBMISSION_CONFIRMATION_MODAL,\n        is_open=False\n    )\n]\n\nlayout = html.Div(\n    [\n        html.Div(LAYOUT_DEFAULT_CONTENTS, id=ID_SUBMISSION_ROOT_DIV),\n        # having this store outside of the layout that gets reset means the long callback is not triggered\n        dcc.Store(id=ID_SUBMISSION_TRIGGER_STORE, storage_type=\"memory\", data=False)  # data value just flips to trigger the long callback\n    ],\n    style={\n        \"margin\": \"10px\",\n        \"padding\": \"10px\",\n        \"borderStyle\": \"double\"\n    }\n)\n\ndef on_submit_button_clicked(\n        proj_number: Union[int, None],\n        file_name: Union[str, None],\n        file_contents: Union[str, None],\n        login_store: Union[Dict[str, str], None]) -> Union[str, None]:\n    # validate\n    if login_store is None or NET_ID not in login_store or PASSWORD not in login_store:\n        return \"There was a problem with the login store!\"\n    net_id = login_store[NET_ID]\n    password = login_store[PASSWORD]\n    if net_id is None or net_id == \"\" or password is None or password == \"\":\n        return \"You must log in before submitting.\"\n    if proj_number is None:\n        return \"The project number must be selected.\"\n    if not (1 \nEnvironment\npython version\n`$ python --version\nPython 3.9.6\n`\ninstalled packages\n`$ pip list\nPackage                   Version\n------------------------- ---------\namqp                      5.0.6\nbilliard                  3.6.4.0\nBrotli                    1.0.9\ncelery                    5.1.2\nclick                     7.1.2\nclick-didyoumean          0.0.3\nclick-plugins             1.1.1\nclick-repl                0.2.0\ndash                      2.0.0\ndash-bootstrap-components 0.13.1\ndash-core-components      2.0.0\ndash-gif-component        1.1.0\ndash-html-components      2.0.0\ndash-table                5.0.0\ndill                      0.3.4\ndiskcache                 5.2.1\nFlask                     2.0.1\nFlask-Compress            1.10.1\ngreenlet                  1.1.1\ngunicorn                  20.1.0\nitsdangerous              2.0.1\nJinja2                    3.0.1\nkombu                     5.1.0\nMarkupSafe                2.0.1\nmultiprocess              0.70.12.2\norjson                    3.6.3\npip                       21.2.4\nplotly                    5.3.1\nprompt-toolkit            3.0.20\npsutil                    5.8.0\npytz                      2021.1\nredis                     3.5.3\nsetuptools                56.0.0\nsix                       1.16.0\nSQLAlchemy                1.4.23\ntenacity                  8.0.1\nvine                      5.0.0\nwcwidth                   0.2.5\nWerkzeug                  2.0.1\nwheel                     0.37.0\n`\nredis\n`$ redis-server --version\nRedis server v=6.2.5 sha=00000000:0 malloc=jemalloc-5.1.0 bits=64 build=2a367e4b809d24de\n$ redis-cli ping\nPONG\n$ curl localhost:6379\ncurl: (52) Empty reply from server\n$ sudo ss -lptn 'sport = :6379'\nState         Recv-Q        Send-Q               Local Address:Port               Peer Address:Port       Process\nLISTEN        0             511                      127.0.0.1:6379                    0.0.0.0:*           users:((\"redis-server\",pid=373,fd=6))\n`\nOS details\nWindows, WSL 2, Ubuntu\n`$ cat /etc/os-release\nNAME=\"Ubuntu\"\nVERSION=\"20.04.3 LTS (Focal Fossa)\"\nID=ubuntu\nID_LIKE=debian\nPRETTY_NAME=\"Ubuntu 20.04.3 LTS\"\nVERSION_ID=\"20.04\"\nHOME_URL=\"https://www.ubuntu.com/\"\nSUPPORT_URL=\"https://help.ubuntu.com/\"\nBUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\nPRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\nVERSION_CODENAME=focal\nUBUNTU_CODENAME=focal\n`",
      "solution": "Re-posting the solution from the plotly community forum:\nhttps://community.plotly.com/t/long-callback-with-celery-redis-how-to-get-the-example-app-work/57663\nSummary\nIn order for the long callback to work, I needed to start 3 separate processes that work in conjunction:\n\nthe Redis server: `redis-server`\nthe Celery app: `celery -A app.celery worker --loglevel=INFO`\nthe Dash app: `python app.py`\n\nThe commands listed above are the simplest version. The full commands used are given further down with appropriate modifications.\nDetails\nI moved the declaration of the celery app from `src/website/long_callback_manager.py` to `src/app.py` for easier external access:\n`import dash\nimport dash_bootstrap_components as dbc\nfrom celery import Celery\nfrom dash.long_callback import CeleryLongCallbackManager\n\nfrom website.layout_main import define_callbacks, layout\n\ncelery_app = Celery(\n    __name__,\n    broker=\"redis://localhost:6379/0\",\n    backend=\"redis://localhost:6379/1\"\n)\nLONG_CALLBACK_MANAGER = CeleryLongCallbackManager(celery_app)\n\napp = dash.Dash(\n    __name__,\n    update_title=\"Loading...\",\n    external_stylesheets=[\n        dbc.themes.BOOTSTRAP,\n        \"https://codepen.io/chriddyp/pen/bWLwgP.css\"\n    ],\n    long_callback_manager=LONG_CALLBACK_MANAGER\n)\n\napp.title = \"CS 236 | Project Submissions\"\napp.layout = layout\ndefine_callbacks(app)\nserver = app.server  # expose for gunicorn\n\nif __name__ == \"__main__\":\n    app.run_server(debug=True, host=\"0.0.0.0\")\n`\nThen I used the following bash script to simplify the process of starting everything up:\n`#!/bin/bash\nset -e  # quit on any error\n\n# make sure the redis server is running\nif ! redis-cli ping > /dev/null 2>&1; then\n  redis-server --daemonize yes --bind 127.0.0.1\n  redis-cli ping > /dev/null 2>&1  # the script halts if redis is not now running (failed to start)\nfi\n\n# activate the venv that has our things installed with pip\n. venv/bin/activate\n\n# make sure it can find the python modules, but still run from this directory\nexport PYTHONPATH=src\n\n# make sure we have a log directory\nmkdir -p Log\n\n# start the celery thing\ncelery -A app.celery_app worker --loglevel=INFO >> Log/celery_info.log 2>&1 &\n\n# start the server\ngunicorn --workers=4 --name=passoff_website_server --bind=127.0.0.1:8050 app:server >> Log/gunicorn.log 2>&1\n`\nThis script's process is then parent to the celery and gunicorn subprocesses and all can be terminated as a bundle by terminating the parent process.\nEdit/Update: deprecation of `long_callback`\nAs pointed out by @punit-vara, Dash 2.6 now allows `@dash.callback` to use `background=True`, which is the recommended replacement for using `long_callback`.\nSee Dash's official page on the topic",
      "question_score": 4,
      "answer_score": 7,
      "created_at": "2021-09-30T21:08:57",
      "url": "https://stackoverflow.com/questions/69397679/dash-plotly-long-callback-fails-with-celery-redis-backend"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 69135302,
      "title": "How to handle external errors when using Tonic where I must use tonic::Status?",
      "problem": "I am making a Tonic-based gRPC microservice that uses the Redis client. I can't figure out an example of implicitly converting a `RedisError` into a `tonic::Status` when an asynchronous error occurs.\n`async fn make_transaction(\n    &self,\n    request: Request,\n) -> Result, Status> {\n    let mut con = self.client.get_async_connection().await?;\n    con.set(\"my_key\", 42).await?;\n    ...\n}\n`\nThe connection from Redis client can fail as well as the set. I would rather not use `.map_err()` since that seems to break the async.\nI was thinking I need to implement the trait `From` and `From` but not sure how to do it. This is my attempt, but it doesn't work since Tonic wants a `tonic::Status`, not a `ApiError` struct that I made:\n`pub struct ApiError {}\n\nimpl From for ApiError {\n    fn from(err: Status) -> ApiError {\n        ApiError {  }\n    }\n}\n\nimpl From for Status {\n    fn from(err: redis::RedisError) -> ApiError {\n        ApiError {  }\n    }\n}\n`",
      "solution": "I managed to get around this type of problem by using a custom type as a transition vessel for the original error (which is `RedisError` in your case), so something like the following might work for you as well:\n`pub struct ApiError {}\n\nimpl From for ApiError {\n    fn from(err: RedisError) -> Self {\n        Self { }\n    }\n}\n\nimpl From for Status {\n    fn from(err: ApiError) -> Self {\n        Self::internal(\"Failed talking to Redis\")\n    }\n}\n\nasync fn set_key(client: ..., key: &str, value: i64) -> Result {\n    let mut con = client.get_async_connection().await?;\n    con.set(key, value).await?\n    ...\n}\n\nasync fn make_transaction(\n    &self,\n    request: Request,\n) -> Result, Status> {\n    set_key(self.client, \"my_key\".into(), 42).await?\n}\n`\nI am not sure if this is the best way, but seems to do the job.",
      "question_score": 4,
      "answer_score": 7,
      "created_at": "2021-09-10T18:31:02",
      "url": "https://stackoverflow.com/questions/69135302/how-to-handle-external-errors-when-using-tonic-where-i-must-use-tonicstatus"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 68124389,
      "title": "Redis Error condition on socket for SYNC: Connection refused",
      "problem": "Using celery with redis in my django app\nEverything was working fine until I ran into a problem. The location of the redis files was changed and redis could not access them. After searching, it turns out that this is due to random attacks on the Internet, and it is necessary to add confg\nAfter I added the file, they both worked fine for a while, and then this problem appeared\n```\n`1:S 25 Jun 2021 00:48:12.029 # Error condition on socket for SYNC: Connection refused\n1:S 25 Jun 2021 00:48:12.901 * Connecting to MASTER 194.38.20.199:8886\n1:S 25 Jun 2021 00:48:12.902 * MASTER  REPLICA sync started\n1:S 25 Jun 2021 00:48:13.034 # Error condition on socket for SYNC: Connection refused\n1:S 25 Jun 2021 00:48:13.907 * Connecting to MASTER 194.38.20.199:8886\n1:S 25 Jun 2021 00:48:13.908 * MASTER  REPLICA sync started\n1:S 25 Jun 2021 00:48:14.041 # Error condition on socket for SYNC: Connection refused\n`\n```\nWhen I rebuild the Redis container it runs for a very short time and I get the same problem\nsettings.py\n```\n`CELERY_BROKER_URL = os.environ.get(\"redis://redis:6379/0\")\nCELERY_RESULT_BACKEND = os.environ.get(\"redis://redis:6379/0\")\n`\n```\n\ndocker-compose.yml\n```\n`     redis:\n      image: 'redis:alpine'\n      restart: unless-stopped\n      command: redis-server /usr/local/etc/redis/redis.conf\n      volumes:\n        - ./docker/redis/redis.conf:/usr/local/etc/redis/redis.conf\n      ports:\n        - '6379:6379'\n\n  celery:\n    restart: unless-stopped\n    build:\n      context: .\n      dockerfile: ./docker/backend/Dockerfile_celery\n    entrypoint: /app/docker/backend/celery-entrypoint.sh\n    environment:\n      - some env vars\n    depends_on:\n      - asgiserver\n      - redis\n`\n```\n\nredis.conf\n```\n`    bind 0.0.0.0 \n    protected-mode yes\n    rename-command CONFIG \"\"\n`\n```",
      "solution": "solved by change `ports` config with `expose`  as Iain Shelvington mentioned above\n```\n`   redis:\n      image: 'redis:alpine'\n      restart: unless-stopped\n      command: redis-server /usr/local/etc/redis/redis.conf\n      volumes:\n        - ./docker/redis/redis.conf:/usr/local/etc/redis/redis.conf\n      expose:\n        - 6379\n`\n```",
      "question_score": 4,
      "answer_score": 7,
      "created_at": "2021-06-25T03:33:58",
      "url": "https://stackoverflow.com/questions/68124389/redis-error-condition-on-socket-for-sync-connection-refused"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 70219951,
      "title": "TypeError [ERR_INVALID_ARG_TYPE], express-session+redis Error",
      "problem": "I am developing with typescript + express\ni want use Redis to Session Storage and i installed redis, connect-redis\nand i code below\n```\n`import { createClient } from 'redis';\nimport * as RedisStore from 'connect-redis';\n...\nconst client = createClient({\n    url: 'redis://default:qwer1234@localhost:6379',\n});\nclient.connect().then(() => {\n    console.log('redis success');\n});\n...\napp.use(\n    session({\n        secret: env.COOKIE_SECRET!,\n        resave: false,\n        saveUninitialized: false,\n        store: new redisSession({\n            client: client,\n        }),\n    })\n);\n\n`\n```\nand i wrote code below to test redis\n```\n`app.get('/', (req, res) => {\n    const sess = req.session;\n    if (sess.key) {\n        res.send(sess.key);\n    } else {\n        res.send('FAIL');\n    }\n});\napp.post('/login', (req, res) => {\n    const sess = req.session;\n    const { username } = req.body;\n    sess.key = username\n    // add username and password validation logic here if you want.If user is authenticated send the response as success\n    res.end('success');\n});\napp.get('/logout', (req, res) => {\n    req.session.destroy((err) => {\n        if (err) {\n            return console.log(err);\n        }\n        res.send('OK');\n    });\n});\n`\n```\nBut , I get the following error when saving a value to the session.\nTypeError [ERR_INVALID_ARG_TYPE]: The \"chunk\" argument must be of type string or an instance of Buffer or Uint8Array. Received an instance of Array\nbecause this error, the passport-local code that was previously implemented using memory storage does not work.\nhelp me plz..",
      "solution": "There is an issue when using the current time-based redis 4.0 version, so you need to set `legacyMode: true` in the redis setting.\n`const client = createClient({\n    url: yourURL,\n    legacyMode: true,\n});\n`",
      "question_score": 4,
      "answer_score": 5,
      "created_at": "2021-12-03T20:49:16",
      "url": "https://stackoverflow.com/questions/70219951/typeerror-err-invalid-arg-type-express-sessionredis-error"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 75393757,
      "title": "How to escape dot (.) in a key to get JSON value with redis-py?",
      "problem": "I had some JSON data containing dot(.) in keys that are written to redis using redis-py like this:\n`r = redis.Redis()\nr.json().set(_id, \"$\", {'First.Last': \"John.Smith\"})\n`\nIt works if reading the whole JSON data like\n`r.json().get(_id)\n`\nbut error throws if directly getting the value with a path containing the dot:\n```\n`r.json().get(_id, \"First\\.Last\")  # ResponseError\n`\n```\nObviously, the dot is not correctly escaped. Also tried to quote it like `\"'First.Last'\"`but doesn't work either. What's the correct way to deal with this special character in a key? The redis instance is a redis-stack-server docker container running on linux. Thank you!",
      "solution": "RedisJSON path supports both dot and bracket notation https://redis.io/docs/stack/json/path/, so you could use this\n```\n`r.json().get(_id, '[\"First.Last\"]')\n`\n```",
      "question_score": 4,
      "answer_score": 4,
      "created_at": "2023-02-09T03:59:30",
      "url": "https://stackoverflow.com/questions/75393757/how-to-escape-dot-in-a-key-to-get-json-value-with-redis-py"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 75852921,
      "title": "Unable to connect to redis with ip address",
      "problem": "I installed redis on my server. I am unable to connect to server with ip address but able to connect with `localhost`. I am unable to connect to this redis from other servers too. What could be the reason?\n```\n`ubuntu@ip-172-31-91-211:~$ ifconfig\neth0: flags=4163  mtu 9001\n        inet 172.31.70.211  netmask 255.255.240.0  broadcast 172.31.95.255\n        inet6 fe80::1052:1ff:fee4:cfdd  prefixlen 64  scopeid 0x20\n        ether 12:52:01:e4:cf:dd  txqueuelen 1000  (Ethernet)\n        RX packets 974  bytes 304521 (304.5 KB)\n        RX errors 0  dropped 0  overruns 0  frame 0\n        TX packets 937  bytes 130927 (130.9 KB)\n        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0\n\nlo: flags=73  mtu 65536\n        inet 127.0.0.1  netmask 255.0.0.0\n        inet6 ::1  prefixlen 128  scopeid 0x10\n        loop  txqueuelen 1000  (Local Loopback)\n        RX packets 794  bytes 63335 (63.3 KB)\n        RX errors 0  dropped 0  overruns 0  frame 0\n        TX packets 794  bytes 63335 (63.3 KB)\n        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0\n\nubuntu@ip-172-31-91-211:~$ redis-cli -h 172.31.70.211\nCould not connect to Redis at 172.31.70.211:6379: Connection refused\nnot connected> \nubuntu@ip-172-31-91-211:~$ redis-cli -h localhost\nlocalhost:6379> \nubuntu@ip-172-31-91-211:~$ redis-cli -h 127.0.0.1\n127.0.0.1:6379> \nubuntu@ip-172-31-91-211:~$\n`\n```\nI am able to connect to this process with ip address when it is different application\n```\n`ubuntu@ip-172-31-70-211:~$ python3 -m http.server 6379 &\n[1] 992\nubuntu@ip-172-31-70-211:~$ Serving HTTP on 0.0.0.0 port 6379 (http://0.0.0.0:6379/) ...\n\nubuntu@ip-172-31-70-211:~$ curl 172.31.91.211:6379\n172.31.91.211 - - [27/Mar/2023 07:01:42] \"GET / HTTP/1.1\" 200 -\n\nDirectory listing for /\n\nDirectory listing for /\n\n.bash_history\n.bash_logout\n.bashrc\n.cache/\n.profile\n.rediscli_history\n.ssh/\n.sudo_as_admin_successful\n.viminfo\nDevelopment/\n\n`\n```",
      "solution": "Try to change bind IP\n\nGo to the redis file location and open the Redis configuration file with a text editor\n`sudo nano /etc/redis/redis.conf\n`\n\nchange bind IP (you can use allowed need IP also)\n`bind 0.0.0.0 ::\n`\n\nfinally restart the redis\n`sudo systemctl restart redis\n`",
      "question_score": 4,
      "answer_score": 3,
      "created_at": "2023-03-27T09:03:26",
      "url": "https://stackoverflow.com/questions/75852921/unable-to-connect-to-redis-with-ip-address"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 73631825,
      "title": "Java how to Uni.createFrom().future() and return that Uni from the enclosing method?",
      "problem": "I am very new to Java and Mutiny.\nAs indicated below, my test function asks Redis for the value of key \"foo\" which is \"bar\". That is working and the Future onCompleted() gets \"bar\".\nSo far so good.\nI have two issues with the `Uni.createFrom().future()` bit.\n\nThe compiler says: `The method future(Future) in the type UniCreate is not applicable for the arguments (Future)`. I have tried the suggested fixes but ended up in a deeper hole. My Java skills are insufficient to fully grasp the meaning of the error.\n\nHow do I get \"bar\" into the `Uni` returned from `test()`? I have tried all sorts of subscribing and CompletableFutures and cannot make anything work. I figure I need to return a function to generate the Uni but am at a loss about how to do that.\n\n`// The abbreviated setup\nimport io.vertx.redis.client.Redis;\nprivate final Redis redisClient;\nthis.redisClient = Redis.createClient(vertx);\n\npublic Uni test () {\n  // Ask Redis for the value of key \"foo\" => \"bar\"\n  Future futureResponse = this.redisClient.send(Request.cmd(Command.create(\"JSON.GET\")).arg(\"foo\"))\n      .compose(response -> {\n        // response == 'bar'\n        return Future.succeededFuture(response);\n      }).onComplete(res -> {\n        // res == 'bar'\n      });\n\n  // How to make the return of the Uni wait for the completed futureResponse \n  // so it makes a Uni from \"bar\" and returns it from the method?\n  Uni respUni = Uni.createFrom().future(futureResponse);\n\n  return respUni;\n}\n`\nThanks. Any suggestions gratefully accepted! (And yes, I have spent many hours trying to work it out for myself) ;-)",
      "solution": "Updated the post, because of errors.\nUniCreate.future() takes a java.util.concurrent.Future of some type and returns Uni of the same type. That is, you'll have to pass a `java.util.concurrent.Future` to get a `Uni`.\nThe send method of the Redis client returns a `io.vertx.core.Future` which is not assignment compatible to java.util.concurrent.Future.\nFortunately, the API provides io.vertx.core.Future#toCompletionStage to convert a vertx Future to a JDK CompletionStage while Mutiny provides UniCreate.completionStage() to get the job done.\n```\n`public Uni test () {\n  Future futureResponse = this.redisClient.send(Request.cmd(Command.create(\"JSON.GET\")).arg(\"foo\"))\n      .compose(response -> {\n        return Future.succeededFuture(response.toString());\n      });\n\n  Uni respUni = Uni.createFrom().completionStage(futureResponse.toCompletionStage());\n\n  return respUni;\n}\n`\n```",
      "question_score": 4,
      "answer_score": 3,
      "created_at": "2022-09-07T09:47:46",
      "url": "https://stackoverflow.com/questions/73631825/java-how-to-uni-createfrom-future-and-return-that-uni-from-the-enclosing-met"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 69869063,
      "title": "How to clean-up / reset redis-mock in an Express Jest test?",
      "problem": "I have an app which tallies the number of visits to the url. The tallying is done in Redis. I'm using `redis-mock` which simulates commands like `INCR` in memory.\nThe following test visits the page 3 times and expects the response object to report `current` as `3`:\n```\n`let app = require('./app');\nconst supertest = require(\"supertest\");\n\njest.mock('redis', () => jest.requireActual('redis-mock'));\n\n/* Preceeded by the exact same test */\n\nit('should report incremented value on multiple requests', (done) => {\n    const COUNT = 3;\n    const testRequest = function (cb) { supertest(app).get('/test').expect(200, cb) };\n\n    async.series([\n      testRequest,\n      testRequest,\n      testRequest\n    ], (err, results) => {\n      if (err) console.error(err);\n\n      const lastResponse = _.last(results).body;\n      expect(\n        lastResponse.current\n      ).toBe(COUNT);\n\n      done();\n    });\n\n  });\n`\n```\nThe issue is that if I keep reusing `app`, the internal \"redis\" mock will continue getting incremented between tests.\nI can side-step this a bit by doing this:\n```\n`beforeEach(() => {\n  app = require('./app');\n  jest.resetAllMocks();\n  jest.resetModules();\n});\n`\n```\nOverwriting app seems to do the trick but isn't there a way to clean-up the \"internal\" mocked module somehow between tests?",
      "solution": "My guess is that somehow the '/test' endpoint gets invoked in some other tests in the suite, you could try to run specific parts of your suite using `.only` or even trying to run the entire suite serially.\nTo answer the original questions the entire suite must be isolated and consistent either if you are running a specific test case scenario or if you are trying to run the entire suite, thus you need to clear up any leftovers that they could actually affect the results.\nSo you can actually use the `.beforeEach` or the `.beforeAll` methods, provided by Jest in order to \"mock\" Redis and the `.afterAll` method for clearance.\nA dummy implementation would look like this:\n`import redis from \"redis\";\nimport redis_mock from \"redis-mock\";\nimport request from \"supertest\";\n\njest.mock(\"redis\", () => jest.requireActual(\"redis-mock\"));\n\n// Client to be used for manually resetting the mocked redis database\nconst redisClient = redis.createClient();\n\n// Sometimes order matters, since we want to setup the mock\n// and boot the app afterwards\nimport app from \"./app\";\n\nconst COUNT = 3;\nconst testRequest = () => supertest(app).get(\"/test\");\n\ndescribe(\"testing\", () => {\n  afterAll((done) => {\n    // Reset the mock after the tests are done\n    jest.clearAllMocks();\n    // You can also flush the mocked database here if neeeded and close the client\n    redisClient.flushall(done);\n    // Alternatively, you can also delete the key as\n    redisClient.del(\"test\", done);\n    redisClient.quit(done);\n  });\n\n  it(\"dummy test to run\", () => {\n    expect(true).toBe(true);\n  });\n\n  it(\"the actual test\", async () => {\n    let last;\n    // Run the requests in serial\n    for (let i = 0; i",
      "question_score": 4,
      "answer_score": 3,
      "created_at": "2021-11-07T02:04:55",
      "url": "https://stackoverflow.com/questions/69869063/how-to-clean-up-reset-redis-mock-in-an-express-jest-test"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 75239727,
      "title": "Docker with Redis via Procfile.dev in iTerm2 output is unreadable",
      "problem": "This is a bit of a strange one and I can't find answers anywhere else... if I have a `Procfile.dev` file with a basic Redis command in it such as `redis: docker run --rm -it -p 6379:6379 redis:latest` and run it via `bin/dev` the output from the Redis ascii art makes the logs unreable. If I remove the Redis command from the `Procfile.dev` it goes back to being neat and readable, below is an example of the messed up output:\n\nDoes anyone know how to make this look nice? I'm having to run docker outside the procfile atm because of this.\nThis is a Ruby on Rails 7 app running via `bin/dev`, if that is relevant.",
      "solution": "If possible try running the docker container in non interactive mode:\n```\n`redis: docker run --rm -p 6379:6379 redis:latest\n`\n```\nNote the removal of the `-it` flag.\nIt will cause redis to not output the logo ascii art.\nThe reasoning behind the solution are the comments defined for the configuration property `always-show-logo` that you can see in `redis.conf`:\n```\n`# By default Redis shows an ASCII art logo only when started to log to the\n# standard output and if the standard output is a TTY and syslog logging is\n# disabled. Basically this means that normally a logo is displayed only in\n# interactive sessions.\n`\n```\nSimilar issues has been reported for the service, like this one related to `syslog`.",
      "question_score": 4,
      "answer_score": 2,
      "created_at": "2023-01-25T21:51:56",
      "url": "https://stackoverflow.com/questions/75239727/docker-with-redis-via-procfile-dev-in-iterm2-output-is-unreadable"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 72489975,
      "title": "NestJS Bull queues - Missing lock for job failed",
      "problem": "I'm using Bull with NestJS to handle a jobs queue. In the process handler I would like to mark a job as failed instead of completed, but it seems - also reading the documentation - that the `Job#moveToFailed()` method is allowed only on waiting jobs.\nIn fact, it triggers an error saying \"Missing lock for job ${jobId} failed\".\nBut, calling the `Job#moveToFailed` with the `ignoreLock` parameter to true everything goes fine.\nWhat happens if I ignore the lock moving a job to failed? Is there some side effect? In my scenario, the queue jobs will be always consumed by the same `@Processor`.\nHere it is the piece of code I'm running for test purpose:\n```\n`@Process()\nasync transcode(job: Job): Promise {\n  const jobData = job.data as Record\n  if (jobData == null) {\n    await job.moveToFailed({ message: 'Hook marked as failed because of missing data' })\n    return\n  }\n\n  // do other stuff for job execution..\n}\n`\n```",
      "solution": "You can pass false as a token parameter (that ignores the token check).\n```\n`await job.moveToFailed({ message: 'Hook marked as failed because of missing data' }, false)\n`\n```\nI was able to add '0' instead of false and that worked for me, but the parameter type is Boolean like mentioned in the comments below.",
      "question_score": 4,
      "answer_score": 2,
      "created_at": "2022-06-03T14:59:27",
      "url": "https://stackoverflow.com/questions/72489975/nestjs-bull-queues-missing-lock-for-job-failed"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 70621547,
      "title": "Django +docker-compose + Celery + redis - How to use Redis deployed in my own remote server?",
      "problem": "I have a Django app deployed in Docker containers.\nI have 3 config environnements: dev, preprod and prod.\ndev is my local environnement (localhost) and preprod/prod are remote linux environnements.\nIt works when using the \"public\" Redis server and standard config.\nBut I need to use our own Redis deployed in Docker container in a remote server (192.168.xx.xx) with name container redis_cont.\nAnd I do not really know how to config. I do not know if it is possible?\nI would appreciate some help.\ndocker-compose\n```\n`version: '3.7'\n\nservices:\n    web:\n        restart: always\n        build: \n            context: ./app\n            dockerfile: Dockerfile.dev\n        restart: always\n        command: python manage.py runserver 0.0.0.0:8000\n        volumes:\n            - ./app:/usr/src/app\n        ports:\n            - 8000:8000\n        env_file:\n            - ./.env.dev\n        entrypoint: [ \"/usr/src/app/entrypoint.dev.sh\" ]\n        depends_on: \n            - redis\n        healthcheck:\n            test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8000/\"]\n            interval: 30s\n            timeout: 10s\n            retries: 50\n    redis:\n        container_name: redis_cont          settings.py\n```\n`CELERY_BROKER_URL = 'redis://redis:6379'\nCELERY_RESULT_BACKEND = 'redis://redis:6379'\nCELERY_ACCEPT_CONTENT = ['application/json']\nCELERY_TASK_SERIALIZER = 'json'\nCELERY_RESULT_SERIALIZER = 'json'\nCELERY_BEAT_SCHEDULE = {\n    'hello': {\n        'task': 'project.tasks.hello',\n        'schedule': crontab()  # execute every minute\n    },\n}\n\n`\n```",
      "solution": "Since the containers are not created via the same `docker-compose`, they won't share the same network. `redis_cont` just doesn't exist to the services built in the isolated network of your `docker-compose`.\nIf Redis container is published on the remote and is accessible using `ip:port`, you should be able to use it directly in your `settings.py`. No need to add a new service in your compose file.\n\nNote\nTo establish a communication between services in the same `docker-compose` you should use the service name (`web`, `celery-beat`, etc in your case) and not the container name.",
      "question_score": 4,
      "answer_score": 2,
      "created_at": "2022-01-07T13:52:04",
      "url": "https://stackoverflow.com/questions/70621547/django-docker-compose-celery-redis-how-to-use-redis-deployed-in-my-own-re"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 69100220,
      "title": "Node.js Redis client doesn&#39;t have Redis commands",
      "problem": "I have a trouble with redis package. There's in it's docs:\n\nThis library is a 1 to 1 mapping of the Redis commands.\n\nBut I can not access Redis commands through this lib's client.\nI'm creating a client like this:\n```\n`const client = createClient({\n  socket: {\n    host: process.env.REDIS_HOST,\n    port: +process.env.REDIS_PORT!,\n    password: process.env.REDIS_PASSWORD\n  }\n});\n`\n```\n`createClient` is imported this way: `import { createClient } from 'redis'; `\nAnd when I try to call any Redis command on this client, e.g.:\n```\n`client.rpush('key', 'value');\n`\n```\nI get an error:\n`this.client.rpush is not a function`\nAnd `console.log(Object.keys(client));` output is:\n```\n`[\n  '_events',\n  '_eventsCount',\n  '_maxListeners',\n  'select',\n  'subscribe',\n  'pSubscribe',\n  'unsubscribe',\n  'pUnsubscribe',\n  'quit'\n]\n`\n```\nAm I doing something wrong? And how can I fix it?",
      "solution": "As per the Redis Commands section in the README file,\n\nThere is built-in support for all of the out-of-the-box Redis commands. They are exposed using the raw Redis command names (HSET, HGETALL, etc.) and a friendlier camel-cased version (hSet, hGetAll, etc.):\n\nSo, use the rpush command as `client.RPUSH('key', 'value');` or `client.rPush('key', 'value');`",
      "question_score": 4,
      "answer_score": 2,
      "created_at": "2021-09-08T11:13:55",
      "url": "https://stackoverflow.com/questions/69100220/node-js-redis-client-doesnt-have-redis-commands"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 76349850,
      "title": "Issue with HSET command in Rust Redis",
      "problem": "I am currently trying to convert the following Redis command into rust:\n```\n`HSET book_store_list 1 1,2,3\n`\n```\nEssentially, create the bookstore_list Hash, with a Field (1) and value (1,2,3) which works fine within the Redis GUI.\nThe issue is that the hset API in rust redis does not seem to work as I envisioned.\nFor example:\n```\n`let vector = vec![1,2,3]\nhset(\"book_store_list\", 1, &vector) \n`\n```\nResults in two Field-Value entries within the book_store_list hash\nField 1 Value 1\nField 2 Value 3\nThe issue is that the redis rust crate seems to interpret the [1,2,3] vector to be separate values (with 2 and 3 being a new field value pair) to be cached in instead of one value. Just wondering if anyone has faced this issue before?",
      "solution": "Believe that I have found the answer. In order for Redis to accept [1,2,3] as one singular value, we must call the `to_string` api from the `serde_json` crate like so:\n```\n`let value = serde_json::to_string(&vector).unwrap();\n`\n```\nI guess the lesson of this is to stringify all values before invoking any HSET commands in rust redis!",
      "question_score": 4,
      "answer_score": 1,
      "created_at": "2023-05-28T05:45:55",
      "url": "https://stackoverflow.com/questions/76349850/issue-with-hset-command-in-rust-redis"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 74421717,
      "title": "Redis creating a Set with expiry",
      "problem": "When I create a new Set (using `SADD`) I need to set the expiration for that Set.\nIs there any way to create a Set key with expiry?\nNote: I need to do that in one command, because my program checks if the set exists before going to update it - so if its existing with no expiration no update will occur.\nMy program runs on AWS so if I split it to 2 commands, only the creation can be executed without the expiration (happened to me in the past).\nThanks!\nI couldn't find a way to set expiry on a Set other than using the `EXPIRE` command - risking in the program crashing before the `EXPIRE` command (i.e. AWS service restarting for any reason) and the Set existing with no expiration.\nEdit: I found a work-around, when I can check if the `TTL` of the key is",
      "solution": "What about using transactions? For example:\n```\n`multi\nsadd key member1 member2 ...\nexpire key seconds\nexec\n`\n```\nIt is guaranteed that the sadd and expire will be executed as a single unit on the node and on replicas if you use any. If you use pipelining you don't need to do it in multiple rounds from the client so perf will have minimal / no impact.",
      "question_score": 4,
      "answer_score": 1,
      "created_at": "2022-11-13T15:06:00",
      "url": "https://stackoverflow.com/questions/74421717/redis-creating-a-set-with-expiry"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 67122133,
      "title": "Socket.io &amp; Redis - Proper architecture for an io game?",
      "problem": "I'm using a basic NodeJS scaling w/ Redis architecture, but I'm having trouble configuring this to suit a real-time multiplayer game.\nMy multiplayer game should have separate lobbies - so when the load balancer places a user in a separate server, there is no way players in the same lobby can communicate, unless I use Redis. The problem is, I can't send every single action back and forth between the servers, as that would overload Redis, as well as ruin the scalability of the server instances, since now I have to store every user in every NodeJS instance (to check for collisions, ect), which defeats the purpose of scaling. Unless I'm doing something wrong?\nBasic NodeJS/Redis Architecture (Inefficient for an io game with lobbies?)\n\nI've also added a separate running Manager which configures (creates/removes lobbies) the lobbies, and sends the information to the Worker instances via Redis, so the users so they can view available lobbies\nI've thought about having each NodeJS instance to be a separate lobby, but the load balancer doesn't work that way. Also, there is no automatic scaling.\nMy current architecture, showcasing players (users)\n\n`Red` is for users\n`Light Pink` is for synchronized users via Redis from the other servers into itself (otherwise, players from other instances would not be visible to one another. Also, wouldn't be able to perform simple updates, such as collision detection)\nEach player is in it's own chosen `lobby`, and is an object which possess `X`, `Y`, `angle`, and several other parameters\nEven though users would join `Worker 2` or `Worker 'n'`, I still need to relay the user profiles to other workers, otherwise users that are not on the same `Worker`, they will not be visible to one another. Now in this case, doesn't it completely defeat the purpose of scaling?\nEither I'm doing something very wrong, or I'm sure there has to be a solution to this!\nEDIT\nThis is what I have come up with myself so far, albeit not sure if it's plausible\n\nI'm getting virtually no help, some comments would be greatly appreciated",
      "solution": "Posting comment as the answer\nAs per the comment, the game is played among teams and each team has some members but each member in the game must know about other members(not only their team members),  given each member has to know about all other members in the game, it's good to go with this design.\nThough the segregation should be done at a game level, each game should have its own set of Lobbies and each lobby contains all the players.\nWe don't have to run workers for each Lobby instead one worker can be used for one game and that'll update all the Lobby data.\nAlso, Loadbalancer should not be used to distribute players among workers instead a middleware should be used in the application like GamePlayerManager/LobbyPlayerManager to add/remove a player to/from a specific lobby.",
      "question_score": 4,
      "answer_score": 1,
      "created_at": "2021-04-16T10:54:44",
      "url": "https://stackoverflow.com/questions/67122133/socket-io-redis-proper-architecture-for-an-io-game"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 67951731,
      "title": "Go backend to redis connection refused after docker compose up",
      "problem": "I'm currently trying to introduce docker compose to my project. It includes a golang backend using the redis in-memory database.\n```\n`version: \"3.9\"\nservices:\n  frontend:\n    ...\n  backend:\n    build:\n      context: ./backend\n    ports:\n      - \"8080:8080\"\n    environment:\n      - NODE_ENV=production\n    env_file:\n      - ./backend/.env\n  redis:\n    image: \"redis\"\n    ports:\n      - \"6379:6379\"\n`\n```\n```\n`FROM golang:1.16-alpine\nRUN mkdir -p /usr/src/app\nENV PORT 8080\nWORKDIR /usr/src/app\nCOPY go.mod /usr/src/app\nCOPY . /usr/src/app\nRUN go build -o main .\nEXPOSE 8080\nCMD [ \"./main\" ]\n`\n```\nThe build runs successfully, but after starting the services, the go backend immediately exits throwing following error:\n\nError trying to ping redis: dial tcp 127.0.0.1:6379: connect: connection refused\n\nError being catched here:\n```\n`_, err = client.Ping(ctx).Result()\nif err != nil {\n    log.Fatalf(\"Error trying to ping redis: %v\", err)\n}\n`\n```\nHow come the backend docker service isn't able to connect to redis? Important note: when the redis service is running and I start my backend manually using `go run *.go`, there's no error and the backend starts successfully.",
      "solution": "When you run your Go application inside a docker container, the localhost IP 127.0.0.1 is referring to this container. You should use the hostname of your Redis container to connect from your Go container, so your connection string would be:\n```\n`redis://redis\n`\n```",
      "question_score": 3,
      "answer_score": 9,
      "created_at": "2021-06-12T20:14:00",
      "url": "https://stackoverflow.com/questions/67951731/go-backend-to-redis-connection-refused-after-docker-compose-up"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 76308716,
      "title": "How to set up a RedisService using Redis from `ioredis`?",
      "problem": "NestJs v9.0.0, ioredis v5.3.2, jest v29.5.0.\nI'm unable to properly set up my redis service to get it working in both, jest unit tests or starting the nest app. I have a service `RedisService` which imports Redis from\u00a0'ioredis'.\nGetting either issues when running the unit tests(jest) for the `RedisService`, or if I fix them then I get the below error when starting Nest:\nError #1\nWhen starting Nest or running the e2e:\n```\n`    Nest can't resolve dependencies of the RedisService (?). Please make sure that the argument Redis at index [0] is available in the RedisModule context.\n\n    Potential solutions:\n    - Is RedisModule a valid NestJS module?\n    - If Redis is a provider, is it part of the current RedisModule?\n    - If Redis is exported from a separate @Module, is that module imported within RedisModule?\n      @Module({\n        imports: [ /* the Module containing Redis */ ]\n      })\n`\n```\nAbove error is reproduced when starting the app or running the e2e tests.\nThis is my `RedisService` with which the unit tests work fine, but when starting the app or running the e2e tests I get the Error #1:\n```\n`import { Injectable, OnModuleDestroy } from '@nestjs/common';\nimport Redis from 'ioredis';\n\n@Injectable()\nexport class RedisService implements OnModuleDestroy {\n  constructor(private client: Redis) {} //  {\n    return await this.client.get(key);\n  }\n}\n`\n```\nI have tried different approaches, and this was the one the unit tests finally worked fine, but clearly not when running e2e tests or starting the app.\nHowever I can easily fix it by making my RedisService not to inject Redis from 'ioredis' into the constructor and instead instantiating it in onModuleInit lifecycle hook. BUT if I stop injecting it into the constructor, then its unit tests fail because the redisClient is an empty object instead of the mock I want it to be. Which leads to fix Error #1 but instead get Error #2 described below.\nError #2\nIn case of the tests failing, I get the following kind of errors:\n`TypeError: Cannot read properties of undefined (reading 'set')`\nand `TypeError: Cannot read properties of undefined (reading 'get')`\nThe unit tests instead fail BUT the e2e and app work successfully if I change the `redis.service.ts` to:\n```\n`import { Injectable, OnModuleDestroy, OnModuleInit } from '@nestjs/common';\nimport Redis from 'ioredis';\n\n@Injectable()\nexport class RedisService implements OnModuleInit, OnModuleDestroy {\n  private client: Redis; // no injection in the constructor\n\n  async onModuleInit() {\n    this.client = new Redis({\n      host: process.env.REDIS_HOST,\n      port: +process.env.REDIS_PORT,\n    });\n  }\n  // ...\n}\n`\n```\nThen the tests fail because the redisService is an empty object.\nContext\nThese are the specs, `redis.service.spec.ts`:\n```\n`import { Test, TestingModule } from '@nestjs/testing';\nimport Redis from 'ioredis';\nimport * as redisMock from 'redis-mock';\nimport { RedisService } from './redis.service';\n\ndescribe('RedisService', () => {\n  let service: RedisService;\n  let redisClientMock: redisMock.RedisClient;\n\n  beforeEach(async () => {\n    redisClientMock = {\n      set: jest.fn(),\n      get: jest.fn(),\n    };\n    const module: TestingModule = await Test.createTestingModule({\n      providers: [\n        RedisService,\n        {\n          provide: Redis,\n          useValue: redisMock.createClient(),\n        },\n      ],\n    }).compile();\n\n    redisClientMock = module.get(Redis);\n    service = module.get(RedisService);\n  });\n\n  it('should be defined', () => {\n    expect(service).toBeDefined();\n  });\n\n  describe('set', () => {\n    it('should set a value in Redis with expiration date', async () => {\n      const spy = jest.spyOn(redisClientMock, 'set');\n      await service.set('my-key', 'my-value', 60);\n      expect(spy).toHaveBeenCalledWith('my-key', 'my-value', 'EX', 60);\n    });\n  });\n\n  describe('get', () => {\n    it('should return null if the key does not exist', async () => {\n      const spy = jest.spyOn(redisClientMock, 'get').mockReturnValue(undefined);\n      const value = await service.get('nonexistent-key');\n      expect(value).toBeUndefined();\n    });\n    it('should return the value if the key exists', async () => {\n      jest.spyOn(redisClientMock, 'get').mockReturnValue('my-value');\n      const value = await service.get('my-key');\n      expect(value).toBe('my-value');\n    });\n  });\n});\n`\n```\nHere is my\n`redis.module.ts`:\n```\n`import { Module } from '@nestjs/common';\nimport { RedisService } from './redis.service';\n\n@Module({\n  providers: [RedisService],\n  exports: [RedisService],\n})\nexport class RedisModule {}\n`\n```\nRedisModule is in the imports array of the module where it is a dependency.\nI guess using ioredis we just have to avoid injecting it in the constructor, but then how can I fix `redis.service.spec.ts` so that it gets the redisClient on time? Should it be injected as a dependency in the constructor? In any case, how should Redis be implemented in Nest so that both, e2e and unit tests work smoothly?",
      "solution": "Fixed it after trying different things. Running the unit tests with this command `NEST_DEBUG=true npm test` helped me narrow down the issues in the end until the unit tests run successfully. Things that fixed it:\n\nCreate file `redis.provider.ts` like this:\n\n```\n`    import { Provider } from '@nestjs/common';\n    import Redis from 'ioredis';\n    \n    export type RedisClient = Redis;\n    \n    export const redisProvider: Provider = {\n      useFactory: (): RedisClient => {\n        return new Redis({\n          host: 'localhost',\n          port: 6379,\n        });\n      },\n      provide: 'REDIS_CLIENT',\n    };\n`\n```\n\nProvide it in the module, in my case, `redis.module.ts`:\n\n```\n`import { Module } from '@nestjs/common';\nimport { redisProvider } from './redis.providers';\nimport { RedisService } from './redis.service';\n\n@Module({\n  providers: [redisProvider, RedisService],\n  exports: [RedisService],\n})\nexport class RedisModule {}\n`\n```\n\nIn the service, `redis.service.ts`, inject it in the constructor like this:\n\n```\n`import { Inject, Injectable } from '@nestjs/common';\nimport { RedisClient } from './redis.providers';\n\n@Injectable()\nexport class RedisService {\n  public constructor(\n    @Inject('REDIS_CLIENT')\n    private readonly client: RedisClient,\n  ) {}\n\n  async set(key: string, value: string, expirationSeconds: number) {\n    await this.client.set(key, value, 'EX', expirationSeconds);\n  }\n\n  async get(key: string): Promise {\n    return await this.client.get(key);\n  }\n}\n`\n```\n\nFinally the test, `redis.service.spec.ts`: use the string `REDIS_CLIENT` instead of Redis imported from `ioredis`. So now it looks like this:\n\n```\n`import { Test, TestingModule } from '@nestjs/testing';\nimport Redis from 'ioredis';\nimport * as redisMock from 'redis-mock';\nimport { RedisService } from './redis.service';\n\ndescribe('RedisService', () => {\n  let service: RedisService;\n  let redisClientMock: redisMock.RedisClient;\n\n  beforeEach(async () => {\n    redisClientMock = {\n      set: jest.fn(),\n      get: jest.fn(),\n    };\n    const module: TestingModule = await Test.createTestingModule({\n      providers: [\n        RedisService,\n        {\n          provide: 'REDIS_CLIENT',\n          useValue: redisMock.createClient(),\n        },\n      ],\n    }).compile();\n\n    redisClientMock = module.get('REDIS_CLIENT');\n    service = module.get(RedisService);\n  });\n\n  it('should be defined', () => {\n    expect(service).toBeDefined();\n  });\n`\n```",
      "question_score": 3,
      "answer_score": 13,
      "created_at": "2023-05-22T19:43:04",
      "url": "https://stackoverflow.com/questions/76308716/how-to-set-up-a-redisservice-using-redis-from-ioredis"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 74573177,
      "title": "In NestJs, I added Redis as cache manager, when I specify {ttl : 0} it throws a type error, it was working before",
      "problem": "I am using Redis as the cache manager in NestJs project. I was using a code like:\n```\n`await this.productCacheManager.set('products/time', data, { ttl: 60} )\n`\n```\nWhen I delete the ttl argument or just put 60 there, it doesn't work and it immediately  removes the record from redis, so I was using { ttl: 60} which was working until now.\nI do not know what happend but now it throws an error like:\n\nArgument of type '{ ttl: number; }' is not assignable to parameter of type 'number'.\n\nThe parameter I am typing is a number...\nTrying to make it work again like before.",
      "solution": "If you use `cache-manager` v4 and `cache-manager-redis-store` v2, you need to pass an options object, as in\n`cache.set(key, value, { ttl: 60 }) // in seconds\n`\n(And that's because the redis store v2 code expects an object)\nIf you are using `cache-manager` v5, though, you may pass an integer, although I've not yet been able to make `cache-manager-redis-store@^3` work well with `cache-manager@^5`.\n`cache.set(key, value, 60000) // in milliseconds with v5!\n`\n\u26a0\ufe0f As of today (2023-02-24), NestJS recommends using cache-manager v4.\n\nSource: NestJS documentation as for the seconds/milliseconds consideration.\nThe object vs number specification, I found out myself by trying different combinations and parsing the redis store code.",
      "question_score": 3,
      "answer_score": 5,
      "created_at": "2022-11-25T13:58:09",
      "url": "https://stackoverflow.com/questions/74573177/in-nestjs-i-added-redis-as-cache-manager-when-i-specify-ttl-0-it-throws-a"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 76928167,
      "title": "How to migrate Redis RDB file created with a module to a deployment without that module? (RedisGraph no longer in Redis Stack versions 7.2+)",
      "problem": "When I try to load a Redis database with Redis Stack 7.2 from an RDB file which has been created with a prior version of RS, it does not load, printing this error:\n```\n`* Loading RDB produced by version 6.2.13\n* RDB age 848231 seconds\n* RDB memory usage when created 1.50 Mb\n# The RDB file contains AUX module data I can't load: no matching module 'graphdata'\n`\n```\nThis is likely due to Redis phasing out RedisGraph, the article about that mentions RG \"is no longer a part of Redis Stack from version 7.2\".\nI was assuming it's all fine, as I have never used any graph related functionality. However, the RDB file seemingly references the RedisGraph module despite not having any entries related to it, which I see no logical reason for.\nWhat is the intended way for users to keep their Redis Stack version up-to-date, if they need RDB persistence? Seems like version 7.2 will simply not accept any RDB file from any previous version.\nIs there a simple way to migrate it and keep using this file?",
      "solution": "This got me too. In particular, I've been using redis-stack for a little project locally and the Homebrew cask updated from redis-stack-server 6.2.6-v9 to 7.2.0 a week ago (commit).\nThe reason we can't load our RDBs, even though we never used RedisGraph, is that loading the module saved metadata to the RDB in AUX fields (custom data types?), which the new version of Redis can't read without the module. This also disallows you from unloading the module at runtime with `MODULE UNLOAD`.\nThis is wack and they're talking through a fix in pull requests #11056 and now #11374.\nAnyway, a basic strategy for rescuing your data is to load it up in the previous working configuration, export it as a plaintext AOF log in a way that doesn't include the offending references to the old module, upgrade to 7.2.0 again\u2014having deleted the offending RDB file\u2014and load in the AOF we generated. `SAVE` and you've got a fresh RDB.\nI'm working locally with about 5 MB of data and there's no production instance to keep available. This approach assumes there are no clients connecting and trying to modify data except for you. So here's a basic outline, but modify for your situation.\n\nMake a backup of your RDB file before you do anything else, so you have a safety net if these steps don't work precisely on the first try. For me, with Homebrew, I just did e.g.\n`cp /opt/homebrew/var/db/redis-stack/dump.rdb ~/Desktop/dump.orig.rdb`\n\nDowngrade back to 6.2.x. Again, this will depend on your OS and configuration. For Homebrew, you can attempt to downgrade a cask by downloading the version of the formula you want and installing it from disk. I downloaded the previous version of the cask formula for redis-stack-server.rb, then\n`brew install --cask ~/Downloads/redis-stack-server.rb`\n\nTry starting `redis-stack-server`. You should be back.\n\nWe can see those problematic references to the module by inspecting the RDB file, btw:\n```\n`$ redis-check-rdb /opt/homebrew/var/db/redis-stack/dump.rdb\n[offset 0] Checking RDB file dump.rdb\n[offset 27] AUX FIELD redis-ver = '6.2.13'\n[offset 41] AUX FIELD redis-bits = '64'\n[offset 53] AUX FIELD ctime = '1692437664'\n[offset 68] AUX FIELD used-mem = '5479248'\n[offset 84] AUX FIELD aof-preamble = '0'\n[offset 96] MODULE AUX for: graphdata\n[offset 111] MODULE AUX for: scdtype00\n[offset 124] MODULE AUX for: ft_index0\n[offset 129] Selecting DB ID 2\n[offset 2666081] MODULE AUX for: graphdata\n[offset 2666096] MODULE AUX for: scdtype00\n[offset 2666108] Checksum OK\n[offset 2666108] \\o/ RDB looks OK! \\o/\n[info] 4713 keys read\n[info] 0 expires\n[info] 0 already expired\n`\n```\n\nExport your data.\n```\n`$ redis-cli\n127.0.0.1:6379> CONFIG SET aof-use-rdb-preamble no\nOK\n127.0.0.1:6379> CONFIG SET appendonly yes\nOK\n127.0.0.1:6379> BGREWRITEAOF\nBackground append only file rewriting started\n`\n```\nCheck the server log for that to finish, then disconnect the client and stop the server.\n\nDelete the active RDB file.\n`rm /opt/homebrew/var/db/redis-stack/dump.rdb`\nThere should be a larger `appendonly.aof` alongside it now. You can move that somewhere safe.\n`mv /opt/homebrew/var/db/redis-stack/appendonly.aof ~/Desktop/appendonly.6.2.aof`\n\nUpgrade back to 7.2.0. For me, using Homebrew, that's just\n`brew upgrade redis-stack-server`.\n\nFinally, start 7.2.0 without any data (since we deleted/moved it all away)\n`redis-stack-server`\nAnd replay the log:\n`redis-cli --pipe \n\nYou can now issue a `SAVE` or `BGSAVE` to generate a new RDB file.\n\nI hope this helps! I haven't been in charge of a production Redis instance in a decade or so, and I just chose Redis Stack for the JSON support, because I'm caching entire responses from a JSON API and thought it might be handy. But this issue with module interoperability makes me wonder whether I should just stick with vanilla Redis for now.\nBy the way, Redis 7.2.0 says this on startup for me:\n```\n`71950:M 21 Aug 2023 04:07:05.933 *  Created new data type 'GearsType'\n71950:M 21 Aug 2023 04:07:05.934 *  Detected redis oss\n71950:M 21 Aug 2023 04:07:05.934 #  could not initialize RedisAI_InitError\n\n71950:M 21 Aug 2023 04:07:05.934 *  Failed loading RedisAI API.\n71950:M 21 Aug 2023 04:07:05.934 *  RedisGears v2.0.11, sha='0aa55951836750ceabd9733decb200f8a5e7bac3', build_type='release', built_for='Macos-mac os12.6.3.arm64v8'.\n71950:M 21 Aug 2023 04:07:05.935 *  Registered backend: js.\n71950:M 21 Aug 2023 04:07:05.935 * Module 'redisgears_2' loaded from /opt/homebrew/Caskroom/redis-stack-server/7.2.0-v0/lib/redisgears.so\n`\n```\nSo... I assume that's fine. :)",
      "question_score": 3,
      "answer_score": 5,
      "created_at": "2023-08-18T11:42:16",
      "url": "https://stackoverflow.com/questions/76928167/how-to-migrate-redis-rdb-file-created-with-a-module-to-a-deployment-without-that"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 75995689,
      "title": "Pipeline, watch() and multi() in redis. How do they really work?",
      "problem": "I'm trying to understand the correct use of commands `multi` and `watch` for the access to a database Redis.\nThe context\nI'm using:\n\nthe Python Client for Redis redis-py version 3.5.3.\nthe version of the Redis server is Redis server v=5.0.5.\n\nOther links\nI have made many researches on Internet and I have found some useful link about the main topic of my question:\n\nthis post is useful but not exhaustive\nthis post explains well how works the `multi` command\n\nExample code\nI have written and executed the following code where is used the instruction `watch` associated to an example Redis key called `keyWatch`:\n`r = redis.Redis()\n\ndef key_incr():\n    print('keyWatch before incr = ' + r.get('keyWatch').decode(\"utf-8\"))\n    pipe = r.pipeline()\n    pipe.watch('keyWatch')\n    pipe.multi()\n    pipe.incr('keyWatch')\n    pipe.execute()\n    print('keyWatch after incr = ' + r.get('keyWatch').decode(\"utf-8\"))\n\nkey_incr()\n`\nPrevious code can be correctly executed and if the initial value of `keyWatch` is `9`, the output of the execution is :\n```\n`keyWatch before incr = 9\nkeyWatch after incr = 10\n`\n```\nIf I remove the instruction `multi()` from the code it becomes:\n`r = redis.Redis()\n\ndef key_incr():\n    print('keyWatch before incr = ' + r.get('keyWatch').decode(\"utf-8\"))\n    pipe = r.pipeline()\n    pipe.watch('keyWatch')\n    # NOTE: here the multi() instruction is commented\n    #pipe.multi()\n    pipe.incr('keyWatch')\n    pipe.execute()\n    print('keyWatch after incr = ' + r.get('keyWatch').decode(\"utf-8\"))\n\nkey_incr()\n`\nIts execution raise the following exception:\n```\n`raise WatchError(\"Watched variable changed.\")\nredis.exceptions.WatchError: Watched variable changed.\n`\n```\nMy need is to avoid that other clients modify the key `keyWatch` while are executed instructions contained inside the transaction.\nThe question\nWhy in my example code the `WatchError` exception is raised only if the `multi()` instruction is not present?\nThanks\n\nEdit\nUse of MONITOR\nI have edited my question and I have integrated it by the use of the Redis command `monitor`.\nBy `redis-cli monitor` (MONITOR in the rest of the post) I can see all the requests to the server during the execution of the previous 2 snippets of code.\nMonitor info with `multi` instruction present\nFor the case where the `multi()` instruction is present requests are the following:\n```\n`> redis-cli monitor\nOK\n1681733993.273545 [0 127.0.0.1:46342] \"GET\" \"keyWatch\"\n1681733993.273790 [0 127.0.0.1:46342] \"WATCH\" \"keyWatch\"\n1681733993.273934 [0 127.0.0.1:46342] \"MULTI\"\n1681733993.273945 [0 127.0.0.1:46342] \"INCRBY\" \"keyWatch\" \"1\"\n1681733993.273950 [0 127.0.0.1:46342] \"EXEC\"\n1681733993.274279 [0 127.0.0.1:46342] \"GET\" \"keyWatch\"  Monitor info without `multi` instruction (WatchError)\nFor the case without the `multi()` instruction, MONITOR shows the following requests:\n```\n`> redis-cli monitor\nOK\n1681737498.462228 [0 127.0.0.1:46368] \"GET\" \"keyWatch\"\n1681737498.462500 [0 127.0.0.1:46368] \"WATCH\" \"keyWatch\"\n1681737498.462663 [0 127.0.0.1:46368] \"INCRBY\" \"keyWatch\" \"1\"\n1681737498.463072 [0 127.0.0.1:46368] \"MULTI\"\n1681737498.463081 [0 127.0.0.1:46368] \"EXEC\"\n`\n```\nAlso in this second case is present the `MULTI` instruction, but between it and the `EXEC` there aren't any requests.\nThe `keyWatch` exception is raised by the `EXEC` instruction  in fact the MONITOR does not show the last `\"GET\" \"keyWatch\"` request (look at the first MONITOR log and you find the last `\"GET\" \"keyWatch\"` request).\n\nAll this suggest me that the exception is caused by the execution of:\n`\"INCRBY\" \"keyWatch\" \"1\"` outside of the block `MULTI/EXEC`.\n\nIf someone can confirm this and explain better the behavior is appreciated.\nThanks",
      "solution": "The WATCH, MULTI, and EXEC are designed to work together. Specifically, calls to MULTI and EXEC allow the queued commands to be executed in isolation. Redis calls this a transaction.\nHere's how it works:\n```\n`MULTI                          While these commands are being queued, other commands can come in. These  other commands might change keys that are part of the transaction, which could be bad. Enter WATCH.\nWATCH is used to, well, watch those keys. If they have been changed by the time EXEC is called, EXEC will return an error. You then need to run code to retry the transaction (or maybe generate an error, depending on you needs).\nIf they haven't been changed, then EXEC executes all the queued commands and life goes on.\nIt works like this:\n```\n`WATCH someKey someOtherKey     Note that you don't have to watch a key as part of a transaction, as my example above shows. In this case, I don't care if someone changes the thing I'm going to delete.\nTransaction in Redis aren't what developers often think they are. If you want to get deeper in the details, there is a guide to transactions on the Redis website.",
      "question_score": 3,
      "answer_score": 6,
      "created_at": "2023-04-12T14:53:26",
      "url": "https://stackoverflow.com/questions/75995689/pipeline-watch-and-multi-in-redis-how-do-they-really-work"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 71287116,
      "title": "NodeJs with Redis as session store throws exception",
      "problem": "I was experimenting with NodeJs with Session. I followed code from many sites. Simplest one I would like to mentioned is\nSample NodeJS Code for session management with Redis\nRest of the forums found similar to it. Issue I am facing is when I am not using the redis as session store everything work fine. Below changes in code for swithcing from redis session to In memory session.\n```\n`app.use(session({\n    secret: 'secret$%^134',\n    resave: false,\n    saveUninitialized: false,\n    cookie: {\n        secure: false, // if true only transmit cookie over https\n        httpOnly: false, // if true prevent client side JS from reading the cookie \n        maxAge: 1000 * 60 * 10 // session max age in miliseconds\n    }\n}))\n`\n```\nBut as I put the Redis store back in sample, I start getting exception Client is closed. Code for enabling the Redis session store is\n```\n`const RedisStore = connectRedis(session)\n//Configure redis client\nconst redisClient = redis.createClient({\n    host: 'localhost',\n    port: 6379\n})\nredisClient.on('error', function (err) {\n    console.log('Could not establish a connection with redis. ' + err);\n});\nredisClient.on('connect', function (err) {\n    console.log('Connected to redis successfully');\n});\n//Configure session middleware\napp.use(session({\n    store: new RedisStore({ client: redisClient }),\n    secret: 'secret$%^134',\n    resave: false,\n    saveUninitialized: false,\n    cookie: {\n        secure: false, // if true only transmit cookie over https\n        httpOnly: false, // if true prevent client side JS from reading the cookie \n        maxAge: 1000 * 60 * 10 // session max age in miliseconds\n    }\n}));\n`\n```",
      "solution": "After lots of reading, I got the answer at npmjs site.\nnpmjs site for redis-connect package\nAfter version 4, small code change is requried. \"legacy-mode\" needs to be enabled and explicite 'connect' call need to made in code.\nThe changed and working code is as below. Added the comments for the changes.\n```\n`const RedisStore = connectRedis(session)\n//Configure redis client\nconst redisClient = redis.createClient({\n    host: 'localhost',\n    port: 6379,\n    legacyMode:true //********New Addition - set legacy mode to true.*******\n})\nredisClient.on('error', function (err) {\n    console.log('Could not establish a connection with redis. ' + err);\n});\nredisClient.on('connect', function (err) {\n    console.log('Connected to redis successfully');\n});\nredisClient.connect(); //******** New Addition - Explicite connect call *********\n//Configure session middleware\napp.use(session({\n    store: new RedisStore({ client: redisClient }),\n    secret: 'secret$%^134',\n    resave: false,\n    saveUninitialized: false,\n    cookie: {\n        secure: false, // if true only transmit cookie over https\n        httpOnly: false, // if true prevent client side JS from reading the cookie \n        maxAge: 1000 * 60 * 10 // session max age in miliseconds\n    }\n}));\n`\n```",
      "question_score": 3,
      "answer_score": 9,
      "created_at": "2022-02-27T18:47:08",
      "url": "https://stackoverflow.com/questions/71287116/nodejs-with-redis-as-session-store-throws-exception"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 73022852,
      "title": "&quot;Invalid protocol&quot; when trying to connect to Elasticache Redis in Node.js",
      "problem": "I'm trying to connect an ElastiCache Redis to an Express server deployed on ECS. I'm using the Official Redis package for Node.js\nI get the Primary Endpoint from ElastiCache as `blablabla.mccjet.ng.0001.euc1.cache.amazonaws.com:6379`\nIn my server I try to connect like this\n`const { createClient } = require(\"redis\");\nconst pubClient = createClient({ url: 'blablabla.mccjet.ng.0001.euc1.cache.amazonaws.com:6379' });\n`\nBut when I check the ECS logs I see\n`/usr/src/app/node_modules/@redis/client/dist/lib/client/index.js:124\nthrow new TypeError('Invalid protocol');\n^\nTypeError: Invalid protocol\nat Function.parseURL (/usr/src/app/node_modules/@redis/client/dist/lib/c...\n`\nHaven't used Redis so no idea why this is happening. Any idea how to use the endpoint properly\neven tried with\n`const pubClient = createClient({ host: 'blablabla.mccjet.ng.0001.euc1.cache.amazonaws.com', port:6379 });\n`\nbut that also didn't work",
      "solution": "The issue was I had to add the prefix `redis://` before my Primary endpoint so that it becomes `redis://blablabla.mccjet.ng.0001.euc1.cache.amazonaws.com:6379`\nGithub Issue\n`const pubClient = createClient({ url: 'redis://blablabla.mccjet.ng.0001.euc1.cache.amazonaws.com:6379' });\n`",
      "question_score": 3,
      "answer_score": 8,
      "created_at": "2022-07-18T15:10:37",
      "url": "https://stackoverflow.com/questions/73022852/invalid-protocol-when-trying-to-connect-to-elasticache-redis-in-node-js"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 72116930,
      "title": "rediSearch Cannot create index on db != 0?",
      "problem": "I have installed the rediSearch module for an application, but I am getting the error Cannot create index on db != 0 do you know what it means?",
      "solution": "This is because RediSearch only operates on the default (0) database in Redis.  Using numbered databases and the `SELECT` command is somewhat of an anti-pattern in Redis and I'd always recommend instead namespacing your keys using something like \":\" as a separator or using different instances of redis-server if you have different types of data storage needs that you're currently spreading out across different numbered databases in the same redis-server instance.",
      "question_score": 3,
      "answer_score": 7,
      "created_at": "2022-05-04T19:10:18",
      "url": "https://stackoverflow.com/questions/72116930/redisearch-cannot-create-index-on-db-0"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 71690304,
      "title": "Error while using Redis Expire command options",
      "problem": "I am trying to set expiry on a key if expiry is not already set by using NX option of Expire command.\nBut I keep getting error from redis-cli, and when I try from code NX option gets ignored.\nWhen I try to use Expire command from redis-cli I get following error\n`127.0.0.1:6379> expire ns1 500 NX `\n`(error) ERR wrong number of arguments for 'expire' command`\nRedis version - v=6.2.6\nAlso If I try to do it programatically, expire command is ignored. Code below -\n```\n`let res = await client.INCRBY('ns1', 5)\nconsole.log('incr val ' + res)\nres = await client.EXPIRE('ns1', 60, { 'NX': true }) // this should set expiry\nres = await client.EXPIRE('ns1', 180, { 'NX': true }) //this should ignore setting expiry\nres = await client.TTL('ns1')console.log('ttl expiry ' + res)`\n`\n```\nThe response I get for above is\n`incr val 5`\n` ttl expiry 180`\nAny help to resolve this will be great\nthanks",
      "solution": "https://redis.io/docs/latest/commands/expire/\n\"Starting with Redis version 7.0.0: Added options: `NX`, `XX`, `GT` and `LT`\"",
      "question_score": 3,
      "answer_score": 8,
      "created_at": "2022-03-31T11:42:15",
      "url": "https://stackoverflow.com/questions/71690304/error-while-using-redis-expire-command-options"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 70203777,
      "title": "Is it possible to store nested dict in Redis as hash structure?",
      "problem": "I am trying to store a nested dictionary as hash with HSET, but that does not seem to be possible.\nAn example that does not work:\n```\n`from redis import StrictRedis\nfoo = {\n    \"host_data\": {\n        \"hostname\": \"some_host\",\n        \"mac_address\": \"82:fa:8e:63:40:05\",\n        \"root_password\": {\n            \"is_crypted\": True,\n            \"password\": \"sha512_password\"\n        },\n        \"network\": {\n            \"ip_address\": \"192.168.0.10/24\",\n            \"default_gateway\": \"192.168.0.1\",\n            \"vmnic\": \"vmnic3\",\n            \"vlan\": 20\n        },\n        \"dns_servers\": [\n            \"dns01.local\",\n            \"dns02.local\"\n        ]\n    },\n    \"installation_type\": \"esxi\",\n    \"image_data\": {\n        \"type\": \"vCenter_contentlib\",\n        \"host\": \"vcenter01\",\n        \"credential\": {\n            \"username\": \"some_user\",\n            \"password\": \"some_password\"\n        },\n        \"content_library\": \"the_content_lib_name\",\n        \"image_name\": \"some_image\"\n    },\n    \"host_short_name\": \"esxi021\"\n}\n\nredis_connection = StrictRedis(host='localhost', port=6379, charset=\"utf-8\", decode_responses=True)\nredis_connection.hset(\"test\", mapping=foo)\n`\n```\nThrows the following error:\n```\n`Traceback (most recent call last):\n  File \"/Users/project/dummy.py\", line 36, in \n    redis_connection.hset(\"test\", mapping=thing)\n  File \"/Users/project/venv/lib/python3.9/site-packages/redis/client.py\", line 3050, in hset\n    return self.execute_command('HSET', name, *items)\n  File \"/Users/project/venv/lib/python3.9/site-packages/redis/client.py\", line 900, in execute_command\n    conn.send_command(*args)\n  File \"/Users/project/venv/lib/python3.9/site-packages/redis/connection.py\", line 725, in send_command\n    self.send_packed_command(self.pack_command(*args),\n  File \"/Users/project/venv/lib/python3.9/site-packages/redis/connection.py\", line 775, in pack_command\n    for arg in imap(self.encoder.encode, args):\n  File \"/Users/project/venv/lib/python3.9/site-packages/redis/connection.py\", line 119, in encode\n    raise DataError(\"Invalid input of type: '%s'. Convert to a \"\nredis.exceptions.DataError: Invalid input of type: 'dict'. Convert to a bytes, string, int or float first.\n`\n```\nAn example that works:\n```\n`from redis import StrictRedis\nfoo = {\n    \"installation_type\": \"esxi\",\n    \"host_short_name\": \"esxi021\"\n}\n\nredis_connection = StrictRedis(host='localhost', port=6379, charset=\"utf-8\", decode_responses=True)\nredis_connection.hset(\"test\", mapping=foo)\n`\n```\nIn summary, my question is:\nIs it possible to store nested dict as hash structure in Redis?\nNOTE: I am aware it can be stored as string and then loaded as json, but I am really trying to avoid it.\nEDIT: Reids version is 6.2.5",
      "solution": "Redis alone doesn't store nested structures in hashes. There is a good answer for how to work around here.\nHowever, there is a module RedisJSON that supports this and the python redis client includes support for this module.\nLoad the module in the redis server:\n```\n`redis-server --loadmodule ../RedisJSON/target/release/librejson.dylib\n`\n```\nIn python you can then set and retrieve nested dicts or portions of the path.\n```\n`import redis\nfrom redis.commands.json.path import Path\n\nredis_connection = StrictRedis(host='localhost', port=6379, charset=\"utf-8\", decode_responses=True)\n\nfoo = {\n    \"host_data\": {\n        \"hostname\": \"some_host\",\n        \"mac_address\": \"82:fa:8e:63:40:05\",\n        \"root_password\": {\n            \"is_crypted\": True,\n            \"password\": \"sha512_password\"\n        },\n        \"network\": {\n            \"ip_address\": \"192.168.0.10/24\",\n            \"default_gateway\": \"192.168.0.1\",\n            \"vmnic\": \"vmnic3\",\n            \"vlan\": 20\n        },\n        \"dns_servers\": [\n            \"dns01.local\",\n            \"dns02.local\"\n        ]\n    },\n    \"installation_type\": \"esxi\",\n    \"image_data\": {\n        \"type\": \"vCenter_contentlib\",\n        \"host\": \"vcenter01\",\n        \"credential\": {\n            \"username\": \"some_user\",\n            \"password\": \"some_password\"\n        },\n        \"content_library\": \"the_content_lib_name\",\n        \"image_name\": \"some_image\"\n    },\n    \"host_short_name\": \"esxi021\"\n}\n\nredis_connection.json().set(\"test\", Path.rootPath(), foo)\nprint(redis_connection.json().get(\"test\", '.host_data.hostname'))\n`\n```",
      "question_score": 3,
      "answer_score": 7,
      "created_at": "2021-12-02T18:12:54",
      "url": "https://stackoverflow.com/questions/70203777/is-it-possible-to-store-nested-dict-in-redis-as-hash-structure"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 73381897,
      "title": "How can \u0131 use GEOADD in Python ? (Redis)",
      "problem": "I want the store geospatial information in Redis.\nI am executing the following code\n```\n`from redis import Redis\n\nredis_con = Redis(host=\"localhost\", port=6379)\nredis_con.geoadd(\"Sicily\", 13.361389, 38.115556, \"Palermo\")\n`\n```\nBut \u0131 got error like that\n```\n`raise DataError(\"GEOADD allows either 'nx' or 'xx', not both\")\n\nredis.exceptions.DataError: GEOADD allows either 'nx' or 'xx', not both\n`\n```",
      "solution": "This will work for you:\n```\n`from redis import Redis\n\nredis_con = Redis(host=\"localhost\", port=6379)\ncoords = (13.361389, 38.115556, \"Palermo\")\nredis_con.geoadd(\"Sicily\", coords)\n`\n```\nThe signature for `geoadd` is:\n```\n`geoadd(name, values, nx=False, xx=False, ch=False)\n    name: Union[bytes, str, memoryview]\n    values: Sequence[Union[bytes, memoryview, str, int, float]]\n    nx (bool, default: False)\n    xx (bool, default: False)\n    ch (bool, default: False) \n`\n```\nYou need to specify your coordinates as a sequence like a `list` or `tuple`, because right now you're specifying arguments so that the method thinks you've specified `nx`, `xx`, and `ch`.",
      "question_score": 3,
      "answer_score": 7,
      "created_at": "2022-08-17T03:14:51",
      "url": "https://stackoverflow.com/questions/73381897/how-can-%c4%b1-use-geoadd-in-python-redis"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 71157673,
      "title": "Using mongo and redis cache with both repositories in Spring Boot",
      "problem": "I want to use both Redis and Mongo with repository manner (I do not want to use spring cache annotations but repository methods).\nI annotate the main class with the following annotations.\n```\n`@EnableMongoRepositories(basePackageClass = PersistencyRepository.class)\n@EnableRedisRepositories(basePackageClass = CacheRepository.class)\n@SpringBootApplication\n`\n```\nRepos\n```\n`public interface PersistencyRepository extends CrudRepository {}\n\npublic interface CacheRepository extends MongoRepository {}\n`\n```\nNow, I am getting the following error.\n```\n`The bean \"cacheRepository\" defined in com.repository.CacheRepository defined in @EnableMongoRepositories declared on StoreApplication, could not be registered. A bean with that name has already been defined in com.repository.CacheRepository defined in @EnableRedisRepositories declared on StoreApplication and overriding is disabled. \n`\n```\nHow can I use repos of differenet databases (mongo, redis)?",
      "solution": "You extended the wrong repository interface (MongoRepository) on CacheRepository try extending CrudRepository instead.\nAlso, your mongo and redis entities should be separated to different packages, usually I just went with com.my.company.entity.mongo and com.my.company.entity.redis for each.\nAfter that, you need to update those Configuration annotations. A better package design, instead of putting all annotations on Main is putting them on a separate package, then putting those annotations there. This has an added benefit of clearly splitting each configurations for what they actually do\nfor example:\n`package com.your.company.configuration;\n\nimport com.your.company.configuration.properties.ApplicationProperties;\nimport com.your.company.entity.mongo.BaseDocument;\nimport com.your.company.entity.postgres.BaseEntity;\nimport com.your.company.entity.redis.BaseHash;\nimport com.your.company.repository.mongo.BaseMongoRepository;\nimport com.your.company.repository.postgres.BaseJpaRepository;\nimport com.your.company.repository.redis.BaseRedisRepository;\nimport org.springframework.boot.context.properties.EnableConfigurationProperties;\nimport org.springframework.context.annotation.Bean;\nimport org.springframework.context.annotation.Configuration;\nimport org.springframework.data.jpa.repository.config.EnableJpaRepositories;\nimport org.springframework.data.mongodb.repository.config.EnableMongoRepositories;\nimport org.springframework.data.redis.repository.configuration.EnableRedisRepositories;\nimport org.springframework.security.crypto.bcrypt.BCryptPasswordEncoder;\nimport org.springframework.security.crypto.password.PasswordEncoder;\n\n@Configuration\n@EnableConfigurationProperties(ApplicationProperties.class)\n@EnableJpaRepositories(basePackageClasses = {BaseEntity.class, BaseJpaRepository.class})\n@EnableMongoRepositories(basePackageClasses = {BaseDocument.class,\n        BaseMongoRepository.class}, repositoryFactoryBeanClass = EnhancedMongoRepositoryFactoryBean.class)\n@EnableRedisRepositories(basePackageClasses = {BaseHash.class, BaseRedisRepository.class})\npublic class BasicConfiguration {\n\n    @Bean\n    public PasswordEncoder passwordEncoder() {\n        return new BCryptPasswordEncoder();\n    }\n}\n`\nThe above is only an example, usually I would split them further into one class each within the same package with names that describes what they are actually configuring, for example: MongoConfiguration.java, JpaConfiguration.java, etc. Note if you decide to go with that design, you need the @Configuration annotation in each of the separate classes",
      "question_score": 3,
      "answer_score": 6,
      "created_at": "2022-02-17T12:54:11",
      "url": "https://stackoverflow.com/questions/71157673/using-mongo-and-redis-cache-with-both-repositories-in-spring-boot"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 70652291,
      "title": "Redis Server Command Line",
      "problem": "I installed Redis server on ubuntu 20.04 with this article step by step. After setting password and other configuration, I run `sudo systemctl restart redis.service` but changes not be applied. Also, while Redis server was running, I got status with this command `sudo systemctl status redis` and It said me below message:\n\nWhat's the problem?!",
      "solution": "I searched a lot and found that I should use below commands (using `systemctl` or `service`):\n\nwith `systemctl`:\n\n`sudo systemctl start redis-server.service` to start Redis server\n`sudo systemctl status redis-server.service` to get status of Redis server\n`sudo systemctl stop redis-server.service` to stop Redis server\n`sudo systemctl restart redis-server.service` to apply the changes of config file (`sudo nano /etc/redis/redis.conf`)\n\nwith `service`:\n\n`sudo service redis-server start` to start Redis server\n\n`sudo service redis-server status` to get status\n\nactive status:\n\nstop status:\n\n`sudo service redis-server stop` to stop Redis server\n\n`sudo service redis-server restart` to apply the changes of config file (`sudo nano /etc/redis/redis.conf`)",
      "question_score": 3,
      "answer_score": 7,
      "created_at": "2022-01-10T13:19:44",
      "url": "https://stackoverflow.com/questions/70652291/redis-server-command-line"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 67696655,
      "title": "What do I need to do to hook up ActionCable on nginx and puma?",
      "problem": "I'm having trouble getting ActionCable hooked up in my prod environment, and related questions haven't had a working solution. I'm using an nginx+puma setup with Rails 6.1.3.2 on Ubuntu 20.04. I have confirmed that `redis-server` is running on port `6379`, and that Rails is running as production.\nHere's what I'm getting in my logs:\n```\n`I, [2021-05-25T22:47:25.335711 #72559]  INFO -- : [5d1a85f7-0102-4d25-bd4e-d81355b846ee] Started GET \"/cable\" for 74.111.15.223 at 2021-05-25 22:47:25 +0000\nI, [2021-05-25T22:47:25.336283 #72559]  INFO -- : [5d1a85f7-0102-4d25-bd4e-d81355b846ee] Started GET \"/cable/\"[non-WebSocket] for 74.111.15.223 at 2021-05-25 22:47:25 +0000\nE, [2021-05-25T22:47:25.336344 #72559] ERROR -- : [5d1a85f7-0102-4d25-bd4e-d81355b846ee] Failed to upgrade to WebSocket (REQUEST_METHOD: GET, HTTP_CONNECTION: close, HTTP_UPGRADE: )\nI, [2021-05-25T22:47:25.336377 #72559]  INFO -- : [5d1a85f7-0102-4d25-bd4e-d81355b846ee] Finished \"/cable/\"[non-WebSocket] for 74.111.15.223 at 2021-05-25 22:47:25 +0000\n`\n```\nThis happens every few seconds. You can see matching output in the browser console:\n\nFor one, I'm pretty sure that I need to add some sections to my nginx site config, such as a `/cable` section, but I haven't figured out the correct settings. Here's my current config:\n```\n`server {\n    root /home/rails/myapp/current/public;\n    server_name myapp.com;\n    index index.htm index.html;\n\n        location ~ /.well-known {\n                allow all;\n        }\n\n        location / {\n        proxy_pass http://localhost:3000;\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header X-Forwarded-Proto $scheme;\n    }\n\n    # needed to allow serving of assets and other public files\n    location ~ ^/(assets|packs|graphs)/ {\n        gzip_static on;\n        expires 1y;\n        add_header Cache-Control public;\n        add_header Last-Modified \"\";\n        add_header ETag \"\";\n    }\n\n    listen 443 ssl; # managed by Certbot\n    ssl_certificate /etc/letsencrypt/live/myapp.com/fullchain.pem; # managed by Certbot\n    ssl_certificate_key /etc/letsencrypt/live/myapp.com/privkey.pem; # managed by Certbot\n    include /etc/letsencrypt/options-ssl-nginx.conf; # managed by Certbot\n    ssl_dhparam /etc/letsencrypt/ssl-dhparams.pem; # managed by Certbot\n}\n\nserver {\n    if ($host = myapp.com) {\n        return 301 https://$host$request_uri;\n    } # managed by Certbot\n\n    listen   80;\n    server_name myapp.com;\n    return 404; # managed by Certbot\n}\n`\n```\nHere's my `config/cable.yml`:\n`development:\n  adapter: async\n\ntest:\n  adapter: test\n\nproduction:\n  adapter: redis\n  url: \n  channel_prefix: myapp_production\n`\nIn `config/environments/production.rb`, I've left these lines commented out:\n`  # Mount Action Cable outside main process or domain.\n  # config.action_cable.mount_path = nil\n  # config.action_cable.url = 'wss://example.com/cable'\n  # config.action_cable.allowed_request_origins = [ 'http://example.com', /http:\\/\\/example.*/ ]\n`\nI have not mounted ActionCable manually in my routes or anything. This is as standard of a setup as you can get. I'm thinking the answer lies centrally in a correct nginx configuration, but I don't know what it should be. Perhaps there are Rails config settings that are needed too, though. I don't remember having to change any when deploying with passenger, but maybe puma is a different story.\nUpdate\nI also noticed that a lot of the proposed solutions in other questions, like this one, seem to reference a `.sock` file in `tmp/sockets/`. My `sockets/` directory is empty, though. Web server's running fine besides ActionCable though.\nUpdate #2\nI also noticed that changing the `config.action_cable.url` to something like `ws://myapp.com` instead of `wss://myapp.com` has no effect even after restarting Rails. The browser console errors still say its trying to connect to `wss://myapp.com`. Possibly due to how I'm set up to force redirect HTTP to HTTPS. I wonder if that has anything to do with it?",
      "solution": "I got it working. Here are the settings I needed:\nnginx config\nThe `server` section from the config in my question must be modified to include the following two sections for the `/` and `/cable` locations:\n```\n`        location / {\n                proxy_pass http://localhost:3000;\n                proxy_set_header Host $host;\n                proxy_set_header X-Real-IP $remote_addr;\n                proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n                proxy_set_header X-Forwarded-Proto $scheme;\n        }\n\n        location /cable {\n                proxy_pass http://localhost:3000;\n                proxy_http_version 1.1;\n                proxy_set_header Upgrade \"websocket\";\n                proxy_set_header Connection \"Upgrade\";\n                proxy_set_header X-Real-IP $remote_addr;\n                proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        }\n`\n```\n`config/environments/production.rb`\n`  # Change myapp.com to your app's location\n  config.action_cable.allowed_request_origins = [ 'https://myapp.com' ]\n  config.hosts \nHuge thanks to Lam Phan in the comments for helping me out.",
      "question_score": 3,
      "answer_score": 7,
      "created_at": "2021-05-26T01:14:38",
      "url": "https://stackoverflow.com/questions/67696655/what-do-i-need-to-do-to-hook-up-actioncable-on-nginx-and-puma"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 78339735,
      "title": "How to get all cached keys with certain prefix from the Cache in Laravel?",
      "problem": "Is there a way in newer laravel versions to cache many things with a certain prefix and then get all by doing Cache::get('prefix_*) or something ?\nI'm trying to cache the Online Users for like 5 minutes and I want the cache to expire for each user individually, unless the expiration is updated for it by using a middleware.\nBut the problem I have run into is that I can't retrieve all users if I store them as individual Keys:\n```\n` Cache::put('online_users_'.auth()->id(), auth()->user(), now()->addMinutes(5));\n`\n```\nAnd I would like to get them by using something like :\n```\n`Cache::get('online_users_'); or Cache::get('online_users_*');\n`\n```\nI could try using tags for this problem , but as they are not even explained in the docs after laravel 10 , I was wondering if there is another way around this , that would allow for the individual keys to expire in their own and be individually stored.\nThanks.",
      "solution": "I manage to get this working with the `File` driver!\nBy storing everyting inside of one key `active_users` and not using the ttl field of the `put()` method itself. But by keeping track of an `expires_at` field of my own inside the array.\nInside the middleware class I do something like this:\n```\n`    $activeForMinutes = 5;\n    $sessionId = session()->getId();\n    $cachedActiveUsers = Cache::get('active_users') ?? [];\n\n    // Remove expired sessions.\n    foreach ($cachedActiveUsers as $key => $cachedActiveUser) {\n        if (!$cachedActiveUser['expires_at']->isFuture()) {\n            unset($cachedActiveUsers[$key]);\n        }\n    }\n\n    $userData = [\n        'session'    => $sessionId,\n        'name'       => $this->getUserName(),\n         // etc..\n    ];\n\n    // Add new data.\n    $cachedActiveUsers[$sessionId] = $userData;\n    Cache::put('active_users', $cachedActiveUsers);\n`\n```\nAnd in the controller I can simply do this:\n```\n`$activeUsers = Cache::get('active_users');\n`\n```\nHope this helps!",
      "question_score": 3,
      "answer_score": 2,
      "created_at": "2024-04-17T11:07:24",
      "url": "https://stackoverflow.com/questions/78339735/how-to-get-all-cached-keys-with-certain-prefix-from-the-cache-in-laravel"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 76323867,
      "title": "Nodejs + Typescript + RedisJSON",
      "problem": "I just started programming a little bit with nodejs, typescript and redis.\nUnfortunately I run into a problem with redis:\nI define an interface of data to be stored in redis. However, as soon as I write the type behind the variable redis complains to me.\nSame when i get Keys from redis: How to tell Typescript which type the data is?\nExample:\n```\n`\nasync function RedisQuestion() {\n  const redis = createClient();\n\n  interface userLogins {\n    token: string;\n    active: boolean;\n  }\n\n  interface myUser {\n    name: string;\n    id: string;\n    logins: userLogins[];\n  }\n\n  const objToStore: myUser = {\n    name: \"Karl Mustermann\",\n    id: \"1\",\n    logins: [\n      {\n        token: \"1234\",\n        active: true\n      },\n      {\n        token: \"2345\",\n        active: false\n      }\n    ]\n  };\n\n  await redis.json.set(\"key\", \"$\", objToStore);\n\n  const redisResult = redis.json.get(\"key\") as myUser;\n  console.log(redisUser);\n}\n`\n```\nThe way it works is if i say\n```\n`await redis.json.set(\"key\", \"$\", objToStore as any);\n`\n```\nAnd if i get values from redis:\n```\n`await redis.json.get(\"key\") as any as myUser;\n`\n```\nBut there must be a better way?! Hope you can help :)\nThanks a lot",
      "solution": "TL;DR\nRedisJSON type declaration expects the json object you provide to have an index signature declaration. Your `myUser` interface doesn't have it, therefore they are incompatible.\nYou need to either use a type (more on that below) instead of an interface, or explicitly include the index signature in your interface declaration.\nThe following code works:\n`type UserLogins = {\n  token: string;\n  active: boolean;\n}\n\ntype MyUser = {\n  name: string;\n  id: string;\n  logins: UserLogins[];\n}\n\nasync function redisQuestion() {\n  const redis = createClient();\n\n  const objToStore: MyUser = {\n    name: 'Karl Mustermann',\n    id: '1',\n    logins: [],\n  };\n\n  await redis.json.set('key', '$', objToStore);\n\n  // You can use type assertion here to tell Typescript which type will the GET return.\n  // You should include the 'null' to force you to check for a null result and prevent\n  // runtime errors.\n  const redisUser = await redis.json.get('key') as MyUser | null;\n  console.log(redisUser);\n}\n`\n\nExplanation\nThe code you provide throws the following Typescript error:\n\nType 'myUser' is not assignable to type 'RedisJSONObject'.\nIndex signature for type 'string' is missing in type 'myUser'\n\nThe method `redis.json.set()` expects the third parameter to be type-compatible with `RedisJSON`, which is defined like so:\n`type RedisJSON = null | boolean | number | string | Date | RedisJSONArray | RedisJSONObject;\ninterface RedisJSONObject {\n    [key: string]: RedisJSON;\n    [key: number]: RedisJSON;\n}\n`\nThis `[key: string]: RedisJSON;` is an index signature declaration.\nWhen defining `MyUser` as an interface, you make it incompatible with `RedisJSONObject` because interfaces don't include an index declaration unless you explicitly define it.\nIt is known that, in Typescript, types and interfaces are \"interchangeable\". However, there are subtle differences in how they behave.\nApart from the key differences, types include an index signature without explicitly defining it, whereas interfaces don't.\nThat's why, in this case, redefining `MyUser` as a type solves the error.\nI hope that helps!",
      "question_score": 3,
      "answer_score": 6,
      "created_at": "2023-05-24T15:04:16",
      "url": "https://stackoverflow.com/questions/76323867/nodejs-typescript-redisjson"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 70251854,
      "title": "How to set dict as json object using redis-py?",
      "problem": "As of `redis-py` `4.0`, json is supported.  I am using `redis-py` version `4.0.2` on `Windows 11`.  I have a redis server running in docker utilizing `redislabs/rejson:latest`.\nI am attempting to load a json object via the following -\n```\n`import redis\nfrom redis.commands.json.path import Path\nfrom redis import exceptions\nfrom redis.commands.json.decoders import unstring, decode_list\n\nr = redis.Redis()\nd = {\"hello\": \"world\", b\"some\": \"value\"}\nr.json().set(\"somekey\", Path.rootPath(), d)\n`\n```\nJust how it is done in the tests - https://github.com/redis/redis-py/blob/e8bcdcb97b8c915e2bb52353aeda17c00e1be215/tests/test_json.py#L19\nHowever I am getting this error -\n```\n`TypeError: keys must be str, int, float, bool or None, not bytes\n`\n```\nWhat am I missing?",
      "solution": "You have a b prefix in your json, which stands for bytes.\nJust remove `'b'` from `b\"some\":` and it would work as expected:\n```\n`d = {\"hello\": \"world\", \"some\": \"value\"}\n`\n```",
      "question_score": 3,
      "answer_score": 3,
      "created_at": "2021-12-06T21:56:36",
      "url": "https://stackoverflow.com/questions/70251854/how-to-set-dict-as-json-object-using-redis-py"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 67884413,
      "title": "Redis connection to my-redis:6379 failed - getaddrinfo ENOTFOUND when running seeds",
      "problem": "I am using Docker for the container service.\nI have created a seed file and run it by `npx sequelize-cli db:seed:all`, then error occur:\n```\n`Sequelize CLI [Node: 13.12.0, CLI: 6.2.0, ORM: 6.5.1]\n\nLoaded configuration file \"migrations/config.js\".\nUsing environment \"development\".\nevents.js:292\n      throw er; // Unhandled 'error' event\n      ^\n\nError: Redis connection to my-redis:6379 failed - getaddrinfo ENOTFOUND my-redis\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (dns.js:66:26)\nEmitted 'error' event on RedisClient instance at:\n    at RedisClient.on_error (/Users/CCCC/Desktop/Source Tree/my-server/node_modules/redis/index.js:342:14)\n    at Socket. (/Users/CCCC/Desktop/Source Tree/my-server/node_modules/redis/index.js:223:14)\n    at Socket.emit (events.js:315:20)\n    at Socket.EventEmitter.emit (domain.js:485:12)\n    at emitErrorNT (internal/streams/destroy.js:84:8)\n    at processTicksAndRejections (internal/process/task_queues.js:84:21) {\n  errno: -3008,\n  code: 'ENOTFOUND',\n  syscall: 'getaddrinfo',\n  hostname: 'my-redis'\n}\n`\n```\nIt seems to show that my redis is not found/not running in port 6379.\nThen I run `docker ps`, it shows `my-redis` run in port 6379.\n```\n`\nCONTAINER ID        IMAGE                            COMMAND                  CREATED             STATUS              PORTS                               NAMES\n...\nf637ee218d03        redis:6                          \"docker-entrypoint.s\u2026\"   18 minutes ago      Up 18 minutes       0.0.0.0:6379->6379/tcp              my-server_my-redis_1\n`\n```\ndocker-compose.yml\n```\n`version: '2.1'\n\nservices:\n  my-db:\n    image: mysql:5.7\n    ...\n    ports:\n      - 3306:3306\n  my-redis:\n    image: redis:6\n    ports:\n      - 6379:6379\n  my-web:\n    restart: always\n    environment:\n      - NODE_ENV=dev\n      - PORT=3030\n    build: .\n    command: >\n      sh -c \"npm install && ./wait-for-db-redis.sh my-db my-redis npm run dev\"\n    ports:\n      - \"3030:3030\"\n    volumes:\n      - ./:/server\n    depends_on:\n      - my-db\n      - my-redis\n\n`\n```\n.sequelizerc\n```\n`const path = require('path');\n\nmodule.exports = {\n  'config': path.resolve('migrations/config.js'),\n  'seeders-path': path.resolve('migrations/seeders'),\n  'models-path': path.resolve('migrations/models.js')\n};\n\n`\n```\nmigrations/model.js\n```\n`const Sequelize = require('sequelize');\nconst app = require('../src/app');\nconst sequelize = app.get('sequelizeClient');\nconst models = sequelize.models;\n\nmodule.exports = Object.assign({\n  Sequelize,\n  sequelize\n}, models);\n\n`\n```\nconfig.js\n```\n`const app = require('../src/app');\nconst env = process.env.NODE_ENV || 'development';\nconst dialect = 'mysql';\n\nmodule.exports = {\n  [env]: {\n    dialect,\n    url: app.get(dialect),\n    migrationStorageTableName: '_migrations'\n  }\n};\n\n`\n```",
      "solution": "Are you running the migration within the Docker Compose container for your app, or on the Docker host machine?\nFrom the host machine's point of view, there is no such hostname as `my-redis`  (it's only a thing within a Docker overlay network with that container in it).\nSince you've exposed the Redis port 6379 to your host (and in fact the whole wide world), you'd use `localhost:6379` on the host machine.",
      "question_score": 3,
      "answer_score": 6,
      "created_at": "2021-06-08T10:51:49",
      "url": "https://stackoverflow.com/questions/67884413/redis-connection-to-my-redis6379-failed-getaddrinfo-enotfound-when-running-se"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 75727667,
      "title": "Does setting spring.session.store-type to &#39;redis&#39; no longer provide a FindByIndexNameSessionRepository bean?",
      "problem": "I wrote a little spring security demo some time ago and I used redis for storing user sessions. I've been trying to upgrade it to Spring Boot 3 and Spring Security 6 but after raising project versions as follows:\nJava: 11 -> 17\nSpring Boot: 2.7.5 -> 3.0.4\nI can no longer autowire org.springframework.session.FindByIndexNameSessionRepository\nThis is my pom.xml:\n```\n`\n\n    4.0.0\n    \n        org.springframework.boot\n        spring-boot-starter-parent\n        3.0.4\n         \n    \n    com.example\n    spring-security-jpa-demo\n    0.0.1-SNAPSHOT\n    spring-security-jpa-demo\n    Demo project for Spring Boot\n    \n        17\n    \n    \n        \n            org.springframework.boot\n            spring-boot-starter-actuator\n        \n        \n            org.springframework.boot\n            spring-boot-starter-data-jpa\n        \n        \n            org.springframework.boot\n            spring-boot-starter-data-redis\n        \n        \n            org.springframework.boot\n            spring-boot-starter-security\n        \n        \n            org.springframework.boot\n            spring-boot-starter-web\n        \n        \n            org.liquibase\n            liquibase-core\n        \n        \n            org.springframework.session\n            spring-session-data-redis\n        \n\n        \n            io.lettuce\n            lettuce-core\n            6.2.3.RELEASE\n        \n        \n            org.mapstruct\n            mapstruct\n            1.5.3.Final\n        \n\n        \n            org.postgresql\n            postgresql\n            runtime\n        \n        \n            org.projectlombok\n            lombok\n            true\n        \n        \n            org.springframework.boot\n            spring-boot-starter-test\n            test\n        \n        \n            org.springframework.security\n            spring-security-test\n            test\n        \n    \n\n    \n        \n            \n                org.springframework.boot\n                spring-boot-maven-plugin\n                \n                    \n                        \n                            org.projectlombok\n                            lombok\n                        \n                    \n                \n            \n            \n                org.apache.maven.plugins\n                maven-compiler-plugin\n                3.5.1\n                \n                    17\n                    17\n                    \n                        \n                            org.projectlombok\n                            lombok\n                            ${lombok.version}\n                        \n                        \n                            org.mapstruct\n                            mapstruct-processor\n                            1.5.3.Final\n                        \n                    \n                \n            \n        \n    \n\n`\n```\nThis is my redis config in application.properties:\n```\n`...\n\nspring.session.store-type=redis\nspring.session.redis.namespace=spring-security-jpa-demo\nspring.data.redis.host=localhost\nspring.data.redis.password=redis\nspring.data.redis.port=6379\n\n...\n\n`\n```\nAnd I have redis and redis commander services in docker compose:\n```\n`version: '3.7'\n\nvolumes:\n  spring-security-jpa-demo-redis-data:\n    name: spring-security-jpa-demo-redis-data\n    driver: local\n\nnetworks:\n  spring-security-jpa-demo:\n    name: spring-security-jpa-demo\n    driver: bridge\n\nservices:\n\n  redis:\n    image: redis:5-alpine\n    volumes:\n      - spring-security-jpa-demo-redis-data:/data\n    command: redis-server --requirepass redis\n    networks:\n      - spring-security-jpa-demo\n    restart: always\n    ports:\n      - \"6379:6379\"\n\n  redis-commander:\n    image: rediscommander/redis-commander:latest\n    environment:\n      REDIS_HOST: redis\n      REDIS_PORT: 6379\n      REDIS_PASSWORD: redis\n    networks:\n      - spring-security-jpa-demo\n    ports:\n      - \"8081:8081\"\n    depends_on:\n      - redis\n\n...\n`\n```\nAnd this is how I was using the FindByIndexNameSessionRepository:\n```\n`package com.example.springsecurityjpademo.session;\n\nimport lombok.RequiredArgsConstructor;\nimport lombok.extern.slf4j.Slf4j;\nimport org.springframework.session.FindByIndexNameSessionRepository;\nimport org.springframework.session.Session;\nimport org.springframework.stereotype.Service;\n\n@Slf4j\n@Service\n@RequiredArgsConstructor\npublic class RedisSessionHandler {\n\n    private final FindByIndexNameSessionRepository sessionRepository;\n\n    public void removeAllUserSessions(String principalName) {\n        log.debug(\"Removing all sessions for user: {}\", principalName);\n        sessionRepository.findByPrincipalName(principalName)\n                .forEach((id, session) -> sessionRepository.deleteById(session.getId()));\n    }\n\n    public void removeUserSession(String principalName, String sessionId) {\n        var sessionOptional = sessionRepository.findByPrincipalName(principalName)\n                .values().stream().filter(session -> sessionId.equals(session.getId())).findFirst();\n\n        sessionOptional.ifPresent(session -> sessionRepository.deleteById(session.getId()));\n    }\n\n}\n\n`\n```\nSince the version upgrades the application fails to start with this error:\n```\n`Description:\n\nParameter 0 of constructor in com.example.springsecurityjpademo.session.RedisSessionHandler required a bean of type 'org.springframework.session.FindByIndexNameSessionRepository' that could not be found.\n\nAction:\n\nConsider defining a bean of type 'org.springframework.session.FindByIndexNameSessionRepository' in your configuration.\n`\n```\nI've tried removing the store-type property from application.properties and using the @EnableRedisHttpSession but with no success. Should this bean be manually configured now or am I just missing something in my configuration?\nNote that I am not running the app in a docker container, I am trying to run it locally first so that's why spring.data.redis.host is set to localhost.\nThanks in advance!",
      "solution": "I had the same issue but I solved it.\nYou use `@EnableRedisIndexedHttpSession` instead of `@EnableRedisHttpSession`.\nPlease see following GitHub issue.\nhttps://github.com/spring-projects/spring-session/issues/2235#issuecomment-1434657799",
      "question_score": 3,
      "answer_score": 3,
      "created_at": "2023-03-14T00:00:55",
      "url": "https://stackoverflow.com/questions/75727667/does-setting-spring-session-store-type-to-redis-no-longer-provide-a-findbyinde"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 73022013,
      "title": "What does &quot;bw: SpinningDown&quot; mean in a RedisTimeoutException?",
      "problem": "What does \"bw: SpinningDown\" mean in this error -\n\nTimeout performing GET (5000ms), next: GET foo!bar!baz, inst: 5, qu: 0, qs: 0, aw: False, bw: SpinningDown, ....\n\nDoes it mean that the Redis server instance is spinning down, or something else?",
      "solution": "It means something else actually. The abbreviation bw stands for Backlog-Writer, which contains the status of what the backlog is doing in Redis.\nFor this particular status: SpinningDown, you actually left out the important bits that relate to it.\nThere are 4 values being tracked for workers being Busy, Free, Min and Max.\nLet's take these hypothetical values: Busy=250,Free=750,Min=200,Max=1000\nIn this case there are 50 more existing (busy) threads than the minimum.\nThe cost of spinning up a new thread is high, especially if you hit the .NET-provided global thread pool limit. In which case only 1 new thread is created every 500ms due to throttling.\nSo once the Backlog is done processing an item, instead of just exiting the thread, it will keep it in a waiting state (SpinningDown) for 5 seconds. If during that time there still is more Backlog to process, the same thread will process another item from the Backlog.\nIf no Backlog item needed to be processed in those 5 seconds, the thread will be exited, which will eventually lead to a decrease in Busy (existing) threads.\nThis only happens for threads above the Min count of course, as those will be kept alive even if there is no work to do.",
      "question_score": 3,
      "answer_score": 5,
      "created_at": "2022-07-18T14:02:20",
      "url": "https://stackoverflow.com/questions/73022013/what-does-bw-spinningdown-mean-in-a-redistimeoutexception"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 71979224,
      "title": "How to solve [No available formula with the name &quot;redis&quot;.] with homebrew",
      "problem": "I can't install Redis server on my MacBook using brew.\n```\n`$ brew install redis \n`\n```\nIt throws this error for me :\nNo available formula with the name \"redis\".\nWhat do i do to solve this error ?\nHere is a SS of my terminal: https://i.sstatic.net/1u6fp.png",
      "solution": "```\n`rm -rf \"/usr/local/Homebrew/Library/Taps/homebrew/homebrew-core\"\nbrew tap homebrew/core\n`\n```",
      "question_score": 3,
      "answer_score": 5,
      "created_at": "2022-04-23T13:29:29",
      "url": "https://stackoverflow.com/questions/71979224/how-to-solve-no-available-formula-with-the-name-redis-with-homebrew"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 71672096,
      "title": "Why is Spring trying to create a `redisReferenceResolver` bean?",
      "problem": "My project does use Redis but my unit test mocks it. However it doesn't mock it enough because I get this stack trace snippet:\n```\n`Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'redisReferenceResolver': Cannot resolve reference to bean 'redisTemplate' while setting constructor argument; nested exception is org.springframework.beans.factory.NoSuchBeanDefinitionException: No bean named 'redisTemplate' available\n    at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveReference(BeanDefinitionValueResolver.java:378)\n    at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveValueIfNecessary(BeanDefinitionValueResolver.java:110)\n`\n```\nIt is correct. I have carefully ensured there is no redisTemplate bean available because I don't want to run redis in my unit test. But I don't know why it is trying to create the `redisReferenceResolver` bean.\nI've turned off autoconfiguration with this:\n```\n`spring:\n  profiles: RedisMock\n  autoconfigure:\n    exclude:\n      - org.springframework.boot.autoconfigure.data.redis.RedisAutoConfiguration\n`\n```\n(my test has  `@ActiveProfiles({\"WebMvcTest\",\"RedisMock\"})` in the top)\nI do have one class annotated with `@RedisHash` but I need that for my test.\nAnyone know what might be triggering the autoconfigure?\nThanks",
      "solution": "`redisReferenceResolver` is defined by Spring Data\u2019s support for Redis-based repositories. Setting `spring.data.redis.repositories.enabled` to `false` and ensuring that you haven\u2019t used `@EnableRedisRepositories` anywhere in your app will disable it.",
      "question_score": 3,
      "answer_score": 5,
      "created_at": "2022-03-30T07:47:36",
      "url": "https://stackoverflow.com/questions/71672096/why-is-spring-trying-to-create-a-redisreferenceresolver-bean"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 68549760,
      "title": "StackExchange.Redis ConnectionMultiplexer pool for synchronous methods",
      "problem": "Does implementing a ConnectionMultiplexer pool make sense if we use it for synchronous methods?\nSo by pool I mean creating multiple instances of StackExchange.Redis ConnectionMultiplexer, storing those objects and when I want to communicate with Redis server I take the least used one from the pool. This is to prevent timeouts due to large queue size as per No. 10 suggestion in this article: https://azure.microsoft.com/en-us/blog/investigating-timeout-exceptions-in-stackexchange-redis-for-azure-redis-cache/\nI'm having doubts because I'm not sure how can a queue even happen if connectionMultiplexer blocks a thread until a call returns.\nIt seems to me that having a pool is pointless with sync method calls, but Redis best practice articles suggest creating this kind of pool regardless of method type (sync/async)",
      "solution": "I think you're getting confused here. `ConnectionMultiplexer` does not \"get blocked\". Creating a `ConnectionMultiplexer` gives you a factory-like object with which you can create `IDatabase` instances. You then use these instances to perform normal Redis queries. You can also do Redis queries with the connection multiplexer itself, but those are server queries and unlikely to be done often.\nSo, to make things short, it can help tremendously to have a pool of connection multiplexers, regardless of sync/async/mixed usage.\n\nTo expand further, here's a very simple pool implementation, which can certainly be enhanced further:\n```\n`public interface IConnectionMultiplexerPool\n{\n    Task GetDatabaseAsync();\n}\n\npublic class ConnectionMultiplexerPool : IConnectionMultiplexerPool\n{\n    private readonly ConnectionMultiplexer[] _pool;\n    private readonly ConfigurationOptions _redisConfigurationOptions;\n\n    public ConnectionMultiplexerPool(int poolSize, string connectionString) : this(poolSize, ConfigurationOptions.Parse(connectionString))\n    {\n    }\n\n    public ConnectionMultiplexerPool(int poolSize, ConfigurationOptions redisConfigurationOptions)\n    {\n        _pool = new ConnectionMultiplexer[poolSize];\n        _redisConfigurationOptions = redisConfigurationOptions;\n    }\n\n    public async Task GetDatabaseAsync()\n    {\n        var leastPendingTasks = long.MaxValue;\n        IDatabase leastPendingDatabase = null;\n\n        for (int i = 0; i < _pool.Length; i++)\n        {\n            var connection = _pool[i];\n\n            if (connection == null)\n            {\n                _pool[i] = await ConnectionMultiplexer.ConnectAsync(_redisConfigurationOptions);\n\n                return _pool[i].GetDatabase();\n            }\n\n            var pending = connection.GetCounters().TotalOutstanding;\n\n            if (pending < leastPendingTasks)\n            {\n                leastPendingTasks = pending;\n                leastPendingDatabase = connection.GetDatabase();\n            }\n        }\n\n        return leastPendingDatabase;\n    }\n}\n`\n```",
      "question_score": 3,
      "answer_score": 4,
      "created_at": "2021-07-27T20:00:40",
      "url": "https://stackoverflow.com/questions/68549760/stackexchange-redis-connectionmultiplexer-pool-for-synchronous-methods"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 65530755,
      "title": "Proper way to pass python flask app context to rq jobs",
      "problem": "I've created a flask app that is effectively a news search app. It currently serves up a number of routes, integrates with elasticsearch for document searching and indexing and integrates with a mySQL DB via Flask-SQLAlchemy.\nThe app should enable logged in users to search for a document/article at which point an elasticsearch is triggered and the results are returned to the user. This all works.\nI am now building out functionality to kick off a background async search using RQ and Ajax. Now I need my RQ to queue up a task which will search (calling a few APIs), update my DB and update my ES Index. All of this functionality has already been built within my Flask app (search API calls, config, DB ORM, ES interface) and so I want to ideally reuse all of that.\nI've got an RQ set up which calls a module which then tries to create a new application instance, so that I can reuse the existing DB/ES setups but I get an error from RQ when the code gets called:\n```\n`File \"./news_functions.py\", line 14, in \n    from app import create_app, db\nModuleNotFoundError: No module named 'app'\n`\n```\nApplication structure is as follows:\n```\n`project\n\u2502   project.py   \n\u2502\n\u2514\u2500\u2500\u2500app\n\u2502   \u2502   __init.py__\n\u2502   \u2502   config.py\n\u2502   \u2502   models.py\n\u2502   \u2502   classes.py\n\u2502   \u2502   news_functions.py\n\u2502   \u2502   .env\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500\u2500main\n\u2502       \u2502   __init__.py\n\u2502       \u2502   forms.py\n\u2502       \u2502   views.py\n\u2502       \u2502   ...\n\u2502   \u2514\u2500\u2500\u2500static\n\u2502       \u2502   css, etc.\n\u2502   \u2514\u2500\u2500\u2500templates\n\u2502       \u2502   templates\n\u2502   \n\u2514\u2500\u2500\u2500migrations\n    \u2502   ... DB migrations here...\n\u2514\u2500\u2500\u2500logs\n    \u2502   ... logs stored here\n`\n```\nIn my views.py file I attempt to queue up the task with the following line:\n```\n`        job = current_app.task_queue.enqueue('news_functions.execute_search', jobKwargs)\n`\n```\nThis causes my no issues on the flask side but throws the aforementioned error on the RQ worker\n```\n`File \"./news_functions.py\", line 14, in \n    from app import create_app, db\nModuleNotFoundError: No module named 'app'\n`\n```\nIn the news_functions.py file I attempted to create an instance of the app so that I could reuse my models, etc with the following imports and setup which seems to be throwing the error:\n```\n`from app import create_app, db\n\napp = create_app()\napp.app_context().push()\n`\n```\nI think that the issue is that I'm trying to import create_app from within the app folder itself but I don't know what my other options are. If I move up a folder then the RQ job can't queue up the task as it's outside the scope of the app.\nI'm not sure if I'm approaching this in the right way but I effectively want to be able to reuse the application config, flask-sqlalchemy, es setup to execute this additional task in the background.\nHopefully makes sesnse and someone can help!",
      "solution": "After absolutely pulling my hair out I realised that I had started the RQ worker from within the app directory itself rather than the main directory which meant that it was effectively looking for a module named 'app' within the 'app' folder, i.e. app.app.news_functions which obviously didn't exist.\nDefinite lesson learned after 2 days of pulling my hair out as to why I couldn't import a module which I knew existed!\nAlways check your working directories, kids!",
      "question_score": 3,
      "answer_score": 5,
      "created_at": "2021-01-01T15:56:19",
      "url": "https://stackoverflow.com/questions/65530755/proper-way-to-pass-python-flask-app-context-to-rq-jobs"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 76729900,
      "title": "Throttling with Redis Only on Production in Laravel",
      "problem": "Background\nOut of the box, Laravel provides two middleware that can be used for rate limiting (throttling):\n```\n`\\Illuminate\\Routing\\Middleware\\ThrottleRequests::class\n\\Illuminate\\Routing\\Middleware\\ThrottleRequestsWithRedis::class\n`\n```\nAs the documentation states, if you are using Redis as the cache driver, you can change the mapping in the `Kernel.php` like so:\n```\n`/**\n * The application's middleware aliases.\n *\n * Aliases may be used instead of class names to conveniently assign middleware to routes and groups.\n *\n * @var array\n */\nprotected $middlewareAliases = [\n    // ...\n    'throttle' => \\Illuminate\\Routing\\Middleware\\ThrottleRequestsWithRedis::class\n    // ...\n];\n`\n```\nThe Issue\nThe issue with this is that the above is not dynamic, dependent on the environment. For instance, on my `staging` and `production` environments, I utilise Redis, but on my `local` and `development` environments, I do not.\nPotential Solution\nThere is an obvious dirty fix, something like this (`Kernel.php`):\n```\n`/**\n * The application's middleware aliases.\n *\n * Aliases may be used instead of class names to conveniently assign middleware to routes and groups.\n *\n * @var array\n */\nprotected $middlewareAliases = [\n    // ...\n    'throttle' => \\Illuminate\\Routing\\Middleware\\ThrottleRequests::class\n    // ...\n];\n\n/**\n * Create a new HTTP kernel instance.\n *\n * @param  \\Illuminate\\Contracts\\Foundation\\Application  $app\n * @param  \\Illuminate\\Routing\\Router  $router\n * @return void\n */\npublic function __construct(Application $app, Router $router)\n{\n    if ($app->environment('production')) {\n        $this->middlewareAliases['throttle'] = \\Illuminate\\Routing\\Middleware\\ThrottleRequestsWithRedis::class;\n    }\n\n    parent::__construct($app, $router);\n}\n`\n```\nIs there a 'standard' way to achieve this without having to override the `Kernel` constructor? Essentially, I would like my app to dynamically choose the relevant middleware dependant on whether the environment is set to `production` (or, the default cache store is set to `redis`).\nUpdate\nThe above solution does not work as the Kernel is accessed prior to the app being bootstrapped, thus the environment is not available at this point. The other solution I am now looking into is extending the base `ThrottleRequests` class in order dynamically call the relevant class.",
      "solution": "After much research and testing, I concluded that the best solution was to dynamically set the `throttle` middleware in the `RouteServiceProvider` like so:\n```\n`class RouteServiceProvider extends ServiceProvider\n{\n    /**\n     * Bootstrap any application services.\n     *\n     * @return void\n     */\n    public function boot(): void\n    {\n        $this->registerThrottleMiddleware();\n    }\n\n    /**\n     * Register the middleware used for throttling requests.\n     *\n     * @return void\n     */\n    private function registerThrottleMiddleware(): void\n    {\n        $router = app()->make('router');\n\n        $middleware = config('cache.default') !== 'redis'\n            ? \\Illuminate\\Routing\\Middleware\\ThrottleRequests::class\n            : \\Illuminate\\Routing\\Middleware\\ThrottleRequestsWithRedis::class;\n\n        $router->aliasMiddleware('throttle', $middleware);\n    }\n}\n`\n```",
      "question_score": 3,
      "answer_score": 6,
      "created_at": "2023-07-20T14:31:05",
      "url": "https://stackoverflow.com/questions/76729900/throttling-with-redis-only-on-production-in-laravel"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 75798268,
      "title": "redis - ERR unknown command &#39;HELLO&#39; (RedisClient::CommandError)",
      "problem": "\"I have enabled Redis locally, but an error occurred while running the script. What should I do to resolve this?\"\n```\n`/home/linlin/.asdf/installs/ruby/3.1.2/lib/ruby/gems/3.1.0/gems/redis-client-0.11.1/lib/redis_client/connection_mixin.rb:48:in `call_pipelined': ERR unknown command 'HELLO' (RedisClient::CommandError)\n    from /home/linlin/.asdf/installs/ruby/3.1.2/lib/ruby/gems/3.1.0/gems/redis-client-0.11.1/lib/redis_client.rb:678:in `block in connect'\n    from /home/linlin/.asdf/installs/ruby/3.1.2/lib/ruby/gems/3.1.0/gems/redis-client-0.11.1/lib/redis_client/middlewares.rb:16:in `call'\n    from /home/linlin/.asdf/installs/ruby/3.1.2/lib/ruby/gems/3.1.0/gems/redis-client-0.11.1/lib/redis_client.rb:677:in `connect'\n    from /home/linlin/.asdf/installs/ruby/3.1.2/lib/ruby/gems/3.1.0/gems/redis-client-0.11.1/lib/redis_client.rb:647:in `raw_connection'\n    from /home/linlin/.asdf/installs/ruby/3.1.2/lib/ruby/gems/3.1.0/gems/redis-client-0.11.1/lib/redis_client.rb:614:in `ensure_connected'\n    from /home/linlin/.asdf/installs/ruby/3.1.2/lib/ruby/gems/3.1.0/gems/redis-client-0.11.1/lib/redis_client.rb:349:in `pipelined'\n    from /home/linlin/.asdf/installs/ruby/3.1.2/lib/ruby/gems/3.1.0/gems/redis-client-0.11.1/lib/redis_client/decorator.rb:51:in `pipelined'\n    from /home/linlin/.asdf/installs/ruby/3.1.2/lib/ruby/gems/3.1.0/gems/sidekiq-7.0.1/lib/sidekiq/client.rb:214:in `block in raw_push'\n    from /home/linlin/.asdf/installs/ruby/3.1.2/lib/ruby/gems/3.1.0/gems/connection_pool-2.3.0/lib/connection_pool.rb:65:in `block (2 levels) in with'\n    from /home/linlin/.asdf/installs/ruby/3.1.2/lib/ruby/gems/3.1.0/gems/connection_pool-2.3.0/lib/connection_pool.rb:64:in `handle_interrupt'\n    from /home/linlin/.asdf/installs/ruby/3.1.2/lib/ruby/gems/3.1.0/gems/connection_pool-2.3.0/lib/connection_pool.rb:64:in `block in with'\n    from /home/linlin/.asdf/installs/ruby/3.1.2/lib/ruby/gems/3.1.0/gems/connection_pool-2.3.0/lib/connection_pool.rb:61:in `handle_interrupt'\n    from /home/linlin/.asdf/installs/ruby/3.1.2/lib/ruby/gems/3.1.0/gems/connection_pool-2.3.0/lib/connection_pool.rb:61:in `with'\n    from /home/linlin/.asdf/installs/ruby/3.1.2/lib/ruby/gems/3.1.0/gems/sidekiq-7.0.1/lib/sidekiq/client.rb:211:in `raw_push'\n    from /home/linlin/.asdf/installs/ruby/3.1.2/lib/ruby/gems/3.1.0/gems/sidekiq-7.0.1/lib/sidekiq/client.rb:92:in `push'\n    from /home/linlin/.asdf/installs/ruby/3.1.2/lib/ruby/gems/3.1.0/gems/sidekiq-7.0.1/lib/sidekiq/job.rb:365:in `client_push'\n    from /home/linlin/.asdf/installs/ruby/3.1.2/lib/ruby/gems/3.1.0/gems/sidekiq-7.0.1/lib/sidekiq/job.rb:198:in `perform_async'\n    from /home/linlin/.asdf/installs/ruby/3.1.2/lib/ruby/gems/3.1.0/gems/sidekiq-7.0.1/lib/sidekiq/job.rb:290:in `perform_async'\n\n`\n```",
      "solution": "Setting the protocol version to `2` will resolve the error for most of the cases, or you can choose to upgrade the redis-client version above 6.\nThe redis protocol version can be set to either 1 or 2.\nProtocol version 2 is an enhanced protocol version introduced in Redis 6.0.\nYou can add the protocol something like below to redis config,\n`Sidekiq.configure_client do |config|\n  config.redis = {\n    url: Rails.application.credentials.config[:REDIS_URL],\n    protocol: 2,\n  }\nend\n`",
      "question_score": 3,
      "answer_score": 4,
      "created_at": "2023-03-21T08:17:43",
      "url": "https://stackoverflow.com/questions/75798268/redis-err-unknown-command-hello-redisclientcommanderror"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 72698156,
      "title": "Why did Redis run out of memory even with maxmemory volatile-lru?",
      "problem": "A few days ago my Redis instance went down. All new write attempts failed with this error:\n```\n`OOM command not allowed when used memory > 'maxmemory'\n`\n```\nIt only recovered after I flushed all the data. It runs in a VPS that has 24gb of RAM and only runs Redis, with this configuration:\n```\n`maxmemory 20gb\nmaxmemory-policy volatile-lru\nsave \"\"\n`\n```\nI use this instance to store sessions (notice though that persistence is disabled). Every session is written into Redis with an expiration time of 2 days, that means that all the keys have a TTL:\n```\n`# Keyspace\ndb0:keys=1426936,expires=1425758,avg_ttl=87980766\n`\n```\nThen why did it run out of memory? If the eviction policy is volatile-lru and all the keys have an expiration time set, why did it failed when it reached the maxmemory setting instead of evicting keys to free up memory?\nAnother thing to consider: the load of my application is very constant and stable, no peaks. The sessions are stored with an expiration time of 2 days. Now it's been six days running since I restarted the instance and flushed all, and Redis reports used_memory_human:781.54M. But when I check my server stats, I can see that the memory usage had been slowly increasing until the incident. And when I say slowly is really slowly: it took almost an year to reach the maxmemory=20gb limit.\nBut wait! How is that possible if sessions expire in two days? Could the incident be related to fragmentation ratio? I mean, sessions expire in two days, and all the time new sessions are being written to Redis. Is it possible that fragmentation ratio increased slowly during an year, making Redis fail regardless it had been configured with an eviction policy?\nOr is there another theoretical situation where Redis can't free up memory fast enough? Thanks in advance!",
      "solution": "After a lot of testing and reading, I conclude that the incident was caused by memory fragmentation. I've been able to solve it turning on \"activedefrag\".\nIt turns out that memory fragmentation is something to expect in certain scenarios. Scenarios like mine: a Redis instance that handles millions of sessions, that is, a lot of small new keys are constantly being written, and also being deleted (because of expiration).\nI was able to verify this: a few days after the incident, having flushed the data and restarted Redis, I saw how mem_fragmentation_ratio went increasing very slowly, from 1.05 up to 2 or even more!\nThen I turned on activedefrag and mem_fragmentation_ratio started to decrease until 1.05 again.\nNow it's been a week since it's running smoothly and mem_fragmentation_ratio never goes beyond 1.08 :)\nIn case you have doubts, I can say that the performance cost of turning on activedefrag is almost negligible.\nHere is some interesting readings:\nhttps://serverfault.com/questions/971804/are-there-situations-were-activedefrag-should-be-kept-disabled-in-redis-5-with",
      "question_score": 3,
      "answer_score": 4,
      "created_at": "2022-06-21T11:13:37",
      "url": "https://stackoverflow.com/questions/72698156/why-did-redis-run-out-of-memory-even-with-maxmemory-volatile-lru"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 72115457,
      "title": "How can I get results/failures for celery tasks from a redis backend when I don&#39;t store their their task-ID?",
      "problem": "In my web-application I start backgrounds jobs with celery without storing their id. Some of the task are periodic and some are triggered by user-interaction. The celery-tasks just do their thing and eventually the user will see the updated data in their browser. When a task has recently failed, I want to notify all logged-in admin-users about it (since they are usually the ones who triggered the recent failure). So they at least know something's up.\nThe relevant celery-methods I found, either require a valid task-id (e.g. `celery.result.AsyncResult`) or they only have infos about active tasks, but not about finished/failed tasks (e.g. `celery.app.control.Inspect`).\nI am using a flask-frontend, a redis-backend for celery and also a regular DB for persistent data.\nHow would I collect information about recently finished or failed celery tasks in this scenario?\nWhat I have tried:\n`# I setup celery with \nmy_celery_project = Celery(__name__,\n                backend='redis://localhost:1234/0',\n                broker='redis://localhost:1234/0')\n\n# later in the view I want to collect status information:\n\ni = my_celery_project.control.inspect()\n\ni.active() # this one exists, but I don't care about it\ni.failed() # this is what I want, but it doesn't exist\ni.results() # this also doesn't exist\n\n# getting the result directly also doesn't work, since they require an id, which i don't have\nres = AsyncResult(id_i_don_have,app=app)\n`\nIt should be possible, since I can get the results manually from redis with `redis-cli --scan` and then do `my_task.AsyncResult('id_from_redis').status` to check the result. Something similar to flower could also work, but that would't work so well with the state-less nature of a web-application, I think.\n\nthis is not a duplicate of these questions, since they don't assume a redis-backend. Also they are 4+ years out-of-date:\n\nCelery: list all tasks, scheduled, active *and* finished\nPython celery: Retrieve tasks arguments if there's an exception\nHow can I get a list of succeeded Celery task ids?\n\nthis is not a duplicate of these questions, since my redis-backend is in fact working:\n\nCelery (Redis) results backend not working\nCelery tasks not returning results from redis\n\nthis is not a duplicate of this questions, since it is exactly the opposite to my questions. They care about old results, while I care explicitly only about recent results: How to read celery results from redis result backend",
      "solution": "in the end my solution was to fetch the IDs directly form the backend and then convert them to Object via my celery-instance:\n`\n  task_results: List[AsyncResult] = []\n  for key in my_celery_project.backend.client.scan_iter(\"celery-task-meta-*\"):\n    task_id = str(key).split(\"celery-task-meta-\", 1)[1].replace(\"'\", \"\")\n    task_results.append(self.celery.AsyncResult(task_id))\n  return task_results\n`\nthen I used `async_result.ready()` to filter out the ones I'm interested on.\non a side note: Now I also call `async_result.forget()` to cleanup old tasks, which I didn't do before.",
      "question_score": 3,
      "answer_score": 2,
      "created_at": "2022-05-04T17:17:36",
      "url": "https://stackoverflow.com/questions/72115457/how-can-i-get-results-failures-for-celery-tasks-from-a-redis-backend-when-i-don"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 72058013,
      "title": "Error in Lua script since last Redis update",
      "problem": "Since Redis 6.2.7 (and Redis 7) an existing Lua script stopped working with the error message:\n\"ERR user_script:6: Attempt to modify a readonly table script: 2f405679dab26da46ec86d29bded48f66a99ff64, on @user_script:6.\"\nThe script is working fine with Redis 6.2.6. I did not find any breaking changes in the last Redis release notes.\nAny clue ? Thanks !!\nHere's the script:\n```\n`-- returns valid task ID if successfull, nil if no tasks\n\nlocal tenantId = unpack(ARGV)\nlocal activeTenantsSet, activeTenantsList, tenantQueue = unpack(KEYS)\n\n-- next task lua based function - return nil or taskId\nfunction next ()\n    local task = redis.call('ZPOPMAX', tenantQueue)\n    if table.getn(task) == 0 then\n        redis.call('SREM', activeTenantsSet, tenantId)\n        redis.call('LREM', activeTenantsList, 0, tenantId)\n        return nil\n    end\n    redis.call('SADD', activeTenantsSet, tenantId)\n    redis.call('RPUSH', activeTenantsList, tenantId)\n    return task[1]\nend\n\n-------------\nreturn next()\n`\n```",
      "solution": "try adding 'local' in front of 'function next'\n```\n`local function next ()\n...\n`\n```",
      "question_score": 3,
      "answer_score": 3,
      "created_at": "2022-04-29T14:29:52",
      "url": "https://stackoverflow.com/questions/72058013/error-in-lua-script-since-last-redis-update"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 71579951,
      "title": "Redis Connection Error with Heroku Redis Instance: Redis connection to failed - read ECONNRESET heroku instance",
      "problem": "I have a node js app. And I use Redis from Heroku Redis(with `async-redis` library).\nActually, I have two different Heroku accounts and two different Node.js apps hosted by Heroku. But except Redis credentials, both apps are the same code.\nThe interesting thing on my app I can connect to first Heroku Redis instance. But I can't connect to new Heroku Redis instance. Besides I deleted and created new instances, bu they don't work.\nThe error is:\n```\n`    Error: Redis connection to redis-123.compute.amazonaws.com:28680 failed - read ECONNRESET\\n    \nat TCP.onStreamRead (internal/stream_base_commons.js:162:27)\n`\n```\nMy connection statement like this:\n```\n`var redisPassword = 'password123';\nvar redisOptions = { host: 'redis-123.cloud.redislabs.com', port: '17371', auth_pass: redisPassword }\n//var redisPassword = 'password123';\n//var redisOptions = { host: 'redis-123.compute.amazonaws.com', port: '28680', auth_pass: redisPassword }\n\nconst client = redis.createClient(redisOptions);\n\nclient.on('connect', function () {\n  console.log('Redis client connected');\n});\n\nclient.on('error', function (err) {\n  console.log('An error on Redis connection: ' + err);\n});\n`\n```\nAs I can see there is the only thing that different on Heroku Redis instances. My first Redis instance hosts at cloud.redislabs.com but the second instance(that i can't connect) hosts at compute.amazonaws.com.\nAny help will be much appreciated.",
      "solution": "I couldn't find the root problem. But after comment of Chris, I checked again Heroku Redis addons I used.\n\nHeroku Redis gives me an instance from amazonaws.com, and Redis Enterprise Cloud gives me an instance from redislabs.com. When I added and used Redis Enterprise Cloud, I could connect to it.\nBut Heroku Redis's connection problem still is a secret for me.",
      "question_score": 3,
      "answer_score": 1,
      "created_at": "2022-03-23T00:02:32",
      "url": "https://stackoverflow.com/questions/71579951/redis-connection-error-with-heroku-redis-instance-redis-connection-to-failed"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 71090906,
      "title": "FastAPI WebSocket replication",
      "problem": "I have implemented a simple WebSocket proxy with FastAPI (using this example)\nThe application target is to just pass through all messages it gets to its active connections  (proxy).\nIt works well only with a single instance because it keeps active WebSocket connections in memory. And memory is not shared when there is more than one instance.\nMy naive approach was to solve it by keeping active connections in some shared storage (Redis). But I was stuck with pickling it.\nHere is the complete app:\n```\n`import pickle\n\nfrom fastapi import FastAPI, WebSocket, WebSocketDisconnect\nfrom collections import defaultdict\nimport redis\n\napp = FastAPI()\nrds = redis.StrictRedis('localhost')\n\nclass ConnectionManager:\n    def __init__(self):\n        self.active_connections = defaultdict(dict)\n\n    async def connect(self, websocket: WebSocket, application: str, client_id: str):\n        await websocket.accept()\n        if application not in self.active_connections:\n            self.active_connections[application] = defaultdict(list)\n\n        self.active_connections[application][client_id].append(websocket)\n\n        #### this is my attempt to store connections ####\n        rds.set('connections', pickle.dumps(self.active_connections)) \n\n    def disconnect(self, websocket: WebSocket, application: str, client_id: str):\n        self.active_connections[application][client_id].remove(websocket)\n\n    async def broadcast(self, message: dict, application: str, client_id: str):\n        for connection in self.active_connections[application][client_id]:\n            try:\n                await connection.send_json(message)\n                print(f\"sent {message}\")\n            except Exception as e:\n                pass\n\nmanager = ConnectionManager()\n\n@app.websocket(\"/ws/channel/{application}/{client_id}/\")\nasync def websocket_endpoint(websocket: WebSocket, application: str, client_id: str):\n    await manager.connect(websocket, application, client_id)\n    while True:\n        try:\n            data = await websocket.receive_json()\n            print(f\"received: {data}\")\n            await manager.broadcast(data, application, client_id)\n        except WebSocketDisconnect:\n            manager.disconnect(websocket, application, client_id)\n        except RuntimeError:\n            break\n\nif __name__ == '__main__':\n    import uvicorn\n\n    uvicorn.run(app, host='0.0.0.0', port=8005)\n`\n```\nHowever, pickling websocket connection was not successful:\n```\n`AttributeError: Can't pickle local object 'FastAPI.setup..openapi'\n`\n```\nWhat is the proper way to have WebSocket connections stored across the application instances?\nUPD The actual solution per @AKX answer.\nEach instance of the server is subscribed to Redis pubsub and tries to send the received message to all its connected clients.\nSince one client cannot be connected to several instances - each message should be delivered to each client only once\n```\n`import json\nimport asyncio\n\nfrom fastapi import FastAPI, WebSocket, WebSocketDisconnect\nfrom collections import defaultdict\nimport redis\n\napp = FastAPI()\nrds = redis.StrictRedis('localhost')\n\nclass ConnectionManager:\n    def __init__(self):\n        self.active_connections = defaultdict(dict)\n\n    async def connect(self, websocket: WebSocket, application: str, client_id: str):\n        await websocket.accept()\n        if application not in self.active_connections:\n            self.active_connections[application] = defaultdict(list)\n\n        self.active_connections[application][client_id].append(websocket)\n\n    def disconnect(self, websocket: WebSocket, application: str, client_id: str):\n        self.active_connections[application][client_id].remove(websocket)\n\n    async def broadcast(self, message: dict, application: str, client_id: str):\n        for connection in self.active_connections[application][client_id]:\n            try:\n                await connection.send_json(message)\n                print(f\"sent {message}\")\n            except Exception as e:\n                pass\n\n    async def consume(self):\n        print(\"started to consume\")\n        sub = rds.pubsub()\n        sub.subscribe('channel')\n        while True:\n            await asyncio.sleep(0.01)\n            message = sub.get_message(ignore_subscribe_messages=True)\n            if message is not None and isinstance(message, dict):\n                msg = json.loads(message.get('data'))\n                await self.broadcast(msg['message'], msg['application'], msg['client_id'])\n\nmanager = ConnectionManager()\n\n@app.on_event(\"startup\")\nasync def subscribe():\n    asyncio.create_task(manager.consume())\n\n@app.websocket(\"/ws/channel/{application}/{client_id}/\")\nasync def websocket_endpoint(websocket: WebSocket, application: str, client_id: str):\n    await manager.connect(websocket, application, client_id)\n    while True:\n        try:\n            data = await websocket.receive_json()\n            print(f\"received: {data}\")\n            rds.publish(\n                'channel',\n                json.dumps({\n                   'application': application,\n                   'client_id': client_id,\n                   'message': data\n                })\n            )\n        except WebSocketDisconnect:\n            manager.disconnect(websocket, application, client_id)\n        except RuntimeError:\n            break\n\nif __name__ == '__main__':  # pragma: no cover\n    import uvicorn\n\n    uvicorn.run(app, host='0.0.0.0', port=8005)\n`\n```",
      "solution": "What is the proper way to have WebSocket connections stored across the application instances?\n\nThere is no practical way for multiple processes to share websocket connections that I know of. As you noticed, you can't pickle the connections (not least because you can't pickle the actual OS-level file descriptor that represents the network connection). You can send file descriptors to other processes with some POSIX magic, but even so, you'd also need to make sure the processes know the state of the websocket and don't e.g. race at sending or receiving data.\nI would probably redesign things to have a single process that manages the websocket connections and e.g. uses Redis (since you already have it) pubsub or streams to communicate with other multiple processes you have.",
      "question_score": 3,
      "answer_score": 4,
      "created_at": "2022-02-12T11:10:37",
      "url": "https://stackoverflow.com/questions/71090906/fastapi-websocket-replication"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis",
      "question_id": 70896382,
      "title": "Django and redis adding :1 to keys",
      "problem": "I'm using django-redis to store some data on my website, and I have a problem where Redis is adding :1 at the beginning so my key looks like this: `:1:my_key`\nI'm not sure why is it doing this, I've read the documentation on django-redis and I couldn't find anything related, so my guess it has something to do with redis but I can't figure out what.\nIn my settings.py I have the regular:\n```\n`CACHES = {\n    \"default\": {\n        \"BACKEND\": \"django_redis.cache.RedisCache\",\n        \"LOCATION\": \"redis://xxxxx/0\",\n        \"OPTIONS\": {\n            \"CLIENT_CLASS\": \"django_redis.client.DefaultClient\",\n       }\n   }\n}\n`\n```\nAnd in my tasks.py I set keys like the documentation says:\n```\n`from django.core.cache import cache\ncache.set(my_key, my_value, 3600)\n`\n```\nSo now I can't get the values using the `cache.get(my_key)`",
      "solution": "`:1` it's version\n`cache.set(key, value, timeout=DEFAULT_TIMEOUT, version=None)`\nYou can remove it by set empty string:\n`cache.set(\"foo\", \"bar\",version='')`\nIn the redis you will get:\n`::foo`",
      "question_score": 3,
      "answer_score": 3,
      "created_at": "2022-01-28T16:44:14",
      "url": "https://stackoverflow.com/questions/70896382/django-and-redis-adding-1-to-keys"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis-cluster",
      "question_id": 73244409,
      "title": "Unable to provision Pod in EKS cluster configured with Fargate",
      "problem": "I have recently setup an EKS cluster with Fargate.\nWhen I tried to deploy Redis Service on k8s using guide, I am getting the following errors:\n\nPod provisioning timed out (will retry) for pod: default/redis-operator-79d6d769dc-j246j\nDisabled logging because aws-logging configmap was not found. configmap \"aws-logging\" not found\n\nFor solving the above errors, I tried the following solutions but none of them worked\n\nCreated a NAT gateway for granting internet connection to the instances in the private subnets.\nUpdated CoreDNS to run pods on Fargate. Reference",
      "solution": "The NAT gateway that I created was in the private subnet. The private subnets themselves don't have any access to the internet. Hence, I was stuck in a loop.\nBy creating a nat gateway in a public subnet and then adding in the router table of private subnets being used by the EKS cluster I was able to schedule the pods",
      "question_score": 4,
      "answer_score": 3,
      "created_at": "2022-08-05T05:40:26",
      "url": "https://stackoverflow.com/questions/73244409/unable-to-provision-pod-in-eks-cluster-configured-with-fargate"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis-cluster",
      "question_id": 69726157,
      "title": "Keys in Redis cluster cannot be deleted (and have empty value)",
      "problem": "We have a Redis cluster (3 master, 3 slave) and are seeing a large number of keys on one of the master nodes (and related slaves) which appear to be empty and cannot be deleted.\nIf I connect to the master node which has a large number of entries (as determined by DBSIZE), I can SCAN for and see the keys:\n`--> redis-cli -h 192.168.100.81 -p 6381\n192.168.100.81:6381> scan 0 match mykey-* count 10\n1) \"2359296\"\n2)  1) \"mykey-1be333a7\"\n    2) \"mykey-e85a9d31\"\n    3) \"mykey-d9162eff\"\n    4) \"mykey-41d12fd8\"\n    5) \"mykey-a6e755d3\"\n    6) \"mykey-c2aa1eaa\"\n    7) \"mykey-c0597cac\"\n    8) \"mykey-10e69376\"\n    9) \"mykey-7263aef0\"\n   10) \"mykey-7fa9de50\"\n`\nHowever, if I try and GET the value of a key, it shows it has moved:\n`192.168.100.81:6381> get mykey-1be333a7\n(error) MOVED 8301 192.168.3.107:6380\n`\nIf I connect to the node to which the key has moved, I am not able to GET or DEL the value:\n`--> redis-cli -h 192.168.3.107 -p 6380\n192.168.3.107:6380> get mykey-1be333a7\n(nil)\n192.168.3.107:6380> del mykey-1be333a7\n(integer) 0\n`\nI am also unable to GET or DEL the value using the cluster (`-c`) flag for redis-cli:\n`--> redis-cli -h 192.168.100.81 -p 6381 -c get mykey-1be333a7\n(nil)\n--> redis-cli -h 192.168.100.81 -p 6381 -c del mykey-1be333a7\n(integer) 0\n--> redis-cli -h 192.168.3.107 -p 6380 -c get mykey-1be333a7\n(nil)\n--> redis-cli -h 192.168.3.107 -p 6380 -c del mykey-1be333a7\n(integer) 0\n`\nWhat can I do to remove these types of keys?",
      "solution": "Interesting question!\nHow to reproduce the problem\nThe following is a scenario that will reproduce your problem:\nYou create a standalone Redis instance, and set some data into it. However, one day, you configure this standalone Redis as a member of a Redis Cluster, without flushing the old data or moving the old data to the right node of the cluster. Let's call these data as dirty data.\nIn this scenario, you can SCAN all keys on this instance, including the dirty data. However, you cannot read or write these dirty data, since your Redis is in cluster mode, it will redirect your request to the right node, which doesn't have such data.\nHow to solve the problem\nIn order to remove these dirty data, you should reconfigure your Redis instance into standalone mode, i.e. cluster-enabled no, and delete dirty data in standalone mode.\nThen you can make it join the cluster again.",
      "question_score": 3,
      "answer_score": 4,
      "created_at": "2021-10-26T17:44:25",
      "url": "https://stackoverflow.com/questions/69726157/keys-in-redis-cluster-cannot-be-deleted-and-have-empty-value"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis-cluster",
      "question_id": 66704270,
      "title": "Run Redis on docker network mode host",
      "problem": "I want to run network mode as host for Redis cluster in docker.\nThe command to execute in the redis image of docker hub, is written as follows.\n```\n`docker run --name some-redis -d redis\n`\n```\nAfter performing port mapping by giving the -p option as follows:\n```\n`docker run -p 6379:6379 --name some-redis redis\n`\n```\nIf you connect to redis-cli, the connection is good.\n```\n`> redis-cli -p 6379\n\n127.0.0.1:6379>\n`\n```\nIf you look at the network with docker container inspect some-redis, the default network mode is bridge.\n```\n`\"Networks\": {\n                \"bridge\": {\n                    \"IPAMConfig\": null,\n                    \"Links\": null,\n                    \"Aliases\": null,\n`\n```\nI tried to proceed with the host.\n```\n`docker run --net=host -p 6379:6379 --name some-redis redis\n`\n```\n```\n`WARNING: Published ports are discarded when using host network mode\n`\n```\nIf a phrase such as, is displayed, the port is not disclosed.\n```\n`CONTAINER ID   IMAGE                        COMMAND                  CREATED          STATUS          PORTS                                            NAMES\n6f4edb2c4f8a   redis                        \"docker-entrypoint.s\u2026\"   5 seconds ago    Up 4 seconds                                                     some-redis\n`\n```\nOf course, you can't even connect to redis-cli.\n```\n`> redis-cli -p 6379\n\nCould not connect to Redis at 127.0.0.1:6379: Connection refused\nnot connected>\n`\n```\nDocker container doesn't expose ports when --net=host is mentioned in the docker run command\nLooking at the above question, it seems that the port should be automatically public.\nHow can I expose ports in network mode host in docker redis?",
      "solution": "Check you machine, the `--net=host` only works in linux.\nhttps://docs.docker.com/network/host/\n\nThe host networking driver only works on Linux hosts, and is not supported on Docker Desktop for Mac, Docker Desktop for Windows, or Docker EE for Windows Server.",
      "question_score": 2,
      "answer_score": 4,
      "created_at": "2021-03-19T08:46:38",
      "url": "https://stackoverflow.com/questions/66704270/run-redis-on-docker-network-mode-host"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis-cluster",
      "question_id": 70795549,
      "title": "What happens when you don&#39;t UNWATCH WATCH&#39;d keys in Redis",
      "problem": "While poking through Redis I came across the WATCH command -- which is a facet of Redis transactions.\nThe related section on Transactions explains in a bit more detail, how WATCH can work with other Redis concurrency commands.\nHowever, one thing that confuses me is: What happens if I call WATCH on keys, but don't (for whatever reason) UNWATCH them? Is there some cache of WATCH'd keys that fills and then starts discarding older WATCH'd keys? Will this cause latency issues?\nAny comments would be helpful :)",
      "solution": "What happens if I call WATCH on keys, but don't (for whatever reason) UNWATCH them?\n\nThese keys will be added to a watched key list.\n\nIs there some cache of WATCH'd keys that fills and then starts discarding older WATCH'd keys?\n\nNO. Watched keys won't be removed unless you unwatch it (calls `UNWATCH`, `EXEC` or `DISCARD`), or the connection is closed.\n\nWill this cause latency issues?\n\nIf there're keys watched, each time you modify a key (no matter whether it 's a watched key or not), Redis costs some CPU circle to handle key watch related stuff. So you'd better unwatch keys ASAP.",
      "question_score": 2,
      "answer_score": 3,
      "created_at": "2022-01-21T03:08:30",
      "url": "https://stackoverflow.com/questions/70795549/what-happens-when-you-dont-unwatch-watchd-keys-in-redis"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis-cluster",
      "question_id": 65751542,
      "title": "Data is not replicated from Redis master to slaves",
      "problem": "I am connecting to a small Redis cluster hosted by AWS which is not sharded, it has just one master and 3 slaves. I have been having difficulty with replication, and this is a simple reproduction of what I tried.\nMy clusters:\n```\n`172.28.52.18:6379> CLUSTER NODES\ncc378ecb71d02c2495e219ce7043ea343eb91c1f 172.28.52.18:6379@1122 myself,slave 6ce7214224036cc42ba272486d9e8fe5d1b11875 0 1610811475000 2 connected\n1e17d7794741c7db491a888dc2bca76590b52e64 172.28.53.83:6379@1122 slave 6ce7214224036cc42ba272486d9e8fe5d1b11875 0 1610811476195 2 connected\n6ce7214224036cc42ba272486d9e8fe5d1b11875 172.28.53.213:6379@1122 master - 0 1610811474182 2 connected 0-16383\nd780dfbb4d33275e50c098032d1cceb1cd368a65 172.28.52.180:6379@1122 slave 6ce7214224036cc42ba272486d9e8fe5d1b11875 0 1610811475189 2 connected\n`\n```\nConnect directly to master (172.28.53.213 in the above output) and set some data:\n```\n`$ redis-cli -h 172.28.53.213\n172.28.53.213:6379> SET key1 value1\nOK\n`\n```\nThen connect to one of the slaves:\n```\n`$ redis-cli -h 172.28.52.180\n172.28.52.180:6379> GET key1 value1\n(error) MOVED 9189 172.28.53.213:6379\n`\n```\nThere are no shards and as I see it:\n\nAll slots (0-16383) are owned by master\nAll of the slaves are connected to master\nEverything is in sync\n\nSo I don't understand why I would be redirected to master. Shouldn't the slave have a copy?\nI am aware that I can connect with `-c` so that redirects are followed automatically, but the whole point of replication should be that redirects aren't necessary!",
      "solution": "Normally slave nodes will redirect clients to the authoritative master for the hash slot involved in a given command, however clients can use slaves in order to scale reads using the READONLY command.\n\nYou can read about it here.\nIf you want to read from a replica you have to execute READONLY command on it. And to revert to old behaviour there is a command READWRITE.",
      "question_score": 2,
      "answer_score": 1,
      "created_at": "2021-01-16T16:55:37",
      "url": "https://stackoverflow.com/questions/65751542/data-is-not-replicated-from-redis-master-to-slaves"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis-cluster",
      "question_id": 68618742,
      "title": "Execute Redis command with server timeout",
      "problem": "Is there a way to execute a command in Redis, where you supply a server-side timeout?\nFor example, I want to execute a command that can take more than 3 sec.\nIn that case, I would want that the Redis server will stop the execution of the command after 3 sec (with no correlation to the client timeout).",
      "solution": "NO.\nYou can do if and only if the command is made that way, e.g. BLPOP command.",
      "question_score": 2,
      "answer_score": 1,
      "created_at": "2021-08-02T10:45:32",
      "url": "https://stackoverflow.com/questions/68618742/execute-redis-command-with-server-timeout"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis-cluster",
      "question_id": 77737062,
      "title": "AttributeError: module &#39;redis&#39; has no attribute &#39;Redis&#39;",
      "problem": "Problem\nInstalled `redis` module and I cannot establish the client according documentation.\nThis is a Redis client in a Redis cluster hosted on Docker container.\nFollowing documentation, as you can see:\n```\n`>>> import redis\n>>> r = redis.Redis(host='localhost', port=6379, db=0)\n>>> r.set('foo', 'bar')\nTrue\n>>> r.get('foo')\nb'bar'\n`\n```\nError\n```\n`backend-web-1        |     from redis.conversation_memory import redis_client, initiate_user_memory\nbackend-web-1        |   File \"/app/redis/conversation_memory.py\", line 5, in \nbackend-web-1        |     redis_client = redis.Redis(host='localhost', port=6379, db=0, decode_responses=True)\nbackend-web-1        |                    ^^^^^^^^^^^\nbackend-web-1        | AttributeError: module 'redis' has no attribute 'Redis'\n`\n```\nQuestion\nWhat's wrong with my implementation?",
      "solution": "This was a rookie move.\nNever name a folder after an official Python module\nI wrongly named it after my `redis` module, which I am inherently using.\nLesson\nName your folder uniquely, if they are to be used as modules.",
      "question_score": 1,
      "answer_score": 6,
      "created_at": "2023-12-30T19:50:40",
      "url": "https://stackoverflow.com/questions/77737062/attributeerror-module-redis-has-no-attribute-redis"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis-cluster",
      "question_id": 79596349,
      "title": "Infinite loop while using redis scan command to delete patterns in golang",
      "problem": "I am using scan command in golang to get redis keys by a provided pattern. I am using redis cluster, and therefore in order to avoid missing keys I use ForEachMaster. This is the code I use:\n```\n`func deleteCacheKeys(ctx context.Context, pattern string, count int64, retries int) error {\n    redisClient := redis.NewClusterClient(&redis.ClusterOptions{\n        Addrs: []string{redisCluster},\n    })\n    if err := redisClient.Ping(ctx).Err(); err != nil {\n        return err\n    }\n\n    var cursor uint64\n    err = redisClient.ForEachMaster(ctx, func(ctx context.Context, nodeClient *redis.Client) error {\n        for {\n            keys, cursor := nodeClient.Scan(ctx, cursor, pattern, count).Val()\n            if len(keys) > 0 {\n                cmd := nodeClient.Del(ctx, keys...)\n                if cmd.Err() != nil {\n                    return cmd.Err()\n                }\n            }\n            if cursor == 0 {\n                break\n            }\n        }\n        return nil\n    })\n\n    return err\n}\n`\n```\nIn this function the tricky part is the count that is used in each node client scan command. When I set it to 1000000, everything works fine. But when I use something lower like 100 or even 100000, this code stucks in a infinite loop (the longest I waited was 30 minutes). When using 1000000 as count it usually takes seconds to delete the pattern.\nBut we were fine using 1 million until our redis dataset became so large that the infinite loop happend with this count as well. I am currently searching for a safe way to delete these pattern without worrying about this count. And I really want to know the reason to why this even happens.\nI have tried setting it to -1 but it still stucks. I also tried using Unlink instead of Del command but the result is the same.",
      "solution": "You got tripped by the effects of the `keys, cursor := ...` statement inside your loop.\nThe `:=` operator will actually define all variables to its left as new variables, scoped to the body of the loop only.\nSo your statement:\n```\n`    var cursor int64 = 0\n    for {\n        keys, cursor := nodeClient.Scan(ctx, cursor, pattern, count).Val()\n        ...\n`\n```\nis actually equivalent to:\n```\n`    var cursor1 int64 = 0\n    for {\n        keys, cursor2 := nodeClient.Scan(ctx, cursor1, pattern, count).Val()\n        ...\n`\n```\nEach of your calls to `.Scan()` will actually be made with `CURSOR 0`, and `cursor2` may never reach `0`.\nHere is an illustration of a similar issue: https://go.dev/play/p/77VFOW4ldCE\n\nOne of the ways to allow assigning to variables in different scopes is:\n\nuse `=` instead of `:=`,\nmake an explicit declaration for your `keys` variable\n\n```\n`    var cursor int64 = 0\n    for {\n        var keys []string\n        // in this statement, no new variable is defined,\n        // so 'cursor' refers to the variable 3 lines up\n        keys, cursor = nodeClient.Scan(ctx, cursor, pattern, count).Val()\n`\n```",
      "question_score": 1,
      "answer_score": 4,
      "created_at": "2025-04-28T13:15:05",
      "url": "https://stackoverflow.com/questions/79596349/infinite-loop-while-using-redis-scan-command-to-delete-patterns-in-golang"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis-cluster",
      "question_id": 75975134,
      "title": "PHP php_network_getaddresses on CI\\CD composer install step",
      "problem": "I have a Laravel application. Because of pressure on our backend caching, we switched from single instance Redis to Redis cluster.\nProblems started here, on my device, everything works fine but, when we push on staging or production environment the build process failed on docker build exactly on RUN composer install.\n`RUN composer install`\nwe are using this docker file and another docker file as base image\n```\n`FROM registry.shenoto.net/devops/base-images/php-base-8.1:latest\n\nCOPY .docker/php.ini /usr/local/etc/php/conf.d/php.ini\nCOPY .docker/php-fpm.conf /usr/local/etc/php-fpm.d/www.conf\nCOPY .docker/nginx.conf /etc/nginx/http.d/default.conf\nCOPY .docker/supervisor.conf /etc/supervisor/conf.d/supervisor.conf\nCOPY  --chown=www-data . /var/www/html/\nRUN composer install\n`\n```\nthis is the error on CI\\CD\n```\n`#12 9.927   - Installing spatie/laravel-ignition (1.6.4): Extracting archive\n#12 9.927   - Installing php-amqplib/php-amqplib (v3.5.1): Extracting archive\n#12 9.928   - Installing vladimir-yuldashev/laravel-queue-rabbitmq (v13.1.0): Extracting archive\n#12 9.961    0/132 [>---------------------------]   0%\n#12 10.11   19/132 [====>-----------------------]  14%\n#12 10.23   29/132 [======>---------------------]  21%\n#12 10.36   41/132 [========>-------------------]  31%\n#12 10.58   60/132 [============>---------------]  45%\n#12 10.71   69/132 [==============>-------------]  52%\n#12 10.92   81/132 [=================>----------]  61%\n#12 11.15   95/132 [====================>-------]  71%\n#12 11.50  114/132 [========================>---]  86%\n#12 11.67  124/132 [==========================>-]  93%\n#12 11.81  132/132 [============================] 100%\n#12 12.37 Generating optimized autoload files\n#12 19.30 > Illuminate\\Foundation\\ComposerScripts::postAutoloadDump\n#12 19.33 > @php artisan package:discover --ansi\n#12 20.30 \n#12 20.30    RedisClusterException \n#12 20.30 \n#12 20.30   Couldn't map cluster keyspace using any provided seed\n#12 20.30 \n#12 20.30   at [internal]:0\n#12 20.30       1\u2595\n#12 20.31 \n#12 20.31   1   Modules/Core/Service/RedisConnector.php:17\n#12 20.31       ErrorException::(\"RedisCluster::__construct(): php_network_getaddresses: getaddrinfo for redis-node-1 failed: Name does not resolve\")\n#12 20.31 \n#12 20.31   2   Modules/Core/Service/RedisConnector.php:17\n#12 20.31       RedisCluster::__construct(\"bitnami\")\n#12 20.32 Script @php artisan package:discover --ansi handling the post-autoload-dump event returned with error code 1\n#12 ERROR: process \"/bin/sh -c composer install\" did not complete successfully: exit code: 1\n`\n```\nand this is the RedisConnector Class\n```\n`class RedisConnector\n{\n    public static function create()\n    {\n        return new RedisCluster(NULL, array(\n            \"redis-node-5:6379\",\n            \"redis-node-4:6379\",\n            \"redis-node-3:6379\",\n            \"redis-node-2:6379\",\n            \"redis-node-1:6379\",\n        ), 1.5, 1.5, true, \"bitnami\");\n    }\n}\n\n`\n```\nI tried to add redis-node-5,redis-node-4,... to /etc/host on the build environment it worked but I don't think it is a good solution for this problem when we switch the build server or add more nodes reconfiguring them takes time. I know its possible to automate it but I'm looking for an easier solution",
      "solution": "If you don't consider faking the hostname entries adequate any longer (IMHO the build should not require that to run), you can split the host configuration from the build configuration (application revision) by not running composer scripts (and plugins) during the build.\n```\n`RUN composer --no-interaction install --no-plugins --no-scripts\n`\n```\nThis effectively prevents triggering the post-autoload-dump event which causes the premature run of\n```\n`#12 19.30 > Illuminate\\Foundation\\ComposerScripts::postAutoloadDump\n#12 19.33 > @php artisan package:discover --ansi\n`\n```\nAnd this is also the recommended way on how to run `composer install` during CI builds for creating the artifact of the application revision.\nThis is no silver-bullet thought, as you still need to run the configuration steps prior deploying. It is just that your artifact (here docker image) remains generic and you can choose the system where you deploy it later (local, staging, production; one, three or five redis nodes etc.).\nConsult the deployment and configuration guides of the Laravel version and modules you have in use and bind the appropriate commands in the deployment stage / let your deployment agent handle them. The docker image has not yet seen them.\nYou can already prepare the split by doing an explicit autoload dump, but with scripts available:\n```\n`RUN composer --no-interaction install --no-plugins --no-scripts\nRUN composer dump-autoload\n`\n```\nThe `composer dump-autoload` only exists to trigger the configured scripts, so it retains the earlier behaviour. The autoloader has been already created with the previous `composer install` command.\nYou can then concentrate on getting rid of it (just apply the correct configuration without errors -or- configure later). For RedisConnector it seems to me that it depends on the actual configuration and needs some form of templating. Perhaps `php artisan package:discover` does that, but it might be a side-effect or race-condition only, I have no detailed insight into such a Laravel project and then of yours.\nAfterwards consider optimizing the autoloader during the `composer install` within the Dockerfile. It can also depend on host configuration so could be part of the deployment, not the build, and another good exercise.\n/Edit:\nThen you can consider how multi-stage Docker builds (test separately) can help you to manage the different stages (revision build / runtime environment specific configuration). There are no hard rules here, you always have an implicit default configuration in your application, it is more where you decide to break it apart for the first time. The driver is the configuration of the application itself. Therefore fix the build error first (close the build gap), then consider which pipelining is more fitting to the benefits of your CI/CD.\nE.g. if the production image does not need composer (IMHO a production application should not depend on composer) don't ship the image with the composer binary. (Yes, sometimes it can help to throw in some stones to realize how little the current workflow did scale, honesty is just a branch away).\nP.S.: You can deploy PHP applications faster if you don't cage them into containers. You can still run them in containers thought. CI/CD allows you to have both. Always stay dynamic in mind, PHP is a dynamic language, don't fight it, just let it run.",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2023-04-10T09:30:41",
      "url": "https://stackoverflow.com/questions/75975134/php-php-network-getaddresses-on-ci-cd-composer-install-step"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis-cluster",
      "question_id": 75388765,
      "title": "How can I pass variables to command module&#39;s argv parameter in Ansible?",
      "problem": "I'm trying to write an Ansible role that configures Redis cluster.\nThe shell command that creates the cluster is:\n`$ /usr/bin/redis-cli --user admin --pass mypass --cluster create 10.226.2.194:6379 10.226.2.196:6379 10.226.2.195:6379 --cluster-replicas 1 --cluster-yes\n`\nI pass username, password, IP addresses of Redis servers and number of replicas as `extra-vars` in shell script\n`#!/bin/sh\nansible-playbook /etc/ansible/playbook-redis.yml -vv \\\n--extra-vars='redis_admin_user=admin redis_admin_password=mypass' \\\n--extra-vars='redis_cluster_members=\"10.226.2.194:6379 10.226.2.196:6379 10.226.2.195:6379\" redis_cluster_replicas=1'\n`\nAfter reading the documentation for the command module I wrote the following task:\n`    - name: Create Redis cluster\n      ansible.builtin.command:\n        argv:\n          - /usr/bin/redis-cli\n          - \"--user {{ redis_admin_user }}\"\n          - \"--pass {{ redis_admin_password }}\"\n          - \"--cluster create {{ redis_cluster_members }}\"\n          - \"--cluster-replicas {{ redis_cluster_replicas }}\"\n          - --cluster-yes\n`\nBut when I run it I get the following error:\n`\"Unrecognized option or bad number of args for: '--user admin'\"\n`\nwhich seems to be error of redis-cli that it doesn't recognize the argument.\nI was able to get around the username and password by changing the task as following:\n`    - name: Create Redis cluster\n      ansible.builtin.command:\n        argv:\n          - /usr/bin/redis-cli\n          - --user \n          - \"{{ redis_admin_user }}\"\n          - --pass\n          - \"{{ redis_admin_password }}\"\n          - --cluster create\n          - \"{{ redis_cluster_members }}\"\n          - --cluster-replicas\n          - \"{{ redis_cluster_replicas }}\"\n          - --cluster-yes\n`\nThis is not exactly nice looking, but at least it passes the username and the password, because the following task works:\n`    - name: Get server info\n      ansible.builtin.command:\n        argv:\n          - /usr/bin/redis-cli\n          - --user\n          - \"{{ redis_admin_user }}\"\n          - --pass\n          - \"{{ redis_admin_password }}\"\n          - info\n`\nBut now it can't get past the `--cluster create` argument:\n`\"Unrecognized option or bad number of args for: '--cluster create'\"\n`\nI also tried `- --cluster create \"{{ redis_cluster_members }}\"` and\n`          - --cluster\n          - create \n          - \"{{ redis_cluster_members }}\"\n`\nto no avail.\nWhat is the correct syntax to run this command?\nP.S.\nThe following task works correctly:\n`    - name: Get Redis version\n      ansible.builtin.command:\n        argv:\n          - /usr/bin/redis-cli\n          - --version\n`\nand returns the version.\nP.P.S\nThe following also works\n`    - name: Create Redis cluster\n      ansible.builtin.command:\n        cmd: \"/usr/bin/redis-cli --user {{ redis_admin_user }} --pass {{ redis_admin_password }} --cluster create {{ redis_cluster_members }} --cluster-replicas {{ redis_cluster_replicas }} --cluster-yes\"\n`",
      "solution": "Got help from Ansible devs at GitHub. The reason for this behavior is how command module and redis-cli utility work with arguments.\n`argv` adds delimiters when passing the command to the command line. So this:\n`    - name: Create Redis cluster\n      ansible.builtin.command:\n        argv:\n          - /usr/bin/redis-cli\n          - \"--user {{ redis_admin_user }}\"\n          - \"--pass {{ redis_admin_password }}\"\n          - \"--cluster create {{ redis_cluster_members }}\"\n          - \"--cluster-replicas {{ redis_cluster_replicas }}\"\n          - --cluster-yes\n`\nturns to (mind the single quotation marks)\n`/usr/bin/redis-cli '--user admin' '--pass mypass' '--cluster create 10.226.2.194:6379 10.226.2.196:6379 10.226.2.195:6379' '--cluster-replicas 1' '--cluster-yes'\n`\nIf you enter the above command directly you'll get the same error when running the task - `\"Unrecognized option or bad number of args for: '--user admin'\"`\nThis is because how redis-cli delimits arguments. From redis-cli documentation:\n\nWhen `redis-cli` parses a command, whitespace characters automatically delimit the arguments.\n\nSo, the correct command should be (note the single quotation marks)\n`/usr/bin/redis-cli '--user' 'admin' '--pass' 'mypass' '--cluster' 'create' '10.226.2.194:6379' '10.226.2.196:6379' '10.226.2.195:6379' '--cluster-replicas' '1' '--cluster-yes'\n`\nBut for the Ansible task there is a problem - `redis_cluster_members` variable.\nThe solution suggested to me at GitHub:\n`    - name: Create Redis cluster\n      ansible.builtin.command:\n        argv: \"{{ redis_argv | flatten }}\"\n      vars:\n        redis_argv:\n          - /usr/bin/redis-cli\n          - --user\n          - \"{{ redis_admin_user }}\"\n          - --pass\n          - \"{{ redis_admin_password }}\"\n          - --cluster\n          - create\n          - \"{{ redis_cluster_members | split }}\"\n          - --cluster-replicas\n          - \"{{ redis_cluster_replicas }}\"\n          - --cluster-yes\n`\nIt works, but I decided to use a workaround.\nIn `vars/main.yml` I defined a variable\n`redis_create_cluster_command: >\n  /usr/bin/redis-cli --user {{ redis_admin_user | quote }} --pass {{ redis_admin_password | quote }}\n  --cluster create {{ redis_cluster_members }}\n  --cluster-replicas {{ redis_cluster_replicas | quote }} --cluster-yes\n`\nFor the explanation about `>` please read this article, particularly the second part.\nThe `quote` filter is for, as suggested in command module's documentation, \"to avoid injection issues\". Note that it's missing from `redis_cluster_members` line because it too breaks the arguments.\nThe task itself:\n`        - name: Create Redis cluster\n          ansible.builtin.command:\n            cmd: \"{{ redis_create_cluster_command }}\"\n            creates: \"{{ redis_etc }}/redis_cluster.created\"\n          no_log: true\n`",
      "question_score": 1,
      "answer_score": 1,
      "created_at": "2023-02-08T17:23:26",
      "url": "https://stackoverflow.com/questions/75388765/how-can-i-pass-variables-to-command-modules-argv-parameter-in-ansible"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis-cluster",
      "question_id": 74610779,
      "title": "How to generate a large amount of redis data of a specific size to a redis cluster",
      "problem": "I need to generate `millions of redis data` with a value size of `1kb` to a redis cluster, assuming that only the value type is string. I learned about two options, the first one is to use `debug populate` to generate a specific amount of data, but it does not set the `value size`.\n`127.0.0.1:6379> DEBUG POPULATE 1000000\nOK\n`\nThe second one is to use shell to call `redis-cli` and I don't know how to generate 1kb data\n`for i in `seq 1000000`; \ndo \n    redis-cli SET key$i val$i ; \ndone\n`\nI am newbie on this. How do I meet the demand? I really appreciate any help with this.\n\nTry the solution based on Mark Setchell.\n`#!/bin/bash\n\n# Generate around 32kB (+ around 33% base64 overhead) of random characters\nstuff=$(head -c 32000 /dev/urandom | base64)\n\n# Set 100,000 keys to 1kB strings, e.g. SET key32 A87H34..PHNQZ\nfor ((i=0;i\nThe following error occurs using the above code\n`sh fake_data_test.sh \nAll data transferred. Waiting for the last reply...\nMOVED 13252 172.20.0.33:6379\nMOVED 9189 172.20.0.32:6379\nERR syntax error\nERR syntax error\nMOVED 13120 172.20.0.33:6379\nMOVED 9057 172.20.0.32:6379\nERR syntax error\nERR syntax error\n...\nERR syntax error\nLast reply received from server.\nerrors: 100, replies: 100\n`\nThen I thought whether it was a value formatting issue, so I put it in double quotes `echo SET key$i \"${stuff:RANDOM:1024}\"`\n`sh fake_data_test.sh \nAll data transferred. Waiting for the last reply...\nMOVED 13252 172.20.0.33:6379\nERR unknown command `kpshETtdvDBpL1BYimJl3FkpuJMom/heyj02qJwUGUCQvSZODHXHwNGodfVyIR6sWSv8agjlGMtl`, with args beginning with: \n...\nERR unknown command `UmBAaiwqgB25mSDhsK7qrveXhJV0cJCBRaz`, with args beginning with: \nMOVED 9189 172.20.0.32:6379\nERR unknown command ERR unknown command `gRolxGVLUVbnU5I/ykaXPCA+0Nev`, with args beginning with: \nLast reply received from server.\nerrors: 1397, replies: 1428\n`\n`for ((i=0;i\nI don't know if I'm using pipe in the wrong way\n\nNote: OS is centos7. redis cluster creation via docker-compose. images is redis:4.0.11-alpine",
      "solution": "Updated Answer\nIf you are doing this in order to just generate test data, there's another much faster way. You could:\n\nflush your Redis, i.e. `FLUSHALL`,\npopulate Redis with the data as per my original answer\nmake a backup with `SAVE`\nget the config directory with `CONFIG GET DIR`\nstop Redis\ntyrn off *\"autoupdate\"\nmove the backup file in the config dir to replace the current file\nrestart Redis\n\nSo, essentially, you empty Redis and set it up how you want it (per my original answer) and back it up. Then, before each test, just replace the main database with the backup file and restart.\nOriginal Answer\nThere are probably better ways, but (before my morning coffee) here's a method...\nFirst, generate 40kB of random text near the start of your script:\n```\n`stuff=$(head -c 40000 /dev/urandom | base64)\n`\n```\nNow, inside your loop, go to a random offset of 0..32767 in the text and take the following 1024 bytes:\n```\n`val=${stuff:RANDOM:1024}\n`\n```\nIn case you wonder, I am trying to avoid expensive creation of processes inside your big loop. So the line `val=${...}` is a `bash` \"internal\" that doesn't create a new process.\nNote that if you take a million random samples starting at offsets 0..32768, there will inevitably be repetitions. You could reduce this by taking multiple smaller chunks from different offsets and appending them together. Or perhaps, generate absolutely unique values by prefixing each value with a sequential number and making the strings slightly over 1024 bytes.\n\nAside, I think you'd be better pipelining some of this, or using Python or some bulk-loading to speed it up.\nThis code does 100,000 insertions of 1024 byte strings in around 49 seconds for example:\n```\n`#!/bin/bash\n\n# Generate around 32kB (+ around 33% base64 overhead) of random characters\nstuff=$(head -c 32000 /dev/urandom | base64)\n\n# Set 100,000 keys to 1kB strings, e.g. SET key32 A87H34..PHNQZ\nfor ((i=0;iIf you want to ensure the values are unique, and don't mind making each value just over 1024 bytes, replace the line in the loop with:\n```\n`echo SET key$i \"${i}-${stuff:RANDOM:1024}\"\n`\n```\nIf you require exactly 1024 unique bytes, you can use the following at a 10% time penalty:\n```\n`#\u00a0Generate value: 8 digits of sequence number, a dash and 1015 random characters\nprintf -v val \"%08d-%s\" $i ${stuff:RANDOM:1015}\necho SET key$i $val\n`\n```",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2022-11-29T08:59:53",
      "url": "https://stackoverflow.com/questions/74610779/how-to-generate-a-large-amount-of-redis-data-of-a-specific-size-to-a-redis-clust"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis-cluster",
      "question_id": 72967230,
      "title": "What Redis commands don&#39;t work in cluster mode?",
      "problem": "UPDATE - 2023 - Verx Redis now supports scan but there's a bug see https://github.com/vert-x3/vertx-redis-client/issues/345\nMy team has been using the Vertx Redis client for some time and using the keys command with a Redis cluster and I was asked to refactor to use the scan command instead since \"keys\" isn't meant for production.\nWhen I went to use scan I got an error from the client saying \"scan not supported - use non cluster client on the right node.\" So it appears that the Vertx Redis client supports cluster-wide \"keys\" but not \"scan.\" I learned that I'm supposed to create 1 client per node if I want to do a cluster-wide \"scan\" e.g. something like this but that seems like an overfly complex and poorly performant choice (or I can use \"hash tags\" which won't work for my use case).\nI'm trying to figure out which commands are supported cluster-wide and which aren't (so we don't make another mistake like this ) and the only hint I see is in the Redis cluster spec:\n\nRedis Cluster implements all the single key commands available in the non-distributed version of Redis.\n\nMy question is - what is a single key command?\nOr another explanation I saw here was the following:\n\nThe only different between distributed and non-distributed Redis clients is that in the distributed case MOVED and ASKS will be \"followed.\"\n\nI'm also not clear on how a MOVED or ASK is followed - I assume it just means that the client reconnect to the correct node and tries again? That seems horribly non-performant as discussed here.\nIt's also odd that the Vertx client documentation doesn't mention this or the more popular Jedis client.\nAm I missing some key documentation that explains all this?",
      "solution": "My question is - what is a single key command?\n\nIt's a command, which has a single key a as parameter. For example, `SET key value`, only sets a single key.\nIn fact, you missed the second sentence: Commands performing complex multi-key operations ... are implemented for cases where all of the keys ... hash to the same slot. So if `k1`, `k2` and `k3` are located on the same hash slot, you can send `MGET k1 k2 k3` command to Redis Cluster.\n\nI assume it just means that the client reconnect to the correct node and tries again?\n\nYES.\n\nThat seems horribly non-performant as discussed here.\n\nA decent Redis client should cache the slot-node mapping. So if the slot-node mapping has not changed, clients can calculate the right slot/node based on key, and send the command to the right node. So that it can avoid the redirection.",
      "question_score": 1,
      "answer_score": 1,
      "created_at": "2022-07-13T15:33:53",
      "url": "https://stackoverflow.com/questions/72967230/what-redis-commands-dont-work-in-cluster-mode"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis-cluster",
      "question_id": 71404596,
      "title": "Using endpoints of AWS ElastiCache for Redis",
      "problem": "I am using `AWS ElastiCache` for `Redis` as the caching solution for my `spring-boot` application. I am using `spring-boot-starter-data-redis` and `jedis` client to connect with my cache.\nImagine that I am having my cache in cluster-mode-enabled and 3 shards with 2 nodes in each. I agree then the best way of doing it is using the configuration-endpoint. Alternatively, I can list all the endpoints of all nodes and let the job done.\nHowever, even if I use a single node's endpoint from one of the shards, my caching solution works. That doesn't looks right to me. I feel even if it works, that might case problems in the cluster in long run. When there are all together 6 nodes partitioned into 3 shards but only using one node's endpoint. I have following questions.\nIs using one node's endpoint create an imbalance in the cluster?\nor\nIs that handled automatically by the `AWS ElastiCache` for `Redis`?\nIf I use only one node's endpoint does that mean the other nodes will never being used?\nThank you!",
      "solution": "To answer your questions;\n\nIs using one node's endpoint create an imbalance in the cluster?\nNO\n\nIs that handled automatically by the AWS ElastiCache for Redis?\nSomewhat\n\nif I use only one node's endpoint does that mean the other nodes will never being used?\nNo. All nodes are being used.\n\nThis is how Cluster Mode Enabled works. In your case, you have 3 shards meaning all your slots (where key-value data is stored) are divided into 3 sub-clusters ie. shards.\nThis was explained in this answer as well - https://stackoverflow.com/a/72058580/6024431\nSo, essentially, your nodes are smart enough to re-direct your requests to the nodes that has the key-slot where your data needs to be stored. So, no imbalances. Redis handles the redirection for you.\nNow, while using Node endpoints, you're going to be facing other problems.\nElasticache is running on cloud (which is essentially AWS Hardware). All hardware faces issues. You have 3 primaries (1p, 2p, 3p) and 3 (1r, 2r, 3r) replicas.\nSo, if a primary goes down due to hardware issue (lets say 1p), the replica will get promoted to become the new Primary for the cluster (1r).\nNow the problem would be, your application is connected directly to 1p which has now been demoted to replica. So, all the WRITE operations will fail.\nAnd you will have to change the application code manually whenever this happens.\nAlternatively, if you were using configurational endpoint (or other cluster level endpoints) instead of node-endpoints, this issue would only be a blip to your application at most, perhaps for 1-2 seconds.\nCheers!",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2022-03-09T06:12:02",
      "url": "https://stackoverflow.com/questions/71404596/using-endpoints-of-aws-elasticache-for-redis"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis-cluster",
      "question_id": 70884685,
      "title": "Redis - Count distinct problem (without hyper log log)",
      "problem": "I should solve a count-distinct problem in Redis without the use of HyperLogLog (because of the 0.81% of known error).\nI got different requests with a list of objects [O1, O2, ... On] for a specific Key A.\nFor each list of objects received, Redis should memorize the Objects not still saved and return the number of new objects saved.\nFor Example:\n\nRequest 1 : Key: A - Objects: [O1, O2, O3] -> Response 1: Number of new objects : 3\nRequest 2 : Key: A - Objects: [O1, O2, O4] -> Response 2: Number of new objects : 1\nRequest 3 : Key: A - Objects: [O1, O2, O4] -> Response 3: Number of new objects : 0\n\nI have tried to solve this problem with the Hyper Log Log and it's working perfectly but with a growing dataset of objects, the number of new objects saved is not so accurate.\nWith the sets and the hashmap, the memory is growing too much.\nI have read some stuff about Bitmaps but is not too clear. Do you have any reference to projects that are already facing this problem?\nThanks in advance",
      "solution": "You might want to consider using a bloom filter.  This is available as a module https://redis.com/redis-best-practices/bloom-filter-pattern/.\nBloom filters allow quick tests for membership with 0 false negatives and a very low false positive, provided you know in advance what the maximum number of elements are.   You would need to write code of the sort:\n```\n`result = bf.exists(key, item)\nif result == 0:\n    bf.add(key, item)\n    bf.inc(key_count)\n`\n```",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2022-01-27T20:34:58",
      "url": "https://stackoverflow.com/questions/70884685/redis-count-distinct-problem-without-hyper-log-log"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis-cluster",
      "question_id": 66738931,
      "title": "How to set password for redis-server",
      "problem": "I have a 3-instance high availability redis deployed. On each server I have redis and sentinel installed. I am trying to set a password\nso that it requests it at the moment of entering with the command \"redis-cli\".\nI am modifying the value of the \"requirepass\" parameter of the \"redis.conf\" file.\n`requirepass password123`\nAlso inside the redis terminal, I am setting the password with the following commands\n```\n`config set requirepass password123\nauth password123\n`\n```\nWhen I connect with the following command\n```\n`redis-cli --tls --cert //redis.crt --key //redis.key --cacert //ca.crt -a password123\n`\n```\nIt works fine, my problem is when I restart the redis service, for some reason the password settings are not kept and I get the following message\n```\n`Warning: AUTH failed\n`\n```\nI do not know what configuration I need to do so that the change is maintained after restarting the redis service.\nThe version of redis that I have installed is \"Redis server v=6.0.6\"",
      "solution": "Check your ACL configuration,Your `requirepass` configuration will be ignored with ACL operation. I get follow infomation from redis.conf example file.\n\nIMPORTANT NOTE: starting with Redis 6 \"requirepass\" is just a compatibility\nlayer on top of the new ACL system. The option effect will be just setting\nthe password for the default user. Clients will still authenticate using\nAUTH  as usually, or more explicitly with AUTH default \nif they follow the new protocol: both will work.\nThe requirepass is not compatable with aclfile option and the ACL LOAD\ncommand, these will cause requirepass to be ignored.",
      "question_score": 1,
      "answer_score": 1,
      "created_at": "2021-03-22T01:22:28",
      "url": "https://stackoverflow.com/questions/66738931/how-to-set-password-for-redis-server"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis-cluster",
      "question_id": 72760493,
      "title": "Concept about rate limiter constructed by redis cluster",
      "problem": "I Have a concept problem about rate limiter constructed by redis/cluster mode.\nMy problem is if I choose token bucket algorithm for rate limiter and use just one redis server, it works well. However if the traffic of redis is too much now, I would like to use redis/cluster mode for horizontal-scaling, but as I know, the redis/cluster mode is eventually consistent, so I think the value of each redis node will not be strongly consistent at the same time, the rate limiter will sometimes have some wrong calculation right? If so, how do I fix the issue, thank you very much.",
      "solution": "Redis Cluster mode is eventually consistent from master node to replica nodes. Any change is immediate within the master node itself. So if you are reading only from master nodes, consistency would not be an issue.",
      "question_score": 1,
      "answer_score": 1,
      "created_at": "2022-06-26T11:48:53",
      "url": "https://stackoverflow.com/questions/72760493/concept-about-rate-limiter-constructed-by-redis-cluster"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis-cluster",
      "question_id": 69754970,
      "title": "Redis mass insertion CSV - data is not inserting to Redis",
      "problem": "I am trying to insert the CSV data into Redis using the below command\n\nColumn1\nColumn2\n\nlong_ago/speech:\nwell-done speech\n\nlong-ago/debate:\nwell-done debate\n\nlong ago/work:\nwell-done work\n\n```\n`awk -F ',' 'FNR > 1 && $1 && $2 {printf(\"SET Topic:%s %s\\n\",$1,$2)}' data_topics.csv | redis-cli --pipe\n`\n```\nThe expectation is when I do\n```\n`GET  \"Topic:long_ago/speech:\"\n`\n```\nshould print\n```\n`>\"well-done speech\"\n`\n```\nBut I am not getting any output when I tried inserting 1000 rows in CSV. So have tried with the above 3 rows in CSV and getting the below error\n```\n`[admin~]$ awk -F ',' 'FNR > 1 && $1 && $2 {printf(\"SET Topic:%s %s\\n\",$1,$2)}' data_topics.csv | redis-cli --pipe\nAll data transferred. Waiting for the last reply...\nERR syntax error\nERR syntax error\nERR syntax error\nLast reply received from the server.\nerrors: 3, Replies: 3\n`\n```\nSo I have tried adding double quotes in the 2nd column, now my CSV looks something like the below\n\nColumn1\nColumn2\n\nlong_ago/speech:\n\"well-done speech\"\n\nlong-ago/debate:\n\"well-done debate\"\n\nlong ago/work:\n\"well-done work\"\n\nand this is the error I am getting now -\n```\n`[admin~]$ awk -F ',' 'FNR > 1 && $1 && $2 {printf(\"SET Topic:%s %s\\n\",$1,$2)}' data_topics.csv | redis-cli --pipe\nAll data transferred. Waiting for the last reply...\nERR Protocol error: unbalanced quotes in request\n`\n```\nPlease help me to insert my CSV data into Redis.",
      "solution": "Using a CSV called `data.csv` that contains this:\n```\n`long_ago/speech:,well-done speech\nlong-ago/debate:,well-done debate\nlong-ago/work:,well-done work\n`\n```\nYou could use:\n```\n`awk -F, '{printf(\"SET \\\"Topic:%s\\\" \\\"%s\\\"\\n\",$1,$2)}' data.csv | redis-cli --pipe\n`\n```\nThen you could do:\n```\n`redis-cli GET \"Topic:long_ago/speech:\"\n\"well-done speech\"\n`\n```",
      "question_score": 1,
      "answer_score": 1,
      "created_at": "2021-10-28T15:26:36",
      "url": "https://stackoverflow.com/questions/69754970/redis-mass-insertion-csv-data-is-not-inserting-to-redis"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis-cluster",
      "question_id": 68585693,
      "title": "how to Redis-cli command to mass insert CSV data into Redis persistence system?",
      "problem": "My CSV data looks something like this ->\n```\n`    Header1   Header2\n    Key100    Value100\n    Key200    Value200\n`\n```\nIt has only two columns with Header as first row. The columns are keys and respective values.\nNow, my requirement is -\n\nI want to store key & value as String data type\nNeed to exclude the Header(the first row) while storing\nNeed to exclude null or \" \" keys\nit is a ',' separated data\nThe word \"Data:\" should be appended to each key while inserting.\n\nFor example - for the above provided csv data, redis should store as -\n```\n`Data:Key100\nData:Key200\n`\n```\nResult:\nWhen I do `get Data:Key100` my output should be : `\"Value100\"`\nHave tried with awk command but it is keep on giving error - invalid stream id specified\n```\n`awk -F ',' 'FNR > 1 {print \"XADD myStream \\\"\"($1==\"\" ? \"NA\" : $1)\" column_1 \"($2==\"\" ? \"NA\" : $2)\" column_2 ... \\n\"}' data.csv | redis-cli --pipe\n`\n```\nPlease help me to get the command. Thank you",
      "solution": "I think you mean this, which sets an individual key to a value for each line of your file, but I am not sure why you seem to be trying to use a stream:\n```\n`awk -F ',' 'FNR>1 { if(!$1){$1=\"NA\"}; if(!$2){$2=\"NA\"}; printf(\"SET %s %s\\n\",$1,$2)}'  data.csv | redis-cli --pipe\n`\n```\nFor this file (which has the key missing on line 3 and the value missing on line 5):\n```\n`Header1,Header2\nKey100,Value100\n,fred\nKey200,Value200\nbill,\n`\n```\nThat produces this code:\n```\n`SET Key100 Value100\nSET NA fred\nSET Key200 Value200\nSET bill NA\n`\n```\nWhich then enables you to do this:\n```\n`redis-cli get Key100\n\"Value100\"\n`\n```\nAlso, you say you want to exclude null keys, but your code replaces them with `\"NA\"` and sets them anyway, so you may prefer to ignore them altogether with:\n```\n`awk -F ',' 'FNR > 1 && $1 && $2 {printf(\"SET %s %s\\n\",$1,$2)}'  data.csv \n`\n```\nwhich produces:\n```\n`SET Key100 Value100\nSET Key200 Value200\n`\n```",
      "question_score": 1,
      "answer_score": 1,
      "created_at": "2021-07-30T07:09:26",
      "url": "https://stackoverflow.com/questions/68585693/how-to-redis-cli-command-to-mass-insert-csv-data-into-redis-persistence-system"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis-cluster",
      "question_id": 67686642,
      "title": "RedisCluster MGET with pipeline",
      "problem": "I am trying to perform an MGET operation on my Redis with the pipeline to increase performance.\nI have tried doing MGET in one go as well as as a batch flow\n```\n`from rediscluster import RedisCluster\nru = RedisCluster(startup_nodes=[{\"host\": \"somecache.aws.com\", \"port\": \"7845\"}], \n        decode_responses=True, \n        skip_full_coverage_check=True)\n    \npipe = ru.pipeline()        \n# pipe.mget(keys)\n    \nfor i in range(0, len(keys), batch_size):\n    temp_list = keys[i:i + batch_size]\n    pipe.mget(temp_list)\n\nresp = pipe.execute()\n`\n```\nSo far I am getting the error\n```\n`raise RedisClusterException(\"ERROR: Calling pipelined function {0} is blocked \nwhen running redis in cluster mode...\".format(func.__name__))\nrediscluster.exceptions.RedisClusterException: ERROR: \nCalling pipelined function mget is blocked when running redis in cluster mode...\n`\n```\nWhat I want to know is that\n\nDoes RedisCluster pipelined MGET?\nIf not then is there any other lib that I can use to archive this?",
      "solution": "Turns out we can not use MGET with the pipeline, below is m final solution\n```\n`from rediscluster import RedisCluster\n    \ndef redis_multi_get(rc: RedisCluster, keys: list):\n    pipe = rc.pipeline()\n    [pipe.get(k) for k in keys]\n    return pipe.execute()\n    \nif __name__ == '__main__':\n    rc = RedisCluster(startup_nodes=[{\"host\": host, \"port\": port}], decode_responses=True, skip_full_coverage_check=True)\n    keys = rc.keys(PREFIX + '*')\n    cache_hit = redis_multi_get(rc, keys)\n`\n```",
      "question_score": 1,
      "answer_score": 1,
      "created_at": "2021-05-25T13:02:45",
      "url": "https://stackoverflow.com/questions/67686642/rediscluster-mget-with-pipeline"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis-sentinel",
      "question_id": 71062644,
      "title": "Bitnami Redis Sentinel &amp; Sidekiq on Kubernetes",
      "problem": "So right now we are trying to get a Bitnami Redis Sentinel cluster working, together with our Rails app + Sidekiq.\nWe tried different things, but it's not really clear to us, how we should specify the sentinels for Sidekiq (crutial part is here, that the sentinel nodes are READ ONLY, so we cannot use them for sidekiq, since job states get written).\nSince on Kubernetes there are only 2 services available: \"redis\" and \"redis-headless\" (not sure how they differ?) - how can I specify the sentinels like this:\n`Sidekiq.configure_server do |config|\n  config.redis = {\n    url: \"redis\",\n    sentinels: [\n      { host: \"?\", port: 26379 } # why do we have to specify it here seperately, since we should be able to get a unified answer via a service, or?\n      { host: \"?\", port: 26379 }  \n      { host: \"?\", port: 26379 } \n    }\n  }\nend\n`\nWould be nice if someone can shed some light on this. As far as I understood, the bitnami redis sentiel only returns the IP of the master and the application has to handle the corresponding writes to this master then (https://github.com/bitnami/charts/tree/master/bitnami/redis#master-replicas-with-sentinel) - but I really don't understand on how to do this with sidekiq?",
      "solution": "Difference between a Kubernetes Service and a Headless Service\nLet's get started by clarifying the difference between a Headless Service and a Service.\nA Service allows one to connect to one Pod, while a headless Service returns the list of available IP addresses from all the available pods, allowing to auto-discover.\nA better detailed explanation by Marco Luksa has been published on SO here:\n\nEach connection to the service is forwarded to one randomly selected backing pod. But what if the client needs to connect to all of those pods? What if the backing pods themselves need to each connect to all the other backing pods. Connecting through the service clearly isn\u2019t the way to do this. What is?\nFor a client to connect to all pods, it needs to figure out the the IP of each individual pod. One option is to have the client call the Kubernetes API server and get the list of pods and their IP addresses through an API call, but because you should always strive to keep your apps Kubernetes-agnostic, using the API server isn\u2019t ideal\nLuckily, Kubernetes allows clients to discover pod IPs through DNS lookups. Usually, when you perform a DNS lookup for a service, the DNS server returns a single IP \u2014 the service\u2019s cluster IP. But if you tell Kubernetes you don\u2019t need a cluster IP for your service (you do this by setting the clusterIP field to None in the service specification ), the DNS server will return the pod IPs instead of the single service IP. Instead of returning a single DNS A record, the DNS server will return multiple A records for the service, each pointing to the IP of an individual pod backing the service at that moment. Clients can therefore do a simple DNS A record lookup and get the IPs of all the pods that are part of the service. The client can then use that information to connect to one, many, or all of them.\nSetting the clusterIP field in a service spec to None makes the service headless, as Kubernetes won\u2019t assign it a cluster IP through which clients could connect to the pods backing it.\n\n\"Kubernetes in Action\" by Marco Luksa\nHow to specify the sentinels\nAs the Redis documentation say:\n\nWhen using the Sentinel support you need to specify a list of sentinels to connect to. The list does not need to enumerate all your Sentinel instances, but a few so that if one is down the client will try the next one. The client is able to remember the last Sentinel that was able to reply correctly and will use it for the next requests.\n\nSo the idea is to give what you have, and if you scale up the redis pods, then you don't need to re-configure Sidekiq (or Rails if you're using Redis for caching).\nCombining all together\nNow you just need a way to fetch the IP addresses from the headless service in Ruby, and configure Redis client sentinels.\nFortunately, since Ruby 2.5.0, the Resolv class is available and can do that for you.\n```\n`irb(main):007:0> Resolv.getaddresses \"redis-headless\"\n=> [\"172.16.105.95\", \"172.16.105.194\", \"172.16.9.197\"]\n`\n```\nSo that you could do:\n`Sidekiq.configure_server do |config|\n  config.redis = {\n    # This `host` parameter is used by the Redis gem with the Redis command\n    # `get-master-addr-by-name` (See https://redis.io/topics/sentinel#obtaining-the-address-of-the-current-master)\n    # in order to retrieve the current Redis master IP address.\n    host: \"mymaster\",\n    sentinels: Resolv.getaddresses('redis-headless').map do |address|\n      { host: address, port: 26379 }\n    end\n  }\nend\n`\nThat will create an Array of Hashes with the IP address as `host:` and 26379 as the `port:`.",
      "question_score": 3,
      "answer_score": 3,
      "created_at": "2022-02-10T10:29:36",
      "url": "https://stackoverflow.com/questions/71062644/bitnami-redis-sentinel-sidekiq-on-kubernetes"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis-sentinel",
      "question_id": 70384566,
      "title": "WARNING: Sentinel was not able to save the new configuration on disk!!!: Device or resource busy",
      "problem": "I am trying to setup one master one slave and one sentinel on docker, for that I wrote this docker compose file.\n```\n`version: '3'\nservices:\n  redis-master:\n    container_name: \"redis-master\"\n    image: redis\n    ports:\n      - \"6379:6379\"\n    command: \"redis-server /etc/redis.conf\"\n    volumes:\n      - \"./data/master:/data/\"\n      - \"./master.conf:/etc/redis.conf\"\n\n  redis-slave:\n    container_name: \"redis-slave\"\n    image: redis\n    ports:\n      - \"6380:6379\"\n    command: \"redis-server /etc/redis.conf\"\n    volumes:\n      - \"./data/slave:/data/\"\n      - \"./slave.conf:/etc/redis.conf\"\n    depends_on:\n      - redis-master\n\n  redis-sentinel:\n    container_name: 'redis-sentinel'\n    image: redis\n    ports:\n      - \"26379:26379\"\n    command: >\n      bash -c \"chmod 777 /etc/sentinel.conf\n      && redis-server /etc/sentinel.conf --sentinel\"\n    volumes:\n      - \"./sentinel.conf:/etc/sentinel.conf\"\n    depends_on:\n      - redis-master\n      - redis-slave\n`\n```\nBut when I try to build it using `sudo docker-compose up --build --force` all the services are runnning fine except the redis-sentinel. I got this error in logs\n```\n`redis-sentinel    | 1:X 16 Dec 2021 19:15:21.486 # +sdown master mymaster 172.23.0.2 6379\nredis-sentinel    | 1:X 16 Dec 2021 19:15:21.486 # +odown master mymaster 172.23.0.2 6379 #quorum 1/1\nredis-sentinel    | 1:X 16 Dec 2021 19:15:21.486 # +new-epoch 8\nredis-sentinel    | 1:X 16 Dec 2021 19:15:21.487 # +try-failover master mymaster 172.23.0.2 6379\nredis-sentinel    | 1:X 16 Dec 2021 19:15:22.955 # Could not rename tmp config file (Device or resource busy)\nredis-sentinel    | 1:X 16 Dec 2021 19:15:22.955 # WARNING: Sentinel was not able to save the new configuration on disk!!!: Device or resource busy\n`\n```\nI understand this is some file permission and I have to make sentinel.conf executable but I am not able to think of any possible solutions in docker.",
      "solution": "First of all, the sentinel command should be only like this (we will sort out the permissions later):\n```\n`redis-server /etc/sentinel.conf --sentinel\n`\n```\nIn order to resolve that warning, you have to do the following (for all the redis nodes - master, slave, sentinel):\nIn the folder where the docker-compose.yml resides, create a \"config\" folder, with the following structure:\n```\n`/config\n  /redis-master\n     redis.conf\n  /redis-slave\n     redis.conf\n  /redis-sentinel\n     redis.conf\n`\n```\nInside config/redis-master folder you copy your current master.conf file as redis.conf (for the sake of simplicity).\nInside config/redis-slave you copy your current slave.conf file as redis.conf\nInside config/redis-sentinel you will copy your current sentinel.conf as redis.conf\nThen, execute this command, in order to give full rights to all the content of your \"config\" folder:\n```\n`chmod -R 0777 config/\n`\n```\nNow, change your service definitions in the docker-compose, like this:\n(notice my changes in the \"command\" and \"volumes\" sections)\n```\n`  redis-master:\n    container_name: \"redis-master\"\n    image: redis\n    ports:\n      - \"6379:6379\"\n    command: \"redis-server /etc/redis-config/redis.conf\"\n    volumes:\n      - \"./data/master:/data/\"\n      - \"./config/redis-master:/etc/redis-config\"\n`\n```\n```\n`  redis-slave:\n    container_name: \"redis-slave\"\n    image: redis\n    ports:\n      - \"6380:6379\"\n    command: \"redis-server /etc/redis-config/redis.conf\"\n    volumes:\n      - \"./data/slave:/data/\"\n      - \"./config/redis-slave:/etc/redis-config\"\n    depends_on:\n      - redis-master\n`\n```\n```\n`  redis-sentinel:\n    container_name: 'redis-sentinel'\n    image: redis\n    ports:\n      - \"26379:26379\"\n    command: \"redis-server /etc/redis-config/redis.conf --sentinel\"\n    volumes:\n      - \"./config/redis-sentinel:/etc/redis-config\"\n    depends_on:\n      - redis-master\n      - redis-slave\n`\n```\nConclusions:\n\nIn order for redis to have rights to modify the configurations, you have mount the entire configuration folder, not the file itself (and the folder contents should be writable).\nBack-up all the original master.conf, slave.conf, sentinel.conf files. Be aware that redis will alter your local configuration files (redis.conf), because eventually you will execute failovers, so the slave config will be turned into a master one and the opposite for the master config. It's up to you how you avoid committing the changes to source control. For example, you can keep the originals in a separate folder, and copy them to the mounted folder by a deploy script, before running \"docker-compose up\" command.\nAll the changes that redis will apply to these config files will be appended at the end of the config and will be preceded by this comment:\n\n```\n`# Generated by CONFIG REWRITE\n`\n```\nThis solution is tested by me and it works. I came out with this solution after reading @yossigo 's answer regarding a similar warning:\nhttps://github.com/redis/redis/issues/8172",
      "question_score": 1,
      "answer_score": 7,
      "created_at": "2021-12-16T20:26:59",
      "url": "https://stackoverflow.com/questions/70384566/warning-sentinel-was-not-able-to-save-the-new-configuration-on-disk-device"
    },
    {
      "tech": "redis",
      "source": "stackoverflow",
      "tag": "redis-sentinel",
      "question_id": 70499332,
      "title": "Redis Sentinel doesn&#39;t auto discover new slave instances",
      "problem": "I've deployed the redis helm chart on k8s with Sentinel enabled.\nI've set up the Master-Replicas with Sentinel topology, it means one master and two slaves. Each pod is running both the redis and sentinel container successfully:\n```\n`NAME             READY   STATUS    RESTARTS   AGE     IP             NODE\nmy-redis-pod-0   2/2     Running   0          5d22h   10.244.0.173   node-pool-u\nmy-redis-pod-1   2/2     Running   0          5d22h   10.244.1.96    node-pool-j\nmy-redis-pod-2   2/2     Running   0          3d23h   10.244.1.145   node-pool-e\n`\n```\nNow, I've a python script that connects to redis and discovers the master by passing it the pod's ip.\n```\n`sentinel = Sentinel([('10.244.0.173', 26379),\n                     ('10.244.1.96',26379),\n                     ('10.244.1.145',26379)],\n                  sentinel_kwargs={'password': 'redispswd'})\n\nhost, port = sentinel.discover_master('mymaster')\nredis_client = StrictRedis(\n            host=host,\n            port=port,\n            password='redispswd')\n`\n```\nLet's suposse the master node is on my-redis-pod-0, when I do `kubectl delete pod` to simulate a problem that leads me to loss the pod, Sentinel will promote one of the others slaves to master and kubernetes will give me a new pod with redis and sentinel.\n```\n`NAME             READY   STATUS    RESTARTS   AGE     IP             NODE\nmy-redis-pod-0   2/2     Running   0          3m      10.244.0.27    node-pool-u\nmy-redis-pod-1   2/2     Running   0          5d22h   10.244.1.96    node-pool-j\nmy-redis-pod-2   2/2     Running   0          3d23h   10.244.1.145   node-pool-e\n`\n```\nThe question is, how can I do to tell Sentinel to add this new ip to the list automatically (without code changes)?\nThanks!",
      "solution": "Instead of using IPs, you may use the dns entries for a headless service.\nA headless service is created by explicitly specifying\n```\n`ClusterIP: None\n`\n```\nThen you will be able to use the dns entries as under, where redis-0 will be the master\n```\n`#syntax\npod_name.service_name.namespace.svc.cluster.local\n\n#Example\nredis-0.redis.redis.svc.cluster.local\nredis-1.redis.redis.svc.cluster.local\nredis-2.redis.redis.svc.cluster.local\n`\n```\nReferences:\nWhat is a headless service, what does it do/accomplish, and what are some legitimate use cases for it?\nhttps://www.containiq.com/post/deploy-redis-cluster-on-kubernetes",
      "question_score": 1,
      "answer_score": 1,
      "created_at": "2021-12-27T19:50:13",
      "url": "https://stackoverflow.com/questions/70499332/redis-sentinel-doesnt-auto-discover-new-slave-instances"
    }
  ]
}