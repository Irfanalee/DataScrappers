{
  "tech": "postgresql",
  "count": 293,
  "examples": [
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "postgresql",
      "question_id": 69754628,
      "title": "psql: error: connection to server on socket &quot;/tmp/.s.PGSQL.5432&quot; failed: No such file or directory",
      "problem": "Not really sure what caused this but most likely exiting the terminal while my rails server which was connected to PostgreSQL database was closed (not a good practice I know but lesson learned!)\nI've already tried the following:\n\nRebooting my machine (using MBA M1 2020)\nRestarting PostgreSQL using homebrew `brew services restart  postgresql`\nRe-installing PostgreSQL using Homebrew\nUpdating PostgreSQL using Homebrew\nI also tried following this link but when I run `cd Library/Application\\ Support/Postgres` terminal tells me Postgres folder doesn't exist, so I'm kind of lost already. Although I have a feeling that deleting postmaster.pid would really fix my issue. Any help would be appreciated!",
      "solution": "Below was the workaround if you prefer to stay in v13.3, if you are on 14.2 you can simply change the port postgresql is listening to. Read their docs about it here.\nNote that this solution works for the setup that I have (take a look at the original post for my setup details ie. OS & how what I'd use to install Postgres)\n\nopen your postgresql.conf file in the following directory and use\nany text editor you want. below uses vscode/vim.\n\nvscode\n```\n`sudo code . /usr/local/var/postgres/postgresql.conf\n`\n```\nvim\n```\n`sudo vi /usr/local/var/postgres/postgresql.conf\n`\n```\n\nchange port to 5432 and restart your machine.\n\nv13.3 solution\nit appears that the upgrade really messed up my postgres since per Nagev's answer it's listening to port 5433 instead of 5432. I downgraded to v13.3 to fix this issue.\n```\n`brew uninstall postgresql\nbrew install postgresql@13\nbrew services start postgresql@13\nbrew link postgresql@13 --force\n`\n```",
      "question_score": 138,
      "answer_score": 77,
      "created_at": "2021-10-28T15:05:12",
      "url": "https://stackoverflow.com/questions/69754628/psql-error-connection-to-server-on-socket-tmp-s-pgsql-5432-failed-no-such"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "postgresql",
      "question_id": 67276391,
      "title": "Why am I getting a permission denied error for schema public on pgAdmin 4?",
      "problem": "I'm trying to view the raw data from the AACT Database in pgAdmin 4. I'm on a Mac computer. When I just try to view the first 100 rows from the 'complete_oncology' table, I get the below error:\n```\n`ERROR: permission denied for schema public\nLINE 1: SELECT * FROM public.complete_oncology\n                      ^\nSQL state: 42501\nCharacter: 15\n`\n```\nDo I have insufficient permissions? If yes, how do I grant myself permissions to view this table? I am able to see other tables from different Schemas in the AACT database. I've read some users suggest granting myself permissions with something like the below, but no luck:\n`GRANT SELECT ON complete_oncology TO PUBLIC\n`\nThis just turns up an error:\n```\n`ERROR:  relation \"complete_oncology\" does not exist\nSQL state: 42P01\n`\n```",
      "solution": "If you get a \"permission denied\" for `public.complete_oncology`, but a \"relation does not exist\" for `complete_oncology`, that can only mean only one thing: you do not have `USAGE` permissions on the `public` schema.\nGet the owner of the schema to run\n```\n`GRANT USAGE ON SCHEMA public TO your_user;\n`\n```\nThen you should be able to see the table. If you still lack permissions on the table itself, get the owner to grant you `SELECT` on the table as well.",
      "question_score": 121,
      "answer_score": 57,
      "created_at": "2021-04-27T04:57:40",
      "url": "https://stackoverflow.com/questions/67276391/why-am-i-getting-a-permission-denied-error-for-schema-public-on-pgadmin-4"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "postgresql",
      "question_id": 69676009,
      "title": "psql: error: connection to server on socket &quot;/var/run/postgresql/.s.PGSQL.5432&quot; failed: FATAL: Peer authentication failed for user &quot;postgres&quot; (Ubuntu)",
      "problem": "When I try to open `psql` with this command:\n```\n`psql -U postgres\n`\n```\nI get this error:\n```\n`psql: error: connection to server on socket \"/var/run/postgresql/.s.PGSQL.5432\" failed: FATAL:  Peer authentication failed for user \"postgres\"\n`\n```\nBut it connects successfully when I use:\n```\n`sudo -u postgres psql\n`\n```\nCan someone please explain what is happening and how to diagnose/fix this problem? My `pg_hba.conf` contains the following:\n```\n`\n# Database administrative login by Unix domain socket\nlocal   all             postgres                                peer\n\n# TYPE  DATABASE        USER            ADDRESS                 METHOD\n\n# \"local\" is for Unix domain socket connections only\nlocal   all             all                                     md5\n# IPv4 local connections:\nhost    all             all             127.0.0.1/32            scram-sha-256\n# IPv6 local connections:\nhost    all             all             ::1/128                 scram-sha-256\n# Allow replication connections from localhost, by a user with the\n# replication privilege.\nlocal   replication     all                                     peer\n`\n```",
      "solution": "You can edit your .conf files with privileges using an editor, for my case it is nano. First edit the `pg_ident.conf` file:\n```\n`$sudo nano /etc/postgresql/14/main/pg_ident.conf\n`\n```\nMap your user by adding this line\n```\n`# MAPNAME       SYSTEM-USERNAME         PG-USERNAME\nuser1                postgres\n`\n```\nReplace the `` with the System-Username, which can be found using the whoami command. Type in your terminal:\n```\n`$whoami\n`\n```\nThen go ahead to edit the `pg_hba.conf` file with privileges\n```\n`$sudo nano /etc/postgresql/14/main/pg_hba.conf\n`\n```\nAdd your postgres user, with method=peer, as shown below:\n```\n`# TYPE  DATABASE        USER            ADDRESS                 METHOD\nlocal   all             postgres                                peer\n`\n```\nThis worked for me.",
      "question_score": 92,
      "answer_score": 61,
      "created_at": "2021-10-22T13:22:23",
      "url": "https://stackoverflow.com/questions/69676009/psql-error-connection-to-server-on-socket-var-run-postgresql-s-pgsql-5432"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "postgresql",
      "question_id": 66693689,
      "title": "How do I fix stale postmaster.pid file on Postgres?",
      "problem": "I went to view a postgres schema on my local (MacOS 11) database, and was not able to view my schemas.\nConnection Refused in PGSQL Editor! (DBeaver specifically)\n\nSo I opened my Postgres desktop application, and got the warning message, \"Stale postmaster.pid file\"\nHow can I fix this?\n(This example is Postgres 12, but i'm sure this fix will work on later versions)",
      "solution": "The problem is that the `postmaster.pid` file needs to be removed manually and then regenerated by Postgres. These are the steps to do remove it. (for MacOs, linux also probably pretty close to this)\nKeep in mind the version might changed, (`var-12`, could be `var-13`,`var-14`,`var-15` etc). But the general theory remains the same for now. Just swap the 12 for your version. We will use 12 as the example as that was i was working with at the time.\n\nOpen your terminal, and cd into the postgres directory: `cd /Users/$USER/Library/Application\\ Support/Postgres`\nMake sure you're in the right place, run `ls`, you should see a directory like `var-12`\nVerify the PID file is inside that \"`var-12`\" directory using `ls`: e.g. `ls var-12/`\n\n```\n`Library/Application Support/Postgres\n\u279c ls var-12\nPG_VERSION           pg_hba.conf          \npg_replslot          pg_subtrans\npostgresql.auto.conf base\npg_ident.conf        pg_serial\npg_tblspc            postgresql.conf\nglobal               pg_logical\npg_snapshots         pg_twophase\npostgresql.log       pg_commit_ts\npg_multixact         pg_stat\npg_wal               postmaster.opts\npg_dynshmem          pg_notify\npg_stat_tmp          pg_xact\npostmaster.pid  \nBefore you remove the file, verify the Postgres server is not running by viewing desktop app or CLI. Terminate Postgres server if needed\nThen remove `postmaster.pid`, e.g. `rm var-12/postmaster.pid`\nGo back to your console, start your Postgres server, and it should regenerate the PID, and it should start successfully and you will have full access to your schemas.",
      "question_score": 85,
      "answer_score": 176,
      "created_at": "2021-03-18T16:02:16",
      "url": "https://stackoverflow.com/questions/66693689/how-do-i-fix-stale-postmaster-pid-file-on-postgres"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "postgresql",
      "question_id": 74110708,
      "title": "Postgres 15. permission denied for schema public",
      "problem": "Can't create tables in public schema as non-superuser\npostgres - super user.\nWhat I've done:\n```\n`ALTER SCHEMA public owner to postgres;  \n\nCREATE USER admin WITH PASSWORD 'my-password';   \n\nGRANT USAGE, CREATE ON SCHEMA public TO postgres;   \nGRANT USAGE, CREATE ON SCHEMA public TO admin;    \n\nCREATE DATABASE mydb;    \nGRANT ALL ON DATABASE mydb TO admin;\n`\n```\nprivileges:\n```\n`postgres=# \\dn+\n                          List of schemas\n  Name  |  Owner   |  Access privileges   |      Description       \n--------+----------+----------------------+------------------------\n public | postgres | postgres=UC/postgres+| standard public schema\n        |          | =UC/postgres        +| \n        |          | admin=UC/postgres    | \n(1 row)\n`\n```\nwhat I got:\n\nHow to create tables in public schema?",
      "solution": "The first comment nailed the most likely reason this is happening. Quoting the release announcement:\n\nPostgreSQL 15 also revokes the `CREATE` permission from all users except a database owner from the `public` (or default) schema.\n\nThe reason your fix didn't work is that all actions you took on database `postgres` in regards to user `admin`'s privileges on schema `public` concern only that schema within the database `postgres`. Schema `public` on database `postgres` is not the same schema `public` as the one on newly created `mydb`.\nAlso, this:\n`GRANT ALL ON DATABASE mydb TO admin;\n`\ngrants privileges on the database itself, not things within the database. `admin` can now drop the database, for example, still without being able to create tables in schema `public`. My guess is that you wanted to make `admin` also the owner of `mydb`, in which case you need to add\n`ALTER DATABASE mydb OWNER TO admin;\n`\nOr you need to repeat your `GRANT USAGE, CREATE ON SCHEMA public TO admin;` on `mydb` if they're supposed to have all that access without being the owner. If you don't mind passing ownership, you can make them the owner at creation time:\n`CREATE DATABASE mydb OWNER admin;\n`\nHere's some more documentation on secure schema usage patterns the PostgreSQL 15 change was based on.",
      "question_score": 83,
      "answer_score": 197,
      "created_at": "2022-10-18T14:06:37",
      "url": "https://stackoverflow.com/questions/74110708/postgres-15-permission-denied-for-schema-public"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "postgresql",
      "question_id": 76899023,
      "title": "RDS while connection error: no pg_hba.conf entry for host",
      "problem": "I need assistance as I'm having trouble connecting to a managed Postgres database on RDS. I'm encountering the following error message: \"no pg_hba.conf entry for host '16.151.149.51', user 'analytics', database 'database', no encryption\" I have confirmed that encryption is functioning properly, and I've also added the IP to the security groups. What steps should I take to resolve this issue?",
      "solution": "First of all I wanna note that Nick's answer resolved my issue, but I just would love to add a detailed steps to follow for those who's new to AWS:\nCreate a New Parameter Group:\n\nOpen the Amazon RDS console at https://console.aws.amazon.com/rds/.\nIn the navigation pane, choose \"Parameter groups\".\nClick \"Create parameter group\" at the top right of the page.\nIn the \"Parameter group family\" dropdown, select \"postgres15\".\nIn the \"Group name\" field, enter a name for the new parameter group.\nIn the \"Description\" field, enter a description for the new parameter group.\nClick \"Create\" at the bottom right of the page.\n\nModify the rds.force_ssl Parameter of your new Parameter Group:\n\nIn the list of parameter groups, click on the name of the new parameter group you just created.\nIn the \"Filter parameters\" box, type rds.force_ssl and press Enter.\nYou should see the rds.force_ssl parameter. Click \"Edit parameters\".\nChange the value of rds.force_ssl from 1 to 0, then click \"Save changes\".\n\nAssociate Your RDS Instance with the New Parameter Group:\n\nIn the navigation pane, choose \"Databases\".\nClick on the name of your RDS instance.\nClick \"Modify\" at the top right of the page.\nIn the \"Database options\" section, find the \"DB parameter group\" setting and select the new parameter group you created from the dropdown menu.\nScroll down and click \"Continue\".\nReview the summary of modifications and click \"Modify DB Instance\".\n\nReboote Your RDS Instance:\n\nIn the navigation pane, choose \"Databases\".\nClick on the name of your RDS instance.\nClick \"Actions\" at the top right of the page, then \"Reboot\".\nConfirm that you want to reboot the instance.\n\nBy following these steps, you should be able to successfully modify the rds.force_ssl parameter in your Amazon RDS instance. And hopefully the connection issue would be resolved.\n\nNote: this method removes the default SSL security in the connection and shouldn't be used in production databases.",
      "question_score": 70,
      "answer_score": 141,
      "created_at": "2023-08-14T14:43:25",
      "url": "https://stackoverflow.com/questions/76899023/rds-while-connection-error-no-pg-hba-conf-entry-for-host"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "postgresql",
      "question_id": 68317383,
      "title": "TypeError: rxjs_1.lastValueFrom is not a function",
      "problem": "I am building an api using nestjs. After adding the typeorm and pg dependencies and adding the `TypeOrmModule.forRoot({})` code in `app.module.ts` like shown below.\n```\n`import { Module } from '@nestjs/common';\nimport { TypeOrmModule } from '@nestjs/typeorm';\nimport { AppController } from './app.controller';\nimport { AppService } from './app.service';\nimport { CoffeesModule } from './coffees/coffees.module';\n\n@Module({\n  imports: [CoffeesModule, TypeOrmModule.forRoot({\n    type: 'postgres',\n    host: 'localhost',\n    port: 5432,\n    username: 'postgres',\n    password: 'xxx',\n    database: 'postgres',\n    autoLoadEntities: true,\n    synchronize: true\n  })],\n  controllers: [AppController],\n  providers: [AppService],\n})\nexport class AppModule { }\n\n`\n```\nI get an error `TypeError: rxjs_1.lastValueFrom is not a function` with but no error when I exclude `TypeOrmModule.forRoot({})`.\nWhat could be the reason for the error ?",
      "solution": "If you're using Nest v8, RxJS version 7 is used, which no longer has a `toPromise()` method for Observables, so Nest uses the `lastValueFrom` method instead. If you're receiving this error, you probably need to update your `rxjs` dependency to >7.\n`npm i rxjs@^7\nyarn add rxjs@^7\npnpm i rxjs @^7\n`\nPick your flavor of package manager and have at it.\nIn the last update of NestJS, when is used cli to initialization of project this error is throw.",
      "question_score": 46,
      "answer_score": 138,
      "created_at": "2021-07-09T15:11:44",
      "url": "https://stackoverflow.com/questions/68317383/typeerror-rxjs-1-lastvaluefrom-is-not-a-function"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "postgresql",
      "question_id": 66325175,
      "title": "DOCKER container with postgres, WARNING: could not open statistics file &quot;pg_stat_tmp/global.stat&quot;: Operation not permitted",
      "problem": "I have a DOCKER container built from a few different images using a .yml, Dockerfile(s), etc.  Everything builds and runs fine so far, except for this one issue that I am seeing mentioned in the title:\n```\n`index-db_1   | 2021-02-22 23:18:33.388 UTC [31] WARNING:  could not open statistics file \"pg_stat_tmp/global.stat\": Operation not permitted\n`\n```\nThat Database Index is mapped to a Folder on the host in the root of the Docker Package, and everything else seems to work fine as far as the database is concerned.  I am using a Mac, but if I list permission from CLI for the DB folder I get:\n```\n`-rw-------@  1 sscotti  staff      3 Feb 22 11:01 PG_VERSION\ndrwx------@  6 sscotti  staff    192 Feb 22 11:54 base\ndrwx------@ 60 sscotti  staff   1920 Feb 22 16:00 global\ndrwx------@  2 sscotti  staff     64 Feb 22 11:01 pg_commit_ts\ndrwx------@  2 sscotti  staff     64 Feb 22 11:01 pg_dynshmem\n-rw-------@  1 sscotti  staff   4782 Feb 22 11:02 pg_hba.conf\n-rw-------@  1 sscotti  staff   1636 Feb 22 11:01 pg_ident.conf\ndrwx------@  5 sscotti  staff    160 Feb 22 17:46 pg_logical\ndrwx------@  4 sscotti  staff    128 Feb 22 11:01 pg_multixact\ndrwx------@  2 sscotti  staff     64 Feb 22 11:01 pg_notify\ndrwx------@  2 sscotti  staff     64 Feb 22 11:01 pg_replslot\ndrwx------@  2 sscotti  staff     64 Feb 22 11:01 pg_serial\ndrwx------@  2 sscotti  staff     64 Feb 22 11:01 pg_snapshots\ndrwx------@  2 sscotti  staff     64 Feb 22 16:00 pg_stat\ndrwx------@  5 sscotti  staff    160 Feb 22 17:50 pg_stat_tmp\ndrwx------@  3 sscotti  staff     96 Feb 22 11:01 pg_subtrans\ndrwx------@  2 sscotti  staff     64 Feb 22 11:01 pg_tblspc\ndrwx------@  2 sscotti  staff     64 Feb 22 11:01 pg_twophase\ndrwx------@  4 sscotti  staff    128 Feb 22 11:01 pg_wal\ndrwx------@  3 sscotti  staff     96 Feb 22 11:01 pg_xact\n-rw-------@  1 sscotti  staff     88 Feb 22 11:01 postgresql.auto.conf\n-rw-------@  1 sscotti  staff  28073 Feb 22 11:01 postgresql.conf\n-rw-------@  1 sscotti  staff     36 Feb 22 16:00 postmaster.opts\n-rw-------   1 sscotti  staff     94 Feb 22 16:00 postmaster.pid\n`\n```\npg_stat folder is actually empty.\nand pg_stat_temp has:\n```\n`-rw-------  1 sscotti  staff   1952 Feb 22 17:54 db_0.stat\n-rw-------  1 sscotti  staff  20360 Feb 22 17:54 db_13395.stat\n-rw-------  1 sscotti  staff   1151 Feb 22 17:54 global.stat\n`\n```\nThe .yml file has this:\n```\n`  index-db:\n      image: postgres\n      restart: unless-stopped\n      volumes:\n          - ./OrthancIndex:/var/lib/postgresql/data\n`\n```\nIs that something that can just be ignored given that it is a Docker container.\nAdding a comment about the same setup on UBUNTU.\nDatabase Folder:\ndrwx------  19 systemd-coredump root           4096 Jun 30 13:12 OrthancIndex\nDatabase:\n```\n`drwx------ 6 systemd-coredump systemd-coredump  4096 Jun 11 13:00 base\ndrwx------ 2 systemd-coredump systemd-coredump  4096 Jun 30 13:12 global\ndrwx------ 2 systemd-coredump systemd-coredump  4096 Mar 12 16:12 pg_commit_ts\ndrwx------ 2 systemd-coredump systemd-coredump  4096 Mar 12 16:12 pg_dynshmem\n-rw------- 1 systemd-coredump systemd-coredump  4782 Mar 12 16:12 pg_hba.conf\n-rw------- 1 systemd-coredump systemd-coredump  1636 Mar 12 16:12 pg_ident.conf\ndrwx------ 4 systemd-coredump systemd-coredump  4096 Jul  1 13:27 pg_logical\ndrwx------ 4 systemd-coredump systemd-coredump  4096 Mar 12 16:12 pg_multixact\ndrwx------ 2 systemd-coredump systemd-coredump  4096 Mar 12 16:12 pg_notify\ndrwx------ 2 systemd-coredump systemd-coredump  4096 Mar 12 16:12 pg_replslot\ndrwx------ 2 systemd-coredump systemd-coredump  4096 Mar 12 16:12 pg_serial\ndrwx------ 2 systemd-coredump systemd-coredump  4096 Mar 12 16:12 pg_snapshots\ndrwx------ 2 systemd-coredump systemd-coredump  4096 Jun 30 13:12 pg_stat\ndrwx------ 2 systemd-coredump systemd-coredump  4096 Jul  1 13:29 pg_stat_tmp\ndrwx------ 2 systemd-coredump systemd-coredump  4096 Jun 24 21:04 pg_subtrans\ndrwx------ 2 systemd-coredump systemd-coredump  4096 Mar 12 16:12 pg_tblspc\ndrwx------ 2 systemd-coredump systemd-coredump  4096 Mar 12 16:12 pg_twophase\n-rw------- 1 systemd-coredump systemd-coredump     3 Mar 12 16:12 PG_VERSION\ndrwx------ 3 systemd-coredump systemd-coredump  4096 Jul  1 12:37 pg_wal\ndrwx------ 2 systemd-coredump systemd-coredump  4096 Mar 12 16:12 pg_xact\n-rw------- 1 systemd-coredump systemd-coredump    88 Mar 12 16:12 postgresql.auto.conf\n-rw------- 1 systemd-coredump systemd-coredump 28073 Mar 12 16:12 postgresql.conf\n-rw------- 1 systemd-coredump systemd-coredump    36 Jun 30 13:12 postmaster.opts\n-rw------- 1 systemd-coredump systemd-coredump    94 Jun 30 13:12 postmaster.pid\n`\n```\npg_stat_temp\n```\n`-rw------- 1 systemd-coredump systemd-coredump  2660 Jul  1 13:30 db_0.stat\n-rw------- 1 systemd-coredump systemd-coredump 31157 Jul  1 13:30 db_13395.stat\n-rw------- 1 systemd-coredump systemd-coredump  1151 Jul  1 13:30 global.stat\n`\n```\nI actually get the same error on UBUNTU:\n```\n`postgres_index-db_1   | 2021-07-01 18:06:45.140 UTC [266] WARNING:  could not open statistics file \"pg_stat_tmp/global.stat\": Operation not permitted\npostgres_index-db_1   | 2021-07-01 18:13:45.583 UTC [273] WARNING:  could not open statistics file \"pg_stat_tmp/global.stat\": Operation not permitted\npostgres_index-db2_1  | 2021-07-01 18:19:43.716 UTC [282] WARNING:  could not open statistics file \"pg_stat_tmp/global.stat\": Operation not permitted\npostgres_index-db2_1  | 2021-07-01 18:21:43.749 UTC [284] WARNING:  could not open statistics file \"pg_stat_tmp/global.stat\": Operation not permitted\n`\n```\nAlthough here the user and group are systemd-coredump.",
      "solution": "Writing this down as much for my future self as to answer the question: looking over at the documentation and taking @Fide's solution above, the following seems to work on MacOSX Monterey running a PostGIS image (which is just built on top of the Postgres images)...\nNote that this is taken from a shell script to launch a Postgres+PostGIS image with a persistent local store on the host system:\n`WORK_DIR=\"$HOME/Documents/git/project\"\nDOCKER_NM=\"postgis\"\nDOCKER_IMG=\"postgis/postgis\"\nPOSTGRES_PWD=\"XXX\"\nPORT_NO=\"5432\"\ndocker run --name $DOCKER_NM \\\n    -e POSTGRES_PASSWORD=$POSTGRES_PWD \\\n    -e PGDATA=/var/lib/postgresql/data/pgdata \\\n    -v \"$WORK_DIR/data/postgres\":/var/lib/postgresql/data \\\n    -p \"$PORT_NO:5432\" -d $DOCKER_IMG \\\n    postgres -c stats_temp_directory=/tmp\n`\nIf you were running from the Terminal directly (i.e. not via zsh/bash) then you'd need to `export` each of the parameters instead of just specifying them.\nWith this the global.stat error seems to have gone away... though I should note that I've only just figured this out and have been running the container for less than 60 minutes. The alternative would be to follow the instructions on that page for creating and using a persistent custom conf file (extending @SScotti's comment above) as mounted from the local FS:\n`-v \"$PWD/my-postgres.conf\":/etc/postgresql/postgresql.conf\n`",
      "question_score": 44,
      "answer_score": 3,
      "created_at": "2021-02-23T00:59:04",
      "url": "https://stackoverflow.com/questions/66325175/docker-container-with-postgres-warning-could-not-open-statistics-file-pg-stat"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "postgresql",
      "question_id": 68895862,
      "title": "How to add foreign key in PostgreSQL",
      "problem": "I created this first table named 'bookstore' where Primary Key is book_name:\n```\n`create table bookstore (book_name varchar primary key, author varchar, price decimal);\n`\n```\nI am trying to create a second table named 'name' where name is primary key. I want to make this primary key- author.name as a foreign key of bookstore.author.\n```\n`create table author (name varchar primary key, place varchar,\n                    constraint fk_author_bookstore foreign key(name) references bookstore(author));\n`\n```\nBut the error is: ERROR:  there is no unique constraint matching given keys for referenced table \"bookstore\"\nSQL state: 42830\nI am new to SQL, so, hoping to get some help. If you can, please write the correct code.\nThanks",
      "solution": "Name column in author table is primary key and it's referenced as foreign key in bookstore table.\n```\n`-- PostgreSQL (v11)\n\ncreate table author (name varchar primary key, place varchar);\n\ncreate table bookstore (book_name varchar primary key, author varchar, price decimal\n, CONSTRAINT fk_author_bookstore\n      FOREIGN KEY(author) \n      REFERENCES author(name));\n`\n```\nPlease check from url https://dbfiddle.uk/?rdbms=postgres_11&fiddle=8394f796433ed8bc170c2889286b3fc2\nAdd foreign key after table creation\n```\n`-- PostgreSQL(v11)\nALTER TABLE bookstore\n      ADD CONSTRAINT fk_author_bookstore FOREIGN KEY (author) \n          REFERENCES author (name);\n`\n```\nPlease check from url https://dbfiddle.uk/?rdbms=postgres_11&fiddle=d93cf071bfd0e3940dfd256861be813c",
      "question_score": 31,
      "answer_score": 49,
      "created_at": "2021-08-23T18:11:41",
      "url": "https://stackoverflow.com/questions/68895862/how-to-add-foreign-key-in-postgresql"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "postgresql",
      "question_id": 77462578,
      "title": "pgadmin error when restoring database from tar file",
      "problem": "I'm trying load a tar file using pgAdmin, but it's failing and all I get is exit code 3221225781. This code seems to indicate a missing dll What is exit code 3221225781 on windows? but I'm not sure why this would be or what dll to look for.",
      "solution": "Updating the path in pgAdmin fixed this.\nSteps:\nSelect Databases > File > Preferences > Paths > Binary paths.\nThen edit to `C:\\Program Files\\PostgreSQL\\15\\bin.`\nYou'll need to use a different path if you're on a different version of PostgreSQL.",
      "question_score": 29,
      "answer_score": 74,
      "created_at": "2023-11-10T20:59:38",
      "url": "https://stackoverflow.com/questions/77462578/pgadmin-error-when-restoring-database-from-tar-file"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "postgresql",
      "question_id": 68476229,
      "title": "M1 Related! - Prisma: Can&#39;t reach database server at `database`:`5432`",
      "problem": "Since I have moved to the new Apple Silicon architecture my docker setup with nextjs and postgres is not working anymore. The database inside the docker cannot be found by the nextjs server where I am using prisma.\nThe prisma client can't reach the postgres database on port 5432.\n\nCan't reach database server at `test-postgres`:`5432`\n\nThe migration also does not work and returns the same error above.\n`docker-compose run --publish 5555:5555 next npx prisma migrate dev\n`\ndocker-compose.yml\n```\n`postgres:\n    container_name: 'test-postgres'\n    restart: unless-stopped\n    image: 'postgres:13'\n    ports:\n      - '15432:5432'\n    volumes:\n      - 'pgdata:/var/lib/postgresql/data/'\n    environment:\n      POSTGRES_PASSWORD: postgres\n`\n```\n.env\n```\n`DATABASE_URL=\"postgres://postgres:postgres@localhost:15432/postgres\"\n`\n```\nI have also added the arm binary target to the schema.prisma\nschema.prisma\n```\n`\ngenerator client {\n  provider        = \"prisma-client-js\"\n  binaryTargets   = [\"native\", \"debian-openssl-1.1.x\", \"linux-arm-openssl-1.1.x\", \"linux-musl\"]\n  previewFeatures = [\"orderByRelation\", \"selectRelationCount\"]\n}\n`\n```\nThe postgres container is actually running and I can see it through the Docker Desktop Dashboard. One thing I have noticed inside the postgres container was this ERROR:\n```\n`2021-07-21 12:52:58.927 UTC [76] ERROR:  relation \"_prisma_migrations\" does not exist at character 126\n`\n```\nHave someone experienced it before and found a solution for it?\n[EDIT]\nHow to reproduce\nclone repo, follow README.md and see expected behaviour on a M1 Apple Silicon Machine: https://github.com/baristikir/prisma-postgres-M1",
      "solution": "Adding `?connect_timeout=300` to the connection string of the database did the trick.\n```\n`DATABASE_URL=\"postgres://postgres:postgres@localhost:15432/postgres?connect_timeout=300\"\n`\n```",
      "question_score": 29,
      "answer_score": 81,
      "created_at": "2021-07-21T22:40:23",
      "url": "https://stackoverflow.com/questions/68476229/m1-related-prisma-cant-reach-database-server-at-database5432"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "postgresql",
      "question_id": 78539678,
      "title": "Using Postgres 16 with Spring Boot 3.3.0",
      "problem": "I just upgraded from spring-boot 3.2.3 -> 3.3.0. After the upgrade flyway refuses to connect to postgres:\n```\n`Caused by: org.flywaydb.core.api.FlywayException: Unsupported Database: PostgreSQL 16.2\n    at org.flywaydb.core.internal.database.DatabaseTypeRegister.getDatabaseTypeForConnection(DatabaseTypeRegister.java:105)\n    at org.flywaydb.core.internal.jdbc.JdbcConnectionFactory.(JdbcConnectionFactory.java:73)\n    at org.flywaydb.core.FlywayExecutor.execute(FlywayExecutor.java:134)\n    at org.flywaydb.core.Flyway.migrate(Flyway.java:147)\n`\n```\nWhat is the expected way to connect to postgres 16 using spring-boot 3.3.0 and flyway?",
      "solution": "There is pinned issue to announce about extracting database support out from `flyway-core`.\nTry to add this dependency to your project:\n```\n`\n    org.flywaydb\n    flyway-database-postgresql\n\n`\n```",
      "question_score": 29,
      "answer_score": 63,
      "created_at": "2024-05-27T16:32:33",
      "url": "https://stackoverflow.com/questions/78539678/using-postgres-16-with-spring-boot-3-3-0"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "postgresql",
      "question_id": 66456952,
      "title": "What does log_cnt mean in the postgres sequence?",
      "problem": "I was trying to figure out why the sequence number in my table is jumping exponentially, while that could be an issue in my API which I still have to figure out, I came across this query\nto find the current value of the sequence\n`SELECT * from some_sequence`\nIt gives me a column named `log_cnt`. The value of this column keeps jumping around 30. I am not sure what does this number means and if at all this can affect the next number which is going to come in the sequence.\nCan someone help me on this?",
      "solution": "Changes in the state of a sequence must be written to transaction log (WAL).\nThis could potentially lead to a lot of WAL records, which would harm performance.\nAs an optimization, PostgreSQL does not log the current sequence counter, but a value that is 32 greater. See this comment in `src/backend/commands/sequence.c`:\n`/*\n * We don't want to log each fetching of a value from a sequence,\n * so we pre-log a few fetches in advance. In the event of\n * crash we can lose (skip over) as many values as we pre-logged.\n */\n#define SEQ_LOG_VALS    32\n`\nThat means that up to 32 sequence values could get lost during a crash (recovery will set the sequence to the logged position), which is no problem.\n`log_cnt` shows how many fetches remain before a new WAL record has to be written.\nAfter the first call to `nextval` after a checkpoint, `log_cnt` will be 32. It will decrease with every call to `nextval`, and once it reaches 0, it is set to 32 again, and a WAL record is written.",
      "question_score": 27,
      "answer_score": 39,
      "created_at": "2021-03-03T13:20:04",
      "url": "https://stackoverflow.com/questions/66456952/what-does-log-cnt-mean-in-the-postgres-sequence"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "postgresql",
      "question_id": 66032256,
      "title": "How to create TimescaleDB Hypertable with time partitioning on non unique timestamp?",
      "problem": "I have just started to use TimescaleDB and want to create a hypertable on a table with events.\nOriginally I thought of following the conventional pattern of:\n```\n`CREATE TABLE event (\n  id serial PRIMARY KEY,\n  ts timestamp with time zone NOT NULL,\n  details varchar(255) NOT NULL\n);\n\nCREATE INDEX event_ts_idx on event(ts);\n`\n```\nHowever, when I tried to create the hypertable with the following query:\n```\n`SELECT create_hypertable('event', 'ts');\n`\n```\nI got: `ERROR:  cannot create a unique index without the column \"ts\" (used in partitioning)`\nAfter doing some research, it seems that the timestamp itself needs to be the (or part of the) primary key.\nHowever, I do not want the timestamp `ts` to be unique. It is very likely that these high frequency events will coincide in the same microsecond (the maximum resolution of the `timestamp` type). It is the whole reason why I am looking into TimescaleDB in the first place.\nWhat is the best practice in this case?\nI was thinking of maybe keeping the `serial id` as part of the primary key, and making it composite like this:\n```\n`CREATE TABLE event_hyper (\n  id serial,\n  ts timestamp with time zone NOT NULL,\n  details varchar(255) NOT NULL,\n  PRIMARY KEY (id, ts)\n);\n\nSELECT create_hypertable('event_hyper', 'ts');\n`\n```\nThis sort of works, but I am unsure if it is the right approach, or if I am creating a complicated primary key which will slow down inserts or create other problems.\nWhat is the right approach when you have possible collision in timestamps when using TimescaleDB hypertables?",
      "solution": "How to create TimescaleDB Hypertable with time partitioning on non unique timestamp?\n\nThere is no need to create unique constraint on time dimension (unique constraints are not required). This works:\n`CREATE TABLE event (\n  id serial,\n  ts timestamp with time zone NOT NULL,\n  details varchar(255) NOT NULL\n);\nSELECT create_hypertable('event', 'ts');\n`\nNote that the primary key on `id` is removed.\nIf you want to create unique constraint or primary key, then TimescaleDB requires that any unique constraint or primary key includes the time dimension. This is similar to limitation of PostgreSQL in declarative partitioning to include partition key into unique constraint:\n\nUnique constraints (and hence primary keys) on partitioned tables must include all the partition key columns. This limitation exists because PostgreSQL can only enforce uniqueness in each partition individually.\n\nTimescaleDB also enforces uniqueness in each chunk individually. Maintaining uniqueness across chunks can affect ingesting performance dramatically.\nThe most common approach to fix the issue with the primary key is to create a composite key and include the time dimension as proposed in the question. If the index on the time dimension is not needed (no queries only on time is expected), then the index on time dimension can be avoided:\n`CREATE TABLE event_hyper (\n  id serial,\n  ts timestamp with time zone NOT NULL,\n  details varchar(255) NOT NULL,\n  PRIMARY KEY (id, ts)\n);\n\nSELECT create_hypertable('event_hyper', 'ts', create_default_indexes => FALSE);\n`\nIt is also possible to use an integer column as the time dimension. It is important that such column has time dimension properties: the value is increasing over time, which is important for insert performance, and queries will select a time range, which is critical for query performance over large database. The common case is for storing unix epoch.\nSince `id` in `event_hyper` is SERIAL, it will increase with time. However, I doubt the queries will select the range on it. For completeness SQL will be:\n`CREATE TABLE event_hyper (\n  id serial PRIMARY KEY,\n  ts timestamp with time zone NOT NULL,\n  details varchar(255) NOT NULL\n);\n\nSELECT create_hypertable('event_hyper', 'id', chunk_time_interval => 1000000);\n`",
      "question_score": 26,
      "answer_score": 26,
      "created_at": "2021-02-03T18:04:56",
      "url": "https://stackoverflow.com/questions/66032256/how-to-create-timescaledb-hypertable-with-time-partitioning-on-non-unique-timest"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "postgresql",
      "question_id": 66168437,
      "title": "TypeORM findOne with nested relations",
      "problem": "I am having some issues performing a nested find query with TypeORM. Here's the basic code:\n`    const { completionId } = req?.params;\n    const user = req.user;\n\n    const retrievedCompletion = await getRepository(\n      CompletionGoogleSearch\n    ).findOne({\n      relations: ['run', 'run.user'],\n      where: {\n        id: completionId,\n        // run: { user: { id: user.id } }, // This is the code that breaks the function\n      },\n    });\n\n    console.log(retrievedCompletion?.run.user.id);\n    console.log(user.id);\n`\nIt looks to me like there's nothing out of order, and that the query should run. Any idea on what I am doing wrong? I know I can get around this issue by writing a querybuilder query or using raw SQL\u2013I am just curious to understand if there's a flaw in my code.",
      "solution": "The feature you're asking about doesn't supported on typeorm yet (Feb 2021).\nCheckout this issue that was opened on 2018.",
      "question_score": 25,
      "answer_score": 3,
      "created_at": "2021-02-12T09:22:01",
      "url": "https://stackoverflow.com/questions/66168437/typeorm-findone-with-nested-relations"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "postgresql",
      "question_id": 76220715,
      "title": "Type &quot;vector&quot; does not exist on postgresql - langchain",
      "problem": "I was trying to embed some documents on postgresql with the help of pgvector extension and langchain. Unfortunately I'm having trouble with the following error:\n`(psycopg2.errors.UndefinedObject) type \"vector\" does not exist\nLINE 4:  embedding VECTOR(1536), \n                   ^\n\n[SQL: \nCREATE TABLE langchain_pg_embedding (\n    collection_id UUID, \n    embedding VECTOR(1536), \n    document VARCHAR, \n    cmetadata JSON, \n    custom_id VARCHAR, \n    uuid UUID NOT NULL, \n    PRIMARY KEY (uuid), \n    FOREIGN KEY(collection_id) REFERENCES langchain_pg_collection (uuid) ON DELETE CASCADE\n)\n]\n`\n\nMy environment info:\n\npgvector docker image `ankane/pgvector:v0.4.1`\npython `3.10.6`, psycopg2 `2.9.6`, pgvector `0.1.6`\n\nList of installed extensions on postgres\n```\n`  Name   | Version |   Schema   |                Description                 \n---------+---------+------------+--------------------------------------------\n plpgsql | 1.0     | pg_catalog | PL/pgSQL procedural language\n vector  | 0.4.1   | public     | vector data type and ivfflat access method\n`\n```\nI've tried the following ways to resolve:\n\nFresh installing the Postgres docker image with pgvector extension enabled.\nManually install the extension with the official instruction.\nManually install the extension on Postgres like the following:\n\n`CREATE EXTENSION IF NOT EXISTS vector\n    SCHEMA public\n    VERSION \"0.4.1\";\n`\nBut no luck.",
      "solution": "Update 17th July 2023\nAs previously I mentioned my issue was somewhere else in my configuration, here is the other reason that may be responsible for the error,\n\nThe pgvector extension isn't enabled in the database you are using. Make sure you run `CREATE EXTENSION vector;` in each database you are using for storing vectors.\nThe vector schema is not in the search_path. Run `SHOW search_path;` to see the available schemas in the search path and `\\dx` to see the list of installed extensions with schemas.\n\nUnfortunately, the issue was somewhere else. My `extension installation` and `search_path` schema were totally okay for the defined database I was supposed to use. But my environment variable which was responsible for which database to use, got messed up and was using the default database `postgres` instead of my defined database, which didn't have the extension enabled.",
      "question_score": 24,
      "answer_score": 25,
      "created_at": "2023-05-10T18:28:47",
      "url": "https://stackoverflow.com/questions/76220715/type-vector-does-not-exist-on-postgresql-langchain"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "postgresql",
      "question_id": 77429653,
      "title": "Install Postgresql on Ubuntu 18.04 Bionic (repo removed)",
      "problem": "I'm getting\n```\n`E: The repository 'https://apt.postgresql.org/pub/repos/apt bionic-pgdg Release' no longer has a Release file.\n`\n```\non sudo apt update. That means that repo for bionic is not accessible.\nIn my case for this time I can't upgrade Ubuntu version to a newer one and somehow need to manage that. In this case I suppose I need to remove file from /etc/apt/sources.list.d which contains:\n```\n`deb https://apt.postgresql.org/pub/repos/apt bionic-pgdg main\n`\n```\nBut then how can I have a possibility to install Postgresql on Bionic? Is there any way e.g. to download and install from source?",
      "solution": "The packages have been moved to apt-archive.\nPoint your sources.list entry to\n```\n`deb https://apt-archive.postgresql.org/pub/repos/apt bionic-pgdg main\n`\n```\nSee the announcement here: https://www.postgresql.org/message-id/ZN4OigxPJA236qlg%40msg.df7cb.de",
      "question_score": 23,
      "answer_score": 63,
      "created_at": "2023-11-06T09:12:45",
      "url": "https://stackoverflow.com/questions/77429653/install-postgresql-on-ubuntu-18-04-bionic-repo-removed"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "postgresql",
      "question_id": 68626017,
      "title": "docker compose psql: error: FATAL: role &quot;postgres&quot; does not exist",
      "problem": "I faced a problem when I try to use psql command with my docker-compose file on my local Ubuntu machine:\n`psql: error: FATAL:  role \"postgres\" does not exist`\nI tried to use others solution like removing docker image, volume. `psql -U postgres` doesn't work for me either.\nI try to use first `docker-compose up`, then `docker exec -it database bash`\nThere's my docker-compose file\n```\n`services:\n  db:\n    container_name: postgres\n    image: postgres:13.3-alpine\n    restart: always\n    user: postgres\n    environment:\n      - POSTGRES_DB=postgres\n      - POSTGRES_PASSWORD=postgres\n      - POSTGRES_USER=root\n    ports:\n      - \"5432:5432\"\n    volumes:\n      - ./data/db:/var/lib/postgresql/data\n`\n```\nMaybe this string tells something?\n`postgres | PostgreSQL Database directory appears to contain a database; Skipping initialization`\nOUTPUT:\n```\n`Attaching to postgres\npostgres | \npostgres | PostgreSQL Database directory appears to contain a database; Skipping initialization\npostgres | \npostgres | 2021-08-02 17:29:10.426 UTC [1] LOG:  starting PostgreSQL 13.3 on x86_64-pc-linux-musl, compiled by gcc (Alpine 10.3.1_git20210424) 10.3.1 20210424, 64-bit\npostgres | 2021-08-02 17:29:10.426 UTC [1] LOG:  listening on IPv4 address \"0.0.0.0\", port 5432\npostgres | 2021-08-02 17:29:10.426 UTC [1] LOG:  listening on IPv6 address \"::\", port 5432\npostgres | 2021-08-02 17:29:10.429 UTC [1] LOG:  listening on Unix socket \"/var/run/postgresql/.s.PGSQL.5432\"\npostgres | 2021-08-02 17:29:10.433 UTC [12] LOG:  database system was shut down at 2021-08-02 17:22:17 UTC\npostgres | 2021-08-02 17:29:10.438 UTC [1] LOG:  database system is ready to accept connections\npostgres | 2021-08-02 17:37:53.452 UTC [33] FATAL:  role \"postgres\" does not exist\npostgres | 2021-08-02 17:37:56.958 UTC [35] FATAL:  role \"user\" does not exist\npostgres | 2021-08-02 17:41:54.294 UTC [45] FATAL:  role \"postgres\" does not exist```\n`\n```",
      "solution": "First, you've set `POSTGRES_USER` to `root`, so you're going to have a `root` user instead of `postgres` user.\nSecond, if a database already exists, it doesn't matter what you set for `POSTGRES_USER` and `POSTGRES_PASSWORD` -- postgres will use whatever is in the database.\nSo you can either:\n\nDelete the database (`rm -rf data/db`) and start over, or\nEdit your `pg_hba.conf` so that you don't need a password",
      "question_score": 23,
      "answer_score": 16,
      "created_at": "2021-08-02T19:53:26",
      "url": "https://stackoverflow.com/questions/68626017/docker-compose-psql-error-fatal-role-postgres-does-not-exist"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "postgresql",
      "question_id": 75352342,
      "title": "Permission denied for schema public at character x",
      "problem": "I am using postgresql 15 and I tried running these:\n```\n`grant all privileges on database my_database to my_database_user;\ngrant all privileges on all tables in schema public to my_database_user;\ngrant all privileges on all sequences in schema public to my_database_user;\ngrant all privileges on all functions in schema public to my_database_user;\n`\n```\nbut when I run:\n```\n`php artisan migrate --seed \n`\n```\nI got:\n\nSQLSTATE[42501]: Insufficient privilege: 7 ERROR:  permission denied for schema public at character 14 (SQL: create table \"migrations\" (\"id\" serial primary key not null, \"migration\" varchar(255) not null, \"batch\" integer not null))\n\nWhat I am missing?\nI do make sure .env has correct credentials:\n```\n`DB_CONNECTION=pgsql\nDB_HOST=127.0.0.1\nDB_PORT=5432\nDB_DATABASE=my_database\nDB_USERNAME=my_database_user\nDB_PASSWORD=password\n`\n```\nChecking that I did:\n```\n`postgres=# \\du my_database_user\n            List of roles\n  Role name  | Attributes | Member of \n-------------+------------+-----------\n my_database_user |            | {}\n`\n```\nand:\n```\n`postgres=# SELECT * FROM pg_roles;\n          rolname          | rolsuper | rolinherit | rolcreaterole | rolcreatedb | rolcanlogin | rolreplication | rolconnlimit | rolpassword | rolvaliduntil | rolbypassrls | rolconfig |  oid  \n---------------------------+----------+------------+---------------+-------------+-------------+----------------+--------------+-------------+---------------+--------------+-----------+-------\n my_database_user               | f        | t          | f             | f           | t           | f              |           -1 | ********    |               | f            |           | 16389\n`\n```",
      "solution": "You are missing permissions on the schema:\n```\n`GRANT CREATE ON SCHEMA public TO my_database_user;\n`\n```\nYou are probably using PostgreSQL v15 or higher. The default permissions on the `public` schema have changed in v15. Before, the insecure default was to allow everyone (`PUBLIC`) to create objects in schema `public`. Now only the database owner can do that, unless you grant extra privileges.\nMake sure that you are connected to the correct database when you grant the permissions, as each database has its own schemas.",
      "question_score": 22,
      "answer_score": 35,
      "created_at": "2023-02-05T13:48:53",
      "url": "https://stackoverflow.com/questions/75352342/permission-denied-for-schema-public-at-character-x"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "postgresql",
      "question_id": 68418224,
      "title": "Prisma $queryRaw with variable length parameter list",
      "problem": "I am using Prisma with Postgres.  I need to use $queryRaw for a particular query due to use of unsupported tsvector type in the underlying table.  In this query, I also need to use an 'in' statement where the items in the 'in' list need to be parameterised.. I have tried this\n```\n`const ids = [41, 55]\nconst result = await prisma.$queryRaw`select * from users where id in (${ids})`;\n`\n```\nbut I get a kernel panic\n\n```\n`PANIC in /root/.cargo/git/checkouts/rust-postgres-dc0fca9be721a90f/8a61d46/postgres-types/src/lib.rs:762:18\nexpected array type\n`\n```\n\nI also tried this...\n```\n`const result = await prisma.$queryRaw`select * from users where id in (${ids.join(',')})`;\n`\n```\nbut then I get this error...\n```\n`Raw query failed. Code: `22P03`. Message: `db error: ERROR: incorrect binary data format in bind parameter 1`\n`\n```\nThe sql-template-tag library which I think is used by prisma, has a way of supporting this so after installing and importing it, I tried this..\n```\n`const result = await prisma.$queryRaw`select * from users where id in (${join(ids)})`;\n`\n```\nbut this throws the same error.\nany Idea how I can achieve this?",
      "solution": "RTFM: https://www.prisma.io/docs/concepts/components/prisma-client/raw-database-access#tagged-template-helpers\n```\n`import { Prisma } from \"@prisma/client\";\n\nconst ids = [1, 3, 5, 10, 20];\nconst result = await prisma.$queryRaw`SELECT * FROM User WHERE id IN (${Prisma.join(\n  ids\n)})`;\n`\n```",
      "question_score": 22,
      "answer_score": 52,
      "created_at": "2021-07-17T09:08:38",
      "url": "https://stackoverflow.com/questions/68418224/prisma-queryraw-with-variable-length-parameter-list"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "postgresql",
      "question_id": 72307187,
      "title": "How to install postgresql-client to Amazon EC2 Linux machine?",
      "problem": "I am trying to install postgresql for the meta data of hive on Amazon EC2 Linux machine.\nAnd at that point, I am trying to connect postgresql outside docker image. Inside, I can connect the postgresql.\nI tried this command:\n```\n`[ec2-user@ip-****-***-** ~]$ sudo yum install postgresql-client -y\n`\n```\nand the result:\n```\n`Failed to set locale, defaulting to C\nLoaded plugins: extras_suggestions, langpacks, priorities, update-motd\namzn2-core                                                                                                                                                                                                                                               | 3.7 kB  00:00:00     \nNo package postgresql-client available.\nError: Nothing to do\n`\n```\nTo ensure locale, I tried these ones:\n```\n`[ec2-user@ip-***-***-*** ~]$ echo \"$LANG\"\nen_US.UTF-8\n[ec2-user@ip-***-***-*** ~]$ echo \"$LC_CTYPE\"\nUTF-8\n`\n```\nThen, I tried this one install postgresql-client lastly:\n```\n`[ec2-user@ip-***-***-*** ~]$ sudo amazon-linux-extras install postgresql-client\nTopic postgresql-client is not found.\n`\n```\nI am not so familiar with these technologies, if you can help I will be so appreciated",
      "solution": "If you get the error...\n`No match for argument: postgresql` ...\nI found that AWS Linux is a dynamic changing OS, so here is the current method + how to detect a new way to do this...\nCurrent method (as of Oct 21, 2022)\n(I have Machine Image (AMI): Amazon Linux 2022)\n```\n`$ sudo yum update \n$ sudo yum install postgresql13\n$ psql --version\npsql (PostgreSQL) 13.5\n`\n```\nFuture proof answer\nTry this first ...\n```\n`$ sudo yum update\n$ sudo yum search \"postgres\"\n`\n```\n... and look in the output for a line like this: `postgresql13.aarch64 : PostgreSQL client programs`. This tells you the current version of an installable client package name - in this case `postgresql13`. Now you know which package to install and avoid: `No match for argument: postgresql` error.\nTLDR;\nIt seems, for Linux that Amazon now says to use `yum` directly vs. the old `amazon-linux-extras` here... Install software packages on an Amazon Linux instance. Then you wonder, how to know which package name to use to install it, which is here: Find software packages on an Amazon Linux instance. I used a small part of the name as a 'key' to search for: `sudo yum search \"postgres\"` which got me the answer, by looking at the output.\nThis currently works for my version of Amazon linux:\n```\n`$ uname -a\nLinux ip-0-0-0-0.ec2.internal 5.15.43-20.123.amzn2022.aarch64 #1 SMP Thu May 26 17:03:36 UTC 2022 aarch64 aarch64 aarch64 GNU/Linux\n`\n```\nThis is a new instance with almost nothing added, not even additional `yum` archives, YMMV.",
      "question_score": 21,
      "answer_score": 32,
      "created_at": "2022-05-19T17:23:26",
      "url": "https://stackoverflow.com/questions/72307187/how-to-install-postgresql-client-to-amazon-ec2-linux-machine"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "postgresql",
      "question_id": 70958090,
      "title": "create index concurrently on partitioned table",
      "problem": "I am using postgresql 14.1, and I re-created my live database using parititons for some tables.\nsince i did that, i could create index when the server wasn't live, but when it's live i can only create the using `concurrently` but unfortunately when I try to create an index concurrently i get an error.\nrunning this:\n```\n`create index concurrently foo  on foo_table(col1,col2,col3));\n`\n```\nprovides the error:\n```\n`ERROR:  cannot create index on partitioned table \"foo_table\" concurrently\n`\n```\nnow it's a live server and i cannot create indexes not concurrently and i need to create some indexes in order to improve performance. any ideas how do to that ?\nthanks",
      "solution": "No problem. First, use `CREATE INDEX CONCURRENTLY` to create the index on each partition. Then use `CREATE INDEX` to create the index on the partitioned table. That will be fast, and the indexes on the partitions will become the partitions of the index.",
      "question_score": 21,
      "answer_score": 30,
      "created_at": "2022-02-02T16:13:17",
      "url": "https://stackoverflow.com/questions/70958090/create-index-concurrently-on-partitioned-table"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "postgresql",
      "question_id": 68894232,
      "title": "Error loading psycopg2 module: Library not loaded: libpq.5.dylib",
      "problem": "I am trying to run a Django project with Postgres database. I use Postgres 13.4 installed via postgressapp (UNIVERSAL with all currently supported versions) and python 3.9 (in venv). I work on Mac with Apple M1 chip, macOS Big Sur.\nI faced the following well-known problem:\n```\n`django.core.exceptions.ImproperlyConfigured: Error loading psycopg2 module: dlopen(/Users/mymac/PyCharmProjects/projectname/venv/lib/python3.9/site-packages/psycopg2/_psycopg.cpython-39-darwin.so, 2): Library not loaded: /opt/homebrew/opt/postgresql/lib/libpq.5.dylib\n\u00a0 Referenced from: /Users/mymac/PyCharmProjects/projectname/venv/lib/python3.9/site-packages/psycopg2/_psycopg.cpython-39-darwin.so\n\u00a0 Reason: image not found\n`\n```\nWith searching, I found some discussions like this: `https://github.com/psycopg/psycopg2/issues/1216`. It seems that the most relevant solution is \"RyanDurk\u00a0commented\u00a0on Jan 27\":\n```\n`$ brew install libpq --build-from-source\n\n$ export LDFLAGS=\"-L/opt/homebrew/opt/libpq/lib\"\n\n$ pip install psycopg2\n`\n```\nUnfortunately, in my case it does not help.\nThen, I found some recommendations here: Library not loaded: /usr/local/lib/libpq.5.4.dylib and tried them. In particular, I tried to reach libpq.5.dylib via symlink like:\n`ln -s\u00a0 /Library/PostgreSQL/13/lib/libpq.5.dylib      /opt/homebrew/opt/postgresql/lib/libpq.5.dylib` (the solution marked as accepted by topic starter), but also unsuccessfully.\nI tried to install postgres from postgresql.org,  then uninstall/reinstall postgres with homebrew, then\n```\n`gem uninstall pg -> bundle install\n`\n```\nwith the same result.\nI have run the same project successfully before, on the mac with Intel chip and PyCharm community edition. Also the same project runs normally on Linux.\nIf you have any idea what happens and how to fix this problem, please help me. I provide more details if necessary.",
      "solution": "I had the same error after brew decided to upgrade my `postgresql` package to version 14 (your version may vary).\nThere was this part of the error message:\n```\n`Library not loaded: '/usr/local/opt/postgresql/lib/libpq.5.dylib'\n...\nReason: tried: '/usr/local/opt/postgresql/lib/libpq.5.dylib' (no such file)\n`\n```\nI basically found the new location of the `libpq` and symlinked it to the location where `psycopg2` was looking for it:\n```\n`ln -s /usr/local/lib/postgresql@14/libpq.5.14.dylib /usr/local/opt/postgresql/lib/libpq.5.dylib\n`\n```\nHope that helps anyone else with a similar issue.",
      "question_score": 20,
      "answer_score": 34,
      "created_at": "2021-08-23T16:13:42",
      "url": "https://stackoverflow.com/questions/68894232/error-loading-psycopg2-module-library-not-loaded-libpq-5-dylib"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "postgresql",
      "question_id": 66909895,
      "title": "TypeORM: QueryFailedError: relation does not exist",
      "problem": "I need a little help with migration. I'm trying to seed DB using migration. But I get an error \"QueryFailedError: relation \"account\" does not exist\". I think it's just typical newbie mistake. So please check my code:\naccount.entity.ts\n`import { BeforeInsert, Column, Entity, OneToMany } from 'typeorm';\nimport { AbstractEntity } from '../../common/abstract.entity';\nimport { SourceEntity } from '../source/source.entity';\nimport { UtilsService } from '../../shared/services/utils.service';\n\n@Entity({ name: 'account' })\nexport class AccountEntity extends AbstractEntity {\n  @Column({ unique: true })\n  username: string;\n\n  @Column({ nullable: true })\n  password: string;\n\n  @OneToMany(() => SourceEntity, (source) => source.account, {\n    cascade: true,\n  })\n  sources: SourceEntity[];\n\n  @BeforeInsert()\n  async setPassword() {\n    this.password = UtilsService.generateHash(this.password);\n  }\n}\n`\nseed-data.migration.ts\n`import { getCustomRepository, MigrationInterface, QueryRunner } from 'typeorm';\nimport { AccountRepository } from '../modules/account/account.repository';\nimport { SourceRepository } from '../modules/source/source.repository';\n\ntype DataType = {\n  username: string;\n  password: string;\n  sources: { username: string }[];\n};\n\nexport class SeedData1617243952346 implements MigrationInterface {\n  private data: DataType[] = [\n    {\n      username: 'test',\n      password: 'password',\n      sources: [\n        { username: 'some_test' },\n        { username: 'okey_test' },\n        { username: 'super_test' },\n      ],\n    },\n    {\n      username: 'account',\n      password: 'password',\n      sources: [\n        { username: 'some_account' },\n        { username: 'okey_account' },\n        { username: 'super_account' },\n      ],\n    },\n  ];\n\n  public async up(): Promise {\n    await Promise.all(\n      this.data.map(async (item) => {\n        const accountRepository = getCustomRepository(AccountRepository);\n        const accountEntity = accountRepository.create();\n        accountEntity.username = item.username;\n        accountEntity.password = item.password;\n\n        const sourceRepository = getCustomRepository(SourceRepository);\n        const sources = [];\n\n        await Promise.all(\n          item.sources.map(async (sourceItem) => {\n            const sourceEntity = sourceRepository.create();\n            sourceEntity.username = sourceItem.username;\n\n            sources.push(sourceEntity);\n          }),\n        );\n\n        accountEntity.sources = sources;\n        const account = await accountRepository.save(accountEntity);\n        console.log('Account created:', account.id);\n      }),\n    );\n  }\n\n  public async down(): Promise {\n    await Promise.all(\n      this.data.map(async (item) => {\n        const sourceRepository = getCustomRepository(SourceRepository);\n        const accountRepository = getCustomRepository(AccountRepository);\n\n        const account = await accountRepository.findOne({\n          where: { username: item.username },\n        });\n        if (account) {\n          await Promise.all(\n            item.sources.map(async (src) => {\n              const source = await sourceRepository.findOne({\n                where: { username: src.username },\n              });\n              if (source) {\n                await sourceRepository.delete(source);\n              }\n            }),\n          );\n\n          await accountRepository.delete(account);\n        }\n      }),\n    );\n  }\n}\n`\nsource.entity.ts\n`import { Column, Entity, ManyToOne } from 'typeorm';\nimport { AbstractEntity } from '../../common/abstract.entity';\nimport { AccountEntity } from '../account/account.entity';\n\n@Entity({ name: 'source' })\nexport class SourceEntity extends AbstractEntity {\n  @Column({ unique: true })\n  username: string;\n\n  @Column({ default: true })\n  overrideCaption: boolean;\n\n  @ManyToOne(() => AccountEntity, (account) => account.sources)\n  account: AccountEntity;\n}\n`\nError:\n`Error during migration run:\nQueryFailedError: relation \"account\" does not exist\n    at new QueryFailedError (/home/wiha/dev/own/instahub/src/error/QueryFailedError.ts:9:9)\n    at PostgresQueryRunner. (/home/wiha/dev/own/instahub/src/driver/postgres/PostgresQueryRunner.ts:228:19)\n    at step (/home/wiha/dev/own/instahub/node_modules/tslib/tslib.js:143:27)\n    at Object.throw (/home/wiha/dev/own/instahub/node_modules/tslib/tslib.js:124:57)\n    at rejected (/home/wiha/dev/own/instahub/node_modules/tslib/tslib.js:115:69)\n    at processTicksAndRejections (internal/process/task_queues.js:97:5) {\n  length: 106,\n  severity: 'ERROR',\n  code: '42P01',\n  detail: undefined,\n  hint: undefined,\n  position: '13',\n  internalPosition: undefined,\n  internalQuery: undefined,\n  where: undefined,\n  schema: undefined,\n  table: undefined,\n  column: undefined,\n  dataType: undefined,\n  constraint: undefined,\n  file: 'parse_relation.c',\n  line: '1191',\n  routine: 'parserOpenTable',\n  query: 'INSERT INTO \"account\"(\"id\", \"created_at\", \"updated_at\", \"username\", \"password\") VALUES (DEFAULT, DEFAULT, DEFAULT, $1, $2) RETURNING \"id\", \"created_at\", \"updated_at\"',\n  parameters: [\n    'test',\n    '$2b$10$iB6yb3D8e6iGmKoVAJ7eYeYfoItclw5lcVXqauPf9VH94DlDrbuSa'\n  ]\n}\n`\nTables are created in another migration\nDB: PostgreSQL 12.6\nNode: 14.0.0\nTypeORM: 0.2.32\n@nestjs/typeorm: 7.1.5",
      "solution": "When creating a connection through TypeORM you need to pass `synchronize: true,` if you want TypeORM to create schema for you. Alternativly you can manually run sync using the CLI with `schema:sync` command.\nMore\ne.g :\n```\n`\ncreateConnection({\n    type: \"mysql\",\n    host: \"localhost\",\n    port: 3306,\n    username: \"root\",\n    password: \"admin\",\n    database: \"test\",\n    entities: [\n        Photo\n    ],\n\n// ---\n    synchronize: true,\n// ---\n    logging: false\n`\n```\nFrom the docs\n\nsynchronize - Indicates if database schema should be auto created on every application launch. Be careful with this option and don't use this in production - otherwise you can lose production data. This option is useful during debug and development. As an alternative to it, you can use CLI and run schema:sync command. Note that for MongoDB database it does not create schema, because MongoDB is schemaless. Instead, it syncs just by creating indices.",
      "question_score": 20,
      "answer_score": 21,
      "created_at": "2021-04-01T20:38:24",
      "url": "https://stackoverflow.com/questions/66909895/typeorm-queryfailederror-relation-does-not-exist"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "postgresql",
      "question_id": 66052693,
      "title": "Copying postgresql local to remote database (both with password) - ERROR: option &quot;locale&quot; not recognized",
      "problem": "Working with Postgres 12 / Windows 10.\nTrying to copy a remote database to localhost with the following command:\n`pg_dump -C -h remotehost -p 5432 -U postgres remotedb | psql -h localhost -p 5432 -U postgres localdb`\nCMD requests for password 2x.\n`Password for user postgres: Password:`\nI input localhost first, hit ENTER, then input remotehost and hit ENTER again.\nThis is the error I get in return:\n```\n`SET\nSET\nSET\nSET\nSET\n set_config\n------------\n\n(1 row)\n\nSET\nSET\nSET\nSET\nERROR:  option \"locale\" not recognized\nLINE 1: ...ting\" WITH TEMPLATE = template0 ENCODING = 'UTF8' LOCALE = '...\n                                                             ^\nERROR:  database \"remotedb\" does not exist\n\\connect: FATAL:  database \"remotedb\" does not exist\npg_dump: error: could not write to output file: Broken pipe\n`\n```\n\nHow to solve 1st error 'option \"locale\" not recognized\"?\nIs the 2nd error related to how I input the passwords? How should I work when both databases request for passwords?",
      "solution": "You don't need to create the db manually.\nYou can do in a one-liner by using `sed` to replace `LOCALE` with `LC_COLLATE`:\nYour command should look like this:\nNote! This works only if you use script (plain-text) file format for backups\n`pg_dump -C -h remotehost -p 5432 -U postgres remotedb |  sed 's/LOCALE/LC_COLLATE/' | psql -h localhost -p 5432 -U postgres localdb\n`\nExplanation:\nScript dumps are plain-text files containing the SQL commands required to reconstruct the database. When you add `-C` flag to `pg_dump` the dump file will contain the following statement:\n\nPostgres 12 and older\n\n`CREATE DATABASE yourdb WITH TEMPLATE = template0 ENCODING = 'UTF8' LC_COLLATE = 'en_US.UTF-8' LC_CTYPE = 'en_US.UTF-8';\n`\n\nPostgres 13\n\n`CREATE DATABASE yourdb WITH TEMPLATE = template0 ENCODING = 'UTF8' LOCALE = 'en_US.UTF-8';\n`\nBy using `sed` we substitute the `LOCALE` word with `LC_COLLATE` in the pg_dump stream so psql will be able to restore the db locally.\nThis works even if `LC_CTYPE = 'en_US.UTF-8'` is missing.",
      "question_score": 19,
      "answer_score": 20,
      "created_at": "2021-02-04T20:39:51",
      "url": "https://stackoverflow.com/questions/66052693/copying-postgresql-local-to-remote-database-both-with-password-error-option"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "postgresql",
      "question_id": 65631547,
      "title": "Spring Batch / Postgres : ERROR: relation &quot;batch_job_instance&quot; does not exist",
      "problem": "I am trying to configure Spring Batch to use PostGres DB.  I have included the following dependencies in my `build.gradle.kts` file:\n```\n`implementation(\"org.springframework.boot:spring-boot-starter-data-jpa\")\nimplementation(\"org.postgresql:postgresql\")\n`\n```\nMy `application.yml` for my SpringBatch module has the following included:\n```\n`spring:\n  datasource:\n    url: jdbc:postgresql://postgres:5432/springbatchdb\n    username: postgres\n    password: root\n    driverClassName: org.postgresql.Driver\n`\n```\ndocker-compose.yml\n```\n`postgres:\n    restart: always\n    image: postgres:12-alpine\n    container_name: postgres\n    environment:\n      - POSTGRES_USER=postgres\n      - POSTGRES_PASSWORD=root\n      - POSTGRES_DB=springbatchdb\n    ports:\n     - \"5432:5432\"\n    volumes:\n      - postgresql:/var/lib/postgresql\n      - postgresql_data:/var/lib/postgresql/data\n`\n```\nHowever, when I try to add a data file I see the following error in the logs of both my SpringBatch Docker container, and the PostGres container:\nSpring Batch:\n```\n`PostGres:\n```\n`LOG:  database system is ready to accept connections\n2021-01-08 09:54:56.778 UTC [56] ERROR:  relation \"batch_job_instance\" does not exist at character 39\n2021-01-08 09:54:56.778 UTC [56] STATEMENT:  SELECT JOB_INSTANCE_ID, JOB_NAME from BATCH_JOB_INSTANCE where JOB_NAME = $1 and JOB_KEY = $2\n2021-01-08 09:55:27.033 UTC [56] ERROR:  relation \"batch_job_instance\" does not exist at character 39\n2021-01-08 09:55:27.033 UTC [56] STATEMENT:  SELECT JOB_INSTANCE_ID, JOB_NAME from BATCH_JOB_INSTANCE where JOB_NAME = $1 and JOB_KEY = $2\n`\n```\nI can see that the SB server is picking up POSTGRES from my metadata ok.\n```\n`JobRepositoryFactoryBean     : No database type set, using meta data indicating: POSTGRES\n`\n```\nWhat am I missing to get the initial db configured during the server start?\nEdit: I've tried adding `spring.datasource.initialize=true` explicitly, but no change.",
      "solution": "Please check below added in application.yml\n```\n`spring.batch.initialize-schema: always\n`\n```\nPlease check below dependencies are added\n```\n`spring-boot-starter-batch\n`\n```",
      "question_score": 18,
      "answer_score": 22,
      "created_at": "2021-01-08T16:09:36",
      "url": "https://stackoverflow.com/questions/65631547/spring-batch-postgres-error-relation-batch-job-instance-does-not-exist"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "postgresql",
      "question_id": 73757001,
      "title": "connect superset to postgresql in a docker container - The port is closed",
      "problem": "My operating system is Linux.\n\nI am going to connect Superset to PostgreSQL.\n\nPostgreSQL port is open and its value is 5432.\n\nPostgreSQL is also running and not closed.\n\nUnfortunately, after a day of research on the Internet, I could not solve the problem and it gives the following error:\n\n```\n`The port is closed.\n`\n```\n\nDatabase port:\n\ncommand: lsof -i TCP:5432\n\n```\n`python3 13127 user   13u  IPv4 279806      0t0  TCP localhost:40166->localhost:postgresql (ESTABLISHED)\npython3 13127 user   14u  IPv4 274261      0t0  TCP localhost:38814->localhost:postgresql (ESTABLISHED)\n`\n```\nPlease help me, I am a beginner, but I searched a lot and did not get any results.\nSolution\nUse `host.docker.internal` instead of `127.0.0.1` or `localhost` .(thanks pdxrlk)",
      "solution": "Since you're running Superset in a docker container, you can't use 127.0.0.1 nor localhost, since they resolve to the container, not the host.  For the host, use host.docker.internal",
      "question_score": 18,
      "answer_score": 34,
      "created_at": "2022-09-17T19:19:15",
      "url": "https://stackoverflow.com/questions/73757001/connect-superset-to-postgresql-in-a-docker-container-the-port-is-closed"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "postgresql",
      "question_id": 70633841,
      "title": "Django + Docker: connection to server at &quot;localhost&quot; (127.0.0.1), port 5432 failed",
      "problem": "I am trying to run my Django app (Nginx, Gunicorn) in docker.\nBut for request http://167.99.137.32/admin/ I have error: (full log https://pastebin.com/0f8CqCQM)\n```\n`onnection to server at \"localhost\" (127.0.0.1), port 5432 failed: Connection refused\n    Is the server running on that host and accepting TCP/IP connections?\nconnection to server at \"localhost\" (::1), port 5432 failed: Address not available\n    Is the server running on that host and accepting TCP/IP connections?\n`\n```\nI was trying answers from Can't run the server on Django (connection refused) but didn't solve my problem\nsettings.py\n```\n`DATABASES = {\n    'default': {\n        'ENGINE': 'django.db.backends.postgresql_psycopg2',\n        'NAME': 'lk_potok_2',\n        'USER': 'postgres',\n        'PASSWORD': 'post222',\n        'HOST': 'localhost',\n        'PORT': 5432,\n    },\n`\n```\ndocker-compose.yml\n```\n`version: '3.9'\n\nservices:\n  django:\n    build: . # path to Dockerfile\n    command: sh -c \"gunicorn --bind 0.0.0.0:8000 potok.wsgi:application\"\n    volumes:\n      - .:/project\n      - static:/project/static\n    expose:\n      - 8000\n    environment:\n      - DATABASE_URL=postgres://postgres:post222@localhost:5432/lk_potok_2\"\n      - DEBUG=1\n\n  db:\n    image: postgres:13-alpine\n    volumes:\n      - pg_data:/var/lib/postgresql/data/\n    expose:\n      - 5432\n    environment:\n      - POSTGRES_USER=postgres\n      - POSTGRES_PASSWORD=post222\n      - POSTGRES_DB=lk_potok_2\n\n  nginx:\n    image: nginx:1.19.8-alpine\n    depends_on:\n      - django\n    ports:\n      - \"80:80\"\n    volumes:\n      - static:/var/www/html/static\n      - ./nginx-conf.d/:/etc/nginx/conf.d\n\nvolumes:\n    pg_data:\n    static:\n`\n```\nnginx-conf.nginx\n```\n`upstream app {\n    server django:8000;\n}\n\nserver {\n    listen 80;\n    server_name 167.99.137.32;\n\n    location / {\n        proxy_pass http://django:8000;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header Host $host;\n        proxy_redirect off;\n    }\n\n    location /static/ {\n        alias /var/www/html/static/;\n    }\n}\n`\n```\nI was trying sudo systemctl start postgresql and sudo systemctl enable postgresql (the same error)",
      "solution": "The postgres database is no longer running at `localhost`. In your case (since you named the container `db`) it is `db`.\n`DATABASES = {\n    'default': {\n        'ENGINE': 'django.db.backends.postgresql_psycopg2',\n        'NAME': 'lk_potok_2',\n        'USER': 'postgres',\n        'PASSWORD': 'post222',\n        'HOST': 'db',\n        'PORT': 5432,\n    },\n`\nI don't really see why you would add this in here:\n`environment:\n      - DATABASE_URL=postgres://postgres:post222@localhost:5432/lk_potok_2\"\n`\nsince you don't use it in your `settings.py`. But here it wil also have to be `db` instead of `localhost`.\n--EDIT--\nExplanation as why `docker` can recognise the other containers can be found here.",
      "question_score": 18,
      "answer_score": 31,
      "created_at": "2022-01-08T16:34:39",
      "url": "https://stackoverflow.com/questions/70633841/django-docker-connection-to-server-at-localhost-127-0-0-1-port-5432-fail"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "postgresql",
      "question_id": 65646647,
      "title": "Cannot connect to database through Intellij&#39;s Data Sources and Drivers",
      "problem": "I have a problem with displaying my databse structure through Intellij's Data Sources and Drivers page. I can connect to my database which is hosted on a redhat server through using application.properties of my spring project:\n\nspring.datasource.url=jdbc:postgresql://192.168.0.38:5432/cms_database\n\nwhich works fine. Although, whenever I try to use Intellij's Data Sources and Drivers page I get an Error Message:\n\n[3D000] FATAL: database \"cms_database\" does not exist.\n\nI can test to connect to database \"postgres\" which returns a Connection:\n\nDBMS: PostgreSQL (ver. 13.0) Case sensitivity: plain=lower,\ndelimited=exact Driver: PostgreSQL JDBC Driver (ver. 42.2.5, JDBC4.2)\nPing: 16 ms SSL: no\n\nAny help would be appreciated. If you need any extra info I would be happy to post!",
      "solution": "Try to connect with settings from this example\n\nI think you will set hostname wrongly",
      "question_score": 18,
      "answer_score": 12,
      "created_at": "2021-01-09T20:17:39",
      "url": "https://stackoverflow.com/questions/65646647/cannot-connect-to-database-through-intellijs-data-sources-and-drivers"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "postgresql",
      "question_id": 66690321,
      "title": "Flask and Heroku sqlalchemy.exc.NoSuchModuleError: Can&#39;t load plugin: sqlalchemy.dialects:postgres",
      "problem": "When I run\n```\n`heroku run python\n>>> from app.main import app\n>>> app.config['SQLALCHEMY_DATABASE_URI']\n'postgres://' # the database url is passed correctly\n>>> from app.main import db\n>>> db.create_all()\n`\n```\nit gives this error:\n```\n`  Traceback (most recent call last):\n  File \"\", line 1, in \n  File \"/app/.heroku/python/lib/python3.6/site-packages/flask_sqlalchemy/__init__.py\", line 1039, in create_all\n    self._execute_for_all_tables(app, bind, 'create_all')\n  File \"/app/.heroku/python/lib/python3.6/site-packages/flask_sqlalchemy/__init__.py\", line 1031, in _execute_for_all_tables\n    op(bind=self.get_engine(app, bind), **extra)\n  File \"/app/.heroku/python/lib/python3.6/site-packages/flask_sqlalchemy/__init__.py\", line 962, in get_engine\n    return connector.get_engine()\n  File \"/app/.heroku/python/lib/python3.6/site-packages/flask_sqlalchemy/__init__.py\", line 556, in get_engine\n    self._engine = rv = self._sa.create_engine(sa_url, options)\n  File \"/app/.heroku/python/lib/python3.6/site-packages/flask_sqlalchemy/__init__.py\", line 972, in create_engine\n    return sqlalchemy.create_engine(sa_url, **engine_opts)\n  File \"\", line 2, in create_engine\n  File \"/app/.heroku/python/lib/python3.6/site-packages/sqlalchemy/util/deprecations.py\", line 298, in warned\n    return fn(*args, **kwargs)\n  File \"/app/.heroku/python/lib/python3.6/site-packages/sqlalchemy/engine/create.py\", line 520, in create_engine\n    entrypoint = u._get_entrypoint()\n  File \"/app/.heroku/python/lib/python3.6/site-packages/sqlalchemy/engine/url.py\", line 653, in _get_entrypoint\n    cls = registry.load(name)\n  File \"/app/.heroku/python/lib/python3.6/site-packages/sqlalchemy/util/langhelpers.py\", line 342, in load\n    \"Can't load plugin: %s:%s\" % (self.group, name)\nsqlalchemy.exc.NoSuchModuleError: Can't load plugin: sqlalchemy.dialects:postgres\n`\n```\nI'm confused because I'm new to Heroku and Postgresql (been using SQLite until now) and none of the tutorials I'm following explain how it all connects to Flask, only how to do it. So I don't understand what to look at to fix the problem. Is there any other code I should include in the question?\n(Most of the other questions like this one are typos or errors that don't fix this issue.)",
      "solution": "This is due to a change in the sqlalchemy library.  It was an announced deprecation in the changing of the dialect (the part before the ':' in the SQLALCHEMY_DATABASE_URI) name `postgres` to `postgresql`.  They released this breaking change from this github commit with a minor version release, which is in their policy to do so.\nHeroku's default dialect is `postgres` in the DATABASE_URL they provide, which gets translated into the SQLALCHEMY_DATABASE_URI.  Heroku could update their postgres add-on if updating does not break other libraries which might depend on it.\nIn the meantime, you can pin your sqlalchemy library back to \nAlternatively, you can update your code to modify the dialect.",
      "question_score": 17,
      "answer_score": 24,
      "created_at": "2021-03-18T12:37:00",
      "url": "https://stackoverflow.com/questions/66690321/flask-and-heroku-sqlalchemy-exc-nosuchmoduleerror-cant-load-plugin-sqlalchemy"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "postgresql",
      "question_id": 74698839,
      "title": "What does &quot;INSERT 0 1&quot; mean after executing direct SQL in PostgreSQL?",
      "problem": "When I execute a file containing SQL commands with `psql -f`, I get output like this (truncated):\n```\n`CREATE DOMAIN\nCREATE TABLE\nCREATE TABLE\nINSERT 0 1\nINSERT 0 1\nINSERT 0 1\n`\n```\nWhat do the numbers 0 and 1 mean? (I think 1 is the number of rows modified. But 0?) I can't find it in PostgreSQL's official documentation or anywhere else.",
      "solution": "This is documented with the INSERT command\n\nOn successful completion, an INSERT command returns a command tag of the form\n```\n`INSERT oid count\n`\n```\nThe count is the number of rows inserted or updated. oid is always 0 (it used to be the OID assigned to the inserted row if count was exactly one and the target table was declared WITH OIDS and 0 otherwise, but creating a table WITH OIDS is not supported anymore).",
      "question_score": 17,
      "answer_score": 31,
      "created_at": "2022-12-06T08:37:41",
      "url": "https://stackoverflow.com/questions/74698839/what-does-insert-0-1-mean-after-executing-direct-sql-in-postgresql"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "postgresql",
      "question_id": 70643895,
      "title": "How to say Datetime - timestamp without time zone in EF Core 6.0",
      "problem": "I migrate an ASP.NET Core project from 3.1 to 6.0.\nI have copied old migration and pasted it to our new version\nMigration on EF Core 3.1 (old)\n```\n`migrationBuilder.AddColumn(\n                name: \"CalendarStartDate\",\n                table: \"DealOverview\",\n                nullable: false,\n                defaultValue: new DateTime(1, 1, 1, 0, 0, 0, 0, DateTimeKind.Unspecified));\n`\n```\nMigration in EF Core 6.0 (new)\n```\n`migrationBuilder.AlterColumn(\n                name: \"StartDate\",\n                table: \"DealOverview\",\n                type: \"timestamp without time zone\",\n                nullable: false,\n                oldClrType: typeof(DateTime),\n                oldType: \"timestamp with time zone\");\n`\n```\nThe migration fails because this line\n```\n`public DateTime StartDate { get; set; }\n`\n```\nhas changed.\nI went from this package:\n```\n`\n`\n```\nto this package:\n```\n` \n`\n```",
      "solution": "EF Core 6 Npgsql has introduced some breaking changes to timestamp handling logic. You can try to \"revert\" back to old behaviour by adding next line either to `Startup` or `Program` file:\n`AppContext.SetSwitch(\"Npgsql.EnableLegacyTimestampBehavior\", true);\n`\nBut in general it is recommended to migrate to the new behaviour.",
      "question_score": 17,
      "answer_score": 31,
      "created_at": "2022-01-09T18:39:33",
      "url": "https://stackoverflow.com/questions/70643895/how-to-say-datetime-timestamp-without-time-zone-in-ef-core-6-0"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "postgresql",
      "question_id": 68405302,
      "title": "How to insert json data into postgres database table",
      "problem": "I m a beginner and trying to insert JSON values into the database using a tutorial\nI have created the table using the following command\n```\n`CREATE TABLE table_name( id character varying(50),\n                         data json NOT NULL,\n                         active boolean NOT NULL,\n                         created_at timestamp with time zone NOT NULL,\n                         updated_at timestamp with time zone NOT NULL,\n                         CONSTRAINT table_name_pkey PRIMARY KEY (id)\n                       );\n`\n```\nThe table is created with table_name.\nNow I am trying to insert the values into the database:\n```\n`INSERT INTO table_name\nSELECT id,data,active,created_at,updated_at\nFROM json_populate_record (NULL::table_name,\n     '{\n        \"id\": \"1\",\n        \"data\":{\n                \"key\":\"value\"\n                },\n         \"active\":true,\n         \"created_at\": SELECT NOW(),\n         \"updated_at\": SELECT NOW()\n       }'\n  );\n`\n```\nIt throws the following error\nERROR: Invalid input syntax for type JSON '{\nCould anyone help me to resolve and insert the JSON values into the DB?",
      "solution": "You can't include arbitrary SQL commands inside a JSON string. From a JSON \"perspective\" `SELECT NOW()` is an invalid value because it lacks the double quotes. But even if you used `\"select now()\"` that would be executed as a SQL query and replaced with the current timestamp) .\nBut I don't understand why you are wrapping this into a `jsonb_populate_record`. The better solution (at least in my opinion) would be:\n```\n`INSERT INTO table_name (id, data, active, created_at, updated_dat)\nVALUES ('1', '{\"key\": \"value\"}', true, now(), now());\n`\n```\nIf you really want to complicate things, you need to use string concatenation:\n```\n`SELECT id,data,active,created_at,updated_at\nFROM json_populate_record (NULL::table_name,\n     format('{\n        \"id\": \"1\",\n        \"data\":{\n                \"key\":\"value\"\n                },\n         \"active\":true,\n         \"created_at\": \"%s\", \n         \"updated_at\": \"%s\"\n       }', now(), now())::json\n  );\n`\n```",
      "question_score": 17,
      "answer_score": 29,
      "created_at": "2021-07-16T09:42:23",
      "url": "https://stackoverflow.com/questions/68405302/how-to-insert-json-data-into-postgres-database-table"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "postgresql",
      "question_id": 75029181,
      "title": "How to RAISE NOTICE in PostgreSQL?",
      "problem": "I'm using pgAdmin, and I want to have a simple raise notice; referring to this, I entered `RAISE NOTICE 'note';` and got this error:\n```\n`ERROR:  syntax error at or near \"RAISE\"\nLINE 1: RAISE NOTICE 'note';\n`\n```\nThe only way I could manage to get an output was by using this (which I don't understand well either):\n```\n`DO $$\nBEGIN\nRAISE NOTICE 'note';\nEND;\n$$ LANGUAGE plpgsql\n`\n```\nAnd got this output:\n```\n`NOTICE:  note\nDO\n`\n```\nCould someone please explain this?",
      "solution": "Wrap `RAISE` into a procedure\n`create procedure raise_notice (s text) language plpgsql as \n$$\nbegin \n    raise notice '%', s;\nend;\n$$;\n`\nand call it in SQL\n```\n`call raise_notice('note');\n`\n```\nFor PG version before 11 create a function that `returns void` with the same body and `select` from it in SQL\n```\n`select raise_notice('note');\n`\n```",
      "question_score": 16,
      "answer_score": 25,
      "created_at": "2023-01-06T10:35:40",
      "url": "https://stackoverflow.com/questions/75029181/how-to-raise-notice-in-postgresql"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "postgresql",
      "question_id": 73281996,
      "title": "How to access custom schema from supabase-js client?",
      "problem": "I love Supabase but our team needs to use schema functionality that PostgreSQL offers - unfortunately we have been unsuccessfully to make schemas working so far.\nOther info:\n\nPostgreSQL 14\nnewest version of the Supabase JS SDK's\nwe have supabase hosted with Supabase's pro plan\n\nWhat we have tried:\n\nWe created a new schema and added access to all types of authentication (why all? We first thought that maybe there is an error with authentication):\n```\n`CREATE SCHEMA Brano;\nGRANT USAGE ON SCHEMA Brano TO postgres, anon, authenticated, service_role, dashboard_user;\n`\n```\n\nExposed schema to the API via this setting:\n\nTried the following code:\n```\n`var options = {\n    schema: 'brano'\n}\n\nconst supabaseUrl = 'supabaseUrl'\nconst supabaseKey = 'supabaseKey'\nconst supabaseClient = createClient(supabaseUrl, supabaseKey, options);\n\nconst { data, error } = await supabaseClient\n    .from('test_brano')\n    .insert([{\n        data: 123\n}]);\n`\n```\n\nGot this error:\n```\n`{\n    \"message\":\"permission denied for table test_brano\",\n    \"code\":\"42501\",\n    \"details\":null,\n    \"hint\":null\n}\n`\n```\n\nLinks and documentation that we have tried reading (unfortunately we didn't make it work either way):\n\nhttps://github.com/supabase/supabase/discussions/7642\nhttps://github.com/supabase/postgrest-js/issues/280\nhttps://supabase.com/docs/reference/javascript/initializing#with-additional-parameters\nhttps://github.com/supabase/supabase/discussions/1222\n\nDid we missed something?\nThanks in advance!",
      "solution": "In addition to the first two steps you did:\n\nGranting usage:\n`CREATE SCHEMA Brano;\n\nGRANT USAGE \nON SCHEMA Brano \nTO postgres, anon, authenticated, service_role, dashboard_user;\n\nALTER DEFAULT PRIVILEGES IN SCHEMA brano\nGRANT ALL ON TABLES TO postgres, anon, authenticated, service_role, dashboard_user;\n`\n\nExposing the schema in the Settings:\n\nThere's a third step that was missing:\n\nGranting actions to be able to insert/select data:\n`GRANT SELECT, INSERT, UPDATE, DELETE \nON ALL TABLES IN SCHEMA brano \nTO postgres, authenticated, service_role, dashboard_user, anon;\n\nGRANT USAGE, SELECT \nON ALL SEQUENCES IN SCHEMA brano \nTO postgres, authenticated, service_role, dashboard_user, anon;\n`\n\n\u26a0\ufe0f Warning \u26a0\ufe0f\nYou must set these grants again for every new table created in the custom schema.\nThen you can call it as in your example:\nSupabase JS v2:\n```\n`    const options = {\n      db: { schema: 'brano' }\n    };\n    const supabase = createClient(supabaseUrl, SUPABASE_KEY, options)\n    const d = new Date(2018, 11, 24, 10, 33, 30, 0);\n    const { data, error } = await supabase\n      .from('test_brano')\n      .insert([\n        { data: 3, created_at: d }\n      ])\n    console.log(data)\n    if (error) {\n        console.log(\"error getting results\");\n        throw error;\n    }\n`\n```\nSupabase JS v1:\n```\n`    const options = {\n      schema: 'brano'\n    }\n    const supabase = createClient(supabaseUrl, SUPABASE_KEY, options)\n    const d = new Date(2018, 11, 24, 10, 33, 30, 0);\n    const { data, error } = await supabase\n      .from('test_brano')\n      .insert([\n        { data: 3, created_at: d }\n      ])\n    console.log(data)\n    if (error) {\n        console.log(\"error getting results\");\n        throw error;\n    }\n`\n```",
      "question_score": 16,
      "answer_score": 16,
      "created_at": "2022-08-08T19:39:10",
      "url": "https://stackoverflow.com/questions/73281996/how-to-access-custom-schema-from-supabase-js-client"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "postgresql",
      "question_id": 66878455,
      "title": "Configure SQLFluff rules",
      "problem": "I use SQLFluff to ensure a uniform syntax in the company and reduce error warnings before running the models in dbt. Since our syntax does not completely match the syntax of SQLFluff I would like to make some changes.\nThe Rules References provided by SQLFluff helped me to set up Inline Ignoring Errors, as displayed in the code below (last line of code).\nSo I have two questions, which I was not able to answer also with the help of the Rules References of SQLFluff.\n\nI would like to set Rule L032 as default 'false' without typing it manually every time in my SQL.\n\nHow do I change the maximal length of a line regarding Rule L016? I would like to set the default value e.g. 150.\n\n```\n`SELECT\n    country.country_name,\n    country.population,\n    currency.currency_name,\n    currency.currency_id,\n    currency.strange_long_variable_name_which_is_too_long as not_so_long_variable_name\nFROM country\nLEFT JOIN currency\n    USING (country) -- noqa: L032\n`\n```\nI tried to figure it out with the Rules References but could not figure it out. Help is very much appreciated!",
      "solution": "With the help of @suhprano's answer, I was able to find the proper solution for my issue. For this reason, I will post an answer to my own question. I do this intending to provide others assistant with similar issues.\nI created the .sqlfluff file in my user profile folder. In this file I have then included the following:\n```\n`[sqlfluff]\nexclude_rules = L032\n[sqlfluff:rules]\nmax_line_length = 150\n`\n```\nIn this case, SQLFluff will load the configuration from any .sql file found at the path specified on this variable.",
      "question_score": 16,
      "answer_score": 10,
      "created_at": "2021-03-30T23:02:13",
      "url": "https://stackoverflow.com/questions/66878455/configure-sqlfluff-rules"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "postgresql",
      "question_id": 74773611,
      "title": "Cannot open connection after connecting to Postgres 15 using Navicat Premium",
      "problem": "I've encountered into a connection issue between PostgreSQL 15 and Navicat 15.\nMy environment is:\n\nWindows 10\nPostgreSQL 15\nNavicat Premium 15.0.16 (Activated)\n\nHow to produce:\n\nOpen 'New Connection' dialog. (Files -> New Connection -> PostgreSQL...)\nPut information in fields.\nCheck if 'Test Connection' works. (It works on my end as well.)\nClick 'OK'\nIn the list of connections, double-click the connection.\n\nExpected: \nShould be able to open the connection.\nActual: \nIt says the following message on an error dialog.\n```\n`ERROR: column \"datlastsysoid\" does not exist\nLINE 1: SELECT DISTINCT datlastsysoid FROM pg_database\n`\n```",
      "solution": "Postgres 15 removed datlastsysoid field from pg_database table, so any version prior to Navicat 15.0.29 or 16.1 will raise this error while looking for this deprecated field.\nTo fix this, either upgrade to the latest Navicat 15.0.29 or 16.1 and up (might require new license), or do this:\n\nExit Navcat.\nOpen Navicat folder (usually under C:\\Program Files\\PremiumSoft\\Navicat....), depends on your Navicat edition\nLocate libcc.dll and create a backup of this file (copy and paste it as \"libcc-backup.dll\" or any other name)\nOpen this file in any HEX editor, you can use online tool such as https://hexed.it/ if you want.\nSearch for \"SELECT DISTINCT datlastsysoid\" in the file, and replace it with \"SELECT DISTINCT dattablespace\"\nSave the file in the original location. If you get any security issues, save it as \".txt\" file, and then rename it to \".dll\"\nThat's it! Navicat now works as before. If you have ESET or other security tools, the dll file might be locked for few minutes, for security checkup. Be patient, and try after ~5 min again...\n\nEnjoy!",
      "question_score": 15,
      "answer_score": 48,
      "created_at": "2022-12-12T16:41:06",
      "url": "https://stackoverflow.com/questions/74773611/cannot-open-connection-after-connecting-to-postgres-15-using-navicat-premium"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "postgresql",
      "question_id": 78329576,
      "title": "How to declare self-referencing foreign key with Drizzle ORM",
      "problem": "In a Typescript project, declaring a table using Drizzle on postgres-node as follows:\n`\nconst contractsTable = pgTable(\"contracts\", {\n    id: serial(\"id\").primaryKey(),\n    underlyingId: integer(\"underlying_id\").references(() => contractsTable.id),\n    //...\n})\n\n`\nresults in the following Typescript error:\n`'contractsTable' implicitly has type 'any' because it does not have a type annotation and is referenced directly or indirectly in its own initializer.`\nMaintaining a separate type would be impractical because the schema is massive and is subject to change.\nIs there a way to get Typescript to infer the correct type? I failed to do this through aliasing or casting the PgIntegerBuilderInitial type.\nWe also define a relationship as follows:\n```\n`const contractsRelations = relations(\n    contractsTable,\n    ({ one, many }) => ({\n        underlying: one(contractsTable, {\n            fields: [contractsTable.underlyingId],\n            references: [contractsTable.id],\n        }),\n        //...\n    })\n);\n`\n```\nbut I do need the database level constraint. Any ideas?",
      "solution": "You can create a self reference in the table being created by doing\n`export const contractsTable = pgTable(\n  \"contracts\",\n  {\n    id: serial(\"id\").primaryKey(),\n    underlyingId: integer(\"underlying_id\"),\n  },\n  (table) => {\n    return {\n      parentReference: foreignKey({\n        columns: [table.underlyingId],\n        foreignColumns: [table.id],\n        name: \"contracts_underlying_id_fkey\",\n      }),\n    };\n  }\n);\n`\nCorrectly generated the foreign key.\nTaken from Discord conversations and tested on\n```\n`   drizzle-kit 0.21.4\n   drizzle-orm 0.30.10\n`\n```",
      "question_score": 15,
      "answer_score": 15,
      "created_at": "2024-04-15T17:44:39",
      "url": "https://stackoverflow.com/questions/78329576/how-to-declare-self-referencing-foreign-key-with-drizzle-orm"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "postgresql",
      "question_id": 67527280,
      "title": "Get fatal error: server could not be contacted when running pgAdmin4. &#39;NoneType&#39; object has no attribute &#39;value&#39;",
      "problem": "I have a fresh install of windows where I am trying to install Postgres and pgAdmin4.\nI did a fresh install of everything (pgAdmin 4, postrgres 13, etc.) with default settings. When I run pgAdmin 4 I get a \"fatal error: server could not be contacted\" with the following message:\n```\n`pgAdmin Runtime Environment\n--------------------------------------------------------\nPython Path: \"C:\\Program Files\\PostgreSQL\\13\\pgAdmin 4\\python\\python.exe\"\nRuntime Config File: \"C:\\Users\\username\\AppData\\Roaming\\pgadmin\\runtime_config.json\"\npgAdmin Config File: \"C:\\Program Files\\PostgreSQL\\13\\pgAdmin 4\\web\\config.py\"\nWebapp Path: \"C:\\Program Files\\PostgreSQL\\13\\pgAdmin 4\\web\\pgAdmin4.py\"\npgAdmin Command: \"C:\\Program Files\\PostgreSQL\\13\\pgAdmin 4\\python\\python.exe -s C:\\Program Files\\PostgreSQL\\13\\pgAdmin 4\\web\\pgAdmin4.py\"\nEnvironment: \n  - ALLUSERSPROFILE: C:\\ProgramData\n  - APPDATA: C:\\Users\\username\\AppData\\Roaming\n  - CHROME_CRASHPAD_PIPE_NAME: \\\\.\\pipe\\crashpad_4296_YJZESWNMDVNGRRUO\n  - CHROME_RESTART: NW.js|Whoa! NW.js has crashed. Relaunch now?|LEFT_TO_RIGHT\n  - CommonProgramFiles: C:\\Program Files\\Common Files\n  - CommonProgramFiles(x86): C:\\Program Files (x86)\\Common Files\n  - CommonProgramW6432: C:\\Program Files\\Common Files\n  - COMPUTERNAME: pcname\n  - ComSpec: C:\\Windows\\system32\\cmd.exe\n  - configsetroot: C:\\Windows\\ConfigSetRoot\n  - DriverData: C:\\Windows\\System32\\Drivers\\DriverData\n  - HOMEDRIVE: C:\n  - HOMEPATH: \\Users\\username\n  - LOCALAPPDATA: C:\\Users\\username\\AppData\\Local\n  - LOGONSERVER: \\\\pcname\n  - NUMBER_OF_PROCESSORS: 8\n  - NVM_HOME: C:\\Users\\username\\AppData\\Roaming\\nvm\n  - NVM_SYMLINK: C:\\Program Files\\nodejs\n  - OneDrive: C:\\Users\\username\\OneDrive\n  - OS: Windows_NT\n  - Path: C:\\Windows\\system32;C:\\Windows;C:\\Windows\\System32\\Wbem;C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\;C:\\Windows\\System32\\OpenSSH\\;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;C:\\Program Files\\Git\\cmd;C:\\Program Files\\PostgreSQL\\12\\bin;C:\\Users\\username\\AppData\\Roaming\\nvm;C:\\Program Files\\nodejs;C:\\Users\\username\\AppData\\Local\\Microsoft\\WindowsApps;C:\\Users\\username\\AppData\\Local\\Programs\\Microsoft VS Code\\bin\n  - PATHEXT: .COM;.EXE;.BAT;.CMD;.VBS;.VBE;.JS;.JSE;.WSF;.WSH;.MSC\n  - PGADMIN_INT_KEY: 2e023387-9c82-4cdd-af2f-c82562c6e4fc\n  - PGADMIN_INT_PORT: 50567\n  - PGADMIN_SERVER_MODE: OFF\n  - PROCESSOR_ARCHITECTURE: AMD64\n  - PROCESSOR_IDENTIFIER: Intel64 Family 6 Model 140 Stepping 1, GenuineIntel\n  - PROCESSOR_LEVEL: 6\n  - PROCESSOR_REVISION: 8c01\n  - ProgramData: C:\\ProgramData\n  - ProgramFiles: C:\\Program Files\n  - ProgramFiles(x86): C:\\Program Files (x86)\n  - ProgramW6432: C:\\Program Files\n  - PSModulePath: C:\\Program Files\\WindowsPowerShell\\Modules;C:\\Windows\\system32\\WindowsPowerShell\\v1.0\\Modules\n  - PUBLIC: C:\\Users\\Public\n  - SystemDrive: C:\n  - SystemRoot: C:\\Windows\n  - TEMP: C:\\Users\\username\\AppData\\Local\\Temp\n  - TMP: C:\\Users\\username\\AppData\\Local\\Temp\n  - USERDOMAIN: pcname\n  - USERDOMAIN_ROAMINGPROFILE: pcname\n  - USERNAME: username\n  - USERPROFILE: C:\\Users\\username\n  - windir: C:\\Windows\n--------------------------------------------------------\n\nTraceback (most recent call last):\n  File \"C:\\Program Files\\PostgreSQL\\13\\pgAdmin 4\\web\\pgAdmin4.py\", line 98, in \n    app = create_app()\n  File \"C:\\Program Files\\PostgreSQL\\13\\pgAdmin 4\\web\\pgadmin\\__init__.py\", line 347, in create_app\n    if not os.path.exists(SQLITE_PATH) or get_version() == -1:\n  File \"C:\\Program Files\\PostgreSQL\\13\\pgAdmin 4\\web\\pgadmin\\setup\\db_version.py\", line 19, in get_version\n    return version.value\nAttributeError: 'NoneType' object has no attribute 'value'\n`\n```\nNote: 'username' and 'pcname' are placeholders that I substituted for the actual strings, some of which were originally capitalized.\nVerified service \"postgresql-x64-13\" was running. I continue to get this message despite trying the following:\n\nRunning as administrator.\nDeleting\n`C:\\Users\\username\\AppData\\Roaming\\pgadmin`.\nAdding `C:\\Program Files\\PostgreSQL\\13\\bin` to the `PATH` environment variable (system and\nuser)\nRestarting the service.\nUninstalling/reinstalling Postgres 13.\nUninstalling/installing Postgres 12.\nRestarting my machine.\n\nThe one thing that yielded different results was stopping the service, deleting C:\\Users\\username\\AppData\\Roaming\\pgadmin, restarting the service, then running pg4admin which resulted in a \"fatal error\" with the following output:\n```\n`pgAdmin Runtime Environment\n--------------------------------------------------------\nPython Path: \"C:\\Program Files\\PostgreSQL\\13\\pgAdmin 4\\python\\python.exe\"\nRuntime Config File: \"C:\\Users\\username\\AppData\\Roaming\\pgadmin\\runtime_config.json\"\npgAdmin Config File: \"C:\\Program Files\\PostgreSQL\\13\\pgAdmin 4\\web\\config.py\"\nWebapp Path: \"C:\\Program Files\\PostgreSQL\\13\\pgAdmin 4\\web\\pgAdmin4.py\"\npgAdmin Command: \"C:\\Program Files\\PostgreSQL\\13\\pgAdmin 4\\python\\python.exe -s C:\\Program Files\\PostgreSQL\\13\\pgAdmin 4\\web\\pgAdmin4.py\"\nEnvironment: \n  - ALLUSERSPROFILE: C:\\ProgramData\n  - APPDATA: C:\\Users\\username\\AppData\\Roaming\n  - CHROME_CRASHPAD_PIPE_NAME: \\\\.\\pipe\\crashpad_10312_CHCJAQAYYFQQIAGB\n  - CHROME_RESTART: NW.js|Whoa! NW.js has crashed. Relaunch now?|LEFT_TO_RIGHT\n  - CommonProgramFiles: C:\\Program Files\\Common Files\n  - CommonProgramFiles(x86): C:\\Program Files (x86)\\Common Files\n  - CommonProgramW6432: C:\\Program Files\\Common Files\n  - COMPUTERNAME: pcname\n  - ComSpec: C:\\Windows\\system32\\cmd.exe\n  - configsetroot: C:\\Windows\\ConfigSetRoot\n  - DriverData: C:\\Windows\\System32\\Drivers\\DriverData\n  - HOMEDRIVE: C:\n  - HOMEPATH: \\Users\\username\n  - LOCALAPPDATA: C:\\Users\\username\\AppData\\Local\n  - LOGONSERVER: \\\\pcname\n  - NUMBER_OF_PROCESSORS: 8\n  - NVM_HOME: C:\\Users\\username\\AppData\\Roaming\\nvm\n  - NVM_SYMLINK: C:\\Program Files\\nodejs\n  - OneDrive: C:\\Users\\username\\OneDrive\n  - OS: Windows_NT\n  - Path: C:\\Windows\\system32;C:\\Windows;C:\\Windows\\System32\\Wbem;C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\;C:\\Windows\\System32\\OpenSSH\\;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;C:\\Program Files\\Git\\cmd;C:\\Program Files\\PostgreSQL\\13\\bin;C:\\Users\\username\\AppData\\Roaming\\nvm;C:\\Program Files\\nodejs;C:\\Users\\username\\AppData\\Local\\Microsoft\\WindowsApps;C:\\Users\\username\\AppData\\Local\\Programs\\Microsoft VS Code\\bin;C:\\Users\\username\\AppData\\Roaming\\nvm;C:\\Program Files\\nodejs;C:\\Program Files\\PostgreSQL\\13\\bin;\n  - PATHEXT: .COM;.EXE;.BAT;.CMD;.VBS;.VBE;.JS;.JSE;.WSF;.WSH;.MSC\n  - PGADMIN_INT_KEY: cb6cf8e1-ca8b-416b-985f-1fd1a945ff06\n  - PGADMIN_INT_PORT: 50906\n  - PGADMIN_SERVER_MODE: OFF\n  - PROCESSOR_ARCHITECTURE: AMD64\n  - PROCESSOR_IDENTIFIER: Intel64 Family 6 Model 140 Stepping 1, GenuineIntel\n  - PROCESSOR_LEVEL: 6\n  - PROCESSOR_REVISION: 8c01\n  - ProgramData: C:\\ProgramData\n  - ProgramFiles: C:\\Program Files\n  - ProgramFiles(x86): C:\\Program Files (x86)\n  - ProgramW6432: C:\\Program Files\n  - PSModulePath: C:\\Program Files\\WindowsPowerShell\\Modules;C:\\Windows\\system32\\WindowsPowerShell\\v1.0\\Modules\n  - PUBLIC: C:\\Users\\Public\n  - SESSIONNAME: Console\n  - SystemDrive: C:\n  - SystemRoot: C:\\Windows\n  - TEMP: C:\\Users\\username\\AppData\\Local\\Temp\n  - TMP: C:\\Users\\username\\AppData\\Local\\Temp\n  - USERDOMAIN: pcname\n  - USERDOMAIN_ROAMINGPROFILE: pcname\n  - USERNAME: username\n  - USERPROFILE: C:\\Users\\username\n  - windir: C:\\Windows\n--------------------------------------------------------\n\nNOTE: Configuring authentication for DESKTOP mode.\n\nTraceback (most recent call last):\n  File \"C:\\Program Files\\PostgreSQL\\13\\pgAdmin 4\\web\\pgAdmin4.py\", line 91, in \n    exec(open(file_quote(setup_py), 'r').read())\n  File \"\", line 506, in \n  File \"\", line 378, in setup_db\n  File \"C:\\Program Files\\PostgreSQL\\13\\pgAdmin 4\\web\\pgadmin\\__init__.py\", line 351, in create_app\n    db_upgrade(app)\n  File \"C:\\Program Files\\PostgreSQL\\13\\pgAdmin 4\\web\\pgadmin\\setup\\db_upgrade.py\", line 25, in db_upgrade\n    flask_migrate.upgrade(migration_folder)\n  File \"C:\\Program Files\\PostgreSQL\\13\\pgAdmin 4\\python\\lib\\site-packages\\flask_migrate\\__init__.py\", line 96, in wrapped\n    f(*args, **kwargs)\n  File \"C:\\Program Files\\PostgreSQL\\13\\pgAdmin 4\\python\\lib\\site-packages\\flask_migrate\\__init__.py\", line 271, in upgrade\n    command.upgrade(config, revision, sql=sql, tag=tag)\n  File \"C:\\Program Files\\PostgreSQL\\13\\pgAdmin 4\\python\\lib\\site-packages\\alembic\\command.py\", line 294, in upgrade\n    script.run_env()\n  File \"C:\\Program Files\\PostgreSQL\\13\\pgAdmin 4\\python\\lib\\site-packages\\alembic\\script\\base.py\", line 490, in run_env\n    util.load_python_file(self.dir, \"env.py\")\n  File \"C:\\Program Files\\PostgreSQL\\13\\pgAdmin 4\\python\\lib\\site-packages\\alembic\\util\\pyfiles.py\", line 97, in load_python_file\n    module = load_module_py(module_id, path)\n  File \"C:\\Program Files\\PostgreSQL\\13\\pgAdmin 4\\python\\lib\\site-packages\\alembic\\util\\compat.py\", line 182, in load_module_py\n    spec.loader.exec_module(module)\n  File \"\", line 783, in exec_module\n  File \"\", line 219, in _call_with_frames_removed\n  File \"C:\\Program Files\\PostgreSQL\\13\\pgAdmin 4\\web\\pgadmin\\setup\\..\\..\\migrations\\env.py\", line 93, in \n    run_migrations_online()\n  File \"C:\\Program Files\\PostgreSQL\\13\\pgAdmin 4\\web\\pgadmin\\setup\\..\\..\\migrations\\env.py\", line 86, in run_migrations_online\n    context.run_migrations()\n  File \"\", line 8, in run_migrations\n  File \"C:\\Program Files\\PostgreSQL\\13\\pgAdmin 4\\python\\lib\\site-packages\\alembic\\runtime\\environment.py\", line 813, in run_migrations\n    self.get_context().run_migrations(**kw)\n  File \"C:\\Program Files\\PostgreSQL\\13\\pgAdmin 4\\python\\lib\\site-packages\\alembic\\runtime\\migration.py\", line 561, in run_migrations\n    step.migration_fn(**kw)\n  File \"C:\\Program Files\\PostgreSQL\\13\\pgAdmin 4\\web\\migrations\\versions\\fdc58d9bd449_.py\", line 122, in upgrade\n    Security(current_app, user_datastore, register_blueprint=False)\n  File \"C:\\Program Files\\PostgreSQL\\13\\pgAdmin 4\\python\\lib\\site-packages\\flask_security\\core.py\", line 1062, in __init__\n    self._state = self.init_app(\n  File \"C:\\Program Files\\PostgreSQL\\13\\pgAdmin 4\\python\\lib\\site-packages\\flask_security\\core.py\", line 1102, in init_app\n    self._state = state = _get_state(app, datastore, **kwargs)\n  File \"C:\\Program Files\\PostgreSQL\\13\\pgAdmin 4\\python\\lib\\site-packages\\flask_security\\core.py\", line 608, in _get_state\n    remember_token_serializer=_get_serializer(app, \"remember\"),\n  File \"C:\\Program Files\\PostgreSQL\\13\\pgAdmin 4\\python\\lib\\site-packages\\flask_security\\core.py\", line 593, in _get_serializer\n    return URLSafeTimedSerializer(secret_key=secret_key, salt=salt)\n  File \"C:\\Program Files\\PostgreSQL\\13\\pgAdmin 4\\python\\lib\\site-packages\\itsdangerous\\serializer.py\", line 104, in __init__\n    self.secret_keys: _t.List[bytes] = _make_keys_list(secret_key)\n  File \"C:\\Program Files\\PostgreSQL\\13\\pgAdmin 4\\python\\lib\\site-packages\\itsdangerous\\signer.py\", line 64, in _make_keys_list\n    return [want_bytes(s) for s in secret_key]\nTypeError: 'NoneType' object is not iterable\n`\n```\nI recently installed this on a different machine without any issues. Seems odd that a fresh build would have the issues. Almost makes me think I am lacking something the older build contains (python?), although this shouldn't be the case. There are lots of posts about this and I have tried everything suggested in them without resolution.",
      "solution": "As Delsx pointed out in the comments, the standalone pgAdmin can be installed without the same issues from pgadmin.org/download. It appears this is a widespread bug that has been reported on the pgAdmin bug list, here and here",
      "question_score": 15,
      "answer_score": 12,
      "created_at": "2021-05-14T01:13:46",
      "url": "https://stackoverflow.com/questions/67527280/get-fatal-error-server-could-not-be-contacted-when-running-pgadmin4-nonetype"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "postgresql",
      "question_id": 73858980,
      "title": "Postgres suddenly raise error &#39;/usr/lib/libpq.5.dylib&#39; (no such file)",
      "problem": "when I run Django project or any code related to Postgres :\n```\n`Referenced from: '/Users/mahmoudnasser/.local/share/virtualenvs/wyspp_backend-PwdII1PB/lib/python3.8/site-packages/psycopg2/_psycopg.cpython-38-darwin.so'\n  Reason: tried: '/opt/homebrew/opt/postgresql/lib/libpq.5.dylib' (no such file), '/usr/local/lib/libpq.5.dylib' (no such file), '/usr/lib/libpq.5.dylib' (no such file)\n`\n```\nI tried many solutions online but none of them worked.\nNote: I use MacOS",
      "solution": "To solve this problem just run the following command:\n```\n`sudo mkdir -p /usr/local/lib && sudo ln -s /opt/homebrew/opt/postgresql@14/lib/postgresql@14/libpq.5.dylib /usr/local/lib/libpq.5.dylib\n`\n```",
      "question_score": 14,
      "answer_score": 19,
      "created_at": "2022-09-26T21:24:46",
      "url": "https://stackoverflow.com/questions/73858980/postgres-suddenly-raise-error-usr-lib-libpq-5-dylib-no-such-file"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "postgresql",
      "question_id": 66563805,
      "title": "How to insert a null foreign key in gorm?",
      "problem": "I have a transaction table in gorm that looks like this:\n```\n`type Transaction struct {\n    gorm.Model\n    UserID      string `gorm:\"index\"`\n    TradeID     int \n    Trade       Trade\n    ExternalID  string\n    Amount      float32\n    Type        string\n    Description string\n}\n`\n```\nAnd I'm trying to insert a transaction without a trade:\n```\n`DB.Create(&Transaction{UserID: \"user-1\", Type: \"unblock\", Amount:  50})\n`\n```\nThis fails because the Transaction structs defaults the int value of the key to 0 so the insert fails at the db level because I don't have a trade with id = 0.\nHow can I do this?",
      "solution": "You can change the `TradeID` to a pointer, so the default value will be `nil`.\n```\n`TradeID     *int \n`\n```",
      "question_score": 14,
      "answer_score": 19,
      "created_at": "2021-03-10T12:26:23",
      "url": "https://stackoverflow.com/questions/66563805/how-to-insert-a-null-foreign-key-in-gorm"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "postgresql",
      "question_id": 73866587,
      "title": "prisma Error: p1001: Can&#39;t reach database server at `db.xocheossqzkirwnhzxxm.supabase.co`:`5432`",
      "problem": "I started learning about prisma and supabase and would like to implement both technologies in my Next.js app. After running `npx prisma migrate dev --name init` I was faced with the following error:\n```\n`Environment variables loaded from .env                                                                                                                                            \nPrisma schema loaded from prisma\\schema.prisma\nDatasource \"db\": PostgreSQL database \"postgres\", schema \"public\" at \"db.xocheossqzkirwnhzxxm.supabase.co:5432\"\n\nError: P1001: Can't reach database server at `db.xocheossqzkirwnhzxxm.supabase.co`:`5432`\n\nPlease make sure your database server is running at `db.xocheossqzkirwnhzxxm.supabase.co`:`5432`.\n`\n```\nmy password to the db does not contain any special characters here is my schema.prisma file:\n```\n`// This is your Prisma schema file,\n// learn more about it in the docs: https://pris.ly/d/prisma-schema\n\ngenerator client {\n  provider = \"prisma-client-js\"\n}\n\ndatasource db {\n  provider = \"postgresql\"\n  url      = env(\"DATABASE_URL\")\n}\n\nmodel Home{\n  id        String @id @default(cuid())\n  image     String?\n  title     String\n  description String\n  price     Float\n  guests    Int\n  beds      Int\n  baths     Int\n  createdAt DateTime @default(now())\n  updateAt  DateTime @updatedAt\n}\n`\n```\nhere is my .env:\n```\n`DATABASE_URL=\"postgresql://postgres:[YOUR-PASSWORD]@db.xocheossqzkirwnhzxxm.supabase.co:5432/postgres\"\n`\n```",
      "solution": "I had a similar issue with a Sveltekit application using Prisma and PlanetScale (MySQL) and Docker on Windows (WSL). I received the same error but did not have this issue when I ran it directly from terminal nor when I connected through the mysql cli.\nSolution\nI ensured that my Docker Node version was the same as on WSL (16.15), I have noticed others have had this issue with different versions of Node so it is worth exploring this. I then added `connect_timeout=300` to my SQL URL to prevent the connection from timing out too early.\nI include more details in my other answer on Stackoverflow.",
      "question_score": 13,
      "answer_score": 16,
      "created_at": "2022-09-27T13:06:02",
      "url": "https://stackoverflow.com/questions/73866587/prisma-error-p1001-cant-reach-database-server-at-db-xocheossqzkirwnhzxxm-sup"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "postgresql",
      "question_id": 71309715,
      "title": "How to reload PostgreSQL configuration",
      "problem": "I am trying to enable the transaction logs in PostgreSQL, where query run time is more than 3 seconds, by setting this flag\n```\n`log_min_duration_statement = 3000\n`\n```\nHow to reload the configuration file without restarting the Postgres?\nI've tried the below options and got these errors\n```\n`1) postgres=# select pg_reload_config();\n**ERROR:  function pg_reload_config() does not exist**\nLINE 1: select pg_reload_config();\n2) postgres@ospostgresql:/$ /usr/lib/postgresql/11/bin/pg_ctl reload\npg_ctl: no database directory specified and environment variable PGDATA unset\nTry \"pg_ctl --help\" for more information.\n`\n```",
      "solution": "```\n`1) postgres=# select pg_reload_conf();\n\n2) \n    1) su postgres\n    2) /usr/lib/postgresql/11/bin/pg_ctl -D /var/lib/postgresql/11/main reload\n`\n```\nIncorrect:\n```\n`select pg_reload_config();\n**ERROR:  function pg_reload_config() does not exist**\n**-->** This is resolved by giving correct function name\n\npostgres@ospostgresql:/$ /usr/lib/postgresql/11/bin/pg_ctl reload\npg_ctl: no database directory specified and environment variable PGDATA unset\nTry \"pg_ctl --help\" for more information.\n**-->** This error is resolved by changing the user to postgres\n`\n```\nThank you @a_horse_with_no_name in helping to identify the typo which causes issue-1",
      "question_score": 13,
      "answer_score": 21,
      "created_at": "2022-03-01T15:04:28",
      "url": "https://stackoverflow.com/questions/71309715/how-to-reload-postgresql-configuration"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "postgresql",
      "question_id": 67816057,
      "title": "How to set not null constraint to columns in postgres",
      "problem": "I tried to set null to columns like following.\n`ALTER TABLE myschema.table ALTER COLUMN (test_id,type) SET NOT NULL;`\nBut it returned syntax error like ` Syntax error at or near Line 3, Position 47`\nAre there any proper way to achieve this ?\nIf someone has opinion please let me know.\nThanks",
      "solution": "You can't provide a list of column in parentheses, you need to use multiple ALTER COLUMN options separated by a comma:\n```\n`ALTER TABLE the_table\n    ALTER COLUMN test_id set not null, \n    ALTER COLUMN type SET NOT NULL;\n`\n```",
      "question_score": 13,
      "answer_score": 18,
      "created_at": "2021-06-03T07:52:53",
      "url": "https://stackoverflow.com/questions/67816057/how-to-set-not-null-constraint-to-columns-in-postgres"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "postgresql",
      "question_id": 70172127,
      "title": "How to generate a UUID field with FastAPI, SQLalchemy, and SQLModel",
      "problem": "I'm struggling to get the syntax to create a UUID field when creating a model in my FastAPI application. I'm using SQLModel.\nSo basically, my models.py file looks like this:\n`from datetime import datetime\nfrom typing import Optional\nimport uuid\n\nfrom sqlalchemy import Column, DateTime\nfrom sqlalchemy.dialects import postgresql as psql\nfrom sqlmodel import SQLModel, Field\n\nclass ModelBase(SQLModel):\n    \"\"\"\n    Base class for database models.\n    \"\"\"\n    id: Optional[int] = Field(default=None, primary_key=True)\n    created_at: datetime = Field(sa_column=Column(DateTime(timezone=True), default=datetime.utcnow))\n    updated_at: datetime = Field(sa_column=Column(DateTime(timezone=True),\n                                 onupdate=datetime.utcnow, default=datetime.utcnow))\n\nclass UUIDModelBase(ModelBase, table=True):\n    \"\"\"\n    Base class for UUID-based models.\n    \"\"\"\n    uuid: uuid.UUID = Field(sa_column=Column(psql.UUID(as_uuid=True)), default=uuid.uuid4)\n\n`\nThe above errors out with\n```\n`AttributeError: 'FieldInfo' object has no attribute 'UUID'\n`\n```\nI also tried\n`id: uuid.UUID = Column(psql.UUID(as_uuid=True), default=uuid.uuid4)\nTypeError: Boolean value of this clause is not defined\n`\nAlso\n`uuid: uuid.UUID = Column(psql.UUID(as_uuid=True), default=uuid.uuid4)\nAttributeError: Neither 'Column' object nor 'Comparator' object has an attribute 'UUID'\n`\nand\n`uuid: uuid.UUID = Field(default_factory=uuid.uuid4, index=True, nullable=False)\n AttributeError: 'FieldInfo' object has no attribute 'UUID'\n`\nYou get the idea. The errors are not helping me, I just need the right syntax.\nIn this case, I'm not actually looking to use UUID as a primary key. And as you can tell from the imports, I'm using postgreSQL. The database is based on a postgres:12 docker image.",
      "solution": "The interpreter might be using `UUID` of your actual field `uuid` instead of the imported package. So, you can change the code as follows.\n`import uuid as uuid_pkg\n\nfrom sqlalchemy import Field\nfrom sqlmodel import Field\n\nclass UUIDModelBase(ModelBase):\n    \"\"\"\n    Base class for UUID-based models.\n    \"\"\"\n    uuid: uuid_pkg.UUID = Field(\n        default_factory=uuid_pkg.uuid4,\n        primary_key=True,\n        index=True,\n        nullable=False,\n    )\n`\nReference: https://github.com/tiangolo/sqlmodel/issues/140",
      "question_score": 13,
      "answer_score": 21,
      "created_at": "2021-11-30T16:46:19",
      "url": "https://stackoverflow.com/questions/70172127/how-to-generate-a-uuid-field-with-fastapi-sqlalchemy-and-sqlmodel"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "postgresql",
      "question_id": 77149123,
      "title": "Enable Postgis extension for PostgreSQL 16 on Mac M2",
      "problem": "I'm trying to install PostgreSQL and Postgis extension on a Mac with M2 processor.\nI've tried to follow various online instructions and had no luck so far.\nBelow are my installation steps:\n\nI've installed PostgreSQL 16 using the Installer from https://www.postgresql.org/download/macosx/. The installation path for the PostgreSQL is at: /Library/PostgreSQL/16/\n\nThen I use 'brew install postgis' to install the postgis extension. It's installed at: /opt/homebrew/Cellar/postgis/3.3.4_1.\n\nExecute 'CREATE EXTENSION postgis;' in PostgreSQL and receive: Detail: Could not open extension control file \"/Library/PostgreSQL/16/share/postgresql/extension/postgis.control\": No such file or directory.\n\nIt seems that the postgis is installed into the brew location that PostgreSQL won't be able to locate. Wondering if anyone can tell me if i miss any obvious steps in the installation.\nI also tried:\n\nadd \"shared_preload_libraries = 'postgis'\" into PostgreSQL config file. But since PostgreSQL can't locate the postgis in /Library/PostgreSQL/16/share/postgresql/extension/. Server won't start after the config is changed.\n\nSome online link suggested to install postgis via Stack Builder. That option is not available in the latest version.\n\nIt seems that I should get postgis 3.4.0 installed for PostgreSQL 16. Brew seems to install 3.3.4 by default for me for some reason. Wondering if this could be the cause of my issue.",
      "solution": "Currently, the brew postgis formula is linked to PostgreSQL 14, but you can manually compile PostGIS for PostgreSQL 16 on a Mac M2.\n`brew install postgresql@16\n\nbrew install postgis\n# or install required libs/packages manually (brew install geos gdal libxml2 sfcgal ...)\n\nsudo ln -s /opt/homebrew/Cellar/postgresql@16/16.0/bin/postgres /usr/local/bin/postgres\n\nwget https://download.osgeo.org/postgis/source/postgis-3.4.0.tar.gz\ntar -xvzf postgis-3.4.0.tar.gz\nrm postgis-3.4.0.tar.gz\ncd postgis-3.4.0\n\n./configure --with-projdir=/opt/homebrew/opt/proj --with-protobufdir=/opt/homebrew/opt/protobuf-c --with-pgconfig=/opt/homebrew/opt/postgresql@16/bin/pg_config --with-jsondir=/opt/homebrew/opt/json-c --with-sfcgal=/opt/homebrew/opt/sfcgal/bin/sfcgal-config --with-pcredir=/opt/homebrew/opt/pcre \"LDFLAGS=$LDFLAGS -L/opt/homebrew/Cellar/gettext/0.22.2/lib\" \"CFLAGS=-I/opt/homebrew/Cellar/gettext/0.22.2/include\"\n\nmake\nmake install\n`",
      "question_score": 13,
      "answer_score": 11,
      "created_at": "2023-09-21T11:53:23",
      "url": "https://stackoverflow.com/questions/77149123/enable-postgis-extension-for-postgresql-16-on-mac-m2"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "postgresql",
      "question_id": 77802646,
      "title": "How to determine which pg_restore version to use with a given dump file?",
      "problem": "I get the following error when running `pg_restore`\n```\n`pg_restore: error: unsupported version (1.15) in file header\n`\n```\nVersions the Docker container (running image `postgres:14.10`) provides\n```\n`root@c33fdb601eab:/# postgres --version\npostgres (PostgreSQL) 14.10 (Debian 14.10-1.pgdg120+1)\nroot@c33fdb601eab:/# pg_restore --version\npg_restore (PostgreSQL) 14.10 (Debian 14.10-1.pgdg120+1)\n`\n```\nSo what does this version `1.15` mean, please? Does it indicate the *.sql dump file version? If so, how do I know which `pg_restore` version to use to be able to restore it?\nPS: clearly there are many similar posts already, but I didn't find an answer explaining how to match a dump file version to a `pg_restore` version.",
      "solution": "It indicates the custom format version. The quickest way I found to find out is do:\n`head  `\nIt should have at the top something like:\n`test 14.10 (Ubuntu 14.10-1.pgdg22.04+1) 16.1 (Ubuntu 16.1-1.pgdg22.04+1).`\nThis is not the custom file format it just indicates a 14.10 instance of Postgres was dumped with a 16.1 version of pg_dump. You then know you have to use 16+ version of pg_restore. In other words pg_restore is backwards compatible but cannot restore a file that came from a pg_dump version that is newer then itself.",
      "question_score": 13,
      "answer_score": 11,
      "created_at": "2024-01-11T20:44:30",
      "url": "https://stackoverflow.com/questions/77802646/how-to-determine-which-pg-restore-version-to-use-with-a-given-dump-file"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "postgresql",
      "question_id": 71879806,
      "title": "How can I specify the migrations directory for typeorm CLI?",
      "problem": "After the new typeorm release I am having some troubles to work with migrations.\nSome time ago I was using this code and it worked -\n```\n`entities: ['./src/modules/**/infra/typeorm/entities/*.ts'],\nmigrations: ['./src/shared/infra/typeorm/migrations/*.ts'],\ncli: {\n  migrationsDir: './src/shared/infra/typeorm/migrations'\n}\n`\n```\nBut now I can't specify the CLI property. To create a new migration, I have to specify the entire migration path -\n```\n`npm run typeorm migration:create ./src/database/migrations -n SomeTest\n`\n```\nIs there another way to do that without specifying the entire path?",
      "solution": "As Jun 2022, the docs is outdated `-n MigrationName` is no longer supported. You can do this instead:\ntypescript esm:\n`npx typeorm-ts-node-esm migration:create src/database/migration/MigrationFileName` where `MigrationFileName` is the filename you want to create and `src/database/migration/` is the path.\ntypescript commonjs:\n`npx typeorm-ts-node-commonjs migration:create`\nP.S This might be late but this could save others.\nP.S I just discover this myself. If this doesn't work in the future, let me know, so I would know as well.",
      "question_score": 12,
      "answer_score": 21,
      "created_at": "2022-04-15T05:36:39",
      "url": "https://stackoverflow.com/questions/71879806/how-can-i-specify-the-migrations-directory-for-typeorm-cli"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "postgresql",
      "question_id": 67923628,
      "title": "How to handle conditional prepared statements using prisma and postgresql?",
      "problem": "I have a search query that its parameters changes depending on the client input.\n```\n`await prisma.$queryRaw(`SELECT column FROM table ${condition ? `WHERE column = '${condition}'`  :' ' } `) \n`\n```\nhow can I write this query using prepared statement and avoiding duplicate queries. The only solution I came up with is the following:\n```\n`const result = condition ? await prisma.$queryRaw(`SELECT column FROM table WHERE column = $1`,condition) : await prisma.$queryRaw(`SELECT column FROM table`)\n`\n```\nThe goal from this is to avoid sql injections from the first query.\nEDIT\nafter trying the solution suggested by @Ryan I got the following error:\n```\n`Raw query failed. Code: `22P03`. Message: `db error: ERROR: incorrect binary data format in bind parameter 1`\n`\n```\nhere's my implementation:\n```\n`    const where = Prisma.sql`WHERE ${searchConditions.join(' AND ')}`;\n    const fetchCount = await prisma.$queryRaw`\n    SELECT \n      COUNT(id)\n    FROM\n      table\n    ${searchConditions.length > 0 ? where : Prisma.empty}\n  `;\n`\n```\nthat will translate to the following in the prisma logs:\n```\n`Query: \n    SELECT \n      COUNT(id)\n    FROM\n      table\n    WHERE $1\n   [\"column = something\"]\n`\n```\nSOLUTION\nI had to do a lot of rework to achieve what I want. Here's the idea behind it:\nfor every search condition you need to do the following:\n```\n`let queryCondition = Prisma.empty;\n    if (searchFilter) {\n      const searchFilterCondition = Prisma.sql`column = ${searchFilter}`;\n\n      queryCondition.sql.length > 0\n        ? (queryCondition = Prisma.sql`${queryCondition} AND ${streamingUnitCondition}`)\n        : (queryCondition = searchFilterCondition);\n    }\n`\n```\nafterwards in the final search query you can do something of this sort:\n```\n`SELECT COUNT(*) FROM table ${queryCondition.sql.length > 0 ? Prisma.sql`WHERE ${queryCondition}` : Prisma.empty}\n`\n```",
      "solution": "You can do it like this:\n```\n`import { Prisma } from '@prisma/client'\n\nconst where = Prisma.sql`where column = ${condition}`\n\nconst result = await prisma.$queryRaw`SELECT column FROM table ${condition ? where : Prisma.empty}`\n`\n```",
      "question_score": 12,
      "answer_score": 22,
      "created_at": "2021-06-10T16:51:18",
      "url": "https://stackoverflow.com/questions/67923628/how-to-handle-conditional-prepared-statements-using-prisma-and-postgresql"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "postgresql",
      "question_id": 69546892,
      "title": "How to CREATE FUNCTION IF NOT EXISTS?",
      "problem": "Is there are an easy way to do CREATE FUNCTION IF NOT EXISTS?\nI have multiple schemas and I am preparing a script that will create missing objects in the target schema. The plan is to run a script to check if the object exists, do nothing if it doesn't it will create it. 'CREATE SOMETHING IF NOT EXISTS' perfectly working with tables sequences and others, however cannot find the solution for functions.\nI am from Tsql world and it has this checking.\nHowever, looks like Postgres 9.6 doesn't have it. Is there any easy way around this limitation?",
      "solution": "You may wrap function definitions with anonymous block and handle duplicate name exception:\n\n```\n`create function f(int)\nreturns int\nlanguage sql\nas 'select $1';\n`\n```\n\u2713\n\n```\n`do $$\nbegin\n  create function f (int)\n  returns int\n  language sql\n  as 'select $1';\nend; $$\n`\n```\n```\n\nERROR:  function \"f\" already exists with same argument types\nCONTEXT:  SQL statement \"create function f (int)\n returns int\n language sql\n as 'select $1'\"\nPL/pgSQL function inline_code_block line 3 at SQL statement\n\n```\n\n```\n`do $$\nbegin\n    create function f (int)\n    returns int\n    language sql\n    as 'select $1';\n    \n  exception\n    when duplicate_function then\n    null;\nend; $$\n`\n```\n\u2713\n\ndb<>fiddle here",
      "question_score": 12,
      "answer_score": 9,
      "created_at": "2021-10-12T22:52:45",
      "url": "https://stackoverflow.com/questions/69546892/how-to-create-function-if-not-exists"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "postgresql",
      "question_id": 69113494,
      "title": "Can not upgrade Aurora RDS Postgres version from 12 to 13",
      "problem": "I am trying to upgrade Aurora RDS Postgres cluster db.r5.xlarge from version 12.7 to 13.3.\nI choose Engine version 13.3, default DB cluster parameter group and DB parameter group default.aurora-postgresql13 and choose 'Apply immediately'.\nReceived error message:\n\nWe're sorry, your request to modify DB cluster clone-cluster has failed.\nCannot modify engine version because instance clone-cluster is running on an old configuration.\nApply any pending maintenance actions on the instance before proceeding with the upgrade\n\nThere are no any pending maintenance actions showing in AWS RDS console and I have no idea what configuration they mean.\nWe are on a free basic support plan no so we cannot get help from AWS. Can anyone please suggest if there's a way to upgrade the whole cluster at once?",
      "solution": "This is not correctly documented at the moment, but it is possible to check and resolve (for those of us automating the upgrade process.) The CLI `aws rds describe-pending-maintenance-actions` sometimes reports the status.  If you know ARNs, you can filter on them, or if just a name, this seems to work:\n```\n`aws rds describe-pending-maintenance-actions --query 'PendingMaintenanceActions[?contains(ResourceIdentifier, `test`)`\n`\n```\nThe maintenance can be applied programmatically using apply-pending-maintenance-action.\nHowever...\nThis message also appears to occur when something about the upgrade path is invalid.  In my case, I believe the case is as follows:\n\nSource is 9.6.19 Aurora PostgreSQL\nTarget is 10.18.  This fails.\nSet target to 10.14 and upgrade occurs\n\nIn short, the UI, and other information about what versions of what are compatible is incomplete.  I have reported this to AWS and they mostly shrugged, telling me to look at the doc.  Pffft.",
      "question_score": 12,
      "answer_score": 1,
      "created_at": "2021-09-09T08:51:14",
      "url": "https://stackoverflow.com/questions/69113494/can-not-upgrade-aurora-rds-postgres-version-from-12-to-13"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "postgresql",
      "question_id": 76555305,
      "title": "Postgres container failed to start with initdb error. popen failure: Cannot allocate memory",
      "problem": "I'm using `postgres:12` Docker image on AWS instance under Ubuntu 20.04.\n```\n`  postgres-tests:\n    image: \"postgres:12\"\n    restart: always\n    command: postgres -c 'max_connections=200'\n    environment:\n      POSTGRES_DB: \"${POSTGRES_DATABASE}\"\n      POSTGRES_USER: \"${POSTGRES_USER}\"\n      POSTGRES_PASSWORD: \"${POSTGRES_PASSWORD}\"\n    ports:\n      - \"8396:5432\"\n\n`\n```\nWhen running this container with `docker-compose up -d` it fails to start with the following error:\n```\n`postgres-tests_1  | popen failure: Cannot allocate memory\npostgres-tests_1  | initdb: error: The program \"postgres\" is needed by initdb but was not found in the\npostgres-tests_1  | same directory as \"/usr/lib/postgresql/12/bin/initdb\".\npostgres-tests_1  | Check your installation.\n\n`\n```\nThe error appeared suddenly after most-resent project deploy. The important thing is that the error happens only with this particular container. There is one more `postgresql:12` container on the machine for another project, which works fine.\nHERE IS WHAT I TRIED:\n\nI've found several suggestions related to increasing the `shmall`/`shmmax` params controlling shared memory on the machine.\n\nBut these system params are already set to high value:\n```\n`ubuntu@ip-172-31-10-246:/var/www$ cat /proc/sys/kernel/shmall\n18446744073692774399\nubuntu@ip-172-31-10-246:/var/www$ cat /proc/sys/kernel/shmmax\n18446744073692774399\nubuntu@ip-172-31-10-246:/var/www$ \n\n`\n```\n\nThe second suggested option was to try a newer postgres image. Tested with postgres 13.0, 14.0 with no effect.\nUPDATE\nTried with `postgres:11` image and it runs OK, but I can not roll down postgres version in production, so it's not a solution in my case.\n\nI tried to stop/start and reboot the instance, also cleaning up docker cache with `docker system prune` and `docker volumes prune`.\n\n```\n`Software:\nUbuntu 20.04.2\nDocker version 20.10.8, build 3967b7d\n\nInstance hardware:\nIntel(R) Xeon(R) Platinum 8124M CPU @ 3.00GHz \nRAM:  4/8GB Used by system and other services\nSwap: 4.5/20GB Used by system and other services\n`\n```",
      "solution": "Maintainers of the postgres docker images have updated the underlying OS image layer:\nPrevious: Debian 11 (bullseye)\nNew: Debian 12 (bookworm)\nThis was pushed up to hub.docker.com on 15 Jun 2023.\nThe move to this container is causing a lot of builds to throw errors and exceptions like the one noted in the question:\n\n`initdb: error: The program \"postgres\" is needed by initdb but was not found in the same directory as \"/usr/lib/postgresql/12/bin/initdb\".`\n\nThe mitigation route is to use the following conatiner:\n\n`postgres:12-bullseye`\n\nThe maintainers will keep providing `bullseye` variants until the Postgres version is EOL or Debian moves to their latest `Trixie`\nIssues can be found here:\n\nPostgres:12 container is not starting anymore #1103\nPostgres container failing with initdb error: `program \u201cpostgres\u201d is needed by initdb but was not found`",
      "question_score": 12,
      "answer_score": 19,
      "created_at": "2023-06-26T11:20:54",
      "url": "https://stackoverflow.com/questions/76555305/postgres-container-failed-to-start-with-initdb-error-popen-failure-cannot-allo"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "postgresql",
      "question_id": 66391481,
      "title": "How to force COMMIT inside function so other sessions can see updated row?",
      "problem": "In a Postgres 12 database, I have multiple queries (`SELECT`, `UPDATE`, ...) in a function that all together take about 20 minutes to complete.\nI have a check at the top that does an `UPDATE` if `status` is not running:\n```\n`create or replace function aaa.fnc_work() returns varchar as \n$body$\n    begin\n        if (select count(*) from aaa.monitor where id='invoicing' and status='running')=0 then\n           return 'running';\n        else\n           update aaa.monitor set status='running' where id='invoicing';\n        end if;\n        --- rest of code ---\n        --finally\n        update aaa.monitor set status='idle' where id='invoicing';\n        return '';\n    exception when others then\n         return SQLERRM::varchar;\n    end\n$body$\nlanguage plpgsql;\n`\n```\nThe idea is to prevent other users from executing the `--- rest of code ---` until `status` is idle.\nHowever, it seems the updated status is not seen by others (calling the same function) who also go ahead and start executing `--- rest of code ---`. How do I force a commit after:\n\nupdate aaa.monitor set status='running' `where id='invoicing'`;\n\nSo that all other user sessions can see the updated `status` and exit accordingly.\nDo I need a transaction?",
      "solution": "Keep reading. I preserved the best for last.\nProof of concept with a `PROCEDURE`\nA Postgres `FUNCTION` is always atomic (runs inside a single transaction). `COMMIT` is disallowed. You could use tricks with `dblink` to work around this (in Postgres 10 or older). See:\n\nDoes Postgres support nested or autonomous transactions?\nHow do I do large non-blocking updates in PostgreSQL?\n\nConsider a `PROCEDURE` instead. Introduced with Postgres 11. There you can manage transactions:\n`CREATE OR REPLACE PROCEDURE aaa.proc_work(_id text, INOUT _result text = NULL)\n  LANGUAGE plpgsql AS\n$proc$\nBEGIN\n   -- optionally assert steering row exists\n   PERFORM FROM aaa.monitor WHERE id = _id FOR KEY SHARE SKIP LOCKED;\n\n   IF NOT FOUND THEN   \n      RAISE EXCEPTION 'monitor.id = % not found or blocked!', quote_literal(_id);\n   END IF;\n\n   -- try UPDATE\n   UPDATE aaa.monitor\n   SET    status = 'running'\n   WHERE  id = _id                   -- assuming valid _id\n   AND    status <> 'running';       -- assuming \"status\" is NOT NULL\n\n   IF NOT FOUND THEN\n      _result := 'running'; RETURN;  -- this is how you return with INOUT params\n   END IF;\n\n   COMMIT;                           -- HERE !!!\n\n   BEGIN                             -- start new code block\n\n      ----- code for big work HERE -----\n      -- PERFORM 1/0;                -- debug: test exception?\n      -- PERFORM pg_sleep(5);        -- debug: test concurrency?\n\n      _result := '';\n\n   -- also catching QUERY_CANCELED and ASSERT_FAILURE\n   -- is a radical step to try and release 'running' rows no matter what\n   EXCEPTION WHEN OTHERS OR QUERY_CANCELED OR ASSERT_FAILURE THEN\n      -- ROLLBACK;                   -- roll back (unfinished?) big work\n      _result := SQLERRM;\n   END;                              -- end of nested block\n\n   UPDATE aaa.monitor                -- final reset\n   SET    status = 'idle'\n   WHERE  id = _id\n   AND    status <> 'idle';          -- only if needed\nEND\n$proc$;\n`\nCall (important!):\n```\n`CALL aaa.proc_work('invoicing');  -- stand-alone call!\n`\n```\nImportant notes\nAdd `COMMIT` after the `UPDATE`. After that, concurrent transactions can see the updated row.\nThere is no additional `BEGIN` or `START TRANSACTION`. The manual:\n\nIn procedures invoked by the `CALL` command as well as in anonymous code\nblocks (`DO` command), it is possible to end transactions using the\ncommands `COMMIT` and `ROLLBACK`. A new transaction is started\nautomatically after a transaction is ended using these commands, so\nthere is no separate `START TRANSACTION` command. (Note that `BEGIN` and\n`END` have different meanings in PL/pgSQL.)\n\nWe need a separate PL/pgSQL code block, because you have a custom exception handler, and (quoting the manual):\n\nA transaction cannot be ended inside a block with exception handlers.\n\n(But we can `COMMIT` / `ROLLBACK` in the `EXCEPTION` handler.)\nYou cannot call this procedure inside an outer transaction, or together with any other DML statement, which would force an outer transaction wrapper. Has to be a stand-alone `CALL`. See:\n\nUnable to Create COMMIT inside PostgreSQL 11.5 Procedure\n\nNote the final `UPDATE aaa.monitor SET status = 'idle' WHERE ...`. Else the (committed!) `status` would remain 'running' indefinitely after an exception.\nAbout returning a value from a procedure:\n\nHow to return a value from a stored procedure (not function)?\n\nI added `DEFAULT NULL` to the `INOUT` parameter, so you don't have to provide an argument with the call.\n`UPDATE` directly. If the row is 'running', no update occurs. (This also fixes the logic: your `IF` expression seems backwards as it returns 'running' when no row with `status='running'` is found. Seems like you'd want the opposite.)\nI added an (optional!) assert to make sure the row in table `aaa.monitor` exists. Adding a `FOR KEY SHARE` lock to also eliminate the tiny time window for a race conditions between the assert and the following `UPDATE`. The lock conflicts with deletion or updating the PK column - but not with updating the `status`. So the exception is never raised in normal operation! The manual:\n\nCurrently, the set of columns considered for the `UPDATE` case are\nthose that have a unique index on them that can be used in a foreign\nkey (so partial indexes and expressional indexes are not considered),\nbut this may change in the future.\n\n`SKIP LOCK` to not wait in case of a conflicting lock. The added exception should never occur. Just demonstrating a water-tight proof of concept.\nYour update revealed 25 rows in `aaa.monitor`, so I added the parameter `_id`.\nSuperior approach\nThe above might make sense to persist more information for the world to see. To just queue operations, there are much more efficient solutions. Work with a lock instead, which is \"visible\" to others instantaneously. Then you don't need a nested transaction to begin with, and a plain `FUNCTION` will do:\n`CREATE OR REPLACE FUNCTION aaa.fnc_work(_id text)\n  RETURNS text\n  LANGUAGE plpgsql AS\n$func$\nBEGIN\n   -- optionally assert that the steering row exists\n   PERFORM FROM aaa.monitor WHERE id = _id FOR KEY SHARE SKIP LOCKED;\n\n   IF NOT FOUND THEN   \n      RAISE EXCEPTION 'monitor.id = % not found or blocked!', quote_literal(_id);\n   END IF;\n\n   -- lock row\n   PERFORM FROM aaa.monitor WHERE id = _id FOR NO KEY UPDATE SKIP LOCKED;\n\n   IF NOT FOUND THEN\n      -- we made sure the row exists, so it must be locked\n      RETURN 'running';\n   END IF;\n\n   ----- code for big work HERE -----\n   -- PERFORM 1/0;                -- debug: test exception?\n   -- PERFORM pg_sleep(5);        -- debug: test concurrency?\n\n   RETURN '';\n\nEXCEPTION WHEN OTHERS THEN\n   RETURN SQLERRM;\n\nEND\n$func$;\n`\nCall:\n```\n`SELECT aaa.fnc_work('invoicing');\n`\n```\nThe call can be nested any way you want. As long as one transaction is working on the big job, no other will start.\nAgain, the optional assert takes out a `FOR KEY SHARE` lock to eliminate the time window for a race condition, and the added exception should never occur in normal operation.\nWe don't need the column `status` at all for this. The row-lock itself is the gatekeeper. Hence the empty `SELECT` list in `PERFORM FROM aaa.monitor ...`. Collateral benefit: this also doesn't produce dead tuples by updating the row back and forth. If you still need to update `status` for some other reason, you are back to the visibility issue of the previous chapter. You can combine both ...\nAbout `PERFORM`:\n\nSELECT or PERFORM in a PL/pgSQL function\n\nAbout the row lock:\n\nPostgres UPDATE \u2026 LIMIT 1",
      "question_score": 12,
      "answer_score": 23,
      "created_at": "2021-02-26T20:08:01",
      "url": "https://stackoverflow.com/questions/66391481/how-to-force-commit-inside-function-so-other-sessions-can-see-updated-row"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "postgresql",
      "question_id": 69706677,
      "title": "Cannot start PostgreSQL Docker container \u2013 &quot;&#39;/docker-entrypoint-initdb.d/&#39;: Operation not permitted&quot;",
      "problem": "Trying to start a PostgreSQL container according to the instructions at https://hub.docker.com/_/postgres (How to use this image \u2192 start a postgres instance),\n`docker run -e POSTGRES_PASSWORD=mysecretpassword postgres:14\n`\ngives the following error:\n\nls: cannot access '/docker-entrypoint-initdb.d/': Operation not permitted\n\nThe only change was removing the `--name` and `-d` parameter while using the version tag `14` of PostgreSQL. But even with the exact same command from Docker Hub the same error shows up.\nWhy is that and how can it be fixed? Is it a bug in the PostgreSQL image or a system configuration issue?\n\nAdditional information:\n`$ docker version\nClient:\n Version:    17.12.0-ce\n API version:    1.35\n Go version:    go1.9.2\n Git commit:    c97c6d6\n Built:    Wed Dec 27 20:10:45 2017\n OS/Arch:    linux/amd64\n\nServer:\n Engine:\n  Version:    17.12.0-ce\n  API version:    1.35 (minimum version 1.12)\n  Go version:    go1.9.2\n  Git commit:    c97c6d6\n  Built:    Wed Dec 27 20:09:19 2017\n  OS/Arch:    linux/amd64\n  Experimental:    false\n\n$ uname -r\n5.13.0-16-generic\n\n$ cat /etc/lsb-release\nDISTRIB_ID=Ubuntu\nDISTRIB_RELEASE=21.10\nDISTRIB_CODENAME=impish\nDISTRIB_DESCRIPTION=\"Ubuntu 21.10\"\n\n$ docker images postgres:latest\nREPOSITORY          TAG                 IMAGE ID            CREATED             SIZE\npostgres            latest              14e58c3f6369        6 days ago          374MB\n$ docker images postgres:14\nREPOSITORY          TAG                 IMAGE ID            CREATED             SIZE\npostgres            14                  14e58c3f6369        6 days ago          374MB\n`\nIt looks like it works with `postgres:14-alpine`.",
      "solution": "I bumped into the same issue.\nPostgreSQL Docker tags `13` and `14` seem to be using Debian's `bullseye` which seems to change things in regards to the file system.\nAt the moment there are two solutions:\n\nDowngrade to PostgreSQL `13-buster`, i.e. Docker tag `postgres:13.4-buster`, as it seems `14` does not have a `-buster` equivalent.\nUpgrade current Docker you are running. From Docker version onwards `20.10.6`, it seems to fix the issue.\n\nAs a reference to the issue on GitHub related to this issue, you can find it at root user has no permissions within container #884 .\nFor posterity, the solution from GitHub:\n\nyou'll need to update Docker, runc, and likely libseccomp on your\nhost.",
      "question_score": 12,
      "answer_score": 15,
      "created_at": "2021-10-25T12:46:22",
      "url": "https://stackoverflow.com/questions/69706677/cannot-start-postgresql-docker-container-docker-entrypoint-initdb-d-oper"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "postgresql",
      "question_id": 72222070,
      "title": "Postgres and Docker Compose; password authentication fails and role &#39;postgres&#39; does not exist. Cannot connect from pgAdmin4",
      "problem": "I have a docker-compose that brings up the psql database as below, currently I'm trying to connect to it with pgAdmin4 (not in a docker container) and be able to view it. I've been having trouble authenticating with the DB and I don't understand why.\ndocker-compose\n```\n`version: \"3\"\n\nservices:\n  # nginx and server also have an override, but not important for this q.\n  nginx:\n    ports:\n      - 1234:80\n      - 1235:443\n  server:\n    build: ./server\n    ports:\n      - 3001:3001 # app server port\n      - 9230:9230 # debugging port\n    env_file: .env\n    command: yarn dev\n    volumes:\n      # Mirror local code but not node_modules\n      - /server/node_modules/\n      - ./server:/server\n  \n  database:\n    container_name: column-db\n    image: 'postgres:latest'\n    restart: always\n    ports:\n      - 5432:5432\n    environment:\n      POSTGRES_USER: postgres # The PostgreSQL user (useful to connect to the database)\n      POSTGRES_PASSWORD: root # The PostgreSQL password (useful to connect to the database)\n      POSTGRES_DB: postgres # The PostgreSQL default database (automatically created at first launch)\n    volumes:\n      - ./db-data/:/var/lib/postgresql/data/\n\nnetworks:\n  app-network:\n    driver: bridge\n`\n```\nI do `docker-compose up` then check the logs, and it says that it is ready for connections. I go to pgAdmin and enter the following:\n\nwhere password is `root`. I then get this error:\n```\n`FATAL:  password authentication failed for user \"postgres\"\n`\n```\nI check the docker logs and I see\n```\n`DETAIL:  Role \"postgres\" does not exist.\n`\n```\nI'm not sure what I'm doing wrong, according to the docs the super user should be created with those specifications. Am I missing something? Been banging my head against this for an hour now. Any help is appreciated!",
      "solution": "@jjanes solved it in a comment, I had used a mapped volume and never properly set up the db. Removed the volume and we're good to go.",
      "question_score": 12,
      "answer_score": 19,
      "created_at": "2022-05-12T23:08:19",
      "url": "https://stackoverflow.com/questions/72222070/postgres-and-docker-compose-password-authentication-fails-and-role-postgres-d"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "postgresql",
      "question_id": 76465657,
      "title": "How do i create custom Postgres enum types in Rust SQLx?",
      "problem": "I am trying to create a custom enum type in Postgres and have done so successfully. My migration looks like this:\n```\n`CREATE TYPE role AS ENUM ('admin', 'user');\n\nALTER TABLE users\nADD role role DEFAULT 'user';\n`\n```\nThen i have created the enum type in Rust like this:\n```\n`#[derive(Serialize, Deserialize, Debug, sqlx::Type)]\n#[sqlx(type_name = \"role\", rename_all = \"lowercase\")] \npub enum Role {\n    ADMIN,\n    USER\n}\n`\n```\nAnd i have altered the user model also:\n```\n`#[derive(sqlx::FromRow, Debug)]\npub struct User {\n    pub id: i32,\n    pub email: String,\n    pub username: String,\n    pub password: String,\n    pub role: Role,\n    pub created_at: DateTime,\n    pub updated_at: DateTime,\n}\n`\n```\nBut now when i try to query the database like this:\n```\n`let user = match sqlx::query_as!(\n    User,\n    \"SELECT * FROM users WHERE email = $1 AND password = $2\",\n    &email,\n    &password,\n)\n`\n```\nI get this error: `unsupported type role of column #7 (\"role\")`\nWhat am i doing wrong?\nI have tried playing around with the macro part\n`#[sqlx(type_name = \"role\", rename_all = \"lowercase\")]`,\nBut that does not seem to help.\nHere is the full error from `cargo check`:\n```\n`error: unsupported type role of column #7 (\"role\")\n   --> src/routes/auth/mod.rs:140:20\n    |\n140 |           let user = match sqlx::query_as!(\n    |  __________________________^\n141 | |             User,\n142 | |             \"SELECT * FROM users WHERE email = $1 AND password = $2\",\n143 | |             &payload.email,\n144 | |             &hash,\n145 | |         )\n    | |_________^\n    |\n    = note: this error originates in the macro `$crate::sqlx_macros::expand_query` which comes from the expansion of the macro `sqlx::query_as` (in Nightly builds, run with -Z macro-backtrace for more info)\n\nerror: could not compile `rust-api-example` (bin \"rust-api-example\") due to previous error\n`\n```",
      "solution": "I ended up fixing like this:\n```\n`let user = sqlx::query_as!(\n    model::User,\n    \"SELECT\n    id,\n    email,\n    username,\n    password,\n    role AS \\\"role: model::Role\\\",\n    created_at,\n    updated_at\n    FROM users WHERE id = $1\",\n    user_id\n)\n.fetch_one(&pool)\n.await\n.unwrap();\n`\n```",
      "question_score": 12,
      "answer_score": 4,
      "created_at": "2023-06-13T15:51:40",
      "url": "https://stackoverflow.com/questions/76465657/how-do-i-create-custom-postgres-enum-types-in-rust-sqlx"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "postgresql",
      "question_id": 76268799,
      "title": "How should I declare enums in SQLAlchemy using mapped_column (to enable type hinting)?",
      "problem": "I am trying to use `Enums` in SQLAlchemy 2.0 with `mapped_column`.  So far I have the following code (taken from another question):\n`from sqlalchemy.dialects.postgresql import ENUM as pgEnum\nimport enum\n\nclass CampaignStatus(str, enum.Enum):\n    activated = \"activated\"\n    deactivated = \"deactivated\"\n\nCampaignStatusType: pgEnum = pgEnum(\n    CampaignStatus,\n    name=\"campaignstatus\",\n    create_constraint=True,\n    metadata=Base.metadata,\n    validate_strings=True,\n)\n\nclass Campaign(Base):\n    __tablename__ = \"campaign\"\n\n    id: Mapped[UUID] = mapped_column(primary_key=True, default=uuid4)\n    created_at: Mapped[dt.datetime] = mapped_column(default=dt.datetime.now)\n    status: Mapped[CampaignStatusType] = mapped_column(nullable=False)\n`\nHowever, that gives the following error upon the construction of the  `Campaign` class itself.\n```\n`Traceback (most recent call last):\n  File \"\", line 27, in \n    class Campaign(Base):\n...\nAttributeError: 'ENUM' object has no attribute '__mro__'\n`\n```\nAny hint about how to make this work?\nThe response from ENUM type in SQLAlchemy with PostgreSQL does not apply as I am using version 2 of SQLAlchemy and those answers did not use `mapped_column` or `Mapped` types. Also, removing `str` from `CampaignStatus` does not help.",
      "solution": "The crux of the issue relating to `__mro__` causing the `AttributeError` is that `CampaignStatusType` is not a class, but rather an instance variable of type `sqlalchemy.dialects.postgresql.ENUM` (using `pyright` may verify this - given that it complains about `Mapped[CampaignStatusType]` being an \"Illegal type annotation: variable not allowed unless it is a type alias\").  As a test, replacing the type annotation for `status` with `Mapped[CampaignStatus]` does resolve the issue (and `pyright` reports no errors), but that does not hook the column type to the enum with postgresql dialect that is desired.\nSo the only way around this while using the dialect specific enum type is to use the non-annotated construct:\n`    status = mapped_column(CampaignStatusType, nullable=False)\n`\nHowever, if type annotation is still desired, i.e. whatever being `Mapped` must be a type, and that `sqlalchemy.dialects.postgresql.ENUM` (which was imported as `pgEnum`) is the underlying type for the instance `CampaignStatusType`, it may be thought that the following might be a solution\n`    # don't do this erroneous example despite it does run\n    status: Mapped[sqlalchemy.dialects.postgresql.ENUM] = mapped_column(\n        CampaignStatusType,\n        nullable=False,\n    )\n`\nWhile it works, it does NOT actually reflect what will be represented by the data, so DO NOT actually do that.  Moreover, it only works because the type annotation is ignored when the specific column type is passed, so putting anything in there will work while having an invalid type.\nNow, given that SQLAlchemy is now 2.0 (as the question explicitly want this newer version), perhaps reviewing the documentation and see now native enums should be handled now.\nAdapting the examples in the documentation, the following MVCE may now be derived, using all the intended keyword arguments that was passed to the PostgreSQL dialect specific ENUM type passed generic `sqlalchemy.Enum` instead (aside from `metadata=Base.metadata` as that's completely superfluous):\n`from typing import Literal\nfrom typing import get_args\nfrom sqlalchemy import Enum\nfrom sqlalchemy.orm import DeclarativeBase\nfrom sqlalchemy.orm import Mapped\nfrom sqlalchemy.orm import mapped_column\n\nCampaignStatus = Literal[\"activated\", \"deactivated\"]\n\nclass Base(DeclarativeBase):\n    pass\n\nclass Campaign(Base):\n    __tablename__ = \"campaign\"\n    id: Mapped[int] = mapped_column(primary_key=True, autoincrement=True)\n    status: Mapped[CampaignStatus] = mapped_column(Enum(\n        *get_args(CampaignStatus),\n        name=\"campaignstatus\",\n        create_constraint=True,\n        validate_strings=True,\n    ))\n`\nNote the use of `typing.get_args` on `CampaignStatus` and splat it to the `Enum` here as opposed to what the official examples have done in repeating themselves.  Now to include the usage:\n`from sqlalchemy import create_engine\nfrom sqlalchemy.orm import sessionmaker\n\ndef main():\n    engine = create_engine('postgresql://postgres@localhost/postgres')\n    Base.metadata.create_all(engine)\n    Session = sessionmaker(bind=engine)\n    session = Session()\n    session.add(Campaign(status='activated'))\n    session.add(Campaign(status='deactivated'))\n    session.commit()\n\n    s = 'some_unvalidated_string'\n    try:\n        session.add(Campaign(status=s))\n        session.commit()\n    except Exception:\n        print(\"failed to insert with %r\" % s)\n\nif __name__ == '__main__':\n    main()\n`\nThe above will produce `failed to insert with 'some_unvalidated_string'` as the output, showing that unvalidated strings will not be inserted, while validated strings that are mapped to some enum are inserted without issues.  Moreover, `pyright` will not produce errors (though honestly, this is not necessarily a good metric because type hinting in Python is still fairly half-baked, as `pyright` did not detect the erroneous example as an error in the very beginning no matter what went inside `Mapped`, but I digress).\nViewing the newly created entities using `psql`\n`postgres=# select * from campaign;\n id |   status    \n----+-------------\n  1 | activated\n  2 | deactivated\n(2 rows)\n`\n`postgres=# \\dt campaign;\n                                Table \"public.campaign\"\n Column |      Type      | Collation | Nullable |               Default                \n--------+----------------+-----------+----------+--------------------------------------\n id     | integer        |           | not null | nextval('campaign_id_seq'::regclass)\n status | campaignstatus |           | not null | \nIndexes:\n    \"campaign_pkey\" PRIMARY KEY, btree (id)\n\npostgres=# \\dT+ campaignstatus;\n                                             List of data types\n Schema |      Name      | Internal name  | Size |  Elements   |  Owner   | Access privileges | Description \n--------+----------------+----------------+------+-------------+----------+-------------------+-------------\n public | campaignstatus | campaignstatus | 4    | activated  +| postgres |                   | \n        |                |                |      | deactivated |          |                   | \n(1 row)\n`\nThe enum of course cannot be dropped without dropping the `campaign` table:\n`postgres=# drop type campaignstatus;\nERROR:  cannot drop type campaignstatus because other objects depend on it\nDETAIL:  column status of table campaign depends on type campaignstatus\nHINT:  Use DROP ... CASCADE to drop the dependent objects too.\n`\nSo the enum more or less behaves as expected despite only using generic SQLAlchemy types, without needing dialect specific imports.",
      "question_score": 12,
      "answer_score": 18,
      "created_at": "2023-05-17T07:26:42",
      "url": "https://stackoverflow.com/questions/76268799/how-should-i-declare-enums-in-sqlalchemy-using-mapped-column-to-enable-type-hin"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "postgresql",
      "question_id": 76149300,
      "title": "EF Core can not handle DateTimeOffset when using an SQLite connection",
      "problem": "I can not generate a SQLite query from EF Core 7.0.4 when the query contains a `DateTimeOffset`.\nE.g., the following is not working:\n`var startDate1 = DateTimeOffset.UtcNow;\nvar query = ctx.Runs\n    .Where(r => r.StartDate > startDate1);\n\nvar sql = query.ToQueryString();\n`\nThis is the error message:\n\nThe LINQ expression 'DbSet()\n.Where(r => r.StartDate > __startDate1_0)' could not be translated. Either rewrite the query in a form that can be translated, or switch to client evaluation explicitly by inserting a call to 'AsEnumerable', 'AsAsyncEnumerable', 'ToList', or 'ToListAsync'. See https://go.microsoft.com/fwlink/?linkid=2101038 for more information.\n\nWhen I remove the `DateTimeOffset` from the where clause, everything is working fine:\n`var id = 1;\nvar query = ctx.Runs\n    .Where(r => r.Id > id);\n\nvar sql = query.ToQueryString();\n`\nThe entity looks like this:\n`public class Run : CarApiModel.Entities.Run\n{\n    public DateTimeOffset StartDate { get; set; }\n    public DateTimeOffset EndDate { get; set; }\n    public int Test { get; set; }\n\n    public new Session Session { get; set; }\n    public new Driver Driver { get; set; }\n    public new RunningClock StartClock { get; set; }\n    public new RunningClock? EndClock { get; set; }\n}\n`\nNote: Querying for the `Test` property works without issues as well:\n`var query = _uiContext.Runs\n    .Where(r => r.Test \nThe `CarApiModel.Entities.Run` looks like this:\n`public class Run\n{\n    public long Id { get; set; }\n    public long StartSecondsSinceClockStart { get; set; }\n    public int StartNanoseconds { get; set; }\n    public long? EndSecondsSinceClockStart { get; set; }\n    public int? EndNanoseconds { get; set; }\n    public string? Description { get; set; }\n\n    public long SessionId { get; set; }\n    public long DriverId { get; set; }\n    public long StartClockId { get; set; }\n    public long? EndClockId { get; set; }\n\n    public virtual Session Session { get; set; }\n    public virtual Driver Driver { get; set; }\n    public virtual RunningClock StartClock { get; set; }\n    public virtual RunningClock? EndClock { get; set; }\n}\n`\nThere is no relevant fluent API configuration for the entities.\nI already set up a new project with (in my opinion) the exact same setup, with just one entity having an ID and a `DateTimeOffset` property. Everything is working fine there, even if I query for the date.\nWhat could cause this problem, or where could I look for issues? Is there any way to deeper debug that query generation?",
      "solution": "Using a `DateTimeOffsetToBinaryConverter` in case of using an SQLite connection resolves the issue. I just added this piece of code to the `OnModelCreating(ModelBuilder modelBuilder)` method in my EFCore context:\n`if (Database.ProviderName == \"Microsoft.EntityFrameworkCore.Sqlite\")\n{\n    // SQLite does not have proper support for DateTimeOffset via Entity Framework Core, see the limitations\n    // here: https://docs.microsoft.com/en-us/ef/core/providers/sqlite/limitations#query-limitations\n    // To work around this, when the Sqlite database provider is used, all model properties of type DateTimeOffset\n    // use the DateTimeOffsetToBinaryConverter\n    // Based on: https://github.com/aspnet/EntityFrameworkCore/issues/10784#issuecomment-415769754\n    // This only supports millisecond precision, but should be sufficient for most use cases.\n    foreach (var entityType in modelBuilder.Model.GetEntityTypes())\n    {\n        var properties = entityType.ClrType.GetProperties().Where(p => p.PropertyType == typeof(DateTimeOffset)\n                                                                       || p.PropertyType == typeof(DateTimeOffset?));\n        foreach (var property in properties)\n        {\n            modelBuilder\n                .Entity(entityType.Name)\n                .Property(property.Name)\n                .HasConversion(new DateTimeOffsetToBinaryConverter());\n        }\n    }\n}\n`\nSource: https://blog.dangl.me/archive/handling-datetimeoffset-in-sqlite-with-entity-framework-core/\nNote: This only works down to millisecond precision.",
      "question_score": 12,
      "answer_score": 18,
      "created_at": "2023-05-01T20:32:51",
      "url": "https://stackoverflow.com/questions/76149300/ef-core-can-not-handle-datetimeoffset-when-using-an-sqlite-connection"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "postgresql",
      "question_id": 65858396,
      "title": "How to re-run Diesel migrations?",
      "problem": "I'm using Diesel with PostgreSQL. I added my migrations, and they worked fine, outputting everything in the `schema.rs` file. Until I noticed that I was missing the `created_at` fields on some of my migrations. I edited the SQL and ran `diesel migration run`. Nothing happened, no errors, no success. Is there a way to fix this and re-run my migrations?",
      "solution": "The command\n`diesel migration run\n`\nOnly applies migrations. If you would like to revert a migration you have to run:\n`diesel migration revert\n`\nUsing these commands together you can \"redo\" an applied migration like this:\n`diesel migration revert\ndiesel migration run\n`\nThis pattern is common enough that `diesel` provides this shortcut command that does the same thing as the above 2 commands:\n`diesel migration redo\n`\nNote: all of these commands only run, revert, or redo a single migration at a time. If you want to run, revert, or redo multiple migrations or all migrations you're going to have to manually run the commands multiple times, that is until a new version of `diesel` is released and this feature becomes available, when you'll be able to redo all migrations by simply running:\n```\n`diesel migration redo --all\n`\n```\nNote: all of the commands will only work correctly if you've written a `down.sql` for every migration you intend to revert or redo.",
      "question_score": 12,
      "answer_score": 13,
      "created_at": "2021-01-23T11:59:48",
      "url": "https://stackoverflow.com/questions/65858396/how-to-re-run-diesel-migrations"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "postgresql",
      "question_id": 67265537,
      "title": "PostgreSQL downgrade password encryption from SCRAM to md5",
      "problem": "I need to downgrade password encryption for user postgres from scram-sha-265 to md5.\nI've tried modifying pg_hba.conf and postgresql.conf files changing password encryption from scram-sha-256 to md5 but after that I was unable to connect to the database.\nI'm using PostgreSQL 13 and PgAdmin 4 v5.\nThanks for any help and suggestion!\nPS: I have to do this because RStudio can't manage connections with scram authentication.",
      "solution": "I solved following these steps:\nChange password_encryption to `md5` in file postgresql.conf\nChange the first 3 occurrences of `scram-sha-256` to `trust` in file pg_hba.conf\n```\n`# \"local\" is for Unix domain socket connections only\nlocal   all             all                                     trust\n# IPv4 local connections:\nhost    all             all             127.0.0.1/32            trust\n# IPv6 local connections:\nhost    all             all             ::1/128                 trust\n`\n```\nRestart postgresql service\nExecute `psql -U postgres` (you won't be asked for password)\nChange password with command `\\password username`\nChange the first 3 occurrences of `trust` to `md5` in file pg_hba.conf\n```\n`# \"local\" is for Unix domain socket connections only\nlocal   all             all                                     md5\n# IPv4 local connections:\nhost    all             all             127.0.0.1/32            md5\n# IPv6 local connections:\nhost    all             all             ::1/128                 md5\n`\n```\nRestart postgresql service",
      "question_score": 12,
      "answer_score": 9,
      "created_at": "2021-04-26T13:08:11",
      "url": "https://stackoverflow.com/questions/67265537/postgresql-downgrade-password-encryption-from-scram-to-md5"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "postgresql",
      "question_id": 67595318,
      "title": "CloudRun Suddenly got `Improper path /cloudsql/{SQL_CONNECTION_NAME} to connect to Postgres Cloud SQL instance &quot;{SQL_CONNECTION_NAME}&quot;`",
      "problem": "We have been running a service using NestJS and TypeORM on fully managed CloudRun without issues for several months. Yesterday PM we started getting `Improper path /cloudsql/{SQL_CONNECTION_NAME} to connect to Postgres Cloud SQL instance \"{SQL_CONNECTION_NAME}\"` errors in our logs.\nWe didn't make any server/SQL changes around this timestamp. Currently there is no impact to the service so we are not sure if this is a serious issue.\nThis error is not from our code, and our third party modules shouldn't know if we use Cloud SQL, so I have no idea where this errors come from.\nMy assumption is Cloud SQL Proxy or any SQL client used in Cloud Run is making this error. We use --add-cloudsql-instances flag when deploying with \"gcloud run deploy\" CLI command.\nLink to the issue here",
      "solution": "This log was recently added in the Cloud Run data path to provide more context for debugging CloudSQL connectivity issues. However, the original logic was overly aggressive, emitting this message even for properly working CloudSQL connections. Your application is working correctly and should not receive this warning.\nThank you for reporting this issue. The fix is ready and should roll out soon. You should not see this message anymore after the fix is out.",
      "question_score": 12,
      "answer_score": 12,
      "created_at": "2021-05-19T01:57:59",
      "url": "https://stackoverflow.com/questions/67595318/cloudrun-suddenly-got-improper-path-cloudsql-sql-connection-name-to-connect"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "postgresql",
      "question_id": 75016462,
      "title": "How to improve or speed up Postgres query with pg_trgm?",
      "problem": "Are there any additional steps I can take to speed up query execution?\nI have a table with more than 100m rows and I need to do search for matching strings. For that I checked two options:\n\nCompare text with to_tsvector `@@` (to_tsquery or plainto_tsquery)\nThis works very fast (under 1s on all data) but it has some problems with finding text similarity\nCompare text with pg_trgm similarity\nThis works fine on text comparison but works bad on large amount of data.\n\nI found that I can use indexes to improve performance.\nFor my GiST index I tried to increase `siglen` from small number to 2024, but for some reason Postgres uses `512` and not higher.\n```\n`CREATE INDEX trgm_idx_512_gg ON table USING GIST (name gist_trgm_ops(siglen=512));\n`\n```\nQuery:\n```\n`SELECT name, similarity(name, '\u043d\u043e\u0443\u0442\u0431\u0443\u043a MSI GF63 Thin 10SC 086XKR 9S7 16R512 086') as sm\nFROM table\nWHERE name % '\u043d\u043e\u0443\u0442\u0431\u0443\u043a MSI GF63 Thin 10SC 086XKR 9S7 16R512 086' \n`\n```\n`EXPLAIN` output:\n```\n`Bitmap Heap Scan on table (cost=1632.01..40051.57 rows=9737 width=126)\n  Recheck Cond: ((name)::text % '\u043d\u043e\u0443\u0442\u0431\u0443\u043a MSI GF63 Thin 10SC 086XKR 9S7 16R512 086'::text)\n  ->  Bitmap Index Scan on trgm_idx_512_gg  (cost=0.00..1629.57 rows=9737 width=0)\n        Index Cond: ((name)::text % '\u043d\u043e\u0443\u0442\u0431\u0443\u043a MSI GF63 Thin 10SC 086XKR 9S7 16R512 086'::text)\n`\n```\nExecution time was about 120 sec.\nQuestion\nHow can I improve or speed up query? Maybe I need to use a different approach or just add something else?\nOutput for `EXPLAIN (ANALYZE, BUFFERS)` (searching for a different name so that the search is completely new and not from the cache):\n```\n`Bitmap Heap Scan on table (cost=1632.01..40051.57 rows=9737 width=126) (actual time=159119.258..159960.251 rows=5645 loops=1)\n  Recheck Cond: ((name)::text % '\u0427\u0435\u0445\u043e\u043b \u043d\u0430 realme C25s / \u0420\u0435\u0430\u043b\u043c\u0438 \u042625\u0441 c \u0440\u0438\u0441\u0443\u043d\u043a\u043e\u043c / \u043f\u0440\u043e\u0437\u0440\u0430\u0447\u043d\u044b\u0439 \u0441 \u043f\u0440\u0438\u043d\u0442\u043e\u043c, Andy&Paul'::text)\n  Heap Blocks: exact=3795\n  Buffers: shared read=1289378\n  ->  Bitmap Index Scan on trgm_idx_512_gg  (cost=0.00..1629.57 rows=9737 width=0) (actual time=159118.616..159118.616 rows=5645 loops=1)\n        Index Cond: ((name)::text % '\u0427\u0435\u0445\u043e\u043b \u043d\u0430 realme C25s / \u0420\u0435\u0430\u043b\u043c\u0438 \u042625\u0441 c \u0440\u0438\u0441\u0443\u043d\u043a\u043e\u043c / \u043f\u0440\u043e\u0437\u0440\u0430\u0447\u043d\u044b\u0439 \u0441 \u043f\u0440\u0438\u043d\u0442\u043e\u043c, Andy&Paul'::text)\n        Buffers: shared read=1285583\nPlanning:\n  Buffers: shared read=5\nPlanning Time: 4.063 ms\nExecution Time: 159961.121 ms\n`\n```\nI also created a GIN index (but Postgres kept using the GiST):\n```\n`CREATE INDEX gin_gg ON table USING GIN (name gin_trgm_ops);\n`\n```\nSize: 12 GB.\nGIST index: 31GB",
      "solution": "A trigram GiST index with `siglen=512` on 100 million rows is very large, and will probably never be cached efficiently. Default is `siglen=12` i.e. 12 bytes. What makes you think this large signature would be a good choice? The manual:\n\nLonger signatures lead to a more precise search (scanning a smaller fraction of the index and fewer heap pages), at the cost of a larger index.\n\nLooks like you went overboard with the size.\nI have better experience with trigram GIN indexes, especially in current versions of Postgres. If the query planner is confused by the existence of an additional GiST index, remove that one to test results with the GIN index.\nBut first, to get a size comparison, look at the output of:\n`SELECT i.indexrelid::regclass::text AS idx\n     , pg_get_indexdef(i.indexrelid) AS idx_def\n     , pg_size_pretty(pg_relation_size(i.indexrelid)) AS idx_size\nFROM   pg_class t\nJOIN   pg_index i ON i.indrelid = t.oid\nWHERE  t.oid = 'public.tbl'::regclass  -- your table name here!\nORDER  BY 1;\n`\nYour query plan shows vast amounts of `Buffers: shared read` for index and main relation (heap). So nothing was found in cache. The key to better performance will be to read fewer data pages to satisfy your queries, and more of them from cache: `hit` instead of `read` in the query plan.\nReducing the size of table and indexes helps in this reagard.\nThe selectivity of the trigram similarity operator `%` is set by the customized option `pg_trgm.similarity_threshold`. The default `0.3` is rather lax and allows many hits. A higher similarity threshold will filter fewer (better matching) result rows. What do you do with `rows=5645` result rows anyway? Try:\n```\n`SET pg_trgm.similarity_threshold = 0.5;  -- or higher\n`\n```\nThen retry your query.\nSee:\n\nFinding similar strings with PostgreSQL quickly\nPostgreSQL LIKE query performance variations\n\nThe latest Postgres version, better server configuration, and more RAM can also help in this regard. You disclosed no information there.",
      "question_score": 12,
      "answer_score": 7,
      "created_at": "2023-01-05T10:43:32",
      "url": "https://stackoverflow.com/questions/75016462/how-to-improve-or-speed-up-postgres-query-with-pg-trgm"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "postgresql",
      "question_id": 76339018,
      "title": "Merging users with multiple refs and count their collective assets",
      "problem": "There is a set of users. A person can have multiple users, but `ref1` and `ref2` might be alike and can therefore link users together. `ref1` and `ref2` does not overlap, one value in `ref1` does not exist in `ref2`.\nA user can own multiple assets. I want to \"merge\" users that has one or more refs alike and then count how many assets they own together. There could be missing entries in the user table, in that case I just want to propagate the owner into ref2 and set the asset_count and asset_ids.\nHere is an example schema to illustrate:\nExample assets\n```\n`SELECT * FROM assets;\n`\n```\n\nid\nname\nowner\n\n1\n#1\na\n\n2\n#2\nb\n\n3\n#3\nc\n\n4\n#4\na\n\n5\n#5\nc\n\n6\n#6\nd\n\n7\n#7\ne\n\n8\n#8\nd\n\n9\n#9\na\n\n10\n#10\na\n\n11\n#11\nz\n\nExample users\n```\n`SELECT * FROM users;\n`\n```\n\nid\nusername\nref1\nref2\n\n1\nbobo\na\nd\n\n2\ntoto\nb\ne\n\n3\nmomo\nc\nd\n\n4\nlolo\na\nf\n\n5\npopo\nc\nf\n\nWhat I want to get in the end\n```\n`SELECT * FROM results;\n`\n```\n\nids\nusernames\nrefs1\nrefs2\nasset_ids\nasset_count\n\n1,3,4,5\nbobo,momo,lolo,popo\na,c\nd,f\n1,3,4,5,6,8,9,10\n8\n\n2\ntoto\nb\ne\n2,7\n2\n\nz\n11\n1\n\nI've tried different approaches, but this is what I currently have:\nClosest I have got\n```\n`SELECT\n  ARRAY_AGG(DISTINCT u.id) AS ids,\n  ARRAY_AGG(DISTINCT u.username) AS usernames,\n  ARRAY_AGG(DISTINCT u.ref1) AS refs1,\n  ARRAY_AGG(DISTINCT u.ref2) AS refs2,\n  COUNT(DISTINCT a.id) AS asset_count\nFROM assets a\nJOIN users u ON a.owner = u.ref1 OR a.owner = u.ref2\nGROUP BY a.owner\nORDER BY MIN(a.id);\n`\n```\n\nids\nusernames\nrefs1\nrefs2\nasset_count\n\n1,4\nbobo,lolo\na\nd,f\n4\n\n2\ntoto\nb\ne\n1\n\n3,5\nmomo,popo\nc\nd,f\n2\n\n1,3\nbobo,momo\na,c\nd\n2\n\n2\ntoto\nb\ne\n1\n\nIf I merge the above table on ids, I almost get the result I want (without the missing entries in the user table). The merging can easily be done in code, but then I cannot paginate etc. I want to to this in DB layer if possible.\nI want either a solution to the problem or a good explanation of why it is not possible to do (with examples).\nPlease check out my DB Fiddle.",
      "solution": "There are two distinct parts to the question:\n\nthe first one is how to generate groups of users that have common references\nthe second part is how to distribute the assets in the groups, while taking in account orphan assets\n\nPart 1 : a graph-walking problem\nIdentifying clusters of users based on common references reads like a graph-walking problem. That's a complex task in SQL, and requires a recursive query. The pattern is to unpivot users' references to generate nodes, then identify edges (nodes that have a ref in common), and finally walk through the graph (whitout looping) to generate groups.\nIn Postgres, arrays come handy to aggregate nodes:\n```\n`with recursive \n    nodes as (\n        select u.id, r.ref\n        from users u \n        cross join lateral ( values (u.ref1), (u.ref2) ) r(ref)\n    ),\n    edges as (\n        select distinct n1.id as id1, n2.id as id2\n        from nodes n1 \n        inner join nodes n2 on n1.ref = n2.ref\n    ),\n    rcte as (\n        select id1, id2, array[id1] as visited from edges where id1 = id2\n        union all\n        select r.id1, e.id2, r.visited || e.id2\n        from rcte r\n        inner join edges e on e.id1 = r.id2\n        where e.id2 <> all(r.visited) \n    ),\n    groups as (\n        select id1 as id, array_agg(distinct id2 order by id2) as ids\n        from rcte\n        group by id1\n    )\nselect * from groups order by id\n`\n```\n\nid\nids\n\n1\n{1,3,4,5}\n\n2\n{2}\n\n3\n{1,3,4,5}\n\n4\n{1,3,4,5}\n\n5\n{1,3,4,5}\n\nPart 2 : `left join` and aggregation\nNow that we identified the groups, we can check for assets. Since you want all assets in the result, we start from the `assets` table, then bring the users and the groups with `left join`s. We can still `group by` the user groups, which puts all orphan assets in the same group (where the group is `null`) - that's exactly what we want.\nThe last step is array aggregation; the \"propagation\" of the owners of orphan assets to `ref2` can be handled with a `case` expression.\n```\n`with recursive \n    nodes  as (...),\n    edges  as (...),\n    rcte   as (...),\n    groups as (...)\nselect g.ids,\n    array_agg(distinct u.username) as usernames,\n    array_agg(distinct u.ref1) as refs1,\n    case when g.ids is null then array_agg(distinct a.owner) else array_agg(distinct u.ref2) end as refs2,\n    array_agg(distinct a.id) as asset_ids,\n    count(distinct a.id) as asset_count\nfrom assets a\nleft join users u on a.owner in (u.ref1, u.ref2)\nleft join groups g on g.id = u.id\ngroup by g.ids\n`\n```\n\nids\nusernames\nrefs1\nrefs2\nasset_ids\nasset_count\n\n{1,3,4,5}\n{bobo,lolo,momo,popo}\n{a,c}\n{d,f}\n{1,3,4,5,6,8,9,10}\n8\n\n{2}\n{toto}\n{b}\n{e}\n{2,7}\n2\n\nnull\n{NULL}\n{NULL}\n{z}\n{11}\n1\n\nDemo on DB Fiddlde\n\nAdd-on: Graph-walking performance\nPerformance will suffer on networks that have a lot of edges, especially if there are just few clusters in the graph, each containing with many users. We can try and optimize the query for such situation; the idea is to try and limit the number of paths that need to be walked, by aggregating all paths of each user at each iteration.\nThis query passes your test case with 200 users that all belong to the same cluster (whereas the first query exhausts the DB Fiddle resources):\n```\n`with recursive \n    nodes as (\n        select u.id, r.ref\n        from users u \n        cross join lateral ( values (u.ref1), (u.ref2) ) r(ref)\n    ),\n    edges as (\n        select distinct n1.id as id1, n2.id as id2\n        from nodes n1 \n        inner join nodes n2 on n1.ref = n2.ref\n    ),\n    rcte as (\n        select id1 as id, array[id1] as visited from edges where id1 = id2\n        union all\n        select id, array_agg(distinct visited) as visited\n        from (\n            select r.id, v.visited\n            from rcte r\n            inner join edges e on e.id1 = any(r.visited)\n            cross join lateral unnest(r.visited || e.id2) v(visited)\n            where e.id2 <> all(r.visited)\n        ) as x\n        group by id\n    ),\n    groups as (\n        select distinct on (id) id, visited as ids \n        from rcte \n        order by id, array_length(visited, 1) desc \n    )\nselect * from groups g order by id\n`\n```",
      "question_score": 12,
      "answer_score": 5,
      "created_at": "2023-05-26T10:28:14",
      "url": "https://stackoverflow.com/questions/76339018/merging-users-with-multiple-refs-and-count-their-collective-assets"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "postgresql",
      "question_id": 72611651,
      "title": "How can I upgrade PostgreSQL from version 11 to version 13?",
      "problem": "I'm trying to upgrade PostgreSQL from 11 to 13 on a Debian system, but it fails. I have a single cluster that needs to be upgraded:\n```\n`$ sudo -u postgres pg_lsclusters\nVer Cluster Port Status Owner    Data directory               Log file\n11  main    5432 online postgres /var/lib/postgresql/11/main  /var/log/postgresql/postgresql-11-main.log\n`\n```\nHere's what I've tried to upgrade it:\n```\n`$ sudo -u postgres pg_upgradecluster 11 main\nStopping old cluster...\nWarning: stopping the cluster using pg_ctlcluster will mark the systemd unit as failed. Consider using systemctl:\n  sudo systemctl stop postgresql@11-main\nRestarting old cluster with restricted connections...\nNotice: extra pg_ctl/postgres options given, bypassing systemctl for start operation\nError: cluster configuration already exists\nError: Could not create target cluster\n`\n```\nAfter this, the system is left in an unusable state:\n```\n`$ sudo systemctl status postgresql@11-main.service \n\u25cf postgresql@11-main.service - PostgreSQL Cluster 11-main\n     Loaded: loaded (/lib/systemd/system/postgresql@.service; enabled-runtime; vendor preset: enabled)\n     Active: failed (Result: exit-code) since Tue 2022-06-14 06:48:20 CEST; 19s ago\n    Process: 597 ExecStart=/usr/bin/pg_ctlcluster --skip-systemctl-redirect 11-main start (code=exited, status=0/SUCCE>\n    Process: 4508 ExecStop=/usr/bin/pg_ctlcluster --skip-systemctl-redirect -m fast 11-main stop (code=exited, status=>\n   Main PID: 684 (code=exited, status=0/SUCCESS)\n        CPU: 1.862s\n\nJun 14 06:47:23 argos systemd[1]: Starting PostgreSQL Cluster 11-main...\nJun 14 06:47:27 argos systemd[1]: Started PostgreSQL Cluster 11-main.\nJun 14 06:48:20 argos postgresql@11-main[4508]: Cluster is not running.\nJun 14 06:48:20 argos systemd[1]: postgresql@11-main.service: Control process exited, code=exited, status=2/INVALIDARG>\nJun 14 06:48:20 argos systemd[1]: postgresql@11-main.service: Failed with result 'exit-code'.\nJun 14 06:48:20 argos systemd[1]: postgresql@11-main.service: Consumed 1.862s CPU time.\n\n$ sudo systemctl start postgresql@11-main.service \nJob for postgresql@11-main.service failed because the service did not take the steps required by its unit configuration.\nSee \"systemctl status postgresql@11-main.service\" and \"journalctl -xe\" for details.\n`\n```\nLuckily, rebooting the system brought the old cluster back online, but nothing has been upgraded. Why does the upgrade fail? What are \"the steps required by its unit configuration\"? How can I upgrade PostgreSQL with minimal downtime?",
      "solution": "I found the source of my problem: a configuration file owned by the wrong user (`root` instead of `postgres`) that could not be removed by the `pg_dropcluster` command because I ran it as the user `postgres`.\nFor future reference, here are the correct steps to upgrade a PostgreSQL cluster from 11 to 13:\n\nVerify the current cluster is the still the old version:\n```\n`$ pg_lsclusters\nVer Cluster Port Status Owner    Data directory               Log file\n11  main    5432 online postgres /var/lib/postgresql/11/main  /var/log/postgresql/postgresql-11-main.log\n13  main    5434 down   postgres /var/lib/postgresql/13/main  /var/log/postgresql/postgresql-13-main.log\n`\n```\n\nRun `pg_dropcluster 13 main` as user `postgres`:\n```\n`$ sudo -u postgres pg_dropcluster 13 main\nWarning: systemd was not informed about the removed cluster yet. \nOperations like \"service postgresql start\" might fail. To fix, run:\nsudo systemctl daemon-reload\n`\n```\n\nRun the `pg_upgradecluster` command as user `postgres`:\n```\n`$ sudo -u postgres pg_upgradecluster 11 main\n`\n```\n\nVerify that everything works, and that the only online cluster is now 13:\n```\n`$ pg_lsclusters \nVer Cluster Port Status Owner    Data directory               Log file\n11  main    5434 down   postgres /var/lib/postgresql/11/main  /var/log/postgresql/postgresql-11-main.log\n13  main    5432 online postgres /var/lib/postgresql/13/main  /var/log/postgresql/postgresql-13-main.log\n`\n```\n\nDrop the old cluster:\n```\n`$ sudo -u postgres pg_dropcluster 11 main\n`\n```\n\nUninstall the previous version of PostgreSQL:\n```\n`$ sudo apt remove 'postgresql*11'\n`\n```",
      "question_score": 11,
      "answer_score": 25,
      "created_at": "2022-06-14T06:46:47",
      "url": "https://stackoverflow.com/questions/72611651/how-can-i-upgrade-postgresql-from-version-11-to-version-13"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "postgresql",
      "question_id": 67445513,
      "title": "Getting Error: Unknown authenticationOk message typeMessage { name: &#39;authenticationOk&#39;, length: 23 }",
      "problem": "I have installed Postgres 13 in windows 10.\nConfigured all the right credentials in the environment file of the project.\nThe project uses the below dependencies and it was created in ubuntu.\n```\n`\"pg\": \"^7.4.3\",\n\"pg-hstore\": \"^2.3.2\",\n\"sequelize\": \"4.38.0\",\n\"sequelize-cli\": \"^6.2.0\"\n`\n```\nI'm trying to set it up in windows.\nAnd getting the below error in windows 10.\n```\n`Error: Unknown authenticationOk message typeMessage { name: 'authenticationOk', length: 23 }\n`\n```\nWhen I hit\n`npx sequelize db:migrate`\nin the terminal for migrating the tables in the database.",
      "solution": "I was able to fix this by upgrading `pg` from `\"^7.4.3\"` to `\"^8.7.1\"`.",
      "question_score": 11,
      "answer_score": 28,
      "created_at": "2021-05-08T10:28:26",
      "url": "https://stackoverflow.com/questions/67445513/getting-error-unknown-authenticationok-message-typemessage-name-authenticat"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "postgresql",
      "question_id": 77856818,
      "title": "Get table name from a PGTable",
      "problem": "I am starting out a new project with Drizzle ORM. Starting out with some assumptions:\n\nThe primary keys on all the tables in my database will be uniformly uniformly typed and they will all be called `id`.\n\nAll the foreign keys will be in the form of `table_name_id` in the database and the model property will always be `tableNameId`.\n\nSince I can make these assumptions, I thought I could significantly reduce the amount of boilerplate required for specifying foreign keys:\n```\n`import {\n  type PgColumn,\n  type PgTableWithColumns,\n  pgTableCreator,\n  serial,\n} from \"drizzle-orm/pg-core\";\n\nimport { camelCase, snakeCase } from \"lodash\";\n\nexport const pgTable = pgTableCreator((name) => {\n  return `some-app_${name}`;\n});\n\nexport function id() {\n  return serial(\"id\").primaryKey();\n}\n\ntype TableWithId = PgTableWithColumns;\n\nexport function fk(...relations: (TableWithId | TableWithId[])[]) {\n  return Object.fromEntries(\n    relations.flatMap((entry) => {\n      const isOptional = Array.isArray(entry);\n      const tables = isOptional ? entry : [entry];\n\n      return tables.map((table) => {\n        const columnName = `${snakeCase(`)}_id`;\n        const propertyName = `${camelCase(table._.name)}Id`;\n        const column = bigint(columnName, { mode: \"number\" }).references(() => {\n          return table.id;\n        });\n\n        return [propertyName, isOptional ? column : column.notNull()];\n      });\n    }),\n  );\n}\n`\n```\nThis code is intended to be used as such:\n```\n`export const contact = pgTable(\"contacts\", {\n  id: id(),\n  firstName: varchar(\"first_name\", { length: 128 }).notNull(),\n  lastName: varchar(\"last_name\", { length: 128 }).notNull(),\n  ...fk(phone, [email], [address]),\n  ...timestamps(),\n});\n`\n```\nHere a relation to the phone and email tables are mandatory (not null), but the relation to the email and address are optional.\nThe issue arises with accessing the name for a given `PgTable`. When I inspect the type of `contact` in my editor I see that they are type hinted with `PgTableWithColumns`. Which is what I used in the type hint to the `fk` function.\nAs a result as far as Typescript is concerned the function is correct. However when I run the code, I get the error:\n```\n`TypeError: Cannot read properties of undefined (reading 'name')\n`\n```\nThis is referring to the expression: `table._.name`. When I log out table, I see that it does not match the type. The underscore property is infact missing, but the name provided to the table is present in a symbol key `Symbol(drizzle:BaseName)`.\nHowever, I am not sure how to access this property as I have not been able to import the symbol from anywhere.\nIt seems like the types for this package are broken and I might end up filing a bug with drizzle. But I kind of want to get this to work.\nAny ideas?",
      "solution": "How about `getTableName`?\n```\n`import { getTableName } from \"drizzle-orm\";\nconst tableName = getTableName(table);\n`\n```",
      "question_score": 11,
      "answer_score": 25,
      "created_at": "2024-01-21T23:12:08",
      "url": "https://stackoverflow.com/questions/77856818/get-table-name-from-a-pgtable"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "postgresql",
      "question_id": 66876181,
      "title": "How do I close a Flask-SQLAlchemy connection that I used in a Thread?",
      "problem": "When I query a Flask-SQLAlchemy model in a Thread, it seems like the database connection is never closed. Here is a test case:\n```\n`from threading import Thread\nfrom sqlalchemy.pool import NullPool\nfrom flask_sqlalchemy import SQLAlchemy\nfrom flask import Flask\ndb = SQLAlchemy()\napp = Flask(__name__)\n\napp.config['SQLALCHEMY_DATABASE_URI'] = 'postgresql:///testdb'\napp.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = False\napp.config['SQLALCHEMY_ENGINE_OPTIONS'] = {\n    'poolclass': NullPool,\n}\ndb.app = app\ndb.init_app(app)\n\nclass TestModel(db.Model):\n    __tablename__ = \"test_table\"\n\n    id = db.Column(db.Integer, autoincrement=True, primary_key=True)\n\n@app.route('/test-connection')\ndef test_connection():\n    def run_test():\n        models = TestModel.query.all()\n        print(models)\n    thread = Thread(target=run_test)\n    thread.start()\n    return \"OK\"\n\napp.run(debug=True, host='0.0.0.0')\n`\n```\nWhen I run my route, it permanently leaves an `idle in transaction` query in my pg_stat_activity table:\n```\n`testdb=# select query, state from pg_stat_activity where query like '%test_table%' and query not like '%pg_stat_activity%';\n                 query                  |        state        \n----------------------------------------+---------------------\n SELECT test_table.id AS test_table_id +| idle in transaction\n FROM test_table                        | \n(1 row)\n`\n```\nThat row doesn't show up if I call the `run_test` function synchronously instead of in the Thread. In production, this is causing my application to run out of database connections and crash. How can I close my Flask-SQLAlchemy database connection after using it in a thread?\nI'm using Python 3.8.6, SQLAlchemy 1.3.18, and Flask-SQLAlchemy 2.4.4.",
      "solution": "We had faced a similar issue with open connections. Though the DB in question was SQL Server, I believe the same solution should work here.\nWe added a method to close the DB connection.\n```\n`def cleanup(session):\n    \"\"\"\n    This method cleans up the session object and closes the connection pool using the dispose \n    method.\n    \"\"\"\n    session.close()\n    engine_container.dispose()\n`\n```\nThe engine_container is defined as:\n`engine_container = db.get_engine(app)`\nWe called this method from the finally block after every request.\n```\n`finally:\n    cleanup(db.session)\n`\n```\nWith this change, your code should look like:\n```\n`from threading import Thread\nfrom sqlalchemy.pool import NullPool\nfrom flask_sqlalchemy import SQLAlchemy\nfrom flask import Flask\n\ndb = SQLAlchemy()\napp = Flask(__name__)\n\napp.config['SQLALCHEMY_DATABASE_URI'] = 'postgresql:///testdb'\napp.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = False\napp.config['SQLALCHEMY_ENGINE_OPTIONS'] = {\n    'poolclass': NullPool,\n}\ndb.app = app\ndb.init_app(app)\nengine_container = db.get_engine(app)\n\nclass TestModel(db.Model):\n    __tablename__ = \"test_table\"\n\n    id = db.Column(db.Integer, autoincrement=True, primary_key=True)\n\n@app.route('/test-connection')\ndef test_connection():\n    def run_test():\n        try:\n            models = TestModel.query.all()\n            print(models)\n        except Exception as err:\n            raise err\n        finally:\n            cleanup(db.session)\n\n    thread = Thread(target=run_test)\n    thread.start()\n    return \"OK\"\n\ndef cleanup(session):\n    \"\"\"\n    This method cleans up the session object and also closes the connection pool using the dispose method.\n    \"\"\"\n    session.close()\n    engine_container.dispose()\n\napp.run(debug=True, host='0.0.0.0')\n`\n```",
      "question_score": 11,
      "answer_score": 14,
      "created_at": "2021-03-30T20:02:03",
      "url": "https://stackoverflow.com/questions/66876181/how-do-i-close-a-flask-sqlalchemy-connection-that-i-used-in-a-thread"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "postgresql",
      "question_id": 69625823,
      "title": "Accessing PostgreSQL (on wsl2) from DBeaver (on Windows) fails: &quot;Connection refused: connect&quot;",
      "problem": "What I'm trying is to use Postgres and access it from DBeaver.\n\nPostgres is installed into wsl2 (Ubuntu 20)\nDBeaver is installed into Windows 10\n\nAccording to this doc, if you access an app running on Linuc from Windows, `localhost` can be used.\nHowever...\n\nConnection is refused with `localhost`. Also, I don't know what this message means: `Connection refused: connect`.\nDoes anyone see potential cause for this? Any advice will be appreciated.\nNote:\n\nThe password should be fine. When I use `psql` in wsl2 and type in the password, `psql` is available with the password\nI don't have Postgres on Windows' side. It exists only on wsl2",
      "solution": "I found a solution by myself.\nI just had to allow the TCP connection on wsl2(Ubuntu) and then restart postgres.\n```\n`sudo ufw allow 5432/tcp\n# You should see \"Rules updated\" and/or \"Rules updated (v6)\"\nsudo service postgresql restart\n`\n```\nI didn't change IPv4/IPv6 connections info. Here's what I see in `pg_hba.conf`:\n```\n`# IPv4 local connections:\nhost    all             all             127.0.0.1/32            md5\n# IPv6 local connections:\nhost    all             all             ::1/128                 md5\n`\n```",
      "question_score": 11,
      "answer_score": 12,
      "created_at": "2021-10-19T08:03:12",
      "url": "https://stackoverflow.com/questions/69625823/accessing-postgresql-on-wsl2-from-dbeaver-on-windows-fails-connection-refu"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "postgresql",
      "question_id": 67556906,
      "title": "ImproperlyConfigured: Error loading psycopg2 module: No module named &#39;psycopg2._psycopg&#39; zappa",
      "problem": "I am using zappa to deploy backend to the AWS Lambda. It worked well, until I decided to use PostgreSQL. I added it in the settings like this:\n```\n`DATABASES = {\n    'default': {\n        'ENGINE': 'django.db.backends.postgresql',\n        'NAME': config('DATABASE_NAME'),\n        'USER': config('DATABASE_USER'),\n        'PASSWORD': config('DATABASE_PASSWORD'),\n        'HOST': config('DATABASE_HOST'),\n        'PORT': '5432'\n    }\n}\n`\n```\nI am using AWS RDS. I installed psycopg2-binary and also psycopg2 (versions 2.8.6), but the issue remains. The python version is 3.8.\nThe full error log:\n```\n`   [1621168086542] [ERROR] ImproperlyConfigured: Error loading psycopg2 module: No module named 'psycopg2._psycopg'\nTraceback (most recent call last):\n\u00a0\u00a0File \"/var/task/handler.py\", line 609, in lambda_handler\n\u00a0\u00a0\u00a0\u00a0return LambdaHandler.lambda_handler(event, context)\n\u00a0\u00a0File \"/var/task/handler.py\", line 240, in lambda_handler\n\u00a0\u00a0\u00a0\u00a0handler = cls()\n\u00a0\u00a0File \"/var/task/handler.py\", line 146, in __init__\n\u00a0\u00a0\u00a0\u00a0wsgi_app_function = get_django_wsgi(self.settings.DJANGO_SETTINGS)\n\u00a0\u00a0File \"/var/task/zappa/ext/django_zappa.py\", line 20, in get_django_wsgi\n\u00a0\u00a0\u00a0\u00a0return get_wsgi_application()\n\u00a0\u00a0File \"/var/task/django/core/wsgi.py\", line 12, in get_wsgi_application\n\u00a0\u00a0\u00a0\u00a0django.setup(set_prefix=False)\n\u00a0\u00a0File \"/var/task/django/__init__.py\", line 24, in setup\n\u00a0\u00a0\u00a0\u00a0apps.populate(settings.INSTALLED_APPS)\n\u00a0\u00a0File \"/var/task/django/apps/registry.py\", line 114, in populate\n\u00a0\u00a0\u00a0\u00a0app_config.import_models()\n\u00a0\u00a0File \"/var/task/django/apps/config.py\", line 211, in import_models\n\u00a0\u00a0\u00a0\u00a0self.models_module = import_module(models_module_name)\n\u00a0\u00a0File \"/var/lang/lib/python3.8/importlib/__init__.py\", line 127, in import_module\n\u00a0\u00a0\u00a0\u00a0return _bootstrap._gcd_import(name[level:], package, level)\n\u00a0\u00a0File \"\", line 1014, in _gcd_import\n\u00a0\u00a0File \"\", line 991, in _find_and_load\n\u00a0\u00a0File \"\", line 975, in _find_and_load_unlocked\n\u00a0\u00a0File \"\", line 671, in _load_unlocked\n\u00a0\u00a0File \"\", line 783, in exec_module\n\u00a0\u00a0File \"\", line 219, in _call_with_frames_removed\n\u00a0\u00a0File \"/var/task/django/contrib/auth/models.py\", line 2, in \n\u00a0\u00a0\u00a0\u00a0from django.contrib.auth.base_user import AbstractBaseUser, BaseUserManager\n\u00a0\u00a0File \"/var/task/django/contrib/auth/base_user.py\", line 48, in \n\u00a0\u00a0\u00a0\u00a0class AbstractBaseUser(models.Model):\n\u00a0\u00a0File \"/var/task/django/db/models/base.py\", line 122, in __new__\n\u00a0\u00a0\u00a0\u00a0new_class.add_to_class('_meta', Options(meta, app_label))\n\u00a0\u00a0File \"/var/task/django/db/models/base.py\", line 326, in add_to_class\n\u00a0\u00a0\u00a0\u00a0value.contribute_to_class(cls, name)\n\u00a0\u00a0File \"/var/task/django/db/models/options.py\", line 206, in contribute_to_class\n\u00a0\u00a0\u00a0\u00a0self.db_table = truncate_name(self.db_table, connection.ops.max_name_length())\n\u00a0\u00a0File \"/var/task/django/db/__init__.py\", line 28, in __getattr__\n\u00a0\u00a0\u00a0\u00a0return getattr(connections[DEFAULT_DB_ALIAS], item)\n\u00a0\u00a0File \"/var/task/django/db/utils.py\", line 214, in __getitem__\n\u00a0\u00a0\u00a0\u00a0backend = load_backend(db['ENGINE'])\n\u00a0\u00a0File \"/var/task/django/db/utils.py\", line 111, in load_backend\n\u00a0\u00a0\u00a0\u00a0return import_module('%s.base' % backend_name)\n\u00a0\u00a0File \"/var/lang/lib/python3.8/importlib/__init__.py\", line 127, in import_module\n\u00a0\u00a0\u00a0\u00a0return _bootstrap._gcd_import(name[level:], package, level)\n\u00a0\u00a0File \"/var/task/django/db/backends/postgresql/base.py\", line 29, in \n\u00a0\u00a0\u00a0\u00a0raise ImproperlyConfigured(\"Error loading psycopg2 module: %s\" % e)\n`\n```",
      "solution": "The (likely) issue is that you are building your lambda `.zip` package on MacOS. When you deploy your lambda function it is running in a Linux environment (specifically, AWS's Linux2 environment). The `psycopg2-binary` is different for the MacOS vs Linux environments, so if you build your lambda package (including the `psycopg2-binary`) on a Mac and then deploy to lambda you'll have the issues noted above.\nYou'll need to build your lambda function inside of an AWS Linux container. Here's a Dockerfile you could use to create a container inside of which you install the `psycopg2-binary` and build your lambda zip package. Then everything should work:\n```\n`FROM amazonlinux:2.0.20200207.1\n\nRUN cd /opt && \\\n    yum install -y gcc openssl-devel bzip2-devel libffi-devel wget tar gzip make && \\\n    wget https://www.python.org/ftp/python/3.8.2/Python-3.8.2.tgz && \\\n    tar xzf Python-3.8.2.tgz && \\\n    cd Python-3.8.2 && \\\n    ./configure --enable-optimizations && \\\n    make altinstall && \\\n    rm -f /opt/Python-3.8.2.tgz && \\\n    echo \"alias python3=python3.8\" > ~/.bashrc\n`\n```\nNote the amaonzonlinux:2.0 operating system, then I just install python 3.8.2 into the environment (you could use a different version of python if desired). From there you can copy in your code and build your lambda `.zip` package and deploy to lambda.",
      "question_score": 11,
      "answer_score": 7,
      "created_at": "2021-05-16T14:53:48",
      "url": "https://stackoverflow.com/questions/67556906/improperlyconfigured-error-loading-psycopg2-module-no-module-named-psycopg2"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "postgresql",
      "question_id": 73672857,
      "title": "How to run postgres in docker as non-root user?",
      "problem": "I know that running docker containers as root is not secure, so I would like to change they way I run them. I have a container running postgres, which up till now was run as root (default). However, when I add this line to my `docker-compose` file:\n```\n`user: ${CURRENT_UID}\n`\n```\nWhere `CURRENT_UID` is:\n```\n`export CURRENT_UID=$(id -u):$(id -g)\n`\n```\nI get the following error in the container logs:\n```\n`initdb: error: could not change permissions of directory \"/var/lib/postgresql/data\": Operation not permitted\n\nfixing permissions on existing directory /var/lib/postgresql/data ... chmod: changing permissions of '/var/lib/postgresql/data': Operation not permitted\n\nchmod: changing permissions of '/var/run/postgresql': Operation not permitted\n`\n```\nThe situation is the same when running the `pg-admin` container, with the error message:\n```\n`PermissionError: [Errno 13] Permission denied: '/var/log/pgadmin'\n`\n```\nThe errors make sense of course, I am non root, yet postgres requires root access.\nDoes it mean it is impossible to run as non root?",
      "solution": "Does it mean it is impossible to run as non root?\n\nIn fact, the postgres service already runs as a non-root user. The startup process for the `postgres` container is effectively:\n\nCreate necessary directories and set appropriate ownership\nSwitch to the `postgres` user\nStart postgres\n\nSo if you start up the `postgres` image, even without specifying a user:\n```\n`docker run -e POSTGRES_PASSWORD=secret postgres:14\n`\n```\nYou will find that it's running as the `postgres` user:\n```\n`# ps -fe\nUID          PID    PPID  C STIME TTY          TIME CMD\npostgres       1       0  0 16:11 ?        00:00:00 postgres\npostgres      56       1  0 16:11 ?        00:00:00 postgres: checkpointer\npostgres      57       1  0 16:11 ?        00:00:00 postgres: background writer\npostgres      58       1  0 16:11 ?        00:00:00 postgres: walwriter\npostgres      59       1  0 16:11 ?        00:00:00 postgres: autovacuum launcher\npostgres      60       1  0 16:11 ?        00:00:00 postgres: stats collector\npostgres      61       1  0 16:11 ?        00:00:00 postgres: logical replication launcher\n`\n```\nIt is entirely possible to start the image as a non-root user as well, but in this case you have to ensure that the necessary directories exist and have appropriate ownership. In an environment like Kubernetes/OpenShift the container orchestration system will take care of this for you when mounting a volume, but when using plain Docker (or docker-compose) you have to take care of it yourself.\nHere's one way of doing it:\n\nHave an \"initialization container\" take care of setting volume permissions\nStart postgres only after the initialization container has finished\n\n```\n`version: \"3.9\"\n\nservices:\n  postgres-init:\n    image: postgres:14\n    volumes:\n      - \"postgres_data:/data\"\n    entrypoint:\n      - sh\n      - -c\n      - |\n        chown -R 2000:2000 /data\n\n  postgres:\n    depends_on:\n      postgres-init:\n        condition: service_completed_successfully\n\n    image: postgres:14\n    environment:\n      POSTGRES_PASSWORD: \"secret\"\n      PGDATA: /pgdata\n    volumes:\n      - \"postgres_data:/pgdata\"\n    user: \"2000\"\n\nvolumes:\n  postgres_data:\n`\n```",
      "question_score": 11,
      "answer_score": 9,
      "created_at": "2022-09-10T17:04:40",
      "url": "https://stackoverflow.com/questions/73672857/how-to-run-postgres-in-docker-as-non-root-user"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "postgresql",
      "question_id": 75612704,
      "title": "How to upgrade AWS RDS Postgres instance from 14 to 15 with postgis?",
      "problem": "AWS added support for Postgres 15.2. Upgrading to this version with postgis extension enabled fails with this error message:\n```\n`The instance could not be upgraded because there are one or more databases with an older version of PostGIS extension or its dependent extensions (address_standardizer, address_standardizer_data_us, postgis_tiger_geocoder, postgis_topology, postgis_raster) installed. Please upgrade all installations of PostGIS and drop its dependent extensions and try again.\n`\n```\nThe RDS instance has postgis `3.1.7` installed, and there is no path to upgrade the extension. Running exact upgrade command `ALTER EXTENSION postgis UPDATE TO \"3.1.8\";` results in\n```\n`extension \"postgis\" has no update path from version \"3.1.7\" to version \"3.1.8\"\n`\n```\nRunning `SELECT postgis_extensions_upgrade();` command which should upgrade the extension results in:\n```\n`Upgrade completed, run SELECT postgis_full_version(); for details\n`\n```\n, but the version after running `SELECT postgis_full_version();` returns\n```\n`POSTGIS=\"3.1.7 aafe1ff\" [EXTENSION] PGSQL=\"140\" GEOS=\"3.9.1-CAPI-1.14.2\" PROJ=\"8.0.1\" LIBXML=\"2.9.1\" LIBJSON=\"0.15\" LIBPROTOBUF=\"1.3.2\" WAGYU=\"0.5.0 (Internal)\"\n`\n```\nThe `postgis_raster` extension is broken, and we are not using this, nor any other postgis extension. Only the core postgis extension.\nIs there a way to upgrade the RDS instance to postgres. 15 without dropping the postgis extension?",
      "solution": "Good news, the normal upgrade workflow is now working:\n\nUpgrade Postgres to 14.8 (newly available)\nUpgrade PostGIS to version 3.3.2 (`SELECT postgis_extensions_upgrade();`)\nUpgrade Postgres to 15.3",
      "question_score": 11,
      "answer_score": 5,
      "created_at": "2023-03-02T09:03:29",
      "url": "https://stackoverflow.com/questions/75612704/how-to-upgrade-aws-rds-postgres-instance-from-14-to-15-with-postgis"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "postgresql",
      "question_id": 70823061,
      "title": "Bulk INSERT in Postgres in GO using pgx",
      "problem": "I am trying to bulk insert keys in db in go here is the code\nKey Struct\n```\n`type tempKey struct {\nkeyVal  string\nlastKey int\n`\n```\n}\nTest Keys\n```\n`data := []tempKey{\n    {keyVal: \"abc\", lastKey: 10},\n    {keyVal: \"dns\", lastKey: 11},\n    {keyVal: \"qwe\", lastKey: 12},\n    {keyVal: \"dss\", lastKey: 13},\n    {keyVal: \"xcmk\", lastKey: 14},\n}\n`\n```\nInsertion part\n```\n`dbUrl := \"db url....\"\nconn, err := pgx.Connect(context.Background(), dbUrl)\nif err != nil {\n    println(\"Errrorr...\")\n}\ndefer conn.Close(context.Background())\nsqlStr := \"INSERT INTO keys (keyval,lastval) VALUES \"\ndollars := \"\"\nvals := []interface{}{}\ncount := 1\nfor _, row := range data {\n    dollars = fmt.Sprintf(\"%s($%d, $%d),\", dollars, count, count+1)\n    vals = append(vals, row.keyVal, row.lastKey)\n    count += 2\n}\nsqlStr += dollars\nsqlStr = sqlStr[0 : len(sqlStr)-1]\nfmt.Printf(\"%s \\n\", sqlStr)\n\n_, erro := conn.Exec(context.Background(), sqlStr, vals)\nif erro != nil {\n    fmt.Fprint(os.Stderr, \"Error : \\n\", erro)\n}\n`\n```\non running it throws error: expected 10 arguments, got 1\nWhat is the correct way of bulk inserting.",
      "solution": "You are crafting the SQL statement by hand, which is fine, but you are not leveraging `pgx` which can help with this (see below).\nAppending to the SQL string like so can be inefficient for large inputs\n```\n`dollars = fmt.Sprintf(\"%s($%d, $%d),\", dollars, count, count+1)\n`\n```\nbut also the final value has a trailing `,` where instead you need a termination character `;` to indicate the end of the statement.\nBTW this string truncation line is redundant:\n```\n`sqlStr = sqlStr[0 : len(sqlStr)-1] // this is a NOOP\n`\n```\nAnyway, better to use something more performant like strings.Builder when crafting long strings.\n\nFrom the `pgx` docs, use pgx.Conn.CopyFrom:\n```\n`func (c *Conn) CopyFrom(tableName Identifier, columnNames []string, rowSrc CopyFromSource) (int, error)\n`\n```\n\nCopyFrom uses the PostgreSQL copy protocol to perform bulk data\ninsertion. It returns the number of rows copied and an error.\n\nexample usage of Copy:\n```\n`rows := [][]interface{}{\n    {\"John\", \"Smith\", int32(36)},\n    {\"Jane\", \"Doe\", int32(29)},\n}\n\ncopyCount, err := conn.CopyFrom(\n    pgx.Identifier{\"people\"},\n    []string{\"first_name\", \"last_name\", \"age\"},\n    pgx.CopyFromRows(rows),\n)\n`\n```",
      "question_score": 10,
      "answer_score": 26,
      "created_at": "2022-01-23T15:30:29",
      "url": "https://stackoverflow.com/questions/70823061/bulk-insert-in-postgres-in-go-using-pgx"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "postgresql",
      "question_id": 68334303,
      "title": "Supabase: how to query users by eMail?",
      "problem": "I'm using Supabase in a Node JS middleware. I am developing an invite function thats receives an eMail address of an existing supabase user via a REST Endpoint. Now it should query the users table in order to get the users ID. But this does not seem to work:\n(im using the Supabase JavaScript library an the admin key that bypasses the row level security):\n```\n`const { createClient,   } = require('@supabase/supabase-js')\nconst supabase = createClient(process.env.SUPABASE_URL, process.env.SUPABASE_KEY)\nsupabase\n  .from('users')\n  .select('id')\n  .eq('email', 'doe@example.com')\n  .then(response => {\n    console.log(response)\n  })\n`\n```\nI'm getting this error:\n\n'relation \"public.users\" does not exist'\n\nI also tried a query with\n```\n`supabase\n  .from('users')\n  .select('id')\n  .eq('email', 'doe@example.com')\n  .then(response => {\n    console.log(response)\n  })\n`\n```\nBut this also failed. It seems that it is not possible to query the users table.\nWhat am I doing wrong? How can I query for a User ID by a given eMail?\nCan anonybody push me in the right direction?\nThanks,\nNiko",
      "solution": "Found the solution by myself:\nYou cannot directly query the auth.users table. Instead you have to create a copy (e.g. public.users or public.profile, etc.) with the data you want to use later on.\nYou can user triggers to automatically create an entry to the public.users table as soon as a user is created.\nI wrote down some code examples and details in my blog post: Supabase: How to query users table?",
      "question_score": 10,
      "answer_score": 12,
      "created_at": "2021-07-11T09:58:30",
      "url": "https://stackoverflow.com/questions/68334303/supabase-how-to-query-users-by-email"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "postgresql",
      "question_id": 74929193,
      "title": "REVINFO table is missing the sequence &quot;revinfo_seq&quot;",
      "problem": "I am migrating to SpringBoot 3.0.1 and updated \"hibernate-envers\" version to \"6.1.6.Final\". My DB is PostgreSQL 13.6.\nHibernate is configured to create the DB schema:\n`spring.jpa.hibernate.ddl-auto:create`\nAfter starting the application I get the following error:\n```\n`pim 2022-12-27 12:00:13,715 WARN  C#c7b942ec-33b4-4749-b113-22cbb2946a8d [http-nio-9637-exec-1]     SqlExceptionHelper/133              - SQL Error: 0, SQLState: 42P01\npim 2022-12-27 12:00:13,715 ERROR C#c7b942ec-33b4-4749-b113-22cbb2946a8d [http-nio-9637-exec-1]     SqlExceptionHelper/138              - ERROR: relation \"revinfo_seq\" does not exist\n  Position: 16\n`\n```\nThe revinfo table look like this:\n```\n`create table revinfo\n(\n    revision           bigint not null\n        primary key,\n    client_id          varchar(255),\n    correlation_id     varchar(255),\n    origin             varchar(255),\n    request_id         varchar(255),\n    revision_timestamp bigint not null,\n    timestamp_utc      timestamp with time zone,\n    user_name          varchar(255)\n);\n`\n```\nThe sequence \"revinfo_seq\" does not exist, but in the old DB structure with envers\n```\n`5.6.8.Final\n`\n```\nand SpringBoot 2.6.6 it didn't exist either without any problems.\nWhat am i Missing?\nI tried to toggle the paramter\n```\n`org.hibernate.envers.use_revision_entity_with_native_id\n`\n```\nbut it did not help.",
      "solution": "You can solve it with this property:\n`spring.jpa.properties.hibernate.id.db_structure_naming_strategy: legacy`\nTested with Spring Boot 3.0.1\nReason:\nHibernate 6 changed the sequence naming strategy, so it was searching for a sequence ending with \"_seq\".\nYou can read a really detailed explanation here: https://thorben-janssen.com/sequence-naming-strategies-in-hibernate-6/",
      "question_score": 10,
      "answer_score": 24,
      "created_at": "2022-12-27T13:20:46",
      "url": "https://stackoverflow.com/questions/74929193/revinfo-table-is-missing-the-sequence-revinfo-seq"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "postgresql",
      "question_id": 78669556,
      "title": "Possible bug with PHP PDO and with PostgreSQL",
      "problem": "At the startup of the docker application (with laravel php), for 1 request, connection to database is fine. After the first request I start to get this error.\n```\n`SQLSTATE[08006] [7] could not send SSL negotiation packet: Resource temporarily unavailable (Connection: pgsql, SQL: (select * from ........)\n`\n```\nUsing:\n\nLaravel v10 and above.\nPHP 8.3 and above\nDocker with Ubuntu Latest\n\nI tracked down this problem until I found out that PDO is actually not opening a connection to PostgreSQL. I tested it with iptraf and both pg_connect and PDO. When we use PDO, we get the error above and but when I try to use pg_connect, we can connect and even make a query.\nSo my findings are, when using iptraf\n\nCannot open a connection using PDO\nIPTraf does not show connection openned with PDO\nI can open a connection using pg_connect\nI can open a connection from a database manager application\nHappening on both development and production environments\n\n[EDIT]\nNew findings:\n\nThe whole setup is working on a virtual machine rather then a docker.",
      "solution": "Check the php-swoole package version on your failed deployment.\nIf it is 6.0.0 probably you have here the problem.\nEdit:\nWe also have this problem, we deployed a container compiled from last week and one with the same code but compiled this week, the difference was that the swoole package had been updated from version 5x to 6.0.0, which is an alpha version. Mysteriously, this version has sneaked into the Ubuntu repository, not being recommended for production and its changelog indicates several changes and incompatibilities with PDO.\nFrom php pecl\n\nNo longer supports Swoole\\Coroutine\\PostgreSQL coroutine client.\nSwoole-v6.0.0-alpha is a test version and cannot be used in any production environment; it is for testing purposes only.\n\nHOW TO SOLVE IT:\nRemove swoole if you don't need it.\nIf you need it, right now the previous version is not listed on the repo, so you need to get it alternatively.",
      "question_score": 10,
      "answer_score": 9,
      "created_at": "2024-06-25T23:20:02",
      "url": "https://stackoverflow.com/questions/78669556/possible-bug-with-php-pdo-and-with-postgresql"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "postgresql",
      "question_id": 72883838,
      "title": "Can&#39;t connect PostgreSQL database to FastAPI",
      "problem": "So, hi. Everything works with SQLite, but when I try to add PostgreSQL according to the user's guide on FastAPI, nothing works and I get:\n`sqlalchemy.exc.ProgrammingError: (psycopg2.ProgrammingError) invalid dsn: invalid connection option \"check_same_thread\"`\nMy `database.py` is:\n`from sqlalchemy import create_engine\nfrom sqlalchemy.ext.declarative import declarative_base\nfrom sqlalchemy.orm import sessionmaker\n\n#SQLALCHEMY_DATABASE_URL = \"sqlite:///./sql_app.db\"\nSQLALCHEMY_DATABASE_URL = \"postgresql://user:password@postgresserver/db\"\n\nengine = create_engine(\n    SQLALCHEMY_DATABASE_URL, connect_args={\"check_same_thread\": False}\n)\nSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\n\nBase = declarative_base()\n`",
      "solution": "`check_same_thread` is an argument specific to sqlite. As you've specified a Postgres URL, you can remove that argument and you should have no issue creating an engine.\ni.e:\n```\n`from sqlalchemy.ext.declarative import declarative_base\nfrom sqlalchemy.orm import sessionmaker\n\nSQLALCHEMY_DATABASE_URL = \"postgresql://user:password@postgresserver/db\"\n\nengine = create_engine(SQLALCHEMY_DATABASE_URL)\nSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\n\nBase = declarative_base()\n`\n```",
      "question_score": 10,
      "answer_score": 24,
      "created_at": "2022-07-06T14:47:28",
      "url": "https://stackoverflow.com/questions/72883838/cant-connect-postgresql-database-to-fastapi"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "postgresql",
      "question_id": 70116148,
      "title": "Getting top rows with the latest related row in joined table quickly",
      "problem": "There are two tables `conversations` and `messages`, I want to fetch conversations along with the content of their latest message.\n`conversations` - id(PRIMARY KEY), name, created_at\n`messages` - id, content, created_at, conversation_id\nCurrently we are running this query to get the required data\n```\n`SELECT\n    conversations.id,\n    m.content AS last_message_content,\n    m.created_at AS last_message_at\nFROM\n    conversations\nINNER JOIN messages m ON conversations.id = m.conversation_id\n                     AND m.id = (\n    SELECT\n        id\n    FROM\n        messages _m\n    WHERE\n        m.conversation_id = _m.conversation_id\n    ORDER BY\n        created_at DESC\n    LIMIT 1)\nORDER BY\n    last_message_at DESC\nLIMIT 15\nOFFSET 0\n`\n```\nThe above query is returning the valid data but its performance decreases with the increasing number of rows. Is there any other way to write this query with increased performance? Attaching the fiddle for example.\nhttp://sqlfiddle.com/#!17/2decb/2\nAlso tried the suggestions in one of the deleted answers:\n```\n`SELECT DISTINCT ON (c.id)\n       c.id,\n       m.content AS last_message_content,\n       m.created_at AS last_message_at\n  FROM conversations AS c\n INNER JOIN messages AS m\n    ON c.id = m.conversation_id \n ORDER BY c.id, m.created_at DESC\n LIMIT 15 OFFSET 0\n`\n```\nhttp://sqlfiddle.com/#!17/2decb/5\nBut the problem with this query is it doesn't sort by `m.created_at`. I want the resultset to be sorted by `m.created_at DESC`",
      "solution": "Have you tried a lateral join instead?\n`SELECT\n    conversations.id,\n    m.content AS last_message_content,\n    m.created_at AS last_message_at\nFROM \"conversations\" \nINNER JOIN LATERAL (\n  SELECT content, created_at \n  FROM  messages m\n  WHERE conversations.id = m.conversation_id \n  ORDER BY created_at DESC\n  FETCH FIRST 1 ROW ONLY\n) m ON TRUE\nORDER BY last_message_at DESC\nLIMIT 15 OFFSET 0\n`",
      "question_score": 10,
      "answer_score": 1,
      "created_at": "2021-11-25T20:02:43",
      "url": "https://stackoverflow.com/questions/70116148/getting-top-rows-with-the-latest-related-row-in-joined-table-quickly"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "postgresql",
      "question_id": 71251937,
      "title": "Error: P1001: Can&#39;t reach database server at `localhost`:`5200`",
      "problem": "Technologies I use:\n\nnestjs -> for backend\nprisma -> for orm\npostgresql -> database\n\nI'm trying to run these technologies using Docker but I'm running into the following issue:\n```\n`prisma schema loaded from prisma/schema.prism\nDatasource \"db\": PostgreSQL database \"nestjs\", schema \"public\" at \"localhost:5200\"\nError: P1001: Can't reach database server at `localhost`:`5200`\nPlease make sure your database server is running at `localhost`:`5200`\n`\n```\ndocker-compose.dev.yml\n```\n`version: '3.7'\nservices:\n  db:\n    image: postgres:12.9\n    ports:\n      - 5200:5432\n    environment:\n      POSTGRES_USER: postgres\n      POSTGRES_PASSWORD: 123\n      POSTGRES_DB: nestjs\n    volumes:\n      - database-data:/var/lib/postgresql/data\n    networks:\n      - sai\n    restart: always\n\n  test:\n    container_name: test\n    image: test\n    build:\n      context: .\n      target: development\n      dockerfile: Dockerfile\n    command: npm run start:prod\n    ports:\n      - 3000:3000\n      - 9229:9229\n    networks:\n      - sai\n    volumes:\n      - .:/usr/src/app\n      - /usr/src/app/node_modules\n    links:\n      - db\n    depends_on:\n      - db\n    restart: always\n\nnetworks:\n  sai:\n    driver: bridge\n\nvolumes:\n  database-data:\n\n`\n```\nNestjs does not see my locahost database on port 5200.\nDockerfile file:\n```\n`FROM node:latest as development\nWORKDIR /usr/src/app\nCOPY package*.json ./\nRUN npm install --only=development\nCOPY . .\nRUN npm run build\n\nFROM node:latest as production\nARG NODE_ENV=production\nENV NODE_ENV=${NODE_ENV}\nWORKDIR /usr/src/app\nCOPY package*.json ./\nRUN npm install --only=production\nCOPY . .\nCOPY --from=development /usr/src/app/prisma ./prisma\nCOPY --from=development /usr/src/app/dist ./dist\nEXPOSE 3000\nCMD npm run start:prod\n`\n```\nThe npm run start:prod command also corresponds to the following in the package.json file:\n```\n`...\n  \"generate:prisma\": \"npx prisma migrate dev --name init\",\n  \"start:prod\": \"npm run generate:prisma && npm run dist/main\",\n...\n`\n```",
      "solution": "Instead of using `localhost:5200` as the address of the database, you need to use `db:5432`.\nLocalhost and the mapped port are used when connecting from the host machine. You're connecting from another container on the bridge network and there you need to use the service name and the port the container is listening on.",
      "question_score": 10,
      "answer_score": 16,
      "created_at": "2022-02-24T13:25:08",
      "url": "https://stackoverflow.com/questions/71251937/error-p1001-cant-reach-database-server-at-localhost5200"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "postgresql",
      "question_id": 69554968,
      "title": "Connections are not closed and piling up when running @SpringBootTest classes",
      "problem": "We have Spring Boot integration tests, and keep writing new ones regularly.\nI noticed database connections have been piling up: the more tests I run, the higher the connection peak to my PostgreSQL instance.\nIt reached a point where there are more than 300 connections requested by Spring Boot when running all tests, and it started failing the build (our `max_connection` is set to 300).\nAfter some research, it came to my understanding that connections are not being released after tests have been run, because of Spring Boot test: if context is not explicitly destroyed, connections are not closed.\nI find it quite weird, but tried using `@DirtiesContext` to prove a point, on all of our test classes, it indeed fixed the issue in a sense that it avoided peaks (no more than 30 connections at once, not piling up to 300 like before) but since this annotation forces context recreation before each test class, the build got much slower and I find it not very satisfactory to need to recreate a Spring context every time just to make sure connections are closed properly.\nData source is a `HikariDataSource`, configured using a configuration class.\nAnother workaround I found is to change maximum pool size for Hikari. I set it to something lower than the default value of 10 (I'm not sure it's useful to reserve 10 connections for each test class).\u2028This change effectively lowers the total number of connections when I run all tests but they are still piling up (only lower!)\nI think I'm missing something, how can I ensure that connections are closed after each test class? There has to be a better way than `@DirtiesContext`, I just can't find it. Thanks for your help.",
      "solution": "It turns out that context was recreated almost with every test class because I was extensively using `@MockBean` annotation in my tests. Since it affects Spring context, each `@MockBean`/No MockBean combination in different test classes counts as a different context, i.e.:\n\nTest class 1: bean MyService is a MockBean, MyOtherService is not\nTest class 2: bean MyService is a MockBean, MyOtherService is also a MockBean\nTest class 3: none of these two beans is a MockBean\n\nIn such case, a new Spring context will created for each class because the bean configuration is different, resulting in an increasing number of connections to the datasource.\nTo (partially) solve this, I looked for patterns in the beans combinations of my test classes and created a new class I called `TestMockBeans`.\nIts sole purpose is to declare as many MockBeans and/or SpyBeans as possible to re-use in similar test configurations. I extend corresponding test classes with `TestMockBeans`, and then, because they share this similar setup, Spring identifies their contexts as similar and does not recreate a new one for every test class.\nAs you can guess, not all of my tests throughout the Spring boot app share the same need for Mockbeans (or absence of Mockbeans) so it's only a partial solution, but I hope it will help someone experiencing the same issue to mitigate it.",
      "question_score": 10,
      "answer_score": 12,
      "created_at": "2021-10-13T13:45:55",
      "url": "https://stackoverflow.com/questions/69554968/connections-are-not-closed-and-piling-up-when-running-springboottest-classes"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "postgresql",
      "question_id": 72192789,
      "title": "CteInsertStrategy can only be used with Dialects that support CTE that can take UPDATE or DELETE statements as well",
      "problem": "Hibernate 6.0.1 with PostgreSQL JDBC driver 42.3.5 causes the following exception:\n```\n`java.lang.UnsupportedOperationException:\nCteInsertStrategy can only be used with Dialects that support CTE that can take UPDATE or DELETE statements as well\nat org.hibernate.query.sqm.mutation.internal.cte.CteInsertStrategy.(CteInsertStrategy.java:123)\nat org.hibernate.query.sqm.mutation.internal.cte.CteInsertStrategy.(CteInsertStrategy.java:107)\nat org.hibernate.dialect.PostgreSQLDialect.getFallbackSqmInsertStrategy(PostgreSQLDialect.java:704)\n...\n`\n```\nWhat's wrong and how can I fix the issue?\nMyEntity.java\n`import jakarta.persistence.*;\n\n@Entity\n@Table(name = \"my_entity\")\npublic class MyEntity {\n\n    private Long id;\n\n    @Id\n    @SequenceGenerator(name = \"id_sequence\", sequenceName = \"my_id_sequence\")\n    @GeneratedValue(strategy = GenerationType.AUTO, generator = \"id_sequence\")\n    public Long getId() {\n        return this.id;\n    }\n    public void setId(Long id) {\n        this.id = id;\n    }\n\n}\n`\nMyTest.java\n`import static org.junit.Assert.assertNotNull;\n\nimport org.hibernate.*;\nimport org.hibernate.cfg.*;\nimport org.junit.*;\n\npublic class MyTest {\n\n    private static Configuration configuration;\n    private static SessionFactory sessionFactory;\n\n    @BeforeClass\n    public static void setUpBeforeClass() throws Exception {\n        configuration = new Configuration().configure();\n        sessionFactory = configuration.buildSessionFactory();\n    }\n\n    @AfterClass\n    public static void tearDownAfterClass() throws Exception {\n        sessionFactory.close();\n    }\n\n    private Session session;\n\n    @Before\n    public void setUp() throws Exception {\n        session = sessionFactory.openSession();\n    }\n\n    @After\n    public void tearDown() throws Exception {\n        session.close();\n    }\n\n    @Test\n    public void test() {\n        Transaction transaction = session.beginTransaction();\n        MyEntity entity = new MyEntity();\n        session.persist(entity);\n        assertNotNull(entity.getId());\n        transaction.commit();\n    }\n\n}\n`\nhibernate.cfg.xml\n`\n\n  \n    org.postgresql.Driver\n    org.hibernate.dialect.PostgreSQLDialect  \n    jdbc:postgresql://localhost:5432/mydb\n    update\n    postgres\n    false\n    1\n    30\n    120\n    100\n    \n  \n\n`\nbuild.gradle\n```\n`plugins {\n    id 'java-library'\n}\n\nrepositories {\n    mavenCentral()\n}\n\next {\n    hibernateVersion = '6.0.1.Final'\n}\n\ndependencies {\n    implementation 'org.postgresql:postgresql:42.3.5'\n    implementation 'org.hibernate.orm:hibernate-c3p0:' + hibernateVersion\n    implementation 'org.hibernate.orm:hibernate-core:' + hibernateVersion\n    testImplementation 'junit:junit:4.13.2'\n}\n`\n```\nSee the full source code here.",
      "solution": "The configuration property `use_jdbc_metadata_defaults` must be `true` for Hibernate to detect the correct version of the PostgreSQL dialect.\nRemoving this line\n```\n`false\n`\n```\nfrom `hibernate.cfg.xml` resolves the issue.\n(Thanks to Christian at Hibernate Zulip channel for sorting this out.)",
      "question_score": 10,
      "answer_score": 8,
      "created_at": "2022-05-10T22:51:08",
      "url": "https://stackoverflow.com/questions/72192789/cteinsertstrategy-can-only-be-used-with-dialects-that-support-cte-that-can-take"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "postgresql",
      "question_id": 74141729,
      "title": "PostgreSQL - &quot;Include Error Detail&quot; connection string parameter - how sensitive is the information returned?",
      "problem": "https://www.npgsql.org/doc/connection-string-parameters.html\n\nInclude Error Detail -\nWhen enabled, PostgreSQL error and notice details are included on PostgresException.Detail and PostgresNotice.Detail. These can contain sensitive data.\n\nIf I provide the \"Include Error Detail=True\" in the connection stringg to PostgreSQL, what sensitive data do I need to be concerned about? If the query itself is returned in an exception or error message, that is fine by me, but if say the connection password were returned in plaintext obviously that would be bad. What sensitive data is conditionally included in errors based on this parameter?",
      "solution": "These messages include no sensitive data that the database user should not see. So I wouldn't worry, unless perhaps you show the information to the application user rather than logging them. Your database user may have access to information that the application user shouldn't see.",
      "question_score": 10,
      "answer_score": 5,
      "created_at": "2022-10-20T16:37:43",
      "url": "https://stackoverflow.com/questions/74141729/postgresql-include-error-detail-connection-string-parameter-how-sensitive"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "postgresql",
      "question_id": 69471840,
      "title": "Cannot query across one-to-many for property NestJS and TypeORM",
      "problem": "I have two entities one is `car` and another one is `carAvailability`\n```\n`import { Entity, Column, PrimaryGeneratedColumn, OneToMany } from 'typeorm';\nimport { CarAvailability } from 'src/car-availabilitys/car-availability.entity';\n\n@Entity('cars')\nexport class Car {\n  @PrimaryGeneratedColumn()\n  id: number;\n\n  @Column()\n  name: string;\n\n  @OneToMany(() => CarAvailability, (carAvailability) => carAvailability.car, {\n    eager: true,\n    cascade: true,\n  })\n  availabilities: CarAvailability[];\n}\n`\n```\nI am trying to add a service that queries and filters cars based on the availabilities. In My Service and tried two ways:\nMethod 1 with repo functions:\n```\n`async test () {\n  const startDateTime = '2012-04-24 02:25:43.511';\n\n  return await this.repo.find({\n    relations: ['availabilities'],\n    where: {\n      availabilities: {\n        start_date_time: startDateTime\n      }\n    }\n  });\n}\n`\n```\nMethod 2 with query builder:\n```\n`async test () {\n  const startDateTime = '2012-04-24 02:25:43.511';\n\n  return this.repo.createQueryBuilder('cars')\n    .innerJoin('cars.availabilities', 'car_availabilities')\n    .where(\"cars.availabilities.start_date_time = :startDateTime\", { startDateTime })\n    .getMany();\n}\n`\n```\nMethod 1 error:\n```\n`Error: Cannot query across one-to-many for property availabilities\n`\n```\nMethod 2 error:\n```\n`QueryFailedError: missing FROM-clause entry for table \"availabilities\"\n`\n```\nI feel like I am missing something but I am not sure. Have referred both NestJS and TypeORM docs but can't seem to figure out what went wrong.",
      "solution": "Using Method 2, because you aliased `car.availabilities` as `car_availabilities`, your `where` clause should use the alias name:\n```\n`car_availabilities.start_date_time\n`\n```\nnot:\n```\n`cars.availabilities.start_date_time\n`\n```",
      "question_score": 10,
      "answer_score": 7,
      "created_at": "2021-10-06T21:44:18",
      "url": "https://stackoverflow.com/questions/69471840/cannot-query-across-one-to-many-for-property-nestjs-and-typeorm"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "postgresql",
      "question_id": 69896106,
      "title": "cannot use column reference in DEFAULT expression - trying to set up default value for custom column name",
      "problem": "I am having this table which I migrated from MySQL using pgLoader\n```\n`CREATE TABLE IF NOT EXISTS schema.example\n(\n    \"ExampleID\" integer NOT NULL,\n    \"Name\" character varying(255) COLLATE pg_catalog.\"default\" NOT NULL,\n    CONSTRAINT \"idx_43997_PRIMARY\" PRIMARY KEY (\"ExampleID\"),\n)\n`\n```\nI am trying to convert the ExampleID to be serial so it will auto increment using this method:\nChanging primary key int type to serial\nSo I am doing\n```\n`CREATE SEQUENCE example_id_seq MINVALUE 3\n`\n```\nWhich works fine, but then\n```\n`ALTER TABLE example ALTER \"ExampleID\" SET DEFAULT nextval('example_id_seq')\n`\n```\nGives the error:\n```\n`ERROR:  cannot use column reference in DEFAULT expression\nSQL state: 0A000\n`\n```\nBut if I remove the \"\" and just put ExampleID like that:\n```\n`ALTER TABLE example ALTER ExampleID SET DEFAULT nextval('example_id_seq')\n`\n```\nI will get the error\n```\n`ERROR:  column \"exampleid\" of relation \"example\" does not exist\nSQL state: 42703\n`\n```",
      "solution": "The problem was that unlike in the example\n```\n`ALTER TABLE example ALTER \"ExampleID\" SET DEFAULT nextval('example_id_seq')\n`\n```\nI forgot to put ' ' next to the sequence name so it was\n```\n`ALTER TABLE example ALTER \"ExampleID\" SET DEFAULT nextval(example_id_seq)\n`\n```\nThe column reference was that it mistaken the sequence for column name reference.",
      "question_score": 9,
      "answer_score": 17,
      "created_at": "2021-11-09T10:57:37",
      "url": "https://stackoverflow.com/questions/69896106/cannot-use-column-reference-in-default-expression-trying-to-set-up-default-val"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "postgresql",
      "question_id": 71077403,
      "title": "What is difference between pg_squeeze and pg_repack?",
      "problem": "I've been working on optimizing tables in database. One of our table requires monthly vacuuming because of cleaning up processes.pg_squeeze Table size can get upto 25 GB. As this table is used by production users, we can't afford downtime every month to run `VACUUM FULL`.\nI found that pg_squeeze and pg_repack can be used for this purpose. But I'm not able to understand the difference between those two. Can someone please explain what is the difference and which will be more suitable for me to use?\nThank you.",
      "solution": "The main difference is that pg_squeeze operates entirely inside the database, with no need for an external binary.  It also has a background worker that will schedule table rewrites automatically if certain criteria are met.\nSo you could say that pg_repack is less invasive (for example, installation requires no restart of the database), but pg_squeeze is more feature complete.\nDisclaimer: I work for the company who wrote pg_squeeze.",
      "question_score": 9,
      "answer_score": 19,
      "created_at": "2022-02-11T09:58:01",
      "url": "https://stackoverflow.com/questions/71077403/what-is-difference-between-pg-squeeze-and-pg-repack"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "postgresql",
      "question_id": 75482094,
      "title": "Postgresql 15 - Trailing junk after numeric literal error",
      "problem": "After the Postgresql update v15, I realised that even I have a column that accepts UUID data type, it will throw me similar error like this whenever I try to Insert UUID data into the table.\nScript:\n`INSERT INTO public.testing(uuid, rating) VALUES (${uuid}, ${rating});\n`\nError:\n\nerror running query error: trailing junk after numeric literal at or near \"45c\"\n\nPostgresql 15 release note:\n\nPrevent numeric literals from having non-numeric trailing characters (Peter Eisentraut)\n\nIs there any solution for this issue? Or there an alternative data type that allows storing UUID into my table?",
      "solution": "It seems that you forgot the single quotes around the UUID, so that the PostgreSQL parser took the value for a subtraction and complained that there were letters mixed in with the digits. This may throw a different error on older PostgreSQL versions, but it won't do the right thing either.\nBe careful about SQL injection when you quote the values.",
      "question_score": 9,
      "answer_score": 20,
      "created_at": "2023-02-17T09:56:12",
      "url": "https://stackoverflow.com/questions/75482094/postgresql-15-trailing-junk-after-numeric-literal-error"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "postgresql",
      "question_id": 77975676,
      "title": "I am getting an error in pgadmin as &quot;unrecognized configuration parameter &quot;lc_collate&quot; &quot; how to fix this?",
      "problem": "Refer this!\nI was creating a database and got this error i tried to uninstall and then reinstall postgresql-16 and pgadmin4 but it didn't work for me! It creates a database but gives this error i am worried that what if it would block my task when i am half done !",
      "solution": "The parameter `lc_collate` got removed in PostgreSQL v16, because it didn't reflect the actual collation any more.\nUse a version of pgAdmin that supports PostgreSQL v16. According to the release notes, that should be 7.8 or better (but use the latest one).",
      "question_score": 9,
      "answer_score": 15,
      "created_at": "2024-02-11T06:17:18",
      "url": "https://stackoverflow.com/questions/77975676/i-am-getting-an-error-in-pgadmin-as-unrecognized-configuration-parameter-lc-co"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "postgresql",
      "question_id": 70637566,
      "title": "cannot link libpq on Mac M1",
      "problem": "I've been trying to run the Rust Diesel crate on my Macbook M1 and it doesn't work. The final part of the compilation gets broken by the following error:\n```\n`  = note: ld: warning: ignoring file /usr/local/Cellar/libpq/14.1/lib/libpq.dylib, building for macOS-arm64 but attempting to link with file built for macOS-x86_64\n          Undefined symbols for architecture arm64:\n`\n```\nWhen I get the info for `libpq` I get the following:\n```\n`maxwellflitton@Maxwells-MacBook-Pro vanguard % brew info libpq                                                           \nlibpq: stable 14.1 (bottled) [keg-only]\nPostgres C API library\nhttps://www.postgresql.org/docs/14/libpq.html\n/usr/local/Cellar/libpq/14.1 (2,335 files, 27.8MB)\n  Poured from bottle on 2022-01-09 at 00:14:32\nFrom: https://github.com/Homebrew/homebrew-core/blob/HEAD/Formula/libpq.rb\nLicense: PostgreSQL\n==> Dependencies\nRequired: krb5 \u2714, openssl@1.1 \u2714\n==> Caveats\nlibpq is keg-only, which means it was not symlinked into /usr/local,\nbecause conflicts with postgres formula.\n\nIf you need to have libpq first in your PATH, run:\n  echo 'export PATH=\"/usr/local/opt/libpq/bin:$PATH\"' >> ~/.zshrc\n\nFor compilers to find libpq you may need to set:\n  export LDFLAGS=\"-L/usr/local/opt/libpq/lib\"\n  export CPPFLAGS=\"-I/usr/local/opt/libpq/include\"\n\n`\n```\nI've tried installing with the following command:\n```\n`RUSTFLAGS='-L /usr/local/Cellar/libpq/14.1/lib' cargo install diesel_cli --no-default-features --features postgres --force\n`\n```\nBut I still get the same error. Will it just be easier to wipe the whole thing and start again and if so how would I do this? Other people on the internet using the M1 seem to be able to get round the problem with a simple `brew install libpq`. Never had any issues with my previous intel mac. My `~/.cargo/config.toml` has the following configuration:\n```\n`[target.aarch64-apple-darwin]\nrustflags = '-L /usr/local/Cellar/libpq/14.1/lib -L /opt/homebrew/lib'\n`\n```",
      "solution": "Surprisingly this worked\n\n`brew install postgresql libpq`\n`cargo clean`\n`cargo build`\n`cargo install diesel_cli --no-default-features --features postgres`\n\nI think it had to do with installing `diesel-cli` before installing the necessary dependencies. Cleaning the cargo dependencies and reinstalling worked for me",
      "question_score": 9,
      "answer_score": 10,
      "created_at": "2022-01-09T01:36:21",
      "url": "https://stackoverflow.com/questions/70637566/cannot-link-libpq-on-mac-m1"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "postgresql",
      "question_id": 66863709,
      "title": "Running a SELECT * FROM table query in Pgx?",
      "problem": "I'm using PostgreSQL with , and I'm trying to run a SELECT * FROM query in Pgx.\nI can't seem to get the iteration down, as it only returns the last key in the table. I'm also trying to serve this as JSON on running the echo HTTP server library.\nmain.go function ( importing connection.Conn from my database connection file)\n```\n`func getLanguages(c echo.Context) (err error) {\n\n    Conn := connection.Conn\n    type Row struct {\n        Id       int\n        Language string\n        Name     string\n    }\n\n    rowArray := Row{}\n\n    rows, err := Conn.Query(context.Background(), \"SELECT * FROM languages\")\n\n    defer rows.Close()\n\n    // rowsCount := 0\n\n    for rows.Next() {\n        err := rows.Scan(&rowArray.Id, &rowArray.Language, &rowArray.Name)\n        if err != nil {\n            log.Fatal(err)\n        }\n    }\n\n    fmt.Println(rowArray)\n\n    return c.JSON(http.StatusOK, rowArray)\n\n} \n`\n```\nExpected Output\n```\n` id |  language  |        name         \n----+------------+---------------------\n  1 | none       | Plaintext\n  2 | go         | Golang\n  3 | python     | Python\n  4 | js         | JavaScript\n  5 | jsx        | React JSX\n  6 | ts         | TypeScript\n  7 | tsx        | React TSX\n  8 | tsconfig   | TSConfig\n  9 | rb         | Ruby\n 10 | sql        | SQL\n 11 | sol        | Solidity (Ethereum)\n 12 | html       | HTML\n 13 | css        | CSS\n 14 | csharp     | C#\n 15 | haskell    | Haskell\n 16 | rust       | Rust\n 17 | scala      | Scala\n 18 | svg        | SVG\n 19 | graphql    | GraphQL\n 20 | php        | PHP\n 21 | powershell | PowerShell\n 22 | yaml       | YAML\n 23 | json       | JSON\n`\n```\nActual Output\n```\n`{\"Id\":23,\"Language\":\"json\",\"Name\":\"JSON\"} \n`\n```\nAny help?",
      "solution": "`rowArray := Row{}` creates a single instance of `Row`. You then loop through the results and store them in that same variable. So each iteration will overwrite whatever was retrieved previously. If there are multiple results then use a slice; something like the following, untested, code:\n`rows, err := Conn.Query(context.Background(), \"SELECT * FROM languages\")\nif err != nil {\n    log.Fatal(err)\n}\ndefer rows.Close()\n\nvar rowSlice []Row\nfor rows.Next() {\n    var r Row\n    err := rows.Scan(&r.Id, &r.Language, &r.Name)\n    if err != nil {\n        log.Fatal(err)\n    }\n   rowSlice = append(rowSlice, r)\n}\nif err := rows.Err(); err != nil {\n    log.Fatal(err)\n}\n\nfmt.Println(rowSlice)\n`",
      "question_score": 9,
      "answer_score": 18,
      "created_at": "2021-03-30T03:39:18",
      "url": "https://stackoverflow.com/questions/66863709/running-a-select-from-table-query-in-pgx"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "postgresql",
      "question_id": 65921227,
      "title": "How to use keyTextTransform() for nested json?",
      "problem": "My model has a json field. I can access jsonfield['key1'] with the following query\n```\n`from django.contrib.postgres.fields.jsonb import KeyTextTransform\nMyModel.objects.annotate(val=KeyTextTransform('key1', 'jsonfield')).order_by('val')\n`\n```\nBut how can I access a key like jsonfield['key1']['key2'] or even more nested ones?\nThis can't be the only solution, right?\n```\n`MyModel.objects.annotate(val=KeyTextTransform('key2', (KeyTextTransform('key1', 'jsonfield'))).order_by('val')\n`\n```",
      "solution": "The hard part is already done, thankfully. `KeyTextTransform` is composable. All we have to do is compose it.\n`class NestableKeyTextTransform:\n    def __new__(cls, field, *path):\n        if not path:\n            raise ValueError(\"Path must contain at least one key.\")\n        head, *tail = path\n        field = KeyTextTransform(head, field)\n        for head in tail:\n            field = KeyTextTransform(head, field)\n        return field\n\nMyModel.objects.annotate(\n    single_nested_value=NestableKeyTextTransform(\n      \"json_field\", \"query\", \"name\"\n    ),\n    array_access=NestableKeyTextTransform(\n      \"json_field\", \"query\", \"address_line\", 1\n    ),\n)\n\n`\nThough I would like to point out that this may be a better way to do it:\n`from django.db.models import F\n\nMyModel.objects.annotate(\n    single_nested_value=F(\"json_field__query__name\"),\n    array_access=F(\"json_field__query__address_line__1\"),\n)\n`",
      "question_score": 9,
      "answer_score": 11,
      "created_at": "2021-01-27T15:31:52",
      "url": "https://stackoverflow.com/questions/65921227/how-to-use-keytexttransform-for-nested-json"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "postgresql",
      "question_id": 76399047,
      "title": "How to represent &#39;bytea&#39; datatype from pg inside new drizzle orm?",
      "problem": "Im trying to learn new drizzle orm for node js, and im here trying to create a small auth database to see how the orm works.\nusing 'pnpm drizzle-kit generate:pg' i generated a schema from a pg database, but bytea datatype was not parsed to ts. as the drizzle is new a orm, the doc dosen't have solution for my problem. i needed a way to represent bytea pg datatype inside drizzle orm.\nhere is the schema code generated by drizzle kit.\n`export const user = pgTable(\n  \"user\",\n  {\n    id: uuid(\"id\").primaryKey().notNull(),\n    firstname: varchar(\"firstname\", { length: 35 }).notNull(),\n    middlename: varchar(\"middlename\", { length: 35 }),\n    lastname: varchar(\"lastname\", { length: 35 }).notNull(),\n    // TODO: failed to parse database type 'bytea'\n    passphrase: unknown(\"passphrase\").notNull(),\n    // TODO: failed to parse database type 'bytea'\n    salt: unknown(\"salt\").notNull(),\n    email: varchar(\"email\", { length: 50 }).notNull(),\n  },\n  (table) => {\n    return {\n      email: uniqueIndex(\"user_email\").on(table.email),\n    };\n  }\n);\n\n`",
      "solution": "You can use custom types. Here is the implementation of the bytea custom type:\n`const bytea = customType({\n  dataType() {\n    return \"bytea\";\n  },\n});\n`\nYou can use toDriver and fromDriver functions to map the app data to db type, and db type to app data. For instance if you want to store bytea in db, but want to retrieve it as hex string in your application, you can go like this:\n`const bytea = customType({\n  dataType() {\n    return \"bytea\";\n  },\n  toDriver(val) {\n    let newVal = val;\n    if (val.startsWith(\"0x\")) {\n      newVal = val.slice(2);\n    }\n\n    return Buffer.from(newVal, \"hex\");\n  },\n  fromDriver(val) {\n    return val.toString(\"hex\");\n  },\n});\n`",
      "question_score": 9,
      "answer_score": 16,
      "created_at": "2023-06-04T07:29:48",
      "url": "https://stackoverflow.com/questions/76399047/how-to-represent-bytea-datatype-from-pg-inside-new-drizzle-orm"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 73596058,
      "title": "Creating an SQLAlchemy engine based on psycopg3",
      "problem": "I need to upgrade the following code to some equivalent code based on `psycopg` version 3:\n```\n`import psycopg2\nfrom sqlalchemy import create_engine\n\nengine = create_engine('postgresql+psycopg2://', creator=connector)\n`\n```\nThis psycopg2 URL worked like a charm, but:\n```\n`import psycopg # v3.1\nfrom sqlalchemy import create_engine\n\nengine = create_engine('postgresql+psycopg://', creator=connector)\n`\n```\n(I also tried the 'psycopg3' word without success)\nreturns:\n```\n`Traceback (most recent call last):\n\n  File \"/tmp/ipykernel_1032556/253047102.py\", line 1, in \n    engine = create_engine('postgresql+psycopg://', creator=connector)\n\n  File \"\", line 2, in create_engine\n\n  File \"/usr/local/lib/python3.10/dist-packages/sqlalchemy/util/deprecations.py\", line 309, in warned\n    return fn(*args, **kwargs)\n\n  File \"/usr/local/lib/python3.10/dist-packages/sqlalchemy/engine/create.py\", line 534, in create_engine\n    entrypoint = u._get_entrypoint()\n\n  File \"/usr/local/lib/python3.10/dist-packages/sqlalchemy/engine/url.py\", line 661, in _get_entrypoint\n    cls = registry.load(name)\n\n  File \"/usr/local/lib/python3.10/dist-packages/sqlalchemy/util/langhelpers.py\", line 343, in load\n    raise exc.NoSuchModuleError(\n\nNoSuchModuleError: Can't load plugin: sqlalchemy.dialects:postgresql.psycopg\n`\n```\nSo, how to properly create an SQLAlchemy engine based on `psycopg` (v3.x)?\nMy `sqlalchemy` version is: '1.4.35' (tried version `1.4.40` but face an `AttributeError: module 'sqlalchemy' has no attribute 'dialects'` error).\npsycopg3 doc: https://www.psycopg.org/psycopg3/docs/api/\nsqlalchemy doc: https://docs.sqlalchemy.org/en/14/core/engines.html",
      "solution": "Just an update to the current answer: Sqlalchemy 2.0 has been released and it does support psycopg3. You need to upgrade to 2.0 to use it.\nNote the connection string will have to be changed from `postgresql` to `postgresql+psycopg` or SqlAlchemy will (at time of writing) try using `psycopg2`.\nSee docs here for more info: https://docs.sqlalchemy.org/en/20/dialects/postgresql.html#module-sqlalchemy.dialects.postgresql.psycopg",
      "question_score": 31,
      "answer_score": 47,
      "created_at": "2022-09-04T01:55:28",
      "url": "https://stackoverflow.com/questions/73596058/creating-an-sqlalchemy-engine-based-on-psycopg3"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 71580859,
      "title": "ImportError when importing psycopg2 on M1",
      "problem": "Has anyone gotten this error when importing `psycopg2` after successful installation?\n```\n`ImportError: dlopen(/Users/chrishicks/Desktop/test/venv/lib/python3.9/site-packages/psycopg2/_psycopg.cpython-39-darwin.so, 0x0002):\ntried: '/Users/chrishicks/Desktop/test/venv/lib/python3.9/site-packages/psycopg2/_psycopg.cpython-39-darwin.so'\n(mach-o file, but is an incompatible architecture\n(have 'x86_64', need 'arm64e')),\n'/usr/local/lib/_psycopg.cpython-39-darwin.so' (no such file),\n'/usr/lib/_psycopg.cpython-39-darwin.so' (no such file)\n`\n```\nI have tried installing `psycopg2` and `psycopg2-binary` and have tried both while running `iTerm` in Rosetta.",
      "solution": "Using this line should fix it:\n```\n`pip3.9 install psycopg2-binary --force-reinstall --no-cache-dir\n`\n```",
      "question_score": 10,
      "answer_score": 35,
      "created_at": "2022-03-23T02:38:00",
      "url": "https://stackoverflow.com/questions/71580859/importerror-when-importing-psycopg2-on-m1"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 77727508,
      "title": "Problem installing psycopg2 for Python venv through poetry",
      "problem": "I am installing psycopg2 into a python venv through poetry `poetry add psycopg2` but i get the below error.\nI have tried to use older versions of psycopg2. Does anyone know why this problem occurs and how to properly fix it?\nFollowing other questions on Stackoverflow I tried to install psycopg2-binary which is installed properly but later, that gives different issues when trying to apply django migrations.\n\nNote: This error originates from the build backend, and is likely not a problem with poetry but with psycopg2 (2.9.9) not supporting PEP 517 builds. You can verify this by running 'pip wheel --use-pep517 \"psycopg2 (==2.9.9)\"'.",
      "solution": "Running the below on my machine solved the issue:\n`apt install libpq-dev gcc`",
      "question_score": 9,
      "answer_score": 11,
      "created_at": "2023-12-28T15:38:05",
      "url": "https://stackoverflow.com/questions/77727508/problem-installing-psycopg2-for-python-venv-through-poetry"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 66660511,
      "title": "Psycopg2 installation error on mac: command &#39;clang&#39; failed with status 1",
      "problem": "I can't install psycopg2 on my m1 Mac. I tried to reinstall openssl with brew. I tried a lot of thing but nothing changed. The error log is super long so I couldn't understand what is wrong. I am facing up with this error when I try to `pip install psycopg2` Waiting for your help.\nHere is the full error log: https://wtools.io/paste-code/b4jG",
      "solution": "The error is happening because it seems to be having some problem with ssl. I used different answers from a similar question to solve it for myself:\n\nInstall openssl if you don't have:\n`brew install openssl`\n\nCheck the path where openssl got installed using:\n`brew --prefix openssl`\n\nUse output from above and add LD flag when running the pip command, for example in my case the output was `/opt/homebrew/opt/openssl@1.1` so I did the following:\n`LDFLAGS=\"-I/opt/homebrew/opt/openssl@1.1/include -L/opt/homebrew/opt/openssl@1.1/lib\" pip install psycopg2`\n\nAnd that did the trick for me.",
      "question_score": 6,
      "answer_score": 11,
      "created_at": "2021-03-16T18:41:22",
      "url": "https://stackoverflow.com/questions/66660511/psycopg2-installation-error-on-mac-command-clang-failed-with-status-1"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 66335662,
      "title": "PostgreSQL COPY FROM csv with different date format",
      "problem": "I am trying to upload CSV data to a PostgreSQL database in Python using the COPY FROM STDIN function.\nIn the CSV file my Date field is DD-MM-YYYY HH:MI and this gives me an error:\n\npsycopg2.errors.DatetimeFieldOverflow: date/time field value out of\nrange: \"31-12-2020 08:09\"\n\nIs there a way I can define the Date/Time format when using COPY FROM?\nDB column is type TIMESTAMP if relevant.\nI only know how to do this with a line-by-line INSERT statement.",
      "solution": "Just before the `COPY` command do:\n```\n`set datestyle = euro;\n\nshow datestyle;\n DateStyle \n-----------\n ISO, DMY\n`\n```\nThen this works:\n```\n`SELECT '31-12-2020 08:09'::timestamp;\n      timestamp      \n---------------------\n 2020-12-31 08:09:00\n`\n```\nOtherwise with my default `datestyle`:\n```\n`show datestyle;\n DateStyle \n-----------\n ISO, MDY\n\nSELECT '31-12-2020 08:09'::timestamp;\nERROR:  date/time field value out of range: \"31-12-2020 08:09\"\nLINE 1: SELECT '31-12-2020 08:09'::timestamp;\n\n`\n```\nFor more information see here Date input Table 8.15. Date Order Conventions",
      "question_score": 6,
      "answer_score": 5,
      "created_at": "2021-02-23T16:04:34",
      "url": "https://stackoverflow.com/questions/66335662/postgresql-copy-from-csv-with-different-date-format"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 65985118,
      "title": "google buildpack psycopg2-binary Error: pg_config executable not found",
      "problem": "This is in my requirements.txt\npsycopg2-binary==2.8.3    # via -r requirements/base.in\nthat I am building inside docker image. I was under impression that if I install `psycopg2-binary vs psycopg2` I should not install additional postgresql devel libs. Am I wrong?\n`pack build --builder=gcr.io/buildpacks/builder:v1 test-python\n\nCollecting psycopg2-binary==2.8.3\n  Downloading psycopg2-binary-2.8.3.tar.gz (378 kB)\n    ERROR: Command errored out with exit status 1:\n     command: /layers/google.python.runtime/python/bin/python3 -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-z7z_l56l/psycopg2-binary_ddfc5ed05bb44cf4b7e2f14d634bd6ae/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-z7z_l56l/psycopg2-binary_ddfc5ed05bb44cf4b7e2f14d634bd6ae/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base /tmp/pip-pip-egg-info-vsc_2hlv\n         cwd: /tmp/pip-install-z7z_l56l/psycopg2-binary_ddfc5ed05bb44cf4b7e2f14d634bd6ae/\n    Complete output (23 lines):\n    running egg_info\n    creating /tmp/pip-pip-egg-info-vsc_2hlv/psycopg2_binary.egg-info\n    writing /tmp/pip-pip-egg-info-vsc_2hlv/psycopg2_binary.egg-info/PKG-INFO\n    writing dependency_links to /tmp/pip-pip-egg-info-vsc_2hlv/psycopg2_binary.egg-info/dependency_links.txt\n    writing top-level names to /tmp/pip-pip-egg-info-vsc_2hlv/psycopg2_binary.egg-info/top_level.txt\n    writing manifest file '/tmp/pip-pip-egg-info-vsc_2hlv/psycopg2_binary.egg-info/SOURCES.txt'\n    \n    Error: pg_config executable not found.\n    \n    pg_config is required to build psycopg2 from source.  Please add the directory\n    containing pg_config to the $PATH or specify the full executable path with the\n    option:\n    \n        python setup.py build_ext --pg-config /path/to/pg_config build ...\n    \n    or with the pg_config option in 'setup.cfg'.\n    \n    If you prefer to avoid building psycopg2 from source, please install the PyPI\n    'psycopg2-binary' package instead.\n    \n    For further information please check the 'doc/src/install.rst' file (also at\n    ).\n\n`",
      "solution": "No, you are not wrong; I had the same issue today, and after a bit of googling I found this (solved) github issue: https://github.com/psycopg/psycopg2/issues/699#\nThe comments there (https://github.com/psycopg/psycopg2/issues/699#issuecomment-377188700) indicate one possible cause for the issue is using an outdated version of pip.\nI was using version 18.x, and after upgrading it (`python -m pip install -U pip`, which got me pip 21.1.3), the installation of psycopg2-binary finished successfully (as expected) even though I don't have any postgres dev libraries installed.",
      "question_score": 6,
      "answer_score": 5,
      "created_at": "2021-01-31T23:54:31",
      "url": "https://stackoverflow.com/questions/65985118/google-buildpack-psycopg2-binary-error-pg-config-executable-not-found"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 72574761,
      "title": "What is the correct way to mock psycopg2 with pytest?",
      "problem": "I need to simulate DB connection without actual connection. All answers I found are trying to mock methods in different ways, connect to docker db, connect to actual PostgreSQL running locally. I believe I need mocking variant but I cannot formulate in my head how should I mock. Am I missing something? Am I moving into wrong direction?\nI use PostgreSQL and psycopg2. Package `psycopg2-binary`\nDatabase connection:\n```\n`import os\n\nimport psycopg2\nfrom loguru import logger\nfrom psycopg2.extensions import parse_dsn\n\ndef init_currency_history_table(cursor):\n    create_users_table_query = \"\"\"\n        CREATE TABLE IF NOT EXISTS history(\n          id BIGINT PRIMARY KEY NOT NULL,\n          event TEXT,\n          creation_date TIMESTAMPTZ DEFAULT NOW()\n        );\n    \"\"\"\n    cursor.execute(create_users_table_query)\n\ndef load_db(db_url):\n    db = psycopg2.connect(**db_url)\n    db.autocommit = True\n    return db\n\nclass PostgresqlApi(object):\n\n    def __init__(self, load=load_db):\n        logger.info(os.environ.get('DATABASE_URL'))\n        db_url = parse_dsn(os.environ.get('DATABASE_URL'))\n        db_url['sslmode'] = 'require'\n        logger.info('HOST: {0}'.format(db_url.get('host')))\n        self.db = load_db(db_url)\n        self.cursor = self.db.cursor()\n\n        init_currency_history_table(self.cursor)\n        self.db.commit()\n\n    def add_event(self, *, event):\n        insert_event_table = \"\"\"\n            INSERT INTO history (event) VALUES (%s);\n        \"\"\"\n        self.cursor.execute(insert_event_table, (event))\n\n    def events(self):\n        select_event_table = \"\"\"SELECT * FROM event;\"\"\"\n        self.cursor.execute(select_event_table)\n        return self.cursor.fetchall()\n\n    def close(self):\n        self.cursor.close()\n        self.db.close()\n\n`\n```\nI use DB for Falcon API.\n```\n`from fastapi import Depends, FastAPI, HTTPException, status\nfrom fastapi.security import HTTPBasic, HTTPBasicCredentials\nfrom decimal import Decimal, getcontext\n\nfrom db import PostgresqlApi\n\napp = FastAPI()\nsecurity = HTTPBasic()\ndatabase = None\n\ndef db_connection():\n    global database\n    if not database:\n        database = PostgresqlApi()\n    return database\n\ndef check_basic_auth_creds(credentials: HTTPBasicCredentials = Depends(security)):\n    correct_username = secrets.compare_digest(credentials.username, os.environ.get('APP_USERNAME'))\n    correct_password = secrets.compare_digest(credentials.password, os.environ.get('APP_PASSWORD'))\n    if not (correct_username and correct_password):\n        raise HTTPException(\n            status_code=status.HTTP_401_UNAUTHORIZED,\n            detail=\"Incorrect username and password\",\n            headers={'WWW-Authenticate': 'Basic'}\n        )\n    return credentials\n\n@app.get(\"/currencies\")\ndef read_currencies(credentials: HTTPBasicCredentials = Depends(check_basic_auth_creds)):\n    db = db_connection()\n    return {'get events': 'ok'}\n`\n```\nI have tried different methods and plugins. Among others are`pytest-pgsql`, `pytest-postgresql`.",
      "solution": "The solution I landed at is below.\n\nCreated fake class that has exactly structure of `PostgresqlApi`. (see implementation below)\nCreated fixture for `db_connection` method. (see implementation below)\n\nFake class implementation\n```\n`class FakePostgresqlApi(PostgresqlApi):\n\n    event_list = []\n\n    def __init__(self):\n        pass\n\n    def add_event(self, *, event):\n        self.event_list.append([1, 'magic trick', 1653630607])\n\n    def events(self):\n        return self.event_list\n\n    def close(self):\n        self.event_list.clear()\n`\n```\nFixture\n```\n`from unittest.mock import MagicMock\n\n@pytest.fixture\ndef mock_db_connection(mocker):\n    mocker.patch('src.main.db_connection', MagicMock(return_value=FakePostgresqlApi()))\n`\n```\nThe test itself was:\n```\n`def test_read_events(mock_db_connection):\n   # Do whatever I need here, in my case call Falcon API test client\n`\n```",
      "question_score": 6,
      "answer_score": 3,
      "created_at": "2022-06-10T15:04:46",
      "url": "https://stackoverflow.com/questions/72574761/what-is-the-correct-way-to-mock-psycopg2-with-pytest"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 73803605,
      "title": "psycopg2.ProgrammingError: the connection cannot be re-entered recursively",
      "problem": "Am calling an endpoint from flask using fetch api from react. I keep getting psycopg2.ProgrammingError: the connection cannot be re-entered recursively\nI the endpoint call is inside a loop.\n```\n`@app.get(\"/api/plot-project//\")\ndef check_and_deactivate(plot_id, project_id):\n    with connection:\n        with connection.cursor() as cursor:\n            cursor.execute(PLOT_PROJECT_CHECK, (plot_id, project_id))\n            data = cursor.fetchall()\n            if len(data) == 0:\n                return \"No data found\", 404\n            removed = data.pop(0)\n            if  len(data) > 1:\n                for row in data:\n                    print(row[0])\n                    cursor.execute(PLOT_PROJECT_DEACTIVATE, ('deleted', row[0], plot_id, project_id))\n            return { \"Remain\": removed }, 200   \n`\n```\nThe react fuctions\n```\n`  const handleGet = () => {\n    data.forEach (async (item) => {\n      \n      await getData(item.plotID, item.projectID);\n    })\n  }\n`\n```\nthe fetch handle\n```\n`  const getData = async (plotID, projectID) => { \n    fetch(`http://127.0.0.1:5000/api/plot-project/${plotID}/${projectID}`, {  method : 'GET',  mode: 'no-cors', headers : {    'Content-Type': 'application/json',    'Authorization': `Bearer ${token}`  }})\n      .then(data => data.json())\n      .then((response) => {\n        console.log('mapping', plotID, \"to\", projectID)\n        console.log('request succeeded with JSON response', response)\n      }).catch(function (error) {\n        console.log('mapping', plotID, \"to\", projectID)\n        console.log('no mapping yet')\n      });\n  }\n`\n```",
      "solution": "This error happens when you are trying to call the context manager of the same connection, which is already called in the context manager.\nTo fix that issue you have few options:\n\nTo use the same connection without context manager and to commit or rollback changes. The code will look like:\n\n```\n`\n    try:\n        result = None\n        with connection.cursor() as cursor:\n            cursor.execute(PLOT_PROJECT_CHECK, (plot_id, project_id))\n            data = cursor.fetchall()\n            if len(data) == 0:\n                result = \"No data found\", 404\n            removed = data.pop(0)\n            if  len(data) > 1:\n                for row in data:\n                    print(row[0])\n                    cursor.execute(PLOT_PROJECT_DEACTIVATE, ('deleted', row[0], plot_id, project_id))\n            result = { \"Remain\": removed }, 200   \n            conn.commit()\n        return result\n        except Exception as e:\n            logging.error(f\"An error occurred: {str(e)}\")\n            conn.rollback()\n        return \"Database error\", 500\n\n`\n```\n\nTo use the same connection without context manager and to use autocommit. The code will look like:\n\n```\n`\n    connection.autocommit = True (you can place that code where you are making the connection)\n    with connection.cursor() as cursor:\n        cursor.execute(PLOT_PROJECT_CHECK, (plot_id, project_id))\n        data = cursor.fetchall()\n        if len(data) == 0:\n            return \"No data found\", 404\n        removed = data.pop(0)\n        if len(data) > 1:\n            for row in data:\n            print(row[0])\n            cursor.execute(PLOT_PROJECT_DEACTIVATE, ('deleted', row[0], plot_id, project_id))\n            return { \"Remain\": removed }, 200   \n\n`\n```\n\nTo use connection pool and to enter the context manager of separate connection for every request.",
      "question_score": 6,
      "answer_score": 2,
      "created_at": "2022-09-21T17:43:35",
      "url": "https://stackoverflow.com/questions/73803605/psycopg2-programmingerror-the-connection-cannot-be-re-entered-recursively"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 73772930,
      "title": "pip failing to install psycopg2-binary on ubuntu 22.04 with python 3.10",
      "problem": "seeing this on Ubuntu 22.04 which has python 3.10\n```\n`pip3 install psycopg2-binary==2.8.5                                                                                                                                                            \nDefaulting to user installation because normal site-packages is not writeable                                                                                                                                          \nCollecting psycopg2-binary==2.8.5                                                                                                                                                                                      \n  Using cached psycopg2-binary-2.8.5.tar.gz (381 kB)                                                                                                                                                                   \n  Preparing metadata (setup.py) ... error                                                                                                                                                                              \n  error: subprocess-exited-with-error                                                                                                                                                                                  \n                                                                                                                                                                                                                       \n  \u00d7 python setup.py egg_info did not run successfully.                                                                                                                                                                 \n  \u2502 exit code: 1                                                                                                                                                                                                       \n  \u2570\u2500> [23 lines of output]                                                                                                                                                                                             \n      running egg_info                                                                                                                                                                                                 \n      creating /tmp/pip-pip-egg-info-qc0tg_5p/psycopg2_binary.egg-info                                                                                                                                                 \n      writing /tmp/pip-pip-egg-info-qc0tg_5p/psycopg2_binary.egg-info/PKG-INFO                                                                                                                                         \n      writing dependency_links to /tmp/pip-pip-egg-info-qc0tg_5p/psycopg2_binary.egg-info/dependency_links.txt                                                                                                         \n      writing top-level names to /tmp/pip-pip-egg-info-qc0tg_5p/psycopg2_binary.egg-info/top_level.txt                                                                                                                 \n      writing manifest file '/tmp/pip-pip-egg-info-qc0tg_5p/psycopg2_binary.egg-info/SOURCES.txt'                                                                                                                      \n                                                                                                                                                                                                                       \n      Error: pg_config executable not found.                                                                                                                                                                           \n                                                                                                                                                                                                                       \n      pg_config is required to build psycopg2 from source.  Please add the directory                                                                                                                                   \n      containing pg_config to the $PATH or specify the full executable path with the                                                                                                                                   \n      option:                                                                                                                                                                                                          \n                                                                                                                                                                                                                       \n          python setup.py build_ext --pg-config /path/to/pg_config build ...                                                                                                                                           \n                                                                                                                                                                                                                       \n      or with the pg_config option in 'setup.cfg'.\n      \n      If you prefer to avoid building psycopg2 from source, please install the PyPI\n      'psycopg2-binary' package instead.\n      \n      For further information please check the 'doc/src/install.rst' file (also at\n      ).\n      \n      [end of output]\n   \n  note: This error originates from a subprocess, and is likely not a problem with pip.\nerror: metadata-generation-failed\n\n\u00d7 Encountered error while generating package metadata.\n\u2570\u2500> See above for output.\n\nnote: This is an issue with the package mentioned above, not pip.\nhint: See above for details.\n`\n```",
      "solution": "This is a known issue. Use the binary version instead:\n`pip install psycopg2-binary`\nEDIT:\nYou dont have 'pg_config' which is part of `libpq-dev` on Ubuntu. Install it with `sudo apt-get install libpq-dev` and try abgain.",
      "question_score": 5,
      "answer_score": 15,
      "created_at": "2022-09-19T13:49:27",
      "url": "https://stackoverflow.com/questions/73772930/pip-failing-to-install-psycopg2-binary-on-ubuntu-22-04-with-python-3-10"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 74748522,
      "title": "pyscopg2 WITHOUT transaction",
      "problem": "Sometimes I have a need to execute a query from psycopg2 that is not in a transaction block.\nFor example:\n```\n`cursor.execute('create index concurrently on my_table (some_column)')\n`\n```\nDoesn't work:\n```\n`InternalError: CREATE INDEX CONCURRENTLY cannot run inside a transaction block\n`\n```\nI don't see any easy way to do this with psycopg2.  What am I missing?\nI can probably call `os.system('psql -c \"create index concurrently\"')` or something similar to get it to run from my python code, however it would be much nicer to be able to do it inside python and not rely on psql to actually be in the container.\nYes, I have to use the `concurrently` option for this particular use case.\n\nAnother time I've explored this and not found an obvious answer is when I have a set of sql commands that I'd like to call with a single `execute()`, where the first one briefly locks a resource.  When I do this, that resource will remain locked for the entire duration of the `execute()` rather than for just when the first statement in the sql string was running because they all run together in one big happy transaction.\nIn that case I could break the query up into a series of execute() statements - each became its own transaction, which was ok.\nIt seems like there should be a way, but I seem to be missing it.  Hopefully this is an easy answer for someone.\n\nEDIT:  Add code sample:\n```\n`#!/usr/bin/env python3.10\n\nimport psycopg2 as pg2\n\n# -- set the standard psql environment variables to specify which database this should connect to.\n\n# We have to set these to 'None' explicitly to get psycopg2 to use the env variables\nconnDetails = {'database': None, 'host': None, 'port': None, 'user': None, 'password': None}\n\nwith (pg2.connect(**connDetails) as conn, conn.cursor() as curs):\n\n    conn.set_session(autocommit=True)\n\n    curs.execute(\"\"\"\ncreate index concurrently if not exists my_new_index on my_table (my_column);\n\"\"\")\n`\n```\nThrows:\n```\n`psycopg2.errors.ActiveSqlTransaction: CREATE INDEX CONCURRENTLY cannot run inside a transaction block\n`\n```",
      "solution": "To summarize and clarify, the working answer is:\n```\n`#!/usr/bin/env python3.10\n\nimport psycopg2 as pg2\n\n# -- set the standard psql environment variables to specify \n# -- which database this should connect to.\n\n# We have to set these to 'None' explicitly to get psycopg2 to use the env variables\nconn_args = {'database': None, 'host': None, 'port': None, 'user': None, 'password': None}\n\nconn = pg2.connect(**conn_args)\nconn.set_session(autocommit=True)\nwith conn.cursor() as curs:\n    curs.execute(\"\"\"\n       create index concurrently if not exists \n       my_new_index on my_table(my_column);\n    \"\"\")\nconn.close()\n`\n```",
      "question_score": 5,
      "answer_score": 1,
      "created_at": "2022-12-09T22:36:44",
      "url": "https://stackoverflow.com/questions/74748522/pyscopg2-without-transaction"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 77716028,
      "title": "Inserting many rows in psycopg3",
      "problem": "I used to use `execute_values` in psycopg2 but it's gone in psycopg3. I tried following the advice in this answer or this github post, but it just doesn't seem to work for my use case. I'm trying to insert multiple values, my SQL is like so:\n```\n`sql = INSERT INTO activities (type_, key_, a, b, c, d, e)\n        VALUES %s\n        ON CONFLICT (key_) DO UPDATE\n        SET\n            a = EXCLUDED.a,\n            b = EXCLUDED.b,\n            c = EXCLUDED.c,\n            d = EXCLUDED.d,\n            e = EXCLUDED.e\nvalues = [['type', 'key', None, None, None, None, None]]\n`\n```\nBut doing `cursor.executemany(sql, values)` results in `{ProgrammingError}the query has 1 placeholder but 7 parameters were passed`. I tried many variations with extra parentheses etc. but always it results in some error. For example doing `self.cursor.executemany(sql, [values])` results in `syntax error near or at \"$1\": Line 3: VALUES $1`.",
      "solution": "The values clause should be consist of one `%s` placeholder for each column being inserted, separated by commas and all within parentheses, like this:\n`INSERT INTO t (a, b, c) VALUES (%s, %s, %s)\n`\nWe can produce the desired string with string manipulation:\n`# Create one placeholder per column inserted.\nplaceholders = ', '.join(['%s'] * len(values[0]))\n# Wrap in parentheses.\nvalues_clause =  f\"\"\"({placeholders})\"\"\"\n# Inject into the query string.\nisql = isql % values_clause\n\nwith psycopg.connect(dbname='test') as conn, conn.cursor() as cur:\n    cur.executemany(isql, values)\n    conn.commit()\n`\nHowever psycopg provides tools for composing SQL statements, and it may be safer to use these tools rather than relying on string manipulation if your query building is very dynamic. Using these tools, you would have this (I've added the parentheses into the main query string this time, as there is no benefit in not doing so):\n`placeholders = sql.SQL(', ').join(sql.Placeholder() * len(values[0]))\nisql = sql.SQL(\"\"\"INSERT INTO t77716028 (type_, key_, a, b, c, d, e)\n        VALUES ({placeholders})\n        ON CONFLICT (key_) DO UPDATE\n        SET\n            a = EXCLUDED.a,\n            b = EXCLUDED.b,\n            c = EXCLUDED.c,\n            d = EXCLUDED.d,\n            e = EXCLUDED.e\"\"\")\nisql = isql.format(placeholders=placeholders)\n\nwith psycopg.connect(dbname='test') as conn, conn.cursor() as cur:\n    print(f'{isql.as_string(conn)=}')\n    cur.executemany(isql, values)\n    conn.commit()\n`",
      "question_score": 5,
      "answer_score": 5,
      "created_at": "2023-12-26T07:57:06",
      "url": "https://stackoverflow.com/questions/77716028/inserting-many-rows-in-psycopg3"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 68070075,
      "title": "Connect to AWS RDS database via psycopg2",
      "problem": "I am trying to connect to my RDS database from my computer with a python script using psycopg2.\npython code:\n```\n`import psycopg2\nfrom db_credentials import *\nimport logging\n\ndef get_psql_conn():\n    conn = psycopg2.connect(dbname=DB_NAME, user=DB_USER, password=DB_PASS, host=DB_HOST)\n    logging.info(\"connected to DB!\")\n    return conn\n`\n```\nI get the following error:\n```\n`psycopg2.OperationalError: could not connect to server: Operation timed out\n        Is the server running on host ********* and accepting\n        TCP/IP connections on port 5432?\n`\n```\nMy security groups assigned to the RDS database:\n SG 1:\n\n SG 2:\n\nNow i tried to make a security group which allows my computer IP to access the DB.\nSG 3:\n\nI can connect to the DB from my ec2 instances, running the same python script as above. This seemingly has to do with the 2nd security group, as when i remove it, i can no longer connect from my ec2 instances either. It then throws the same error i get when trying to connect from my computer.\nI have little understanding of RDS or security groups, i just followed internet tutorials, but seemingly couldnt make much sense out of it.\n\nAny help is greatly appreciated! Thanks",
      "solution": "When accessing an Amazon RDS database from the Internet, the database needs to be configured for `Publicly Accessible = Yes`.\nThis will assign a Public IP address to the database instance. The DNS Name of the instance will also resolve to the public IP address.\nFor good security on publicly-accessible databases, ensure that the Security Group only permits access from your personal IP address.",
      "question_score": 5,
      "answer_score": 4,
      "created_at": "2021-06-21T16:49:55",
      "url": "https://stackoverflow.com/questions/68070075/connect-to-aws-rds-database-via-psycopg2"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 74416482,
      "title": "(psycopg2.ProgrammingError) can&#39;t adapt type &#39;Point&#39;",
      "problem": "I'm trying to add data to postgresql data base in flask framework using flask sqlalchemy, geoalchemy2, but when trying to add point data I'm getting this error,\n```\n`(psycopg2.ProgrammingError) can't adapt type 'Point'\n`\n```\nAnyone with way through it or resource that can help me get the solution for this",
      "solution": "I ran into the same issue but with with the multipolygon type.\n```\n`sqlalchemy.exc.programmingerror: (psycopg2.programmingerror) can't adapt type 'multipolygon'\n`\n```\nThe issue was that the Shapely object needed to be converted to a WKBElement before being added to the model field.\nThe solution was to convert the Shapely object using the `from_shape` function (see: https://geoalchemy-2.readthedocs.io/en/0.2.6/shape.html)\nHere's an example assuming you've already defined your orm models:\n`\nfrom geoalchemy2.shape import from_shape\nfrom shapely.geometry import Point\n\n# import local orm model that has a geoalchemy2 Geometry column\nfrom orm import PointOfInterest\n\n# convert point using from_shape\nmy_point = from_shape(Point(16.62, 24.68), srid=4326)\n\nwith Session() as session:\n    session.add(\n        PointOfInterest(\n            name='example',\n            geometry=my_point,\n        )\n    )\n    session.commit()\n`",
      "question_score": 5,
      "answer_score": 3,
      "created_at": "2022-11-12T21:49:43",
      "url": "https://stackoverflow.com/questions/74416482/psycopg2-programmingerror-cant-adapt-type-point"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 75534231,
      "title": "How can I connect to remote database using psycopg3",
      "problem": "I'm using Psycopg3 (not 2!) and I can't figure out how can I connect to a remote Postgres server\n`psycopg.connect(connection_string)\n`\nhttps://www.psycopg.org/psycopg3/docs/\nThanks!",
      "solution": "Psycopg3 uses the postgresql connection string which can either be a string of keyword=value elements (separated by spaces)\n```\n`with psycopg.connect(\"host=your_server_hostname port=5432 dbname=your_db_name\") as conn:\n`\n```\nor a URI\n```\n`with psycopg.connect(\"postgresql://user:user_password@db_server_hostname:5432\") as conn:\n`\n```\nSo, put all together with their example (mixed with one example from above connection strings) from their docs:\n```\n`# Note: the module name is psycopg, not psycopg3\nimport psycopg\n\n# Connect to an existing database\nwith psycopg.connect(\"postgresql://user:user_password@db_server_hostname:5432\") as conn:\n\n    # Open a cursor to perform database operations\n    with conn.cursor() as cur:\n\n        # Execute a command: this creates a new table\n        cur.execute(\"\"\"\n            CREATE TABLE test (\n                id serial PRIMARY KEY,\n                num integer,\n                data text)\n            \"\"\")\n\n        # Pass data to fill a query placeholders and let Psycopg perform\n        # the correct conversion (no SQL injections!)\n        cur.execute(\n            \"INSERT INTO test (num, data) VALUES (%s, %s)\",\n            (100, \"abc'def\"))\n\n        # Query the database and obtain data as Python objects.\n        cur.execute(\"SELECT * FROM test\")\n        cur.fetchone()\n        # will return (1, 100, \"abc'def\")\n\n        # You can use `cur.fetchmany()`, `cur.fetchall()` to return a list\n        # of several records, or even iterate on the cursor\n        for record in cur:\n            print(record)\n\n        # Make the changes to the database persistent\n        conn.commit()\n`\n```",
      "question_score": 4,
      "answer_score": 10,
      "created_at": "2023-02-22T15:36:16",
      "url": "https://stackoverflow.com/questions/75534231/how-can-i-connect-to-remote-database-using-psycopg3"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 74899785,
      "title": "psycopg2.errors.ActiveSqlTransaction: CREATE DATABASE cannot run inside a transaction block",
      "problem": "I am trying to create a Django app that creates a new database for every user when he/she signs up. I am going with this approach due to some reason. I have tried many ways using management commands and even Celery. But I am still getting the same error.\n```\n`2022-12-23 07:16:07.410 UTC [49] STATEMENT:  CREATE DATABASE tenant_asdadsad\n[2022-12-23 07:16:07,415: ERROR/ForkPoolWorker-4] Task user.utils.create_database[089b0bc0-0b5f-4199-8cf3-bc336acc7624] raised unexpected: ActiveSqlTransaction('CREATE DATABASE cannot run inside a transaction block\\n')\nTraceback (most recent call last):\n File \"/usr/local/lib/python3.9/site-packages/celery/app/trace.py\", line 451, in trace_task\nR = retval = fun(*args, **kwargs)\nFile \"/usr/local/lib/python3.9/site-packages/celery/app/trace.py\", line 734, in __protected_call__\nreturn self.run(*args, **kwargs)\nFile \"/app/user/utils.py\", line 45, in create_database\ncursor.execute(f'CREATE DATABASE tenant_{tenant_id}')\npsycopg2.errors.ActiveSqlTransaction: CREATE DATABASE cannot run inside a transaction block\n`\n```\nThis is my task\n```\n`@shared_task\ndef create_database(tenant_id):\n  conn = psycopg2.connect(database=\"mydb\", user=\"dbuser\", password=\"mypass\", host=\"db\")\n  cursor = conn.cursor()\n  transaction.set_autocommit(True)\n  cursor.execute(f'CREATE DATABASE tenant_{tenant_id}')\n  cursor.execute(f'GRANT ALL PRIVILEGES ON DATABASE tenant_{tenant_id} TO dbuser')\n  cursor.close()\n  conn.close()\n`\n```\nI have tried several ways but I always get the same error\nThis is my API call\n```\n`def create(self, request, *args, **kwargs):\n    serializer_class = mySerializer(data=request.data)\n    if serializer_class.is_valid():\n        validated_data = serializer_class.validated_data\n        or = validated_data[\"org\"]\n        or = Org.objects.create(**org)\n        create_database.delay(str(or.id))\n        return Response(create_user(validated_data))\n`\n```",
      "solution": "Use the `autocommit` property of the connection:\n`from psycopg2 import sql\n\ndef create_database(tenant_id):\n  conn = psycopg2.connect(database=\"mydb\", user=\"dbuser\", password=\"mypass\", host=\"db\")\n  cursor = conn.cursor()\n  conn.autocommit = True #!\n#  transaction.set_autocommit(True) #?\n  dbname = sql.Identifier(f'tenant_{tenant_id}')\n  create_cmd = sql.SQL('CREATE DATABASE {}').format(dbname)\n  grant_cmd = sql.SQL('GRANT ALL PRIVILEGES ON DATABASE {} TO dbuser').format(dbname)\n  cursor.execute(create_cmd)\n  cursor.execute(grant_cmd)\n  cursor.close()\n  conn.close()\n`\nRead in the docs about connection.autocommit.\nNote also the use of the SQL string composition to avoid SQL injection.",
      "question_score": 4,
      "answer_score": 10,
      "created_at": "2022-12-23T13:44:47",
      "url": "https://stackoverflow.com/questions/74899785/psycopg2-errors-activesqltransaction-create-database-cannot-run-inside-a-transa"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 74827982,
      "title": "Using a buffer to write a psycopg3 copy result through pandas",
      "problem": "Using `psycopg2`, I could write large results as CSV using `copy_expert` and a `BytesIO` buffer like this with `pandas`:\n`copy_sql = \"COPY (SELECT * FROM big_table) TO STDOUT CSV\"\n\nbuffer = BytesIO()\ncursor.copy_expert(copy_sql, buffer, size=8192)\nbuffer.seek(0)\npd.read_csv(buffer, engine=\"c\").to_excel(self.output_file)\n`\nHowever, I can't figure out how to replace the `buffer` in `copy_expert` with `psycopg3`'s new copy command. Has anyone figured out a way to do this?",
      "solution": "The key to writing a large query to a file through `psycopg3` in this fashion is to use a `SpooledTemporaryFile`, which will limit the amount of memory usage in Python (see `max_size`). Then after the CSV is written to disk, convert with `pandas`.\n`from tempfile import SpooledTemporaryFile\nfrom pandas import read_csv\nfrom psycopg import connect\n\ncursor = connect([connection]).cursor()\ncopy_sql = \"COPY (SELECT * FROM stocks WHERE price > %s) TO STDOUT\"\nprice = 100\n\nwith SpooledTemporaryFile(\n    mode=\"wb\",\n    max_size=65546,\n    buffering=8192,\n) as tmpfile:\n    with cursor.copy(copy_sql, (price,)) as copy:\n        for data in copy:\n            tmpfile.write(data)\n    tmpfile.seek(0)\n\n    read_csv(tmpfile, engine=\"c\").to_excel(\"my_spreadsheet.xlsx\")\n`",
      "question_score": 4,
      "answer_score": 4,
      "created_at": "2022-12-16T18:39:55",
      "url": "https://stackoverflow.com/questions/74827982/using-a-buffer-to-write-a-psycopg3-copy-result-through-pandas"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 74809457,
      "title": "Catch any of the errors in psycopg2 without listing them explicitly",
      "problem": "I have a `try` and `except` block where I would like to catch only the errors in the `psycopg2.errors` and not any other error.\nThe explicit way would be:\n```\n`try:\n    # execute a query\n    cur = connection.cursor()\n    cur.execute(sql_query)\nexcept psycopg2.errors.SyntaxError, psycopg2.errors.GroupingError as err:\n    # handle in case of error\n`\n```\nThe query will always be some SELECT statement. If the execution fails it should be handled. Any other exception not belonging to `psycopg`, e.g. like `ZeroDivisionError`, should not be caught from the `except` clause. However, I would like to avoid to list all errors after the `except` clause. In fact, if you list the psycopg errors, you get a quite extensive list:\n```\n`from psycopg2 import errors\ndir(errors)\n`\n```\nI have searched quite extensively and am not sure if this question has been asked already.",
      "solution": "You can you use the base class `psycopg2.Error` it catch all psycopg2 related errors\n```\n`import psycopg2\n\ntry:\n   cur = connection.cursor()\n   cur.execute(sql_query)\nexcept psycopg2.Error as err:\n   # handle in case of error\n   \n`\n```\nsee official documentation",
      "question_score": 4,
      "answer_score": 5,
      "created_at": "2022-12-15T10:34:47",
      "url": "https://stackoverflow.com/questions/74809457/catch-any-of-the-errors-in-psycopg2-without-listing-them-explicitly"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 74518627,
      "title": "How to insert ObjectId to PostgreSQL",
      "problem": "I am writing a script in which I fetch data from MongoDB and dump it into PostgreSQL. Everything works fine but when I try to insert the MongoDBs ObjectId `_id`, it gives the following error.\n\n(psycopg2.errors.SyntaxError) trailing junk after numeric literal at\nor near \"63711d\"\n\nI want to insert the `_id` in the PostgreSQL db as primary key, so that duplicate rows do not exist.\nThe query generated by psycopg2 is as follows\n```\n`[SQL: INSERT INTO employees (_id, candidate_last_name, candidate_first_name, candidate_state, candidate_experience, candidate_relocation, candidate_skills, candidate_specialty)VALUES (6375364d1ad809ab6108a544, NULL, NULL, NULL, NULL, NULL, NULL, NULL), (6375364d1ad809ab6108a545, NULL, NULL, NULL, NULL, NULL, NULL, NULL)]\n`\n```\nThe `_id` field in PostgreSQL is VARCHAR.\nThe code that I am using is as follows:\n```\n`def insert_psql(db, table_name: str, fields: dict, data: list):\n\n    new_fields = {}\n    for field in fields:\n        new_fields[field.replace('.', '_')] = fields[field]\n    insert_query = f'INSERT INTO {table_name} ('\n    insert_query += ', '.join(new_fields.keys()) + ')'\n    insert_query += 'VALUES '\n    for i, row in enumerate(data):\n        insert_query += '('\n        for j, field in enumerate(fields):\n            if not row.get(field):\n                insert_query += 'NULL'\n            else:\n                insert_query += f'{str(row.get(field))}'\n            if not j == len(fields) - 1:\n                insert_query += ', '\n        insert_query += ')'\n        if not i == len(data) - 1:\n            insert_query += ', '\n    # print(insert_query)\n    try:\n        db.execute(insert_query)\n        db.commit()\n    except Exception as e:\n        print(e)\n`\n```\nThe fields dict is a dictionary containing column names and their data_types as value. The data list a list of records to insert",
      "solution": "Your error code is: `trailing junk after numeric literal at or near \"63711d\"`\nSpecifically, it is warning about unexpected characters after numeric literals. In the text that is printed we can see five digits (`63711`) followed by the character `a`. It seems that the code is attempting to parse this set of characters as a number and failing to do so once it finds the first alpha character.\nIndeed when we look at the SQL statement that is generated we can see this:\n```\n`VALUES (6375364d1ad809ab6108a544,\n`\n```\nIf you are attempting to insert a string (`VARCHAR`), then @Mark Rotteveel said what you need to do in the very first comment on this question:\n\nIf this is a VARCHAR in PostgreSQL, then the value should be enclosed in quotes in the generated statement.\n\nYour `INSERT` statement should presumably have something like this instead:\n```\n`VALUES ('6375364d1ad809ab6108a544',\n`\n```\nEdited to use single quotes instead of double. Also, Mark raises another important warning about this approach overall here.",
      "question_score": 4,
      "answer_score": 3,
      "created_at": "2022-11-21T13:05:00",
      "url": "https://stackoverflow.com/questions/74518627/how-to-insert-objectid-to-postgresql"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 71304312,
      "title": "pytest_postgresql example raises import error psycopg",
      "problem": "I am currently trying to get pytest_postgresql running in my enviroment.\nbefor I start: I can test the sql code it self with this right? not just things like your database connection would work like this?\nso I copyed the Sample test out of github readme:\n```\n`def test_example_postgres(postgresql):\n    \"\"\"Check main postgresql fixture.\"\"\"\n    cur = postgresql.cursor()\n    cur.execute(\"CREATE TABLE test (id serial PRIMARY KEY, num integer, data varchar);\")\n    postgresql.commit()\n    cur.close()\n`\n```\nthan I tryed: `pytest`\nI got:\n```\n`\n================================================================================================================= ERRORS =================================================================================================================\n________________________________________________________________________________________________ ERROR at setup of test_example_postgres _________________________________________________________________________________________________\n\nrequest = >\n\n    @pytest.fixture\n    def postgresql_factory(request: FixtureRequest) -> Iterator[connection]:\n        \"\"\"\n        Fixture factory for PostgreSQL.\n\n        :param request: fixture request object\n        :returns: postgresql client\n        \"\"\"\n>       check_for_psycopg()\n\nvenv\\lib\\site-packages\\pytest_postgresql\\factories\\client.py:56:\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\n    def check_for_psycopg() -> None:\n        \"\"\"\n        Function checks whether psycopg was imported.\n\n        Raises ImportError if not.\n        \"\"\"\n        if not psycopg:\n>           raise ImportError(\"No module named psycopg. Please install psycopg.\")\nE           ImportError: No module named psycopg. Please install psycopg.\n\nvenv\\lib\\site-packages\\pytest_postgresql\\compat.py:35: ImportError\n======================================================================================================== short test summary info =========================================================================================================\nERROR test_postgresql.py::test_example_postgres - ImportError: No module named psycopg. Please install psycopg.\n============================================================================================================ 1 error in 0.12s ============================================================================================================\n`\n```\nSo I tryed: `pip install psycopg`\nI got:\n```\n`\nRequirement already satisfied: psycopg in c:\\users\\a05922\\pycharmprojects\\try_out_pytest_postgresql\\venv\\lib\\site-packages (3.0.9)\nRequirement already satisfied: tzdata in c:\\users\\a05922\\pycharmprojects\\try_out_pytest_postgresql\\venv\\lib\\site-packages (from psycopg) (2021.5)\n\n`\n```\nSo I thought if I import it python may realise that the package is already installed:\n```\n`import psycopg\n\ndef test_example_postgres(postgresql):\n    \"\"\"Check main postgresql fixture.\"\"\"\n    cur = postgresql.cursor()\n    cur.execute(\"CREATE TABLE test (id serial PRIMARY KEY, num integer, data varchar);\")\n    postgresql.commit()\n    cur.close()\n`\n```\nI tried: `pytest`\nI got:\n```\n`\n========================================================================================================== test session starts ===========================================================================================================\nplatform win32 -- Python 3.9.4, pytest-7.0.1, pluggy-1.0.0\nrootdir: C:\\Users\\a05922\\PycharmProjects\\try_out_pytest_postgresql\nplugins: postgresql-4.1.0\ncollected 0 items / 1 error                                                                                                                                                                                                               \n\n================================================================================================================= ERRORS =================================================================================================================\n__________________________________________________________________________________________________ ERROR collecting test_postgresql.py ___________________________________________________________________________________________________\nImportError while importing test module 'C:\\Users\\a05922\\PycharmProjects\\try_out_pytest_postgresql\\test_postgresql.py'.\nHint: make sure your test modules/packages have valid Python names.\nTraceback:\nC:\\Programme\\Python\\Python39\\lib\\importlib\\__init__.py:127: in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\ntest_postgresql.py:1: in \n    import psycopg\nvenv\\lib\\site-packages\\psycopg\\__init__.py:9: in \n    from . import pq  # noqa: F401 import early to stabilize side effects\nvenv\\lib\\site-packages\\psycopg\\pq\\__init__.py:114: in \n    import_from_libpq()\nvenv\\lib\\site-packages\\psycopg\\pq\\__init__.py:106: in import_from_libpq\n    raise ImportError(\nE   ImportError: no pq wrapper available.\nE   Attempts made:\nE   - couldn't import psycopg 'c' implementation: No module named 'psycopg_c'\nE   - couldn't import psycopg 'binary' implementation: No module named 'psycopg_binary'\nE   - couldn't import psycopg 'python' implementation: libpq library not found\n======================================================================================================== short test summary info =========================================================================================================\nERROR test_postgresql.py\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n============================================================================================================ 1 error in 0.15s ============================================================================================================\n`\n```\nthis is where i ran out of ideas what to try next. I tried googleing the problem but i cam up empty handed. Has anyone an idea how I can fix this?\nedit:\natfter suggestion of `pip uninstall psycopg` then `pip install psycopg[binary]`\nI got:\n```\n`Microsoft Windows [Version 10.0.18363.2094]\n(c) 2019 Microsoft Corporation. Alle Rechte vorbehalten.\n\n(venv) C:\\Users\\a05922\\PycharmProjects\\try_out_pytest_postgresql>pip install psycopg\nRequirement already satisfied: psycopg in c:\\users\\a05922\\pycharmprojects\\try_out_pytest_postgresql\\venv\\lib\\site-packages (3.0.9)\nRequirement already satisfied: tzdata in c:\\users\\a05922\\pycharmprojects\\try_out_pytest_postgresql\\venv\\lib\\site-packages (from psycopg) (2021.5)\n\n(venv) C:\\Users\\a05922\\PycharmProjects\\try_out_pytest_postgresql>pip install psycopg[binary]\nRequirement already satisfied: psycopg[binary] in c:\\users\\a05922\\pycharmprojects\\try_out_pytest_postgresql\\venv\\lib\\site-packages (3.0.9)\nRequirement already satisfied: tzdata in c:\\users\\a05922\\pycharmprojects\\try_out_pytest_postgresql\\venv\\lib\\site-packages (from psycopg[binary]) (2021.5)\nCollecting psycopg-binary==3.0.9\n  Downloading psycopg_binary-3.0.9-cp39-cp39-win_amd64.whl (2.9 MB)\n     ---------------------------------------- 2.9/2.9 MB 10.3 MB/s eta 0:00:00\nInstalling collected packages: psycopg-binary\nSuccessfully installed psycopg-binary-3.0.9\n`\n```\n`pytest` got me:\n```\n`\n========================================================================================================== test session starts ===========================================================================================================\nplatform win32 -- Python 3.9.4, pytest-7.0.1, pluggy-1.0.0\nrootdir: C:\\Users\\a05922\\PycharmProjects\\try_out_pytest_postgresql\nplugins: postgresql-4.1.0\ncollected 1 item                                                                                                                                                                                                                          \n\ntest_postgresql.py E                                                                                                                                                                                                                [100%]\n\n================================================================================================================= ERRORS =================================================================================================================\n________________________________________________________________________________________________ ERROR at setup of test_example_postgres _________________________________________________________________________________________________\n\nrequest = >\n\n    @pytest.fixture\n    def postgresql_factory(request: FixtureRequest) -> Iterator[connection]:\n        \"\"\"\n        Fixture factory for PostgreSQL.\n\n        :param request: fixture request object\n        :returns: postgresql client\n        \"\"\"\n        check_for_psycopg()\n>       proc_fixture: Union[PostgreSQLExecutor, NoopExecutor] = request.getfixturevalue(\n            process_fixture_name\n        )\n\nvenv\\lib\\site-packages\\pytest_postgresql\\factories\\client.py:57:\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nvenv\\lib\\site-packages\\pytest_postgresql\\factories\\process.py:104: in postgresql_proc_fixture\n    pg_bindir = subprocess.check_output(\nC:\\Programme\\Python\\Python39\\lib\\subprocess.py:424: in check_output\n    return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\nC:\\Programme\\Python\\Python39\\lib\\subprocess.py:505: in run\n    with Popen(*popenargs, **kwargs) as process:\nC:\\Programme\\Python\\Python39\\lib\\subprocess.py:951: in __init__\n    self._execute_child(args, executable, preexec_fn, close_fds,\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = , args = 'pg_config --bindir', executable = None, preexec_fn = None, close_fds = False, pass_fds = (), cwd = None, env = None\nstartupinfo = , creationflags = 0, shell = False, p2cread = Handle(880), p2cwrite = -1, c2pread = 14, c2pwrite = Handle(748), errread = -1, errwrite = Handle(632)\nunused_restore_signals = True, unused_gid = None, unused_gids = None, unused_uid = None, unused_umask = -1, unused_start_new_session = False\n\n    def _execute_child(self, args, executable, preexec_fn, close_fds,\n                       pass_fds, cwd, env,\n                       startupinfo, creationflags, shell,\n                       p2cread, p2cwrite,\n                       c2pread, c2pwrite,\n                       errread, errwrite,\n                       unused_restore_signals,\n                       unused_gid, unused_gids, unused_uid,\n                       unused_umask,\n                       unused_start_new_session):\n        \"\"\"Execute program (MS Windows version)\"\"\"\n\n        assert not pass_fds, \"pass_fds not supported on Windows.\"\n\n        if isinstance(args, str):\n            pass\n        elif isinstance(args, bytes):\n            if shell:\n                raise TypeError('bytes args is not allowed on Windows')\n            args = list2cmdline([args])\n        elif isinstance(args, os.PathLike):\n            if shell:\n                raise TypeError('path-like args is not allowed when '\n                                'shell is true')\n            args = list2cmdline([args])\n        else:\n            args = list2cmdline(args)\n\n        if executable is not None:\n            executable = os.fsdecode(executable)\n\n        # Process startup details\n        if startupinfo is None:\n            startupinfo = STARTUPINFO()\n        else:\n            # bpo-34044: Copy STARTUPINFO since it is modified above,\n            # so the caller can reuse it multiple times.\n            startupinfo = startupinfo.copy()\n\n        use_std_handles = -1 not in (p2cread, c2pwrite, errwrite)\n        if use_std_handles:\n            startupinfo.dwFlags |= _winapi.STARTF_USESTDHANDLES\n            startupinfo.hStdInput = p2cread\n            startupinfo.hStdOutput = c2pwrite\n            startupinfo.hStdError = errwrite\n\n        attribute_list = startupinfo.lpAttributeList\n        have_handle_list = bool(attribute_list and\n                                \"handle_list\" in attribute_list and\n                                attribute_list[\"handle_list\"])\n\n        # If we were given an handle_list or need to create one\n        if have_handle_list or (use_std_handles and close_fds):\n            if attribute_list is None:\n                attribute_list = startupinfo.lpAttributeList = {}\n            handle_list = attribute_list[\"handle_list\"] = \\\n                list(attribute_list.get(\"handle_list\", []))\n\n            if use_std_handles:\n                handle_list += [int(p2cread), int(c2pwrite), int(errwrite)]\n\n            handle_list[:] = self._filter_handle_list(handle_list)\n\n            if handle_list:\n                if not close_fds:\n                    warnings.warn(\"startupinfo.lpAttributeList['handle_list'] \"\n                                  \"overriding close_fds\", RuntimeWarning)\n\n                # When using the handle_list we always request to inherit\n                # handles but the only handles that will be inherited are\n                # the ones in the handle_list\n                close_fds = False\n\n        if shell:\n            startupinfo.dwFlags |= _winapi.STARTF_USESHOWWINDOW\n            startupinfo.wShowWindow = _winapi.SW_HIDE\n            comspec = os.environ.get(\"COMSPEC\", \"cmd.exe\")\n            args = '{} /c \"{}\"'.format (comspec, args)\n\n        if cwd is not None:\n            cwd = os.fsdecode(cwd)\n\n        sys.audit(\"subprocess.Popen\", executable, args, cwd, env)\n\n        # Start the process\n        try:\n>           hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n                                     # no special security\n                                     None, None,\n                                     int(not close_fds),\n                                     creationflags,\n                                     env,\n                                     cwd,\n                                     startupinfo)\nE                                    FileNotFoundError: [WinError 2] Das System kann die angegebene Datei nicht finden\n\nC:\\Programme\\Python\\Python39\\lib\\subprocess.py:1420: FileNotFoundError\n======================================================================================================== short test summary info =========================================================================================================\nERROR test_postgresql.py::test_example_postgres - FileNotFoundError: [WinError 2] Das System kann die angegebene Datei nicht finden\n============================================================================================================ 1 error in 0.35s ============================================================================================================\n\n`\n```\nThe German line at the end says: \"The System can not finde the stated file\"\nI have a hard time figering this out. the argument that looks the most like a file name to me is cwd. that argument is created a few lines above from fsdecode as long as cwd already exists.\nis the line:\n```\n`self = , args = 'pg_config --bindir', executable = None, preexec_fn = None, close_fds = False, pass_fds = (), cwd = None, env = None\nstartupinfo = , creationflags = 0, shell = False, p2cread = Handle(880), p2cwrite = -1, c2pread = 14, c2pwrite = Handle(748), errread = -1, errwrite = Handle(632)\nunused_restore_signals = True, unused_gid = None, unused_gids = None, unused_uid = None, unused_umask = -1, unused_start_new_session = False\n`\n```\nthe arguments `_execute_child` is called with? if so cwd would be None and could not be the problem. Anyone any ideas?",
      "solution": "If you did the original install as `pip install psycopg` you got the pure Python version. Per Installation instructions Pure Python installation that means you need  to have the `libpq` library installed also. Not sure how you would get that on Windows. I would say the solution is to:\n`pip uninstall psycopg`\nthen\n`pip install psycopg[binary]`\nThis will get you the self-contained(all needed libraries) install.",
      "question_score": 4,
      "answer_score": 3,
      "created_at": "2022-03-01T06:59:23",
      "url": "https://stackoverflow.com/questions/71304312/pytest-postgresql-example-raises-import-error-psycopg"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 70548240,
      "title": "Error &quot;ModuleNotFoundError: No module named &#39;psycopg2&#39;&quot; when deploying Django app to Google App Engine",
      "problem": "I'm working on a Django application and using Fabric for deployment.\nWhen I deployed the apps to staging in Google App Engine using `fab testing`, I got this error:\n```\n`Updating service [staging] (this may take several minutes)...failed.                                \nERROR: (gcloud.app.deploy) Error Response: [9] An internal error occurred while processing task /app-engine-flex/flex_await_healthy/flex_await_healthy>2022-01-01T09:48:30.226Z15496.fj.0: Traceback (most recent call last):\n  File \"manage.py\", line 10, in \n    execute_from_command_line(sys.argv)\n  File \"/env/lib/python3.6/site-packages/django/core/management/__init__.py\", line 371, in execute_from_command_line\n    utility.execute()\n  File \"/env/lib/python3.6/site-packages/django/core/management/__init__.py\", line 347, in execute\n    django.setup()\n  File \"/env/lib/python3.6/site-packages/django/__init__.py\", line 24, in setup\n    apps.populate(settings.INSTALLED_APPS)\n  File \"/env/lib/python3.6/site-packages/django/apps/registry.py\", line 89, in populate\n    app_config = AppConfig.create(entry)\n  File \"/env/lib/python3.6/site-packages/django/apps/config.py\", line 116, in create\n    mod = import_module(mod_path)\n  File \"/opt/python3.6/lib/python3.6/importlib/__init__.py\", line 126, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File \"\", line 994, in _gcd_import\n  File \"\", line 971, in _find_and_load\n  File \"\", line 955, in _find_and_load_unlocked\n  File \"\", line 665, in _load_unlocked\n  File \"\", line 678, in exec_module\n  File \"\", line 219, in _call_with_frames_removed\n  File \"/env/lib/python3.6/site-packages/django/contrib/postgres/apps.py\", line 8, in \n    from .signals import register_type_handlers\n  File \"/env/lib/python3.6/site-packages/django/contrib/postgres/signals.py\", line 3, in \n    import psycopg2\nModuleNotFoundError: No module named 'psycopg2'\n`\n```\nI'm sure the `psycopg2` has been installed successfully earlier. Here's the list installed dependencies after checking by `pip list`:\n```\n`...\nplatformdirs                      2.4.0\nprompt-toolkit                    3.0.24\nprotobuf                          3.19.1\npsutil                            5.5.1\npsycopg2                          2.8.6\npyasn1                            0.4.8\npyasn1-modules                    0.2.8\npycairo                           1.16.2\n...\n`\n```\nAnyone can help? Thanks!",
      "solution": "Thank you guys for your prompt support. I've managed to resolve it.\nBasically during coding I've accidentally commented the `psycopg2` line in the requirements.txt. Because earlier all dependencies were installed successfully and everything is running on dev mode properly so I just kept that line excluded out.\nBut when deploying with Fabric, it will execute the gcloud command (gcloud app deploy) and here's the thing: GCloud will re-run everything (including installing all dependencies all over again). And the `psycopg2` wasn't installed (because it's commented out) during that GCloud runtime. That's why GCloud couldn't find it for further executions.\nSo I've put it back and everything works like a charm. What a stupid move of mine, but it's fantastic to understand how GCloud works on its way",
      "question_score": 4,
      "answer_score": 2,
      "created_at": "2022-01-01T11:21:59",
      "url": "https://stackoverflow.com/questions/70548240/error-modulenotfounderror-no-module-named-psycopg2-when-deploying-django-ap"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 70116787,
      "title": "Django takes ubuntu username to connect database instead of db user from settings.py",
      "problem": "I trying to run django website on aws ec2 machine (ubuntu). my postgresql database running on aws RDS\nwhen I run $python manage.py runserver, it throws this error:\n```\n`django.db.utils.OperationalError: connection to server at \"xxxxx.xxxxx.us-east-2.rds.amazonaws.com\" (xxx.xx.xx.xxx), port 5432 failed: FATAL:  password authentication failed for user \"ubuntu\"\n`\n```\nbasically django trying to connect database using ubuntu user instead of postgres user in settings.py\nmy settings.py:\n```\n`DATABASES = {\n'default': {\n    'ENGINE': 'django.db.backends.postgresql',\n    'NAME': config('NAME'),\n    'USER': config('USER'),\n    'PASSWORD':config('PASSWORD'),\n    'HOST':config('HOST')\n    #'HOST':'localhost'\n}\n`\n```\n}\n.env file:\n```\n`SECRET_KEY=xxxx\nDEBUG=True\nHOST=xxx.xxx.us-east-2.rds.amazonaws.com\nNAME=xxxx\nUSER=postgres\nPASSWORD=xxxxxx\n`\n```\nmy password has special special char '#'",
      "solution": "A Linux distribution typically uses the `$USER` variable to specify the name of the user, not of the database.\nI would advise to use `$DB_USER` or something similar, so:\n```\n`DATABASES = {\n    'default': {\n        'ENGINE': 'django.db.backends.postgresql',\n        'NAME': config('DB_NAME'),\n        'USER': config('DB_USER'),\n        'PASSWORD':config('DB_PASSWORD'),\n        'HOST':config('DB_HOST')\n    }\n}`\n```\nand then configure with:\n```\n`SECRET_KEY=xxxx\nDEBUG=True\nDB_HOST=xxx.xxx.us-east-2.rds.amazonaws.com\nDB_NAME=xxxx\nDB_USER=postgres\nDB_PASSWORD=xxxxxx`\n```",
      "question_score": 4,
      "answer_score": 2,
      "created_at": "2021-11-25T21:16:05",
      "url": "https://stackoverflow.com/questions/70116787/django-takes-ubuntu-username-to-connect-database-instead-of-db-user-from-setting"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 67792770,
      "title": "Psycopg2 execute_values sending all values as text",
      "problem": "I have this table in postgres\n```\n`CREATE TABLE target (\n    a json\n    b integer\n    c text []\n    id integer\n    CONSTRAINT id_fkey FOREIGN KEY (id)\n        REFERENCES public.other_table(id) MATCH SIMPLE\n        ON UPDATE NO ACTION\n        ON DELETE NO ACTION,\n)\n`\n```\nWhich I would like to insert data to from psycopg2 using\n```\n`import psycopg2\nimport psycopg2.extras as extras\n\n# data is of the form dict, integer, list(string), string When I run this I get the `error: column \"a\" is of type json but expression is of type text`. I tried to solve this by adding a type cast in the SELECT statement but then I got the same error for c and then for b.\nOriginally I thought the problem lies in the WITH statement but based on the answers to my previous question this seems to not be the case Postgres `WITH ins AS ...` casting everything as text\nIt seems that `execute_values` is sending all the values as text with `' '`.\nMain Question: How can I get `execute_values` to send the values based on their python data type rather than just as text?\nSub questions: \nHow can I confirm that `execute_values` is in fact sending the values as text with quotation marks?\nWhat is the purpose of the template argument of `execute_values` https://www.psycopg.org/docs/extras.html and could that be of help?",
      "solution": "The issue, as Adrian Klaver points out in their comment, and also seen in this answer, is that the typing is lost in the CTE.\nWe can show this with an example in the `psql` shell:\n`CREATE TABLE test (col1 json);\n\nWITH cte (c) AS (VALUES ('{\"a\": 1}'))\nINSERT INTO test (col) SELECT c FROM cte; \n`\nresulting in\n`ERROR:  column \"col\" is of type json but expression is of type text\n`\nwhereas this version, with the type specified, succeeds:\n`WITH cte(c)  AS  (VALUES ('{\"a\": 1}'::json))\nINSERT INTO test (col) SELECT c FROM cte;\n`\nWe can mimic this in `execute_values`by providing the typing information in the template argument:\n`extras.execute_values(cursor, query, data, template='(%s::json, %s, %s, %s)')\n`",
      "question_score": 4,
      "answer_score": 2,
      "created_at": "2021-06-01T18:37:39",
      "url": "https://stackoverflow.com/questions/67792770/psycopg2-execute-values-sending-all-values-as-text"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 70120968,
      "title": "Logical replication with psycopg2 and publishers",
      "problem": "There is logical replication script written in python and psycopg2:\nhttps://www.psycopg.org/docs/advanced.html#logical-replication-quick-start\nIt works. But I want to receive not all actions in the database, but only those that are specified in publisher.\nFor this:\n\nI created publisher in source database (ver. 13.3) with name \"PUB\" and one table. I also deployed the second database (ver. 14.1) and created a subscription to this publication in it in order to check the functionality of the publication. And it works.\n\nIn psycopg2 documentation (https://www.psycopg.org/docs/extras.html#replication-support-objects) and postgresql documentation (https://postgrespro.ru/docs/postgresql/13/protocol-logical-replication?lang=en) I found a way to set publication_names in start_replication\n\n```\n`cur.start_replication(slot_name='pytest', decode=True, options={'publication_names': 'PUB'})\n`\n```\nBut it didn't work. I got error:\n```\n`psycopg2.errors.InvalidParameterValue: option \"publication_names\" = \"PUB,\" is unknown\n`\n```\nActually the question. Can logical replication in psycorpg2 be associated with a subscription? And if so, how?\nP.s. There is alternative (Trigger/Notify -> Listen/psycopg2), but i want to solve this problem if it possible.",
      "solution": "The answer to your question is probably \"yes and no\".\nAs jjanes states in their answer, you must use the `pgoutput` output plugin to make use of publications.  You can configure `psycopg2` to use it like this:\n`conn = psycopg2.connect(\n    dbname='test',\n    connection_factory=psycopg2.extras.LogicalReplicationConnection,\n)\n\noptions = {'publication_names': 'pub', 'proto_version': '1'}\ncur = conn.cursor()\ntry:\n    cur.start_replication(slot_name='pytest', decode=False, options=options)\nexcept psycopg2.ProgrammingError:\n    cur.create_replication_slot(\n        'pytest',\n        output_plugin='pgoutput',\n    )\n    cur.start_replication(slot_name='pytest', decode=False, options=options)\n`\nHowever `pgoutput` uses a binary protocol, so the output for these statements\n```\n`insert into table1 (col) values ('hello hello');\nupdate table1 set col = 'hello goodbye' where id = 6;\n`\n```\nis\n`b'B\\x00\\x00\\x00\\x00r!\nbecause `psycopg2` doesn't know how to decode the messages.  You could decode them yourself, using the struct module from Python's standard library and the published message formats, or you could consider using wal2json instead of `pgoutput`. `wal2json` does not currently support publications, but it does provide the means to emulate them.",
      "question_score": 3,
      "answer_score": 4,
      "created_at": "2021-11-26T08:41:43",
      "url": "https://stackoverflow.com/questions/70120968/logical-replication-with-psycopg2-and-publishers"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 74534284,
      "title": "Anotate return type with psycopg2 type stub",
      "problem": "I have a function which returns a psycopg2 connection, if a connection can be established. So the return type should be `Optional[psycopg2.connection]`, or `psycopg2.connection | None`. However I am unable to import `psycopg2.connection` at runtime. I've tried the workaround mentioned in How can I import type-definitions from a typeshed stub-file? but that gives me this mypy error: `Single overload definition, multiple required`. Here's my code\n```\n`import psycopg2\nfrom typing import Optional, TYPE_CHECKING, overload\n\nif TYPE_CHECKING:\n    from psycopg2 import connection\n    \n    @overload\n    def get_connection() -> Optional[connection]: ...\n\n# Make DB error logging less spammy\nhas_logged_error = False\n\ndef get_connection():\n    try:\n        conn = psycopg2.connect(\n            dbname=settings.db_name,\n            user=settings.db_user,\n            password=settings.db_password,\n            host=settings.db_host,\n            port=settings.db_port,\n        )\n        return conn\n    except Exception as e:\n        global has_logged_error\n        if not has_logged_error:\n            logger.error(f\"Error connecting to DB: {e}\")\n            has_logged_error = True\n        return\n`\n```",
      "solution": "The question you linked proposes some extremely dirty hack which doesn't seem to work any more. There is absolutely no need for it under such simple circumstances. Moreover, to be honest, I cannot reproduce that solution on any `mypy` version starting from `0.800` (old enough, given that the linked answer is recent), so that perhaps never worked.\nI reduced your code samples to contain only minimal return for the sake of readability.\nVariant 1: use conditional import and string annotation\n`import psycopg2\nfrom typing import Optional, TYPE_CHECKING\n\nif TYPE_CHECKING:\n    from psycopg2 import connection\n    \ndef get_connection() -> Optional['connection']:\n    return psycopg2.connect(...)\n`\nThis is simple: `mypy` known what `connection` is (defined in stubs); runtime does not try to learn something about `connection` because it sees simply a string.\nVariant 2: use conditional import and annotations future\n`from __future__ import annotations\nimport psycopg2\nfrom typing import Optional, TYPE_CHECKING\n\nif TYPE_CHECKING:\n    from psycopg2 import connection\n    \ndef get_connection() -> Optional[connection]:\n    return psycopg2.connect(...)\n`\nDocs for future imports. This is very similar to direct string usage, but looks nicer and is more convenient, IMO.\nVariant 3: use string annotation, but avoid conditional import\n`import psycopg2\nfrom typing import Optional\n    \ndef get_connection() -> Optional['psycopg2.connection']:\n    return psycopg2.connect(...)\n`\nVariant 4: use annotations future, but avoid conditional import\n`from __future__ import annotations\nimport psycopg2\nfrom typing import Optional\n    \ndef get_connection() -> Optional[psycopg2.connection]:\n    return psycopg2.connect(...)\n`\nVariants 3 and 4 do not expose that `connection` is stub-only, hiding it as implementation detail. You may prefer to state that explicitly - then use 1 or 2.\nModification to use current features\nThis is my favorite. Union syntax is valid in python 3.10+, so if you use an older one - you may want to stick with `Optional` as described above for consistency. However, `annotations` future-import makes this expression effectively a string, so if your tools do not perform any runtime typing introspection - you can still use pipe union syntax on older versions. Just be aware that `typing.get_type_hints` and similar utilities will fail with this syntax on python before 3.10.\n`from __future__ import annotations\nimport psycopg2\n    \ndef get_connection() -> psycopg2.connection | None:\n    return psycopg2.connect(...)\n`",
      "question_score": 3,
      "answer_score": 7,
      "created_at": "2022-11-22T15:31:33",
      "url": "https://stackoverflow.com/questions/74534284/anotate-return-type-with-psycopg2-type-stub"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 70620960,
      "title": "How to create a database using psycopg3?",
      "problem": "This does not work:\n```\n`conn = psycopg.connect(dsn)\nconn.execute(\"CREATE DATABASE test\")\n`\n```\nHere is the documentation about transactions in psycopg3: https://www.psycopg.org/psycopg3/docs/basic/transactions.html\nThe most important statement for this problem:\n\nPsycopg has a behaviour that may seem surprising compared to psql: by default, any database operation will start a new transaction.\n\nIt is quite a long page, but it does not tell anywhere how to execute a statement without starting a new transaction. There is an `autocommit=True` argument for `connect()`, but it doesn't work either.\nNo matter what I do, I always get this error:\n```\n`psycopg.errors.ActiveSqlTransaction: CREATE DATABASE cannot run inside a transaction block\n`\n```\nHow can I create a database with psycopg3?",
      "solution": "Using an autocommit connection works for me:\n```\n`>>> conn = psycopg.connect(dbname='postgres', autocommit=True)\n>>> cur = conn.cursor()\n>>> cur.execute('drop database if exists test3')\n\n>>> cur.execute('create database test3')\n\n>>> \n`\n```\n`xxx@host psql postgres -tl | grep test3\n test3             \u2502 xxx      \u2502 UTF8     \u2502 en_GB.UTF-8 \u2502 en_GB.UTF-8 \u2502 \n`",
      "question_score": 3,
      "answer_score": 7,
      "created_at": "2022-01-07T13:01:34",
      "url": "https://stackoverflow.com/questions/70620960/how-to-create-a-database-using-psycopg3"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 79176006,
      "title": "Why are parameterized queries not possible with DO ... END?",
      "problem": "The following works fine:\n`conn = psycopg.connect(self.conn.params.conn_str)\ncur = conn.cursor()\ncur.execute(\"\"\"\n    SELECT 2, %s;\n    \"\"\", (1,),\n)\n`\nBut inside a `DO`:\n`cur.execute(\"\"\"\nDO $$\nBEGIN\n  SELECT 2, %s;\nEND$$;\n\"\"\",  (1,),\n)\n`\nit causes\n```\n`psycopg.errors.UndefinedParameter: there is no parameter $1\nLINE 1: SELECT 2, $1\n                  ^\nQUERY:  SELECT 2, $1\nCONTEXT:  PL/pgSQL function inline_code_block line 3 at SQL statement\n`\n```\nIs this expected?",
      "solution": "```\n`import psycopg\nfrom psycopg import sql\n\ncon = psycopg.connect(\"postgresql://postgres:postgres@127.0.0.1:5432/test\")\ncur = con.cursor()\n\ncur.execute(sql.SQL(\"\"\"\nDO $$\nBEGIN\n  PERFORM 2, {};\nEND$$;\n\"\"\").format(sql.Literal(1))\n)\n`\n```\nThis uses the sql module of `psycopg` to build a dynamic SQL statement using proper escaping. `DO` can't `return` anything so you will not get any result from the function.",
      "question_score": 3,
      "answer_score": 2,
      "created_at": "2024-11-11T00:08:31",
      "url": "https://stackoverflow.com/questions/79176006/why-are-parameterized-queries-not-possible-with-do-end"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 72982495,
      "title": "couldn&#39;t install psycopg2 for a FastAPI project on macOs 10.13.6 : python setup.py egg_info did not run successfully",
      "problem": "I tried to install psycopg2 with the command line :\n`pip install psycopg2`\nand this is what I get\n```\n`  Preparing metadata (setup.py) ... error\n  error: subprocess-exited-with-error\n  \n  \u00d7 python setup.py egg_info did not run successfully.\n  \u2502 exit code: 1\n  \u2570\u2500> [25 lines of output]\n      /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/setuptools/config/setupcfg.py:463: SetuptoolsDeprecationWarning: The license_file parameter is deprecated, use license_files instead.\n        warnings.warn(msg, warning_class)\n      running egg_info\n      creating /private/var/folders/lx/rssnrrbj15jcz8pj88rr89f00000gn/T/pip-pip-egg-info-lpapm88u/psycopg2.egg-info\n      writing /private/var/folders/lx/rssnrrbj15jcz8pj88rr89f00000gn/T/pip-pip-egg-info-lpapm88u/psycopg2.egg-info/PKG-INFO\n      writing dependency_links to /private/var/folders/lx/rssnrrbj15jcz8pj88rr89f00000gn/T/pip-pip-egg-info-lpapm88u/psycopg2.egg-info/dependency_links.txt\n      writing top-level names to /private/var/folders/lx/rssnrrbj15jcz8pj88rr89f00000gn/T/pip-pip-egg-info-lpapm88u/psycopg2.egg-info/top_level.txt\n      writing manifest file '/private/var/folders/lx/rssnrrbj15jcz8pj88rr89f00000gn/T/pip-pip-egg-info-lpapm88u/psycopg2.egg-info/SOURCES.txt'\n      \n      Error: pg_config executable not found.\n      \n      pg_config is required to build psycopg2 from source.  Please add the directory\n      containing pg_config to the $PATH or specify the full executable path with the\n      option:\n      \n          python setup.py build_ext --pg-config /path/to/pg_config build ...\n      \n      or with the pg_config option in 'setup.cfg'.\n      \n      If you prefer to avoid building psycopg2 from source, please install the PyPI\n      'psycopg2-binary' package instead.\n      \n      For further information please check the 'doc/src/install.rst' file (also at\n      ).\n      \n      [end of output]\n  \n  note: This error originates from a subprocess, and is likely not a problem with pip.\nerror: metadata-generation-failed\n\n\u00d7 Encountered error while generating package metadata.\n\u2570\u2500> See above for output.\n\nnote: This is an issue with the package mentioned above, not pip.\nhint: See above for details. \n`\n```\nthen I tried to download the package from the official web site and put in the PATH:\n`/Users/t/PycharmProjects/API/venv/lib/python3.10/site-packages`\n(Im using a virtual environement) but I get  No module named 'psycopg2'",
      "solution": "After reading some similar questions on Stackoverflow here is the solution that worked for me.\nFirst, install Homebrew in case you don't already have it installed\n`/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\" `\nthen install postgresql :\n`brew install postgresql `\nfinally use the pip command to install psycpg2 :\n`pip install psycopg2`",
      "question_score": 3,
      "answer_score": 4,
      "created_at": "2022-07-14T16:49:28",
      "url": "https://stackoverflow.com/questions/72982495/couldnt-install-psycopg2-for-a-fastapi-project-on-macos-10-13-6-python-setup"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 71343175,
      "title": "Why are the `keepalives` params in `psycopg2.connect(...)` required to run long running postgres queries in docker (ubuntu:18.04)?",
      "problem": "We just transitioned to using Docker for development and are using the `ubuntu:18.04` image. We noticed that queries using `psycopg2` failed after a few minutes. This answer solved the problem using the following `keepalives` params:\n```\n`self.db = pg.connect(\n    dbname=config.db_name,\n    user=config.db_user,\n    password=config.db_password,\n    host=config.db_host,\n    port=config.db_port,\n    keepalives=1,\n    keepalives_idle=30,\n    keepalives_interval=10,\n    keepalives_count=5\n)\n`\n```\nThis works for us as well, but why does this work? The psycopg2 docs do not give insight into what the params do, however, this third party documentation does, and this postgres documentation does.\nThe question is, what is different in the docker environment vs the host environment which makes these non-default settings required? They work in a standard Ubuntu 18.04 environment too, but not in docker. I am hoping we could reconfigure our docker image so that these non-standard parameters aren't necessary in the first place.\n\nPostgres version: `PostgreSQL 13.4 (Ubuntu 13.4-1.pgdg20.04+1) on x86_64-pc-linux-gnu, compiled by gcc (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0, 64-bit`\npsycopg2 version: `2.8.5`\nHost OS: Windows 10\nDocker Image OS: Ubuntu 18:04",
      "solution": "You are probably using Dockers Overlay Network feature (or Ingress network for loadbalanced services), which is based on Linux IP Virtual Server (IPVS), a.k.a.Linux Virtual Server. This uses a default 900 second (15 minutes) timeout for idle TCP connections.\nSee: https://github.com/moby/moby/issues/31208\nDefault Linux TCP Keep-Alive settings only start sending packets much later (if enabled at all) and thus you are left with the options of:\n\nchange the TCP Keep-Alive settings on the server or client\nchange the Docker networking to use the host network directly\nchange your software to avoid idle TCP connections, e.g. configure connection pools for databases to remove idle connections or check health more often\nchange the Kernel IPVS defaults or TCP defaults",
      "question_score": 3,
      "answer_score": 5,
      "created_at": "2022-03-03T21:08:10",
      "url": "https://stackoverflow.com/questions/71343175/why-are-the-keepalives-params-in-psycopg2-connect-required-to-run-long"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 66239722,
      "title": "Cannot update existing row on conflict in PostgreSQL with Psycopg2",
      "problem": "I have the following function defined to insert several rows with iteration in Python using Psycopg2 and PostgreSQL 11.\nWhen I receive the same obj (with same id), I want to update its date.\n```\n`def insert_execute_values_iterator(\n    connection,\n    objs: Iterator[Dict[str, Any]],\n    page_size: int = 1000,\n) -> None:\n    with connection.cursor() as cursor:\n        try:\n            psycopg2.extras.execute_values(cursor, \"\"\"\n                INSERT INTO objs(\\\n                                                id,\\\n                                                date,\\\n                ) VALUES %s \\\n                ON CONFLICT (id) \\\n                    DO UPDATE SET (date) = (EXCLUDED.date) \\\n            \"\"\", ((\n                obj['id'],\n                obj['date'],\n\n            ) for obj in objs), page_size=page_size)\n        except (Exception, Error) as error:\n            print(\"Error while inserting as in database\", error)\n`\n```\nWhen a conflict happens on the unique primary key of the table while inserting an element, I get the error:\n\nError while inserting as in database ON CONFLICT DO UPDATE command\ncannot affect row a second time\nHINT:  Ensure that no rows proposed for insertion within the same command have duplicate constrained values.\n\nFYI, the clause works on PostgreSQL directly but not from the Python code.",
      "solution": "Use unique VALUE-combinations in your INSERT statement:\n```\n`create table foo(id int primary key, date date);\n`\n```\nThis should work:\n```\n`INSERT INTO foo(id, date) \nVALUES(1,'2021-02-17') \nON CONFLICT(id) \nDO UPDATE SET date = excluded.date;\n`\n```\nThis one won't:\n```\n`INSERT INTO foo(id, date) \nVALUES(1,'2021-02-17') , (1, '2021-02-16') -- 2 conflicting rows\nON CONFLICT(id) \nDO UPDATE SET date = excluded.date;\n`\n```\nDEMO\nYou can fix this by using DISTINCT ON() in a SELECT statement:\n```\n`INSERT INTO foo(id, date) \nSELECT DISTINCT ON(id) id, date\nFROM (VALUES(1,CAST('2021-02-17' AS date)) , (1, '2021-02-16')) s(id, date)\nORDER BY id, date ASC\nON CONFLICT(id) \nDO UPDATE SET date = excluded.date;\n`\n```",
      "question_score": 3,
      "answer_score": 5,
      "created_at": "2021-02-17T11:06:06",
      "url": "https://stackoverflow.com/questions/66239722/cannot-update-existing-row-on-conflict-in-postgresql-with-psycopg2"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 65709643,
      "title": "python psycopg2 select current_timestamp problem with timezone",
      "problem": "I'm calling a simple select to obtain current timestamp with timezone with psycopg2 and it's retrieving UTC time instead of my local time (-3).\ndatetime.datetime(2021, 1, 13, 20, 49, 47, 931834, tzinfo=psycopg2.tz.FixedOffsetTimezone(offset=0, name=None))\nIn postgresql I'm doing:\n```\n`select current_timestamp\n`\n```\nThis retrieves (Argentina time -3):\n```\n`2021-01-13 17:39:57\n`\n```\nSo this is correct, but in Python:\n```\n`class DatabaseUtils():\n    def __init__(self):\n        self.dsn = \"dbname=my_database user=postgres host=127.0.0.1\"\n        self.conn, self.cur = self.connect_db()\n        self.database_name = \"my_table\"\n\n    def connect_db(self):\n        \"\"\"\n        :param DSN: data source name. ex: \"dbname=sigbase user=postgres\"\n        :return: connection, cursor   Select method retrieves:\n```\n`Selected records [(datetime.datetime(2021, 1, 13, 20, 49, 47, 931834, tzinfo=psycopg2.tz.FixedOffsetTimezone(offset=0, name=None)),)]\n`\n```\nSo datetime object is comming with offset 0, i.e UTC.\nIs it possible to retrieve current timestamp in psycopg2 with the proper timezone?\nIf not, how can I convert the datetime object timezone?",
      "solution": "I solved this issue in this way:\nI added a method in the class init to set the timezone. In this way, the SELECT statements are giving the proper time.\n```\n`def set_timezone(self):\n    # Get current time zone.\n    timezone = datetime.datetime.now(datetime.timezone.utc).astimezone().tzname()\n    # Set timezone.\n    self.cur.execute(f\"SET TIME ZONE {timezone};\")\n`\n```\nResult logged in python is:\n```\n`Selected records [(datetime.datetime(2021, 1, 14, 14, 21, 18, 455322, tzinfo=psycopg2.tz.FixedOffsetTimezone(offset=-180, name=None)),)]\n`\n```\nThis is correct (Argentina time now).\nExtra info:\nI based on the psycopg documentation, in the example is telling the timezone first, before the query. I think the select current_time in this library is working in UTC by default.\nSource: https://www.psycopg.org/docs/usage.html#time-zones-handling",
      "question_score": 3,
      "answer_score": 4,
      "created_at": "2021-01-13T21:55:32",
      "url": "https://stackoverflow.com/questions/65709643/python-psycopg2-select-current-timestamp-problem-with-timezone"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 79736957,
      "title": "Odoo 18 on Windows: UnicodeDecodeError: &#39;utf-8&#39; codec can&#39;t decode byte 0xe7 on server startup",
      "problem": "I'm setting up a development environment for Odoo 18 on a Windows machine. I've cloned the repository, created a virtual environment (venv), and successfully installed all dependencies from the requirements.txt file.\nHowever, when I try to start the Odoo server for the first time using the command below, the process fails with a UnicodeDecodeError.\nCommand executed:\nPowerShell\n```\n`(venv) PS D:\\tarefas\\trarefa_kaue_1\\teste\\odoo> python odoo-bin -r odoo -w 123\nFull Error (Traceback):\n\n2025-08-16 04:07:47,493 14676 INFO ? odoo: Odoo version 18.0\n2025-08-16 04:07:47,493 14676 INFO ? odoo: addons paths: ['D:\\\\tarefas\\\\trarefa_kaue_1\\\\teste\\\\odoo\\\\odoo\\\\addons', 'c:\\\\users\\\\kauen\\\\appdata\\\\local\\\\openerp s.a\\\\odoo\\\\addons\\\\18.0', 'd:\\\\tarefas\\\\trarefa_kaue_1\\\\teste\\\\odoo\\\\odoo\\\\addons', 'd:\\\\tarefas\\\\trarefa_kaue_1\\\\teste\\\\odoo\\\\addons']\n2025-08-16 04:07:47,493 14676 INFO ? odoo: database: odoo@default:default\n2025-08-16 04:07:50,148 14676 INFO ? odoo.addons.base.models.ir_actions_report: You need Wkhtmltopdf to print a pdf version of the reports.\n2025-08-16 04:07:50,152 14676 INFO ? odoo.addons.base.models.ir_actions_report: You need Wkhtmltoimage to generate images from html.\n2025-08-16 04:07:52,396 14676 INFO ? odoo.service.server: HTTP service (werkzeug) running on kaue_martins.intelbras.local:8069\nException in thread odoo.service.cron.cron0:\nTraceback (most recent call last):\n  File \"C:\\Python312\\Lib\\threading.py\", line 1075, in _bootstrap_inner\n    self.run()\n  File \"C:\\Python312\\Lib\\threading.py\", line 1012, in run\n    self._target(*self._args, **self._kwargs)\n  File \"D:\\tarefas\\trarefa_kaue_1\\teste\\odoo\\odoo\\service\\server.py\", line 541, in target\n    self.cron_thread(i)\n  File \"D:\\tarefas\\trarefa_kaue_1\\teste\\odoo\\odoo\\service\\server.py\", line 522, in cron_thread\n    with contextlib.closing(conn.cursor()) as cr:\n  File \"D:\\tarefas\\trarefa_kaue_1\\teste\\odoo\\odoo\\sql_db.py\", line 799, in cursor\n    return Cursor(self.__pool, self.__dbname, self.__dsn)\n  File \"D:\\tarefas\\trarefa_kaue_1\\teste\\odoo\\odoo\\sql_db.py\", line 288, in __init__\n    self._cnx = pool.borrow(dsn)\n  File \"D:\\tarefas\\trarefa_kaue_1\\teste\\odoo\\venv\\Lib\\site-packages\\decorator.py\", line 232, in fun\n    return caller(func, *(extras + args), **kw)\n  File \"D:\\tarefas\\trarefa_kaue_1\\teste\\odoo\\odoo\\tools\\func.py\", line 97, in locked\n    return func(inst, *args, **kwargs)\n  File \"D:\\tarefas\\trarefa_kaue_1\\teste\\odoo\\odoo\\sql_db.py\", line 726, in borrow\n    result = psycopg2.connect(\n  File \"D:\\tarefas\\trarefa_kaue_1\\teste\\odoo\\venv\\Lib\\site-packages\\psycopg2\\__init__.py\", line 122, in connect\n    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xe7 in position 78: invalid continuation byte\n`\n```\nMy Environment Details:\nOS: Windows 11\nOdoo: Version 18.0\nPython: Version 3.12 (installed at C:\\Python312)\nPostgreSQL: Version 15\nWhat I've investigated so far:\nThe traceback indicates the error occurs during the psycopg2.connect call, which is the library used to connect to PostgreSQL.\nThe UnicodeDecodeError with byte 0xe7 suggests there's a special character (likely \u00e7 in a latin-1 or windows-1252 encoding) somewhere in the connection string that is not UTF-8 encoded.\nI have checked the parameters I passed on the command line (-r odoo and -w 123), and they do not contain any special characters.\nThe PostgreSQL database was initialized with UTF-8 encoding.\nMy suspicion is that the problem might be in a Windows environment variable or in the Odoo configuration file (odoo.conf), which might be read with an incorrect encoding (like windows-1252) instead of UTF-8.\nMy questions are:\nWhere do Odoo or psycopg2 on Windows look for connection information besides the command-line parameters?\nWhat is the most likely cause for this UnicodeDecodeError in this scenario, and how can I fix it?\nThanks for any help!",
      "solution": "That 0xe7 is the \u00e7 character (c cedille). Somewhere in your database DSN (connection string, database name...) or Windows environment variable there\u2019s a non-UTF-8 character (probably \u00e7 in a folder, user, or host name...).\npsycopg2 builds the DSN based on :\n\nodoo-bin's arguments that are passed to psycopg2.connect():\nhttps://www.odoo.com/documentation/18.0/developer/reference/cli.html\n```\n` \u2022 -d ,--database \n\n \u2022 -r ,--db_user \n\n \u2022 -w , --db_password \n\n \u2022 --db_host \n\n \u2022 --db_port \n\n \u2022 -i ,--init  to install a module\n\n \u2022 -u ,--update  comma-separated modules list to update \n\n \u2022 --addons-path  comma-separated list of modules directories\n\n \u2022 -c ,--config  path to an alternate configuration file. If not defined, Odoo checks ODOO_RC environmental variable and default location $HOME/.odoorc. \n\n \u2022 -D ,--data-dir  directory path where to store Odoo data (eg. filestore, sessions). If not specified, Odoo will fallback to a predefined path. On Unix systems its one defined in $XDG_DATA_HOME environmental variable or ~/.local/share/Odoo or /var/lib/Odoo. \n`\n```\n\nodoo_file.conf (config file to store these parameters - listed here above)\nhttps://www.odoo.com/documentation/18.0/developer/reference/cli.html#reference-cmdline-config\n\u2022 db_host = localhost\n\u2022 db_port = 5432\n\u2022 db_user = odoo\n\u2022 db_password = 123...\n\nEnvironment variables (used if above are missing):\n\u2022 PGHOST\n\u2022 PGPORT\n\u2022 PGUSER\n\u2022 PGPASSWORD\n\u2022 PGDATABASE\n\nPostgres service file (on Windows: located at: %APPDATA%\\postgresql\\pg_service.conf)",
      "question_score": 3,
      "answer_score": 3,
      "created_at": "2025-08-16T06:34:51",
      "url": "https://stackoverflow.com/questions/79736957/odoo-18-on-windows-unicodedecodeerror-utf-8-codec-cant-decode-byte-0xe7-on"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 73208691,
      "title": "SqlAlchemy + psycopg2 using ssl certificate",
      "problem": "So far I was using just psycopg2 to create my postgresql connection like\n```\n`connection = psy.connect(host=self.host, port=self.port, dbname=self.database, user=self.user, password=os.environ.get('POSTGRES_READONLY_USER_PASSWORD'), sslmode=self.sslmode, sslrootcert=os.environ.get('CA_FILE_PATH'))\n`\n```\nhowever seeing now that it is deprecated I wanted to switch to sqlalchmey + psycopg2. I am using a CA.pem certificate for ssl, which works fine in the old version\nI tried\n```\n`args = {\n        'host' : self.host,\n        'user' : self.user,\n        'password' : os.environ.get('POSTGRES_READONLY_USER_PASSWORD'),    \n        'port' : self.port,\n        'dbname' : self.database,\n        'sslmode' : 'require',\n        'sslrootcert' : os.environ.get('CA_FILE_PATH')\n        }\n\n    engine = sqlalchemy.create_engine(\"postgresql+psycopg2://\", connect_args=args)\n    connection = engine.connect()\n`\n```\nBut I am getting an error\n```\n`Exception has occurred: OperationalError\n(psycopg2.OperationalError) connection to server at \"postgres.amf\" (10.10.10.20), port 5433 failed: certificate present, but not private key file \"C:\\Users\\Administrator\\AppData\\Roaming/postgresql/postgresql.key\"\n`\n```\nAnyone knows how I can get this working without the additional private keys ????",
      "solution": "I had\n```\n`'sslcert' : os.environ.get('CA_FILE_PATH')\n`\n```\nwhich was supposed to be\n```\n`'sslrootcert' : os.environ.get('CA_FILE_PATH')\n`\n```",
      "question_score": 3,
      "answer_score": 3,
      "created_at": "2022-08-02T15:46:57",
      "url": "https://stackoverflow.com/questions/73208691/sqlalchemy-psycopg2-using-ssl-certificate"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 70066823,
      "title": "Batch upsert multiple rows with psycopg2",
      "problem": "I need to upsert (`INSERT ... ON CONFLICT DO UPDATE`) multiple rows at once into a postgreSQL database using psycopg2. Essentially, I have a list of tuples representing \"rows\", and I need to insert them into the database, or update the database if there is a conflict. I need (possibly) every column to be updated (if not inserted), along with every row.\nI've tried two main approaches, using psycopg2's `cursor.execute()` function and `execute_many()` function. First, I did the following:\n```\n`upsert_statement = 'INSERT INTO table (col1, col2, col3) VALUES %s ON CONFLICT (col1) DO UPDATE SET (col1, col2, col3) = ROW (excluded.*) WHERE table IS DISTINCT FROM excluded'\n\npsycopg2.extras.execute_values(cursor, upsert_statement, values)\n`\n```\nI create an SQL statement that inserts the values using `execute_many()` (where `values` passed to it is a list of tuples), and on a conflict the column values should be updated to excluded. However, I get the error `SyntaxError: number of columns does not match number of values` sometimes, even though I know for a fact that the number of columns and values are the same.\nSo, I tried using only `execute()`:\n```\n`upsert_statement = f'INSERT INTO table (col1, col2, col3) VALUES (value1, value2, value3), (value4, value5, value6)... ON CONFLICT (col1) DO UPDATE SET (col1, col2, col3) = (value1, value2, value3), (value4, value5, value6)...'\n\ncursor.execute(upsert_statement)\n`\n```\nHere, I do the batch upsert as part of the SQL, and so don't have to use `execute_values()`. However, I get a `SyntaxError` after the `DO UPDATE SET`, because I don't think it's valid to have `(col1, col2, col3) = (value1, value2, value3), (value4, value5, value6)...`.\nWhat am I doing wrong? How can I bulk upsert multiple rows using psycopg2?\n(I should note that in reality, `(col1, col2, col3)` and `(value1, value2, value3)` are dynamic, and change frequently)",
      "solution": "You need to use table `EXCLUDED` instead of value literals in your `ON CONFLICT` statement:\n\nThe `SET` and `WHERE` clauses in `ON CONFLICT DO UPDATE` have access to the existing row using the table's name (or an alias), and to the row proposed for insertion using the special `excluded` table.\n\nYou also don't need to re-set the conflicting values, only the rest.\n```\n`INSERT INTO table (col1, col2, col3) \nVALUES \n    (value1, value2, value3), \n    (value4, value5, value6)\nON CONFLICT (col1) DO UPDATE \nSET (col2, col3) = (EXCLUDED.col2, EXCLUDED.col3);\n`\n```\nFor readability, you can format your in-line SQLs if you triple-quote your f-strings. I'm not sure if and which IDEs can detect it's an in-line SQL in Python and switch syntax highlighting, but I find indentation helpful enough.\n```\n`upsert_statement = f\"\"\"\n    INSERT INTO table (col1, col2, col3) \n    VALUES \n        ({value1}, {value2}, {value3}), \n        ({value4}, {value5}, {value6})\n    ON CONFLICT (col1) DO UPDATE \n    SET (col2, col3) = (EXCLUDED.col2, EXCLUDED.col3)\"\"\"\n`\n```\nHere's a test at db<>fiddle:\n`drop table if exists test_70066823 cascade;\ncreate table test_70066823 (\n    id integer primary key, \n    text_column_1 text, \n    text_column_2 text);\ninsert into test_70066823 values\n  (1,'first','first')\n ,(2,'second','second') returning *;\n`\n\nid\ntext_column_1\ntext_column_2\n\n1\nfirst\nfirst\n\n2\nsecond\nsecond\n\n`insert into test_70066823\nvalues  (1, 'third','first'),\n        (3, 'fourth','third'),\n        (4, 'fifth','fourth'),\n        (2, 'sixth','second')\non conflict (id) do update \nset text_column_1=EXCLUDED.text_column_1,\n    text_column_2=EXCLUDED.text_column_2\nreturning *;\n`\n\nid\ntext_column_1\ntext_column_2\n\n1\nthird\nfirst\n\n3\nfourth\nthird\n\n4\nfifth\nfourth\n\n2\nsixth\nsecond\n\nYou can refer to this for improved insert performance. Inserts with a simple string-based `execute` or `execute_many` are the top 2 slowest approaches mentioned there.",
      "question_score": 3,
      "answer_score": 3,
      "created_at": "2021-11-22T14:48:53",
      "url": "https://stackoverflow.com/questions/70066823/batch-upsert-multiple-rows-with-psycopg2"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 77940460,
      "title": "psycopg3 pool: all connections getting lost instantly after long idle time",
      "problem": "If it's a standalone persistant connection, I have no problem, connection lasts for hours.\nIf I use psycopg(3) Connection pool, I make make requests, first requests are ok and my pool size stays at 5, at one point pool size decreases and I get a Pool Timeout when client makes a new request.\nThen I tried: start pool, do not request anything, just wait. After some time (around 1h) I look at postgresql (pg_stat_activity), I have 5 idle (=pool size) connections. Then I make a request from client, and all connections vanish at same time (I can see it from pg_stat_activity) and Pool Timeout, and situation is stuck.\nI also tried to decrease max_timeout to 900 but still same issue.\n```\n`def init_pool(self, min_cnx=5):\n    cnx_str = f\"host={DB_HOST} port={DB_PORT} dbname={DB_NAME} user={DB_USERNAME} password={DB_USERPWD}\"\n    self.pool = ConnectionPool(conninfo=cnx_str, min_size=min_cnx, open=True, check=ConnectionPool.check_connection)\n\ndef query(self, q, dbv=None, debug=False) -> list:\n\n    print(\"pool size: \", len(self.pool._pool))\n    print(\"pool stats before: \", self.pool.get_stats())\n\n    with self.pool.connection() as cnx:\n\n        if cnx.closed:\n            self.pool.check()\n            raise ConnectionError(\"ERROR: PostgreSQL cnx from pool is closed.\")\n\n        cnx.autocommit = True\n        cnx.row_factory = self.row_factory\n\n        with psycopg.ClientCursor(cnx) as rdc:\n            rdc.execute(q, dbv) if dbv else rdc.execute(q)\n\n            if debug and rdc._query:\n                print(rdc._query.query)\n\n            if rdc.description:\n                data = rdc.fetchall()\n            else:\n                data = []\n\n        print(\"pool stats after query: \", self.pool.get_stats())\n    print(\"pool stats after: \", self.pool.get_stats())\n    return data\n`\n```\nAnd logs:\n```\n`[pid: 236344|app: 0|req: 26/26] () {56 vars in 1083 bytes} [Mon Feb  5 11:41:56 2024] POST /v1/user => generated 933 bytes in \n109 msecs (HTTP/1.1 200) 8 headers in 749 bytes (1 switches on core 0)                                                                                                                                         \npool size:  3                                                                                                                               \npool stats before:  {'connections_num': 5, 'requests_num': 3, 'requests_queued': 1, 'connections_ms': 268, 'requests_wait_ms': 34, 'usage_ms\n': 34, 'pool_min': 5, 'pool_max': 5, 'pool_size': 5, 'pool_available': 3, 'requests_waiting': 0}                                            \npool stats after query:  {'connections_num': 5, 'requests_num': 4, 'requests_queued': 1, 'connections_ms': 268, 'requests_wait_ms': 34, 'usa\nge_ms': 34, 'pool_min': 5, 'pool_max': 5, 'pool_size': 5, 'pool_available': 2, 'requests_waiting': 0}                                       \npool stats after:  {'connections_num': 5, 'requests_num': 4, 'requests_queued': 1, 'connections_ms': 268, 'requests_wait_ms': 34, 'usage_ms'\n: 49, 'pool_min': 5, 'pool_max': 5, 'pool_size': 5, 'pool_available': 2, 'requests_waiting': 0}                                             \n[pid: 236344|app: 0|req: 28/28] () {56 vars in 1087 bytes} [Mon Feb  5 11:41:58 2024] POST /v1/iobjs => generated 4788 bytes i\nn 29 msecs (HTTP/1.1 200) 6 headers in 302 bytes (1 switches on core 0)                                                                     \n[pid: 236344|app: 0|req: 29/29] () {54 vars in 816 bytes} [Mon Feb  5 11:42:05 2024] OPTIONS /v1/user/quit => generated 0 byte\ns in 0 msecs (HTTP/1.1 200) 6 headers in 307 bytes (0 switches on core 0)                                                                                                                                                              \npool size:  0   \npool stats before:  {'connections_num': 5, 'requests_num': 6, 'requests_queued': 1, 'connections_ms': 268, 'requests_wait_ms': 34, 'usage_ms\n': 62, 'pool_min': 5, 'pool_max': 5, 'pool_size': 5, 'pool_available': 0, 'requests_waiting': 0}                                            \nTraceback (most recent call last):                                                          \n  File \"/var/srvr/log.py\", line 68, in process\n    self.db.query(\n  File \"/var/srvr/pg3p.py\", line 71, in query\n    with self.pool.connection() as cnx:\n  File \"/usr/local/lib/python3.12/contextlib.py\", line 137, in __enter__\n    return next(self.gen)\n           ^^^^^^^^^^^^^^\n  File \"/var/srvr/lib/python3.12/site-packages/psycopg_pool/pool.py\", line 170, in connection\n    conn = self.getconn(timeout=timeout)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/srvr/lib/python3.12/site-packages/psycopg_pool/pool.py\", line 204, in getconn\n    raise PoolTimeout(\npsycopg_pool.PoolTimeout: couldn't get a connection after 30.00 sec  \npool size:  0  \npool stats before:  {'connections_num': 5, 'requests_num': 7, 'requests_queued': 2, 'connections_ms': 268, 'requests_wait_ms': 30035, 'usage\n_ms': 62, 'requests_errors': 1, 'pool_min': 5, 'pool_max': 5, 'pool_size': 5, 'pool_available': 0, 'requests_waiting': 1}\n`\n```\nEDIT:\nI switched back to 1 single persistant connection, it is very stable (days). Then following advices in comments, I moved back to pool with min_size=10 and max_size=20.\nNo change in behaviour: pool is loosing connections without trying to initiate new ones to replace the lost (also tried 20 and 50 min/max, no difference)\n```\n`pool stats after:  {'connections_num': 11, 'requests_num': 34, 'requests_queued': 1, 'connections_ms': 641, 'requests_wait_ms': 37, 'usage_m s': 67, 'pool_min': 10, 'pool_max': 20, 'pool_size': 11, 'pool_available': 5, 'requests_waiting': 0}\n[pid: 282017|app: 0|req: 39/39]  () {56 vars in 1087 bytes}[Thu Feb  8 11:02:17 2024] POST /v1/iobjs => generated 30081 bytes  in 10 msecs (HTTP/1.1 200) 6 headers in 303 bytes (1 switches on core 0)\npool size:  5                                                         \npool stats before:  {'connections_num': 11, 'requests_num': 34, 'requests_queued': 1, 'connections_ms': 641, 'requests_wait_ms': 37, 'usage_ ms': 67, 'pool_min': 10, 'pool_max': 20, 'pool_size': 11, 'pool_available': 5, 'requests_waiting': 0}                          \npool stats after query:  {'connections_num': 11, 'requests_num': 35, 'requests_queued': 1, 'connections_ms': 641, 'requests_wait_ms': 37, 'u sage_ms': 67, 'connections_lost': 1, 'pool_min': 10, 'pool_max': 20, 'pool_size': 11, 'pool_available': 3, 'requests_waiting': 0}       \npool stats after:  {'connections_num': 11, 'requests_num': 35, 'requests_queued': 1, 'connections_ms': 641, 'requests_wait_ms': 37, 'usage_m s': 70, 'connections_lost': 1, 'pool_min': 10, 'pool_max': 20, 'pool_size': 11, 'pool_available': 3, 'requests_waiting': 0}       \n[pid: 282017|app: 0|req: 40/40]  () {56 vars in 1087 bytes} [Thu Feb  8 11:02:17 2024] POST /v1/iobjs => generated 4788 bytes i n 5 msecs (HTTP/1.1 200) 6 headers in 302 bytes (1 switches on core 0)   \n[pid: 282017|app: 0|req: 41/41]  () {54 vars in 808 bytes} [Thu Feb  8 11:02:26 2024] OPTIONS /v1/iobjs => generated 0 bytes in  0 msecs (HTTP/1.1 200) 6 headers in 307 bytes (0 switches on core 0)   \n[pid: 282017|app: 0|req: 42/42]  () {54 vars in 814 bytes} [Thu Feb  8 11:02:26 2024] OPTIONS /v1/settings => generated 0 bytes  in 0 msecs (HTTP/1.1 200) 6 headers in 307 bytes (0 switches on core 0)                                                                  \npool size:  3                                                          \npool stats before:  {'connections_num': 11, 'requests_num': 35, 'requests_queued': 1, 'connections_ms': 641, 'requests_wait_ms': 37, 'usage_ ms': 70, 'connections_lost': 1, 'pool_min': 10, 'pool_max': 20, 'pool_size': 11, 'pool_available': 3, 'requests_waiting': 0}       \npool stats after query:  {'connections_num': 11, 'requests_num': 36, 'requests_queued': 1, 'connections_ms': 641, 'requests_wait_ms': 37, 'u sage_ms': 70, 'connections_lost': 1, 'pool_min': 10, 'pool_max': 20, 'pool_size': 11, 'pool_available': 2, 'requests_waiting': 0}       \npool stats after:  {'connections_num': 11, 'requests_num': 36, 'requests_queued': 1, 'connections_ms': 641, 'requests_wait_ms': 37, 'usage_m s': 73, 'connections_lost': 1, 'pool_min': 10, 'pool_max': 20, 'pool_size': 11, 'pool_available': 2, 'requests_waiting': 0}       \n[pid: 282017|app: 0|req: 43/43]  () {56 vars in 1087 bytes} [Thu Feb  8 11:02:26 2024] POST /v1/iobjs => generated 4788 bytes i n 6 msecs (HTTP/1.1 200) 6 headers in 302 bytes (1 switches on core 0)   \nTraceback (most recent call last):                                     File \"main.py\", line 326, in v1_settings                              \n\n    row = db.query_row(                                                                                                                     \n          ^^^^^^^^^^^^^\n`\n```\nand postgresql logs (debug3) show nothing special (AFAIU):\n```\n`2024-02-08 11:02:17.075 CET [282007] pguser@maindb DEBUG:  proc_exit(0): 2 callbacks to make                                                \n2024-02-08 11:02:17.075 CET [282007] pguser@maindb DEBUG:  exit(0)                                                                          \n2024-02-08 11:02:17.075 CET [282007] pguser@maindb DEBUG:  shmem_exit(-1): 0 before_shmem_exit callbacks to make                            \n2024-02-08 11:02:17.075 CET [282007] pguser@maindb DEBUG:  shmem_exit(-1): 0 on_shmem_exit callbacks to make                                \n2024-02-08 11:02:17.075 CET [282007] pguser@maindb DEBUG:  proc_exit(-1): 0 callbacks to make                                               \n2024-02-08 11:02:17.079 CET [281970] DEBUG:  server process (PID 282007) exited with exit code 0                                            \n2024-02-08 11:02:26.183 CET [282006] pguser@maindb DEBUG:  shmem_exit(0): 4 before_shmem_exit callbacks to make                             \n2024-02-08 11:02:26.183 CET [282006] pguser@maindb DEBUG:  shmem_exit(0): 6 on_shmem_exit callbacks to make                                 \n2024-02-08 11:02:26.183 CET [282006] pguser@maindb DEBUG:  proc_exit(0): 2 callbacks to make                                                \n2024-02-08 11:02:26.183 CET [282006] pguser@maindb DEBUG:  exit(0)                                                                          \n2024-02-08 11:02:26.183 CET [282006] pguser@maindb DEBUG:  shmem_exit(-1): 0 before_shmem_exit callbacks to make                            \n2024-02-08 11:02:26.183 CET [282006] pguser@maindb DEBUG:  shmem_exit(-1): 0 on_shmem_exit callbacks to make                                \n2024-02-08 11:02:26.183 CET [282006] pguser@maindb DEBUG:  proc_exit(-1): 0 callbacks to make                                               \n2024-02-08 11:02:26.188 CET [282009] pguser@maindb DEBUG:  shmem_exit(0): 4 before_shmem_exit callbacks to make                             \n2024-02-08 11:02:26.189 CET [282009] pguser@maindb DEBUG:  shmem_exit(0): 6 on_shmem_exit callbacks to make\n2024-02-08 11:02:26.189 CET [282009] pguser@maindb DEBUG:  proc_exit(0): 2 callbacks to make\n2024-02-08 11:02:26.189 CET [282009] pguser@maindb DEBUG:  exit(0)\n2024-02-08 11:02:26.189 CET [282009] pguser@maindb DEBUG:  shmem_exit(-1): 0 before_shmem_exit callbacks to make\n2024-02-08 11:02:26.189 CET [282009] pguser@maindb DEBUG:  shmem_exit(-1): 0 on_shmem_exit callbacks to make\n2024-02-08 11:02:26.189 CET [282009] pguser@maindb DEBUG:  proc_exit(-1): 0 callbacks to make\n2024-02-08 11:02:26.189 CET [281970] DEBUG:  server process (PID 282006) exited with exit code 0\n2024-02-08 11:02:26.191 CET [282011] pguser@maindb DEBUG:  shmem_exit(0): 4 before_shmem_exit callbacks to make\n2024-02-08 11:02:26.191 CET [282011] pguser@maindb DEBUG:  shmem_exit(0): 6 on_shmem_exit callbacks to make\n2024-02-08 11:02:26.191 CET [282011] pguser@maindb DEBUG:  proc_exit(0): 2 callbacks to make\n2024-02-08 11:02:26.192 CET [282011] pguser@maindb DEBUG:  exit(0)\n2024-02-08 11:02:26.192 CET [282011] pguser@maindb DEBUG:  shmem_exit(-1): 0 before_shmem_exit callbacks to make\n2024-02-08 11:02:26.192 CET [282011] pguser@maindb DEBUG:  shmem_exit(-1): 0 on_shmem_exit callbacks to make\n2024-02-08 11:02:26.192 CET [282011] pguser@maindb DEBUG:  proc_exit(-1): 0 callbacks to make\n2024-02-08 11:02:26.193 CET [281970] DEBUG:  server process (PID 282009) exited with exit code 0\n2024-02-08 11:02:26.194 CET [281970] DEBUG:  server process (PID 282011) exited with exit code 0\n2024-02-08 11:02:33.979 CET [281970] DEBUG:  postmaster received pmsignal signal\n2024-02-08 11:02:33.983 CET [282844] DEBUG:  InitPostgres\n2024-02-08 11:02:33.985 CET [282844] DEBUG:  autovacuum: processing database \"template1\"\n`\n```\nquery_row() being same as query():\n```\n`def query_row(self, q, dbv=None, debug=False):\n\n    with self.pool.connection() as cnx:\n        cnx.autocommit = True\n        cnx.row_factory = self.row_factory\n\n        with psycopg.ClientCursor(cnx) as c:\n            c.execute(q, dbv) if dbv else c.execute(q)\n\n            if debug and c._query:\n                print(c._query.query)\n\n            if c.rowcount == 1:\n                return c.fetchone()\n            else:\n                return None\n`\n```",
      "solution": "connection logs from postgresql gave a hint: SSL error before all connection going dust:\n```\n`2024-02-09 10:38:05.627 CET [297036] LOG:  checkpoint starting: time                                                                        \n2024-02-09 10:38:05.739 CET [297036] LOG:  checkpoint complete: wrote 2 buffers (0.0%); 0 WAL file(s) added, 0 removed, 0 recycled; write=0. 103 s, sync=0.002 s, total=0.112 s; sync files=2, longest=0.001 s, average=0.001 s; distance=1 kB, estimate=1 kB; lsn=0/1C4BB70, redo lsn=0/1C4BB38\n2024-02-09 10:46:46.488 CET [297518] user@db LOG:  SSL error: decryption failed or bad record mac\n2024-02-09 10:46:46.489 CET [297518] user@db LOG:  could not receive data from client: Connection reset by peer\n2024-02-09 10:46:46.489 CET [297518] user@db LOG:  disconnection: session time: 0:10:00.165 user=user database=db host=127.0.0.1 port=56266\n2024-02-09 12:18:21.905 CET [297528] user@db LOG:  disconnection: session time: 1:41:35.416 user=user database=db host=127.0.0.1 port=56356\n2024-02-09 12:18:21.909 CET [297519] user@db LOG:  disconnection: session time: 1:41:35.587 user=user database=db host=127.0.0.1 port=56276\n2024-02-09 12:18:24.998 CET [297520] user@db LOG:  disconnect103 s, sync=0.002 s, total=0.112 s; sync files=2, longest=0.001 s, average=0.001 s; distance=1 kB, estimate=1 kB; lsn=0/1C4BB70, redo lsn=0/1C4BB38\n2024-02-09 12:18:25.033 CET [297521] user@db LOG:  disconnection: session time: 1:41:38.672 user=user database=db host=127.0.0.1 port=56296\n2024-02-09 12:18:33.726 CET [297522] user@db LOG:  disconnection: session time: 1:41:47.360 user=user database=db host=127.0.0.1 port=56302\n2024-02-09 12:18:33.739 CET [297523] user@db LOG:  disconnection: session time: 1:41:47.372 user=user database=db host=127.0.0.1 port=56308\n2024-02-09 12:18:33.745 CET [297524] user@db LOG:  disconnection: session time: 1:41:47.316 user=user database=db host=127.0.0.1 port=56310\n2024-02-09 12:18:33.760 CET [297525] user@db LOG:  disconnection: session time: 1:41:47.325 user=user database=db host=127.0.0.1 port=56320\n2024-02-09 12:18:35.793 CET [297526] user@db LOG:  disconnection: session time: 1:41:49.353 user=user database=db host=127.0.0.1 port=56336\n2024-02-09 12:18:35.847 CET [297527] user@db LOG:  disconnection: session time: 1:41:49.365 user=user database=db host=127.0.0.1 port=56340\n`\n```\nReason of this SSL error comes from uwsgi default behaviour: if you have 2 workers, at init a context is created (variables initialized, among them Pool and connections) then workers share memory. It results the classical case where connections are shared between 2 workers.\nAt one point in time, it is causing a SSL inconsistency, resulting in mass-closing.\nSolution was to instruct, with `lazy-apps=true`, uwsgi to create each worker with its own context.",
      "question_score": 3,
      "answer_score": 2,
      "created_at": "2024-02-05T12:12:26",
      "url": "https://stackoverflow.com/questions/77940460/psycopg3-pool-all-connections-getting-lost-instantly-after-long-idle-time"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 77913003,
      "title": "How does autocommit work with multiple queries?",
      "problem": "I have a psycopg connection with autocommit turned on. Say, I run a query that is a combination of multiple queries, e.g.:\n```\n`query = \";\".join([create_table, insert_data, analyze_table])\nconn.execute(query)\n`\n```\nIs this batch executed in a single transaction or multiple transactions? What happens if a query in the middle fails?",
      "solution": "From Chain multiple statements within Psycopg2\n\nIn SQL chaining refers to the process of linking multiple SQL\nstatements together into a single string, separated by semicolons.\nThis allows you to execute multiple SQL statements at once, without\nhaving to execute them individually.\nFor example, you might chain together a SELECT statement to retrieve\ndata from a table, followed by an UPDATE statement to modify the data,\nand then a DELETE statement to remove it. When using chaining, it is\nimportant to note that each statement will be executed in the order\nthey appear in the chain and that the results of one statement can be\nused in the next one. Additionally, when chaining SQL statements, if\nany statement in the chain fails, the entire chain will fail and none\nof the statements will be executed.",
      "question_score": 3,
      "answer_score": 2,
      "created_at": "2024-01-31T12:34:03",
      "url": "https://stackoverflow.com/questions/77913003/how-does-autocommit-work-with-multiple-queries"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 69622944,
      "title": "Apply type cast to items of array in parameter with psycopg2",
      "problem": "Issue\nI am struggling with inserting data into table with column of array of custom data type in Python.\nThe scheme looks like:\n`CREATE TYPE data_source AS ENUM ('smtp', 'ftp', 'http');\nCREATE TABLE IF NOT EXISTS data(\n    id BIGSERIAL PRIMARY KEY,\n    foo TEXT NOT NULL,\n    sources data_source[]\n);\n`\nthen, I want to insert some data into such table from Python using psycopg2:\n`foo = \"my_text\"\nsources = [\"ftp\", \"http\"]\n\ncursor.execute(\n    \"\"\"\n        INSERT INTO data(foo, sources)\n        VALUES (%s, %s)\n    \"\"\",\n    (foo, sources),\n)\n`\nthis code ends up with runtime exception:\n```\n`LINE 3: ...('my text', ARRAY['ftp...\n                       ^\nHINT:  You will need to rewrite or cast the expression.\n`\n```\nI understand that I need to call `::data_source` type casting to each of element of the `ARRAY`. How can I achieve this?\nA variant with class and adapt()\nI tried to take advantage of `adapt` function from the `psycopg2.extensions` package\n`class Source:\n    def __init__(self, source):\n        self.string = source\n\ndef adapt_source(source):\n     quoted = psycopg2.extensions.adapt(source.string).getquoted()\n     return psycopg2.extensions.AsIs(f\"{quoted}::data_source'\")\n\npsycopg2.extensions.register_adapter(Source, adapt_source)\n\nfoo = \"my_text\"\nsources = [Source(\"ftp\"), Source(\"http\")]\n\ncursor.execute(\n    \"\"\"\n        INSERT INTO data(foo, sources)\n        VALUES (%s, %s)\n    \"\"\",\n    (foo, sources),\n)\n`\nbut this code ends up with:\n```\n`psycopg2.errors.SyntaxError: syntax error at or near \"\"'ftp'\"\"\nLINE 3: ...my text', (b\"'ftp'\"::...\n                       ^\n`\n```\nI guess the problem is in `AsIs` function which combines bytes from `getquoted` function and formatted string.\nCan anybody helps me or point me to any solution?\nThanks",
      "solution": "Extending the Answer of Adrian Klaver, you also need to cast to the database type `data_source` you defined in schema.\n`cur.execute(\n    \"\"\"\n        INSERT INTO data(foo, sources)\n        VALUES (%s, %s::data_source[])\n    \"\"\",\n    (foo, sources),\n)\ncon.commit()\n`\nThis works for me.",
      "question_score": 3,
      "answer_score": 2,
      "created_at": "2021-10-18T23:52:56",
      "url": "https://stackoverflow.com/questions/69622944/apply-type-cast-to-items-of-array-in-parameter-with-psycopg2"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 69254037,
      "title": "Python psycog2 | Use a tuple or array in where clause",
      "problem": "I have a tuple like as shown below\n`vilist = (1,2,3,4)\n`\nI am trying to use them in Psycopg2 query like as shown below\n`sql = \"select * from temp.table1 where ids in {}\"\ncur.execute(sql,vilist)\n`\nWhich should parse a SQL string like\n`SELECT * FROM temp.table1 WHERE ids IDS (1,2,3,4);\n`\nHowever, I get an error like as shown below\n`SyntaxError                               Traceback (most recent call last)\n in \n      1 sql = \"select * from temp.table1 where ids in {}\"\n----> 2 cur.execute(sql,vilist)\n\nSyntaxError: syntax error at or near \"{\"\nLINE 1: ...rom temp.table1 where ids in {}\n`\nCan help me resolve this error please.",
      "solution": "Use SQL string composition to pass identifiers or literals to a query text.\n`import psycopg2\nimport psycopg2.sql as sql\n# ...\n\nvilist = (1,2,3,4)\nquery = sql.SQL(\"select * from temp.table1 where ids in {}\").format(sql.Literal(vilist))\ncur.execute(query)\n`",
      "question_score": 3,
      "answer_score": 2,
      "created_at": "2021-09-20T13:56:27",
      "url": "https://stackoverflow.com/questions/69254037/python-psycog2-use-a-tuple-or-array-in-where-clause"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 69088111,
      "title": "Random database disconnects with Django and Postgresql in AWS",
      "problem": "I'm trying to get to the bottom of a problem with Django and database connection errors. At this point I'm after debugging tips as I think the symptoms are too non-specific.\nSome background - I've been using this stack, deployed in AWS for many years without issue:\n\nUbuntu (in this case 20.04 LTS)\nNginx\nUwsgi\nPostgresql (v12 in RDS - tried v13 but same errors)\n\nAn AWS load balancer sends traffic to the Ubuntu instance, which is handled by Nginx, which forwards on to Django (3.2.6) running in Uwsgi. Django connects to the database using psycopg2 (2.9.1). Normally this setup works perfectly for me.\nThe issue I have it that the database connection seems to be closing randomly. Django reports errors like this:\n```\n`Traceback (most recent call last):\n  [my code...]\n    for answer in q.select_related('entry__session__player'):\n  File \"/usr/local/lib/python3.8/dist-packages/django/db/models/query.py\", line 280, in __iter__\n    self._fetch_all()\n  File \"/usr/local/lib/python3.8/dist-packages/django/db/models/query.py\", line 1324, in _fetch_all\n    self._result_cache = list(self._iterable_class(self))\n  File \"/usr/local/lib/python3.8/dist-packages/django/db/models/query.py\", line 51, in __iter__\n    results = compiler.execute_sql(chunked_fetch=self.chunked_fetch, chunk_size=self.chunk_size)\n  File \"/usr/local/lib/python3.8/dist-packages/django/db/models/sql/compiler.py\", line 1173, in execute_sql\n    cursor = self.connection.cursor()\n  File \"/usr/local/lib/python3.8/dist-packages/django/utils/asyncio.py\", line 26, in inner\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.8/dist-packages/django/db/backends/base/base.py\", line 259, in cursor\n    return self._cursor()\n  File \"/usr/local/lib/python3.8/dist-packages/django/db/backends/base/base.py\", line 237, in _cursor\n    return self._prepare_cursor(self.create_cursor(name))\n  File \"/usr/local/lib/python3.8/dist-packages/django/db/utils.py\", line 90, in __exit__\n    raise dj_exc_value.with_traceback(traceback) from exc_value\n  File \"/usr/local/lib/python3.8/dist-packages/django/db/backends/base/base.py\", line 237, in _cursor\n    return self._prepare_cursor(self.create_cursor(name))\n  File \"/usr/local/lib/python3.8/dist-packages/django/utils/asyncio.py\", line 26, in inner\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.8/dist-packages/django/db/backends/postgresql/base.py\", line 236, in create_cursor\n    cursor = self.connection.cursor()\ndjango.db.utils.InterfaceError: connection already closed\n`\n```\nThe location in my code varies. Sometimes (less frequently) I get this too:\n```\n`Traceback (most recent call last):\n  [my code...]\n    group = contest.groups.create(restaurant = restaurant, supergroup = supergroup)\n  File \"/usr/local/lib/python3.8/dist-packages/django/db/models/fields/related_descriptors.py\", line 677, in create\n    return super(RelatedManager, self.db_manager(db)).create(**kwargs)\n  File \"/usr/local/lib/python3.8/dist-packages/django/db/models/manager.py\", line 85, in manager_method\n    return getattr(self.get_queryset(), name)(*args, **kwargs)\n  File \"/usr/local/lib/python3.8/dist-packages/django/db/models/query.py\", line 453, in create\n    obj.save(force_insert=True, using=self.db)\n  File \"/usr/local/lib/python3.8/dist-packages/django/db/models/base.py\", line 726, in save\n    self.save_base(using=using, force_insert=force_insert,\n  File \"/usr/local/lib/python3.8/dist-packages/django/db/models/base.py\", line 763, in save_base\n    updated = self._save_table(\n  File \"/usr/local/lib/python3.8/dist-packages/django/db/models/base.py\", line 868, in _save_table\n    results = self._do_insert(cls._base_manager, using, fields, returning_fields, raw)\n  File \"/usr/local/lib/python3.8/dist-packages/django/db/models/base.py\", line 906, in _do_insert\n    return manager._insert(\n  File \"/usr/local/lib/python3.8/dist-packages/django/db/models/manager.py\", line 85, in manager_method\n    return getattr(self.get_queryset(), name)(*args, **kwargs)\n  File \"/usr/local/lib/python3.8/dist-packages/django/db/models/query.py\", line 1270, in _insert\n    return query.get_compiler(using=using).execute_sql(returning_fields)\n  File \"/usr/local/lib/python3.8/dist-packages/django/db/models/sql/compiler.py\", line 1416, in execute_sql\n    cursor.execute(sql, params)\n  File \"/usr/local/lib/python3.8/dist-packages/django/db/backends/utils.py\", line 66, in execute\n    return self._execute_with_wrappers(sql, params, many=False, executor=self._execute)\n  File \"/usr/local/lib/python3.8/dist-packages/django/db/backends/utils.py\", line 75, in _execute_with_wrappers\n    return executor(sql, params, many, context)\n  File \"/usr/local/lib/python3.8/dist-packages/django/db/backends/utils.py\", line 78, in _execute\n    self.db.validate_no_broken_transaction()\n  File \"/usr/local/lib/python3.8/dist-packages/django/db/backends/base/base.py\", line 447, in validate_no_broken_transaction\n    raise TransactionManagementError(\ndjango.db.transaction.TransactionManagementError: An error occurred in the current transaction. You can't execute queries until the end of the 'atomic' block.\n`\n```\nAgain, the location in my code varies, and it's not always in a simple create call - sometimes it's bulk_create, sometimes get_or_create. I'm guessing that the underlying cause is probably the same as the 'connection_closed' error but I'm not sure.\nSo that's what Django tells me. The Postgresql log doesn't contain any errors that coincide in time with the errors reported by Django. The only errors in the log are of this form:\n```\n`LOG: could not receive data from client: Connection reset by peer\n`\n```\nThese coincide with uwsgi killing off worker processes (I have a 1000 request limit set for each worker to avoid any potential memory leak issues), so they aren't related.\nSo Postgresql is reporting no relevant errors - I can only assume that the connection was closed properly, and Django wasn't expecting that. There are no errors in the systemd journal at all on the Ubuntu instance.\nI'm unsure how to proceed. I doubt this is a bug in Django but no other component in the system is complaining and it must be a low-level issue. This happens quite rarely, but enough to be a concern - something like 1 in 1000 requests.\nAny insights or suggestions of how to investigate this further would be gratefully accepted :)",
      "solution": "I've figured this out. It's pretty wild, but makes sense.\nFundamentally, the HTTP client is disconnecting early, while the request is still being processed.\nuWSGI must be handling the disconnect in a separate thread than the request handler - this is fine because it acquires the Python GIL before calling into Python.\nA totally unexpected (to me anyways!) consequence of this is that the close call is simply injected into the request handler's call stack - the request handler code is running, then suddenly execution switches to the HttpResponse close code. I observed this in stack traces during debugging - see this question for the details.\nAs part of Django's end-of-request handling, the DB connection may be closed if Django thinks it's broken - this happens because an unexpected transaction is in effect - one started by my own code above the injected `HttpResponse.close()` call.\nWhen the `HttpReponse.close()` call finishes running and the interpreter returns to my code, the transaction error is raised because the DB connection has been closed.\nSo really I think this is all as it should be, more or less. It would be nice to have a more specific 'premature close' error rather than the more generic \"connection already closed\" error, but I'm not sure how you would neatly construct such a thing.",
      "question_score": 3,
      "answer_score": 2,
      "created_at": "2021-09-07T14:18:11",
      "url": "https://stackoverflow.com/questions/69088111/random-database-disconnects-with-django-and-postgresql-in-aws"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 67790537,
      "title": "Postgres `WITH ins AS ...` casting everything as text",
      "problem": "I have this table in postgres\n```\n`CREATE TABLE target (\n    a json\n    b integer\n    c text []\n    id integer\n    CONSTRAINT id_fkey FOREIGN KEY (id)\n        REFERENCES public.other_table(id) MATCH SIMPLE\n        ON UPDATE NO ACTION\n        ON DELETE NO ACTION,\n)\n`\n```\nWhich I would like to insert data to from psycopg2 using\n```\n`import psycopg2\nimport psycopg2.extras as extras\n\n# data is of the form dict, integer, list(string), string When I run this as is I get the error: `column \"a\" is of type json but expression is of type text`\nI tried to solve this by adding a type cast in the SELECT statement\n```\n`         'SELECT '\n            'ins.a::json '\n            'ins.b '\n            'ins.c '\n            'other_table.id'\n`\n```\nbut then I get the error `column \"c\" is of type text[] but expression is of type text`\nSo I fixed that in the same way:\n```\n`         'SELECT '\n            'ins.a::json '\n            'ins.b '\n            'ins.c::text[]'\n            'other_table.id'\n`\n```\nso now I am getting the error `column \"b\" is of type integer but expression is of type text`\nThis example is somewhat simplified as I have many more columns in the original query.\n\nDoes the `WITH ins ...` statement always convert everything to text? This seems like an odd behavior to me\nIs there a way to code this without having to manually typecast every column? This seems inelegant and computationally inefficient data gets converted eg. from python int to postgres text to postgres integer.",
      "solution": "The problem isn't with the `CTE`, but with how you're passing values into the `VALUES` clause. Somehow all values created inside of the `CTE` at `VALUES` are being sent as text (perhaps the query is created with all values between single quotes?). The following example reproduces your query with pure SQL statements, and it works as it should:\n```\n`WITH ins (a, b, c, id) AS (\n  VALUES ('{\"answer\":42}'::json,42,array['foo','bar'],1)\n) \nINSERT INTO target (a,b,c,id)\nSELECT ins.a,ins.b,ins.c,other_table.id \nFROM ins \nLEFT JOIN other_table ON ins.id = other_table.id;\n`\n```\nNote that I cast the json the value inside of the `CTE`, not in the `SELECT` clause. So, if the origin is correct, there is no way that postgres will cast it to text without you telling it to do so ;)\nDemo: `db<>fiddle`",
      "question_score": 3,
      "answer_score": 2,
      "created_at": "2021-06-01T16:17:10",
      "url": "https://stackoverflow.com/questions/67790537/postgres-with-ins-as-casting-everything-as-text"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 66768703,
      "title": "psycopg2 prepared delete statement",
      "problem": "I am struggling with generating the delete query where parameters for the query is actually a set of values.\nSo I need to delete rows where parameters are a pair values for example:\n`delete from table where col1 = %s and col2 = %s`\nwhich can be executed in Python like:\n```\n`cur = conn.cursor()\ncur.execute(query, (col1_value, col2_value))\n`\n```\nNow I would like to run a query:\n```\n`delete from table where (col1, col2) in ( (col1_value1, col2_value1), (col1_value2, col2_value2) );\n`\n```\nI can generate the queries and values and execute the exact SQL but I can't quite generate prepared statement.\nI tried:\n```\n`delete from table where (col1, col2) in %s\n`\n```\nand\n```\n`delete from table where (col1, col2) in (%s)\n`\n```\nBut when I try to execute:\n```\n`cur.execute(query, list_of_col_value_tuples)\n`\n```\nor\n```\n`cur.execute(query, tuple_of_col_value_tuples)\n`\n```\nI get an exception that indicates that psycopg2 cannot convert arguments to strings.\nIs there any way to use psycopg2 to execute a query like this?",
      "solution": "Actually the resolution is quite easy if carefully constructed.\nIn the miscellaneous goodies of psycopg2 there is a function `execute_values`.\nWhile all the examples that are given by psycopg2 deal with inserts as the function basically converts the list of arguments into a `VALUES` list if the call to delete is formatted like so:\n```\n`qry = \"delete from table where (col1, col2) in (%s)\"\n`\n```\nThe call:\n```\n`execute_values(cur=cur, qry=qry, argslist=)\n`\n```\nwill make the delete perform exactly as required.",
      "question_score": 3,
      "answer_score": 1,
      "created_at": "2021-03-23T18:58:50",
      "url": "https://stackoverflow.com/questions/66768703/psycopg2-prepared-delete-statement"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 79474998,
      "title": "python psycopg cursor : str is not assignable to LiteralString",
      "problem": "I want to switch from synchronous db calls to asynchronous db calls.\nI don't want to change my query builder. My query builder outputs a sql query as a string (actually in reality it outputs a query with '%s' to escape parameters and the escaped parameters in a list).\nThis works great in psycopg2 but does not seem to work in psycopg since the cursor does not want to take an \"str\" as the query (quite annoying).\nHow can I make it work ?\n`import asyncio\nimport psycopg\n\nclass MyQueryBuilderObj:\n    def getFinalQuery(self) -> str:\n        return \"SELECT * FROM test_table\"\n\nasync def fetch_data_from_db():\n    qb_obj = MyQueryBuilderObj()\n    async with await psycopg.AsyncConnection.connect(\"dbname=test user=postgres password=yourpassword host=localhost\") as aconn:\n        async with aconn.cursor() as acur:\n            await acur.execute(qb_obj.getFinalQuery())\n            result = await acur.fetchone()\n            return result\n\nasync def main():\n    try:\n        result = await fetch_data_from_db()\n        print(\"Fetched data:\", result)\n    except Exception as e:\n        print(\"An error occurred:\", e)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n`\nIssue at \"acur.execute\" :\n```\n`Argument of type \"str\" cannot be assigned to parameter \"query\" of type \"Query\" in function \"execute\"\n  Type \"str\" is not assignable to type \"Query\"\n    \"str\" is not assignable to \"LiteralString\"\n    \"str\" is not assignable to \"bytes\"\n    \"str\" is not assignable to \"SQL\"\n    \"str\" is not assignable to \"Composed\"\n`\n```\nVersion :\n\npsycopg 3.2.5\npython 3.12.5",
      "solution": "I encountered the same issue when building dynamic queries.\nThe problem came from type conversion. The type checker won't like it, but it will work.\n`def select_row(row_name: str):\n    query_template = \"SELECT {} FROM pg_catalog.pg_tables\"\n    query = query_template.format(row_name)\n    with psycopg.connect(**connection) as conn:\n        with conn.cursor() as cur:\n            cur.execute(query)\n            result = cur.fetchone()\n\nselect_row('table'+'name')\n`\nA simple change of type from `str` to `LiteralString` made the problem go away.\nAnother solution is type casting, where you explicitly give the function what it wants.\n`import typing\nquery = typing.cast(typing.LiteralString, query)\n`\n\nThe other answer did not work as SQL didn't liked string or even casted to it's own psycopg Literal type. (...type \"str\"/\"Literal\" cannot be assigned to parameter of type \"LiteralString\"...)\n`from psycopg.sql import Literal,SQL\nquery = Literal(query)\nSQL(query)\n`",
      "question_score": 3,
      "answer_score": 1,
      "created_at": "2025-02-28T10:08:37",
      "url": "https://stackoverflow.com/questions/79474998/python-psycopg-cursor-str-is-not-assignable-to-literalstring"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 75345086,
      "title": "Pandas DataFrame.to_sql() doesn&#39;t work anymore with an sqlalchemy 2.0.1 engine.connect() as a context manager and doesn&#39;t throw any error",
      "problem": "This code with pandas `1.5.3` and sqlalchemy `2.0.1` is not working anymore and surprisingly, it doesn't raises any error, the code passes silently:\n`# python 3.10.6\nimport pandas as pd # 1.5.3\nimport psycopg2 # '2.9.5 (dt dec pq3 ext lo64)'\nfrom sqlalchemy import create_engine # 2.0.1\n\ndef connector():\n    return psycopg2.connect(**DB_PARAMS)\n\nengine = create_engine('postgresql+psycopg2://', creator=connector)\n\nwith engine.connect() as connection:\n    df.to_sql(\n        name='my_table',\n        con=connection,\n        if_exists='replace',\n        index=False,\n    )\n`\nCurrently, with sqlalchemy `2.0.1` my table is no more populated with the DataFrame content.\nWhereas it was correctly populated with sqlalchemy version `1.4.45`.\nEdit\nApparently, it works when I don't use a context manager:\n`connection = engine.connect()\n\nres.to_sql(\n    name='my_table',\n    con=connection,\n    if_exists='replace',\n    index=False\n)\nOut[2]: 133 # \nHow could I get it to work with a context manager (aka a `with` statement)?",
      "solution": "The context manager that you are using rolls back on exit. Instead, use `engine.begin()`, which will commit.\n`with engine.begin() as connection:\n    df.to_sql(\n        name='my_table',\n        con=connection,\n        if_exists='replace',\n        index=False,\n    )\n`",
      "question_score": 2,
      "answer_score": 13,
      "created_at": "2023-02-04T13:27:23",
      "url": "https://stackoverflow.com/questions/75345086/pandas-dataframe-to-sql-doesnt-work-anymore-with-an-sqlalchemy-2-0-1-engine-c"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 69872948,
      "title": "async SQLAlchemy can&#39;t create engine",
      "problem": "I've made a small app, that uses `SQLAlchemy` to handle connection with postgresql database. Now I want to rewrite it using asincio. For some reason, when I run it, i get the following error:\n```\n`Traceback (most recent call last):\n  File \"D:\\Space\\discord_count_bot\\bot\\bot\\main.py\", line 12, in \n    dbConnection.init_connection(\n  File \"D:\\Space\\discord_count_bot\\bot\\bot\\db_hanler.py\", line 78, in init_connection\n    engine = create_async_engine(connection_string, future=True, echo=True)\n  File \"D:\\Space\\discord_count_bot\\bot_env\\lib\\site-packages\\sqlalchemy\\ext\\asyncio\\engine.py\", line 40, in create_async_engine\n    sync_engine = _create_engine(*arg, **kw)\n  File \"\", line 2, in create_engine\n  File \"D:\\Space\\discord_count_bot\\bot_env\\lib\\site-packages\\sqlalchemy\\util\\deprecations.py\", line 298, in warned\n    return fn(*args, **kwargs)\n  File \"D:\\Space\\discord_count_bot\\bot_env\\lib\\site-packages\\sqlalchemy\\engine\\create.py\", line 560, in create_engine\n    dbapi = dialect_cls.dbapi(**dbapi_args)\n  File \"D:\\Space\\discord_count_bot\\bot_env\\lib\\site-packages\\sqlalchemy\\dialects\\postgresql\\psycopg2.py\", line 782, in dbapi\n    import psycopg2\nModuleNotFoundError: No module named 'psycopg2'\n`\n```\nAnd if `psycopg2` is installed, I get\n```\n`Traceback (most recent call last):\n  File \"D:\\Space\\discord_count_bot\\bot\\bot\\main.py\", line 12, in \n    dbConnection.init_connection(\n  File \"D:\\Space\\discord_count_bot\\bot\\bot\\db_hanler.py\", line 78, in init_connection\n    engine = create_async_engine(connection_string, future=True, echo=True)\n  File \"D:\\Space\\discord_count_bot\\bot_env\\lib\\site-packages\\sqlalchemy\\ext\\asyncio\\engine.py\", line 41, in create_async_engine\n    return AsyncEngine(sync_engine)\n  File \"D:\\Space\\discord_count_bot\\bot_env\\lib\\site-packages\\sqlalchemy\\ext\\asyncio\\engine.py\", line 598, in __init__\n    raise exc.InvalidRequestError(\nsqlalchemy.exc.InvalidRequestError: The asyncio extension requires an async driver to be used. The loaded 'psycopg2' is not async. \n`\n```\nI got `asyncpg` installed, I guess, I need to specificaly tell `SQLAlchemy` to use it. Or maybe, there is something in my code, that makes `SQLAlchemy` think, it should use `psycopg2`... I cant find anything about it, in every tutorial I've encountered everething seems to work just fine.\n`\nfrom datetime import datetime, timedelta\nimport logging\n\nfrom sqlalchemy.ext.asyncio import create_async_engine, AsyncSession\nfrom sqlalchemy.ext.declarative import declarative_base\nfrom sqlalchemy.orm import sessionmaker\nfrom sqlalchemy import select, and_\nfrom sqlalchemy import Column, Integer, String, DateTime, Boolean\n\nlogger = logging.getLogger('discord')\nBase = declarative_base()\n\nclass TaskModel(Base):\n    \"\"\"Counting task model for database.\"\"\"\n\n    __tablename__ = 'tasks'\n\n    id = Column(Integer, primary_key=True)\n    author = Column(String(200))\n    channel_id = Column(Integer)\n    is_dm = Column(Boolean)\n    start_time = Column(DateTime)\n    end_time = Column(DateTime)\n    count = Column(Integer)\n    canceled = Column(Boolean)\n\nclass DBConnection:\n    \"\"\"Class handles all the db operations.\"\"\"\n\n    def __init__(self):\n        \"\"\"Create new uninitialized handler.\"\"\"\n        self._session: AsyncSession = None\n\n    def init_connection(self, user, password, host, port, db):\n        \"\"\"Connect to actual database.\"\"\"\n        connection_string = \"postgresql://{}:{}@{}:{}/{}\".format(\n            user, password, host, port, db\n        )\n        engine = create_async_engine(connection_string, future=True, echo=True)\n        self._session = sessionmaker(engine, expire_on_commit=False, class_=AsyncSession)\n\n    async def add_task(self, author, channel_id, count, is_dm):\n        \"\"\"Add new task to db.\"\"\"\n        now = datetime.utcnow()\n        task = TaskModel(\n            author=author,\n            channel_id=channel_id,\n            is_dm=is_dm,\n            start_time=now,\n            end_time=now + timedelta(seconds=count),\n            count=count,\n            canceled=False\n        )\n        self._session.add(task)\n        await self._session.commit()\n        logger.info(f\"task added to db: {task}\")\n        return task\n\n    async def get_active_tasks(self):\n        \"\"\"Get all active tasks.\"\"\"\n        now = datetime.utcnow()\n        async with self._session() as session:\n            query = select(TaskModel).where(and_(\n                    TaskModel.end_time > now,\n                    TaskModel.canceled == False\n            ))\n            result = await session.execute(query)\n            return result.fetchall()\n\ndbConnection = DBConnection()\n\n`",
      "solution": "As Gord Thompson said, I needed to be more specific in my connection string, ` postgresql+asyncpg://\u2026` did the trick, thank you)",
      "question_score": 2,
      "answer_score": 11,
      "created_at": "2021-11-07T14:39:19",
      "url": "https://stackoverflow.com/questions/69872948/async-sqlalchemy-cant-create-engine"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 71652499,
      "title": "Can&#39;t install psycopg2-binary on macOS Monterey 12.3",
      "problem": "I've been trying to install psycopg2-binary for my django project, and so far nothing has been working. It keeps asking for the `pg_config` file, which to my knowledge is only required if you are building psycopg2. So, what is happening here?\n```\n`Collecting psycopg2-binary\n  Using cached psycopg2-binary-2.9.3.tar.gz (380 kB)\n  Preparing metadata (setup.py) ... error\n  error: subprocess-exited-with-error\n  \n  \u00d7 python setup.py egg_info did not run successfully.\n  \u2502 exit code: 1\n  \u2570\u2500> [23 lines of output]\n      running egg_info\n      creating /private/var/folders/7t/15s08lqd0r51727h4ljk1bq00000gn/T/pip-pip-egg-info-03hv65aq/psycopg2_binary.egg-info\n      writing /private/var/folders/7t/15s08lqd0r51727h4ljk1bq00000gn/T/pip-pip-egg-info-03hv65aq/psycopg2_binary.egg-info/PKG-INFO\n      writing dependency_links to /private/var/folders/7t/15s08lqd0r51727h4ljk1bq00000gn/T/pip-pip-egg-info-03hv65aq/psycopg2_binary.egg-info/dependency_links.txt\n      writing top-level names to /private/var/folders/7t/15s08lqd0r51727h4ljk1bq00000gn/T/pip-pip-egg-info-03hv65aq/psycopg2_binary.egg-info/top_level.txt\n      writing manifest file '/private/var/folders/7t/15s08lqd0r51727h4ljk1bq00000gn/T/pip-pip-egg-info-03hv65aq/psycopg2_binary.egg-info/SOURCES.txt'\n      \n      Error: pg_config executable not found.\n      \n      pg_config is required to build psycopg2 from source.  Please add the directory\n      containing pg_config to the $PATH or specify the full executable path with the\n      option:\n      \n          python setup.py build_ext --pg-config /path/to/pg_config build ...\n      \n      or with the pg_config option in 'setup.cfg'.\n      \n      If you prefer to avoid building psycopg2 from source, please install the PyPI\n      'psycopg2-binary' package instead.\n      \n      For further information please check the 'doc/src/install.rst' file (also at\n      ).\n      \n      [end of output]\n`\n```\nOf course, pip is at its latest version, and I've specified `pip install psycopg2-binary`.\nMy mac's arch is arm64.\nAny help would be appreciated. Thanks",
      "solution": "`psycopg2-binary` 2.9.3 doesn't provide binary wheels for MacOS arm64; binary wheels for MacOS are only provided for Intel x86 64 bits. So `pip` tried to compile from sources \u2014 and failed.\nTo compile from sources you need a lot of prerequisites.",
      "question_score": 2,
      "answer_score": 4,
      "created_at": "2022-03-28T21:05:58",
      "url": "https://stackoverflow.com/questions/71652499/cant-install-psycopg2-binary-on-macos-monterey-12-3"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 73042760,
      "title": "pip install psycopg2 on MacOS M1 and python 3.10.5 not working",
      "problem": "psycopg2 is not working on M1.  Did anyone successfully install?\n```\n`  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/psycopg2/__init__.py\", line 51, in \n    from psycopg2._psycopg import (                     # noqa\nImportError: dlopen(/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/psycopg2/_psycopg.cpython-310-darwin.so, 0x0002): symbol not found in flat namespace '_PQbackendPID'\n`\n```\nI've tried installing:\n\npsycopg2\npsycopg2 and psycopg2-binary\npsycopg2-binary alone\n\nNothing seems to work.  I'm using python3.10.5\nMany Thanks!",
      "solution": "Install `libpq` and `openssl` from brew:\n\n`brew install libpq`\n`brew install openssl`\n\nCheck path and compiler flags:\n```\n`>>> brew info libpq\nlibpq: stable 14.4 (bottled) [keg-only]\nPostgres C API library\nhttps://www.postgresql.org/docs/14/libpq.html\n/opt/homebrew/Cellar/libpq/14.4 (2,338 files, 28.3MB)\n  Poured from bottle on 2022-07-20 at 16:31:04\nFrom: https://github.com/Homebrew/homebrew-core/blob/HEAD/Formula/libpq.rb\nLicense: PostgreSQL\n==> Dependencies\nRequired: krb5 \u2714, openssl@1.1 \u2714\n==> Caveats\nlibpq is keg-only, which means it was not symlinked into /opt/homebrew,\nbecause conflicts with postgres formula.\n\nIf you need to have libpq first in your PATH, run:\n  echo 'export PATH=\"/opt/homebrew/opt/libpq/bin:$PATH\"' >> ~/.zshrc\n\nFor compilers to find libpq you may need to set:\n  export LDFLAGS=\"-L/opt/homebrew/opt/libpq/lib\"\n  export CPPFLAGS=\"-I/opt/homebrew/opt/libpq/include\"\n\n>>> brew info openssl\nopenssl@3: stable 3.0.5 (bottled) [keg-only]\nCryptography and SSL/TLS Toolkit\nhttps://openssl.org/\n/opt/homebrew/Cellar/openssl@3/3.0.5 (6,444 files, 27.9MB)\n  Poured from bottle on 2022-07-20 at 16:31:21\nFrom: https://github.com/Homebrew/homebrew-core/blob/HEAD/Formula/openssl@3.rb\nLicense: Apache-2.0\n==> Dependencies\nRequired: ca-certificates \u2714\n==> Caveats\nA CA file has been bootstrapped using certificates from the system\nkeychain. To add additional certificates, place .pem files in\n  /opt/homebrew/etc/openssl@3/certs\n\nand run\n  /opt/homebrew/opt/openssl@3/bin/c_rehash\n\nopenssl@3 is keg-only, which means it was not symlinked into /opt/homebrew,\nbecause macOS provides LibreSSL.\n\nIf you need to have openssl@3 first in your PATH, run:\n  echo 'export PATH=\"/opt/homebrew/opt/openssl@3/bin:$PATH\"' >> ~/.zshrc\n\nFor compilers to find openssl@3 you may need to set:\n  export LDFLAGS=\"-L/opt/homebrew/opt/openssl@3/lib\"\n  export CPPFLAGS=\"-I/opt/homebrew/opt/openssl@3/include\"\n`\n```\nExport `path` for libpq and export `LD/CPPFLAGS` for openssl (taken from output above) and install `psycopg2`:\n```\n`>>> export PATH=/opt/homebrew/opt/libpq/bin:$PATH\n>>> export LDFLAGS=\"-L/opt/homebrew/opt/openssl@3/lib\"\n>>> export CPPFLAGS=\"-I/opt/homebrew/opt/openssl@3/include\"\n>>> python -m pip install psycopg2\nCollecting psycopg2\n  Using cached psycopg2-2.9.3.tar.gz (380 kB)\n  Preparing metadata (setup.py) ... done\nBuilding wheels for collected packages: psycopg2\n  Building wheel for psycopg2 (setup.py) ... done\n  Created wheel for psycopg2: filename=psycopg2-2.9.3-cp310-cp310-macosx_12_0_arm64.whl size=142252 sha256=78ca9fc7ca6752e234904bf38d052937e20b063cb68eb67caa874511207e076e\n  Stored in directory: /Users/.../Library/Caches/pip/wheels/81/b6/3d/091aad3e8919ea76c84c2674b02ce3ab52de882e091c39249e\nSuccessfully built psycopg2\nInstalling collected packages: psycopg2\nSuccessfully installed psycopg2-2.9.3\n`\n```\nTest:\n```\n`python\nPython 3.10.5 (main, Jul 20 2022, 17:05:05) [Clang 13.0.0 (clang-1300.0.27.3)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import psycopg2\n>>> c = psycopg2.connect(\"port=5556 host=localhost ...\")\n>>> c\n\n>>> exit()\n`\n```",
      "question_score": 2,
      "answer_score": 6,
      "created_at": "2022-07-19T21:52:14",
      "url": "https://stackoverflow.com/questions/73042760/pip-install-psycopg2-on-macos-m1-and-python-3-10-5-not-working"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 77184491,
      "title": "Psycopg2: how to deal with special characters in password?",
      "problem": "I am trying to connect to a db instance, but my password has the following special characters: backslash, plus, dot, asterisk/star and at symbol. For example, 12@34\\56.78*90 (regex nightmare lol)\nHow do I safe pass it to the connection string? My code looks like that:\n```\n`connection_string = f'user={user} password={pass} host={host} dbname={dbname} port={port}'\n\nconnection = psg2.connect(connection_string)\n`\n```\nIt gives me wrong pass/username error. However, I tried this combination directly on the db and it works, and I tried another combination on the python code and it worked as well. So looks like the problem is the password being passed weirdly to the connection.\nI tried urllib scape, I tried double quotes on the password, nothing works so far :(",
      "solution": "Based on a reddit thread, I found out that passing variable by variable directly instead of a connection string did the trick:\ncon = psycopg2.connect(\ndbname=dn,\nuser=du,\npassword=dp,\nhost=dh,\nport=dbp,\n)",
      "question_score": 2,
      "answer_score": 1,
      "created_at": "2023-09-27T05:15:06",
      "url": "https://stackoverflow.com/questions/77184491/psycopg2-how-to-deal-with-special-characters-in-password"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 72663734,
      "title": "Can&#39;t acces a postgresql table",
      "problem": "Simple queries run for hours, but only for one table, what can be the cause?\nI tried deleting it, obviously it got stalled too, and I can't do any other basic or complex queries for that table. I am at digitalocean, so I can't just do a restart on the DB.\nI imported this table from a URL with python and I forgot to close the psycopg2 connection before running the script so I had to terminate it, and maybe that could have caused the problem.\nBTW. this is my personal database, I don't have other admins who could have locked me out or anything.\nI also looked at the logs, but there isn't any sign of unusual activities.\nHere is it though, originally its in German, I had to translate it.\n09:18:35.644: PostgreSQL 14.2, compiled by Visual C++ build 1914, 64-bit launches\n09:18:35.645: expected connections on IPv6 address \"::\", port 25060\n09:18:35.645: expecting connections on IPv4 address \"0.0.0.0\", port 25060\n09:18:35.691: database system was interrupted; last known action on 2022-06-16 23:54:18 CEST\n09:18:36.137: database system was not shut down properly; automatic\nrecovery in progress\n09:18:36.140: Redo starts at 0/172B560\n09:18:36.141: invalid record length at 0/172B598: 24 expected, 0 received\n09:18:36.141: redo complete at 0/172B560 System usage: CPU: user: 0.00 s, system: 0.00 s,\nelapsed: 0.00 s\n09:18:36.164: database system is ready to accept connections\n20:34:07.203: fast shutdown requested\n20:34:07.208: any active transactions are aborted\n20:34:07.232: background worker \"logical replication launcher\" (PID 15088) terminated with status 1\n20:34:07.241: shut down\n20:34:07.292: database system shut down",
      "solution": "You probably have an orphaned idle process that has acquired a lock on the table which has not been released due to an uncommitted transaction -and will never be released since you have no ability to commit it.\nIf simple queries like `SELECT * FROM table` are blocking, you most likely have an `Access Exclusive Lock` on the table from something like an `ALTER`/`DROP` statement that had run in the script that you cancelled.\nYou can verify this by running:\n`SELECT\n    psa.pid,\n    psa.query,\n    psa.state,\n    pg_locks.mode lock_mode,\n    relation::regclass locked_relation\n\nFROM\n  pg_locks\n  JOIN pg_stat_activity psa on pg_locks.pid = psa.pid\n\nWHERE\n  granted\n  and 'my_schema.my_table'::regclass = relation\n`\nThen you can kill any process that has a lock on your table:\n`SELECT\n    pg_terminate_backend(psa.pid),\n\nFROM\n  pg_locks\n  JOIN pg_stat_activity psa on pg_locks.pid = psa.pid\n\nWHERE\n  granted\n  and 'my_schema.my_table'::regclass = relation\n`\nIf you still have blocked queries, you can open a new connection while the blocked query is running, and execute the following:\n`SELECT\n    activity.pid pid,\n    activity.state state,\n    activity.query blocked_query,\n    blocking_pid,\n    blocking_activity.state blocking_state,\n    blocking_activity.query blocking_query,\n    locks.locked_relations blocking_relation_locks\nFROM\n pg_stat_activity activity\n JOIN LATERAL unnest(pg_blocking_pids(activity.pid)) blocking_pids(blocking_pid) ON TRUE\n JOIN pg_stat_activity blocking_activity ON (blocking_pids.blocking_pid = blocking_activity.pid)\n JOIN LATERAL (\n     SELECT\n        string_agg(locks.relation::regclass::text,',') locked_relations\n     FROM\n\n       pg_locks locks\n\n     WHERE\n         locks.pid = blocking_pid\n\n     GROUP BY\n       blocking_pid\n) locks ON TRUE;\n`\nThis will show you every process that is blocking another process. Once you find the offending processes you can exterminate them using `pg_terminate_backend(pid)`\nIf you want to kill every pid blocking another pid, you can run:\n`SELECT \n  pg_terminate_backend(unnest(pg_blocking_pids(pid)))\nFROM\n  pg_stat_activity \n`\nIf don't have other critical processes running and want to eliminate every single database connection for the sake of simplicity, do:\n`SELECT\n  pg_terminate_backend(pg_stat_activity.pid)\nFROM\n  pg_stat_activity\nWHERE\n  --delete this line if you want to kill all connections on the whole cluster\n  pg_stat_activity.datname = ''\n  AND pid <> pg_backend_pid();\n`\nOnce complete, you'll only have the connection on which you executed this query.\nIf none of the above steps work it's possible -as Laurenz Albe pointed out in the comments -that you may have a prepared transaction that is locking your table. The locks that a prepared transaction creates can survive backend terminations -and even server restarts. They must be explicitly rolled back or committed.\nYou can check if a prepared transaction is locking any of your tables with the following query\n`SELECT\n  ptx.transaction prep_tx_id,\n  ptx.gid prep_tx_name,\n  locks.mode lock_mode,\n  locks.relation::regclass locked_relation\nFROM\n  pg_prepared_xacts ptx\n  JOIN pg_locks tx ON ptx.transaction = tx.transactionid and tx.locktype = 'transactionid'\n  JOIN pg_locks locks ON tx.virtualtransaction = locks.virtualtransaction\n\nWHERE\n  locks.granted\n  and locks.relation = 'my_schema.my_table'::regclass\n`\nYou can then rollback the prepared transaction like:\n`ROLLBACK PREPARED '' \n`",
      "question_score": 2,
      "answer_score": 5,
      "created_at": "2022-06-17T20:59:22",
      "url": "https://stackoverflow.com/questions/72663734/cant-acces-a-postgresql-table"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 70187284,
      "title": "Psycopg2 not auto generating id when using copy_from a csv file to Postgres db",
      "problem": "I have a csv file that has several columns:\n`upc    date    quantity   customer`\nIn my `physical` table, I have an auto generating `id` column for each row:\n`id    upc    date    quantity    customer`\nIt seems as though the db is interpreting the `upc` as the actual id when I run my python script to copy into the db. I'm getting this error message:\n```\n`Error: value \"1111111\" is out of range for type integer\nCONTEXT:  COPY physical, line 1, column id: \"1111111\"\n`\n```\nI've never attempted this before, but I believe this is correct:\n```\n`def insert_csv(f, table):\n    connection = get_postgres_connection()\n    cursor = connection.cursor()\n    try:\n        cursor.copy_from(f, table, sep=',')\n        connection.commit()\n        return True\n    except (psycopg2.Error) as e:\n        print(e)\n        return False\n    finally:\n        cursor.close()\n        connection.close()\n`\n```\nAm I doing something wrong here, or do I have to create another script to get the last id from the table?\nUpdated working code:\n```\n`def insert_csv(f, table, columns):\n    connection = get_postgres_connection()\n    cursor = connection.cursor()\n    try:\n        column_names = ','.join(columns)\n        query = f'''\n            COPY {table}({column_names})\n            FROM STDOUT (FORMAT CSV)\n        '''\n        cursor.copy_expert(query, f)\n        connection.commit()\n        return True\n    except (psycopg2.Error) as e:\n        print(e)\n        return False\n    finally:\n        cursor.close()\n        connection.close()\n`\n```\n```\n`columns = (\n        \"upc\",\n        \"date_thru\",\n        \"transaction_type\",\n        \"transaction_type_subtype\",\n        \"country_code\",\n        \"customer\",\n        \"quantity\",\n        \"income_gross\",\n        \"fm_serial\",\n        \"date_usage\"\n    )\n`\n```\n```\n`with open(dump_file, 'r', newline='', encoding=\"ISO-8859-1\") as f:\n        inserted = insert_csv(f, 'physical', columns)\n`\n```",
      "solution": "You need to specify columns to import. From the documentation:\n\ncolumns \u2013 iterable with name of the columns to import. The length and types should match the content of the file to read. If not specified, it is assumed that the entire table matches the file structure.\n\nYour code may look like this:\n`def insert_csv(f, table, columns):\n    connection = connect()\n    cursor = connection.cursor()\n    try:\n        cursor.copy_from(f, table, sep=',', columns=columns)\n        connection.commit()\n        return True\n    except (psycopg2.Error) as e:\n        print(e)\n        return False\n    finally:\n        cursor.close()\n        connection.close()\n        \nwith open(\"path_to_my_csv\") as file:\n    insert_csv(file, \"my_table\", (\"upc\", \"date\", \"quantity\", \"customer\"))\n`\nIf you have to use `copy_expert()` modify your function in the way as follow:\n`def insert_csv(f, table, columns):\n    connection = connect()\n    cursor = connection.cursor()\n    try:\n        column_names = ','.join(columns)\n        copy_cmd = f\"copy {table}({column_names}) from stdout (format csv)\"\n        cursor.copy_expert(copy_cmd, f)\n        connection.commit()\n        return True\n    except (psycopg2.Error) as e:\n        print(e)\n        return False\n    finally:\n        cursor.close()\n        connection.close()\n`",
      "question_score": 2,
      "answer_score": 4,
      "created_at": "2021-12-01T16:51:35",
      "url": "https://stackoverflow.com/questions/70187284/psycopg2-not-auto-generating-id-when-using-copy-from-a-csv-file-to-postgres-db"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 68535632,
      "title": "Why can&#39;t my airflow webserver initialize properly?",
      "problem": "When I create a dummy DAG following the Apache guide to airflow in Docker and run `docker-compose up`, the `webserver` container repeatedly fails and restarts with errors that look like this:\n```\n`webserver_1  | [2021-07-26 18:44:42,905] {cli_action_loggers.py:105} WARNING - Failed to log action with (psycopg2.errors.UndefinedTable) relation \"log\" does not exist\nwebserver_1  | LINE 1: INSERT INTO log (dttm, dag_id, task_id, event, execution_dat...\n`\n```\nThis error suggests to me that the webserver is unable to initialize the airflow db. This dummy DAG uses Airflow 2 and `CeleryExecutor`. I have run into the same issue following this example using Airflow 2 and `LocalExecutor`, as well as my team's current setup, which uses Airflow 1 and `CeleryExecutor`.\nWhat confuses me is that my teammates can run DAGs based on our docker-compose without issue (and presumably, others can run Apache's official quickstart successfully). So something is wrong with my machine (likely related to Postgres) that is somehow affecting my Docker images. What could cause this error across several different `docker-compose` airflow setups?\nWhat I have tried so far:\n\nRestarting my computer\nKilling, removing, and rebuilding my Docker images\nReinstalling Docker for Mac\nUninstalling and reinstalling `libpq`, `postgres`, and `psycopg2` on my local machine (though in theory this shouldn't matter for the Docker environment?)\n\nI am on Docker version 20.10.7, and docker-compose version 1.29.2.",
      "solution": "I was able to solve this problem by completely cleaning my computer of Docker artifacts using `docker system prune`. I ran the following commands:\n```\n`docker system prune --all\ndocker system prune --all --volumes\n`\n```\nMy previous attempts to clear Docker artifacts from my computer had been incomplete; killing and removing containers and uninstalling/reinstalling Docker was not sufficient.",
      "question_score": 2,
      "answer_score": 4,
      "created_at": "2021-07-26T21:47:36",
      "url": "https://stackoverflow.com/questions/68535632/why-cant-my-airflow-webserver-initialize-properly"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 67681689,
      "title": "psycopg2 sql.SQL: Composed elements must be Composable",
      "problem": "`import psycopg2\nfrom psycopg2 import sql\nimport datetime\n\ndef connect(conn):\n    \"\"\" Connect to the PostgreSQL database server \"\"\"\n    # create a cursor\n    cur = conn.cursor()\n\n    # https://www.psycopg.org/docs/sql.html\n\n    col_names = sql.SQL(', ').join([\n        sql.Identifier('lookup'),\n        sql.Identifier('LOL1'),\n        sql.Identifier('LOL2'),\n        sql.Identifier('identifier1'),\n        sql.Identifier('identifier2'),\n        sql.Identifier('timestamp_first_entry'),\n        sql.Identifier('timestamp_last_entry'),\n        sql.Identifier('some_number')\n    ]),\n\n    values = sql.SQL(', ').join([\n        \"hash\",\n        [  # LOL1\n            [[1.0, 2.0],  # list of lists of lists 1.1\n             [3.0, 0.5]],\n            [[-1.0, -2.0], # list of lists of lists 1.2\n             [-3.0, -4.0]]\n        ], [  # LOL2\n            [[1.0, 2.0],  # list of lists of lists 2.1\n             [3.0, 0.5]],\n            [[-1.0, -2.0], # list of lists of lists 2.2\n             [-3.0, -4.0]]\n        ],\n        \"identifier1\",\n        \"identifier2\",\n        datetime.datetime(2021, 5, 10),\n        datetime.datetime(2021, 5, 12),\n        20\n    ])\n\n    query_base = sql.SQL(\"insert into {table_name}({col_names} values ({values}))\".format(\n        table_name=sql.Identifier(\"raw_orderbook\"),\n        col_names=col_names,\n        values=values\n    ))\n\n    cur.execute(query_base)\n\n    cur.close()\n\n# https://www.psycopg.org/docs/sql.html\n\nif __name__ == '__main__':\n    conn = psycopg2.connect(\n        host=\"localhost\",\n        database=\"test\",\n        user=\"ian\",\n        password=\"\")\n\n    connect(conn)\n`\nresults in an error of\n`Traceback (most recent call last):\n  File \"/home/ian/PycharmProjects/panthera/src/database/tester.py\", line 73, in \n    connect(conn)\n  File \"/home/ian/PycharmProjects/panthera/src/database/tester.py\", line 33, in connect\n    values = sql.SQL(', ').join([\n  File \"/home/ian/anaconda3/envs/panthera/lib/python3.9/site-packages/psycopg2/sql.py\", line 288, in join\n    return Composed(rv)\n  File \"/home/ian/anaconda3/envs/panthera/lib/python3.9/site-packages/psycopg2/sql.py\", line 109, in __init__\n    raise TypeError(\nTypeError: Composed elements must be Composable, got 'lookup' instead\n`\nGiven that `composable` is the base class, I'm not really sure what the error is because of. As far as I know it seems correct based on the examples from the docs. My `sql_schema`\n```\n`DROP TABLE IF EXISTS \"raw_orderbooks\";\nCREATE TABLE \"raw_orderbooks\"\n(\n\n    lookup                text, -- essentially our lookup key comprising: hash(exchange_pair_timestapOfFirstEntry)\n    LOL1                  DOUBLE PRECISION[][][],\n    LOL2                  DOUBLE PRECISION[][][],\n    identifier1                  text,\n    identifier2            text,\n    timestamp_first_entry TIMESTAMP,\n    timestamp_last_entry  TIMESTAMP,\n    some_number       int\n);\n`\n```\none other idea I had was to wrap the non-datetime values in `sql.Literal` but then I run into the datetime issue.\nLooking around, I've seen other people using different notation but it feels less \"clean\" which is definitely subjective but IDK.\nEdit\nBased on the comments from the answer, my updated script is\n```\n`import psycopg2\nfrom psycopg2 import sql\nimport datetime\n\ndef connect(conn):\n    \"\"\" Connect to the PostgreSQL database server \"\"\"\n    # create a cursor\n    cur = conn.cursor()\n\n    # https://www.psycopg.org/docs/sql.html\n\n    col_names = sql.SQL(', ').join([\n        sql.Identifier('lookup'),\n        sql.Identifier('LOL1'),\n        sql.Identifier('LOL2'),\n        sql.Identifier('identifier1'),\n        sql.Identifier('identifier2'),\n        sql.Identifier('timestamp_first_entry'),\n        sql.Identifier('timestamp_last_entry'),\n        sql.Identifier('some_number')\n    ]),\n\n    values = sql.SQL(', ').join([\n        \"hash\",\n        [  # LOL1\n            [[1.0, 2.0],  # list of lists of lists 1.1\n             [3.0, 0.5]],\n            [[-1.0, -2.0], # list of lists of lists 1.2\n             [-3.0, -4.0]]\n        ], [  # LOL2\n            [[1.0, 2.0],  # list of lists of lists 2.1\n             [3.0, 0.5]],\n            [[-1.0, -2.0], # list of lists of lists 2.2\n             [-3.0, -4.0]]\n        ],\n        \"identifier1\",\n        \"identifier2\",\n        datetime.datetime(2021, 5, 10),\n        datetime.datetime(2021, 5, 12),\n        20\n    ])\n    table_col_names = ['lookup','LOL1','LOL2','identifier1','identifier2','timestamp_first_entry','timestamp_last_entry','some_number']\n\n    col_names = sql.SQL(', ').join(sql.Identifier(n) for n in table_col_names )\n\n    place_holders = sql.SQL(', ').join(sql.Placeholder() * len(table_col_names ))\n\n    query_base = sql.SQL(\"insert into {table_name} ({col_names}) values ({values})\").format(\n        table_name=sql.Identifier(\"raw_orderbook\"),\n        col_names=col_names,\n        values=place_holders\n    )\n\n    print(query_base.as_string(conn))\n    cur.execute(query_base,values)\n\n    cur.close(\n`\n```\nbut I'm still running into the issue `TypeError: Composed elements must be Composable, got 'lookup' instead`",
      "solution": "`sql.SQL.join` join a sequence of `Composable`  but you are providing normal variable like string , list etc in `values` variable.\nsecondly i will suggest use `placeholder`  for values in query then pass `values` to execute .\n```\n`table_col_names = ['lookup','LOL1','LOL2','identifier1','identifier2','timestamp_first_entry','timestamp_last_entry','some_number']\n\ncol_names = sql.SQL(', ').join(sql.Identifier(n) for n in table_col_names )\n\nplace_holders = sql.SQL(', ').join(sql.Placeholder() * len(table_col_names ))\n\nquery_base = sql.SQL(\"insert into {table_name} ({col_names}) values ({values})\").format(\n        table_name=sql.Identifier(\"raw_orderbook\"),\n        col_names=col_names,\n        values=place_holders\n    )\n\nprint(query_base.as_string(conn))\ncur.execute(query_base,values)\n`\n```",
      "question_score": 2,
      "answer_score": 4,
      "created_at": "2021-05-25T06:13:46",
      "url": "https://stackoverflow.com/questions/67681689/psycopg2-sql-sql-composed-elements-must-be-composable"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 78410960,
      "title": "psycopg3 mogrify: AttributeError: &#39;Connection&#39; object has no attribute &#39;mogrify&#39;",
      "problem": "Im using psycopg3 and would like to see what the actual sql with the parameters is so i can execute it using dbweaver...\n```\n`         with psycopg.connect(self.connect_str, autocommit=True) as conn:\n            if self.log.level == logging.DEBUG:\n                  cur = conn.cursor()\n                  sql_mogr = cur.mogrify(sql, params)\n                  self.log.debug(sql_mogr)\n            else:\n               self.log.info(f'sql: {sql}, params:{params}')\n\n            df = pd.read_sql(sql, con = conn, params = params)\n`\n```\nThe results of the mogrify line is:\n```\n`AttributeError: 'Cursor' object has no attribute 'mogrify'\n`\n```\nDoes psycopg3 not support this method?  if not, what is an alternate solution?\nversion of psycopg:\n```\n`psycopg==3.1.18\npsycopg-binary==3.1.18\npsycopg-pool==3.2.1\n`\n```",
      "solution": "The `mogrify` method is only available on the ClientCursor class.\n```\n`>>> conn = psycopg.connect(dbname='test', cursor_factory=psycopg.ClientCursor)\n>>> cur = conn.cursor()\n>>> q = \"\"\"select * from users\"\"\"\n\n>>> cur.mogrify(q)\n'select * from users'\n`\n```",
      "question_score": 2,
      "answer_score": 3,
      "created_at": "2024-04-30T22:50:24",
      "url": "https://stackoverflow.com/questions/78410960/psycopg3-mogrify-attributeerror-connection-object-has-no-attribute-mogrify"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 72416515,
      "title": "Installation error psycopg2 termux Android",
      "problem": "How can I install psycopg2 on my termux Android?\nI tried\n```\n`pip install psycopg2==2.2\npip3 install psycopg2-binary\n\nLDFLAGS=\"-L/system/lib64/\" CFLAGS=\"-I/data/data/com.termux/files/usr/include/\" pip install psycopg2-binary\n`\n```\nBut everytime got this error\n\nRead other answers on stack overflow but they don't work.\n```\n`apt-get\nsudo\n`\n```\nCommands Requires rooted Android and I'm not rooted.\n\nI would be grateful if someone help me",
      "solution": "Install PostgreSQL itself before installing python client\n`pkg install postgresql`\nIf it doesn't help, try to install dev version and other dependencies\n`pkg install postgresql-dev python make clang`\nAfter that install postgresql client `pip3 install psycopg2`\nFound working solution here:\nhttps://github.com/termux/termux-packages/issues/835",
      "question_score": 2,
      "answer_score": 3,
      "created_at": "2022-05-28T16:35:39",
      "url": "https://stackoverflow.com/questions/72416515/installation-error-psycopg2-termux-android"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 72395674,
      "title": "Export Postgresql Table to excel with header in Python",
      "problem": "My code works but it doesn't bring the header with the names, it only brings the numbers 0 1 ... 10 , what can I do ?\nUtils_db.py\n`    def consulta_sql(sql):\n    try:\n       connection = psycopg2.connect(user=\"postgres\",\n                                    password=\"postgres\",\n                                    host=\"localhost\",\n                                    port=\"5432\",\n                                    database=\"tb_cliente\")\n       cursor = connection.cursor()\n    except (Exception, psycopg2.Error) as error:\n           \n    try:\n       cursor.execute(sql)\n       connection.commit()\n       \n    except (Exception, psycopg2.Error) as error:\n       \n    finally:\n       if connection:\n          result = cursor.fetchall()\n          cursor.close()\n          connection.close()\n          return result\n`\nmain.py\n`Excel = Utils_db.consulta_sql(\"Select * from corantes\")\n\ndf = pd.DataFrame(Excel)\ndf.to_excel('C:/Users/Dnaxis2/Downloads/Corantes.xlsx', index=False)\n`\ngenerated excel\n```\n`0   1   2   3   4   5   6   7   8   9   10\n\n1   FF  BK  20  200 10  200 200 200 200 30\n\n2   PP  BK  100 500 150 0   0   0   35  30\n`\n```\ncorrect excel (would have to come like this)\n```\n`Corant Pags Table Polo Jetta Fox Ps Ilu Kik Qly\n\n1   FF  BK  20  200 10  200 200 200 200 30\n\n2   PP  BK  100 500 150 0   0   0   35  30\n`\n```",
      "solution": "You can ask psycopg2 to return the result as a dictionary, using the column names as keys for the dictionary.\nPass the value ` RealDictCursor` (import it from `psycopg2.extras`) to `cursor_factory` parameter in the connect method.\nThe line will be\n```\n`from psycopg2.extras import RealDictCursor\nconnection = psycopg2.connect(user=\"postgres\",\n                                password=\"postgres\",\n                                host=\"localhost\",\n                                port=\"5432\",\n                                database=\"tb_cliente\",\n                                cursor_factory=RealDictCursor)\n\n`\n```",
      "question_score": 2,
      "answer_score": 3,
      "created_at": "2022-05-26T19:16:26",
      "url": "https://stackoverflow.com/questions/72395674/export-postgresql-table-to-excel-with-header-in-python"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 72233481,
      "title": "copying a table across servers with psycopg3",
      "problem": "I'm working from and example in the psycopg3 documentation to copy a table from one database to another: link\n`dsn_src = 'postgresql:///dev_db'\ndsn_tgt = 'postgresql:///prod_test'\n\nwith psycopg.connect(dsn_src) as conn1, psycopg.connect(dsn_tgt) as conn2:\n    with conn1.cursor().copy(\"COPY sample TO STDOUT (FORMAT BINARY)\") as copy1:\n        with conn2.cursor().copy(\"COPY sample FROM STDIN (FORMAT BINARY)\") as copy2:\n            for data in copy1:\n                copy2.write(data)\n`\nrunning this results in the following error\n```\n`QueryCanceled: COPY from stdin failed: error from Python: TypeError - can't write memoryview\nCONTEXT:  COPY sample, line 1\n`\n```\nThe source and target schema are identical, as the documentation recommends, and this error persists if the format specification `(FORMAT BINARY)` is removed.\nIs there are way to resolve this `memoryview` error?",
      "solution": "Guessing you are using `psycopg3` Release Notes. I ran the code in 3.0.11 and it failed as you showed. I upgraded to 3.0.13 and it worked.",
      "question_score": 2,
      "answer_score": 3,
      "created_at": "2022-05-13T19:34:08",
      "url": "https://stackoverflow.com/questions/72233481/copying-a-table-across-servers-with-psycopg3"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 70009367,
      "title": "psycopg2-binary installation into a virtual environment fails when trying to compile source code",
      "problem": "I am trying to install `psycopg2-binary` into my Django project. I am using a virtual environment and the command I'm running is\n`pip install psycopg2-binary`\nHowever, I'm getting a massive error message, the gist of it is this:\n```\n`    Error: pg_config executable not found.\n\n    pg_config is required to build psycopg2 from source.  Please add the directory\n    containing pg_config to the $PATH or specify the full executable path with the\n    option:\n\n    python setup.py build_ext --pg-config /path/to/pg_config build ...\n\n    or with the pg_config option in 'setup.cfg'.\n\n    If you prefer to avoid building psycopg2 from source, please install the PyPI\n    'psycopg2-binary' package instead.\n`\n```\nBut, hey, I'm installing exactly 'psycopg2-binary'\nWhy am I getting all this mess?",
      "solution": "Pip cannot find you processor+OS at https://pypi.org/project/psycopg2-binary/2.9.2/#files so it tried to install from sources (the last file at the page) and failed.\nCompiling from sources is currently the only way. If you can donate some spare processor cycles to the Psycopg2 authors they perhaps could start compiling and publishing wheels for OSX on M1.",
      "question_score": 2,
      "answer_score": 1,
      "created_at": "2021-11-17T18:58:39",
      "url": "https://stackoverflow.com/questions/70009367/psycopg2-binary-installation-into-a-virtual-environment-fails-when-trying-to-com"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 69320280,
      "title": "Postgresql update column of numeric type with NULL value fails if all value of this column is NULL",
      "problem": "I have a database table like this:\n\nidx[PK]\na[numeric]\nb[numeric]\n\n1\n1\n1\n\n2\n2\n2\n\n3\n3\n3\n\n4\n4\n4\n\n...\n...\n...\n\nIn pgadmin4, I tried to update this table with some null values, and I noticed the following queries failed:\n```\n`UPDATE test as t SET\na = e.a,b = e.b\nFROM (VALUES (1,NULL,NULL),(2,NULL,NULL),(3,NULL,NULL)) \nAS e(idx, a, b)\nWHERE t.idx = e.idx \n`\n```\n```\n`UPDATE test as t SET\na = e.a,b = e.b\nFROM (VALUES (1,NULL,1),(2,NULL,2),(3,NULL,NULL)) \nAS e(idx, a, b)\nWHERE t.idx = e.idx \n`\n```\nThe error message is like this:\n```\n`ERROR:  column \"a\" is of type numeric but expression is of type text\nLINE 2: a = e.a,b = e.b\n            ^\nHINT:  You will need to rewrite or cast the expression.\nSQL state: 42804\nCharacter: 43\n`\n```\nHowever, this will be successful:\n```\n`UPDATE test as t SET\na = e.a,b = e.b\nFROM (VALUES (1,NULL,1),(2,2,NULL),(3,NULL,NULL)) \nAS e(idx, a, b)\nWHERE t.idx = e.idx \n`\n```\nIt seems like if the new values for one of the columns I am updating are all `NULL`, then the query fails. However, as long as there is at least one value is numeric but NOT `NULL`, the query would be successful. Why is this?\nI did simplify my real world case here as my actual table has millions of rows and more than 10 columns. Using Python and psycopg2, when I tried to update 50,000 rows in one query, even though there is a value in a column is NOT `NULL`, the previous error could still show up. I guess that is because the system scans a certain number of rows to decide if the type is correct or not instead of all 50,000 rows.\nTherefore, how to avoid this failure in my real world situation? Is there a better query to use instead of `UPDATE`?\nThank you very much!\nUPDATE\nPer comments from @Marth and @Gordon Linoff, and as I am using psycopg2, I did the following in my code:\n```\n`from psycopg2.extras import execute_values\nsql = \"\"\"UPDATE test as t SET\na = (e.a::numeric),\nb = (e.b::numeric)\nFROM (VALUES %s) \nAS e(idx, a, b)\nWHERE t.idx = e.idx\"\"\"\nexecute_values(cursor, sql, data)\n`\n```\n`cursor` is from the database connection. `data` is a list of tuples in the form `(idx, a, b)` of my values.",
      "solution": "This is due to the default behavior of how `NULL` works in these situations.  `NULL` is generally an unknown type, which is then treated as whatever type is necessary.\nIn a `values()` statement, Postgres tries to decipher the types.  It treats the individual records as it would with a `union`.  But if all are `NULL` . . . well, then there is no information.  And Postgres decides on using `text` as the universal default.\nIt is also important to understand that this fails with the same error:\n```\n`UPDATE test t\n    SET a = ''\n    WHERE t.id = 1;\n`\n```\nThe issue is that Postgres does not convert empty strings to numbers (unlike some other databases).\nIn any case, this is easily fixed by casting the `NULL` to an appropriate type:\n```\n`UPDATE test t\n    SET a = e.a,b = e.b\n    FROM (VALUES (1, NULL::numeric, NULL::numeric),\n                 (2, NULL, NULL),\n                 (3, NULL, NULL)\n         ) e(idx, a, b)\nWHERE t.idx = e.idx ;\n`\n```\nYou can be explicit for all occurrences of `NULL`, but that is not necessary.\nHere is a db<>fiddle that illustrates some of this.",
      "question_score": 2,
      "answer_score": 3,
      "created_at": "2021-09-24T21:34:25",
      "url": "https://stackoverflow.com/questions/69320280/postgresql-update-column-of-numeric-type-with-null-value-fails-if-all-value-of-t"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 67127045,
      "title": "Error: No Module Named &#39;psycopg2.__psycopg&#39; in AWS Lambda with python 3.8.7 version",
      "problem": "I have install psycopg2==2.8.6 module in window 10 (python version 3.8.7) and i am able to import in my code without any issues. But when I zipped and upload it to AWS Lambda giving this error. all the library folders in right place and AWS also with python 3.8.7 but not sure why this error. I tried to downgrade the psycopg2=2.8.5 but no luck. Can some help on priority please",
      "solution": "Your question implicates that you have downloaded `psycopg2` library on `Windows`, and are trying to use the same library within Lambda runtime environment, which ultimately uses `Linux` operating system.\nAWS Documentation on Lambda Runtime Environment provides more documentation, but to get this working you have few options\n\nRun amazon provided Linux container, install and zip dependencies in there. You could achieve this by running following in your project folder\n\n```\n`$ docker run --rm -v $PWD:/src --entrypoint '' amazon/aws-lambda-python:3.8 bash\n# execute below within container\n$ pip3 install psycopg2-binary -t /src/lambdalib\n`\n```\nAlternatively, you can build extension on Linux platform, within that container using following\n```\n`$ docker run --rm -v $PWD:/src --entrypoint '' amazon/aws-lambda-python:3.8 bash\n# execute below within container\n$ yum -y install cmake c++ gcc postgresql-devel && pip3 install psycopg2 -t /src/lambdalib\n`\n```\nThis will install linux binary version of `psycopg2` within your `lambdalib` folder. I suggest you execute this from cloud console, or Cloud9 IDE if Windows workstation is not having\n\nSearch for publicly available Lambda Layer  for `psycopg2`, add it to your lambda function, and avoid packaging this library altogether.",
      "question_score": 2,
      "answer_score": 3,
      "created_at": "2021-04-16T16:25:14",
      "url": "https://stackoverflow.com/questions/67127045/error-no-module-named-psycopg2-psycopg-in-aws-lambda-with-python-3-8-7-vers"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 66742889,
      "title": "Postgres SQL - CURSOR WITHOUT HOLD FOR CREATE TYPE",
      "problem": "I'm trying to create an enum in my Postgres server using Python & psycopg2\nThis is my query :\n`CREATE TYPE parser_type AS ENUM (\n            'regex',\n            'delimiter',\n            'column',\n            'json'\n        );\n`\nWhen I execute it from the IDE it is working, but when I'm trying to execute this query from my code it raises an exception\nThis is the execution code\n`    def execute_statment(\n        self,\n        statment,\n        statment_params=None,\n    ):\n        try:\n            self.connect()\n            cursor_id = uuid.uuid4().hex\n            with self.connection.cursor(\n                cursor_id,\n            ) as cursor:\n                cursor.execute(\n                    statment,\n                    statment_params,\n                )\n            self.connection.commit()\n        except Exception as exception:\n            if self.connection is not None:\n                self.connection.rollback()\n        finally:\n            self.disconnect()\n`\nWhen I execute the query I get an exception\n```\n`psycopg2.errors.SyntaxError: syntax error at or near \"CREATE\"\nLINE 1: ...0bbe4cc0b7d0b9d44f74a378\" CURSOR WITHOUT HOLD FOR CREATE TYPE\n`\n```",
      "solution": "The documentation says:\n\n`cursor`(name=None, cursor_factory=None, scrollable=None, withhold=False)\nReturn a new cursor object using the connection.\nIf name is specified, the returned cursor will be a server side cursor (also known as named cursor). Otherwise it will be a regular client side cursor.\n\nThe error message tells you that you cannot use a server-side cursor to run a DDL statement, so don't specify an argument for the `cursor()` method.",
      "question_score": 2,
      "answer_score": 3,
      "created_at": "2021-03-22T09:54:01",
      "url": "https://stackoverflow.com/questions/66742889/postgres-sql-cursor-without-hold-for-create-type"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 79618357,
      "title": "Sqlalchemy and psycopg2: pandas.read_sql_query &quot;dict is not a sequence&quot; error with param substitution",
      "problem": "Package Versions:\n\nSQLAlchemy 2.0.40\npandas 2.2.3\npsycopg2-binary 2.9.10\n\nI am trying to run a query using pandas' native param substitution, but I can't seem to get it to run without erroring. I tried simplifying the query to:\n```\n`select *\nFROM public.bq_results br \nWHERE cast(\"eventDate\" as date) between \n  TO_DATE('%test_start_date', 'YYYYMMDD') AND TO_DATE('%test_end_date', 'YYYYMMDD')\nlimit 10000\n`\n```\nbut I get error:\n```\n`TypeError: dict is not a sequence\n`\n```\nwhen running:\n```\n`df = pd.read_sql_query(query, self.__engine, params={\"test_start_date\": \"20250101\", \"test_end_date\": \"20250131\"})\n`\n```\nwhere\n```\n`self.__engine = create_engine(f'postgresql://{self.user}:{self.password}@{self.host}:{self.port}/{self.database}')\n`\n```",
      "solution": "The documentation of read_sql_query says the following:\n```\n`params : list, tuple or mapping, optional, default: None\n\n    List of parameters to pass to execute method. The syntax used to pass parameters is database driver dependent. Check your database driver documentation for which of the five syntax styles, described in PEP 249\u2019s paramstyle, is supported. Eg. for psycopg2, uses %(name)s so use params={\u2018name\u2019 : \u2018value\u2019}.\n`\n```\nSince you use the psycopg2 driver the parameters should be noted as @JonSG has mentioned. It should be:\n```\n`select *\nFROM public.bq_results br \nWHERE cast(\"eventDate\" as date) between \n  TO_DATE(%(test_start_date)s, 'YYYYMMDD') AND TO_DATE(%(test_end_date)s, 'YYYYMMDD')\nlimit 10000\n`\n```\nHope this works.",
      "question_score": 2,
      "answer_score": 2,
      "created_at": "2025-05-12T19:23:38",
      "url": "https://stackoverflow.com/questions/79618357/sqlalchemy-and-psycopg2-pandas-read-sql-query-dict-is-not-a-sequence-error-wi"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 74251820,
      "title": "Postgres: psycopg2.errors.InvalidTextRepresentation: invalid input syntax for type boolean",
      "problem": "I'm running the following query in Postgres:\n```\n`    INSERT INTO \"attack_paths_attackpathprevscan\"\n    (\"createdAt\",\"account_id\",\"org_id\",\"event_id\",\"action_id\",\"assets\")\n    \n    VALUES (('2022-10-30 08:51:13.934641')::timestamp,\n           ('cd802832-c0e8-4376-a1f7-730836cb0885'::uuid)::uuid,\n           ('e5f8eaff-54f0-4ea0-8428-b331a504d744'::uuid)::uuid,\n           ('11111111-1111-1111-1111-111111111119')::uuid,\n           ('11111111-1111-1111-1111-111111111119')::uuid,\n    \n    (ARRAY[hstore(ARRAY['id','type','asset_id','group_id','is_internet_facing','is_running','connecting_agent_id','policies'],\n    ARRAY['ap1-node1','VmNodeData','11111111-1111-1111-1111-111111111111',\n'11111111-1111-1111-1111-111111111112',true,true,'ap1-node1-al',NULL])])::jsonb)\n`\n```\nand unfortunately I get the error:\n\nE               psycopg2.errors.InvalidTextRepresentation: invalid\ninput syntax for type boolean: \"ap1-node1\" E               LINE 3:\n...running','connecting_alert_id','policies'], ARRAY['ap1-node1...\n\nFrom the error message, I understand that the SQL query expects that the string \"ap1-node1\" would be a boolean.\nCan you explain why?",
      "solution": "You left out the part where you are actually doing this in `psycopg2`.\n\nPostgres arrays cannot have mixed types , so the `...true,true,...` is creating the error.  You will have to change to something like  `...'true','true',...` or  `...'t','t',...`\n\nAs demonstration:\n```\n`select ARRAY['ap1-node1', true];\nERROR:  invalid input syntax for type boolean: \"ap1-node1\"\nLINE 1: select ARRAY['ap1-node1', true];\n\nselect ARRAY['ap1-node1', 'true'];\n      array       \n------------------\n {ap1-node1,true}\n`\n```\nPostgres cannot make the elements all be one single type in the first case so it flags the first element as not being the same as the boolean type represented by `true`.\n\nThis `(ARRAY[hstore(ARRAY['id' ...` whole field looks like it would be better as a child table to `attack_paths_attackpathprevscan`.",
      "question_score": 2,
      "answer_score": 2,
      "created_at": "2022-10-30T10:18:56",
      "url": "https://stackoverflow.com/questions/74251820/postgres-psycopg2-errors-invalidtextrepresentation-invalid-input-syntax-for-ty"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 73901630,
      "title": "psycopg2 execute_values fails with &gt; 100 rows in supabase?",
      "problem": "Not sure if this is a Supabase issue or a `psycopg2` issue honestly and would love some help debugging.\nI have the following code:\n```\n`args = [('HaurGlass','60000','2022-10-20T21:15:39.751Z','10130506261','ac76e8db-ace0-40df-b6fa-f470641805e9','ad43639e-f66e-49d5-8fe8-d1ce5cd26193','{}')]\n\nstatement = ('''\n      INSERT INTO %s (%s) VALUES %s ON CONFLICT (company_id, crm_id)\n      DO UPDATE SET (%s)=(%s) RETURNING crm_id, id''')\nstatement = cur.mogrify(statement,\n      (AsIs(db_table), AsIs(','.join(keys)),\n       AsIs(\"%s\"), AsIs(','.join(update_keys)), \n       AsIs(','.join(excluded_keys))))\noutput = execute_values(cur, statement, args, fetch=True)\n\n`\n```\nThe weird thing is that if args is \n```\n`INSERT INTO licenses (name,value,subscription_end,crm_id,company_id,csm_id,custom_data) VALUES ('HaurGlass','60000','2022-10-20T21:15:39.751Z','10130506261','ac76e8db-ace0-40df-b6fa-f470641805e9','ad43639e-f66e-49d5-8fe8-d1ce5cd26193','{}')...\n`\n```\nwhich would be good, except that it's immediately followed by:\n```\n`INSERT INTO licenses (name,value,subscription_end,crm_id,company_id,csm_id,custom_data) VALUES ('HaurGlass','60000','2022-10-20T21:15:39.751Z','10130506261','ac76e8db-ace0-40df-b6fa-f470641805e9',NULL,'{}'),...\n`\n```\nI've also confirmed that the number of records in the second \"NULLifying\" query is exactly equal to `len(args)-100`.\nAny idea what is going on?",
      "solution": "OK so it turns out I was missing the `page_size` parameter. All I had to do was:\n`output = execute_values(cur, statement, args, fetch=True, page_size=len(args))`",
      "question_score": 2,
      "answer_score": 2,
      "created_at": "2022-09-29T23:08:33",
      "url": "https://stackoverflow.com/questions/73901630/psycopg2-execute-values-fails-with-100-rows-in-supabase"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 71445303,
      "title": "psycopg2 sql.Identifier fails executing when query is valid",
      "problem": "I have a simple psycoph2 snippet which tries to grant usage on a schema but it displays syntax however the query is perfectly valid when executed on the terminal\n```\n`import psycopg2\nfrom psycopg2 import Error, sql, extras, extensions\n\ncursor.execute(sql.SQL(\"GRANT USAGE on SCHEMA public TO {role}\".format(role=sql.Identifier(\"readonly_role\"))))\n`\n```\n```\n`Error while connecting to PostgreSQL syntax error at or near \"(\"\nLINE 1: GRANT USAGE on SCHEMA public TO Identifier('readonly_role')\n`\n```",
      "solution": "Your parens are in the wrong place.\nUse `sql.SQL()` on the full raw SQL string, then `.format()` on the result of that (not on the raw string itself).\nHere it is broken up and indented for clarity:\n`cursor.execute(\n    sql.SQL(\n        \"GRANT USAGE on SCHEMA public TO {role}\"\n    ).format(\n        role=sql.Identifier(\n            \"readonly_role\"\n        )\n    )\n)\n`",
      "question_score": 2,
      "answer_score": 2,
      "created_at": "2022-03-11T23:30:29",
      "url": "https://stackoverflow.com/questions/71445303/psycopg2-sql-identifier-fails-executing-when-query-is-valid"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 70880046,
      "title": "Parametric table name for pandas read_sql_query on Postgres",
      "problem": "I cannot find out, how to use `pandas.read_sql_query` and correctly (= safely against sql injection) parametrize table names (or other sql identifiers). Using `sqlalchemy+psycopg2` to access PostgreSQL database.\nExample of what doesn't work:\n```\n`import os\nimport pandas\nfrom sqlalchemy import create_engine\ndb = create_engine(os.getenv(POSTGRES_CONNSTRING))\npandas.read_sql_query(sql='select * from %(schema)s.%(table)s',\n                      con = db,\n                      params={'schema': 'public', 'table': 'table_name'})\n`\n```\nYields:\n```\n`SyntaxError: syntax error at or near \"'public'\"\nLINE 1: select * from 'public'.'table_name'\n`\n```\nFor psycopg2 the correct solution is described here.\n```\n`import psycopg2\nquery = psycopg2.sql.SQL('select * from {schema}.{table}') \\\n                        .format(schema = psycopg2.sql.Identifier('public'),\n                                table = psycopg2.sql.Identifier('table_name'))\n`\n```\nBut the query is now of type `psycopg2.sql.Composed`, which I can pass to the execute methods in `psycopg2` but not to `pandas.read_sql_query`.\nIs there any good solution to this?",
      "solution": "You can use the `as_string` method to turn the `Composed` query into a string that you can pass to Pandas (docs).\n```\n`import pandas as pd\nimport psycopg2\nfrom sqlalchemy import create_engine\n\nengine = create_engine('postgresql+psycopg2://user:pw@host:port/db')\ncur = engine.raw_connection().cursor()\n\nquery = psycopg2.sql.SQL('select * from {schema}.{table}') \\\n                        .format(schema = psycopg2.sql.Identifier('public'),\n                                table = psycopg2.sql.Identifier('table_name'))\nquery_string = query.as_string(cur)\n\npd.read_sql_query(query_string, engine)\n`\n```",
      "question_score": 2,
      "answer_score": 2,
      "created_at": "2022-01-27T15:13:06",
      "url": "https://stackoverflow.com/questions/70880046/parametric-table-name-for-pandas-read-sql-query-on-postgres"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 69243024,
      "title": "How to properly build a SELECT query on many fields from two joined tables using their aliases in a dot notation manner with psycopg2",
      "problem": "Is there a way to convert these identifiers to a PostgreSQL query using psycopg2:\n```\n`total_query_fields = (\n    'p.id',\n    'p.name',\n    'p.type',\n    'p.price',\n    'o.date',        # please, notice the 'o' alias here\n    'o.transaction', # please, notice the 'o' alias here\n    'p.warehouse',\n    'p.location',\n)\n\n# they may get split into their own tables if necessary:\nproduct_query_fields = ('id', 'name', 'type', 'price', 'warehouse', 'location',)\norder_query_fields = ('date', 'transaction',)\n`\n```\ninto something like this:\n`import psycopg2\nfrom psycopg2 import sql\n\nmyid = 100\n\nsql_query = sql.SQL(\"\"\"\n    SELECT {fields} FROM product p\n    INNER JOIN owner o ON p.id = o.product_id\n    WHERE p.id = {jidx} AND (o.dateof_purchase IS NOT NULL\n    OR o.state = 'checked_out' );\"\"\"\n).format(\n    fields = # there should be the readable ***magic***\n    jidx = sql.Literal(myid)\n)\n`\n?\nEven after understanding my problem by reading this thread, I'm not able to figure out a nice way for getting the list of my fields using a dotted notation. There must probably be two `map()` used, I guess, and both an `sql.SQL('.').join(...)` and `sql.SQL(', ').join(...)`.\nOr maybe something more elegant, e.g. using `SELECT {}.{}...`?\n\nBecause for the moment I'm having trouble with that:     \n`fields = sql.SQL(', ').join(map(sql.Identifier, total_query_fields)),\n`\nbecause it will escape all the sequences \"table.fields\" with double quotes, which is definitely not valid in SQL:\n`# /!\\ INVALID SQL QUERY /!\\:\nprint(sql_query.as_string(conn))\n# will print:\n# SELECT \"p.id\", \"p.name\", \"p.type\", \"p.price\", \"o.date\", \"o.transaction\", \"p.warehouse\", \"p.location\" FROM product p\n#    INNER JOIN owner o ON p.id = o.product_id\n#    WHERE p.id = 100 AND (o.dateof_purchase IS NOT NULL\n#    OR o.state = 'checked_out' );\n`\nAnd indeed, if I copy/paste the last query directly in my favorite PostgreSQL query tool:\n```\n`ERROR:  column \"p.id\" does not exist\nLINE 1: SELECT \"p.id\", \"p.name\", \"p.type\", \"p.price\", \"o.date\",...\n               ^\nHINT:  Perhaps you meant to reference the column \"p.id\" or the column \"o.id\".\nSQL state: 42703\nCharacter: 8\n`\n```\nThe same error is obviously raised by psycopg2:\n`UndefinedColumn: column \"p.id\" does not exist\nLINE 1: SELECT \"p.id\", \"p.name\", \"p.type\", \"p.price\", \"o.date...\n               ^\nHINT:  Perhaps you meant to reference the column \"p.id\" or the column \"o.id\".\n`\nThe documentation is also clear on that:\n\nVersioning:\n```\n`psycopg2.__version__\n '2.9.1 (dt dec pq3 ext lo64)'\n`\n```",
      "solution": "If I understand the question correctly, I think the trick here is that `sql.Identifier` accepts one or more strings (`*strings`), so you can split the columns on the dot and pass both parts to `sql.Identifier` which will compose the desired `\"alias\".\"column\"` result.\n```\n`>>> i = sql.Identifier('a', 'col')\n>>> i.strings\n('a', 'col')\n>>> conn = psycopg2.connect(database='test')\n>>> cur = conn.cursor()\n>>> i.as_string(cur)\n'\"a\".\"col\"'\n`\n```\nQuoting all the fields can be done like this:\n`fields = sql.SQL(', ').join(sql.Identifier(*f.split('.')) for f in total_query_fields)\n\nsql_query = sql.SQL(\n    \"\"\"\n    SELECT {fields} FROM product p\n    INNER JOIN owner o ON p.id = o.product_id\n    WHERE p.id = {jidx} AND (o.dateof_purchase IS NOT NULL\n    OR o.state = 'checked_out' );\"\"\"\n).format(fields=fields, jidx=sql.Literal(myid))\n`\nThe resulting query (from `cursor.mogrify`) is\n`b'\\n    SELECT \"p\".\"id\", \"p\".\"name\", \"p\".\"type\", \"p\".\"price\", \"o\".\"date\", \"o\".\"transaction\", \"p\".\"warehouse\", \"p\".\"location\" FROM product p\\n    INNER JOIN owner o ON p.id = o.product_id\\n    WHERE p.id = 100 AND (o.dateof_purchase IS NOT NULL\\n    OR o.state = \\'checked_out\\' );'\n`\nIf you prefer to use `map` rather than a generator expression you could use itertools.starmap\n```\n`from itertools import starmap\n\nfields = sql.SQL(', ').join(\n    starmap(sql.Identifier, map(lambda f: f.split('.'), total_query_fields)))\n`\n```",
      "question_score": 2,
      "answer_score": 2,
      "created_at": "2021-09-19T13:55:16",
      "url": "https://stackoverflow.com/questions/69243024/how-to-properly-build-a-select-query-on-many-fields-from-two-joined-tables-using"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 66499593,
      "title": "Inserting into a table with Psycopg2: Do Nothing on Conflict",
      "problem": "Hello here is my current query:\n```\n`        query = sql.SQL(\"insert into {schema}.{table} ({fields}) values ({placeholder}) ON CONFLICT DO UPDATE SET {updates}\").format(\n        schema=sql.Identifier(self.schema),\n        table=sql.Identifier(tbl_name),\n        fields=sql.SQL(', ').join(sql.Identifier(field_name) for field_name in column_names_lst),\n        placeholder=sql.SQL(', ').join(sql.Placeholder() for field_value in column_data_lst),\n        updates = ', '.join(f\"{column_name} = '{column_value}'\" for column_name, column_value\n                               in tbl_data.items())\n    )\n`\n```\nIt worked before adding the ON CONFLICT DO UPDATE part, and adding the updates parameter, but since then I required to make some columns into the primary key for some of the target tables.\nI have something wrong with my syntax but can't seem to debug it, getting a TypeError:\n```\n`Composed elements must be Composable, got...\n`\n```\nAny insight would be helpful, this is a new library for me.",
      "solution": "`updates` is just a composed string, it might be better to make use of `psycopg2.sql`:\n```\n`schema = 'public'\ntbl_name = 'testTable'\ncolumn_names_lst = ['column_1', 'column_2', 'column_3']\ncolumn_data_lst = (1, 2, 3)\ntbl_data = dict(zip(column_names_lst, column_data_lst))\n\nquery = sql.SQL(\n    \"insert into {schema}.{table} ({fields}) values ({placeholder}) ON CONFLICT DO UPDATE SET {updates}\"\n).format(\n    schema=sql.Identifier(schema),\n    table=sql.Identifier(tbl_name),\n    fields=sql.SQL(', ').join(\n        sql.Identifier(field_name) for field_name in column_names_lst\n    ),\n    placeholder=sql.SQL(', ').join(\n        sql.Placeholder() for field_value in column_data_lst\n    ),\n    updates=sql.SQL(', ').join(\n        [\n            sql.SQL(\"{}={}\").format(sql.Identifier(k), sql.Placeholder())\n            for k in column_names_lst\n        ]\n    )\n)\n\nprint(query.as_string(cur))\n# duplicate values in order to fill UPDATE values\nallValues = column_data_lst + column_data_lst\nprint(cur.mogrify(query, allValues).decode('utf-8'))\n`\n```\nOut:\n```\n`insert into \"public\".\"testTable\" (\"column_1\", \"column_2\", \"column_3\") values (%s, %s, %s) ON CONFLICT DO UPDATE SET \"column_1\"=%s, \"column_2\"=%s, \"column_3\"=%s\ninsert into \"public\".\"testTable\" (\"column_1\", \"column_2\", \"column_3\") values (1, 2, 3) ON CONFLICT DO UPDATE SET \"column_1\"=1, \"column_2\"=2, \"column_3\"=3\n`\n```",
      "question_score": 2,
      "answer_score": 2,
      "created_at": "2021-03-05T22:02:18",
      "url": "https://stackoverflow.com/questions/66499593/inserting-into-a-table-with-psycopg2-do-nothing-on-conflict"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 75458994,
      "title": "Error when applying a timeframe mask to dataframe",
      "problem": "Intro\nI am writing a piece of python code that uses pcycopg2, pandas and datetime that retrieves information from a PGAdmin database, which filters the data to the last 14 days of activity.\nThe columns I am pulling from the database are: id (Unique Number), createdby (Name), created (Date)\nI pull all of that information as I thought I could transfer it into a dataframe and then use a mask to get the bits that I want.\nCode\n```\n`import psycopg2\nimport pandas as pd\nimport datetime\n\nToday = datetime.datetime.now() #gets todays date\nTimeFrameInDays = datetime.timedelta(days = 14) #gets int amount of days\nTwoWeeksAgo = Today - TimeFrameInDays #calculates date two weeks ago\n\nQuery = 'SELECT createdby, created, id FROM \"02 Planning\".\"fibre\"'\n\nprint(\"Robo-Cop Initialised\") #DEV\n\ntry:\n    connection = psycopg2.connect(database = DB_Name,\n                            user = DB_User,\n                            password = DB_Pass,\n                            host = DB_Host,\n                            port = DB_Port)\n\n    print(\"Database connected\")\n\nexcept (Exception, psycopg2.Error) as error: \n    #if error occurs, message is returned\n    print(\"Error Occured Trying to Connect\")\n\nfinally:\n    cursor = connection.cursor() #makes refrencing the cursor easier\n    cursor.execute(Query) #executes query\n    Data = cursor.fetchall() #saves results\n    \n    DataFrame = pd.DataFrame(Data) #assembles into dataframe\n    DataFrame.rename(columns={0:'Created By', 1:'Created On', 2:'Database Id'}) #renames columns\n    #print(DataFrame)\n    mask = (DataFrame['Created On'] > TwoWeeksAgo.date) & (DataFrame['Created On'] Error\n```\n`Traceback (most recent call last):\n  File \"c:\\Users\\\\Documents\\GitHub\\\\main.py\", line 42, in \n    mask = (DataFrame['Created On'] > TwoWeeksAgo.date) & (DataFrame['Created On'] Why my question is not a duplicate\nMy question has nothing to do with renaming columns, I am not trying to edit how I rename my columns. What i am trying to do is apply a mask to a dataframe that only shows me dates in the last 14 days.\nNew Error\nWith this piece of code\n`DataFrame = pd.DataFrame(Data, columns=['Created By','Created On','Database Id'])`,\ni get\n```\n`  File \"c:\\Users\\\\Documents\\GitHub\\\\main.py\", line 42, in \n    mask = (DataFrame['Created On'].dt.date > TwoWeeksAgo.date) & (DataFrame['Created On'].dt.date <= Today.date)\n  File \"C:\\Users\\\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\generic.py\", line 5902, in __getattr__\n    return object.__getattribute__(self, name)\n  File \"C:\\Users\\\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\accessor.py\", line 182, in __get__\n    accessor_obj = self._accessor(obj)\n  File \"C:\\Users\\\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\indexes\\accessors.py\", line 512, in __new__\n    raise AttributeError(\"Can only use .dt accessor with datetimelike values\")\nAttributeError: Can only use .dt accessor with datetimelike values. Did you mean: 'at'?\n`\n```",
      "solution": "Complete solution:\n```\n`#pass columns names to DataFrame constructor\nDataFrame = pd.DataFrame(Data, columns=['Created By','Created On','Database Id'])\n\nToday = datetime.datetime.now() #gets todays date\nTimeFrameInDays = datetime.timedelta(days = 14) #gets int amount of days\n #calculates date two weeks ago\nTwoWeeksAgo = Today - TimeFrameInDays\n\n#convert column to datetimes\nDataFrame['Created On'] = pd.to_datetime(DataFrame['Created On'])\n\n#compare dates\nmask = (DataFrame['Created On'].dt.date > TwoWeeksAgo.date()) & \n        (DataFrame['Created On'].dt.date Or use `Timestamp.normalize` for datetimes without times (times are `00:00:00`), so possible compare datetimes columns:\n```\n`Today = pd.Timestamp.now().normalize()\nTwoWeeksAgo = Today - pd.Timedelta(days = 14)\n\nmask = (DataFrame['Created On'] > TwoWeeksAgo) & (DataFrame['Created On'] Or:\n```\n`Today = pd.Timestamp.now().normalize()\nTwoWeeksAgo = Today - pd.Timedelta(days = 14)\n\nmask = DataFrame['Created On'].between(TwoWeeksAgo, Today, inclusive='right')\nDataFrame1 = DataFrame.loc[mask] \n`\n```",
      "question_score": 2,
      "answer_score": 1,
      "created_at": "2023-02-15T12:17:04",
      "url": "https://stackoverflow.com/questions/75458994/error-when-applying-a-timeframe-mask-to-dataframe"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 73369970,
      "title": "`mogrify` throws &quot;SystemError: bad argument to internal function&quot; on passing `__slots__` based instance",
      "problem": "I defined the below slots based class to satisfy my \"mutable namedtuple with default None values\" requirement:\n`class Row:\n    __slots__ = tuple(fields)\n\n    def __init__(self):\n        for attr in self.__slots__:\n            setattr(self, attr, None)\n    \n    def __setitem__(self, name, value):\n        setattr(name, value)\n\n    def __getitem__(self, i):\n        return getattr(self, self.__slots__[i])\n`\nBut when I use its instance for generating query string using `cur.mogrify` I get\n\nSystemError: ../Objects/tupleobject.c:85: bad argument to internal function\n\nComplete code:\n`import psycopg2 as pg2\n\ndef define_row(fields):\n    class Row:\n        pass # as mentioned above\n    return Row\n\nfields = (\"age\", \"gender\")\nrow = define_row(fields)()\nrow.age = 33\nrow.gender = \"m\"\n\nrows = [row]\n\ncur = pg2.connect(dbname=\"playground\", user=\"postgres\").cursor()\nplaceholders = \"({})\".format(\",\".join([\"%s\"] * len(fields)))\nargs_str = \",\".join(\n    cur.mogrify(placeholders, r).decode(\"utf-8\") for r in rows\n)  # Exception on this line\nqry = f\"INSERT INTO playground({','.join(fields)}) VALUES \"\ncur.execute(qry + args_str)\n`\n`mogrify` docs do not explicitly mention what the datatype of `parameters` should be.",
      "solution": "The `Row` class needs to implement a `__len__` method:\n`class Row:\n    ...\n    def __len__(self):\n        return len(self.__slots__)\n`\n`cursor.mogrify` expects its second argument to be a sequence or mapping, and Python sequences are expected to define `__len__`.",
      "question_score": 2,
      "answer_score": 1,
      "created_at": "2022-08-16T09:03:42",
      "url": "https://stackoverflow.com/questions/73369970/mogrify-throws-systemerror-bad-argument-to-internal-function-on-passing"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 72903928,
      "title": "SQLAlchemy bulk insert statement in Postgres database throws AttributeError",
      "problem": "I am trying to insert rows in Python SQLAlchemy by bulk into a Postgres database by using an `insert` statement. I need to use the insert statement instead of `bulk_insert_mappings`, as I want to silently ignore failed insertion of duplicate entries. This was not apparent before, but I have added it now.\nThe table is created as it should. However, even a very simple insert operation via statement API throws this error:\n```\n`AttributeError: '_NoResultMetaData' object has no attribute '_indexes_for_keys'\n`\n```\nMinimal Verifiable Example:\n```\n`import os\n\nimport sqlalchemy\nfrom sqlalchemy import (\n    Column,\n    INTEGER,\n    TEXT\n)\nfrom sqlalchemy.dialects.postgresql import insert\nfrom sqlalchemy.ext.declarative import declarative_base\nfrom sqlalchemy.orm import sessionmaker\n\nBase = declarative_base()\n\nclass Test(Base):\n    __tablename__ = 'test'\n    id = Column(INTEGER, primary_key=True)\n    data = Column(TEXT)\n\nengine = sqlalchemy.create_engine(os.environ['DATABASE_CONNECTION'])\nSession = sessionmaker(engine)\n\nBase.metadata.create_all(engine, Base.metadata.tables.values(), checkfirst=True)\n\nconnection = engine.connect()\nbuffer = [\n    {\n        'data': \"First test\"\n    },\n    {\n        'data': \"Second test\"\n    }\n]\n\ninsert_statement = insert(Test).values(buffer)\n# Using insert statement instead of bulk_insert_mappings so I can do nothing when adding duplicate entries\ninsert_or_do_nothing = insert_statement.on_conflict_do_nothing(index_elements=[Company.local_id])\norm_statement = sqlalchemy.select(Test).from_statement(insert_or_do_nothing)\n\nwith Session() as session:\n    session.execute(orm_statement).scalars()\n\nconnection.close()\n`\n```\nFull stacktrace:\n```\n`Traceback (most recent call last):\n  File \"/project/path/test.py\", line 41, in \n    session.execute(orm_statement).scalars()\n  File \"/venv/path/sqlalchemy/orm/session.py\", line 1715, in execute\n    result = compile_state_cls.orm_setup_cursor_result(\n  File \"/venv/path/sqlalchemy/orm/context.py\", line 354, in orm_setup_cursor_result\n    return loading.instances(result, querycontext)\n  File \"/venv/path/sqlalchemy/orm/loading.py\", line 89, in instances\n    cursor.close()\n  File \"/venv/path/sqlalchemy/util/langhelpers.py\", line 70, in __exit__\n    compat.raise_(\n  File \"/venv/path/sqlalchemy/util/compat.py\", line 208, in raise_\n    raise exception\n  File \"/venv/path/sqlalchemy/orm/loading.py\", line 69, in instances\n    *[\n  File \"/venv/path/sqlalchemy/orm/loading.py\", line 70, in \n    query_entity.row_processor(context, cursor)\n  File \"/venv/path/sqlalchemy/orm/context.py\", line 2627, in row_processor\n    _instance = loading._instance_processor(\n  File \"/venv/path/sqlalchemy/orm/loading.py\", line 715, in _instance_processor\n    primary_key_getter = result._tuple_getter(pk_cols)\n  File \"/venv/path/sqlalchemy/engine/result.py\", line 934, in _tuple_getter\n    return self._metadata._row_as_tuple_getter(keys)\n  File \"/venv/path/sqlalchemy/engine/result.py\", line 106, in _row_as_tuple_getter\n    indexes = self._indexes_for_keys(keys)\nAttributeError: '_NoResultMetaData' object has no attribute '_indexes_for_keys'\n`\n```\nAm I misusing the statement interface? The ORM statement looks fine:\n```\n`INSERT INTO test (data) VALUES (:data_m0), (:data_m1)\n`\n```\nI am using\n\nPostgreSQL 14.4\npsycopg2-binary 2.9.3\nSQLAlchemy 1.4.39",
      "solution": "I found a solution that uses insert statement: Avoid using the ORM statements. For some reason, using plain statements seems to do the job, whilst ORM ones throw the `AttributeError`.\nThis is confusing, as the official documentation calls for ORM statements:\n```\n`# THIS APPROACH DID NOT WORK FOR ME\n\nstmt = stmt.on_conflict_do_update(\n    index_elements=[User.name], set_=dict(fullname=stmt.excluded.fullname)\n).returning(User)\n\norm_stmt = (\n    select(User)\n    .from_statement(stmt)\n    .execution_options(populate_existing=True)\n)\nfor user in session.execute(\n    orm_stmt,\n).scalars():\n    print(\"inserted or updated: %s\" % user)\n`\n```\nBut if you omit the ORM statement part, all is good\n```\n`# THIS WORKS\n\ninsert_statement = insert(Test).values(buffer)\ninsert_or_do_nothing = insert_statement.on_conflict_do_nothing(index_elements=[Test.id])\n\n    with Session() as session:\n        session.execute(insert_or_do_nothing)\n        session.commit()\n`\n```",
      "question_score": 2,
      "answer_score": 1,
      "created_at": "2022-07-07T22:35:26",
      "url": "https://stackoverflow.com/questions/72903928/sqlalchemy-bulk-insert-statement-in-postgres-database-throws-attributeerror"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 72684561,
      "title": "Any way to pass operators &lt;, &lt;&gt;, &gt;= into sql-query?",
      "problem": "I have such piece of program:\n`if self.current_condition == 'comparison':\ncomparison_command = '''SELECT * FROM {table} WHERE {pkey} \nWhat I want to do is write '\nThe face of the app looks like this\nA little bit more of code context. It's an app, that should get data from database by sql-request, that creates from interface. As you can see, there's a bit more operators than one to choose.\n```\n`    def run_func(self):\n    conn = None\n    try:\n        conn = psycopg2.connect(\n            host='localhost',\n            database='1rl',\n            user='postgres',\n            password=passwor)\n        cur = conn.cursor()\n\n        if self.current_condition == 'comparison':\n            comparison_command = '''SELECT * FROM {table} WHERE {pkey} < %s'''\n            cur.execute(sql.SQL(comparison_command).format(table=sql.Identifier(self.current_table),\n                                                           pkey=sql.Identifier(self.current_columns[0].text())),\n                                                            (self.comp_value, ))\n            print(cur.fetchall())\n\n    except (Exception, psycopg2.DatabaseError) as error:\n        print(error)\n    finally:\n        if conn is not None:\n            conn.close()\n\ndef display(self, i):\n    self.list_of_conditions.setCurrentIndex(i)\n    self.current_condition = self.all_conditions[i]\n    print(self.current_condition)\n\ndef comp_value_changed(self):\n    self.comp_value = self.value.text()\n\ndef comp_on_selected(self):\n    sender = self.sender()\n    self.comp_selec = sender.text()\n\ndef comparison_fun(self):\n    layout = QFormLayout()\n    compars = QHBoxLayout()\n\n    for i in self.all_comparisons:\n        temp = QRadioButton(i)\n        temp.toggled.connect(self.comp_on_selected)\n        compars.addWidget(temp)\n\n    layout.addRow(QLabel('Operators'), compars)\n\n    self.value = QLineEdit()\n    self.value.textChanged.connect(self.comp_value_changed)\n    layout.addRow(\"Value\", self.value)\n\n    rune = QPushButton('Run')\n    rune.clicked.connect(self.run_func)\n    layout.addRow(rune)\n    self.comparison.setLayout(layout)\n`\n```",
      "solution": "You can use string interpolation on `comparison_command`, use f-string notation, and double the existing braces to escape them:\n```\n`comparison_command = f'SELECT * FROM {{table}} WHERE {{pkey}} {self.comp_selec} %s'\ncur.execute(sql.SQL(comparison_command).format(\n                    table=sql.Identifier(self.current_table),\n                    pkey=sql.Identifier(self.current_columns[0].text())\n                ), \n                (self.comp_value, )\n           )\n`\n```\nThis assumes that `self.comp_selec` has the operator in the SQL syntax, i.e. it should be like \"=\", \">\", and not \"==\", \"ge\", \"greater\", ...etc.\nI need to add the disclaimer about the risk of SQL injection. As I understand all of this runs on a local machine, a smart user could potentially tamper with the executable and make the SQL execute something harmful for the database or its security.",
      "question_score": 2,
      "answer_score": 1,
      "created_at": "2022-06-20T10:59:52",
      "url": "https://stackoverflow.com/questions/72684561/any-way-to-pass-operators-into-sql-query"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 72389393,
      "title": "How to pass psycopg2 cursor object to foreachPartition() in pyspark?",
      "problem": "I'm getting following error\n```\n`Traceback (most recent call last):\n  File \"/databricks/spark/python/pyspark/serializers.py\", line 473, in dumps\n    return cloudpickle.dumps(obj, pickle_protocol)\n  File \"/databricks/spark/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 73, in dumps\n    cp.dump(obj)\n  File \"/databricks/spark/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 563, in dump\n    return Pickler.dump(self, obj)\nTypeError: cannot pickle 'psycopg2.extensions.cursor' object\nPicklingError: Could not serialize object: TypeError: cannot pickle 'psycopg2.extensions.cursor' object\n`\n```\nwhile running the below script\n```\n`def get_connection():\n    conn_props = brConnect.value\n    print(conn_props)\n    #extract value from broadcast variables\n    database = conn_props.get(\"database\")\n    user = conn_props.get(\"user\")\n    pwd = conn_props.get(\"password\")\n    host = conn_props.get(\"host\") \n    db_conn = psycopg2.connect(\n                host = host,\n                user = user,\n                password = pwd,\n                database = database,\n                port = 5432\n                )\n    return db_conn\n`\n```\n```\n`def process_partition_up(partition, db_cur):\n    updated_rows = 0\n    try:\n        for row in partition:\n            process_row(row, myq, db_cur)\n    \n    except Exception as e:\n        print(\"Not connected\")\n   \n    return updated_rows \n`\n```\n```\n`def update_final(df, db_cur):\n    df.rdd.coalesce(2).foreachPartition(lambda x: process_partition_up(x, db_cur))\n`\n```\n```\n`def etl_process():\n    for id in ['003']:\n        conn = get_connection()\n        for t in ['email_table']:        \n            query = f'''(select * from public.{t} where id= '{id}') as tab'''\n            df_updated = load_data(query)\n            if df_updated.count() > 0:\n                q1 = insert_ops(df_updated, t) #assume this function returns a insert query\n                query_props = q1\n                sc = spark.sparkContext\n                brConnectQ = sc.broadcast(query_props)\n                db_conn = get_connection()\n                db_cur = db_conn.cursor()\n                update_final(df_updated, db_cur) \n        conn.commit()\n        conn.close()\n`\n```\nExplanation:\n\nHere etl_process() internally calling get_connection() which returns a psycopg2 connection object. After that it's calling a update_final() which takes dataframe and psycopg2 cursor object as an arguments.\nNow update_final() is calling process_partition_up() on each partition(df.rdd.coalesce(2).foreachPartition) which takes dataframe and psycopg2 cursor object as an arguments.\nHere after passing psycopg2 cursor object to the process_partition_up(), I'm not getting cursor object rather I'm getting above error.\n\nCan anyone help me out to resolve this error?\nThank you.",
      "solution": "I think that you don't understand what's happening here.\nYou are creating a database connection in your driver(`etl_process`), and then trying to ship that live connection from the driver, across your network to executor to do the work.(your lambda in foreachPartitions is executed on the executor.)\nThat is what spark is telling you \"cannot pickle 'psycopg2.extensions.cursor'\". (It can't serialize your live connection to the database to ship it to an executor.)\nYou need to call `conn = get_connection()` from inside `process_partition_up`  this will initialize the connection to the database from inside the executor.(And any other book keeping you need to do.)\nFYI: The worst part that I want to call out is that this code will work on your local machine.  This is because it's both the executor and the driver.",
      "question_score": 2,
      "answer_score": 1,
      "created_at": "2022-05-26T11:09:01",
      "url": "https://stackoverflow.com/questions/72389393/how-to-pass-psycopg2-cursor-object-to-foreachpartition-in-pyspark"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 68039548,
      "title": "Django with postgresql, the datetime microsecond&#39;s leading 0 is disappeared",
      "problem": "I'm trying to retrieve `datetime` typed data from postgresql on django.\nBut if the microsecond has leading 0, like below\n```\n`|        datetime_field        |\n|2021-06-07 09:22:13.099866+00 |\n`\n```\nthe result is shown as\n```\n`datetime.datetime(2021, 6, 7, 9, 22, 13, 998660, tzinfo=)\n`\n```\nin Python.\nTake notice of microsecond 099866. It has been changed to 998660.\nIf I insert the resulted datetime object without any change to postgresql, it is uploaded as below.\n```\n`|        datetime_field        |\n| 2021-06-07 09:22:13.99866+00 |\n`\n```\nThe 0 at the head is disappeared.\nIt seems this problem is derived from psycopg2, not just django but I can't find any resolution for this.\nHow can I get the microseconds in its entierty?\np.s. Welcome for editing for just the English. As I'm not an English speaker, I'm not sure whether I wrote correct expressions.\n\nAddition of the exact step.\nI'm doing the job on docker containers. Two containers are used now, each for django server and PostgreSQL server.\n```\n`Version infos:\nPython 3.9.1\nDjango==3.1.4\npsycopg2==2.8.6\npostgres (PostgreSQL) 13.1 (Debian 13.1-1.pgdg100+1)\n`\n```\nI'm testing with a clone of production database.\n\nSELECT query for a problematic data.\n\n```\n`TEST=# SELECT issued_at FROM table_name WHERE id = 153;\n           issued_at\n-------------------------------\n 2021-06-18 10:10:49.075392+00\n(1 row)\n`\n```\n\nIn `python manage.py shell`, retrieve data with `connections.cursor()`\n\n```\n`>>> with transaction.atomic():\n...     query = f'''SELECT issued_at FROM table_name WHERE id = 153;'''\n...     with connections['test_db'].cursor() as cursor: \n...             cursor.execute(query)\n...             print(cursor.fetchone())\n...\n(datetime.datetime(2021, 6, 18, 10, 10, 49, 753920, tzinfo=),)\n`\n```\n\nAddition 2.\nWhen I get data with django's `Model.objects.get()`, the result comes out good.\n```\n`>>> data = TableName.objects.get(id=153)\n>>> data.issued_at\ndatetime.datetime(2021, 6, 18, 10, 10, 49, 75392, tzinfo=)\n`\n```\nIt seems a kind of problem of `django.db.connections`.",
      "solution": "I'm not seeing it:\n```\n`CREATE TABLE public.dt_test (\n    id integer,\n    ts_fld timestamp without time zone,\n    tsz_fld timestamp with time zone\n);\ninsert into dt_test values (1, '2021-06-07 09:22:13.099866+00', '2021-06-07 09:22:13.099866+00');\ninsert into dt_test values (2, '2021-06-07 09:22:13.99866+00', '2021-06-07 09:22:13.99866+00');\nselect * from dt_test ;\n id |           ts_fld           |            tsz_fld            \n----+----------------------------+-------------------------------\n  1 | 2021-06-07 09:22:13.099866 | 2021-06-07 02:22:13.099866-07\n  2 | 2021-06-07 09:22:13.99866  | 2021-06-07 02:22:13.99866-07\n\nimport psycopg2 \ncur.execute('select * from dt_test')\nrs = cur.fetchall()\nrs                                                                                                                                                                        \nOut[23]: \n[(1,\n  datetime.datetime(2021, 6, 7, 9, 22, 13, 99866),\n  datetime.datetime(2021, 6, 7, 2, 22, 13, 99866, tzinfo=psycopg2.tz.FixedOffsetTimezone(offset=-420, name=None))),\n (2,\n  datetime.datetime(2021, 6, 7, 9, 22, 13, 998660),\n  datetime.datetime(2021, 6, 7, 2, 22, 13, 998660, tzinfo=psycopg2.tz.FixedOffsetTimezone(offset=-420, name=None)))]\ncur.execute('insert into dt_test(id, ts_fld) values(%s, %s)', [3, datetime.datetime(2021, 6, 7, 9, 22, 13, 99866)])\ncon.commit()\n select * from dt_test ;\n id |           ts_fld           |            tsz_fld            \n----+----------------------------+-------------------------------\n  1 | 2021-06-07 09:22:13.099866 | 2021-06-07 02:22:13.099866-07\n  2 | 2021-06-07 09:22:13.99866  | 2021-06-07 02:22:13.99866-07\n  3 | 2021-06-07 09:22:13.099866 | NULL\n`\n```\nWhat you are seeing is normalization to 1000000:\n```\n`select 998660/1000000.0;\n        ?column?        \n------------------------\n 0.99866000000000000000\n\nselect 99866/1000000.0;\n        ?column?        \n------------------------\n 0.09986600000000000000\n`\n```",
      "question_score": 2,
      "answer_score": 1,
      "created_at": "2021-06-18T19:52:21",
      "url": "https://stackoverflow.com/questions/68039548/django-with-postgresql-the-datetime-microseconds-leading-0-is-disappeared"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 66969678,
      "title": "What is the proper way to handle Postgres server side errors",
      "problem": "I am using `psycopg2` to query from my Postgres server, this is the code that query:\n`def execute_query(\n        self,\n        query,\n        query_params=None,\n        cursor_factory=psycopg2.extras.NamedTupleCursor,\n    ):\n        try:\n            self.connect()\n            cursor_id = uuid.uuid4().hex\n\n            with self.connection.cursor(\n                cursor_id,\n                cursor_factory=cursor_factory,\n            ) as cursor:\n                cursor.execute(\n                    query,\n                    query_params,\n                )\n\n                yield from cursor\n        finally:\n            self.disconnect()\n`\nSometimes without any server indication (normal CPU & RAM usage), the connection breaks.\nWhen it does, it raises one of the following exceptions:\n```\n`psycopg2.OperationalError: connection pointer is NULL\npsycopg2.InterfaceError: cursor already closed\n`\n```\nI'm not sure if the proper way to handle those specific exceptions is to retry:\n`    def execute_query(\n        self,\n        query,\n        page_size=2000,\n        query_params=None,\n        cursor_factory=psycopg2.extras.NamedTupleCursor,\n    ):\n        try:\n            self.connect()\n            cursor_id = uuid.uuid4().hex\n\n            with self.connection.cursor(\n                cursor_id,\n                cursor_factory=cursor_factory,\n            ) as cursor:\n                cursor.itersize = page_size\n                cursor.execute(\n                    query,\n                    query_params,\n                )\n\n                yield from cursor\n        except (\n            psycopg2.OperationalError,\n            psycopg2.InterfaceError,\n        ):\n            self.execute_query(\n                query=query,\n                page_size=page_size,\n                query_params=query_params,\n                cursor_factory=cursor_factory,\n            )\n        finally:\n            self.disconnect()\n`\nI need to know if there is any way to predict those exceptions without catching it after it already raised\nThanks!",
      "solution": "In my experience, there are no magic tricks when communicating between two servers of some sort. And you cannot predict when they will happen those exceptions. So I will recommend firstly to manage the response when everything breaks. This are more mindset and general design suggestions than a deep dive into psycopg2 code.\nI will add some quick tips I use, I hope this helps :D\nKeep track of your exceptions\nExpect weird outcomes when working with server-client applications, keep in mind that there are tons of errors you cannot possibly control and/or predict, but that does not mean you cannot classify them and escalate your code accordingly. A good thing is to separate your exceptions and handle them one by one.\nHave an unhandled exception behavior in place\nIt could be retry every x seconds, it could be raise it to make it visible, or retry and store the traceback and exception name in a log to classify it later.\nMonitorize your server but do not worry too much about it\nSince it is for internal use, it is good for the sake of learning to try to find the reason behind every error. But there are lots of not so obvious factors that can meddle in your queries like: lan cables are bad, os in server crashes and quickly recovers, noise in wifi connections, concurrency, database size, etc.\nSo have your default error behavior in place and work for your method to perform not to be perfect in the first try.\nTransactions are your best friends\nI will start assuming that your queries fall into one of these two categories:\n\nDQL (Data Query Language): to ask things (SELECT). These are the easy ones\nDML (Data Manipulation Language): to edit data (DELETE, UPDATE, INSERT, ...)\n\n(For more info on types of queries I leave you this link: https://www.geeksforgeeks.org/sql-ddl-dql-dml-dcl-tcl-commands/)\nDML queries should always be enclosed inside a transaction, to avoid inconsistency when an unexpected error comes: Transactions in PostgreSQL\nIf you are modifying data and something crashes midway transactions can help you by undoing what was left unfinished.\nI hope this helps you and have a nice coding experience!!",
      "question_score": 2,
      "answer_score": 1,
      "created_at": "2021-04-06T15:32:45",
      "url": "https://stackoverflow.com/questions/66969678/what-is-the-proper-way-to-handle-postgres-server-side-errors"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 65844613,
      "title": "psycopg2.OperationalError: server closed the connection unexpectedly (Airflow in AWS, connection drops on both sides)",
      "problem": "We have an Airflow instance running in AWS Fargate. It connects to an on-premise Postgres server (on Windows) and tries to load data from a (complicated) view. It uses a `PostgresHook` for that. However, the task in the DAG fails in Airflow with this error:\n```\n`  File \"/usr/local/lib/python3.7/site-packages/airflow/hooks/dbapi_hook.py\", line 120, in get_records\n    cur.execute(sql)\npsycopg2.OperationalError: server closed the connection unexpectedly\n    This probably means the server terminated abnormally\n    before or while processing the request.\n`\n```\nA while ago, the error occurred after some 10-15 minutes. Now, it occurs faster, after 5 minutes or even faster.\nI have looked in the Postgres logs, that shows (confusingly) that it was the client that closed the connection:\n```\n`LOG:  could not send data to client: An existing connection was forcibly closed by the remote host.\nFATAL:  connection to client lost\n`\n```\nI have tried a bunch of potential solutions already.\nWithout Airflow\nConnnecting to the server outside of Airflow, using `psycopg2` directly: works (using the complicated view).\nDifferent table\nTrying to load data from a different table from Airflow in the cloud: works, finishes quickly too. So this \"timeout\" only occurs because the query takes a while.\nRunning the Airflow container locally\nAt first I could reproduce this issue, but I (think I) solved it by adding some extra parameters in the postgres connection string: `keepalives=1&keepalives_idle=60&keepalives_interval=60`. However, I cannot reproduce this fix in the Airflow in the cloud, because when I add these parameters there, the error remains.\nIncrease timeouts\nSee above, I added keepalives, but I also tried to reason about other potential timeouts. I added a timeout `execution_timeout` to the DAG arguments, to no avail. We also checked networking timeouts, but given the irregular pattern of the connection failures, it doesn't really sound like such a hard timeout...\nI am at a loss here. Any suggestions?",
      "solution": "Update: we have solved this problem through a workaround. Instead of keeping the connection open while the complex view is being queried, we have turned the connection into an asynchronous connection (i.e., `aconn = psycopg2.connect(database='test', async=1)` from psycopg docs). Furthermore, we have turned the view into a materialized view, such that we only call a `REFRESH MATERIALIZED VIEW` through the asynchronous connection, and then we can just `SELECT *` on the materialized view a while later, which is very fast.",
      "question_score": 2,
      "answer_score": 1,
      "created_at": "2021-01-22T12:50:37",
      "url": "https://stackoverflow.com/questions/65844613/psycopg2-operationalerror-server-closed-the-connection-unexpectedly-airflow-in"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 73623152,
      "title": "Psycopg2 copy_from() is inserting data with double quotes when whitespace is present in csv file",
      "problem": "I am trying to import the following table to my Postgres server using cursor.copy_from() in psycopg2 because the file is too large.\n\nid\nmail\nname\n\n1\njohn123@gmail.com\nJohn Stokes\n\n2\nemily123@gmail.com\nEmily Ray\n\nHere is my code:\n```\n`import psycopg2\nimport os\n\nconn = psycopg2.connect(\n    dbname = name,\n    user = username,\n    password = pwd,\n    host = hst,\n    port = 5432\n)\n\ncur = conn.cursor()\n\npath = os.getcwd() + '\\users.csv'\n\nfile = open(path, 'r')\n\ncur.copy_from(file, table_name, sep=',')\nconn.commit()\n\nconn.close()\n`\n```\nThis inserts the data to the table but there is double quotes in the third column like below.\n\nid\nmail\nname\n\n1\njohn123@gmail.com\n\"John Stokes\"\n\n2\nemily123@gmail.com\n\"Emily Ray\"\n\nLater I found out that the problem lies in the open() itself. Because if I print the first line by doing `file.readline()`, I get:\n\n1,john123@gmail.com,\"John Stokes\"\n\nI don't want these double quotes in my table. I tried using cursor.execute() with `COPY FROM` query but it says that I am not a superuser even if I am.",
      "solution": "Use copy_expert. Then you are not working as the server user but as the client user. Also you can use `WITH CSV` which will take care of the quoting. `copy_from` and `copy_to` work using the text format as described here COPY.\n```\n`cat test.csv                                                                                                                                                            \n1,john123@gmail.com,\"John Stokes\"\n2,emily123@gmail.com,\"Emily Ray\"\n\ncreate table test_csv (id integer, mail varchar, name varchar);\n\nimport psycopg2\n\ncon = psycopg2.connect(dbname=\"test\", host='localhost', user='postgres', port=5432)\ncur = con.cursor()\n\nwith open('test.csv') as f:\n    cur.copy_expert('COPY test_csv FROM stdin WITH CSV', f)\n\ncon.commit()\n\nselect * from test_csv ;\n id |        mail        |    name     \n----+--------------------+-------------\n  1 | john123@gmail.com  | John Stokes\n  2 | emily123@gmail.com | Emily Ray\n\n`\n```\nFYI, in `psycopg3(psycopg)` this behavior has changed substantially. See here psycopg3 COPY for how to handle in that case.\nUPDATE\nUsing `psycopg3` the answer for Python 3.8+ where the walrus operator is available would be:\n```\n`import psycopg\n\nwith open('test.csv') as f:\n    with cur.copy(\"COPY test_csv FROM STDIN WITH CSV\") as copy:\n        while data := f.read(1000):\n            copy.write(data)\ncon.commit()\n`\n```\nOr using Python 3.7-, something like:\n```\n`# Function copied from here https://www.iditect.com/guide/python/python_howto_read_big_file_in_chunks.html\ndef read_in_chunks(file, chunk_size=1024*10):  # Default chunk size: 10k.\n    while True:\n        chunk = file.read(chunk_size)\n        if chunk:\n            yield chunk\n        else: # The chunk was empty, which means we're at the end of the file\n            return\n\nwith open('test.csv') as f:\n    with cur.copy(\"COPY test_csv FROM STDIN WITH CSV\") as copy:\n        for chunk in read_in_chunks(f):\n            copy.write(chunk)\n\ncon.commit()\n\n`\n```",
      "question_score": 1,
      "answer_score": 7,
      "created_at": "2022-09-06T15:45:35",
      "url": "https://stackoverflow.com/questions/73623152/psycopg2-copy-from-is-inserting-data-with-double-quotes-when-whitespace-is-pre"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 72555603,
      "title": "Run psycopg version 3 on Lambda",
      "problem": "I'm using the lambda docker base image for python3.9\n`FROM public.ecr.aws/lambda/python:3.9`\nAnd I'm trying to use psycopg in my code. Here is a minimum reproducible example:\nDockerfile\n```\n`# This works!\n# FROM python:3.9\n\n# This doesn't work\nFROM public.ecr.aws/lambda/python:3.9 \n\nRUN pip install psycopg\n\nCOPY . . \n\nENTRYPOINT [\"python\", \"script.py\"]\n\n`\n```\nscript.py\n```\n`# I'm running these docker commands\n# docker build -t test .\n# docker rune test\n\nimport psycopg\n\npsycopg.connect(\"host=localhost\")\n\nprint('hello world')\n`\n```\nUsing the python3.9 base image I see the correct output. Using the AWS lambda base image I see:\n```\n`Traceback (most recent call last):\n  File \"/var/task/script.py\", line 4, in \n    import psycopg\n  File \"/var/lang/lib/python3.9/site-packages/psycopg/__init__.py\", line 9, in \n    from . import pq  # noqa: F401 import early to stabilize side effects\n  File \"/var/lang/lib/python3.9/site-packages/psycopg/pq/__init__.py\", line 114, in \n    import_from_libpq()\n  File \"/var/lang/lib/python3.9/site-packages/psycopg/pq/__init__.py\", line 106, in import_from_libpq\n    raise ImportError(\nImportError: no pq wrapper available.\nAttempts made:\n - couldn't import psycopg 'c' implementation: No module named 'psycopg_c'\n - couldn't import psycopg 'binary' implementation: No module named 'psycopg_binary'\n - couldn't import psycopg 'python' implementation: /lib64/libpq.so.5: undefined symbol: PQconninfo\n`\n```\nI've tried installing psycopg-binary instead but my new error there is:\n`Traceback (most recent call last): File \"/var/task/script.py\", line 4, in  import psycopg ModuleNotFoundError: No module named 'psycopg'`\nI'd like to find a way to use psycopg in a Lambda function. We have older functions using psycopg2 but the lack of maintenance is worrying (psycopg appears more active + supports postgres v14+ and python3.9+) so we'd like to move to psycopg (which now has a version 3 released).\nThanks for your help in advance",
      "solution": "We ditched psycopg entirely and moved to `pg8000` which is a pure python implementation and all of the code we wrote was able to port over easily. This way we don't need to worry about binaries or other nonsense.\nhttps://github.com/tlocke/pg8000\nThen the simple `pip install pg8000` is all our Dockerfile needed and `import pg8000.native` in our `script.py` file.",
      "question_score": 1,
      "answer_score": 5,
      "created_at": "2022-06-09T08:27:14",
      "url": "https://stackoverflow.com/questions/72555603/run-psycopg-version-3-on-lambda"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 69194772,
      "title": "postgres delete query for batch using python",
      "problem": "I want to do the bulk delete in postgres using python and my where clause has multiple columns to fetch the correct record.\nEx:\nFor One records i ran this\ndelete products where company_id='123' and product_id=1;\nI tried this for multiple records but getting this error\n\nthe query contains more than one '%s' placeholder\n\n```\n`query = delete products where company_id='%s' and product_id=%s; \nvalues =[(1,2),(3,4)]\npsycopg2.extras.execute_values(self.cursor, delete_query, values)\n`\n```",
      "solution": "I see a couple of issues with the snippet you shared\n\nthe delete syntax in postgresql is `delete from  where ...`\ncompany_id seems to be a string but in values its expressed as an integer.\n\nyou can either execute multiple queries to delete the records or you can pass a list of values to compare against the compound field `(company_id, product_id)` & use `execute_values`\nassuming company_id is text & your values list contains strings\nmultiple queries:\n`stmt = \"delete from products where company_id = %s and product_id = %s\"\ncur.execute('begin')\ntry:\n    for cid, pid in values:\n      cur.execute(stmt, (cid, pid))\n    cur.execute('commit')\n    # do other things\nexcept:\n    cur.execute('rollback')\n    # do other things to handle this exception\n`\none query + execute_values\n`from postgresql.extras import execute_values\n\nstmt = \"delete from products where (company_id, product_id) IN (%s)\"\nexecute_values(cur, stmt, values)\n`\nThe `psycopg2.extras` documentation page contains lots of helpful functions, including documentations for `execute_values`",
      "question_score": 1,
      "answer_score": 3,
      "created_at": "2021-09-15T16:09:56",
      "url": "https://stackoverflow.com/questions/69194772/postgres-delete-query-for-batch-using-python"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 73741546,
      "title": "can not connect with docker postgress sql container with psycopg2",
      "problem": "I am running a Postgres SQL database container with following command:\n```\n`docker run  --name db -p 5432:5432 -e POSTGRES_USER=postgres -e POSTGRES_PASSWORD=postgres -v pg\n`\n```\nOf course I have changed the 'localhost' to 'db' since I am trying to connect with this container.\nwhen I try to connect to the container database I get the following error:\n```\n`psycopg2.OperationalError: could not translate host name \"db\" to address: Name or service not known\n`\n```\nI cant use here Docker compose in this context ( I know how to run it though ).\nWhat else I need to add in my docker command so that I can connect from python ?",
      "solution": "Of course I have changed the 'localhost' to 'db' since I am trying to connect with this container.\n\nNo, you don't, your dockerfile is exposing the port 5432 to the host machine as stated by the flag `-p 5432:5432`\nSo if, you are trying to connect to the docker from your host machine, yoi will use the host `localhost`\nI think you are confusing between docker and docker networking when we have multiple docker trying to communicate with each other as is the case is with docker-compose.\nIn case of docker-compose, when you have multiple services running, they can communicate with each other using the docker containers name as the host. Similar if you have a network between docker containers, they can communicate with each other using the docker name as the host\nSo if it was docker-compose, with the docker running on one container, and your app in another, in that case you would replace localhost with db.\nHope that clarifies things",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2022-09-16T09:46:34",
      "url": "https://stackoverflow.com/questions/73741546/can-not-connect-with-docker-postgress-sql-container-with-psycopg2"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 73532745,
      "title": "psycopg2 return missing &quot;id&quot; primary key column instead of generate itself",
      "problem": "Hello I have the following model:\n```\n`@dataclass\nclass User(Base, TimestampMixin):\n    __tablename__ = \"users\"\n    email: str\n    password: int\n    name: str\n    id: Optional[int] = None\n\n    id = Column(UUID(as_uuid=True), primary_key=True)\n    email = Column(String(50), unique=True, nullable=False)\n    password = Column(String(20), nullable=False)\n    name = Column(String(15), unique=True, nullable=False)\n`\n```\nWhen I try to generate a new user with the following code:\n```\n`def insert_user(session: Session, name: str, email: str, password: str) -> None:\n    try:\n        user = User(name=name, email=email, password=password)\n        session.add(user)\n    except:\n        session.rollback()\n        raise InsertUserError\n    else:\n        session.commit()\n`\n```\nI get the following error:\n```\n`ERROR in handlers: (psycopg2.errors.InvalidForeignKey) missing \"id\" primary key column\n`\n```\nI wasn't put the id because I guessed that psycopg2 will generate it itself, but it didn't.\nHow can I create that field automatically?",
      "solution": "Just because a column is marked as a primary key, doesn't mean it has a default value. SQLAlchemy will automatically produce the correct SQL to auto-generate primary keys only when using an integer-based primary key column. For UUID primary keys, you are responsible for defining a default.\nYou can use a Python default:\n```\n`id = db.Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)\n`\n```\n(where you imported the `uuid` module). This default is used only when you create SQLAlchemy model instances, not when you insert into the table using SQL generated elsewhere.\nIf you have installed the `uuid-osp` module (e.g. `create extension uuid-ossp`) and want to use the `uuid_generate_v4` function to produce the default on the server, you'd use `server_default` parameter:\n```\n`id = Column(\n    UUID(as_uuid=True),\n    primary_key=True,\n    server_default=func.uuid_generate_v4(),\n)\n`\n```\nwhere `func` is `sqlalchemy.sql.expression.func` (`from sqlalchemy.sql import func` should suffice).",
      "question_score": 1,
      "answer_score": 4,
      "created_at": "2022-08-29T19:47:32",
      "url": "https://stackoverflow.com/questions/73532745/psycopg2-return-missing-id-primary-key-column-instead-of-generate-itself"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 71197070,
      "title": "What is the Postgres _text type?",
      "problem": "I have a Postgres table with a `_text` type (note the underscore) and am unable to determine how to insert the string `[]` into that table.\nHere is my table definition:\n```\n`CREATE TABLE public.newtable (\n    column1 _text NULL\n);\n\n`\n```\nI have the `postgis` extension enabled:\n```\n`CREATE EXTENSION IF NOT EXISTS postgis;\n`\n```\nAnd my python code:\n```\n`conn = psycopg2.connect()\nconn.autocommit = True\ncur = conn.cursor(cursor_factory=psycopg2.extras.DictCursor)\n\nrows = [(\"[]\",)]\ninsert_query = f\"INSERT INTO newtable (column1) values %s\"\npsycopg2.extras.execute_values(cur, insert_query, rows, template=None, page_size=100)\n`\n```\nThis returns the following error:\n```\n`psycopg2.errors.InvalidTextRepresentation: malformed array literal: \"[]\"\nLINE 1: INSERT INTO newtable (column1) values ('[]')\n                                               ^\nDETAIL:  \"[\" must introduce explicitly-specified array dimensions.\n`\n```\nHow can I insert this data? What does this error mean? And what is a `_text` type in Postgres?",
      "solution": "Pulling my comments together:\n```\n`CREATE TABLE public.newtable (\n    column1 _text NULL\n);\n\n--_text gets transformed into text[]\n\n\\d newtable \n              Table \"public.newtable\"\n Column  |  Type  | Collation | Nullable | Default \n---------+--------+-----------+----------+---------\n column1 | text[] |           |          | \n\ninsert into newtable values ('{}');\n\nselect * from newtable ;\n column1 \n---------\n {}\n`\n```\nIn Python:\n```\n`import psycopg2\ncon = psycopg2.connect(dbname=\"test\", host='localhost', user='postgres')\ncur = con.cursor()\ncur.execute(\"insert into newtable values ('{}')\")\ncon.commit()\ncur.execute(\"select * from newtable\")\ncur.fetchone()\n([],)\ncur.execute(\"truncate newtable\")\ncon.commit()\ncur.execute(\"insert into newtable values (%s)\", [[]])\ncon.commit()\ncur.execute(\"select * from newtable\")\ncur.fetchone()                                                                                                                                                            \n([],)\n\n`\n```\nFrom the `psycopg2` docs Type adaption Postgres arrays are adapted to Python lists and vice versa.\nUPDATE\nFinding `_text` type in Postgres system catalog pg_type. In `psql`:\n```\n`\\x\nExpanded display is on.\n\nselect * from pg_type where typname = '_text';\n-[ RECORD 1 ]--+-----------------\noid            | 1009\ntypname        | _text\ntypnamespace   | 11\ntypowner       | 10\ntyplen         | -1\ntypbyval       | f\ntyptype        | b\ntypcategory    | A\ntypispreferred | f\ntypisdefined   | t\ntypdelim       | ,\ntyprelid       | 0\ntypelem        | 25\ntyparray       | 0\ntypinput       | array_in\ntypoutput      | array_out\ntypreceive     | array_recv\ntypsend        | array_send\ntypmodin       | -\ntypmodout      | -\ntypanalyze     | array_typanalyze\ntypalign       | i\ntypstorage     | x\ntypnotnull     | f\ntypbasetype    | 0\ntyptypmod      | -1\ntypndims       | 0\ntypcollation   | 100\ntypdefaultbin  | NULL\ntypdefault     | NULL\ntypacl         | NULL\n\n`\n```\nRefer to the `pg_type` link above to get information on what the columns refer to. The `typcategory` of `A` as mapped in \"Table 52.63. typcategory Codes Code   Category A  Array types\" at the link is one clue. As well as `typinput`, `typoutput`, etc values.",
      "question_score": 1,
      "answer_score": 4,
      "created_at": "2022-02-20T18:44:02",
      "url": "https://stackoverflow.com/questions/71197070/what-is-the-postgres-text-type"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 70051611,
      "title": "Connect to postgres container from another running container",
      "problem": "I am struggling to connect to a running container with a postgres database from another container. I can connect when running locally, but I suspect that there is some networking issue that I'm overlooking when trying to connect from another container. The specific error I am currently seeing is `psycopg2.OperationalError: could not translate host name \"my_network\" to address: Temporary failure in name resolution`\nHere is my docker-compose:\n```\n`version: '3'                                                                                       \nservices:                                                                                          \n  data_collection:                                                                                 \n    build: ./docker/data_collection                                                                \n    ports:                                                                                         \n      - \"8888:8888\"                                                                                \n      - \"6006:6006\"                                                                                \n      - \"8000:8000\"                                                                                                           \n    networks:                                                                                      \n      - my_network                                                                                 \n    depends_on:                                                                                    \n      - db                                                                                         \n  db:                                                                                              \n    image: 'postgres:13.2-alpine'                                                                  \n    ports: \n      - \"5432:5432\"\n    environment:\n      - POSTGRES_USER=postgres\n      - POSTGRES_PASSWORD=mypassword\n      - POSTGRES_DB=spotify\n    volumes:\n      - db-data:/var/lib/postgresql/data # persist data even if container shuts down               \n    networks:\n      - my_network                                                                                 \n                                                                                                   volumes:                                                                                           \n  db-data: # names volumes can be managed easier using docker-compose                                                                                                                                 \nnetworks:                                                                                          \n  my_network:                                                                                      \n~              \n`\n```\nI then enter a shell in my `data_colleciton` container and try to connect by reading those params into a variables `params`, which gets passed in like this:\n```\n`import psycopg2\n\nparams = dict(host='my_network', database='spotify', user='postgres', password='mypassword', port='5432')\nconn = psycopg2.connect(**params)\n`\n```\nHowever, running this locally and replacing the host above with localhost does work as expected",
      "solution": "Change the `host='my_network'` to `host='db'`. You are not connecting to the network, you are connecting to a specific host in that network.",
      "question_score": 1,
      "answer_score": 4,
      "created_at": "2021-11-21T04:32:10",
      "url": "https://stackoverflow.com/questions/70051611/connect-to-postgres-container-from-another-running-container"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 69668255,
      "title": "Insert returns &quot;the last operation didn&#39;t produce a result&quot;",
      "problem": "For the following code:\n```\n`with psycopg.connect(host=\"host_name\", dbname=\"db_name\", user=\"user\", password=\"pass\") as conn:\n  with conn.cursor() as cur:\n    cur.execute(\"SELECT * FROM table_name\")\n    res = cur.fetchall()\n    print(res)\n`\n```\nEverything is working fine, but if I try to insert values by replacing\n```\n`cur.execute(\"SELECT * FROM table_name\")\n`\n```\nwith\n```\n`cur.execute(\"INSERT INTO table_name (column_1, column_2) VALUES (%s, %s)\", (\"a\",\"b\"))\n`\n```\nI am getting the following error \"the last operation didn't produce a result\" what can cause it?",
      "solution": "Two ways to handle this:\n```\n`import psycopg2\ncon = psycopg2.connect(database=\"test\", host='localhost', user='postgres')\ncur = con.cursor()\ncur.execute(\"insert into animals values(24, 'fair', 'cat')\")\ncur.rowcount\n1\n\ncur.execute(\"insert into animals values(34, 'good', 'dog') returning id\")\ncur.fetchone()\n(34,)\n\n`\n```\nThe first way returns the number of rows affected by the operation per Cursor.\nThe second way returns the `id` field so there is actually a result.",
      "question_score": 1,
      "answer_score": 4,
      "created_at": "2021-10-21T22:15:27",
      "url": "https://stackoverflow.com/questions/69668255/insert-returns-the-last-operation-didnt-produce-a-result"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 69582029,
      "title": "Python list to PostgreSQL HSTORE via SQLAlchemy",
      "problem": "I'd like to store Python dicts containing lists as HSTORE object in a PostgreSQL database using SQLAlchemy. Following my table class.\n```\n`from sqlalchemy.dialects.postgresql import HSTORE\nfrom sqlalchemy import Column, String\nfrom sqlalchemy.ext.declarative import declarative_base\n\nBase = declarative_base()\n\nclass Data(Base):\n    id = Column(String, primary_key=True)\n    data = Column(HSTORE)\n`\n```\nI insert my items in the following matter.\n```\n`data = Data(\n    id='fancy_id',\n    data={\n        'foo': ['bar', 'bar']\n    },\n)\n\ndb.Session.add(scan)\ndb.Session.commit()\n`\n```\nThis causes a ROLLBACK to happen and the following exception raises.\n```\n`sqlalchemy.exc.DataError: (psycopg2.errors.ArraySubscriptError) wrong number of array subscripts\n\n[SQL: INSERT INTO data (id, data) VALUES (%(id)s, %(data)s)]\n[parameters: {'id': 'fancy_id', 'data': {'foo': ['bar', 'bar']}}]\n`\n```\nHowever, the insertion works without the list.\n```\n`data = Data(\n    id='fancy_id',\n    data={\n        'foo': 'bar'\n    },\n)\n`\n```\nI followed the PostgreSQL SQLAlchemy 1.4 documentation and can't spot a note indicating this limitation. Am I missing something? How can I store Python lists inside PostgreSQL HSTORE objects via SQLAlchemy?\nThanks in advance.",
      "solution": "Honestly for this sort of thing `json/jsonb` would be a better option. Then you would have a direct mapping of Python dict with list to JSON object with array. If you want to use `hstore` then:\n```\n`create table hstore_test(id int, hs_fld hstore);\ninsert into hstore_test values (1, 'a=>1');\ninsert into hstore_test values (1, 'b=>\"[1,2]\"'::hstore);\nselect * from hstore_test ;\n id |    hs_fld    \n----+--------------\n  1 | \"a\"=>\"1\"\n  1 | \"b\"=>\"[1,2]\"\n\n--So for your example\n\ndata={\n        'foo': \"['bar', 'bar']\"\n    },\n\n`\n```",
      "question_score": 1,
      "answer_score": 4,
      "created_at": "2021-10-15T10:40:51",
      "url": "https://stackoverflow.com/questions/69582029/python-list-to-postgresql-hstore-via-sqlalchemy"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 66942285,
      "title": "Connecting to Google Cloud SQL Postgres instance using psycopg2",
      "problem": "I am trying to connect to Google Cloud SQL Postgres Using psycopg2.\nI have created an postgreSQL instance and using the default database `postgres` for now.\nI am able to connect from pgadmin tool as well as from the gcloud shell and the queries give expected result.\nI have developed a flask application and deploying on standard app engine.\n`conn = psycopg2.connect(database=\"postgres\", user = \"postgres\", password = \"password\", host = \"/cloudsql/my-new-db\")`\nAnd when I run it, the get `psycopg2.OperationalError: could not connect to server: No such file or directory` error.\nI have a hunch that host value is not correct. I tried various options like `/cloudsql/..`\nBut, nothing seems to be working. What else should I be doing to remove this error?",
      "solution": "As mentioned in this article on connecting to Cloud SQL from app engine:\nConnecting with Unix sockets\nOnce correctly configured, you can connect your service to your Cloud SQL instance's Unix domain socket accessed on the environment's filesystem at the following path: /cloudsql/INSTANCE_CONNECTION_NAME.\nThe INSTANCE_CONNECTION_NAME can be found on the Overview page for your instance in the Google Cloud Console or by running the following command:\n```\n`    gcloud sql instances describe [INSTANCE_NAME]\n`\n```\nedit: formatting",
      "question_score": 1,
      "answer_score": 1,
      "created_at": "2021-04-04T16:29:02",
      "url": "https://stackoverflow.com/questions/66942285/connecting-to-google-cloud-sql-postgres-instance-using-psycopg2"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 78909467,
      "title": "Django venv finding old version of PostgreSQL -&gt; django.db.utils.NotSupportedError: PostgreSQL 13 or later is required",
      "problem": "I'm resurrecting my local environment for a Django project that I haven't run locally in 2 years, working through issues around things that have gone stale. But I have one a little different: it looks like Django is finding/using an older version of PostgreSQL than the version I see in the venv itself. What's a good approach for tracking down old versions so I can remove them?\nWhen I run `python mysite/manage.py runserver`, I get\n```\n`django.db.utils.NotSupportedError: PostgreSQL 13 or later is required (found 10.13).\n`\n```\nBUT when I check package versions in the venv I'm running, most packages are current, and PostgreSQL is 3.12.5 (not 13 or later like we'll ultimately need, but also not 10.13).\n\n(from pip list) Django 5.1\n\n(from pip list) psycopg2             2.9.9\n\n(from pip list) psycopg2-binary      2.9.9\n\n(from pip list) psycopg2-pool        1.2\n\npsql -V gives: psql (PostgreSQL) 12.3\n\npython -v gives: Python 3.12.5\n\nUnsurprisingly, if I try a naive uninstall from the venv (`pip uninstall postgresql-10.13`), it says it's not installed.\nWhat's a good approach for tracing where that 10.13 could be coming from?\nLooking in the stack trace this NotSupportedError is raised while connecting to the database, from  .venv/lib/python3.12/site-packages/django/db/backends/base/base.py\", line 200, in check_database_version_supported\nFrom the venv, my $PATH variable has:\n\n/Users/dkaplan/.vscode/extensions/ms-python.python-2024.12.3-darwin-x64/python_files/deactivate/bash:/Users/dkaplan/family-django/.venv/bin:/Library/Frameworks/Python.framework/Versions/3.12/bin:/Library/Frameworks/Python.framework/Versions/3.12/bin:/Library/Frameworks/Python.framework/Versions/3.10/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/Users/dkaplan/.m2:/Applications/Postgres.app/Contents/Versions/latest/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/local/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/appleinternal/bin:/Users/dkaplan/.vscode/extensions/ms-python.python-2024.12.3-darwin-x64/python_files/deactivate/bash:/Users/dkaplan/family-django/.venv/bin:/Library/Frameworks/Python.framework/Versions/3.12/bin:/Library/Frameworks/Python.framework/Versions/3.10/bin\n\nMy default databases settings is:\n```\n`DATABASES = {\n    \"default\": {\n        \"ENGINE\": \"django.db.backends.postgresql_psycopg2\",\n        \"NAME\": DB_DATABASE,\n        \"USER\": DB_USER,\n        \"PASSWORD\": DB_PASSWORD,\n        \"HOST\": DB_HOST,\n        \"PORT\": \"5432\",\n        \"OPTIONS\": DB_OPTIONS,\n    }\n}\n`\n```",
      "solution": "Basically you need to upgrade PostgreSQL\ndig into pg_upgrade\nhere are docs\nhttps://www.postgresql.org/docs/current/pgupgrade.html\nthe problem occures because of  postgresql database, that is on your machine. It is old :)\nIt is not a python package, pip won't help here.\nOr you can consider dumping db with pg_dump, using something like docker compose version of postgres and django-dbbackup package to restore db",
      "question_score": 1,
      "answer_score": 4,
      "created_at": "2024-08-24T18:20:33",
      "url": "https://stackoverflow.com/questions/78909467/django-venv-finding-old-version-of-postgresql-django-db-utils-notsupportederr"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 78802634,
      "title": "inserting values in database from sentiment analysis",
      "problem": "I have little  project created by my self:\nuser  is entering some comments on  website,  then using flask+python  we are getting those text, send it to sentiment analysis with help of transformers and all results(text, label , score) is inserted into  `Postgresql`\nthis project is divided into two main part, first i have created empty table in database :\n```\n`CREATE TABLE Message_sentiment (\n    mytext text,\n    text_label text,\n    score numeric\n);\n`\n```\nBased on second part, here is my little code :\n```\n`if request.method=='POST':\n    text =request.form.get('user_comment')\n    label =model(text)[0]['label']\n    score =model(text)[0]['score']\n    # print(f'{text} has  following label {label} with score {score}')\n    curr.execute(\"\"\"INSERT INTO message_sentiment(mytext,text_label,score) VALUES\n    (text,label, score)\n    \"\"\")\n`\n```\nwhen i am running this code , getting following error :\n```\n`UndefinedColumn\npsycopg2.errors.UndefinedColumn: column \"text\" does not exist\nLINE 2:         (text,label, score)\n                 ^\nHINT:  Perhaps you meant to reference the column \"message_sentiment.mytext\".\n`\n```\nso i think problem is here :\n```\n` curr.execute(\"\"\"INSERT INTO message_sentiment(mytext,text_label,score) VALUES\n        (text,label, score)\n        \"\"\")\n`\n```\ni can't  use {} brackets, please help me how to modify this line? thanks in advance",
      "solution": "You should always used parameters when insert data, or querying or uodating\n```\n`curr.execute(\"\"\"\n    INSERT INTO message_sentiment(mytext,text_label,score)\n    VALUES (%s, %s, %s);\n    \"\"\",\n    (text,label, score))\n`\n```",
      "question_score": 1,
      "answer_score": 3,
      "created_at": "2024-07-28T00:57:48",
      "url": "https://stackoverflow.com/questions/78802634/inserting-values-in-database-from-sentiment-analysis"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 73660822,
      "title": "How to solve &quot;NameError: name &#39;basestring&#39; is not defined&quot; when importing psycopg2?",
      "problem": "I am trying to import the psycopg2 package on python 3.7.13 but I get the following error:\n`python -c \"import psycopg2\"                                                                    % 11:22:56\nTraceback (most recent call last):\n  File \"\", line 1, in \n  File \"/Users/mm/.pyenv/versions/3.7.13/lib/python3.7/site-packages/psycopg2/__init__.py\", line 68, in \n    from psycopg2 import extensions as _ext\n  File \"/Users/mm/.pyenv/versions/3.7.13/lib/python3.7/site-packages/psycopg2/extensions.py\", line 211, in \n    from psycopg2. _range import Range                              # noqa\n  File \"/Users/mm/.pyenv/versions/3.7.13/lib/python3.7/site-packages/psycopg2/_range.py\", line 503, in \n    oid=3904, subtype_oid=23, array_oid=3905)\n  File \"/Users/mm/.pyenv/versions/3.7.13/lib/python3.7/site-packages/psycopg2/_range.py\", line 283, in __init__\n    self._create_ranges(pgrange, pyrange)\n  File \"/Users/mm/.pyenv/versions/3.7.13/lib/python3.7/site-packages/psycopg2/_range.py\", line 302, in _create_ranges\n    if isinstance(pgrange, basestring):\nNameError: name 'basestring' is not define\n`\nI am wondering how can I solve it ?\nI already tried downgrading pip to 20.3 after seeing this issue https://github.com/psycopg/psycopg2/issues/1419.",
      "solution": "Resolved this by manually transforming psycopg2 package to a python 3 compatible version running:\n```\n`pip install 2to3\n2to3 ~/.pyenv/versions/3.7.13/lib/python3.7/site-packages/psycopg2/ -w \n`\n```\nStill have no idea why this error occured though.",
      "question_score": 1,
      "answer_score": 3,
      "created_at": "2022-09-09T12:34:15",
      "url": "https://stackoverflow.com/questions/73660822/how-to-solve-nameerror-name-basestring-is-not-defined-when-importing-psycop"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 73334408,
      "title": "Query String Composition in Psycopg2",
      "problem": "I am trying to run a SQL \"SELECT\" query in Postgres from  Python using Psycopg2. I am trying to compose the query string as below, but getting error message, using psycopg2 version 2.9.\n```\n`from psycopg2 import sql\n\ntablename = \"mytab\"\nschema = \"public\"\nquery = sql.SQL(\"SELECT table_name from information_schema.tables where table_name = {tablename} and table_schema = {schema};\")\nquery = query.format(tablename=sql.Identifier(tablename), schema=sql.Identifier(schema))\ncursor.execute(query)\nresult = cursor.fetchone()[0]\n`\n```\nError:\n```\n`psycopg2.error.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block\n`\n```\nCan someone please help. Thanks.",
      "solution": "In the (a bit strange) query\n`select table_name \nfrom information_schema.tables \nwhere table_name = 'mytab'\nand table_schema = 'public';\n`\n`'mytab'` and `'public'` are literals, not identifiers. For comparison, `mytab` is an identifier here:\n`select *\nfrom mytab;\n`\nThus your `format` statement should look like this:\n`query = query.format(tablename=sql.Literal(tablename), schema=sql.Literal(schema))\n`\nNote that the quoted error message is somewhat misleading as it is about executing a query other than what is shown in the question.",
      "question_score": 1,
      "answer_score": 3,
      "created_at": "2022-08-12T14:44:54",
      "url": "https://stackoverflow.com/questions/73334408/query-string-composition-in-psycopg2"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 73028024,
      "title": "Why do I get this error when I deploy a Django App in Railway?",
      "problem": "There is a problem when I deploy a Django app in Railway, Railway is an infrastructure platform that is very similar to Heroku and it uses debian, this is the error that is in the logs.\n```\n`#10 16.69   Using cached selenium-4.1.0-py3-none-any.whl (958 kB)\n#10 16.75 Collecting six==1.16.0\n \n#10 16.75   Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\n#10 16.78 Collecting sniffio==1.2.0\n#10 16.78   Using cached sniffio-1.2.0-py3-none-any.whl (10 kB)\n#10 16.83 Collecting sortedcontainers==2.4.0\n#10 16.83   Using cached sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n#10 16.89 Collecting soupsieve==2.2.1\n \n#10 16.89   Using cached soupsieve-2.2.1-py3-none-any.whl (33 kB)\n#10 16.93 Collecting sqlparse==0.4.2\n#10 16.93   Using cached sqlparse-0.4.2-py3-none-any.whl (42 kB)\n \n#10 17.07 Collecting tqdm==4.62.3\n#10 17.08   Using cached tqdm-4.62.3-py2.py3-none-any.whl (76 kB)\n#10 17.12 Collecting trio==0.19.0\n#10 17.12   Using cached trio-0.19.0-py3-none-any.whl (356 kB)\n#10 17.16 Collecting trio-websocket==0.9.2\n \n#10 17.16   Using cached trio_websocket-0.9.2-py3-none-any.whl (16 kB)\n#10 17.23 Collecting twine==3.4.2\n#10 17.24   Using cached twine-3.4.2-py3-none-any.whl (34 kB)\n#10 17.28 Collecting tzdata==2022.1\n \n#10 17.29   Using cached tzdata-2022.1-py2.py3-none-any.whl (339 kB)\n#10 17.38 Collecting urllib3==1.26.6\n#10 17.38   Using cached urllib3-1.26.6-py2.py3-none-any.whl (138 kB)\n#10 17.41 Collecting webencodings==0.5.1\n \n#10 17.42   Using cached webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n#10 17.47 Collecting whitenoise==6.2.0\n#10 17.47   Using cached whitenoise-6.2.0-py3-none-any.whl (19 kB)\n#10 17.51 Collecting wsproto==1.0.0\n#10 17.51   Using cached wsproto-1.0.0-py3-none-any.whl (24 kB)\n \n#10 17.57 Collecting zipp==3.6.0\n#10 17.57   Using cached zipp-3.6.0-py3-none-any.whl (5.3 kB)\n \n#10 18.00 Collecting psycopg2\n \n#10 18.00   Using cached psycopg2-2.9.3.tar.gz (380 kB)\n#10 18.09   Preparing metadata (setup.py): started\n \n#10 18.38   Preparing metadata (setup.py): finished with status 'error'\n#10 18.39   error: subprocess-exited-with-error\n#10 18.39\n#10 18.39   \u00d7 python setup.py egg_info did not run successfully.\n#10 18.39   \u2502 exit code: 1\n#10 18.39   \u2570\u2500> [23 lines of output]\n#10 18.39       running egg_info\n#10 18.39       creating /tmp/pip-pip-egg-info-bw0tcpyi/psycopg2.egg-info\n#10 18.39       writing /tmp/pip-pip-egg-info-bw0tcpyi/psycopg2.egg-info/PKG-INFO\n#10 18.39       writing dependency_links to /tmp/pip-pip-egg-info-bw0tcpyi/psycopg2.egg-info/dependency_links.txt\n#10 18.39       writing top-level names to /tmp/pip-pip-egg-info-bw0tcpyi/psycopg2.egg-info/top_level.txt\n#10 18.39       writing manifest file '/tmp/pip-pip-egg-info-bw0tcpyi/psycopg2.egg-info/SOURCES.txt'\n#10 18.39\n#10 18.39       Error: pg_config executable not found.\n#10 18.39\n#10 18.39       pg_config is required to build psycopg2 from source.  Please add the directory\n#10 18.39       containing pg_config to the $PATH or specify the full executable path with the\n#10 18.39       option:\n#10 18.39\n#10 18.39           python setup.py build_ext --pg-config /path/to/pg_config build ...\n#10 18.39\n#10 18.39       or with the pg_config option in 'setup.cfg'.\n#10 18.39\n#10 18.39       If you prefer to avoid building psycopg2 from source, please install the PyPI\n#10 18.39       'psycopg2-binary' package instead.\n#10 18.39\n#10 18.39       For further information please check the 'doc/src/install.rst' file (also at\n#10 18.39       ;).\n#10 18.39\n#10 18.39       [end of output]\n#10 18.39\n#10 18.39   note: This error originates from a subprocess, and is likely not a problem with pip.\n#10 18.39 error: metadata-generation-failed\n#10 18.39\n#10 18.39 \u00d7 Encountered error while generating package metadata.\n \n#10 18.39 \u2570\u2500> See above for output.\n#10 18.39\n#10 18.39 note: This is an issue with the package mentioned above, not pip.\n#10 18.39 hint: See above for details.\n#10 18.40 WARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\n#10 18.40 You should consider upgrading via the '/opt/venv/bin/python -m pip install --upgrade pip' command.\n \n#10 ERROR: executor failed running [/bin/bash -ol pipefail -c python -m venv /opt/venv && . /opt/venv/bin/activate && pip install -r requirements.txt]: exit code: 1\n-----\n> [stage-0 6/9] RUN --mount=type=cache,id=s/d3b41cd2-3817-4a81-ba7d-ff021a7d5a6f-/root/.cache/pip,target=/root/.cache/pip python -m venv /opt/venv && . /opt/venv/bin/activate && pip install -r requirements.txt:\n-----\nexecutor failed running [/bin/bash -ol pipefail -c python -m venv /opt/venv && . /opt/venv/bin/activate && pip install -r requirements.txt]: exit code: 1\n \nError: Docker build failed\n`\n```\nThe error could be \"Error: pg_config executable not found.\", so I have looked up the error in Google, and I got that I have to change the library \"psycopg2\" to \"psycopg2-binary\", but I still get the same error.\nAfter I tried a \"template\" in railway, which is like configurations for each framework, and it brings a sample app to test with, in this case, it was a Django app, in that app I saw that the dependency \"psycopg2\" was the version \"2.8.6\", and it was working, so I also replaced the version \"2.9.3\" to \"2.8.6\", but with the same result, even with replacing it with the dependency \"psycopg2-binary\".\nThis is the requirement.txt file:\n```\n`asgiref==3.5.2\nasync-generator==1.10\nattrs==21.4.0\nbackports.zoneinfo==0.2.1\nbeautifulsoup4==4.10.0\nbleach==4.1.0\ncached-property==1.5.2\ncertifi==2021.5.30\ncffi==1.15.0\ncharset-normalizer==2.0.6\ncloudinary==1.29.0\ncolorama==0.4.4\ncoverage==6.4.1\ncryptography==36.0.1\ndj-database-url==0.5.0\nDjango==4.0.4\ndjango-environ==0.8.1\ndjango-heroku==0.3.1\ndocutils==0.17.1\net-xmlfile==1.1.0\nfrozendict==2.0.6\ngenanki==0.11.0\nh11==0.13.0\nidna==3.2\nimportlib-metadata==4.8.1\nkeyring==23.2.1\nnumpy==1.22.3\nopenpyxl==3.0.9\noutcome==1.1.0\npackaging==21.0\npandas==1.4.2\npkginfo==1.7.1\npsycopg2-binary==2.8.6\npycparser==2.21\nPygments==2.10.0\npyOpenSSL==22.0.0\npyparsing==3.0.0\npystache==0.6.0\npython-dateutil==2.8.2\npytz==2022.1\npywin32-ctypes==0.2.0\nPyYAML==5.4.1\nreadme-renderer==30.0\nrequests==2.26.0\nrequests-toolbelt==0.9.1\nrfc3986==1.5.0\nselenium==4.1.0\nsix==1.16.0\nsniffio==1.2.0\nsortedcontainers==2.4.0\nsoupsieve==2.2.1\nsqlparse==0.4.2\ntqdm==4.62.3\ntrio==0.19.0\ntrio-websocket==0.9.2\ntwine==3.4.2\ntzdata==2022.1\nurllib3==1.26.6\nwebencodings==0.5.1\nwhitenoise==6.2.0\nwsproto==1.0.0\nzipp==3.6.0\n`\n```\nIs there some idea about what is happening?",
      "solution": "Running `johnnydep` on your requirements.txt, shows that `django-heroku` depends on `psycopg2`:\n```\n`>>> johnnydep django-heroku\n2022-07-19 17:03:28 [info     ] init johnnydist                [johnnydep.lib] dist=django-heroku parent=None\n2022-07-19 17:03:30 [info     ] init johnnydist                [johnnydep.lib] dist=dj-database-url>=0.5.0 parent=django-heroku\n2022-07-19 17:03:32 [info     ] init johnnydist                [johnnydep.lib] dist=django parent=django-heroku\n2022-07-19 17:03:37 [info     ] init johnnydist                [johnnydep.lib] dist=psycopg2 parent=django-heroku\n2022-07-19 17:03:45 [warning  ] Created temporary directory: /private/var/folders/0t/p6l6_9qd15bb9qx6bq2mrt1r0000gn/T/pip-ephem-wheel-cache-q2agslxa\n....\n`\n```\nNotes:\n1.) I stopped after the first match (possibly there are more dependencies).\n2.) I dont know if it is sufficient to put `psycopg2-binary` at the beginning of your requirements.txt, to prevent `psycopg2` needed to get installed.",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2022-07-18T22:07:00",
      "url": "https://stackoverflow.com/questions/73028024/why-do-i-get-this-error-when-i-deploy-a-django-app-in-railway"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 71850416,
      "title": "psycopg2 AttributeError: &#39;Cursor&#39; object has no attribute &#39;mogrify&#39;",
      "problem": "I have a class that helps me with SQL queries and inserts in Postgres.  I'm using `psycopg2==2.7.5` right now.  One of the methods I'm using looks like this:\n```\n`import pandas as pd    \nimport psycopg2.extensions as ps_ext\nfrom typing import List\n\ndef insert_with_open_connection(self, df: pd.DataFrame, table_name: str, cursor: ps_ext.cursor,\n                                conn: ps_ext.connection,\n                                success_msg: str = 'Success',\n                                conflict_cols: List[str] = None):\n    try:\n        # Format the INSERT SQL query\n        cols = str(tuple(df.columns)).replace(\"'\", '')\n        nc = df.shape[1]\n        ss = \"(\" + ''.join('%s,' for _ in range(nc))[:-1] + \")\"\n        try:\n            args_str = str(b','.join(cursor.mogrify(ss, x) for x in df.values), 'utf-8')\n        except psycopg2.ProgrammingError:\n            args_str = str(b','.join(cursor.mogrify(ss, x) for x in self.clean_numpy_int_for_mogrify(df.values)),\n                           'utf-8')\n        args_str = args_str.replace(\"\\'NaN\\'::float\", 'NULL')\n        insert_sql = f'INSERT INTO {table_name} {cols} VALUES {args_str}'\n        if conflict_cols is not None:\n            conf_cols = str(tuple(conflict_cols)).replace(\"'\", '').replace(',)', ')')\n            insert_sql += f\"\\nON CONFLICT {conf_cols} DO NOTHING\"\n        insert_sql += ';'\n        cursor.execute(insert_sql)\n        conn.commit()\n        return success_msg, 200\n    except Exception:\n        return traceback.format_exc(), 400\n`\n```\nThe `conn` and `cursor` parameters are generated from a SqlAlchemy `Engine` with code like this:\n```\n`def create_pool(self, **db_config):\n    db_user = self.user\n    db_pass = self.password\n    db_name = self.database\n\n    # e.g. \"/cloudsql\"\n    db_socket_dir = os.environ.get(\"DB_SOCKET_DIR\", \"/cloudsql\")\n\n    # i.e \"::\"\n    cloud_sql_connection_name = os.environ.get(\"CLOUD_SQL_CONNECTION_NAME\",\n                                               '::')\n\n    self.pool = sqlalchemy.create_engine(\n\n        # Equivalent URL:\n        # postgresql+pg8000://:@/\n        #                     ?unix_sock=//.s.PGSQL.5432\n        sqlalchemy.engine.url.URL.create(drivername=\"postgresql+pg8000\",\n                                         username=db_user,  # e.g. \"my-database-user\"\n                                         password=db_pass,  # e.g. \"my-database-password\"\n                                         database=db_name,  # e.g. \"my-database-name\"\n                                         query={\"unix_sock\":\n                                                    f\"{db_socket_dir}/{cloud_sql_connection_name}/.s.PGSQL.5432\"}),\n        **db_config\n    )\n\ndef get_db_connection(self) -> Connection:\n    if self.pool is None:\n        self.create_pool()\n\n    assert isinstance(self.pool, Engine)\n    try:\n        return self.pool.raw_connection()\n    except psycopg2.OperationalError:\n        self.create_pool()\n        return self.pool.raw_connection()\n\n@contextlib.contextmanager\ndef db_connect(self):\n    db = self.get_db_connection()\n    cur = db.cursor()\n    try:\n        yield db, cur\n    finally:\n        db.close()\n`\n```\nI'm trying to use this code inside a Google Cloud Function (Linux) and I get the following error/traceback when I run the `insert_with_open_connection` method there:\n```\n`Traceback (most recent call last):\n  File \"/workspace/db/sql_helper.py\", line 221, in insert_with_open_connection\n    args_str = str(b','.join(cursor.mogrify(ss, x) for x in df.values), 'utf-8')\n  File \"/workspace/db/sql_helper.py\", line 221, in \n    args_str = str(b','.join(cursor.mogrify(ss, x) for x in df.values), 'utf-8')\nAttributeError: 'Cursor' object has no attribute 'mogrify'\n`\n```\nIt's obvious that the `cursor` in the code doesn't seem to have the attribute `mogrify`, but based on the docs here, the `mogrify` method should exist.",
      "solution": "I took a look at the code and noticed that you were using `import psycopg2.extensions as ps_ext`; and clearly that had `mogrify` in\nthe docs.\nThen I came across this line:\n```\n`self.pool = sqlalchemy.create_engine(\n\n        # Equivalent URL:\n        # postgresql+pg8000://:@/\n        #                     ?unix_sock=//.s.PGSQL.5432\n        sqlalchemy.engine.url.URL.create(drivername=\"postgresql+pg8000\",\n                                         username=db_user,  # e.g. \"my-database-user\"\n                                         password=db_pass,  # e.g. \"my-database-password\"\n                                         database=db_name,  # e.g. \"my-database-name\"\n                                         query={\"unix_sock\":\n                                                    f\"{db_socket_dir}/{cloud_sql_connection_name}/.s.PGSQL.5432\"}),\n        **db_config\n    )\n`\n```\nYou aren't using the psycopg2 driver; but the pg8000 one and tracing\nthe way things are generated, the cursor as returned by the `db.cursor()` which in turn was created by the self.pool.raw_connection(),\nI came to the conclusion that the cursor wasn't a ps_ext cursor but\na pg8000 cursor, which doesn't have the `mogrify` method as\nshown in: https://github.com/tlocke/pg8000/blob/main/pg8000/dbapi.py\nThis is the likelihood of why you're having this error.  I think\nthe solution is to change to using psycopg2 driver instead.\nThat said,  this answer could be wrong and I'm barking up the wrong tree.",
      "question_score": 1,
      "answer_score": 3,
      "created_at": "2022-04-13T02:12:01",
      "url": "https://stackoverflow.com/questions/71850416/psycopg2-attributeerror-cursor-object-has-no-attribute-mogrify"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 70177212,
      "title": "Psycopg2 formatting error when using SQL module",
      "problem": "I imagine this will be an easy one for somebody but I can't figure it out...\nI am developing a SQL query dynamically following the instructions here. After creating postgresql connection and cursor object, I use the psycopg2 sql module to create a query with parameters as such:\n```\n`query = sql.SQL(\"SELECT * FROM {t_name} WHERE {col} in %s;\".format(\n    t_name=sql.Identifier(table_name),\n    col=sql.Identifier(fips_col)))\n`\n```\nThis works fine (as far as I know) with this result:\n```\n`SQL(\"SELECT * FROM Identifier('cb_tracts') WHERE Identifier('st_cnty_fips') in %s;\")\n`\n```\nI then try to execute with an argument (or list of arguments):\n```\n`cursor.execute(query, ('06037',))\n`\n```\nor\n```\n`cursor.execute(query, (['06037'],))\n`\n```\nBut I am getting this error:\n```\n`psycopg2.errors.SyntaxError: syntax error at or near \"'06073'\"\nLINE 1: ...er('cb_tracts') WHERE Identifier('st_cnty_fips') in '06073';\n`\n```\nand this error, respectively:\n```\n`psycopg2.errors.SyntaxError: syntax error at or near \"ARRAY\"\nLINE 1: ...('cb_tracts') WHERE Identifier('st_cnty_fips') in ARRAY['060...\n`\n```",
      "solution": "The SQL `IN (...)` is actually a syntactic construction rather than a function or anything like that. Which means that it takes a comma-separated list of literal values rather than a single value that is a list (if you can see the difference).\nThis means the value-binding that almost every database driver uses can't work with `IN (...)` even though everybody thinks it should.\nThe way you would express a list as a single value in PostgreSQL is an array. The way to check for membership of an array is with\n```\n`... my_column = ANY(%s)\n`\n```",
      "question_score": 1,
      "answer_score": 3,
      "created_at": "2021-12-01T00:07:07",
      "url": "https://stackoverflow.com/questions/70177212/psycopg2-formatting-error-when-using-sql-module"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 69529422,
      "title": "How Python psycopg2 query in condition with uuid array",
      "problem": "For given a table\n```\n`create table test_db (\n  id uuid\n)\n`\n```\nIn Python with library psycopg2, we can do query\n```\n`cursor.execute(\"select * from test_db where id in\" +\n   \" ('5ed11bbf-ffd1-4124-ba3d-5e392dc9db96','14acfb5b-3b09-4728-b3b3-8cd484b310db')\")\n`\n```\nBut if I parameterize `id`, change to\n```\n`cursor.execute(\"select * from testdb where id in (%s)\",\n   (\"'5ed11bbf-ffd1-4124-ba3d-5e392dc9db96','14acfb5b-3b09-4728-b3b3-8cd484b310db'\",))\n`\n```\nIt's not working, says\n\npsycopg2.errors.InvalidTextRepresentation: invalid input syntax for type uuid: \"'5ed11bbf-ffd1-4124-ba3d-5e392dc9db96','14acfb5b-3b09-4728-b3b3-8cd484b310db'\"\n\nHow can I use `in (%s)` with uuid array?",
      "solution": "Thank @piro's answer, after try several time, got the working code\n```\n`cursor.execute(\"select * from testdb where id = any(%s::uuid)\",\n   (['5ed11bbf-ffd1-4124-ba3d-5e392dc9db96',\n     '14acfb5b-3b09-4728-b3b3-8cd484b310db'],))\n`\n```\nand `id = any(%s::uuid)` can't be replaced with `id in (%s::uuid)`.",
      "question_score": 1,
      "answer_score": 1,
      "created_at": "2021-10-11T18:17:17",
      "url": "https://stackoverflow.com/questions/69529422/how-python-psycopg2-query-in-condition-with-uuid-array"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 69472889,
      "title": "See sanitized sql query",
      "problem": "The advised way to run a parameterized query with psycopg2, if I understand correctly, is to use the second argument to the execute method:\n`SQL = \"INSERT INTO authors (name) VALUES (%s);\"\ndata = (\"O'Reilly\", )\ncur.execute(SQL, data)\n`\nThis will sanitize the data, do quoting, etc. That's great. Less great is it appears to be opaque;  it seems to be a black box that modifies my code. I'd like to see what the actual query sent to the database is. Is there some way to see that?",
      "solution": "You can use the cursor.mogrify method to print the query with interpolated values:\n```\n`>>> conn = psycopg2.connect(database='test')\n>>> cur = conn.cursor()\n>>> cur.mogrify('SELECT foo, bar, baz FROM quux WHERE a = %s', (42,))\nb'SELECT foo, bar, baz FROM quux WHERE a = 42'\n`\n```\nNote that this is a psycopg2-specific method - it won't work for other DB-API connectors.",
      "question_score": 1,
      "answer_score": 3,
      "created_at": "2021-10-06T23:29:19",
      "url": "https://stackoverflow.com/questions/69472889/see-sanitized-sql-query"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 68641885,
      "title": "psycopg2.errors.SyntaxError: syntax error at or near",
      "problem": "Psycopg2 has been giving me this error whenever I try to update a row:\n```\n`Traceback (most recent call last):\n  File \"p:/Projects/Freelance/Optometry_Inventory/code/tester.py\", line 39, in \n    new_user.updateUser(*(fname, lname, username, role, hashed_pwd, user_id))\n  File \"p:\\Projects\\Freelance\\Optometry_Inventory\\code\\UserDC.py\", line 64, in updateUser\n    cursor.execute(update_query, args)\npsycopg2.errors.SyntaxError: syntax error at or near \"lname\"\nLINE 1: UPDATE users SET fname= ('a'), SET lname= ('b'),\n`\n```\nWhen I run the following function:\n```\n`def updateUser(self, user_id):\n        self.user_id = user_id\n        update_query = \"\"\"UPDATE users SET fname= %s, SET lname= (%s),\n         SET username= (%s), SET user_role= (%s), SET h_pwd= (%s), WHERE user_id= (%s)\"\"\"\n        with pool() as cursor:\n            cursor.execute(update_query, (self.fname, self.lname, self.user_name,\n                           self.user_role, self.user_pwd, self.user_id))\n`\n```\nI have tried just updating a single field and I still get the same error. Is there something more I am missing? The error code isn't giving me much information.",
      "solution": "The issue is that you redundantly write `SET` for every modified column, one `SET` is more than enough :)\n`def updateUser(self, user_id):\n    self.user_id = user_id\n    update_query = \"\"\"\n        UPDATE users \n        SET fname= %s, \n            lname= (%s),\n            username= (%s), \n            user_role= (%s),\n            h_pwd= (%s) \n        WHERE user_id= (%s)\"\"\"\n        with pool() as cursor:\n            cursor.execute(update_query, (self.fname, self.lname, self.user_name,\n                           self.user_role, self.user_pwd, self.user_id))\n`",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2021-08-03T21:35:59",
      "url": "https://stackoverflow.com/questions/68641885/psycopg2-errors-syntaxerror-syntax-error-at-or-near"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 68264692,
      "title": "Psycopg2 relation db does not exist",
      "problem": "I recently started using Macbook because my laptop was changed at work and right after that I started having problems with some of my code that I use to upload a dataframe to a postgresql database.\n```\n`import psycopg2\nfrom io import StringIO\n\ndef create_connection(user,password):\n    return psycopg2.connect(\n    host='HOST',\n    database='DBNAME',\n    user=user,\n    password=password)\n\nconn = create_connection(user,password)\n\ntable = \"data_analytics.tbl_summary_wingmans_rt\"\nbuffer = StringIO()\ndf.to_csv(buffer, header=False, index=False)\nbuffer.seek(0)\ncursor = conn.cursor()\ncursor.copy_from(buffer, table, sep=\",\", null=\"\")\nconn.commit()\ncursor.close()\n`\n```\nAs you can see, the code is quite simple and even before the change of equipment it ran without major problem on Windows. But as soon as I run this same code on the mac it throws me the following error:\n```\n`Error: relation \"data_analytics.tbl_summary_wingmans_rt\" does not exist\n`\n```\nIn several posts I saw that it could be the use of double quotes but I have already used the following and I still do not have a positive result.\n```\n`\"data_analytics.\"tbl_summary_wingmans_rt\"\"\n\"\"data_analytics\".\"tbl_summary_wingmans_rt\"\"\n'data_analytics.\"tbl_summary_wingmans_rt\"'\n`\n```",
      "solution": "The behaviour of `copy_from` changed in psycopg2 2.9 to properly quote the table name, which means that you can no longer supply a schema-qualified table name that way; you have to use `copy_expert` instead.",
      "question_score": 1,
      "answer_score": 3,
      "created_at": "2021-07-06T06:48:36",
      "url": "https://stackoverflow.com/questions/68264692/psycopg2-relation-db-does-not-exist"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 65847289,
      "title": "psycopg2: TypeError: not all arguments converted during string formatting",
      "problem": "I looked into many stack overflow posts and most of them told that I need to use a `tuple` or a `list` when inserting into `VALUES (%s)`. I tried both lists and tuples, but I am still getting the same error: `not all arguments converted during string formatting`. Here is the code of the function used to insert some data into PostgreSQL database:\n```\n`sql = '''\n    INSERT INTO immediate_info (\n    bytes_sent,\n    bytes_recv,\n    packets_sent,\n    packets_recv,\n    errin,\n    errout,\n    dropin,\n    dropout)\n    VALUES (%s);\n'''\n\nconn = None\n\nbytes_recv, bytes_sent, packets_sent, packets_recv, errin, errout, dropin, dropout = data\n\ntry:\n    conn = psycopg2.connect('all the connect stuff')\n    # create a new cursor\n    cur = conn.cursor()\n    # execute the INSERT statement\n    cur.execute(sql, (bytes_recv, bytes_sent, packets_sent, packets_recv, errin, errout, dropin, dropout,))\n    # commit the changes to the database\n    conn.commit()\n    # close communication with the database\n    cur.close()\n\nexcept (Exception, psycopg2.DatabaseError) as error:\n\n    print(error)\n\nfinally:\n    if conn is not None:\n        conn.close()\n`\n```\nAs I mentioned above, I have tried using lists as well. This time I decided to first unpack the `data` list first, though simply going through the `data` list using indexes (data[0],data[1], and so on) will, in my opinion, lead to the same results.\nThe `data` contains some network info used to measure the bandwidth of my computer. All of its contents is in `int` format.\nAlso, if you noticed, the string formatting here is the old one (referring to `VALUES (%s)`). How can the `f`-formatting be used in this case?\nAnd how do I get rid of this error?",
      "solution": "When executing `INSERT` statements using `cursor.execute`\n\nthe number of columns being inserted must match the number of placeholders in the `VALUES` clause\nthe number of elements in the second argument to `cursor.execute` must match the number of placeholders in the `VALUES` clause\n\nSo\n`cursor.execute(\"\"\"INSERT INTO foo (bar, baz, quux) VALUES (%s, %s)\"\"\", args)\n`\nis wrong because there are three columns being inserted but only two values placeholders\nand\n`cursor.execute(\"\"\"INSERT INTO foo (bar, baz, quux) VALUES (%s, %s, %s)\"\"\",\n               ('a', 'b', 'c', 'd'))\n`\nis wrong because the number of values in the second argument does not match the number of placeholders in the `VALUES` clause.",
      "question_score": 1,
      "answer_score": 3,
      "created_at": "2021-01-22T15:44:21",
      "url": "https://stackoverflow.com/questions/65847289/psycopg2-typeerror-not-all-arguments-converted-during-string-formatting"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 77506714,
      "title": "How does Psycopg2 handle sessions and transactions concurrency on high traffic apps?",
      "problem": "I've been going through psycopg2 documentation to understand how it handles transactions and so on and so forth.\nI got that a transaction is created on a connection level, regardless of the cursor you use, the session is visible to all of them.\n\nWarning By default even a simple SELECT will start a transaction: in long-running programs, if no further action is taken, the session will remain \u201cidle in transaction\u201d, an undesirable condition for several reasons (locks are held by the session, tables bloat\u2026). For long-lived scripts, either make sure to terminate a transaction as soon as possible or use an autocommit connection.\n\nSo I was wondering how a high-traffic API would deal with this to avoid weird situations where multiple queries are committed or rolled back because a different API request was completed and as the session is shared ... everything falls inside the same transaction.\nConnection pools seem to be the answer but still, the number of connections is limited so virtually you could encounter the same issue if the same connection from the pool is given to different API requests.\nIs it a matter of having a pool big enough to reduce the risk of a connection being shared by 2 different processes or am I missing something on how this works?\nThanks in advance ^^ sorry for the long question.",
      "solution": "Indeed connection pools are the answer, here is why:\nAs mentioned in Psycopg2 pool docs:\n\ngetconn(key=None)\nGet a free connection from the pool.\n\nThe thing to highlight here is the free word.\nBased on a different question I went looking into the library to see how the connections are being retrieved from the pool:\n```\n`def _getconn(self, key=None):\n    \"\"\"Get a free connection and assign it to 'key' if not None.\"\"\"\n    if self.closed:\n        raise PoolError(\"connection pool is closed\")\n    if key is None:\n        key = self._getkey()\n\n    if key in self._used:\n        return self._used[key]\n\n    if self._pool:\n        self._used[key] = conn = self._pool.pop()\n        self._rused[id(conn)] = key\n        return conn\n    else:\n        if len(self._used) == self.maxconn:\n            raise PoolError(\"connection pool exhausted\")\n        return self._connect(key)\n`\n```\nPsycopg2 keeps a list of the available connections and the ones being used. So, as mentioned in the comments, it indeed gives you a free connection. Each API request will have its own isolated transaction as they will receive a free connection each time.\nMore importantly, as it was the question, is that if we request for a new connection when we have the max number of conn created and all of them are in use, Psycopg2 will not give you a connection being used by another request. It will rise a PoolError. No risk of weird overlapping transaction issues happening.\nThings to consider\nKeep in mind the DB has its own limit for open connections:\n\nmax_connections (integer)\nDetermines the maximum number of concurrent connections to the database server. The default is typically 100 connections, but might be less if your kernel settings will not support it (as determined during initdb). This parameter can only be set at server start. (from: Postgres doc)\n\nIf your API scales based on demand, consider setting your maxconn on the server side considering that the same 100 available will be shared between all your API instances.\nThanks to @snakecharmerb and @AdrianKlaver as their comments led me to the answer.",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2023-11-18T12:35:12",
      "url": "https://stackoverflow.com/questions/77506714/how-does-psycopg2-handle-sessions-and-transactions-concurrency-on-high-traffic-a"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 77117102,
      "title": "psycopg2.errors.UndefinedColumn: column &quot;source_id_id&quot; of relation &quot;service_approval_domains&quot; does not exist",
      "problem": "I have created model in django for my database. After trying to add data to table I got this error:\n`\n```\n`lib/python3.10/site-packages/django/db/backends/utils.py\", line 89, in _execute\n    return self.cursor.execute(sql, params)\npsycopg2.errors.UndefinedColumn: column \"source_id_id\" of relation \"service_approval_domains\" does not exist\nLINE 1: ...domains\" (\"domain\", \"created_at\", \"retrieved_at\", \"source_id...\n                                                             ^\n`\n```\nI am not really sure where is the problem since I have checked everything and did not find any declaration of `source_id_id` column.\nHere is my model:\n`class ServiceApprovalDomains(models.Model):\n    id = models.BigAutoField(primary_key=True)\n    domain = models.TextField(unique=True ,null=False)\n    created_at = models.DateTimeField(auto_now_add=True)\n    retrieved_at = models.DateTimeField(auto_now=True)\n    source_id = models.ForeignKey(\n        ServiceApprovalDomainSources, on_delete=models.CASCADE\n    )\n\n    class Meta:\n        managed = False\n        db_table = '\"privacy\".\"service_approval_domains\"'\n`\nAnd I used manual migration so here is my SQL file:\n```\n`CREATE TABLE \"privacy\".\"service_approval_domains\"(\n    \"id\" BIGSERIAL PRIMARY KEY,\n    \"domain\" TEXT UNIQUE NOT NULL,\n    \"created_at\" timestamp with time zone not null default current_timestamp,\n    \"retrieved_at\" timestamp with time zone default current_timestamp,\n    \"source_id\" bigserial not null references service_approval_domain_sources(\"id\") on delete cascade\n);\n`\n```\nAnd this is the function I am using to add data to database:\n`def add_domains(data: list[AddServiceApprovalDomains]):\n    for domain in data:\n        source = ServiceApprovalDomainSources.objects.get_or_create(name=domain.source.name)\n        try:\n            ServiceApprovalDomains.objects.create(domain=domain.domain, source_id=source[0])\n        except IntegrityError:\n            pass\n    return True \n`",
      "solution": "If you make a `ForeignKey` named `foo`, django uses `foo_id` as name of the column to store the primary key it refers to.\nSo you can rename the `ForeignKey` to `source`:\n```\n`class ServiceApprovalDomains(models.Model):\n    id = models.BigAutoField(primary_key=True)\n    domain = models.TextField(unique=True, null=False)\n    created_at = models.DateTimeField(auto_now_add=True)\n    retrieved_at = models.DateTimeField(auto_now=True)\n    source = models.ForeignKey(\n        ServiceApprovalDomainSources, on_delete=models.CASCADE\n    )\n\n    class Meta:\n        managed = False\n        db_table = '\"privacy\".\"service_approval_domains\"'`\n```",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2023-09-16T11:18:40",
      "url": "https://stackoverflow.com/questions/77117102/psycopg2-errors-undefinedcolumn-column-source-id-id-of-relation-service-appr"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 76963335,
      "title": "sqlalchemy with postgres connection: .execute GRANT and DELETE not affecting database",
      "problem": "Passing \"SELECT\"-based SQL queries works. But passing \"DELETE\"- or \"GRANT\"-based SQL executed in python without error but makes no change to my database (python 3.8, sqlalchemy 2.0.10). These \"DELETE\" and \"GRANT\" queries worked in Python 3.6, sqlalchemy 1.3.23. I could be missing something, but I don't see anything in the sqlalchemy docs/change notes to indicate that this \"DELETE\" or \"GRANT\" functionality should be lost.\nHere is an example that might shed light on where I'm going wrong. After initializing the engine (where I use '' to indicate some arbitrary input):\n```\n`from sqlalchemy import create_engine\nfrom sqlalchemy import text    \ndb_name = ''\nport = ':/'\nhost = ''\nusername = ''\npassword = ''\nengine = create_engine('postgresql_psycopg2://'+username+':'+\n                            password+'@'+host+port+db_name)\n`\n```\nI am able read and write through connection.execute or even pandas. Something like the following works just fine (reading with pandas, writing with pandas, reading with .execute):\n```\n`import pandas as pd\nquery_1 = \"SELECT * from ''.''\"\ndf = pd.read_sql(sql_query, engine)\ndf.to_sql(name = '', con = engine, schema = )\nwith engine.connect() as con:\n    con.execute(text(\"SELECT * from schema.''))\n`\n```\nPassing GRANT- or DELETE-based SQL through the .execute runs in Python without errors, but there are no changes to the database--as if the Python script never ran. The following examples run in Python without error but I get no effect in the database:\n```\n`query_2 = \"GRANT SELECT ON TABLE ''.'' TO \"\nquery_3 = \"DELETE FROM ''.'' WHERE \"\nwith engine.connect() as con:\n    con.execute(text(query_2))\n    con.execute(text(query_3))\n`\n```\nAgain, this same code worked in Python 3.6/sqlalchemy 1.3.23 (removing the sqlalchemy \"text()\" as it wasn't necessary in older versions as described in sqlalchemy change notes).",
      "solution": "You can add turn on `AUTOCOMMIT` when initializing the engine or you can explicitly commit the changes to your database by modifying your code to include the `COMMIT` command.\n```\n`query_2 = \"GRANT SELECT ON TABLE ''.'' TO \"\nquery_3 = \"DELETE FROM ''.'' WHERE \"\nwith engine.connect() as con:\n    con.execute(text(query_2))\n    con.execute(text(query_3))\n    con.execute(text(\"COMMIT\"))\n`\n```",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2023-08-23T18:39:26",
      "url": "https://stackoverflow.com/questions/76963335/sqlalchemy-with-postgres-connection-execute-grant-and-delete-not-affecting-dat"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 75773057,
      "title": "Trying to convert psycopg2 -&gt; Python data from list tuples to dict",
      "problem": "I have a postgreSQL table called stockdata with financial statement information for about 250 companies with the following setup:\n```\n`Company  q4_2022_revenue q3_2022_revenue\nCPI Card 126436000       124577000  \nZuora    103041000       101072000 \n\u2026\n`\n```\nI used the psycopg2 package in Python to import this data into a \u2018main.py\u2019 file with the following code:\n```\n`import psycopg2\n\nconn=psycopg2.connect(\u2018dbname=postgres user=postgres password=\u2018\u2026\u2019)\ncur=conn.cursor()\ncur.execute(\u2018SELECT company, (q4_2022_revenue-q3_2022_revenue)/CAST(q3_2022_revenue AS float) AS revenue_growth FROM stockdata ORDER BY revenue_growth DESC LIMIT 25;\nrecords=cur.fetchall()\nprint(records)\n`\n```\nWhich gave me the following results:\n```\n`[(\u2018Alico\u2019, 9.269641125121241),(\u2018Arrowhead Pharmaceuticals\u2019, 1.7705869324473975),\u2026],\n`\n```\nwhere records[0] is equal to (\u2018Alico\u2019, 9.269641125121241) and the type for records as a whole was given as list.\nI\u2019m trying to figure out how I can assign a variable separately to both the company name and change in revenue so I can modify the revenue change from, for example, 9.2696 to 926.96% (the company is in a seasonal industry - agriculture - and is recovering from a hurricane so the data makes sense). The revenue change would be multiplied by 100 and given a % sign on the end and rounded to 2 decimal digits.\nI tried adding the line:\n```\n`list((x,y) for x,y in records)\n`\n```\nwith the intention of assigning x as key values for company name and y as pair values for revenue change, but I received a \u2018name \u2018x\u2019 is not defined\u2019 error when calling records[x].\nHow can I convert my psycopg2 list of tuples to a mutable dict and assign the results to a key-value pair so I can modify the formatting of the values?\nEDIT: Typo in 926.96%.",
      "solution": "In psycopg2.extras there is the RealDictCursor which can output query results as dicts.\nChange your cursor to use it as below:\n```\n`conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)\n`\n```",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2023-03-18T00:53:24",
      "url": "https://stackoverflow.com/questions/75773057/trying-to-convert-psycopg2-python-data-from-list-tuples-to-dict"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 74291298,
      "title": "TypeError: Object of type Response is not JSON serializable",
      "problem": "I am trying to use Python and APIs to build an SQL table with the names of the first 100 Pokemon in the Poke API. Here's my code\n`import psycopg2, json, requests, hidden\n\n# Load secrets\nsecrets = hidden.secrets()\n\nconn = psycopg2.connect(host=secrets['host'],\n        port=secrets['port'],\n        database=secrets['database'],\n        ...,\n        connect_timeout=3)\n\ncur = conn.cursor()\n\ndefaulturl = 'https://pokeapi.co/api/v2/pokemon?limit=100&offset=0'\n\nsql = '''\nCREATE TABLE IF NOT EXISTS pokeapi\n(id SERIAL, body JSONB); \n'''\n\ncur.execute(sql)\n\nresponse = requests.get(defaulturl)\njs = response.json() \n\nresults = js['results'] \n\nfor x in range(len(results)):\n    body = requests.get(results[x]['url'])\n    js_body = json.dumps(body) \n    sql = f\"INSERT INTO pokeapi (body) VALUES ('{js_body}'::JSONB)\";\n    cur.execute(sql, (defaulturl))\n\nprint('Closing database connection...')\nconn.commit()\ncur.close() \n`\nAnd the error is coming up for this line\n` ---> 35     js_body = json.dumps(body)`\nI'm not sure what's causing the error.",
      "solution": "`requests.get()` returns the entire response, not just the body text.\nIf you want the body text, use the `text` attribute of the response object:\n```\n`response = requests.get(results[x]['url'])\njs_body = json.dumps(response.text)\n`\n```",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2022-11-02T15:57:58",
      "url": "https://stackoverflow.com/questions/74291298/typeerror-object-of-type-response-is-not-json-serializable"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 73911616,
      "title": "PostgresSQL query returns None in Python (Works on PgAdmin)",
      "problem": "I have the following query:\n```\n`def is_paying_user(user_id: int) -> bool:\n    with Database() as cursor:\n        res = cursor.execute(f\"\"\"SELECT is_paying FROM public.\"Users\" WHERE user_id = {user_id}\"\"\")\n        return res.fetchone()\n`\n```\nuser_id is an integer.\n`Database` is a context manager defined as:\n```\n`class Database:\n    def __init__(self):\n        self.connection = psycopg2.connect(database=\"postgres\", user=DB_USER, password=DB_PASS,\n                                           host=LOCALHOST, port=DB_PORT)\n\n    def __enter__(self):\n        return self.connection.cursor()\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.connection.commit()\n        self.connection.close()\n`\n```\nAnd I am getting the following error when calling `is_paying_user`:\n```\n`AttributeError: 'NoneType' object has no attribute 'fetchone'\n`\n```\nWhen I grab the integer value and run the same query on PgAdmin it succeeds.\nI also tried a few variations of the same query (got same error):\n```\n`cursor.execute(\"\"\"SELECT trackables_amount FROM public.\"Users\" WHERE user_id = %s\"\"\", (user_id,))\n\ncursor.execute(\"\"\"SELECT trackables_amount FROM public.\"Users\" WHERE user_id = %s\"\"\", user_id)\n`\n```",
      "solution": "The return value of `cursor.execute` is undefined; you want to call `.fetchall` (or `.fetchone`) on the cursor object!",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2022-09-30T18:30:55",
      "url": "https://stackoverflow.com/questions/73911616/postgressql-query-returns-none-in-python-works-on-pgadmin"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 73711594,
      "title": "How to capture/fetch the result from a delete query to ascertain the # of rows deleted?",
      "problem": "I am trying to understand how to capture the number of records deleted from a delete command in Python code using psycopg2. Typically after a delete command is issued in PostgreSql the number of rows deleted appears (i.e., `DELETE 8`, where 8 represents the number of rows deleted). I would like to record this count after each delete in order to report back to the caller the number of rows deleted without needing to make a separate `SELECT count(*)` before the DELETE command.\nConsider this function:\n```\n`def qexe(conn, query, fetch_type):\n    cursor = conn.cursor()\n    cursor.execute(query)\n    if fetch_type != None:\n        if fetch_type == 'fetchall':\n            query_result = cursor.fetchall()\n            return query_result\n        elif fetch_type == 'fetchone':\n            query_result = cursor.fetchone()\n            return query_result\n        else:\n            raise Exception(f\"Invalid arg fetch_type: {fetch_type}\")\n    cursor.close()\n`\n```\nAfter running each of the following, I keep getting the error: `psycopg2.ProgrammingError: no results to fetch`:\n```\n`qq = \"\"\"DELETE FROM INV WHERE ITEM_ID = '{}';\"\"\"\n\nitem='123abc'\n\nresp0 = qexe(conn, qq.format(item), 'fetchone')\nresp1 = qexe(conn, qq.format(item), 'fetchall')\n`\n```",
      "solution": "You could make use of `RETURNING`, so you will be able to get 'something' back which can be fetched from the cursor:\n```\n`qq = \"\"\"DELETE FROM INV WHERE ITEM_ID = '{}' RETURNING ITEM_ID\"\"\"\nresp0 = qexe(conn, qq.format(item), 'fetchone')\n`\n```",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2022-09-14T06:39:31",
      "url": "https://stackoverflow.com/questions/73711594/how-to-capture-fetch-the-result-from-a-delete-query-to-ascertain-the-of-rows-d"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 73312844,
      "title": "What is the correct way to implement a context manager in psycopg2?",
      "problem": "I've been refactoring my psycopg2 code using functions, previously I had it all on a try-except-finally block, however I'm not quite sure how to implement a context-manager to handle the connection and cursor. My SQL queries work and look like this:\n```\n`def random_query(schema, table, username, number_of_files):\n    random_query = sql.SQL(\"SELECT * FROM {schema}.{table} WHERE username = {username} ORDER BY RANDOM() LIMIT {limit}\").format(\n        schema=sql.Identifier(schema),\n        table=sql.Identifier(table),\n        username=sql.Literal(username),\n        limit=sql.Literal(number_of_files)\n        )\n    cursor.execute(random_query)\n    return cursor.fetchone()\n\ndef insert_query(schema, table, values):\n    insert_query = sql.SQL(\"INSERT INTO {schema}.{table}(shortcode, username, filename, extension) VALUES ({shortcode}, {username}, {filename}, {extension})\").format(\n        schema=sql.Identifier(schema),\n        table=sql.Identifier(table),\n        shortcode=sql.Literal(values[0]),\n        username=sql.Literal(values[1]),\n        filename=sql.Literal(values[2]),\n        extension=sql.Literal(values[3])\n        )\n    cursor.execute(insert_query)\n    conn.commit()\n`\n```\n\nFirst version:\n\n```\n`@contextmanager\ndef get_connection():\n    connection = psycopg2.connect(**DB_CONNECTION)\n    try:\n        yield connection\n    except Exception as err:\n        connection.rollback()\n        print('Error: ', err)\n        raise\n    finally:\n        if (connection):\n            connection.close()\n            print(\"Connection is closed.\")\n\n@contextmanager\ndef get_cursor(connection):\n    cursor = connection.cursor()\n    try:\n        yield cursor\n    finally:\n        cursor.close()\n\nwith get_connection() as conn, get_cursor(conn) as cursor:\n    random_record = random_query('test_schema', 'test_table', 'username', 1)\n    insert_query('test_schema', 'test_table2', random_record)\n`\n```\n\nSecond version:\n\n```\n`@contextmanager\ndef sql_connection():\n    connection = psycopg2.connect(**DB_CONNECTION)\n    cursor = connection.cursor()\n    try:\n        yield connection,cursor\n    except Exception as err:\n        connection.rollback()\n        print('Error : ', err)\n        raise\n    finally:\n        if (connection):\n            cursor.close()\n            connection.close()\n            print(\"Connection is closed\")\n\nwith sql_connection() as (conn, cursor):\n    random_record = random_query('test_schema', 'test_table', 'username', 1)\n    insert_query('test_schema', 'test_table2', random_record)\n\n`\n```\nMy questions are:\n\nIs there any difference between the first and the second version? Which one is preferable?\nAs you can see in `insert_query`, there is a line that calls `conn.commit()` From the documentation, I understand that this is not necessary if we are using a context manager. Can I remove them?\n\nChanged in version 2.5: if the connection is used in a with statement,\nthe method is automatically called if no exception is raised in the\nwith block.",
      "solution": "Neither version is preferable, you are still over complicating things by duplicating behavior. Per the example here Connection:\n```\n`import psycopg2\n\nconnection = psycopg2.connect(**DB_CONNECTION)\n\nwith connection:\n    with connection.cursor() as cur:\n        cur.execute()\n\nwith connection:\n    with connection.cursor() as cur:\n        cur.execute()\n`\n```\nCommitting, rollback on the connection and closing of cursor is done for you. All you have to do is `connection.close()` when you no longer want to use the connection.\nUPDATE\nThe question and answer are for `psycopg2`, if you are using `psycopg(3)` then the connection context manager behavior has changed in that version. In `psycopg(3)` `with connection` will close the connection on completion, whereas in `psycopg2` it just closed the transaction.",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2022-08-10T23:20:32",
      "url": "https://stackoverflow.com/questions/73312844/what-is-the-correct-way-to-implement-a-context-manager-in-psycopg2"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 71970584,
      "title": "pandas to_sql with dict raises &quot;can&#39;t adapt type &#39;dict&#39;&quot;, is there a way to avoid `dumps`?",
      "problem": "I am trying to load a simple pandas DataFrame on a PostgreSQL table (pandas 1.4, Postgres 13):\n```\n`df = pd.DataFrame([{\"index\": 1, \"properties\": {\"a\": 1, \"b\": 2}}])\ndf.to_sql(\"_test_table\", con, if_exists=\"replace\")\n`\n```\nHowever, I get `ProgrammingError: can't adapt type 'dict'`.\nI have seen in other Stack Overflow answers that applying `json.dumps` fixes the issue, and they are right. However, I would like to know if there's a way to leverage PostgreSQL `JSON` types instead of converting the information to a string.\nThe `psycopg2` documentation mentions \"JSON Adaptation\", but running `psycopg2.extensions.register_adapter(dict, psycopg2.extras.Json)` at the beginning ends up, again, storing the data as `text`.\nThere are a couple of mentions of JSON on the psycopg2 FAQ, but I don't think they answer my question.",
      "solution": "This is essentially a duplicate of Writing JSON column to Postgres using Pandas .to_sql: the solution is to use the `dtype` parameter of `.to_sql` with `sqlalchemy.types.JSON`:\n```\n`import sqlalchemy\n\ndf = pd.DataFrame([{\"index\": 1, \"properties\": {\"a\": 1, \"b\": 2}}])\ndf.to_sql(\"_test_table\", dwh_con, if_exists=\"replace\", dtype={\"properties\": sqlalchemy.types.JSON})\n`\n```\nAnd now everything works.",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2022-04-22T16:41:54",
      "url": "https://stackoverflow.com/questions/71970584/pandas-to-sql-with-dict-raises-cant-adapt-type-dict-is-there-a-way-to-avoi"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 71909436,
      "title": "InvalidTextRepresentation: Invalid input syntax for type bigint:&quot;All Forms&quot;",
      "problem": "I had a field in my model with\n```\n`book_classes = ((\"\",\"Select Form\"),(\"1\",'F1'),(\"2\",'F2'),(\"3\",'F3'),(\"4\",'F4'),(\"All Forms\",\"All Forms\"))\nb_classes = models.CharField('Form',max_length=9,choices=book_classes,default=\"n/a\")\n`\n```\nAnd then changed it to\n```\n`b_class =models.ForeignKey(ClassBooks,on_delete=models.CASCADE)\n`\n```\nWhere\n```\n`class ClassBooks(models.Model):\n    name = models.CharField(max_length=10)\n`\n```\nI'm now stuck because when I try to migrate I get an error.\n```\n`Invalid input syntax for type bigint:\"All Forms\"\n`\n```\nMakemigrations and migrate worked well in development. When I pushed to digital ocean, the migrate returned the error stated.\nWhat do I need to do, please?",
      "solution": "See Foreign Key field. By default a FK field is going to use the Primary Key of the referenced table(model), in this case the `id` field of `ClassBooks`. The `id` field is an integer so you get the error when trying to use a string field. To make this work, from the documentation link :\n\nForeignKey.to_field\n\nThe field on the related object that the relation is to. By default, Django uses the primary key of the related object. If you reference a different field, that field must have unique=True.\n\nWhich in your case becomes:\n```\n`b_class =models.ForeignKey(ClassBooks,to_field='name',on_delete=models.CASCADE)\n`\n```\nThis assumes that the `name` field has a `Unique` constraint on it.\nThough I am not sure how `\"\", \"1\", \"2\" ...` map to `ClassBooks.name`.",
      "question_score": 1,
      "answer_score": 1,
      "created_at": "2022-04-18T10:34:36",
      "url": "https://stackoverflow.com/questions/71909436/invalidtextrepresentation-invalid-input-syntax-for-type-bigintall-forms"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 71667338,
      "title": "Django can&#39;t connet to PostgreSQL Docker container via psycopg2",
      "problem": "I am trying to migrate a django project on macOS Monterey 12.3, and I am having some troubles.\nIt seems like psycopg2 doesn't want to connect to my docker container at all. Everytime it prints this error:\n```\n`django.db.utils.OperationalError: connection to server at \"localhost\" (127.0.0.1), port 5433 failed: server closed the connection unexpectedly\n    This probably means the server terminated abnormally\n    before or while processing the request.\n`\n```\nFor my docker process, I create it by running\n```\n`docker run --name local-postgres -p 5433:5433 -e POSTGRES_PASSWORD=test123 -d postgres\n`\n```\nI am running python 3.9.12 in a virtual environment using pipenv, and my arch is arm64, for anyone wondering.\nI've tried changing the ports, I've tried resetting, completely removing docker and downloading it again, reinstalling django and reinstalling the venv again and so far nothing has worked. I've also tried setting the `CONN_MAX_AGE=0` in settings, which has not worked.\nPlease help",
      "solution": "Postgres listens on port 5432, so you need to map that to the port you want to connect to on the host. It looks like you want to use port 5433, so you'd do\n```\n`docker run --name local-postgres -p 5433:5432 -e POSTGRES_PASSWORD=test123 -d postgres\n`\n```\nThen you can connect on the host using localhost port 5433.",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2022-03-29T20:48:52",
      "url": "https://stackoverflow.com/questions/71667338/django-cant-connet-to-postgresql-docker-container-via-psycopg2"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 71587366,
      "title": "How to rollback in psycopg2?",
      "problem": "I\u00b4m forcing an error in qry2 so that a can run qry3 suposing it could rollback to save_1 but  it doesn\u00b4t. Any catch? Using PostgreSql 14.2\n```\n`qry1 = ('begin;' +\n        'savepoint save_1;' +\n        'delete from ntnb_cup;')\n\nqry2 = ...\n        # Force error\n\nqry3 = 'rollback to save_1;'\n\ntry:\n    cursor = conn.cursor()\n    cursor.execute(qry1)\n    conn.commit()\nexcept Exception as err:\n    cursor.close()\n    conn.close()\n    exit()\n\ntry:\n    cursor.execute(qry2)\n    conn.commit()  \nexcept Exception as err:\n    cursor.execute(qry3)\n    conn.commit()    \nfinally:\n    cursor.close()\n    conn.close()\n`\n```",
      "solution": "```\n`try:\n    cursor = conn.cursor()\n    cursor.execute(qry1)\n    conn.commit()\n`\n```\nOnce you commit, you are committed. To preserve the option of rolling back to a savepoint later, don't commit here.",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2022-03-23T13:37:47",
      "url": "https://stackoverflow.com/questions/71587366/how-to-rollback-in-psycopg2"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 71263702,
      "title": "Postgres server-side cursor with LEFT JOIN does not return on Heroku PG",
      "problem": "I have a Heroku app that uses a psycopg server-side cursor together with a  `LEFT JOIN` query running on Heroku PG 13.5.\nThe query basically says \u201cfetch items from one table, that don\u2019t appear in another table\u201d.\nMy data volume is pretty stable, and this has been working well for some time.\nThis week these queries stopped returning.  In `pg_stat_activity` they appeared as `active` indefinitely (17+ hours), similarly in `heroku pg:ps`. There appeared to be no deadlocks. All the Heroku database metrics and logs appeared healthy.\nIf I run the same queries directly in the console (without a cursor) they return in a few seconds.\nI was able to get it working again in the cursor by making the query a bit more efficient (switching from `LEFT JOIN` to `NOT EXISTS`; dropping one of the joins).\nMy questions are:\n\nWhy might the original query perform fine in the console, but not return with a psycopg server-side cursor?\nHow might I debug this?\nWhat might have changed this week to trigger the issue?\n\nI can say that:\n\nHowever I write the query (`LEFT JOIN`, Subquery, `NOT EXISTS`), the query plan involves a Nested Loop Anti Join\nI don\u2019t believe this is related to the Heroku outage the following day (and which didn\u2019t affect Heroku PG)\nHaving Googled extensively, the closest thing I can find to a hypothesis to explain this is a post on the PG message boards from 2003 entitled left join in cursor where the response is \u201cSome plan node types don't cope very well with being run backwards.\u201d\n\nAny advice appreciated!",
      "solution": "If you are using a cursor, PostgreSQL estimates that only 10% of the query result will be fetched quickly and prefers plans that return the first few rows quickly, at the expense of the total query cost.\nYou can disable this optimization by setting the PostgreSQL parameter `cursor_tuple_fraction` to 1.0.",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2022-02-25T10:27:49",
      "url": "https://stackoverflow.com/questions/71263702/postgres-server-side-cursor-with-left-join-does-not-return-on-heroku-pg"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 70573466,
      "title": "Installing psycopg2 on M1",
      "problem": "First of all, I know there are many similar questions already out there on SO. But none of them are working for me.\nI'm using a 2020 Macbook Air with an Apple M1 chip installed. I need `psycopg2` to run my `postgres` Django database. `psycopg2` just isn't installing, how many ever times I install `lipq` or `openssl` via homebrew or installing only `psycopg2-binary`: nothing's working.\nI'm doing all of this inside my `conda` virtual environment, if that's going to help you, using PIP.\nWhenever I do `pip install psycopg2`, PIP just cycles through all the available versions and tries to install it.\nIt keeps saying this\n```\n`ERROR: Command errored out with exit status 1:\n   command: /opt/homebrew/opt/python@3.9/bin/python3.9 -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/private/var/folders/d_/gv1n_nvn2f99dqd7kf7vc3xw0000gn/T/pip-install-389xqqoi/psycopg2_54b9167ec8544ee0844b2f85085d9e31/setup.py'\"'\"'; __file__='\"'\"'/private/var/folders/d_/gv1n_nvn2f99dqd7kf7vc3xw0000gn/T/pip-install-389xqqoi/psycopg2_54b9167ec8544ee0844b2f85085d9e31/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base /private/var/folders/d_/gv1n_nvn2f99dqd7kf7vc3xw0000gn/T/pip-pip-egg-info-8vyyh57i\n       cwd: /private/var/folders/d_/gv1n_nvn2f99dqd7kf7vc3xw0000gn/T/pip-install-389xqqoi/psycopg2_54b9167ec8544ee0844b2f85085d9e31/\n  Complete output (6 lines):\n  Traceback (most recent call last):\n    File \"\", line 1, in \n    File \"/private/var/folders/d_/gv1n_nvn2f99dqd7kf7vc3xw0000gn/T/pip-install-389xqqoi/psycopg2_54b9167ec8544ee0844b2f85085d9e31/setup.py\", line 245\n      except Warning, w:\n                    ^\n  SyntaxError: invalid syntax\n`\n```\nAt the last iteration\n```\n`ERROR: Command errored out with exit status 1:\n   command: /opt/homebrew/opt/python@3.9/bin/python3.9 -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/private/var/folders/d_/gv1n_nvn2f99dqd7kf7vc3xw0000gn/T/pip-install-389xqqoi/psycopg2_25faf90bbbc84c69ace950d40ca94f61/setup.py'\"'\"'; __file__='\"'\"'/private/var/folders/d_/gv1n_nvn2f99dqd7kf7vc3xw0000gn/T/pip-install-389xqqoi/psycopg2_25faf90bbbc84c69ace950d40ca94f61/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base /private/var/folders/d_/gv1n_nvn2f99dqd7kf7vc3xw0000gn/T/pip-pip-egg-info-zyskdyv8\n       cwd: /private/var/folders/d_/gv1n_nvn2f99dqd7kf7vc3xw0000gn/T/pip-install-389xqqoi/psycopg2_25faf90bbbc84c69ace950d40ca94f61/\n  Complete output (5 lines):\n  Traceback (most recent call last):\n    File \"\", line 1, in \n    File \"/private/var/folders/d_/gv1n_nvn2f99dqd7kf7vc3xw0000gn/T/pip-install-389xqqoi/psycopg2_25faf90bbbc84c69ace950d40ca94f61/setup.py\", line 50, in \n      import ConfigParser\n  ModuleNotFoundError: No module named 'ConfigParser'\n`\n```\nAnd then it says\n```\n`WARNING: Discarding https://files.pythonhosted.org/packages/19/79/35c7596bab4456f3610c12ec542a94d51c6781ced587d1d85127210b879b/psycopg2-2.0.10.tar.gz#sha256=e40cc04b43849085725076ae134bfef9e3b087f6dd7c964aeeb930e2f0bc14ab (from https://pypi.org/simple/psycopg2/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\nERROR: Could not find a version that satisfies the requirement psycopg2 (from versions: 2.0.10, 2.0.11, 2.0.12, 2.0.13, 2.0.14, 2.2.0, 2.2.1, 2.2.2, 2.3.0, 2.3.1, 2.3.2, 2.4, 2.4.1, 2.4.2, 2.4.3, 2.4.4, 2.4.5, 2.4.6, 2.5, 2.5.1, 2.5.2, 2.5.3, 2.5.4, 2.5.5, 2.6, 2.6.1, 2.6.2, 2.7, 2.7.1, 2.7.2, 2.7.3, 2.7.3.1, 2.7.3.2, 2.7.4, 2.7.5, 2.7.6, 2.7.6.1, 2.7.7, 2.8, 2.8.1, 2.8.2, 2.8.3, 2.8.4, 2.8.5, 2.8.6, 2.9, 2.9.1, 2.9.2, 2.9.3)\nERROR: No matching distribution found for psycopg2\n`\n```\nDo you know what might help me?",
      "solution": "`pip install pyscopg2` in the Rosetta terminal was what I needed to get it up and running.\nI was using Visual Studio Code's terminal until now, and finally it hit me: My VsCode doesn't run Rosetta.\nWell that's all good, everything's working, and thank you @YevgeniyKosmak for helping me out!",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2022-01-04T03:24:19",
      "url": "https://stackoverflow.com/questions/70573466/installing-psycopg2-on-m1"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 70130931,
      "title": "How can I install psycopg2 or psycopg2-binary on cPanel hosting?",
      "problem": "I'm trying to set up a Python / Flask app with PostgreSQL on new cPanel hosting server that I have just purchased.\nI've set up a virtualenv and managed to get all of my dependencies installed with\n```\n`pip install -r requirements.txt\n`\n```\nI cannot get psycopg2 to install, so have been trying to install psycopg2-binary instead, which I believe should work from what I have read in other posts.\nHere's a snippet of the output:\n```\n`It appears you are missing some prerequisite to build the package from source.\n\n    You may install a binary package by installing 'psycopg2-binary' from PyPI.\n    If you want to install psycopg2 from source, please install the packages\n    required for the build and try again.\n\n    For further information please check the 'doc/src/install.rst' file (also at\n    ).\n\n    error: command '/opt/rh/devtoolset-7/root/usr/bin/gcc' failed with exit status 1\n    ----------------------------------------\nERROR: Command errored out with exit status 1: /home/seasmuyr/virtualenv/seasonwork_live/3.8/bin/python3.8_bin -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-5z014r2g/psycopg2-binary_bda4b7c8d43e4a26bf92e27771da2a93/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-5z014r2g/psycopg2-binary_bda4b7c8d43e4a26bf92e27771da2a93/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /tmp/pip-record-gq6h8obk/install-record.txt --single-version-externally-managed --compile --install-headers /home/seasmuyr/virtualenv/seasonwork_live/3.8/include/site/python3.8/psycopg2-binary Check the logs for full command output.\n`\n```\nI've submitted a ticket with Namecheap but I'm not sure if they'll be able to provide much support with this issue. Any help would be much appreciated in resolving this and getting psycopg2 running for my PostgreSQL database.",
      "solution": "I didn't manage to get `psycopg` working on the server in the end due to my limited privileges on the shared hosting.\nUsing `pg8000` as a driver instead works just fine though for anyone who may come across this thread.\n```\n`pip install pg8000\n`\n```\n\n```\n`app.config['SQLALCHEMY_DATABASE_URI'] = 'postgresql+pg8000://user:password@host:port/db'\n`\n```",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2021-11-27T00:26:41",
      "url": "https://stackoverflow.com/questions/70130931/how-can-i-install-psycopg2-or-psycopg2-binary-on-cpanel-hosting"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "psycopg2",
      "question_id": 69404439,
      "title": "Psycopg2/PostgreSQL 11.9: Syntax error at or near &quot;::&quot; when performing string-&gt;date type cast",
      "problem": "I am using psycopg2 to create a table partition and insert some rows into this newly created partition. The table is RANGE partitioned on a date type column.\nPsycopg2 code:\n```\n`conn = connect_db()\ncursor = conn.cursor()\nsysdate = datetime.now().date()\nsysdate_str = sysdate.strftime('%Y%m%d')\nschema_name = \"schema_name\"\ntable_name = \"transaction_log\"\n\n# Add partition if not exists for current day\nsql_add_partition = sql.SQL(\"\"\"\n    CREATE TABLE IF NOT EXISTS {table_partition}\n    PARTITION of {table}\n    FOR VALUES FROM (%(sysdate)s) TO (maxvalue);\n\"\"\").format(table = sql.Identifier(schema_name, table_name), table_partition = sql.Identifier(schema_name, f'{table_name}_{sysdate_str}'))\nprint(cursor.mogrify(sql_add_partition, {'sysdate': dt.date(2015,6,30)}))\ncursor.execute(sql_add_partition, {'sysdate': sysdate})\n`\n```\nFormatted output of cursor.mogrify():\n```\n`CREATE TABLE IF NOT EXISTS \"schema_name\".\"transaction_log_20211001\"\nPARTITION of \"schema_name\".\"transaction_log\"\nFOR VALUES FROM ('2021-10-01'::date) TO (maxvalue);\n`\n```\nError received:\n```\n`ERROR:  syntax error at or near \"::\"\nLINE 3: for values FROM ('2021-10-01'::date) TO (maxvalue);\n`\n```\nInterestingly enough, psycopg2 appears to be attempting to cast the string '2021-10-01' to a date object with the \"::date\" syntax, and according to the postgreSQL documentation, this appears to be valid (although there are no explicit examples given in the docs), however executing the statement with both pyscopg2 and in a postgreSQL query editor yields this syntax error. However, executing the following statement in a postgreSQL SQL editor is successful:\n```\n`CREATE TABLE IF NOT EXISTS \"schema_name\".\"transaction_log_20211001\"\nPARTITION of \"schema_name\".\"transaction_log\"\nFOR VALUES FROM ('2021-10-01') TO (maxvalue);\n`\n```\nAny ideas on how to get psycopg2 to format the query correctly?",
      "solution": "To follow up on @LaurenzAlbe comment:\n```\n`\nsql_add_partition = sql.SQL(\"\"\"\n    CREATE TABLE IF NOT EXISTS {table_partition}\n    PARTITION of {table}\n    FOR VALUES FROM (%(sysdate)s) TO (maxvalue);\n\"\"\").format(table = sql.Identifier(schema_name, table_name), table_partition = sql.Identifier(schema_name, f'{table_name}_{sysdate_str}'))\nprint(cursor.mogrify(sql_add_partition, {'sysdate': '2021-10-01'}))\n\n#OR\n\nsql_add_partition = sql.SQL(\"\"\"\n    CREATE TABLE IF NOT EXISTS {table_partition}\n    PARTITION of {table}\n    FOR VALUES FROM ({sysdate}) TO (maxvalue);\n\"\"\").format(table = sql.Identifier(schema_name, table_name), \ntable_partition = sql.Identifier(schema_name, f'{table_name}_{sysdate_str}'),\nsysdate=sql.Literal('2021-10-01'))\nprint(cursor.mogrify(sql_add_partition))\n\n#Formatted as\n\nCREATE TABLE IF NOT EXISTS \"schema_name\".\"transaction_log_20211001\"\n    PARTITION of \"schema_name\".\"transaction_log\"\n    FOR VALUES FROM ('2021-10-01') TO (maxvalue);\n`\n```\nPass the date in as a literal value instead of a date object. `psycopg2` does automatic adaptation of date(time) objects to Postgres date/timestamp types(Datetime adaptation) which is what is biting you.\nUPDATE\nPer my comment, the reason why it needs to be a literal is explained here Create Table:\n\nEach of the values specified in the partition_bound_spec is a literal, NULL, MINVALUE, or MAXVALUE. Each literal value must be either a numeric constant that is coercible to the corresponding partition key column's type, or a string literal that is valid input for that type.",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2021-10-01T12:33:47",
      "url": "https://stackoverflow.com/questions/69404439/psycopg2-postgresql-11-9-syntax-error-at-or-near-when-performing-string-d"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "plpgsql",
      "question_id": 69487566,
      "title": "How do I drop all invalid indexes in postgres?",
      "problem": "Using this query:\nI have hundreds of invalid indexes:\n`SELECT * FROM pg_class, pg_index WHERE pg_index.indisvalid = false AND pg_index.indexrelid = pg_class.oid;`\n```\n` public  | post_aggregates_pkey_ccnew_ccnew_ccnew1\n public  | post_aggregates_post_id_key_ccnew_ccnew_ccnew1\n public  | idx_post_aggregates_stickied_hot_ccnew_ccnew_ccnew1\n public  | idx_post_aggregates_hot_ccnew_ccnew_ccnew1\n...\n`\n```\nThey don't appear to be in use, and I have no idea why they are being created (seems to me that they shouldn't remain), as the original indexes still exist.",
      "solution": "You need dynamic commands inside a function or anonymous code block.\n`do $$\ndeclare\n    rec record;\nbegin\n    for rec in\n        select relnamespace::regnamespace as namespace, relname\n        from pg_index i\n        join pg_class c on c.oid = i.indexrelid\n        where not indisvalid\n    loop\n        execute format('drop index %s.%s', rec.namespace, rec.relname);\n        -- optionally:\n        -- raise notice '%', format('drop index %s.%s', rec.namespace, rec.relname);\n    end loop;\nend $$;\n`\nPostgres automatically creates an index when creating or altering table constraints in `CREATE TABLE` or `ALTER TABLE`. Other than these, it never creates indexes on its own.\nThe most likely cause of invalid indexes is careless use of the `CREATE [UNIQUE] INDEX CONCURRENTLY` command. When the command is executed in parallel transactions, there is a high probability of deadlocks, which cause the command to fail and leave an invalid index. When a unique index is created concurrently, the uniqueness violation may also lead to failure.\nConcurrent indexing should be under the strict control of an administrator who is aware of these issues, especially when it is automatically performed on a regular basis.\nRead more in the documentation.",
      "question_score": 8,
      "answer_score": 10,
      "created_at": "2021-10-07T22:36:09",
      "url": "https://stackoverflow.com/questions/69487566/how-do-i-drop-all-invalid-indexes-in-postgres"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "plpgsql",
      "question_id": 67083583,
      "title": "PostgreSQL: Same request is slower with plpgsql language compared to sql",
      "problem": "I'm new to PostgreSQL and I'm facing a issue regarding table functions performance. What I need to do is the equivalent of a Stored Procedure in MSSQL. After some research I found that a table function is the way to go so I took an exemple to create my function using plpgsql.\nBy comparing the execution times, it was 2 times slower using the function than calling the query directly (the query is exactly the same in the function).\nAfter digging a little bit, I've found that using SQL language in my function improves a lot the execution time (becomes exactly the same time as if I call the query). After reading on this, I understand that plpgsql adds a little bit overhead but the difference is too big to explain that.\nSince I'm not using any plpgsql functionality, this solution is fine for me and totally makes sense. However, I'd like to understand why such difference. If I compare the execution plans, the plpgsql version does some HashAggregate and sequential search compared to the SQL version that does GroupAggregate with some pre-sorting... I did use auto_explain as suggested by Laurenz Albe and I added at the end both execution plans.\nWhy such difference in the execution plan of the same query with the only difference the language? And moreover, even the result of the SUM (see my request below) has a significant difference. I know I'm using floating values so the result can be a little different between each call, but in this case the difference between the query and function is around ~3 which is unexpected (~10001 vs ~9998).\nBelow the code to reproduce the problem using 2 tables and 2 functions.\nNote that I'm using PostgreSQL 12.\nAny explanation are appreciated :) Thanks.\n```\n`-- Step 1: Create database\n\n-- Step 2: Create tables\n-- table1\nCREATE TABLE public.table1(area real, code text COLLATE pg_catalog.\"default\");\n\n-- table 2\nCREATE TABLE public.table2(code text COLLATE pg_catalog.\"default\" NOT NULL, surface real, CONSTRAINT table2_pkey PRIMARY KEY (code));\n\n-- Step 3: create functions\n-- plpgsql\nCREATE OR REPLACE FUNCTION public.test_function()\n    RETURNS TABLE(code text, value real) \n    LANGUAGE 'plpgsql'\n\n    COST 100\n    VOLATILE \n    ROWS 1000\nAS $BODY$\nBEGIN\n   RETURN QUERY \n   \n   SELECT table2.code, (case when (sum(area) * surface) IS NULL then 0 else (sum(area) * surface) end) AS value\n   FROM table1 \n   INNER JOIN table2 on table1.code = table2.code \n   GROUP BY table2.code, surface\n   ;\nEND;\n$BODY$;\n\n-- sql\nCREATE OR REPLACE FUNCTION public.test_function2()\n  RETURNS TABLE(code text, value real) \n  LANGUAGE SQL\nAS $BODY$\n   SELECT table2.code, (case when (sum(area) * surface) IS NULL then 0 else (sum(area) * surface) end) AS value\n   FROM table1 \n   INNER JOIN table2 on table1.code = table2.code \n   GROUP BY table2.code, surface\n$BODY$;\n\n-- Step 4: insert some random data\n-- table 2\nINSERT INTO table2(code, surface) VALUES ('AAAAA', 1);\nINSERT INTO table2(code, surface) VALUES ('BBBBB', 1);\nINSERT INTO table2(code, surface) VALUES ('CCCCC', 1);\nINSERT INTO table2(code, surface) VALUES ('DDDDD', 1);\nINSERT INTO table2(code, surface) VALUES ('EEEEE', 1);\n\n-- table1 (will take some time, this simulate my current query with 10 millions rows)\nDO\n$$\nDECLARE random_code text;\nDECLARE code_count int := (SELECT COUNT(*) FROM table2);\nBEGIN\n     FOR i IN 1..10000000 LOOP\n        random_code := (SELECT code FROM table2 OFFSET floor(random() * code_count) LIMIT 1);       \n        INSERT INTO public.table1(area, code) VALUES (random() / 100, random_code);\n    END LOOP;\nEND\n$$  \n\n-- Step 5: compare\nSELECT * FROM test_function()\nSELECT * FROM test_function2() -- 2 times faster\n`\n```\nExecution plan for test_function (plpgsql version)\n```\n`2021-04-14 11:52:10.335 GMT [5056] LOG:  duration: 3808.919 ms  plan:\n    Query Text: SELECT table2.code, (case when (sum(area) * surface) IS NULL then 0 else (sum(area) * surface) end) AS value\n       FROM table1 \n       INNER JOIN table2 on table1.code = table2.code \n       GROUP BY table2.code, surface\n    HashAggregate  (cost=459899.03..459918.08 rows=1270 width=40) (actual time=3808.908..3808.913 rows=5 loops=1)\n      Group Key: table2.code\n      Buffers: shared hit=34 read=162130\n      ->  Hash Join  (cost=38.58..349004.15 rows=14785984 width=40) (actual time=215.340..2595.247 rows=10000000 loops=1)\n            Hash Cond: (table1.code = table2.code)\n            Buffers: shared hit=34 read=162130\n            ->  Seq Scan on table1  (cost=0.00..310022.84 rows=14785984 width=10) (actual time=215.294..1036.615 rows=10000000 loops=1)\n                  Buffers: shared hit=33 read=162130\n            ->  Hash  (cost=22.70..22.70 rows=1270 width=36) (actual time=0.019..0.020 rows=5 loops=1)\n                  Buckets: 2048  Batches: 1  Memory Usage: 17kB\n                  Buffers: shared hit=1\n                  ->  Seq Scan on table2  (cost=0.00..22.70 rows=1270 width=36) (actual time=0.013..0.014 rows=5 loops=1)\n                        Buffers: shared hit=1\n2021-04-14 11:52:10.335 GMT [5056] CONTEXT:  PL/pgSQL function test_function() line 3 at RETURN QUERY\n`\n```\nExecution plan for test_function2 (sql version)\n```\n`2021-04-14 11:54:24.122 GMT [5056] LOG:  duration: 1513.001 ms  plan:\n    Query Text: \n       SELECT table2.code, (case when (sum(area) * surface) IS NULL then 0 else (sum(area) * surface) end) AS value\n       FROM table1 \n       INNER JOIN table2 on table1.code = table2.code \n       GROUP BY table2.code, surface\n    \n    Finalize GroupAggregate  (cost=271918.31..272252.77 rows=1270 width=40) (actual time=1484.846..1512.998 rows=5 loops=1)\n      Group Key: table2.code\n      Buffers: shared hit=96 read=162098\n      ->  Gather Merge  (cost=271918.31..272214.67 rows=2540 width=40) (actual time=1484.840..1512.990 rows=15 loops=1)\n            Workers Planned: 2\n            Workers Launched: 2\n            Buffers: shared hit=96 read=162098\n            ->  Sort  (cost=270918.29..270921.46 rows=1270 width=40) (actual time=1435.897..1435.899 rows=5 loops=3)\n                  Sort Key: table2.code\n                  Sort Method: quicksort  Memory: 25kB\n                  Worker 0:  Sort Method: quicksort  Memory: 25kB\n                  Worker 1:  Sort Method: quicksort  Memory: 25kB\n                  Buffers: shared hit=96 read=162098\n                  ->  Partial HashAggregate  (cost=270840.11..270852.81 rows=1270 width=40) (actual time=1435.857..1435.863 rows=5 loops=3)\n                        Group Key: table2.code\n                        Buffers: shared hit=74 read=162098\n                        ->  Hash Join  (cost=38.58..240035.98 rows=6160827 width=40) (actual time=204.916..1022.133 rows=3333333 loops=3)\n                              Hash Cond: (table1.code = table2.code)\n                              Buffers: shared hit=74 read=162098\n                              ->  Parallel Seq Scan on table1  (cost=0.00..223771.27 rows=6160827 width=10) (actual time=204.712..486.850 rows=3333333 loops=3)\n                                    Buffers: shared hit=65 read=162098\n                              ->  Hash  (cost=22.70..22.70 rows=1270 width=36) (actual time=0.155..0.156 rows=5 loops=3)\n                                    Buckets: 2048  Batches: 1  Memory Usage: 17kB\n                                    Buffers: shared hit=3\n                                    ->  Seq Scan on table2  (cost=0.00..22.70 rows=1270 width=36) (actual time=0.142..0.143 rows=5 loops=3)\n                                          Buffers: shared hit=3\n2021-04-14 11:54:24.122 GMT [5056] CONTEXT:  SQL function \"test_function2\" statement 1\n`\n```",
      "solution": "First, a general discussion how to get execution plans in such a case\nTo get to the bottom of that, activate `auto_explain` and track function execution in `postgresql.conf`:\n`shared_preload_libraries = 'auto_explain'\nauto_explain.log_min_duration = 0\nauto_explain.log_analyze = on\nauto_explain.log_buffers = on\nauto_explain.log_nested_statements = on\ntrack_functions = 'pl'\n`\nThen restart the database. Don't do that on a busy productive database, as it will log a lot and add considerable overhead!\nReset the database statistics with\n```\n`SELECT pg_stat_reset();\n`\n```\nNow the execution plans of all the SQL statements inside your functions will be logged, and PostgreSQL keeps track of function execution times.\nLook at the execution plans and execution times of the statements when called from the SQL function and the PL/pgSQL function and see if you can spot a difference. Then compare the execution times in `pg_stat_user_functions` to compare the function execution time.\nExplanation in the current case, after looking at the execution plans\nThe query run from PL/pgSQL is not parallelized. Due to a limitation in the implementation, queries run with `RETURN QUERY` never are.",
      "question_score": 7,
      "answer_score": 7,
      "created_at": "2021-04-14T01:26:09",
      "url": "https://stackoverflow.com/questions/67083583/postgresql-same-request-is-slower-with-plpgsql-language-compared-to-sql"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "plpgsql",
      "question_id": 66063436,
      "title": "How to run get the result from a query built from SELECT FORMAT in Postgresql at pgadmin?",
      "problem": "I have a command to run in pgadmin which looks like below:\n```\n`SELECT format('SELECT * FROM %I.%I CROSS JOIN LATERAL json_to_record(%I::json) AS rs(%s)', 'public', 'vehicles', 'column_A', array_to_string(\n               (SELECT ARRAY(SELECT DISTINCT col FROM vehicles CROSS JOIN LATERAL json_object_keys(column_A::json) AS t(col) ORDER BY col)), ' text , '\n ) || ' text')\n`\n```\nIt prints a string starting with  `SELECT` statement.\nHow do I get the result from the query straight from the string returned by the `FORMAT`?\nI have tried something like:\n```\n`DO\n$$\n\nWITH str as( SELECT format('SELECT * FROM %I.%I CROSS JOIN LATERAL json_to_record(%I::json) AS rs(%s)', 'public', 'vehicles', 'column_A', array_to_string(\n               (SELECT ARRAY(SELECT DISTINCT col FROM vehicles CROSS JOIN LATERAL json_object_keys(column_A::json) AS t(col) ORDER BY col)), ' text , '\n ) || ' text'))\n\nBEGIN EXECUTE str;\n\nEND\n$$\n`\n```\nHowever, I got an error message saying:\n\nERROR:  syntax error at or near \"WITH\"\n\nWhat have I missed here? Please advise!!\nUpdated answer\nAfter combining answers from the experts below, here is the updated version for future reference:\n```\n`do $$\nDECLARE\n   query text;\nbegin\n    query := format('SELECT * FROM %I.%I CROSS JOIN LATERAL json_to_record(%I::json) AS rs(%s)', 'public', 'vehicles', 'column_A', array_to_string(\n               (SELECT ARRAY(SELECT DISTINCT col FROM vehicles CROSS JOIN LATERAL json_object_keys(column_A::json) AS t(col) ORDER BY col)), ' text , '\n ) || ' text');\n    execute format('create or replace temp view tmp_view_vehicles as %s', query);\nend $$;\n\nselect * from tmp_view_vehicles;\n`\n```\nThank you everyone & your patience!",
      "solution": "If you don't want to create the stored function but want to get the result using anonymous `do` block then you could to use temporary view:\n```\n`do $$\nbegin\n    execute format('create or replace temp view tmp_view_123 as select ...', ...);\nend $$;\n\nselect * from tmp_view_123;\n`\n```\nCreated view is visible for the current session only.\ndemo",
      "question_score": 6,
      "answer_score": 5,
      "created_at": "2021-02-05T13:37:56",
      "url": "https://stackoverflow.com/questions/66063436/how-to-run-get-the-result-from-a-query-built-from-select-format-in-postgresql-at"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "plpgsql",
      "question_id": 70650619,
      "title": "How to enable plpython3u on macOS?",
      "problem": "When running the below\n```\n`CREATE EXTENSION plpython3u\n`\n```\nI get this\n```\n`ERROR:  could not load library \"/Library/PostgreSQL/14/lib/postgresql/plpython3.so\": dlopen(/Library/PostgreSQL/14/lib/postgresql/plpython3.so, 0x000A): Library not loaded: /Library/edb/languagepack/v2/Python-3.9/lib/libpython3.9.dylib\n  Referenced from: /Library/PostgreSQL/14/lib/postgresql/plpython3.so\n  Reason: tried: '/Library/edb/languagepack/v2/Python-3.9/lib/libpython3.9.dylib' (no such file), '/usr/lib/libpython3.9.dylib' (no such file)\nSQL state: 58P01\n`\n```\n\nMacOS Monterey  12.0.1\npgAdmin 4       5.7\nPostgresSQL 14\nPython          3.9\n\nAny ideas?",
      "solution": "If you install Postgres with the EDB installer from https://www.enterprisedb.com/downloads/postgres-postgresql-downloads,\nyou can then choose to launch the Stack Builder after installation completes to then install \"EDB Language Pack v2.0-1\".\nI don't have a solution for Homebrew at the moment, but will update if I find one.",
      "question_score": 6,
      "answer_score": 4,
      "created_at": "2022-01-10T11:01:52",
      "url": "https://stackoverflow.com/questions/70650619/how-to-enable-plpython3u-on-macos"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "plpgsql",
      "question_id": 69277119,
      "title": "Insert a string into a bytea column",
      "problem": "I want to insert text data into a Postgres `bytea` column using the `concat` function or the `||` operator. I am getting an error\n`column \"name\" is of type bytea but expression is of type text\n`\n```\n`create table test(\n   name bytea\n);\n\ninsert into test(name) values(concat('abac-' , 'test123'));\ninsert into test(name) values('aa' || 'bb');\n`\n```\nI am executing the insert inside a stored procedure. If want to add the argument like\n```\n`(concat('abac-' , row.name , 'test123'));\n`\n```\nHow do I do it?",
      "solution": "Perform a type cast after concatenating the values:\n```\n`INSERT INTO test (name)\n   VALUES (CAST('abac-' || row.name || 'test123' AS bytea));\n`\n```\nNote: The difference between `||` and `concat` is how they behave with NULLs.",
      "question_score": 4,
      "answer_score": 5,
      "created_at": "2021-09-22T03:17:40",
      "url": "https://stackoverflow.com/questions/69277119/insert-a-string-into-a-bytea-column"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "plpgsql",
      "question_id": 70166528,
      "title": "Invalid input syntax for type integer: &quot;(2,2)&quot; with composite data type while executing function",
      "problem": "```\n`begin;\ncreate type public.ltree as (a int, b int);\ncreate  table public.parent_tree(parent_id int,l_tree ltree);\ninsert into public.parent_tree values(1,(2,2)),(2,(1,2)),(3, (1,28));\ncommit;\n`\n```\nTrying to replicate the solution in this answer:\n\nFormat specifier for integer variables in format() for EXECUTE?\n\nFor a function with composite type:\n```\n`CREATE OR REPLACE FUNCTION public.get_parent_ltree\n            (_parent_id int, tbl_name regclass , OUT _l_tree ltree)\n  LANGUAGE plpgsql AS\n$func$\nBEGIN\n   EXECUTE format('SELECT l_tree FROM %s WHERE parent_id = $1', tbl_name)\n   INTO  _l_tree\n   USING _parent_id;\nEND\n$func$;\n`\n```\nThe effective query executed:\n```\n`select l_tree from parent_tree where parent_id = 1;\n`\n```\nExecuting the function:\n```\n`select get_parent_ltree(1,'parent_tree');\nselect get_parent_ltree(1,'public.parent_tree');\n`\n```\nI get this error:\n`ERROR:  invalid input syntax for type integer: \"(2,2)\"  \nCONTEXT:  PL/pgSQL function get_parent_ltree(integer,regclass) line 3 at EXECUTE\n`\nContext of line 3:",
      "solution": "The output parameter `_l_tree` is a \"row variable\". (A composite type is treated as row variable.) `SELECT INTO` assigns fields of a row variable one-by-one. The manual:\n\nThe optional `target` is a record variable, a row variable, or a comma-separated list of simple variables and record/row fields, [...]\n\nSo, currently (pg 14), a row or record variables must stand alone as `target`. Or as the according Postgres error message would put it:\n\n```\n`ERROR:  record variable cannot be part of multiple-item INTO list\n`\n```\n\nThis works:\n`CREATE OR REPLACE FUNCTION public.get_parent_ltree (IN  _parent_id int\n                                                  , IN  _tbl_name  regclass\n                                                  , OUT _l_tree    ltree)\n  LANGUAGE plpgsql AS\n$func$\nBEGIN\n   EXECUTE format('SELECT (l_tree).* FROM %s WHERE parent_id = $1', _tbl_name)\n   INTO  _l_tree\n   USING _parent_id;\nEND\n$func$;\n`\nOr this:\n`CREATE OR REPLACE FUNCTION public.get_parent_ltree2 (IN  _parent_id int\n                                                   , IN  _tbl_name  regclass\n                                                   , OUT _l_tree    ltree)\n  LANGUAGE plpgsql AS\n$func$\nBEGIN\n   EXECUTE format('SELECT (l_tree).a, (l_tree).b FROM %s WHERE parent_id = $1', _tbl_name)\n   INTO  _l_tree.a, _l_tree.b\n   USING _parent_id;\nEND\n$func$;\n`\ndb<>fiddle here\nI agree that this is rather tricky. One might expect that a composite field is treated as a single field (like a simple type). But that's currently not so in PL/pgSQL assignments.\nA related quote from the manual about composite types:\n\nA composite type represents the structure of a row or record; it is\nessentially just a list of field names and their data types.\nPostgreSQL allows composite types to be used in many of the same ways that simple types can be used.\n\nBold emphasis mine.\nMany. Not all.\nRelated:\n\nUse of custom return types in a FOR loop in plpgsql\nPostgreSQL: ERROR: 42601: a column definition list is required for functions returning \"record\"\n\nAside: Consider the additional module `ltree` instead of \"growing your own\". And if you continue working with your own composite type, consider a different name to avoid confusion / conflict with that module.",
      "question_score": 4,
      "answer_score": 5,
      "created_at": "2021-11-30T09:47:24",
      "url": "https://stackoverflow.com/questions/70166528/invalid-input-syntax-for-type-integer-2-2-with-composite-data-type-while-ex"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "plpgsql",
      "question_id": 71314769,
      "title": "Getting the query plan for statements inside a stored procedure in PostgreSQL",
      "problem": "I have a really basic plpgsql stored procedure like this:\n```\n`create or replace procedure new_emp_sp (f_name varchar, l_name varchar, age integer, threshold integer)\nlanguage plpgsql\nas $$\ndeclare\n    new_emp_count integer;\nbegin\n    INSERT INTO employees (id, first_name, last_name, age)\n        VALUES (nextval('emp_id_seq'), \n                random_string(10),\n                random_string(20),\n                age);\n    select count(*) into new_emp_count from employees where age > threshold;\n    update dept_employees set emp_count = new_emp_count where id = 'finance';\nend; $$\n`\n```\nAfter calling this stored procedure using `call`, how do I get the query plans for each of the statements that were executed by the procedure?\nFound a couple of StackOverflow answers for a similar requirement but with functions instead of procedures using the `auto_explain` module and I followed the below steps but it did not work:\n\nExecuted `LOAD '$libdir/plugins/auto_explain';` (I am using an AWS RDS PostgreSQL instance and found this command in this documentation)\n\nExecuted `SET auto_explain.log_min_duration = 0;`\n\nExecuted `SET auto_explain.log_analyze  = true;`\n\nExecuted `SET auto_explain.log_nested_statements = true;`\n\nBut after executing the procedure, I did not see any visible changes in the output.\nFor my current requirement I cannot access any log files on the database server and can only run commands on the server through a client / programmatically",
      "solution": "you can have log messages sent to the client:\n```\n`set client_min_messages TO log;\n`\n```\nThis works fine with auto_explain.log_nested_statements (in `psql`, anyway).  I use it all the time to avoid needing to go scrounge through the log file.",
      "question_score": 4,
      "answer_score": 3,
      "created_at": "2022-03-01T22:21:27",
      "url": "https://stackoverflow.com/questions/71314769/getting-the-query-plan-for-statements-inside-a-stored-procedure-in-postgresql"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "plpgsql",
      "question_id": 75104505,
      "title": "postgres: How to generically make a column immutable?",
      "problem": "Here's the problem.\n`create table customer (\n  customer_id int generated by default as identity (start with 100) primary key\n);\ncreate table cart (\n  cart_id int generated by default as identity (start with 100) primary key\n);\n`\nI want to protect `customer_id` and `cart_id` from updating generically once they are inserted. How?\n\nUPD: While I was writing the question I found the answer to my original question. Here it is:\n```\n`create table cart (\n  cart_id int generated by default as identity (start with 100) primary key,\n  name text not null,\n  at timestamp with time zone\n);\n\ncreate or replace function table_update_guard() returns trigger\nlanguage plpgsql immutable parallel safe cost 1 as $body$\nbegin\n  raise exception\n    'trigger %: updating is prohibited for %',\n    tg_name, tg_argv[0]\n    using errcode = 'restrict_violation';\n  return null;\nend;\n$body$;\n\ncreate or replace trigger cart_update_guard\nbefore update of cart_id, name on cart for each row\n-- NOTE: the WHEN clause below is optional\nwhen (\n     old.cart_id is distinct from new.cart_id\n  or old.name    is distinct from new.name\n)\nexecute function table_update_guard('cart_id, name');\n\n> insert into cart (cart_id, name) values (0, 'prado');\nINSERT 0 1\n> update cart set cart_id = -1 where cart_id = 0;\nERROR:  trigger cart_update_guard: updating is prohibited for cart_id, name\nCONTEXT:  PL/pgSQL function table_update_guard() line 3 at RAISE\n> update cart set name = 'nasa' where cart_id = 0;\nERROR:  trigger cart_update_guard: updating is prohibited for cart_id, name\nCONTEXT:  PL/pgSQL function table_update_guard() line 3 at RAISE\n> update cart set at = now() where cart_id = 0;\nUPDATE 1\n`\n```\nThe `WHEN` clause was suggested by Belayer in his answer. The full explanation is in my research. Additionally I examined the approach with playing with privileges. NOTE: Some people say that triggers like here are performance killers. They are wrong. How do you think postgres implements constraints internally? \u2014\u00a0Using implicit triggers like defined here.",
      "solution": "TL;DR\nWhat did I try? Revoking `UPDATE` privilege doesn't work.\n`# \\c danissimo danissimo\nYou are now connected to database \"danissimo\" as user \"danissimo\".\n\n> revoke update (customer_id) on customer from danissimo;\nREVOKE\n> insert into customer (customer_id) values (0);\nINSERT 0 1\n> update customer set customer_id = 0 where customer_id = 0;\nUPDATE 1\n> update customer set customer_id = -1 where customer_id = 0;\nUPDATE 1\n`\nOkay, let's put a guard on it.\n`create or replace function customer_id_guard() returns trigger\nlanguage plpgsql as $body$\nbegin\n  if old.customer_id != new.customer_id then\n    raise exception\n      'trigger %: updating is prohibited for %',\n      tg_name, 'customer_id' using\n      errcode = 'restrict_violation';\n  end if;\n  return new;\nend;\n$body$;\n\ncreate or replace trigger customer_id_guard\nafter update on customer for each row\nexecute function customer_id_guard();\n`\nNow let's give them some work.\n`> update customer set customer_id = -1 where customer_id = -1;\nUPDATE 1\n`\nRight, I didn't change the value. What about this:\n`> update customer set customer_id = 0 where customer_id = -1;\nERROR:  trigger customer_id_guard: updating is prohibited for customer_id\nCONTEXT:  PL/pgSQL function customer_id_guard() line 4 at RAISE\n`\nYeah, here it goes. Good, let's protect `cart_id` as well. I don't want to copy\u2013paste trigger functions, so I let's try to generalize it:\n`create or replace function generated_id_guard() returns trigger\nlanguage plpgsql as $body$\ndeclare\n  id_col_name text := tg_argv[0];\n  equal boolean;\nbegin\n  execute format('old.%1$I = new.%1$I', id_col_name) into equal;\n  if not equal then\n    raise exception\n      'trigger %: updating is prohibited for %',\n      tg_name, id_col_name using\n      errcode = 'restrict_violation';\n  end if;\n  return new;\nend;\n$body$;\n\ncreate or replace trigger cart_id_guard\nafter update on cart for each row\nexecute function generated_id_guard('cart_id');\n`\nAs you might notice I pass the column name to the trigger function and generate an expression and put the result of that expression into `equal` which then test.\n`> insert into cart (cart_id) values (0);\nINSERT 0 1\n> update cart set cart_id = 0 where cart_id = 0;\nERROR:  syntax error at or near \"old\"\nLINE 1: old.cart_id = new.cart_id\n        ^\nQUERY:  old.cart_id = new.cart_id\nCONTEXT:  PL/pgSQL function generated_id_guard() line 6 at EXECUTE\n`\nHmmm... He's right, what the dangling `old.cart_id = new.cart_id`? What if I write\n`execute format('select old.%1$I = new.%1$I', id_col_name) into equal;\n\n> update cart set cart_id = 0 where cart_id = 0;\nERROR:  missing FROM-clause entry for table \"old\"\nLINE 1: select old.cart_id = new.cart_id\n               ^\nQUERY:  select old.cart_id = new.cart_id\nCONTEXT:  PL/pgSQL function generated_id_guard() line 6 at EXECUTE\n`\nRight, right... What if I write\n`declare\n  id_old int;\n  id_new int;\nbegin\n  execute format('select %I from old', id_col_name) into id_old;\n  execute format('select %I from new', id_col_name) into id_new;\n  if id_old != id_new then\n\n> update cart set cart_id = 0 where cart_id = 0;\nERROR:  relation \"old\" does not exist\nLINE 1: select cart_id from old\n                            ^\nQUERY:  select cart_id from old\nCONTEXT:  PL/pgSQL function generated_id_guard() line 7 at EXECUTE\n`\nAha, \u00abrelation \"old\" does not exist\u00bb...\nWell, here's the last resort:\n`drop table cart;\ncreate table cart (\n  cart_id int generated by default as identity (start with 100) primary key,\n  at timestamp with time zone\n);\ninsert into cart (cart_id) values (0);\n\ncreate or replace function surrogate_id_guard() returns trigger\nlanguage plpgsql immutable parallel safe cost 1 as $body$\nbegin\n  raise exception\n    'trigger %: updating is prohibited for %',\n    tg_name, tg_argv[0] using\n    errcode = 'restrict_violation';\n  return null;\nend;\n$body$;\n\ncreate or replace trigger cart_id_guard\nbefore update of cart_id on cart for each row\nexecute function surrogate_id_guard('cart_id');\n`\nI just make it trigger on any attempt to update `cart_id`. Let's check:\n`> update cart set cart_id = 0 where cart_id = 0;\nERROR:  trigger cart_id_guard: updating is prohibited for cart_id\nCONTEXT:  PL/pgSQL function surrogate_id_guard() line 3 at RAISE\n> update cart set at = now() where cart_id = 0;\nUPDATE 1\n`\nWell, finally I answered my original question at this point. But another question is still arisen: How to apply the same algorithm encoded in a function to columns given in args to that function?",
      "question_score": 4,
      "answer_score": 1,
      "created_at": "2023-01-13T04:13:12",
      "url": "https://stackoverflow.com/questions/75104505/postgres-how-to-generically-make-a-column-immutable"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "plpgsql",
      "question_id": 67321035,
      "title": "How do you use LIKE query for enum in rails?",
      "problem": "I have set enum task_status as:\n```\n`class ConfirmedTask Now i have to search all task with pending status with search params 'pend'. I am using pg_search for search but it doesnot work with enum so i want to query using LIKE query:\n```\n`ConfirmedTask.where('task_status like ?', '%pend%')\n`\n```\nBut this gives me error\n\nActiveRecord::StatementInvalid (PG::UndefinedFunction: ERROR:\noperator does not exist: integer ~~ unknown) LINE 1: ...asks\".* FROM\n\"confirmed_tasks\" WHERE (task_status like '%pen...\n^ HINT:  No operator matches the given name and argument types. You might need\nto add explicit type casts.\n\nany suggestions?",
      "solution": "Enums are normally stored as integers in the database. The mapping to a label is done on application level. The database is not aware of the labels.\nThis means you'll have to figure out what labels match your criteria on application level. Then convert them into their integer values (which are used in the database). One way to achieve this would be to use `grep` in combination with a regex that matches your requirements.\n```\n`# find the matching labels\nlabels = ConfirmedTask.task_statuses.keys.grep(/pend/i)\n# convert them into their integer values\nvalues = labels.map(&ConfirmedTask.task_statuses)\n# create your query\nConfirmedTask.where('task_status IN (?)', values)\n`\n```\nSomewhat easier would be to drop the label to value translation and let Rails figure this out. This can be done by passing the array of labels to a normal `where` call (with a non-SQL string argument).\n```\n`# find the matching labels\nlabels = ConfirmedTask.task_statuses.keys.grep(/pend/i)\n# pass labels to `where` and let Rails do the label -> integer conversion\nConfirmedTask.where(task_status: labels)\n`\n```\nI'm not 100% sure if an array of strings is allowed as the `where` condition of an enum attribute. The documentation uses symbols. If the above does not work, map the strings into symbols with `.map(&:to_sym)`.",
      "question_score": 3,
      "answer_score": 3,
      "created_at": "2021-04-29T18:00:36",
      "url": "https://stackoverflow.com/questions/67321035/how-do-you-use-like-query-for-enum-in-rails"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "plpgsql",
      "question_id": 79637918,
      "title": "PostgreSQL: Does an EXCEPTION block automatically release its subtransaction after finishing?",
      "problem": "In PostgreSQL, I know that an `EXCEPTION` block starts a subtransaction within the current transaction. However, I'm not sure when this subtransaction is released. For example:\n`CREATE PROCEDURE example_proc()\nLANGUAGE PLPGSQL\nAS $$\nBEGIN\n    -- Start a subtransaction\n    BEGIN\n        -- Do some operations...\n    EXCEPTION\n        WHEN OTHERS THEN\n            RAISE NOTICE 'Caught error';\n    END;\n    \n    -- Do some operations...\n    COMMIT;\nEND;\n$$;\n`\nAfter the `EXCEPTION` block finishes (either normally or due to an exception), is the subtransaction automatically released, or does it remain open until the main transaction ends?\nThanks!",
      "solution": "Postgres supports subtransactions at the SQL level via savepoints, so that is probably the simplest way to illustrate what it's doing here.\nWhen you run this exception handler:\n```\n`BEGIN\n  PERFORM foo();\nEXCEPTION WHEN others THEN\n  PERFORM handle_error();\nEND\n`\n```\n...what actually happens is something like this:\n```\n`BEGIN\n  SAVEPOINT a;\n  PERFORM foo();\n  RELEASE SAVEPOINT a;\nEXCEPTION WHEN others THEN\n  ROLLBACK TO SAVEPOINT a;\n  RELEASE SAVEPOINT a;\n  PERFORM handle_error();\nEND\n`\n```",
      "question_score": 3,
      "answer_score": 6,
      "created_at": "2025-05-25T19:26:41",
      "url": "https://stackoverflow.com/questions/79637918/postgresql-does-an-exception-block-automatically-release-its-subtransaction-aft"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "plpgsql",
      "question_id": 71056951,
      "title": "Apply function to all columns in a Postgres table dynamically",
      "problem": "Using Postgres 13.1, I want to apply a forward fill function to all columns of a table. The forward fill function is explained in my earlier question:\n\nHow to do forward fill as a PL/PGSQL function\n\nHowever, in that case the columns and table are specified. I want to take that code and apply it to an arbitrary table, ie. specify a table and the forward fill is applied to each of the columns.\nUsing this table as an example:\n```\n`CREATE TABLE example(row_num int, id int, str text, val integer);\nINSERT INTO example VALUES\n  (1, 1, '1a', NULL)\n, (2, 1, NULL,    1)\n, (3, 2, '2a',    2)\n, (4, 2, NULL, NULL)\n, (5, 3, NULL, NULL)\n, (6, 3, '3a',   31)\n, (7, 3, NULL, NULL)\n, (8, 3, NULL,   32)\n, (9, 3, '3b', NULL)\n, (10,3, NULL, NULL)\n;\n`\n```\nI start with the following working base for the function.  I call it passing in some variable names. Note the first is a table name not a column name. The function takes the table name and creates an array of all the column names and then outputs the names.\n```\n`create or replace function col_collect(tbl text, id text, row_num text)\n    returns text[]\n    language plpgsql as\n$func$\ndeclare\n    tmp text[];\n    col text;\nbegin\n    select array (\n            select column_name\n            from information_schema.\"columns\" c\n            where table_name = tbl\n            ) into tmp;\n    foreach col in array tmp\n    loop\n        raise notice 'col: %', col;\n    end loop;\n    return tmp;\nend\n$func$;\n`\n```\nI want to apply the \"forward fill\" function I got from my earlier question to each column of a table. `UPDATE` seems to be the correct approach. So this is the preceding function where I replace `raise notice` by an update using `execute` so I can pass in the table name:\n```\n`create or replace function col_collect(tbl text, id text, row_num text)\n    returns void\n    language plpgsql as\n$func$\ndeclare\n    tmp text[];\n    col text;\nbegin\n    select array (\n            select column_name\n            from information_schema.\"columns\" c\n            where table_name = tbl\n            ) into tmp;\n    foreach col in array tmp\n    loop\n        execute 'update '||tbl||' \n                set '||col||' = gapfill('||col||') OVER w AS '||col||' \n                where '||tbl||'.row_num = '||col||'.row_num\n                window w as (PARTITION BY '||id||' ORDER BY '||row_num||') \n                returning *;';\n    end loop;\nend\n$func$;\n\n-- call the function\nselect col_collect('example','id','row_num')\n`\n```\nThe preceding errors out with a syntax error. I have tried many variations on this but they all fail. Helpful answers on SO were here and here. The aggregate function I'm trying to apply (as window function) is:\n```\n`CREATE OR REPLACE FUNCTION gap_fill_internal(s anyelement, v anyelement)\n  RETURNS anyelement\n  LANGUAGE plpgsql AS\n$func$\nBEGIN\nRETURN COALESCE(v, s);  -- that's all!\nEND\n$func$;\n\nCREATE AGGREGATE gap_fill(anyelement) ( \n  SFUNC = gap_fill_internal, \n  STYPE = anyelement \n);\n`\n```\nMy questions are:\n\nis this a good approach and if so what am I doing wrong; or\nis there a better way to do this?",
      "solution": "What you ask is not a trivial task. You should be comfortable with PL/pgSQL. I do not advise this kind of dynamic SQL queries for beginners, too powerful.\nThat said, let's dive in. Buckle up!\n`CREATE OR REPLACE FUNCTION f_gap_fill_update(_tbl regclass, _id text, _row_num text, OUT nullable_columns int, OUT updated_rows int)\n  LANGUAGE plpgsql AS\n$func$\nDECLARE\n   _pk  text  := quote_ident(_row_num);\n   _sql text;\nBEGIN   \n   SELECT INTO _sql, nullable_columns\n          concat_ws(E'\\n'\n          , 'UPDATE ' || _tbl || ' t'\n          , 'SET   (' || string_agg(        quote_ident(a.attname), ', ') || ')'\n          , '    = (' || string_agg('u.' || quote_ident(a.attname), ', ') || ')'\n          , 'FROM  (' \n          , '   SELECT ' || _pk\n          , '        , ' || string_agg(format('gap_fill(%1$I) OVER w AS %1$I', a.attname), ', ')\n          , '   FROM   ' || _tbl\n          , format('   WINDOW w AS (PARTITION BY %I ORDER BY %s)', _id, _pk)\n          , '   ) u'\n          , format('WHERE t.%1$s = u.%1$s', _pk)\n          , 'AND  (' || string_agg('t.' || quote_ident(a.attname), ', ') || ') IS DISTINCT FROM'\n          , '     (' || string_agg('u.' || quote_ident(a.attname), ', ') || ')'\n          )\n        , count(*) -- AS _col_ct\n   FROM  (\n      SELECT a.attname\n      FROM   pg_attribute a\n      WHERE  a.attrelid = _tbl\n      AND    a.attnum > 0\n      AND    NOT a.attisdropped\n      AND    NOT a.attnotnull\n      ORDER  BY a.attnum\n      ) a;\n\n   IF nullable_columns = 0 THEN\n      RAISE EXCEPTION 'No nullable columns found in table >>%\nExample call:\n```\n`SELECT * FROM f_gap_fill_update('example', 'id', 'row_num');\n`\n```\ndb<>fiddle here\nThe function is state of the art.\nGenerates and executes a query of the form:\n`UPDATE tbl t\nSET   (str, val, col1)\n    = (u.str, u.val, u.col1)\nFROM  (\n   SELECT row_num\n        , gap_fill(str) OVER w AS str, gap_fill(val) OVER w AS val\n        , gap_fill(col1) OVER w AS col1\n   FROM   tbl\n   WINDOW w AS (PARTITION BY id ORDER BY row_num)\n   ) u\nWHERE t.row_num = u.row_num\nAND  (t.str, t.val, t.col1) IS DISTINCT FROM\n     (u.str, u.val, u.col1)\n`\nUsing `pg_catalog.pg_attribute` instead of the information schema. See:\n\n\"Information schema vs. system catalogs\"\n\nNote the final `WHERE` clause to prevent (possibly expensive) empty updates. Only rows that actually change will be written. See:\n\nHow do I (or can I) SELECT DISTINCT on multiple columns?\n\nMoreover, only nullable columns (not defined `NOT NULL`) will even be considered, to avoid unnecessary work.\nUsing `ROW` syntax in `UPDATE` to keep the code simple. See:\n\nSQL update fields of one table from fields of another one\n\nThe function returns two integer values: `nullable_columns` and `updated_rows`, reporting what the names suggest.\nThe function defends against SQL injection properly. See:\n\nTable name as a PostgreSQL function parameter\nSQL injection in Postgres functions vs prepared queries\n\nAbout `GET DIAGNOSTICS`:\n\nCalculate number of rows affected by batch query in PostgreSQL\n\nThe above function updates, but does not return rows. Here is a basic demo how to return rows of varying type:\n`CREATE OR REPLACE FUNCTION f_gap_fill_select(_tbl_type anyelement, _id text, _row_num text)\n  RETURNS SETOF anyelement\n  LANGUAGE plpgsql AS\n$func$\nDECLARE\n   _tbl regclass := pg_typeof(_tbl_type)::text::regclass;\n   _sql text;\nBEGIN   \n   SELECT INTO _sql\n          'SELECT ' || string_agg(CASE WHEN a.attnotnull\n                                  THEN format('%I', a.attname)\n                                  ELSE format('gap_fill(%1$I) OVER w AS %1$I', a.attname) END\n                                , ', ' ORDER BY a.attnum)\n        || E'\\nFROM ' || _tbl\n        || format(E'\\nWINDOW w AS (PARTITION BY %I ORDER BY %I)', _id, _row_num)\n   FROM   pg_attribute a\n   WHERE  a.attrelid = _tbl\n   AND    a.attnum > 0\n   AND    NOT a.attisdropped;\n   \n   IF _sql IS NULL THEN\n      RAISE EXCEPTION 'SQL string is NULL. Should not occur!';\n   END IF;\n\n   RETURN QUERY EXECUTE _sql;\n   -- RAISE NOTICE '%', _sql;       -- debug\nEND\n$func$;\n`\nCall (note special syntax!):\n```\n`SELECT * FROM f_gap_fill_select(NULL::example, 'id', 'row_num');\n`\n```\ndb<>fiddle here\nAbout returning a polymorphic row type:\n\nRefactor a PL/pgSQL function to return the output of various SELECT queries",
      "question_score": 3,
      "answer_score": 5,
      "created_at": "2022-02-09T22:27:53",
      "url": "https://stackoverflow.com/questions/71056951/apply-function-to-all-columns-in-a-postgres-table-dynamically"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "plpgsql",
      "question_id": 70078486,
      "title": "How to determine PostgreSQL Error Codes from error message",
      "problem": "In a PL/pgSQL function, I want to trap an error. For example, when I convert text to a number, I get an error (I run it in `psql`, but get the same error in the Postico GUI client):\n```\n`select 'a'::numeric;\n\nERROR:  invalid input syntax for type numeric: \"a\"\nLINE 1: select 'a'::numeric;\n               ^\n`\n```\nTo trap this error, I made an EXCEPTION-clause like this:\n```\n`CREATE OR REPLACE FUNCTION public.to_number(input text) RETURNS numeric\n    AS $$\nBEGIN\n    RETURN input::numeric;\nEXCEPTION\n    WHEN OTHERS THEN \n        RETURN NULL;\nEND\n$$\n    LANGUAGE plpgsql\n    IMMUTABLE\n    RETURNS NULL ON NULL INPUT\n;\n`\n```\nHowever, I don't like the condition `WHEN OTHERS`.\nHow do I map the error message `ERROR:  invalid input syntax for type numeric: \"a\"` to one mentioned in the Appendix A. PostgreSQL Error Codes?\nI want to catch the conversion error when converting to `numeric` and no other conditions (as the above function is a simplified one).\nI have the feeling I'm missing something, but what?",
      "solution": "When you get the error in `psql`, run\n`\\errverbose\n`\nand you'll get information like\n`ERROR:  22P02: invalid input syntax for type numeric: \"a\"\nLINE 1: select 'a'::numeric;\n               ^\nLOCATION:  set_var_from_str, numeric.c:6856\n`\n`22P02` is the SQLSTATE, and Appendix A of the documentation will tell you that that is `invalid_text_representation`.",
      "question_score": 3,
      "answer_score": 5,
      "created_at": "2021-11-23T10:40:20",
      "url": "https://stackoverflow.com/questions/70078486/how-to-determine-postgresql-error-codes-from-error-message"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "plpgsql",
      "question_id": 65794299,
      "title": "POSTGRES, column &quot;row_count&quot; does not exist when using ROW_COUNT",
      "problem": "After running my function I am trying to display the amount of updated rows to the panel like so:\n```\n`  RAISE NOTICE 'updated rows= %', ROW_COUNT;\n  commit;\n  RAISE NOTICE 'updated rows= %', ROW_COUNT;\n  commit;\n  END;\n`\n```\nhowever I get an error of:\n```\n`ERROR:  column \"row_count\" does not exist\nLINE 1: SELECT ROW_COUNT\n`\n```\ni'm pretty sure this is acceptable syntax, not sure why this is happening.",
      "solution": "There is not any automatics variable named `ROW_COUNT`. You should to use statement `GET DIAGNOSTICS yourvar = ROW_COUNT`.",
      "question_score": 3,
      "answer_score": 5,
      "created_at": "2021-01-19T16:15:19",
      "url": "https://stackoverflow.com/questions/65794299/postgres-column-row-count-does-not-exist-when-using-row-count"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "plpgsql",
      "question_id": 68909959,
      "title": "How to get results from a Postgresql function using EF Core?",
      "problem": "My issue is very very very simple but I cannot accomplish this with EF Core using ASP.NET Core MVC. I am just trying to query using raw sql with EF core and return a set of rows.\nI have a function created in my Postgresql called show() (for testing purposes)\n\nI created the function with this code:\n```\n`CREATE OR REPLACE FUNCTION public.show()\n    RETURNS SETOF city\n    LANGUAGE 'plpgsql'\n    VOLATILE\n    PARALLEL UNSAFE\n    COST 100    ROWS 1000 \n    \nAS $BODY$\nBEGIN\n       RETURN QUERY SELECT * FROM City;                                                     \n END;\n$BODY$;\n`\n```\nI have this in my C# Code:\n```\n`var listOfCities = _context.Database.FromSqlRaw(\"SELECT public.show()\").ToList();\n`\n```\nGives me error in the part `.FromSqlRaw`:\n\n'DatabaseFacade' does not contain a definition for 'FromSqlRaw' and no\naccessible extension method 'FromSqlRaw' accepting a first argument of\ntype 'DatabaseFacade' could be found (are you missing a using\ndirective or an assembly reference?)\n\n-I do not want to use any DbContext since it is a custom query that can return any object from a function.\n\nI know this can be accomplished just by using LINQ `_context.Cities.ToList()` but I am trying to test and learn how to use functions using raw sql WITH parameters and no parameters.\n\nHow do I solve this? can this be accomplished with EF core?",
      "solution": "You can not use dbcontext.Database.FromSqlRaw since FromSqlRaw returns item list, you can only use Database.ExecuteSqlCommand with database.\nYou will have to create a special DTO for your function result, it should include ALL data properties that are selected in your db function\n```\n`public class SpCityResult\n{\n        public int Id { get; set; }\n      \n        public string Name { get; set; }\n       \n         ....... and so on\n\n     \n}\n`\n```\nAll properties in this class should be the same name and type as it is in your function\nadd SpCityResult to db context\n```\n`public virtual DbSet SpCityResults { get; set; }\n......\n\nmodelBuilder.Entity().HasNoKey().ToView(null);\n\n`\n```\nrun code\n```\n`var cities= _context.SpCityResults.FromSqlRaw(\"SELECT public.show()\").ToList();\n\n`\n```\nif your existing City model has the same properties as db function, you don't need to create an extra DTO.\nyou can try this\n```\n`var cities= _context.Cities.FromSqlRaw(\"SELECT public.show()\").ToList();\n`\n```",
      "question_score": 3,
      "answer_score": 4,
      "created_at": "2021-08-24T17:21:05",
      "url": "https://stackoverflow.com/questions/68909959/how-to-get-results-from-a-postgresql-function-using-ef-core"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "plpgsql",
      "question_id": 67474133,
      "title": "ERROR cursor does not exist after first loop in PL/pgSQL",
      "problem": "I need to load a large number of csv files in to a PostgreSQL database. I have a table `source_files` which contains the file paths and a flag which indicates whether a file has already been loaded for all the csv files I need to load.\nI have written the following code which loads the first file correctly but then throws the error:\n\n`ERROR:  cursor \"curs\" does not exist`\n\nWhy am I getting this error and how can I fix it?\n```\n`DO $$\nDECLARE\n    file_record record;\n    curs CURSOR\n        FOR SELECT id, file_path\n            FROM source_files\n            WHERE added_to_db=FALSE\n            ORDER BY id;\nBEGIN\n    OPEN curs;\n    LOOP\n        -- Get next entry in source file which contains name of csv to load\n        FETCH curs INTO file_record;\n        exit WHEN NOT found;\n        BEGIN\n            -- As we need to add a column to the data after loading csv but before inserting\n            -- into final table we use a temporary table mytemp\n            DROP TABLE mytemp;\n\n            CREATE TABLE mytemp\n            (\n                dataA numeric,\n                dataB numeric\n            );\n\n            -- Load csv file   \n            EXECUTE FORMAT('COPY mytemp\n                            FROM ''%s''\n                            DELIMITER '',''\n                            CSV HEADER;', file_record.file_path);\n\n            -- Add Column specifying what source file the data is from\n            ALTER TABLE mytemp\n                ADD COLUMN source_id int;\n\n            UPDATE mytemp \n                SET source_id=file_record.id;\n\n            -- Add the data to the destination table\n            INSERT INTO data_table(\n                dataA,\n                dataB,\n                source_id\n            )\n            SELECT \n                mytemp.dataA,\n                mytemp.dataB\n                mytemp.source_id\n            FROM \n                mytemp\n\n            -- Set a flag to indicate that the current file in source_files has been loaded\n            UPDATE source_files\n                SET added_to_db=TRUE WHERE CURRENT OF curs;\n\n            COMMIT;\n        END;\n\n    END LOOP;\n    CLOSE curs;\n\nEND $$;\n`\n```",
      "solution": "There big problem with your code is the `COMMIT`. You can use `COMMIT` in a `DO` statement, but the cursor is closed as soon as the transaction ends.\nIn SQL you can create a cursor `WITH HOLD` that remains valid after the transaction has ended, but that is not available in PL/pgSQL.\nI suggest removing the `COMMIT`.\nAnother error in your code is your use of the `format` function, which exposes you to SQL injection. Instead of\n```\n`FORMAT('COPY mytemp\n        FROM ''%s''\n        DELIMITER '',''\n        CSV HEADER;', file_record.file_path);\n`\n```\nuse\n```\n`FORMAT('COPY mytemp\n        FROM %L\n        DELIMITER '',''\n        CSV HEADER;', file_record.file_path);\n`\n```\nYou can make the code much simpler by using an implicit cursor in the loop:\n```\n`FOR file_record IN\n   SELECT id, file_path\n   FROM source_files\n   WHERE added_to_db=FALSE\n   ORDER BY id\nLOOP\n   ...\nEND LOOP;\n`\n```\nThat saves you declaring the cursor and the `EXIT WHEN`. The `OPEN` and `CLOSE` statements are unnecessary anyway.",
      "question_score": 3,
      "answer_score": 3,
      "created_at": "2021-05-10T18:21:37",
      "url": "https://stackoverflow.com/questions/67474133/error-cursor-does-not-exist-after-first-loop-in-pl-pgsql"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "plpgsql",
      "question_id": 79711921,
      "title": "Using a script variable to generate a stored procedure in Postgresql",
      "problem": "I would like to use a script variable to set the data type, so that I can generate a few functions with different types, like so:\n`\\set bucket_data_type DATE`\nThen run a script to generate this function:\n`CREATE OR REPLACE FUNCTION insert(\n    ids BIGINT[],\n    types TEXT[],\n    buckets :bucket_data_type[]\n) RETURNS VOID\nLANGUAGE PLPGSQL\nAS $$\nBEGIN\n   EXECUTE \n    FORMAT('INSERT INTO myTable(id, type, buckets)\n    SELECT _.id, _.type, _.bucket\n    FROM(\n        SELECT unnest(%L::bigint[]) AS monitor_id,\n               unnest(%L::text[]) AS feature_type,\n               unnest(%L::'|| :bucket_data_type ||'[]) AS bucket\n        ) _\n        ON CONFLICT DO NOTHING;',\n           ids, types, buckets);\nEND\n$$;\n`\nand then again with `\\set bucket_data_type TIMESTAMP`, for example.\nIt works for the parameter definitions, but not the SQL inside the FORMAT block.\nI am expecting something like\n```\n`\\set bucket_data_type DATE\n\n`\n```\nto return\n`CREATE OR REPLACE FUNCTION insert(\n    ids BIGINT[],\n    types TEXT[],\n    buckets DATE[]\n) RETURNS VOID\nLANGUAGE PLPGSQL\nAS $$\nBEGIN\n   EXECUTE \n    FORMAT('INSERT INTO myTable(id, type, buckets)\n    SELECT _.id, _.type, _.bucket\n    FROM(\n        SELECT unnest(%L::bigint[]) AS monitor_id,\n               unnest(%L::text[]) AS feature_type,\n               unnest(%L::DATE[]) AS bucket\n        ) _\n        ON CONFLICT DO NOTHING;',\n           ids, types, buckets);\nEND\n$$;\n`\nbut I'm unable to get `unnest(%L::DATE[]) AS bucket` to appear.\nI either get format errors like:\n```\n`ERROR:  syntax error at or near \":\"\nLINE 15:                unnest(%L::'|| :bucket_data_type ||'[]) AS bu...\n`\n```\nor I simply get `:bucket_data_type` as a string inside the execute block.\nI've tried `SELECT set_config('tsp.bucket_data_type', :'bucket_data_type', false);` as well (as this works in DO BLOCKS where things are string literals).\nIs this even possible?",
      "solution": "Here is a solution that works with `psql`:\n```\n`\\set bucket_data_type date\n\nSELECT 'CREATE OR REPLACE FUNCTION insert(\n    ids BIGINT[],\n    types TEXT[],\n    buckets ' || :'bucket_data_type' || '[]\n) RETURNS VOID\nLANGUAGE PLPGSQL\nAS $$\nBEGIN\n   EXECUTE \n    FORMAT(''INSERT INTO myTable(id, type, buckets)\n    SELECT _.id, _.type, _.bucket\n    FROM(\n        SELECT unnest(%L::bigint[]) AS monitor_id,\n               unnest(%L::text[]) AS feature_type,\n               unnest(%L::'|| :'bucket_data_type' ||'[]) AS bucket\n        ) _\n        ON CONFLICT DO NOTHING;'',\n           ids, types, buckets);\nEND\n$$' \\gexec\n`\n```\nThe `SELECT` statement composes the statement that creates the function, and `\\gexec` executes the statement.",
      "question_score": 3,
      "answer_score": 2,
      "created_at": "2025-07-23T14:55:56",
      "url": "https://stackoverflow.com/questions/79711921/using-a-script-variable-to-generate-a-stored-procedure-in-postgresql"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "plpgsql",
      "question_id": 66777343,
      "title": "Adding ST_Transform in a PL/pgSQL function",
      "problem": "I use a function from Paul Ramsey\u00b4s blog to query geoJSON data from a postGIS database.\nI adjusted the function a little, which worked so far:\n```\n`CREATE OR REPLACE FUNCTION rowjsonb_to_geojson(\n  rowjsonb JSONB, \n  geom_column TEXT DEFAULT 'geom')\nRETURNS json AS \n$$\nDECLARE \n json_props jsonb;\n json_geom jsonb;\n json_type jsonb;\nBEGIN\n IF NOT rowjsonb ? geom_column THEN\n   RAISE EXCEPTION 'geometry column ''%'' is missing', geom_column;\n END IF;\n json_geom := ST_AsGeoJSON((rowjsonb ->> geom_column)::geometry)::jsonb;\n json_geom := jsonb_build_object('geometry', json_geom);\n json_props := jsonb_build_object('properties', rowjsonb - geom_column);\n json_type := jsonb_build_object('type', 'Feature');\n return (json_type || json_geom || json_props)::text;\nEND; \n$$ \nLANGUAGE 'plpgsql' IMMUTABLE STRICT;\n`\n```\nNow I\u00b4m on the point, where I want to integrate a ST_Transform(geom_column, 4326) to give me back lat/lng data for a leaflet application:\n\nI tried adjusting the line\n\n```\n`json_geom := ST_AsGeoJSON(((rowjsonb ->> ST_Transform(geom_column, 4326))::geometry)::jsonb;\n`\n```\nwhich doesn\u00b4t work, because ST_Transform needs to be performed on a geometry and not a text, or json;\n\nMy other idea, to declare a new variable geom_c and perform the transformation as first in the block\n\n```\n`geom_c := ST_Transform(geom_column, 4326)::geometry;\n`\n```\nwhich also doesn\u00b4t work either.\nI also tried the following:\n`json_geom := ST_AsGeoJSON(rowjsonb ->> ST_Transform((geom_column->>'geom')::geometry, 4326))::jsonb;` which gives back the error: operator does not exist: text ->> unknown\n`json_geom := ST_AsGeoJSON(rowjsonb ->> ST_Transform(ST_GeomFromGeoJSON(geom_column), 4326))::jsonb;` which gives the error unexpected character (at offset 0)\nHere are a two sample points from the standorts table, that I\u00b4m querying:\n```\n`\"id\": \"0\", \"geom\": \"0101000020787F0000000000001DDF2541000000800B285441\"\n\"id\": \"1\", \"geom\": \"0101000020787F000000000000EFE42541000000A074275441\"\n     \n`\n```\nThe query I use is:\n```\n`SELECT 'FeatureCollection' AS type, \n   'standorts' AS name, \n   json_build_object('type', 'name', 'properties', \n   json_build_object('name', 'urn:ogc:def:crs:OGC:1.3:CRS84')) AS CRS,\n   array_to_json(array_agg(rowjsonb_to_geojson(to_jsonb(standort.*)))) AS FEATURES FROM standort\";\n\n`\n```\nCan I even integrate the ST_Transform function into the block segment? Or do I need to rewrite the block logically?",
      "solution": "Welcome to SO. The parameter must be a geometry, so you need to cast the string in the parameter itself, not the result of function, e.g.\n```\n`json_geom := ST_AsGeoJSON(((rowjsonb ->> ST_Transform(geom_column::geometry, 4326)))::jsonb;\n`\n```\nExample:\n```\n`SELECT \n  ST_AsGeoJSON(\n    ST_Transform('SRID=32636;POINT(1 2)'::GEOMETRY,4326));\n\n                       st_asgeojson                        \n-----------------------------------------------------------\n {\"type\":\"Point\",\"coordinates\":[28.511265075,0.000018039]}\n`\n```\nThat being said, your function could be modified like that:\n```\n`CREATE OR REPLACE FUNCTION rowjsonb_to_geojson(\n  rowjsonb JSONB, \n  geom_column TEXT DEFAULT 'geom')\nRETURNS json AS \n$$\nDECLARE \n json_props jsonb;\n json_geom jsonb;\n json_type jsonb;\nBEGIN\n IF NOT rowjsonb ? geom_column THEN\n   RAISE EXCEPTION 'geometry column ''%'' is missing', geom_column;\n END IF;\n json_geom := ST_AsGeoJSON(ST_Transform((rowjsonb ->> geom_column)::geometry,4326))::jsonb;\n json_geom := jsonb_build_object('geometry', json_geom);\n json_props := jsonb_build_object('properties', rowjsonb - geom_column);\n json_type := jsonb_build_object('type', 'Feature');\n return (json_type || json_geom || json_props)::text;\nEND; \n$$ \nLANGUAGE 'plpgsql' IMMUTABLE STRICT;\n`\n```\nTest with your sample data\n```\n`WITH standort (id,geom) AS (\n  VALUES\n    (0,'0101000020787F0000000000001DDF2541000000800B285441'),\n    (1,'0101000020787F000000000000EFE42541000000A074275441')\n) \nSELECT row_to_json(q) AS my_collection FROM (\nSELECT 'FeatureCollection' AS type, \n   'standorts' AS name, \n   json_build_object('type', 'name', 'properties', \n   json_build_object('name', 'urn:ogc:def:crs:OGC:1.3:CRS84')) AS CRS,\n   array_to_json(array_agg(rowjsonb_to_geojson(to_jsonb(standort.*)))) AS features \nFROM standort) q;\n\n                      my_collection\n-----------------------------------------------\n\n{\n  \"type\": \"FeatureCollection\",\n  \"name\": \"standorts\",\n  \"crs\": {\n    \"type\": \"name\",\n    \"properties\": {\n      \"name\": \"urn:ogc:def:crs:OGC:1.3:CRS84\"\n    }\n  },\n  \"features\": [\n    {\n      \"type\": \"Feature\",\n      \"geometry\": {\n        \"type\": \"Point\",\n        \"coordinates\": [\n          11.886684554,\n          47.672030583\n        ]\n      },\n      \"properties\": {\n        \"id\": 0\n      }\n    },\n    {\n      \"type\": \"Feature\",\n      \"geometry\": {\n        \"type\": \"Point\",\n        \"coordinates\": [\n          11.896296029,\n          47.666357408\n        ]\n      },\n      \"properties\": {\n        \"id\": 1\n      }\n    }\n  ]\n}\n`\n```\nNote to the usage of ST_AsGeoJSON: `ST_Transforms` expects a geometry and `ST_AsGeoJSON` returns a text containing a representation of the geometry, not the geometry itself. So you first need to transform the geometry and then you can serialise it as GeoJSON.\nDemo: `db<>fiddle`",
      "question_score": 3,
      "answer_score": 2,
      "created_at": "2021-03-24T09:33:10",
      "url": "https://stackoverflow.com/questions/66777343/adding-st-transform-in-a-pl-pgsql-function"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "plpgsql",
      "question_id": 70346945,
      "title": "PL/pgSQL Loops don&#39;t update current timestamp / now()",
      "problem": "I want to see how long a loop iteration takes inside a DO block in postgres. The basic layout is as follows:\n```\n`DO $$\ndeclare v_time timestamptz;\ndeclare i record;\nbegin\nfor i in select generate_series(1, 5) t\nloop\n\nselect current_timestamp into v_time;\n\nperform pg_sleep(i.t);\n-- something done here (pg_sleep to ensure some time passes)\n\nraise notice '%', v_time - (select current_timestamp);\n-- expect negative interval from RAISE. \nend loop;\nend; $$;\n`\n```\nHowever, when I run this (have tried on Postgres 13 and 9), I get an interval of 0S returned:\n```\n`NOTICE:  00:00:00\nNOTICE:  00:00:00\nNOTICE:  00:00:00\nNOTICE:  00:00:00\nNOTICE:  00:00:00\nDO\n\nQuery returned successfully in 15 secs 389 msec.\n`\n```\nI have done this previously and have never run into this issue before, so I guess my question is \"what am I doing wrong this time?\" instead of \"why is postgres behaving unexpectedly?\"",
      "solution": "`current_timestamp` is defined to be:\n\nthe start time of the current transaction, their values do not change during the transaction\n\nyou probably want to use `clock_timestamp()` instead which returns a value that changes within a transaction, see the link above for a more complete description.",
      "question_score": 2,
      "answer_score": 9,
      "created_at": "2021-12-14T10:57:59",
      "url": "https://stackoverflow.com/questions/70346945/pl-pgsql-loops-dont-update-current-timestamp-now"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "plpgsql",
      "question_id": 77018847,
      "title": "IP increment function in PL/pgSQL",
      "problem": "I need a PL/pgSQL function that takes an IP address as text and adds a given integer value to it, carrying over the octets as needed.\nI have a working version of this written in JavaScript already:\n```\n`export const incrementIpAddress = (ipAddress: string, amount: number) => {\n    const octets = ipAddress.split('.');\n    const numbers = octets.map(octet => parseInt(octet));\n\n    let carry = amount;\n    for(let i = numbers.length - 1; i >= 0 && carry > 0; i--) {\n        const sum = numbers[i] + carry;\n        numbers[i] = sum % 256;\n        carry = Math.floor(sum / 256);\n    }\n\n    return numbers.join('.');\n}\n`\n```\nI tried having ChatGPT convert this into PL/pgSQL (I know, I'm sorry), and this is what it came up with:\n```\n`CREATE OR REPLACE FUNCTION increment_ip_address(ip_address text, amount integer) RETURNS text AS $$\nDECLARE\n    octets text[];\n    numbers integer[];\n    carry integer;\n    i integer;\nBEGIN\n    octets := string_to_array(ip_address, '.');\n    FOREACH i IN ARRAY octets LOOP\n        numbers := numbers || i::integer;\n    END LOOP;\n\n    carry := amount;\n    FOR i IN REVERSE 1 .. array_upper(numbers, 1) LOOP\n        numbers[i] := numbers[i] + carry;\n        carry := FLOOR(numbers[i] / 256);\n        numbers[i] := numbers[i] % 256;\n    END LOOP;\n\n    RETURN array_to_string(numbers, '.')::text;\nEND;\n$$ LANGUAGE plpgsql;\n`\n```\nHowever this doesn't work and always outputs the same IP it was given, not incremented. Can I get some help making this function work?",
      "solution": "PostgreSQL has a built-in `inet` type you can use for exactly that purpose. It works with a `+` operator as you'd expect, adding with a proper carry-over behaviour: demo\n`select '127.0.0.255'::inet+23;\n--127.0.1.22\n`",
      "question_score": 2,
      "answer_score": 6,
      "created_at": "2023-08-31T21:46:29",
      "url": "https://stackoverflow.com/questions/77018847/ip-increment-function-in-pl-pgsql"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "plpgsql",
      "question_id": 69660453,
      "title": "Translation of Postgres Query to be Snowflake compatible",
      "problem": "I'm trying to convert this pl/pgSQL function to work on Snowflake. Unfortunately, I just started using Snowflake and cannot manage to convert it properly. Snowflake plans to support pgSQL queries by the end of the year, unfortunately, it's not the case yet..\nHere is a quick introduction for you to understand what my tables store and what the function does.\nI have three tables. You can find DDL statements for all tables and sample data at the bottom of this question.\n\nEvents - this is the source table.\nDurations - this is the target table.\nProperties - this is a table used for reference that is updated whenever the function is called.\n\nOnce a day, I import data to the events table. The events we are interested in are events with device_types 1 and 2 (entry/exit). Then I run my function and it calculates the correct durations between events with the same card_nr. After that, I import those durations into my duration table and update the properties table.\nHere's an example of the events:\n\nAnd here's an example of the durations once the function was called:\n\nThe most important things I need to cover are:\n\nI should filter on the manufacturer to always be 'XX' since there are manufacturers like YY or ZZ\nI should only calculate the durations where events.event_time >= durationLimitDate from properties\nI need to make sure that the event_id_arrival and event_id_departure are not in the target table (duration) already to avoid duplicates!\nOnce I inserted the calculations to the duration table, I must update the durationLimitDate in the properties. Knowing that `durationLimitDate = (Max(event_time) - durationLimitDays))`\n\nFunction\n```\n`CREATE OR REPLACE FUNCTION calculateduration() RETURNS void AS $function$\n\nWITH cte AS (SELECT e.id, e.card_nr, e.event_time, e.ticket_type, e.manufacturer, e.carpark_id, e.device_type,\nROW_NUMBER() OVER (ORDER BY e.card_nr, e.carpark_id, e.event_time, e.device_type) AS rn\nFROM events e\nLEFT JOIN durations d ON d.event_id_arrival = e.id OR d.event_id_departure = e.id\nWHERE e.event_time >= (SELECT PROP_VALUE::timestamp FROM properties WHERE prop_key = 'DURATION.LIMIT.DATE')\nAND e.device_type IN (1, 2)\nAND event_type = 2\nAND e.manufacturer LIKE 'XX%'\nAND d.id IS NULL)\n\nINSERT INTO durations (id, odb_created_at, event_id_arrival, event_id_departure,\nevent_time_arrival, event_time_departure,\ncard_nr, ticket_type, duration, manufacturer, carpark_id)\n\nSELECT nextval('durations_id_seq'),\ncurrent_timestamp,\narrived_entry.id,\ndeparted_entry.id,\narrived_entry.event_time,\ndeparted_entry.event_time,\narrived_entry.card_nr,\narrived_entry.ticket_type,\ndate_part('epoch', departed_entry.event_time::timestamp - arrived_entry.event_time::timestamp),\narrived_entry.manufacturer,\narrived_entry.carpark_id\nFROM (SELECT * FROM cte WHERE cte.device_type = 1) AS arrived_entry\nINNER JOIN (SELECT * FROM cte WHERE cte.device_type = 2) AS departed_entry ON arrived_entry.card_nr = departed_entry.card_nr\nAND arrived_entry.carpark_id = departed_entry.carpark_id\nAND arrived_entry.rn + 1 = departed_entry.rn;\n\nUPDATE properties\nSET PROP_VALUE = (SELECT (MAX(event_time) - ((SELECT PROP_VALUE FROM properties WHERE prop_key = 'DURATION.LIMIT.DAYS') ||' day')::interval) FROM events WHERE event_time >= (SELECT PROP_VALUE::timestamp FROM properties WHERE prop_key = 'DURATION.LIMIT.DATE'))\nWHERE PROP_KEY ='DURATION.LIMIT.DATE';\n\n$function$\n\nLANGUAGE sql;\n`\n```\nDDL SCRIPTS\n```\n`-- events\n        CREATE TABLE IF NOT EXISTS events (\n        id bigint NOT NULL autoincrement start 1 increment 1 PRIMARY KEY,\n        odb_created_at timestamp without time zone NOT NULL,\n        event_time timestamp without time zone NOT NULL,\n        device_type integer NOT NULL,\n        event_type integer NOT NULL,\n        ticket_type integer NOT NULL,\n        card_nr character varying(100),\n        count integer DEFAULT 1 NOT NULL,\n        manufacturer character varying(200),\n        carpark_id bigint\n    ); \n\n     -- durations\n    CREATE TABLE IF NOT EXISTS durations (\n        id bigint NOT NULL autoincrement start 1 increment 1 PRIMARY KEY,\n        odb_created_at timestamp without time zone NOT NULL,\n        event_id_arrival bigint,\n        event_id_departure bigint,\n        event_time_arrival timestamp without time zone,\n        event_time_departure timestamp without time zone,\n        card_nr character varying(100),\n        ticket_type integer,\n        duration integer,\n        manufacturer character varying(200),\n        carpark_id bigint\n    );\n\n    --properties\n    create or replace TABLE PROPERTIES (\n        PROP_KEY VARCHAR(80) NOT NULL,\n        PROP_VALUE VARCHAR(250),\n        primary key (PROP_KEY)\n    );\n`\n```\nSample data:\n```\n`INSERT INTO properties (prop_key,prop_value) VALUES\n     ('DURATION.LIMIT.DAYS','30'),\n     ('DURATION.LIMIT.DATE','2021-08-01 00:00:00.00');\n\n    INSERT INTO public.events (id, odb_created_at, event_time, device_type, event_type, ticket_type, card_nr, count, manufacturer, carpark_id) VALUES(188160996, '2021-10-02 04:28:26.338', '2021-10-01 09:14:41.32', 1, 2, 11, '03998988030897300007782', 1, 'XX', 1852);\nINSERT INTO public.events (id, odb_created_at, event_time, device_type, event_type, ticket_type, card_nr, count, manufacturer, carpark_id) VALUES(188160790, '2021-10-02 04:28:26.248', '2021-10-01 09:31:10.94', 2, 2, 11, '03998988030897300007782', 1, 'XX', 1852);\nINSERT INTO public.events (id, odb_created_at, event_time, device_type, event_type, ticket_type, card_nr, count, manufacturer, carpark_id) VALUES(188146489, '2021-10-02 04:26:55.069', '2021-10-01 10:03:01.57', 1, 2, 500, '01479804030429500089598', 1, 'XX', 1563);\nINSERT INTO public.events (id, odb_created_at, event_time, device_type, event_type, ticket_type, card_nr, count, manufacturer, carpark_id) VALUES(188146069, '2021-10-02 04:26:54.852', '2021-10-01 11:49:58.45', 2, 2, 500, '01479804030429500089598', 1, 'XX', 1563);\nINSERT INTO public.events (id, odb_created_at, event_time, device_type, event_type, ticket_type, card_nr, count, manufacturer, carpark_id) VALUES(188161161, '2021-10-02 04:28:26.372', '2021-10-01 18:44:33.62', 1, 2, 11, '03998988030897300007782', 1, 'XX', 1852);\nINSERT INTO public.events (id, odb_created_at, event_time, device_type, event_type, ticket_type, card_nr, count, manufacturer, carpark_id) VALUES(188160950, '2021-10-02 04:28:26.329', '2021-10-01 18:45:51.903', 2, 2, 11, '03998988030897300007782', 1, 'XX', 1852);\nINSERT INTO public.events (id, odb_created_at, event_time, device_type, event_type, ticket_type, card_nr, count, manufacturer, carpark_id) VALUES(188161227, '2021-10-02 04:28:26.374', '2021-10-01 23:21:18.58', 1, 2, 11, '04139733030897300003136', 1, 'XX', 1852);\nINSERT INTO public.events (id, odb_created_at, event_time, device_type, event_type, ticket_type, card_nr, count, manufacturer, carpark_id) VALUES(188160974, '2021-10-02 04:28:26.334', '2021-10-01 23:24:03.29', 2, 2, 11, '04139733030897300003136', 1, 'XX', 1852);\nINSERT INTO public.events (id, odb_created_at, event_time, device_type, event_type, ticket_type, card_nr, count, manufacturer, carpark_id) VALUES(188239864, '2021-10-03 04:24:43.345', '2021-10-02 06:49:55.97', 1, 2, 11, '01719400030897300061410', 1, 'XX', 1852);\nINSERT INTO public.events (id, odb_created_at, event_time, device_type, event_type, ticket_type, card_nr, count, manufacturer, carpark_id) VALUES(188239649, '2021-10-03 04:24:43.308', '2021-10-02 07:02:08.72', 2, 2, 11, '01719400030897300061410', 1, 'XX', 1852);\n`\n```\nThank you!\nUNIQUE TEST\n```\n`      CREATE TABLE IF NOT EXISTS test (\n        id bigint NOT NULL AUTOINCREMENT PRIMARY KEY -- Check the syntax!\n        , odb_created_at timestamp without time zone NOT NULL\n        , event_time timestamp without time zone NOT NULL\n        , device_type integer NOT NULL\n        , event_type integer NOT NULL\n        , ticket_type integer NOT NULL\n        , card_nr character varying(100)\n        , count integer DEFAULT 1 NOT NULL\n        , manufacturer character varying(200)\n        , carpark_id bigint\n        , UNIQUE (card_nr, event_time) -- NATURAL KEY\n    );\n   \n   \n   INSERT INTO EUILOGS_DEV.ILOGS.TEST (ODB_CREATED_AT, EVENT_TIME, DEVICE_TYPE, EVENT_TYPE, TICKET_TYPE, CARD_NR, COUNT, MANUFACTURER, CARPARK_ID) \n   VALUES(current_timestamp(), '2021-01-01 15:00:00.000', 1, 1, 1, 'CARD1', 1, 'MAN1', 1);\n  INSERT INTO EUILOGS_DEV.ILOGS.TEST (ODB_CREATED_AT, EVENT_TIME, DEVICE_TYPE, EVENT_TYPE, TICKET_TYPE, CARD_NR, COUNT, MANUFACTURER, CARPARK_ID) \n   VALUES(current_timestamp(), '2021-01-01 15:00:00.000', 1, 1, 1, 'CARD1', 1, 'MAN1', 1);\n`\n```",
      "solution": "You need to use sequence to generate auto numbers:\nhttps://docs.snowflake.com/en/user-guide/querying-sequences.html\nSo create a sequence first to use it:\n```\n`create or replace sequence seq1;\n`\n```\nThen use the below function:\n```\n`create or replace procedure calculateduration() \nRETURNS string\nLANGUAGE JAVASCRIPT \nAS $$\nvar query1 = \n`\n  INSERT INTO durations (id, odb_created_at, event_id_arrival, event_id_departure,\n  event_time_arrival, event_time_departure,\n  card_nr, ticket_type, duration, manufacturer, carpark_id)\n\n  WITH cte AS (\n      SELECT e.id, e.card_nr, e.event_time, e.ticket_type, e.manufacturer, e.carpark_id, e.device_type,\n      ROW_NUMBER() OVER (ORDER BY e.card_nr, e.carpark_id, e.event_time, e.device_type) AS rn\n      FROM events e\n      LEFT JOIN durations d ON d.event_id_arrival = e.id OR d.event_id_departure = e.id\n      WHERE e.event_time >= (SELECT PROP_VALUE::timestamp FROM properties WHERE prop_key = 'DURATION.LIMIT.DATE')\n      AND e.device_type IN (1, 2)\n      AND event_type = 2\n      AND e.manufacturer LIKE 'XX%'\n      AND d.id IS NULL\n  )\n  SELECT \n    seq1.nextval,\n    current_timestamp(),\n    arrived_entry.id,\n    departed_entry.id,\n    arrived_entry.event_time,\n    departed_entry.event_time,\n    arrived_entry.card_nr,\n    arrived_entry.ticket_type,\n    timestampdiff(second, arrived_entry.event_time, departed_entry.event_time),\n    arrived_entry.manufacturer,\n    arrived_entry.carpark_id\n  FROM (SELECT * FROM cte WHERE cte.device_type = 1) AS arrived_entry\n  INNER JOIN (SELECT * FROM cte WHERE cte.device_type = 2) AS departed_entry \n    ON arrived_entry.card_nr = departed_entry.card_nr\n  AND arrived_entry.carpark_id = departed_entry.carpark_id\n  AND arrived_entry.rn + 1 = departed_entry.rn\n`;\n\nsnowflake.execute({ sqlText: query1 });\n\nvar query2 = \"SELECT PROP_VALUE FROM properties WHERE prop_key = 'DURATION.LIMIT.DAYS'\";\nvar stmt = snowflake.createStatement({ sqlText: query2 });\nvar resultSet = stmt.execute(); \nresultSet.next();\nvar prop_value = resultSet.getColumnValue(1); \n\nvar query3 =\n`\n  UPDATE properties\n  SET PROP_VALUE = (\n    SELECT dateadd(day, -1 * ${prop_value}, MAX(event_time)) FROM events \n    WHERE event_time >= (\n      SELECT PROP_VALUE::timestamp FROM properties WHERE prop_key = 'DURATION.LIMIT.DATE'\n    )\n  )\n  WHERE PROP_KEY ='DURATION.LIMIT.DATE';\n`\n\nstmt = snowflake.createStatement({ sqlText: query3 });\nstmt.execute();\n\nreturn 'true';\n$$;\n`\n```\nThen call the procedure:\n```\n`call calculateduration();\n`\n```\nThe code is simple without much validation and checking, but it should do the job you need.",
      "question_score": 2,
      "answer_score": 3,
      "created_at": "2021-10-21T12:41:58",
      "url": "https://stackoverflow.com/questions/69660453/translation-of-postgres-query-to-be-snowflake-compatible"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "plpgsql",
      "question_id": 66795694,
      "title": "How to call DROP USER from pl/pgsql?",
      "problem": "How to call `DROP USER` from Pl/PgSql (see example) ?\n```\n`CREATE PROCEDURE myfunc()\nLANGUAGE PLPGSQL AS\n$$\nDECLARE\n  super_users TEXT[];\n  ldap_users TEXT[];\n  u TEXT;\nBEGIN\n  super_users := ARRAY(SELECT usename::TEXT FROM pg_catalog.pg_user WHERE usesuper);\n  ldap_users := ARRAY(SELECT uid::TEXT FROM ldap_users);\n\n  FOREACH u IN ARRAY ldap_users LOOP\n    IF (u <> 'postgres' AND u <> ALL(super_users)) THEN\n      DROP USER IF EXISTS u;\n    END IF;\n  END LOOP;\nEND;\n$$;\n`\n```\nIt leads to error that \"role u does not exist\"...\nIMHO PL/PGSQL does not treat `u` as a variable, but as a name. And `DROP USER...` is not SQL but some extension. How to do it? Maybe some system function? Or special syntax to substitute `u`?\nEDIT:\nMy solution (just found):\n```\n`DECLARE\n  stm TEXT;\n  ...\nBEGIN\n  ...\n  stm := 'DROP USER IF EXISTS \"' || u '\"';\n  EXECUTE stm;\n...\n`\n```\nIt seems to work. Maybe there is other solutions? More canonical?",
      "solution": "You need dynamic SQL for this:\n```\n`execute format('DROP USER IF EXISTS %I', u);\n`\n```",
      "question_score": 2,
      "answer_score": 3,
      "created_at": "2021-03-25T09:30:40",
      "url": "https://stackoverflow.com/questions/66795694/how-to-call-drop-user-from-pl-pgsql"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "plpgsql",
      "question_id": 65586539,
      "title": "Best practice to identify a jsonb null in plpgsql",
      "problem": "I'm aware that variants of this question have been asked before:\n\nWhy can't NULL be converted to JSON's null in postgreSQL?\nWhy does JSON null not cast to SQL null in postgres?\nperhaps others...\n\nWhat I wasn't able to glean from the above links is whether there is a best practice.\nConsider the following code:\n```\n`DO\n$$\nDECLARE\n  _main_jsonb jsonb = '{\"i_am_null\": null, \"a_string\": \"null\"}';\n  _sub_jsonb jsonb;\nBEGIN\n  SELECT (_main_jsonb->'i_am_null') INTO _sub_jsonb;\n  IF _sub_jsonb IS NULL THEN\n    RAISE INFO 'This point *not* reached. Bad?';\n  END IF;\n\n  -- THIS IS THE PART I AM REALLY INTERESTED IN\n  SELECT (_main_jsonb->>'i_am_null')::jsonb INTO _sub_jsonb;\n  IF _sub_jsonb IS NULL THEN\n    RAISE INFO 'This point *is* reached. Good.';\n  END IF;\n  -- THIS IS THE PART I AM REALLY INTERESTED IN\n\n  SELECT (_main_jsonb->>'a_string')::jsonb INTO _sub_jsonb;\n  IF _sub_jsonb IS NULL THEN\n    RAISE INFO 'Bonus points. This point *not* reached. Good.';\n  END IF;\nEND;\n$$\n`\n```\nIs there a better way to figure out if i_am_null is null?\nEdit: FYI to those interested in this question, you might be interested in this follow-up question...",
      "solution": "Both of your linked answers contain solutions, but it might be good to have an omnibus answer.\nPostgres is strongly typed. Its functions and operators return specific types.\n`->` returns jsonb. Compare it not to SQL null but jsonb null.\n```\n`test=# select '{\"i_am_null\": null, \"a_string\": \"null\"}'::jsonb->'i_am_null' = 'null'::jsonb;\n ?column? \n----------\n t\n(1 row)\n\ntest=# select '{\"i_am_null\": null, \"a_string\": \"null\"}'::jsonb->'a_string' = 'null'::jsonb;\n ?column? \n----------\n f\n(1 row)\n`\n```\n\n`->>` returns text and will convert jsonb null into SQL null.\n```\n`test=# select '{\"i_am_null\": null, \"a_string\": \"null\"}'::jsonb->>'i_am_null' is null;\n ?column? \n----------\n t\n(1 row)\n\ntest=# select '{\"i_am_null\": null, \"a_string\": \"null\"}'::jsonb->>'a_string' is null;\n ?column? \n----------\n f\n(1 row)\n`\n```\n\nNote that while jsonb null is just another value, SQL null is very special. Null is not a value, it is the lack of a value. Null equals nothing, not even null. It might seem like casting null to jsonb should produce jsonb null, but the SQL standard requires that null only casts to null otherwise that would mean null is equivalent to something.\nThis is why jsonb null can be converted to null, but null is not cast to jsonb null. `null::jsonb` is null. This is inconvenient, but required by the SQL standard. It is one of the reasons casting back and forth between jsonb and text is not recommended.",
      "question_score": 2,
      "answer_score": 3,
      "created_at": "2021-01-05T22:04:23",
      "url": "https://stackoverflow.com/questions/65586539/best-practice-to-identify-a-jsonb-null-in-plpgsql"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "plpgsql",
      "question_id": 79677351,
      "title": "Catch pg_cancel_backend in plpgsql",
      "problem": "I understand that I can't error-handle `pg_terminate_backend` in PL/pgSQL. But I am surprised that I can't error-handle `pg_cancel_backend`.\nBelow is one attempt that does not work, but I also tried similar ways in my code and had no success to catch it. Is there really no way to catch it in PL/pgSQL?\n```\n`CREATE OR REPLACE FUNCTION deleteme_try_sleep() RETURNS void AS $$\nBEGIN\n  PERFORM pg_sleep(600);\nEND;\n$$ LANGUAGE plpgsql;\n\nDO $$\nBEGIN\n  RAISE NOTICE 'Calling sleeper...';\n  BEGIN\n    PERFORM deleteme_try_sleep();\n  EXCEPTION\n    WHEN OTHERS THEN\n      RAISE NOTICE 'Inner call failed: %', SQLERRM;\n  END;\n  RAISE NOTICE 'Block continued';\nEND;\n$$;\n\n-- use pg_cancel_backend on the above\n`\n```",
      "solution": "As the documentation says:\n\nThe special condition name `OTHERS` matches every error type except `QUERY_CANCELED` and `ASSERT_FAILURE`. (It is possible, but often unwise, to trap those two error types by name.)\n\nSo you would have to use\n```\n`BEGIN\n   ...\nEXCEPTION\n   WHEN OTHERS OR QUERY_CANCELED THEN\n      ...\nEND;\n`\n```",
      "question_score": 2,
      "answer_score": 4,
      "created_at": "2025-06-24T11:36:46",
      "url": "https://stackoverflow.com/questions/79677351/catch-pg-cancel-backend-in-plpgsql"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "plpgsql",
      "question_id": 74906963,
      "title": "Performance of function call degrades after calling 5 calls in a row",
      "problem": "I have this plpgsql function (running PG 14.5 on UBUNTU 18.04 with 8GB Ram) which returns 200 rows just fine:\n`db=# explain (analyze,buffers) Select * from f_client_getlistasshown('{\"limit\":\"200\",\"startdate\":\"2014-01-01\",\"enddate\":\"2100-01-01\",\"showRequiresActionFromTaxadvisor\":false}');\n\n                    \n--------------------------------------------------------------------------------------------------------------------------------\n Function Scan on f_client_getlistasshown  (cost=0.25..10.25 rows=1000 width=400) (actual time=69.515..69.529 rows=200 loops=1)\n   Buffers: shared hit=8939 dirtied=1\n Planning Time: 0.066 ms\n Execution Time: 70.282 ms\n(4 rows)\n`\nNow I repeat this query 5 times, each time the query returns the result fast. But then on the 6th attempt:\n`Function Scan on f_client_getlistasshown  (cost=0.25..10.25 rows=1000 width=400) (actual time=8790.305..8790.319 rows=200 loops=1)\n   Buffers: shared hit=2147651\n Planning Time: 0.034 ms\n Execution Time: 8790.351 ms\n`\nAs you can see I suddenly ran out of buffers and the execution time is terrible.\n`shared_buffers` is set to 2GB. I don't see the problem if I just execute the query the function calls internally so I didn't bother showing it here. What could be causing this?",
      "solution": "It is known issue. The first 5 executions of embedded SQL in PL/pgSQL use custom plans optimized for current arguments. After this some heuristic chooses if custom plans will be generated or one generic plan will be used. In your case, the generic plan of some query in your function doesn't work well.\nThe simplest solution is just force custom plans for your function like:\n```\n`CREATE OR REPLACE FUNCTION f_client_getlistasshown(...)\nRETURNS ...\nAS $$\n  ...\nLANGUAGE plpgsql\nSET plan_cache_mode TO force_custom_plan;\n`\n```",
      "question_score": 2,
      "answer_score": 4,
      "created_at": "2022-12-24T11:21:26",
      "url": "https://stackoverflow.com/questions/74906963/performance-of-function-call-degrades-after-calling-5-calls-in-a-row"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "plpgsql",
      "question_id": 73382104,
      "title": "Is it good idea to reuse auto-generated ids? [PostgreSQL]",
      "problem": "I am evaluating the possibility of reusing ids that have been removed from a column with auto-generated ids.\nMany records will be deleted per day and we do not want the identifier to exceed 9999.\n\nWould I have concurrency problems when inserting into the table? (around 100 records would be inserted per day)\nIs there a correct way to reuse ids?\nShould I use another approach to assign this case of ids?\nShould I leave development forever?\n\nI'm just evaluating this possibility.\nThank you for your time and experience.\ncreate a table\n```\n`CREATE TABLE IF NOT EXISTS test(\nid BIGINT GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,\ndata TEXT);\n`\n```\ncreate function\n```\n`CREATE OR REPLACE function get_id_test()\nreturns bigint\nlanguage plpgsql\nas\n$$\nDECLARE\nmin_id bigint;\nprevious bigint;\nnext_id bigint;\nf record;\nBEGIN\n   SELECT MIN(id) into min_id FROM test;\n   IF  min_id > 1 THEN\n       RETURN 1;\n   END IF;\n\n   FOR f\n   IN select t.id from test t order by t.id\n   LOOP\n       IF  f.id = min_id THEN\n           previous = f.id;\n           CONTINUE;\n       END IF;\n       IF  f.id > (previous + 1) THEN\n           RETURN previous + 1;\n       ELSE \n           previous = f.id;\n       END IF;\n   END LOOP;\n   next_id = previous + 1;\n   RETURN nextval(pg_get_serial_sequence('test', 'id'));\nend;\n$$;\n`\n```\ninsert 5 rows\n```\n`INSERT INTO test (id,data) VALUES (get_id_test(),'test'),\n                                  (get_id_test(),'test'),\n                                  (get_id_test(),'test'),\n                                  (get_id_test(),'test'),\n                                  (get_id_test(),'test');\n`\n```\n\ndelete 2 rows (2 and 4)\n```\n`DELETE FROM test WHERE id=2 OR id=4;\n`\n```\n\ninsert row (expecting id=2)\n```\n`INSERT INTO test (id,data) VALUES (get_id_test(),'test expecting id 2');\n`\n```\n\ninsert row (expecting id=4)\n```\n`INSERT INTO test (id,data) VALUES (get_id_test(),'test expecting id 4');\n`\n```\n\ninsert row (expecting normal next id from nextval())\n```\n`INSERT INTO test (id,data) VALUES (get_id_test(),'test expecting id 6');\n`\n```",
      "solution": "tl;dr: Don't do it.\n\nYou certainly can do it, and transactions will save you.\nBut you're violating assumptions that humans make,\nand increasing your costs of maintenance and\nof onboarding new staff to the project.\nWhen I see an autoinc PK, in postgres and other databases,\nI assume\n\nmonotonic increasing\ntherefore \"recent\" rows have \"large\" IDs, which makes a difference for physical layout on-disk and for query optimization\nIDs may have been sent out into the world, e.g. appearing in syslog messages, and they mean something\n\nYour hole-filling approach is very very slow.\nAt least use ORDER BY ID DESC,\nsince presumably there's few holes at\nthe bottom and most opportunities are\nfrom recent deletions near the top.\nConsider comparing RANK() to ID\nso the backend finds a hole without\nyou having to loop looking for it.\nMost allocators will amortize the cost\nof a scan by exploiting a cache of scan results.\nThere is a rich literature on this topic --\nstart with malloc slab allocators.\nFile systems need to fill holes, also,\nwhich Seltzer's \"A Comparison of FFS Disk Allocation Policies\" briefly touches on.\nConsider maintaining a table of at most\nten thousand rows which maps integer\nto either NULL or timestamp last released.\nA timestamp index would help you quickly\nfind the next unallocated ID.\nUse old ones or recent ones first,\nwhichever policy you prefer.\n\nYou're deliberately limiting the magnitude\nof the `bigint` ID, so `int` would probably suffice,\nsaving a little storage.\n\nConsider leaving the PK alone,\nand define a new integer attribute column\nwhere you perform hole-filling to your heart's content.",
      "question_score": 2,
      "answer_score": 4,
      "created_at": "2022-08-17T03:57:15",
      "url": "https://stackoverflow.com/questions/73382104/is-it-good-idea-to-reuse-auto-generated-ids-postgresql"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "plpgsql",
      "question_id": 72173443,
      "title": "Cannot rollback while a subtransaction is active - Error 2D000",
      "problem": "I have written a stored procedure that basically loops over an array of `fields` and performs some manipulation in the db for each iteration. What I want to achieve is, either all the iterations of loops should occur or neither one of them should occur.\nSo let's say there were 5 elements in the fields array and the loop iterates up to the 3rd element before noticing that some condition is true and throwing an error, I want to rollback all the changes that occurred during the first 2 iterations. I've used `ROLLBACK` statements to achieve the same, but every time it reaches the `ROLLBACK` statement it throws the following error:\n\nCannot rollback while a subtransaction is active : 2D000\n\nSurprisingly, it works as normal if I comment out the `outobj := json_build_object('code',0);` statement within the `EXCEPTION WHEN OTHERS THEN` block or if I remove that whole block completely.\nI've checked the PostgreSQL documentation for error codes, but it didn't really help. My stored procedure is as follows:\n`\nCREATE OR REPLACE PROCEDURE public.usp_add_fields(\n    field_data json,\n    INOUT outobj json DEFAULT NULL::json)\nLANGUAGE 'plpgsql'\nAS $BODY$\nDECLARE \nv_user_id bigint;\nfarm_and_bussiness json;\n_field_obj json;\n_are_wells_inserted boolean;\nBEGIN\n\n-- get user id\n v_user_id = ___uf_get_user_id(json_extract_path_text(field_data,'user_email'));\n\nIF(v_user_id IS NULL) THEN\n    outobj := json_build_object('code',17);\n    RETURN;\nEND IF;\n\n-- Loop over entities to create farms & businesses\nFOR _field_obj IN SELECT * FROM json_array_elements(json_extract_path(field_data,'fields'))\nLOOP\n    -- check if irrigation unit id is already linked to some other field\n    IF(SELECT EXISTS(\n        SELECT field_id FROM user_fields WHERE irrig_unit_id LIKE json_extract_path_text(_field_obj,'irrig_unit_id') AND deleted=FALSE\n    )) THEN\n        outobj := json_build_object('code',26);\n        -- Rollback any changes made by previous iterations of loop\n        ROLLBACK;\n        RETURN;\n    END IF;\n    \n    -- check if this field name already exists\n    IF( SELECT EXISTS(\n            SELECT uf.field_id FROM user_fields uf\n            INNER JOIN user_farms ufa ON (ufa.farm_id=uf.user_farm_id AND ufa.deleted=FALSE)\n            INNER JOIN user_businesses ub ON (ub.business_id=ufa.user_business_id AND ub.deleted=FALSE)\n            INNER JOIN users u ON (ub.user_id = u.user_id AND u.deleted=FALSE)\n            WHERE u.user_id = v_user_id\n            AND uf.field_name LIKE json_extract_path_text(_field_obj,'field_name')\n            AND uf.deleted=FALSE\n        )) THEN \n        outobj := json_build_object('code', 22);\n        -- Rollback any changes made by previous iterations of loop\n        ROLLBACK;\n        RETURN;\n    END IF;\n\n    --create/update user business and farm and return farm_id \n    CALL usp_add_user_bussiness_and_farm(\n        json_build_object('user_email', json_extract_path_text(field_data,'user_email'),\n                          'business_name', json_extract_path_text(_field_obj,'business_name'),\n                          'farm_name', json_extract_path_text(_field_obj,'farm_name')\n        ), farm_and_bussiness);\n\n    IF(json_extract_path_text(farm_and_bussiness, 'code')::int != 1) THEN\n        outobj := farm_and_bussiness;\n        -- Rollback any changes made by previous iterations of loop\n        ROLLBACK;\n        RETURN;\n    END IF;\n\n    -- insert into users fields\n    INSERT INTO user_fields (user_farm_id, irrig_unit_id, field_name, ground_water_percent, surface_water_percent)\n    SELECT json_extract_path_text(farm_and_bussiness,'farm_id')::bigint,\n    json_extract_path_text(_field_obj,'irrig_unit_id'),\n    json_extract_path_text(_field_obj,'field_name'),\n    json_extract_path_text(_field_obj,'groundWaterPercentage'):: int,\n    json_extract_path_text(_field_obj,'surfaceWaterPercentage'):: int;\n\n    -- add to user wells\n    CALL usp_insert_user_wells(json_extract_path(_field_obj,'well_data'), v_user_id, _are_wells_inserted);\nEND LOOP;\n\noutobj := json_build_object('code',1);\nRETURN;\n\nEXCEPTION WHEN OTHERS THEN \n    raise notice '% : %', SQLERRM, SQLSTATE;\n    outobj := json_build_object('code',0);\nRETURN;\n\nEND;\n$BODY$;\n`",
      "solution": "Explanation:\nBased on the clue provided by @Laurez Albe, I came up with a cleaner way to solve the above problem.\nBasically, what I've done is, I've raised a `custom exception` whenever a condition is `true`. So when an exception is thrown, all the changes made by `block X` are rolled back gracefully. I can even perform last minute cleanup within the exception conditional blocks.\nImplementation:\n`CREATE OR REPLACE procedure mProcedure(INOUT resp json DEFAULT NULL::JSON)\nLANGUAGE 'plpgsql'\nAS $BODY$\nDECLARE\nfield_data json := '{ \"fields\": [1,2,3,4,5] }';\n_field_id int;\nBEGIN\n\n-- Start of block X\nFOR _field_id IN SELECT * FROM json_array_elements(json_extract_path(field_data,'fields'))\nLOOP\n    INSERT INTO demo VALUES(_field_id);\n    \n    IF(_field_id = 3) THEN\n      RAISE EXCEPTION USING ERRCODE='22013';\n    END IF;\n   \n    IF(_field_id = 5) THEN\n      RAISE EXCEPTION USING ERRCODE='22014';\n    END IF;\nEND LOOP;\n\nSELECT json_agg(row_to_json(d)) INTO resp FROM demo d;\nRETURN;\n-- end of block X\n\n-- if an exception occurs in block X, then all the changes made within the block are rollback\n-- and the control is passed on to the EXCEPTION WHEN OTHERS block.  \nEXCEPTION \nWHEN sqlstate '22013' THEN\nresp := json_build_object('code',26);\n\nWHEN sqlstate '22014' THEN\nresp := json_build_object('code',22);\n\nEND;\n$BODY$;\n`\nDemo:\nDbfiddle",
      "question_score": 2,
      "answer_score": 1,
      "created_at": "2022-05-09T16:14:29",
      "url": "https://stackoverflow.com/questions/72173443/cannot-rollback-while-a-subtransaction-is-active-error-2d000"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "plpgsql",
      "question_id": 71242103,
      "title": "why does postgres set_config&#39;s is_local = true not persist the variable for the whole transaction?",
      "problem": "I have a function that uses `set_config` with `is_local = true` to set a variable.\nNow I expected that a select statement using the variable with `current_settings` within the same transaction would be able to access the variable, because the docs state:\n\nset_config(setting_name, new_value, is_local) ... set parameter and\nreturn new value\nset_config sets the parameter setting_name to new_value. If is_local\nis true, the new value will only apply to the current transaction.\n\nBut that doesn't work in my case and I get\n```\n`ERROR:  unrecognized configuration parameter \"auth.tenant_id\"\nSQL state: 42704\n`\n```\nAny ideas where I am wrong here?\nThis is the function:\n```\n`CREATE OR REPLACE FUNCTION auth.authorize(IN a_user_id uuid)\n    RETURNS boolean\n    LANGUAGE 'plpgsql'\n    SECURITY DEFINER \nAS $BODY$\ndeclare\nv_tenant_id uuid;\nv_user_role auth.user_role;\nbegin\nselect tenant_id, user_role into strict v_tenant_id, v_user_role from auth.authorizations where user_id = a_user_id;\nperform set_config('auth.tenant_id', v_tenant_id::text, true);\nperform set_config('auth.user_role', v_user_role::text, true);\nreturn true;\nexception when no_data_found then return false;\nend;\n$BODY$;\n`\n```\nAnd this is the transaction in which the second select statement fails\n```\n`begin;\nselect * from auth.authorize(uuid('180e1b14-21e5-4e66-a9b8-db09139d6278'));\nselect current_setting('auth.tenant_id') as tenant_id, current_setting('auth.user_role') as user_role;\ncommit;\n`\n```",
      "solution": "The effect of `set_config(is_local => true)` seems to be restrained to the implicit subtransaction created by your `BEGIN \u2026 EXCEPTION` block.\n(Skip to the end of my answer for my TL;DR solution to your problem.)\nHere's what happens when I do something similar, but with the call to `set_config()` outside of the `BEGIN \u2026 EXCEPTION` subtransaction:\n`create or replace function set_config_outside_of_begin_except_block(denominator int)\n    returns bool\n    language 'plpgsql'\nas $body$\nbegin\n    perform set_config('my.setting', 'set before subtransaction', true);\n\n    begin\n        perform 10 / denominator;\n        return true;\n    exception when division_by_zero then\n        return false;\n    end;\nend;\n$body$;\n\nbegin\nselect current_setting('my.setting') as my_setting;\nselect set_config_outside_of_begin_except_block(1);\nrollback;\n`\n```\n`CREATE FUNCTION\nBEGIN\nERROR:  unrecognized configuration parameter \"my.setting\"\n set_config_outside_of_begin_except_block\n------------------------------------------\n t\n(1 row)\n\n        my_setting\n---------------------------\n set before subtransaction\n(1 row)\n\nROLLBACK\n`\n```\nHaving moved the `set_config()` call to before the `BEGIN \u2026 EXCEPTION` block, the new setting for `my.setting` has persisted outside our function call.\nAnd, as you will next see, it doesn't even matter if an exception occurs in the above `BEGIN \u2026 EXCEPTION` block, which makes sense, given that the `set_config()` is called before that block/subtransaction is entered:\n`begin\nselect current_setting('my.setting') as my_setting;\nrollback;\n`\n```\n`BEGIN\n set_config_outside_of_begin_except_block\n------------------------------------------\n f\n(1 row)\n        my_setting\n\n---------------------------\n set before subtransaction\n(1 row)\n\nROLLBACK\n`\n```\nNow, I will modify the code a bit to come closer to your example, and closer to reproducing the behavior you describe:\n`create or replace function set_config_in_begin_except_block(denominator int)\n    returns bool\n    language 'plpgsql'\nas $body$\nbegin\n    perform set_config('my.setting', 'set in subtransaction', true);\n    perform 10 / denominator;\n    return true;\nexception when division_by_zero then\n    return false;\nend;\n$body$;\n\nbegin;\nselect set_config_in_begin_except_block(0);\nselect current_setting('my.setting') as my_setting;\nrollback;\n`\n```\n`CREATE FUNCTION\nBEGIN\n set_config_in_begin_except_block\n----------------------------------\n f\n(1 row)\n\n my_setting\n------------\n\n(1 row)\n\nROLLBACK\n`\n```\n`begin\nselect set_config_in_begin_except_block(1);\nselect current_setting('my.setting') as my_setting;\nrollback;\n`\n```\n`BEGIN\n set_config_in_begin_except_block\n----------------------------------\n t\n(1 row)\n\n      my_setting\n-----------------------\n set in subtransaction\n(1 row)\n\nROLLBACK\n`\n```\nNote that, now,\n\nthe `BEGIN \u2026 EXCEPTION` block does cause the change to `my.setting` to get lost,\nbut only if an exception (`division_by_zero`) indeed got raised,\neven though the `division_by_zero` was triggered by the statement after `set_config()`.\n\nSo, it's somewhat close to the behaviour you describe, but not quite. Your setting didn't persist outside of your `BEGIN \u2026 EXCEPTION` block regardless of whether a `no_data_found` exception was hit or not.\nAs a little detour: I stumbled upon your question, because I was interested in whether I could treat the settings like a stack that unwinds together with the (sub)transaction stack. It turns out that I could:\n`create or replace function set_config_stacked(denominator int)\n    returns bool\n    language 'plpgsql'\nas $body$\nbegin\n    perform set_config('my.setting', 'set before substraction', true);\n\n    begin\n        perform set_config('my.setting', 'set within subtransaction before division', true);\n        perform 10 / denominator;\n        return true;\n    exception when division_by_zero then\n        return false;\n    end;\nend;\n$body$;\n\nbegin;\nselect set_config_stacked(1);\nselect current_setting('my.setting') as my_setting;\nrollback;\n`\n```\n`CREATE FUNCTION\nBEGIN\n set_config_stacked\n--------------------\n t\n(1 row)\n\n                my_setting\n-------------------------------------------\n set within subtransaction before division\n(1 row)\n\nROLLBACK\n`\n```\n`begin;\nselect set_config_stacked(0);\nselect current_setting('my.setting') as my_setting;\nrollback;\n`\n```\n`BEGIN\n set_config_stacked\n--------------------\n f\n(1 row)\n\n       my_setting\n-------------------------\n set before substraction\n(1 row)\n\nROLLBACK\n`\n```\nSo `my.setting` is actually restored to its previous setting when the subtransaction of the `BEGIN \u2026 EXCEPTION` block closes.\nI need one final change to reproduce your function's behaviour:\n`create table tab (id uuid primary key, stuff text);\ninsert into tab (id, stuff) values (uuid('180e1b14-21e5-4e66-a9b8-db09139d6278'), 'some stuff');\n\ncreate or replace function set_config_stacked(id$ uuid)\n    returns bool\n    language 'plpgsql'\nas $body$\ndeclare\n    rec tab%rowtype;\nbegin\n    perform set_config('my.setting', 'set before substraction', true);\n\n    begin\n        perform set_config('my.setting', 'set within subtransaction before SELECT', true);\n        select * into strict rec from tab where id = id$;\n        perform set_config('my.setting', 'set within subtransaction after SELECT', true);\n        return true;\n    exception when no_data_found then\n        return false;\n    end;\nend;\n$body$;\n\nbegin;\nselect set_config_stacked(uuid('180e1b14-21e5-4e66-a9b8-db09139d6278'));  -- exist\nselect current_setting('my.setting') as my_setting;\nrollback;\n`\n```\n`CREATE TABLE\nINSERT 1 0\nCREATE FUNCTION\n set_config_stacked\n--------------------\n t\n(1 row)\n\n               my_setting\n----------------------------------------\n set within subtransaction after SELECT\n(1 row)\n\nROLLBACK\n`\n```\n`begin;\nselect set_config_stacked(uuid('cc7ad0c3-7e3a-49a0-b7d8-7b4093ae0028'));  -- doesn't exist\nselect current_setting('my.setting') as my_setting;\nrollback;\n`\n```\n`BEGIN\n set_config_stacked\n--------------------\n f\n(1 row)\n\n       my_setting\n-------------------------\n set before substraction\n(1 row)\n\nROLLBACK\n`\n```\nAs you can see, there is still one aspect of the behavior that you describe which I cannot reproduce. This version of `set_config_stacked()` that operates around a `no_data_found` exception behaves the same as the previous version based around the `division_by_zero` exception.\nHowever, your example suggests that your settings also don't persist outside of `auth.authorize()` when no exception is hit. That confounds me, and is not something I am able to reproduce, at least here on Postgres 14.\nRegardless of this, your problem can be solved by moving the calls to `set_config()` below the `BEGIN \u2026 EXCEPTION \u2026 END` block. You will still want to set your variables within that block, but do these calls and the `return true` from a new to be created outer `BEGIN \u2026 END` block (without `EXCEPTION`).",
      "question_score": 2,
      "answer_score": 4,
      "created_at": "2022-02-23T19:12:13",
      "url": "https://stackoverflow.com/questions/71242103/why-does-postgres-set-configs-is-local-true-not-persist-the-variable-for-the"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "plpgsql",
      "question_id": 71040855,
      "title": "How to &quot;PERFORM&quot; a CTE query returning multiple rows/columns?",
      "problem": "As a follow-up to this question:\n\nHow to \"PERFORM\" CTE queries in PL/pgSQL?\n\nI try:\n```\n`perform (with test_as_cte as(select * from myTable) select * from test_as_cte);\n`\n```\nBut get the following error:\n\n```\n`SQL Error [42601]: ERROR: subquery must return only one column\nWhere: PL/pgSQL function inline_code_block line 9 at PERFORM\n`\n```\n\nIf I replace `*` with `myCol` in the above code there is no error.\nHowever, I need to do realistic performance testing with the CTE and return multiple columns.",
      "solution": "The `WITH` query enclosed in parentheses is treated like a sub-select. It works fine the way you have it as long as it returns a single value (one column of one row). Else you must treat it as subquery and call it like this (inside a PL/pgSQL code block!):\n```\n`PERFORM * FROM (with test_as_cte as (select * from b2) select * from test_as_cte t) sub;\n`\n```\nOr just:\n```\n`PERFORM FROM () sub;\n`\n```\nThe manual:\n\n`PERFORM` `query`;\nThis executes `query` and discards the result. Write the query\nthe same way you would write an SQL `SELECT` command, but replace the\ninitial keyword `SELECT` with `PERFORM`. For `WITH` queries, use\n`PERFORM` and then place the query in parentheses. (In this case, the\nquery can only return one row.)\n\nI think this could be clearer. I'll suggest a fix for the documentation.",
      "question_score": 2,
      "answer_score": 4,
      "created_at": "2022-02-08T22:02:40",
      "url": "https://stackoverflow.com/questions/71040855/how-to-perform-a-cte-query-returning-multiple-rows-columns"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "plpgsql",
      "question_id": 69825388,
      "title": "PostgreSQL: function with array argument",
      "problem": "For the first time I'm trying to create function in PostgreSQL:\nThis function must accept parameter with array type. Array contains sequence of biginteger values. Size of array not arbitrary, but known in advance.\n```\n` create function get_total (cols ARRAY) returns biginteger AS\n   $BODY$\n       begin \n         // Some logics \n       end;\n   $BODY$ \n`\n```\nUsage in query\n```\n` select \n     stats.value1, \n     stats.value2,\n     get_total(array_agg(ARRAY[stats.value2, stats.value1])) \n from stats;\n`\n```\nIt returns error:\n```\n` type cols[] does not exist\n SQL state: 42704\n`\n```\nWhen I run in Select only  array_agg(ARRAY[stats.value2, stats.value1]),  I see, that array created successfully. So the problem in function parameter.\nWhat am I doing wrong?",
      "solution": "You have to declare the parameter as `bigint[]`, which reads an array of type `bigint` e.g.\n```\n`CREATE OR REPLACE FUNCTION get_total (bigint[]) \nRETURNS bigint AS\n$$\nBEGIN\n -- your fancy logic goes here\nEND;\n$$ LANGUAGE plpgsql\n`\n```\nFunction call:\n```\n`SELECT get_total(ARRAY[1111111111111,1111111111111]); \n`\n```\nAn elegant alternative is to declare the parameter as `VARIADIC`. Doing so you may call your function with multiple parameters,e.g.:\n```\n`CREATE OR REPLACE FUNCTION get_total (VARIADIC bigint[]) \nRETURNS bigint AS\n$$\nBEGIN  \n -- your fancy logic goes here\nEND;\n$$ LANGUAGE plpgsql;   \n`\n```\nFunction call:\n```\n`SELECT get_total(1111111111111,1111111111111,1111111111111); \n`\n```\nDemo: `db<>fiddle`",
      "question_score": 2,
      "answer_score": 3,
      "created_at": "2021-11-03T14:07:27",
      "url": "https://stackoverflow.com/questions/69825388/postgresql-function-with-array-argument"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "plpgsql",
      "question_id": 68618193,
      "title": "Can I create and access a table in the same SQL function?",
      "problem": "I am trying to create a Postgres SQL-function which runs some routine for my database.\nThe SQL-function calls a plpgsql-function which creates several temporary tables, but doesn't return anything (`RETURNS void`).\nOne of the tables created by the plpgsql-function is supposed to be used in my sql-function.\n`CREATE OR REPLACE FUNCTION public.my_sql_function()\n  RETURNS text AS\n$BODY$\n\nselect public.my_plpsql_function(); -- this returns void, but has created a temp table \"tmp_tbl\"\n\nDROP TABLE IF EXISTS mytable CASCADE;\nCREATE TABLE mytable (\n  skov_id int8 PRIMARY KEY,\n  skov_stor int4,\n  skov_areal_ha numeric,\n  virkningfra timestamp(0) without time zone,\n  plannoejagtighed float8,\n  vertikalnoejagtighed float8,\n  geom geometry(MultiPolygon,25832),\n  orig_geom geometry(Polygon, 25832) \n);\n\nINSERT INTO mytable\nselect * from tmp_tbl ....\n\n$BODY$  LANGUAGE sql;\n`\nWhen I try to run the lines, I get the following error:\n\n```\n`ERROR: relation \"tmp_tbl\" does not exist\n`\n```\n\npgAdmin underlines the line `select * from tmp_tbl ...` as the part with an error.\nSo the SQL-function doesn't notice that the plpsql-function has created a temporary table.\nIs there a workaround?",
      "solution": "I think so it is not possible - and minimally it should not by possible in future versions. SQL functions are similar to views, and then references to database object should be valid in function's creating time.\nThere is not any workaround - if you need temp table, use PLpgSQL, or try to write your code without temp table (it can be much better).",
      "question_score": 2,
      "answer_score": 1,
      "created_at": "2021-08-02T09:59:17",
      "url": "https://stackoverflow.com/questions/68618193/can-i-create-and-access-a-table-in-the-same-sql-function"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "plpgsql",
      "question_id": 73128371,
      "title": "Postgres function returning a string from a select statement",
      "problem": "I am trying to get this function to return a string that comes from the SELECT statement.\n```\n`CREATE OR REPLACE FUNCTION dbo.fnRepID (pram_ID BIGINT)\nRETURNS varchar AS $$\nDECLARE \n    pram varchar := '';\nBEGIN\n    SELECT pram =  pram || (Case COALESCE(a.Name, '') WHEN '' THEN '' ELSE b.Name || ' - ' END )\n        || (Case COALESCE(b.Name, '' ) WHEN '' THEN '' ELSE b.Name || ' - ' END )\n        || f.NAME || ';'\n    FROM dbo.ClassRelationship a\n        LEFT JOIN dbo.ClassRelationship b ON a.ParentClassID = b.ClassID and b.Type = 2 and a.Type = 1;\n \n    RETURN pram;\nEND;\n$$ LANGUAGE plpgsql;\n\nGRANT EXECUTE ON FUNCTION dbo.fnRepID (BIGINT) TO \"postgres\";\n`\n```\nAnd this function creation seem to go through just fine, now I want to call this function with 0 as pram_ID but it's saying this basically no matter what I do.\n```\n`ERROR:  query has no destination for result data\nHINT:  If you want to discard the results of a SELECT, use PERFORM instead.\nCONTEXT:  PL/pgSQL function dbo.fnrepid(bigint) line 5 at SQL statement\nSQL statement \"SELECT dbo.fnRepID(0)\"\nPL/pgSQL function inline_code_block line 2 at PERFORM\nSQL state: 42601\n`\n```\nI have tried a DO PERFORM block as I have seen similar examples\n`DO $$ BEGIN PERFORM dbo.fnRepID(0); END $$;`but I basically want to get the return value from this function somehow. If there's anything wrong with my procedures please let me know, as I am new to postgres. Thank you.\nEdit: Would the following work if I want to append each row of the results into the same var? (can I use select into with the variable on both sides to append to itself?)\n```\n`CREATE OR REPLACE FUNCTION dbo.fnRepID (pram_ID BIGINT)\nRETURNS varchar AS $$\nDECLARE \n    pram varchar := '';\nBEGIN\n    SELECT pram || (Case COALESCE(a.Name, '') WHEN '' THEN '' ELSE b.Name || ' - ' END )\n        || (Case COALESCE(b.Name, '' ) WHEN '' THEN '' ELSE b.Name || ' - ' END )\n        || f.NAME || ';' INTO pram\n    FROM dbo.ClassRelationship a\n        LEFT JOIN dbo.ClassRelationship b ON a.ParentClassID = b.ClassID and b.Type = 2 and a.Type = 1;\n \n    RETURN pram;\nEND;\n$$ LANGUAGE plpgsql;\n\nGRANT EXECUTE ON FUNCTION dbo.fnRepID (BIGINT) TO \"postgres\";\n`\n```",
      "solution": "You're doing\n```\n`SELECT pram = (\u2026) FROM dbo.ClassRelationship a \u2026;\n`\n```\nwhere `(\u2026)` is an expression that is evaluated and then compared to the current value of `pram` (which was initialised to an empty string). The query does nothing else, there is no destination for this boolean value (comparison result) it computes, you're getting an error.\nYou most likely meant to either perform an assignment\n```\n`pram = SELECT (\u2026) FROM dbo.ClassRelationship a \u2026;\n`\n```\nor use an `INTO` clause:\n```\n`SELECT (\u2026) INTO pram FROM dbo.ClassRelationship a \u2026;\n`\n```\n\nNotice that you don't even need pl/pgsql to do this. A plain sql function would do as well:\n`CREATE OR REPLACE FUNCTION dbo.fnRepID(pram_ID BIGINT) RETURNS varchar\nLANGUAGE sql\nSTABLE\nRETURN (\n  SELECT\n    '' ||\n    (CASE COALESCE(a.Name, '') WHEN '' THEN '' ELSE b.Name || ' - ' END) ||\n    (CASE COALESCE(b.Name, '' ) WHEN '' THEN '' ELSE b.Name || ' - ' END) ||\n    f.NAME ||\n    ';'\n  FROM dbo.ClassRelationship a\n  LEFT JOIN dbo.ClassRelationship b ON a.ParentClassID = b.ClassID AND b.Type = 2 AND a.Type = 1\n);\n`",
      "question_score": 2,
      "answer_score": 3,
      "created_at": "2022-07-26T20:48:16",
      "url": "https://stackoverflow.com/questions/73128371/postgres-function-returning-a-string-from-a-select-statement"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "plpgsql",
      "question_id": 71072257,
      "title": "PSQL passing external variables",
      "problem": "I'm trying to pass a value/variable from a batch script to a SQL script and I am having trouble.\nThe code flows as such:\nStarting in my batch script, I make a psql command call referencing `myFirstSQLFile.sql`, and passing `env` as a variable with a value of `test`\n```\n`%PSQLCOMMAND% -h {server} -U {user} -d {dbname} -E -q -f myFirstSQLFile.sql -v env=\"test\"\n`\n```\nIn my `myFirstSQLFile.sql`, it makes references to a number of other sql files, but I'll just show one for simplicity. It is in this file that I know I can access my `env` variable by using `:env`\n```\n`\\i myLastSQLFile.sql;\n`\n```\nWithin `myLastSQLFile.sql`, I have sql code that is meant to use that external `env` var for conditional statements as below\n```\n`DO\n$do$\nBEGIN\n   IF :env = 'test' THEN\n       # Do Something\n   END IF;\nEND\n$do$\n`\n```\nI know outside of those `$do$` tags, I can access my variable by using the notation `:env`, However within them I cannot. I don't really understand what the `$do$` tags or the `BEGIN/END` block are for and if/how I can access my `env` variable within them.",
      "solution": "A `DO` statement executes an anonymous code block. The \"$do$ tags\" are dollar-quotes. Read this first to understand:\n\nWhat are '$$' used for in PL/pgSQL\n\n`BEGIN`and `END` are decorators for a PL/pgSQL code block - which is the default PL (programming language) of a `DO` statement.\npsql does not interpolate variables inside quoted SQL literals and identifiers.\nOne way to fix your problem is using Postgres string processing and then execute the result with `\\gexec`. From psql:\n`SELECT format($$\nDO\n$do$\nBEGIN\n   IF %L = 'test' THEN\n      -- do something\n   END IF;\nEND\n$do$;\n$$, :'env')\\gexec\n`\nThere are various other ways. Maybe a (temporary) function can serve you better: you can pass parameters to it. Depends on the complete picture.\nSee this answers to a very similar question on dba.SE:\n\nHow to pass variable to PL/pgSQL code from the command line?",
      "question_score": 2,
      "answer_score": 3,
      "created_at": "2022-02-10T22:04:57",
      "url": "https://stackoverflow.com/questions/71072257/psql-passing-external-variables"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "plpgsql",
      "question_id": 70987689,
      "title": "How to do forward fill as a PL/PGSQL function",
      "problem": "I am trying to create a pl/pgsql equivalent to the pandas 'ffill' function. The function should forward fill null values. In the example I can do a forward fill but I get errors when I try to create a function from my procedure. The function seems to reflect exactly the procedure but I get a syntax error at the portion `... as $1`.\nWhy? What should I be reading to clarify?\n```\n`-- Forward fill experiment\nDROP TABLE IF EXISTS example;\ncreate temporary table example(id int, str text, val integer);\ninsert into example values\n(1, 'a', null),\n(1, null, 1),\n(2, 'b', 2),\n(2,null ,null );\n\nselect * from example\n\nselect (case\n            when str is null\n            then lag(str,1) over (order by id)\n            else str\n            end) as str,\n            (case\n            when val is null\n            then lag(val,1) over (order by id)\n            else val\n            end) as val\nfrom example\n\n-- Forward fill function\ncreate or replace function ffill(text, text, text) -- takes column to fill, the table, and the ordering column \nreturns text as $$\n    begin\n        select (case\n            when $1 is null\n            then lag($1 ,1) over (order by $3)\n            else $1\n            end) as $1\n        from $2;\n    end;\n$$ LANGUAGE plpgsql;\n`\n```\nUpdate 1:   I did some additional experimenting taking a different approach.  The code is below.  It uses the same example table as above.\n```\n`CREATE OR REPLACE FUNCTION GapFillInternal( \n    s anyelement, \n    v anyelement) RETURNS anyelement AS \n$$\ndeclare \n    temp alias for $0 ;\nbegin\n    RAISE NOTICE 's= %, v= %', s, v;\n    if v is null and s notnull then\n        temp := s;\n    elsif s is null and v notnull then\n        temp := v;\n    elsif s notnull and v notnull then \n        temp := v;\n    else\n        temp := null;\n    end if;\n    RAISE NOTICE 'temp= %', temp;\n    return temp;\nEND; \n$$ LANGUAGE PLPGSQL; \n\nCREATE AGGREGATE GapFill(anyelement) ( \n  SFUNC=GapFillInternal, \n  STYPE=anyelement \n);\n\nselect id, str, val, GapFill(val) OVER (ORDER by id) as valx\nfrom example;\n`\n```\nThe resulting table is this:\n\nI don't understand where the '1' in the first row of `valx` column comes from. From the `raise notice` output it should be `Null` and that seems a correct expectation from the `CREATE AGGREGATE` docs.",
      "solution": "Correct call\nSeems like your displayed query is incorrect, and the test case is just too reduced to show it.\nAssuming you want to \"forward fill\" partitioned by `id`, you'll have to say so:\n`SELECT row_num, id\n     , str, gap_fill(str) OVER w AS strx\n     , val, gap_fill(val) OVER w AS valx\nFROM   example\nWINDOW w AS (PARTITION BY id ORDER BY row_num);  -- !\n`\nThe `WINDOW` clause is just a syntactical convenience to avoid spelling out the same window frame repeatedly. The important part is the added `PARTITION` clause.\nSimpler function\nMuch simpler, actually:\n`CREATE OR REPLACE FUNCTION gap_fill_internal(s anyelement, v anyelement)\n  RETURNS anyelement\n  LANGUAGE plpgsql AS\n$func$\nBEGIN\nRETURN COALESCE(v, s);  -- that's all!\nEND\n$func$;\n\nCREATE AGGREGATE gap_fill(anyelement) ( \n  SFUNC = gap_fill_internal, \n  STYPE = anyelement \n);\n`\nSlightly faster in a quick test.\nStandard SQL\nWithout custom function:\n`SELECT row_num, id\n     , str, first_value(str) OVER (PARTITION BY id, ct_str ORDER BY row_num) AS strx\n     , val, first_value(val) OVER (PARTITION BY id, ct_val ORDER BY row_num) AS valx\nFROM (     \n   SELECT *, count(str) OVER w AS ct_str, count(val) OVER w AS ct_val\n   FROM   example\n   WINDOW w AS (PARTITION BY id ORDER BY row_num)\n   ) sub;\n`\nThe query becomes more complex with a subquery. Performance is similar. Slightly slower in a quick test.\nMore explanation in these related answers:\n\nCarry over long sequence of missing values with Postgres\nRetrieve last known value for each column of a row\nSQL group table by \"leading rows\" without pl/sql\n\ndb<>fiddle here - showing all with extended test case",
      "question_score": 2,
      "answer_score": 3,
      "created_at": "2022-02-04T15:06:30",
      "url": "https://stackoverflow.com/questions/70987689/how-to-do-forward-fill-as-a-pl-pgsql-function"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "plpgsql",
      "question_id": 68186665,
      "title": "Execute multiple SQL strings created in query",
      "problem": "I am trying to dynamically create audit tables for (almost) all tables in the database. I can generate the appropriate SQL dynamically, like so:\n```\n`SELECT                          \n    'CREATE TABLE IF NOT EXISTS '\n    || tab_name || '_audit(timestamp TIMESTAMPTZ NOT NULL, entity JSONB NOT NULL);'\nFROM (\n    SELECT                                                                     \n        quote_ident(table_schema) || '.' || quote_ident(table_name) as tab_name\n    FROM                         \n        information_schema.tables\n    WHERE                                                       \n        table_schema NOT IN ('pg_catalog', 'information_schema')\n        AND table_schema NOT LIKE 'pg_toast%'\n) tablist;\n`\n```\nThis gives me a series of rows of the form:\n```\n`CREATE TABLE IF NOT EXISTS public.table1_audit(timestamp TIMESTAMPTZ NOT NULL, entity JSONB NOT NULL);\nCREATE TABLE IF NOT EXISTS public.table2_audit(timestamp TIMESTAMPTZ NOT NULL, entity JSONB NOT NULL);\n`\n```\nEtc.\nWhat I am struggling with is actually executing those dynamically generated queries. From searching `EXECUTE` seemed to be the required function, but I could not get it to work without either producing a syntax error, or just doing nothing. I would appreciate a point in the right direction.",
      "solution": "You can use dynamic SQL with `EXECUTE` in a loop in a `DO` statement:\n`DO\n$$\nDECLARE\n   _sql text;\nBEGIN\n   FOR _sql IN\n      SELECT format('CREATE TABLE IF NOT EXISTS %I.%I(timestamp timestamptz NOT NULL, entity jsonb NOT NULL)'\n                  , schemaname\n                  , tablename || '_audit')\n      FROM   pg_catalog.pg_tables  -- only tables and partitioned tables, no toast tables\n      WHERE  schemaname NOT IN ('pg_catalog', 'information_schema')\n   LOOP\n      RAISE NOTICE '%', _sql;\n      -- EXECUTE _sql;\n   END LOOP;\nEND\n$$;\n`\nI threw in a `RAISE NOTICE` to inspect the payload first.\nUncomment the `EXECUTE` line to actually execute.\nYou had `quote_ident(table_name)` before appending '_audit'. That would fail for all table names that actually require double-quoting. You'd have to do `quote_ident(table_name || 'audit')`. But I use `format()` instead. More convenient. See:\n\nDefine table and column names as arguments in a plpgsql function?\n\nI also use `pg_catalog.pg_tables` instead of `information_schema.tables`. Faster, and exactly what you need. See:\n\nHow to check if a table exists in a given schema\n\nAnd I scrapped the subquery - not needed.",
      "question_score": 2,
      "answer_score": 3,
      "created_at": "2021-06-30T01:13:58",
      "url": "https://stackoverflow.com/questions/68186665/execute-multiple-sql-strings-created-in-query"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "plpgsql",
      "question_id": 67506420,
      "title": "Access column using variable instead of explicit column name",
      "problem": "I would like to access a column by using variable instead of a static column name.\nExample:\n```\n`variable := 'customer';\n\nSELECT table.variable (this is what I would prefer) instead of table.customer\n`\n```\nI need this functionality as records in my table vary in terms of data length (eg. some have data in 10 columns, some in 14 or 16 columns etc.) so I need to address columns dynamically. As I understand, I can't address columns by their index (eg. select 8-th column of the table) right?\nI can loop and put the desired column name in a variable for the given iteration. However, I get errors when I try to access a column using that variable (e.g. `table_name.variable` is not working).\nFor the sake of simplicity, I paste just some dummy code to illustrate the issue:\n```\n`CREATE OR REPLACE FUNCTION dynamic_column_name() returns text\nLANGUAGE PLPGSQL\nAS $$\nDECLARE\ncol_name text;\nreturn_value text;\n\nBEGIN\n\ncreate table customer (\n    id bigint,\n    name varchar\n);\n\nINSERT INTO customer VALUES(1, 'Adam');\n\ncol_name := 'name';\n\n-- SELECT customer.name INTO return_value FROM customer WHERE id = 1; -- WORKING, returns 'Adam' but it is not DYNAMIC.\n-- SELECT customer.col_name INTO return_value FROM customer WHERE id = 1; -- ERROR:  column customer.col_name does not exist\n-- SELECT 'customer.'||col_name INTO return_value FROM customer WHERE id = 1; -- NOT working, returns 'customer.name'\n-- SELECT customer||'.'||col_name INTO return_value FROM customer WHERE id = 1; -- NOT working, returns whole record + .name, i.e.: (1,Adam).name\n\nDROP TABLE customer;\nRETURN return_value;\nEND;\n$$;\n\nSELECT dynamic_column_name();\n`\n```\nSo how to obtain 'Adam' string with SQL query using `col_name` variable when addressing column of `customer` table?",
      "solution": "SQL does not allow to parameterize identifiers  (incl. column names) or syntax elements. Only values can be parameters.\nYou need dynamic SQL for that. (Basically, build the SQL string and execute.) Use `EXECUTE` in a plpgsql function. There are multiple syntax variants. For your simple example:\n`CREATE OR REPLACE FUNCTION dynamic_column_name(_col_name text, OUT return_value text)\n  RETURNS text\n  LANGUAGE plpgsql AS\n$func$\nBEGIN\n   EXECUTE format('SELECT %I FROM customer WHERE id = 1', _col_name)\n   INTO return_value;\nEND\n$func$;\n`\nCall:\n```\n`SELECT dynamic_column_name('name');\n`\n```\ndb<>fiddle here\nData types have to be compatible, of course.\nMore examples:\n\nHow to use text input as column name(s) in a Postgres function?\nhttps://stackoverflow.com/search?q=%5Bpostgres%5D+%5Bdynamic-sql%5D+parameter+column+code%3AEXECUTE",
      "question_score": 2,
      "answer_score": 3,
      "created_at": "2021-05-12T17:14:42",
      "url": "https://stackoverflow.com/questions/67506420/access-column-using-variable-instead-of-explicit-column-name"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "plpgsql",
      "question_id": 66459092,
      "title": "PostgreSQL : How to run ALTER queries returned as a result from SQL SELECT statement",
      "problem": "I want to add a new column to all tables with table name pattern table_<>_details.\nI use this query :\n```\n` select 'alter table ' || table_name || ' ADD COLUMN CREATED TIMESTAMP;'\n from information_schema.tables\n where table_name like 'table_%_details';\n`\n```\nto generate the DDL queries which looks like :\n```\n`alter table table_1_details ADD COLUMN CREATED TIMESTAMP;\nalter table table_2_details ADD COLUMN CREATED TIMESTAMP;\nalter table table_3_details ADD COLUMN CREATED TIMESTAMP;\nalter table table_4_details ADD COLUMN CREATED TIMESTAMP;\nalter table table_5_details ADD COLUMN CREATED TIMESTAMP;\nalter table table_6_details ADD COLUMN CREATED TIMESTAMP;\nalter table table_7_details ADD COLUMN CREATED TIMESTAMP;\n`\n```\nI tried to loop through these records using the following script :\n```\n`do $$ declare c_query cursor for\nselect\n    'alter table ' || table_name || ' ADD COLUMN CREATED TIMESTAMP;'\nfrom\n    information_schema.tables\nwhere\n    table_name like 'table_%_details';\n\nbegin\n for rec in c_query loop\n execute rec;\nend loop;\n\nclose c_query;\nend $$\n\n`\n```\nI have tried to fine tune this script but with no success, I'm getting the following error:\n```\n`SQL Error [42601]: ERROR: syntax error at or near \"\"alter table table_1_details ADD COLUMN CREATED TIMESTAMP;\"\"\n  Where: PL/pgSQL function inline_code_block line 13 at EXECUTE statement\n`\n```\nmy question is how to modify this scrip to loop through all these results and apply the DDL to database , note (I do not want to create functions).\nplease any Ideas will be appreciated.",
      "solution": "Just loop over the resultset of `infomation_schema.tables` and then use `EXECUTE` with your concatenated `ALTER TABLE` statements\n```\n`DO $$\nDECLARE\n  row record;\nBEGIN\n  FOR row IN SELECT table_name FROM information_schema.tables\n             WHERE table_name LIKE 'table_%_details' LOOP\n    EXECUTE 'ALTER TABLE ' || row.table_name || ' ADD COLUMN CREATED TIMESTAMP;';\n  END LOOP;\nEND;\n$$;\n`\n```\nEDIT: Alternatively you can use `FORMAT` to concatenate your strings instead of using `||`, as pointed out by @a_horse_with_no_name\n```\n`EXECUTE FORMAT('ALTER TABLE %I ADD COLUMN CREATED TIMESTAMP;',row.table_name);\n`\n```\nCheck this `db<>fiddle`",
      "question_score": 2,
      "answer_score": 3,
      "created_at": "2021-03-03T15:31:51",
      "url": "https://stackoverflow.com/questions/66459092/postgresql-how-to-run-alter-queries-returned-as-a-result-from-sql-select-state"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "plpgsql",
      "question_id": 65842142,
      "title": "How to update multiple nested keys in a json field in single update query?",
      "problem": "I'm trying to write a function that updates a `json` (not `jsonb`) field (called `settings`) in a table (called `user`).\nMy json object looks like this (however it might not have some of the keys on any nesting level):\n`{\n  \"audiences\": [\"val1\", \"val2\"],\n  \"inviteNumber\": \"123\",\n  \"workExperience\": [\n    {\n      \"company\": {\n        \"name\": \"Ace\",\n        \"industry\": \"Accounting\",\n        \"revenues\": \"1M-10M\",\n        \"size\": \"1-10\"\n      },\n      \"title\": \"Product\",\n      \"startDate\": {\n        \"month\": 1,\n        \"year\": 2018\n      }\n    }\n  ],\n  \"notifications\": {\n    \"emailNotifications\": true\n  },\n  \"jobFunction\": \"Research & Development\",\n  \"phoneNumber\": \"2134447777\",\n  \"areasOfInterest\": {\n    \"Recruiting and Staffing\": true\n  }\n}\n`\nI need to be able to update the \"title\" and \"name\" fields of the 0th element inside \"workExperience\" array.\nWhat I currently have is this:\n`create or replace function my_function()\nreturns trigger language plpgsql as $$\ndeclare\n    companyName character varying(255);\nbegin\n    select company.name into companyName from company where id = new.companyid;\n\n    update \"user\" set\n        email = new.email,\n        \"settings\" = jsonb_set(\n            \"settings\"::jsonb,\n            '{workExperience,0}'::TEXT[],\n            format(\n                '{\"company\": {\"name\": %s}, \"title\": %s}',\n                '\"' || companyName || '\"', '\"' || new.title || '\"'\n            )::jsonb\n        )\n    where id = new.userid;\n    \n    return new;\nend $$;\n`\nHowever the above implementation rewrites the while `workExperience` object removing the keys other than `company` and `title`.\nI've tried looking through this answer on SO, but still wasn't able to implement the updating correctly.\nI see that the problem is with how the `jsonb_set` works and it does just what I tell it to do: set 0th element of \"workExperience\" array to the object I define inside `format` function.\nSeems like I need to use multiple `jsonb_set` one inside another, but I can't figure out the way to do it correctly.\n\nHow can I update my json field correctly without removing other keys from the object?",
      "solution": "jsonb_set() returns the modified JSON value.\nYou could nest the calls and change the company name first, and use the result of that as the input to another jsonb_set().\n```\n`\"settings\" = jsonb_set(jsonb_set(\"settings\"::jsonb, '{workExperience,0,company,name}', to_jsonb(companyname)), \n                       '{workExperience,0,title}', to_jsonb(new.title)\n                      )\n`\n```",
      "question_score": 2,
      "answer_score": 3,
      "created_at": "2021-01-22T10:02:33",
      "url": "https://stackoverflow.com/questions/65842142/how-to-update-multiple-nested-keys-in-a-json-field-in-single-update-query"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "plpgsql",
      "question_id": 65745220,
      "title": "How do I insert an empty row, but have the serial update correctly?",
      "problem": "I would like to insert a blank record into a table and have its `serial` primary key value update. I would then like to get the new value and insert it into a temporary table. This will take place within a function using language `plpgsql`.\nSo far I have this:\n```\n`CREATE TEMP TABLE _InsertedpostID ( -- to store inserted postid\n        postid int\n    );\n\n    INSERT INTO post\n    (\n        postid, --serial which needs to be held in the temp table above\n        title,\n        location\n    )\n    VALUES(NULL);\n\n    --- here I need to get the just inserted postid serial and put it into the _InsertedpostID table\n`\n```\nThe above does not insert anything (I grabbed the solution from a MySQL answer). It returns an error of:\n\n[42601] ERROR: INSERT has more target columns than expressions\n\nRemoving the `VALUES(NULL);` part does not work either like it does in SQL Server. How can I therefore insert a blank record with only the `serial` updating?\nOnce a new record is generated with a new `serial` number, how do I output that back into the temp table?",
      "solution": "You don't really need PL/pgSQL for that. If `post.postid` really is a serial (an `identity` would be better), then the following will work:\n```\n`create temp table _insertedpostid (\n    postid int\n);\n\nwith new_post as (\n  insert into post (postid)\n  values(default)\n  returning postid\n)\ninsert into _insertedpostid (postid)\nselect postid\nfrom new_post;\n`\n```\nHowever, if this is really inside a PL/pgSQL function, there is no need for a costly temp table:\n```\n`....\ndeclare\n  l_postid integer;\nbegin\n  insert into post (postid) values (default)\n  returning postid\n  into l_postid;\n  \n  --- work with l_postid\nend;\n`\n```\n\nIf you only want to increment the column's sequence and you don't really need the new row (which seems likely, given the fact that you don't provide any column values at all), then why don't you simply call `nextval()`?\n```\n` select nextval(pg_get_serial_sequence('post', 'postid'));\n`\n```\nIn PL/pgSQL you can simply assign that to a variable without the need for a dummy row:\n```\n`....\ndeclare\n  l_postid integer;\nbegin\n  ...\n  l_postid := nextval(pg_get_serial_sequence('post', 'postid'));\n  ....\nend;\n`\n```",
      "question_score": 2,
      "answer_score": 2,
      "created_at": "2021-01-16T01:16:52",
      "url": "https://stackoverflow.com/questions/65745220/how-do-i-insert-an-empty-row-but-have-the-serial-update-correctly"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "plpgsql",
      "question_id": 78739142,
      "title": "How do I pass JSON to a Postgres COPY tmp FROM PROGRAM command",
      "problem": "I'm trying to use Postgres (PL/pgSQL) to communicate with a shell script that expects JSON data. A simplified example of what I'm trying to do is\n```\n`CREATE OR REPLACE FUNCTION json_func(IN json_in JSONB, OUT json_out JSONB)\nLANGUAGE plpgsql\nAS $code_block$\nBEGIN\nCREATE TEMPORARY TABLE tmp (json_tmp JSONB);\nEXECUTE format($bash$ COPY tmp FROM PROGRAM 'echo ''%s'' | jq . ' $bash$, json_in);\nSELECT json_tmp FROM tmp INTO json_out;\nDROP TABLE tmp;\nEND;\n$code_block$;\n`\n```\nWhen I run\n`SELECT json_func('{\"key1\": \"val1\", \"key2\": \"val2\"}'::JSONB);\n`\nI get\n```\n`ERROR:  invalid input syntax for type json\nDETAIL:  The input string ended unexpectedly.\nCONTEXT:  JSON data, line 1: {\nCOPY tmp, line 1, column json_tmp: \"{\"\nSQL statement \" COPY tmp FROM PROGRAM 'echo ''{\"key1\": \"val1\", \"key2\": \"val2\"}'' | jq . ' \"\nPL/pgSQL function json_func(jsonb) line 4 at EXECUTE\n`\n```\nI've tried various ways of escaping the single quotes needed to surround the JSON input, but no luck.",
      "solution": "You're getting the error because `COPY` expects one value per line, while `jq` pretty-prints by default, splitting the single json structure into multiple lines. As already pointed out by @Francisco Puga, you can disable that:\n\n`--compact-output / -c:`\nBy default, jq pretty-prints JSON output. Using this option will result in more compact output by instead putting each JSON object on a single line.\n\nOtherwise, this\n```\n`'{\"key1\": \"val1\", \"key2\": \"val2\"}'\n`\n```\nGets turned by `jq` into this\n```\n`{\n\u2007\u2007\u2007\u2007\"key1\": \"val1\",\n\u2007\u2007\u2007\u2007\"key2\": \"val2\"\n}\n`\n```\nAnd due to the line-separation thing, `copy` thinks you're trying to do something this:\n```\n`insert into tmp(json_tmp)values \n ('{')\n,('\"key1\": \"val1\",')\n,('\"key2\": \"val2\"')\n,('}');\n`\n```\nHence, the error complaining about the single opening curly bracket. It seems to work fine if you enable compaction:\n`CREATE OR REPLACE FUNCTION json_func(IN json_in JSONB, OUT json_out JSONB)\nLANGUAGE plpgsql\nAS $code_block$\nBEGIN\nCREATE TEMPORARY TABLE tmp (json_tmp JSONB);\nEXECUTE format($dynsql$ COPY tmp FROM PROGRAM $bash$echo %1$L |jq -c . $bash$ \n               $dynsql$, json_in);\nSELECT json_tmp FROM tmp INTO json_out;\nDROP TABLE tmp;\nEND;\n$code_block$;\n`\n\nIf that's an attempt to pretty-print, Postgres offers a built-in `jsonb_pretty()`:\ndemo at db<>fiddle\n```\n`select jsonb_pretty('{\"abc\":123}');\n`\n```\n\njsonb_pretty\n\n{\u2007\u2007\u2007\u2007\"abc\": 123}\n\nIf it's for validation, In version 16+ you can check that with `'abc' IS JSON` or `pg_input_is_valid('abc','jsonb')`:\n`select 'abc' is json;                          --false\nselect pg_input_is_valid('abc','json');        --false\nselect '{\"abc\":123}' is json;                  --true\nselect pg_input_is_valid('{\"abc\":123}','json');--true\n`",
      "question_score": 2,
      "answer_score": 2,
      "created_at": "2024-07-12T09:43:11",
      "url": "https://stackoverflow.com/questions/78739142/how-do-i-pass-json-to-a-postgres-copy-tmp-from-program-command"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "plpgsql",
      "question_id": 66764086,
      "title": "Postgresql create function syntax error near BEGIN",
      "problem": "This is the function I'm trying to execute via migration script.\n```\n`CREATE OR REPLACE FUNCTION test(key1 text)\nRETURNS void AS $$ \nBEGIN\n  INSERT INTO table1(c1) VALUES($1);\nEND;\n$$ LANGUAGE 'plpgsql';\n`\n```\nThe above script executes successfully, but when I tried to with a .NET Core idempotent migration script it gives an error `ERROR:  syntax error at or near \"BEGIN\"`\nThis is the migration script.\n```\n`DO $$\nBEGIN\n    IF NOT EXISTS(SELECT 1 FROM __EFMigrationsHistory WHERE \"MigrationId\" = 'migrationid') THEN\n    CREATE OR REPLACE FUNCTION test(key1 text)\n    RETURNS void AS $$ \n    BEGIN\n      INSERT INTO table1(c1) VALUES($1);\n    END;\n    $$ LANGUAGE 'plpgsql';\n    END IF;\nEND $$;\n`\n```\nI also tried adding EXECUTE still same error.\n```\n`DO $$\nBEGIN\n    IF NOT EXISTS(SELECT 1 FROM __EFMigrationsHistory WHERE \"MigrationId\" = 'migrationid') THEN\n    EXECUTE('CREATE OR REPLACE FUNCTION test(key1 text)\n    RETURNS void AS $$ \n    BEGIN\n      INSERT INTO table1(c1) VALUES($1);\n    END;\n    $$ LANGUAGE ''plpgsql'';');\n    END IF;\nEND $$;\n`\n```\nHow can I create a SQL statement to create/replace the function by checking the migrationhistory table?",
      "solution": "The parser is getting confused by the repetitive usage of dollar quotes. Just give one of them an alias and it should work, e.g.\n```\n`DO $body$\nBEGIN\n    IF NOT EXISTS(SELECT 1 FROM __EFMigrationsHistory WHERE \"MigrationId\" = 'migrationid') THEN\n    EXECUTE('CREATE OR REPLACE FUNCTION test(key1 text)\n    RETURNS void AS $$ \n    BEGIN\n      INSERT INTO table1(c1) VALUES($1);\n    END;\n    $$ LANGUAGE ''plpgsql'';');\n    END IF;\nEND $body$;\n`\n```",
      "question_score": 2,
      "answer_score": 2,
      "created_at": "2021-03-23T14:25:29",
      "url": "https://stackoverflow.com/questions/66764086/postgresql-create-function-syntax-error-near-begin"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "plpgsql",
      "question_id": 78808105,
      "title": "Locking write operations on tables while Postgres procedure executes",
      "problem": "I have created a procedure in Postgres that will create a table from the contents of three other tables (`SELECT` with `JOIN`s). Then the resulting table is altered adding two new columns. Finally, 3 triggers are created on those 3 source tables, so any new writes are propagated to the new table.\nI know the procedure itself is atomic and transactional from its own scope. But the procedure has no means of knowing anything about the three tables. I'm afraid that in the time between the new table creation and the creation of the triggers, some writes to the existing tables could happen thus de-synchronizing the new table, which won't register those writes. That cannot happen.\nMy table creation/trigger creation procedure looks like this:\n```\n`CREATE OR REPLACE PROCEDURE myschema.table_creation()\nLANGUAGE plpgsql\nAS $procedure$\nBEGIN\ncreate table newtable as\nSELECT * FROM myschema.session a NATURAL JOIN (SELECT * FROM myschema.message b NATURAL left JOIN myschema.data) as d;\nALTER TABLE myschema.newtable ADD created_at timestamp;\nALTER TABLE myschema.newtable ADD source text;\nCREATE TRIGGER mytrigger\nafter INSERT\nON myschema.session\nFOR EACH ROW\nEXECUTE PROCEDURE myschema.trigger_proc();\nCREATE TRIGGER mytrigger\nafter INSERT\nON myschema.messages\nFOR EACH ROW\nEXECUTE PROCEDURE myschema.trigger_proc();\nCREATE TRIGGER mytrigger\nafter INSERT\nON myschema.data\nFOR EACH ROW\nEXECUTE PROCEDURE myschema.trigger_proc();\nEND;\n$procedure$; \n`\n```\nHow can I lock the writes in the existing tables to postpone them until the whole `table_creation()` procedure has finished? Otherwise, I would have a race condition and rows could be lost in the new table. I don't think my procedure in its current state has any protection against writes.",
      "solution": "To be absolutely sure, `LOCK` the three source tables exclusively first.\nI suggest a `SHARE` lock. The manual:\n\n[...] This mode protects a table against concurrent data changes.\n\n`BEGIN;\n\nLOCK TABLE myschema.session, myschema.message, myschema.data IN SHARE MODE;  -- \u2460 !!!\n\nCREATE TABLE newtable AS\nSELECT *\nFROM   myschema.session a\nNATURAL JOIN (\n   SELECT *\n   FROM   myschema.message b\n   NATURAL LEFT JOIN myschema.data\n   ) d;\n\nALTER TABLE myschema.newtable  -- \u2461 single command\n  ADD created_at timestamp\n, ADD source text;\n\nCREATE TRIGGER mytrigger\nAFTER INSERT ON myschema.session\nFOR EACH ROW EXECUTE FUNCTION myschema.trigger_proc();  -- \u2462 it's really a function\n\nCREATE TRIGGER mytrigger\nAFTER INSERT ON myschema.messages\nFOR EACH ROW EXECUTE FUNCTION myschema.trigger_proc();\n\nCREATE TRIGGER mytrigger\nAFTER INSERT ON myschema.data\nFOR EACH ROW EXECUTE FUNCTION myschema.trigger_proc();\n\nCOMMIT;\n`\nSince this obviously is a one-time operation with no PL/pgSQL-specific operations, I wouldn't create a procedure - or even use a PL/pgSQL code block at all. A plain transaction does the job.\nConcurrent attempts to write to the table will have to wait until your transaction has finished.\nBut reads are not hindered at all.\n\u2460 Note that the `CREATE TRIGGER`command acquires the slightly stricter lock `SHARE ROW EXCLUSIVE`, but we don't strictly need that lock strength earlier. All locks are released at the end of the transaction.\n\u2461 A single `ALTER` is slightly cheaper, and possible for this.\n\nCannot cast type numeric to boolean\nRenaming multiple columns in one statement with PostgreSQL\n\n\u2462 Assuming at least Postgres 11. See:\n\nTrigger uses a procedure or a function?\n\nAccordingly, I wouldn't use the name `trigger_proc` for the trigger function - though obviously just symbolic here.",
      "question_score": 2,
      "answer_score": 1,
      "created_at": "2024-07-29T18:15:04",
      "url": "https://stackoverflow.com/questions/78808105/locking-write-operations-on-tables-while-postgres-procedure-executes"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "plpgsql",
      "question_id": 68638554,
      "title": "Change all column names to lowercase Postgresql",
      "problem": "I am having an issue with my postgresql database. I added 5 Tables with a lot of data and a lot of columns. Now I noticed I added the columns with a mix of upper and lowercase letters, which makes it difficult to query them using sqlalchemy or pandas.read_sql_query, because I need double quotes to access them.\nIs there a way to change all values in the column names to lowercase letters with a single command?\nIm new to SQL, any help is appreciated.",
      "solution": "Use an `anonymous code block` with a `FOR LOOP` over the table columns:\n```\n`DO $$\nDECLARE row record;\nBEGIN\n  FOR row IN SELECT table_schema,table_name,column_name\n             FROM information_schema.columns\n             WHERE table_schema = 'public' AND \n             table_name   = 'table1'\n  LOOP\n    EXECUTE format('ALTER TABLE %I.%I RENAME COLUMN %I TO %I',\n      row.table_schema,row.table_name,row.column_name,lower(row.column_name));  \n  END LOOP;\nEND $$;\n`\n```\nDemo: `db<>fiddle`",
      "question_score": 1,
      "answer_score": 3,
      "created_at": "2021-08-03T17:08:28",
      "url": "https://stackoverflow.com/questions/68638554/change-all-column-names-to-lowercase-postgresql"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "plpgsql",
      "question_id": 72500652,
      "title": "How to identify slow queries in PostgreSQL function?",
      "problem": "How can I identify slow queries in a Postgres function?\nFor example:\n`CREATE OR REPLACE FUNCTION my_function ()\nRETURNS void AS $$ \nBEGIN\n    query#1;\n    \n    query#2; --> slow query (duration 4 sec)\n    \n    query#3;\n    \n    query#4;\nEND\n$$ LANGUAGE plpgsql;\n`\nAfter executing `my_function()` I get something like this in my Postgres log file:\n\nduration: 4.904 ms  statement: select my_function ();\",,,,,,,,,\"psql\"\n\nSo I can't identify slow queries in my function.",
      "solution": "By default, PL/pgSQL functions are black boxes to the Postgres query planner and logging.\nThe additional module `auto_explain` allows for more insights. It can be loaded dynamically, but you have to be a superuser. (Does not have to be installed like most other modules.)\nTo load it an individual session:\n`LOAD 'auto_explain';\n-- SET auto_explain.log_min_duration = 1; -- exclude very fast trivial queries?\nSET auto_explain.log_nested_statements = ON; -- statements inside functions\n-- SET auto_explain.log_analyze = ON; -- get actual times, too?\n`\nAny query running in the same session will get extra logging. Just:\n```\n`SELECT my_function();  -- your function\n`\n```\nSee:\n\nGet query plan for SQL statement nested in a PL/pgSQL function",
      "question_score": 1,
      "answer_score": 5,
      "created_at": "2022-06-04T16:39:41",
      "url": "https://stackoverflow.com/questions/72500652/how-to-identify-slow-queries-in-postgresql-function"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "plpgsql",
      "question_id": 77981603,
      "title": "Looping in PL/pgSQL to insert rows from multiple unnested arrays?",
      "problem": "I'm trying to create 3 arrays, iterate through each, and insert records depending on the value of those iterations. My code is as follows:\n```\n`do $$\n  declare \n    days integer[];\n    times time[];\n    durations integer[];\n  begin\n    days := array[1,2, 3, 4, 5, 6, 7];\n    times := array['00:00:00',\n      '00:00:00',\n      '01:00:00',\n      '02:00:00',\n      '03:00:00',\n      '04:00:00',\n      '05:00:00',\n      '06:00:00',\n      '07:00:00',\n      '08:00:00',\n      '09:00:00',\n      '10:00:00',\n      '11:00:00',\n      '12:00:00',\n      '13:00:00',\n      '14:00:00',\n      '15:00:00',\n      '16:00:00',\n      '17:00:00',\n      '18:00:00',\n      '19:00:00',\n      '20:00:00',\n      '21:00:00',\n      '22:00:00',\n      '23:00:00'\n    ];\n    durations := array[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 24];\n    FOR day IN days LOOP\n      FOR time IN times LOOP\n        FOR duration IN durations LOOP\n          INSERT INTO public.pricing(id, site_id, start_time, price, day_of_week, booking_duration, inserted_at, updated_at) VALUES (10_000_000_000_000, 9999, time, 1000, day, duration, '2021-09-08 10:19:27.000000 +00:00', '2021-09-08 10:19:27.000000 +00:00');\n        END LOOP;\n      END LOOP;\n    END LOOP;\n  end;\n$$;\n`\n```\nWhen executing this I get:\n`ERROR:  syntax error at or near \"days\"\nLINE 35:     FOR day IN days LOOP\n`\nI've tried not using a variable:\n```\n`FOR day IN (1, 2, 3, 4, 5, 6) LOOP\n`\n```\nBut this throws the same error. I've tried following Looping through a given list of values in PL/pgSQL but can't see where I'm going wrong.",
      "solution": "See the documentation:\n```\n`FOREACH day IN ARRAY days LOOP\n   ...\nEND LOOP;\n`\n```",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2024-02-12T13:55:43",
      "url": "https://stackoverflow.com/questions/77981603/looping-in-pl-pgsql-to-insert-rows-from-multiple-unnested-arrays"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "plpgsql",
      "question_id": 75893986,
      "title": "How to parameterize parametric column type in PostgreSQL `execute format()`?",
      "problem": "I've got some PL/pgSQL code like this:\n`DECLARE\n    v_schema_name pg_catalog.pg_namespace.nspname%type := 'my_schema';\n    v_table_name pg_catalog.pg_tables.tablename%type := 'my_table';\n    v_column_name pg_catalog.pg_attribute.attname%type := 'my_column';\n    v_new_type TEXT := 'DECIMAL(16, 12)';\nBEGIN\n    -- Omitted other code using v_new_type\n    EXECUTE format(\n        'ALTER TABLE %I.%I ALTER COLUMN %I TYPE %I',\n        v_schema_name,\n        v_table_name,\n        v_column_name,\n        v_new_type\n    );\nEND;\n`\nThis results in the following error:\n\nERROR: type \"DECIMAL(16, 12)\" does not exist\n\nI tried changing the last part of the format string to `%L` instead, but that results in this error:\n\nERROR: syntax error at or near \"'DECIMAL(16, 12)'\"\n\nHow do I parameterize this query? Do I need to break it into three parts or something?\nUpdate:\n\n\"The types decimal and numeric are equivalent.\"\nThe `numeric` type can be found with `select * from pg_catalog.pg_type where typname = 'numeric';`, so `v_new_type pg_catalog.pg_type.typname%type := 'decimal';` should be usable.",
      "solution": "type specification in Postgres can be very complex due ANSI SQL types like `DOUBLE PRECISION` or `TIMESTAMP WITH TIME ZONE`. Parsing of these types should be supported by Postgres's parser, and then this syntax is allowed only for built-in types. For more common types the type specification has two parts: identifier and optional type modifier. So your code should to look like:\n```\n`DECLARE\n    v_schema_name pg_catalog.pg_namespace.nspname%type := 'my_schema';\n    v_table_name pg_catalog.pg_tables.tablename%type := 'my_table';\n    v_column_name pg_catalog.pg_attribute.attname%type := 'my_column';\n    v_new_type TEXT := 'decimal'; -- Postgres uses lowercase notation\n    v_new_type_mod TEXT := '(16, 12)'\nBEGIN\n    -- Omitted other code using v_new_type\n    EXECUTE format(\n        'ALTER TABLE %I.%I ALTER COLUMN %I TYPE %I%s',\n        v_schema_name,\n        v_table_name,\n        v_column_name,\n        v_new_type,\n        v_new_type_mod\n    );\nEND;\n`\n```\nNote - you can use %s like proposes @klin, and you can sanitize it by casting to regtype. But it requires correctly entered type name.\n```\n`DECLARE\n  ...\n  v_new_type TEXT := 'decimal(16, 12)';\nBEGIN\n  -- sanitize data type\n  PERFORM v_new_type::regtype;\n  EXECUTE format(... TYPE %s',\n     ...\n     v_new_type);\nEND;\n`\n```",
      "question_score": 1,
      "answer_score": 3,
      "created_at": "2023-03-31T02:47:56",
      "url": "https://stackoverflow.com/questions/75893986/how-to-parameterize-parametric-column-type-in-postgresql-execute-format"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "plpgsql",
      "question_id": 74875797,
      "title": "PostgreSQL: Obtaining multiple values from the record type",
      "problem": "I am writing an sql program that creates a table with columns computed in a function. The function returns the record type.\nThis is what the header looks like:\n```\n`create or replace function get_items(col1 int, col2 int) returns record\n`\n```\nWhat I would like to be able to do is:\n```\n`create table table_items as (\nwith q as (\n    select *,\n   (SELECT * FROM get_items(t.col1, t.col2) AS (item1 integer, item2 integer))\n    from \n    table_t as t\n    )\n    select * from q\n);\n`\n```\nhowever, that results in:\n```\n`ERROR:  subquery must return only one column\n`\n```\nTo fix the error I changed the code into:\n```\n`create table table_items as (\nwith q as (\n    select *,\n   (SELECT item1 FROM get_items(t.col1, t.col2) AS (item1 integer, item2 integer)),\n   (SELECT item2 FROM get_items(t.col1, t.col2) AS (item1 integer, item2 integer)) \n    from \n    table_t as t\n    )\n    select * from q\n);\n`\n```\nThis solution works, though twice as slow when item2 is obtained in addition to item1.\nI assume that it is because the same query is executed twice.\nIs there a way to obtain both items having the function invoked only once?\nThanks a lot!",
      "solution": "This is a good use case for a lateral join.\n`create table table_items as \nselect *\nfrom table_t as t\ncross join lateral\nget_items(t.col1, t.col2) as l(item1 integer, item2 integer);\n`",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2022-12-21T13:03:32",
      "url": "https://stackoverflow.com/questions/74875797/postgresql-obtaining-multiple-values-from-the-record-type"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "plpgsql",
      "question_id": 67350225,
      "title": "How to use text input as column name(s) in a Postgres function?",
      "problem": "I'm working with Postgres and PostGIS. Trying to write a function that that selects specific columns according to the given argument.\nI'm using a `WITH` statement to create the result table before converting it to `bytea` to return.\nThe part I need help with is the `$4` part. I tried it is demonstrated below and `$4::text` and both give me back the text value of the input and not the column value in the table if `cols=name` so I get back from the query name and not the actual names in the table. I also try `data($4)` and got type error.\nThe code is like this:\n`CREATE OR REPLACE FUNCTION select_by_txt(z integer,x integer,y integer, cols text)\n        RETURNS bytea\n        LANGUAGE 'plpgsql'\n    \nAS $BODY$\ndeclare\nres bytea;\nbegin\n    WITH bounds AS (\n      SELECT ST_TileEnvelope(z, x, y) AS geom\n    ),\n    mvtgeom AS (\n      SELECT ST_AsMVTGeom(ST_Transform(t.geom, 3857), bounds.geom) AS geom, $4\n      FROM table1 t, bounds\n      WHERE ST_Intersects(t.geom, ST_Transform(bounds.geom, 4326))\n    )\n    \n    SELECT ST_AsMVT(mvtgeom, 'public.select_by_txt')\n    INTO res\n    FROM mvtgeom;\n    RETURN res;\nend;\n$BODY$;\n`\nExample for calling the function:\n```\n`select_by_txt(10,32,33,\"col1,col2\")\n`\n```\nThe argument cols can be multiple column names from 1 and not limited from above. The names of the columns inside `cols` will be checked before calling the function that they are valid columns.",
      "solution": "Passing multiple column names as concatenated string for dynamic execution urgently requires decontamination. I suggest a `VARIADIC` function parameter instead, with properly quoted identifiers (using `quote_ident()` in this case):\n`CREATE OR REPLACE FUNCTION select_by_txt(z int, x int, y int, VARIADIC cols text[] = NULL, OUT res text)\n  LANGUAGE plpgsql AS\n$func$\nBEGIN\n   EXECUTE format(\n$$\nSELECT ST_AsMVT(mvtgeom, 'public.select_by_txt')\nFROM  (\n   SELECT ST_AsMVTGeom(ST_Transform(t.geom, 3857), bounds.geom) AS geom%s\n   FROM   table1 t\n   JOIN  (SELECT ST_TileEnvelope($1, $2, $3)) AS bounds(geom)\n          ON ST_Intersects(t.geom, ST_Transform(bounds.geom, 4326))\n   ) mvtgeom\n$$, (SELECT ', ' || string_agg(quote_ident (col), ', ') FROM unnest(cols) col)\n   )\n   INTO  res\n   USING z, x, y;\nEND\n$func$;\n`\ndb<>fiddle here\nThe format specifier `%I` for `format()` deals with a single identifier. You have to put in more work for multiple identifiers, especially for a variable number of 0-n identifiers. This implementation quotes every single column name, and only add a `,` if any column names have been passed. So it works for every possible input, even no input at all. Note `VARIADIC cols text[] = NULL` as last input parameter with NULL as default value:\n\nOptional argument in PL/pgSQL function\n\nRelated:\n\nquote_ident() does not add quotes to column name \"first\"\n\nColumn names are case sensitive in this context!\nCall for your example (important!):\n```\n`SELECT select_by_txt(10,32,33,'col1', 'col2');\n`\n```\nAlternative syntax:\n```\n`SELECT select_by_txt(10,32,33, VARIADIC '{col1,col2}');\n`\n```\nMore revealing call, with a third column name and malicious (though futile) intent:\n```\n`SELECT select_by_txt(10,32,33,'col1', 'col2', $$col3'); DROP TABLE table1;--$$);\n`\n```\nAbout that odd third column name and SQL injection:\n\nhttps://www.explainxkcd.com/wiki/index.php/Little_Bobby_Tables\n\nAbout `VAIRADIC` parameters:\n\nReturn rows matching elements of input array in plpgsql function\nPass multiple values in single parameter\n\nUsing an `OUT` parameter for simplicity. That's totally optional. See:\n\nReturning from a function with OUT parameter\n\nWhat I would not do\nIf you really, really trust the input to be a properly formatted list of 1 or more valid column names at all times - and you asserted that ...\n\nthe names of the columns inside cols will be checked before calling the function that they are valid columns\n\nYou could simplify:\n`CREATE OR REPLACE FUNCTION select_by_txt(z int, x int, y int, cols text, OUT res text)\n  LANGUAGE plpgsql AS\n$func$\nBEGIN\n   EXECUTE format(\n$$\nSELECT ST_AsMVT(mvtgeom, 'public.select_by_txt')\nFROM  (\n   SELECT ST_AsMVTGeom(ST_Transform(t.geom, 3857), bounds.geom) AS geom, %s\n   FROM   table1 t\n   JOIN  (SELECT ST_TileEnvelope($1, $2, $3)) AS bounds(geom)\n          ON ST_Intersects(t.geom, ST_Transform(bounds.geom, 4326))\n   ) mvtgeom\n$$, cols\n   )\n   INTO  res\n   USING z, x, y;\nEND\n$func$;\n`\n(How can you be so sure that the input will always be reliable?)",
      "question_score": 1,
      "answer_score": 4,
      "created_at": "2021-05-01T21:39:55",
      "url": "https://stackoverflow.com/questions/67350225/how-to-use-text-input-as-column-names-in-a-postgres-function"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "plpgsql",
      "question_id": 66950911,
      "title": "Concatenate rows in function PostgreSQL",
      "problem": "Assume there's a table projects containing project name, location, team id, start and end years. How can I concatenate rows so that the same names would combine the other information into one string?\n```\n`name        location       team_id      start       end \nLibrary     Atlanta        2389         2015        2017\nLibrary     Georgetown     9920         2003        2007\nMuseum      Auckland       3092         2005        2007\n`\n```\nExpected output would look like this:\n```\n`name                 Records \nLibrary     Atlanta, 2389, 2015-2017\n            Georgetown, 9920, 2003-2007\nMuseum      Auckland, 3092, 2005-2007\n`\n```\nEach line should contain end-of-line / new line character.\nI have a function for this, but I don't think it would work with just using `CONCAT`. What are other ways this can be done? What I tried:\n```\n`CREATE OR REPLACE TYPE projects (name TEXT, records TEXT); \n\nCREATE OR REPLACE FUNCTION records (INT) \nRETURNS SETOF projects AS\n$$ \n     RETURN QUERY\n     SELECT p.name\n            CONCAT(p.location, ', ', p.team_id, ', ', p.start, '-', p.end, CHAR(10))\n     FROM projects($1) p; \n$$ \nLANGUAGE PLpgSQL; \n`\n```\nI tried using CHAR(10) for new line, but its giving a syntax error (not sure why?).\nThe above sample concatenate the string but expectedly leaving out duplicated names.",
      "solution": "You do not need PL/pgSQL for that.\nFirst eliminate duplicate `name`s using `DISTINCT` and then in a subquery you can `concat` the columns into a single string. After that use `array_agg` to create an array out of it. It will then \"merge\" multiple arrays, in case the subquery returns more than one row. Finally, get rid of the commas and curly braces using `array_to_string`. Instead of using the char value of a newline, you can simply use `E'\\n'` (`E` stands for escape):\n```\n`WITH j (name,location,team_id,start,end_) AS (\n  VALUES ('Library','Atlanta',2389,2015,2017),\n         ('Library','Georgetown',9920,2003,2007),\n         ('Museum','Auckland',3092,2005,2007)\n)\nSELECT \n  DISTINCT q1.name,\n  array_to_string(\n  (SELECT array_agg(concat(location,', ',team_id,', ',start,'-', end_, E'\\n'))\n   FROM j WHERE name = q1.name),'') AS records\nFROM j q1;\n\n  name   |           records           \n---------+----------------------------\n Library | Atlanta, 2389, 2015-2017   \n         | Georgetown, 9920, 2003-2007\n         | \n Museum  | Auckland, 3092, 2005-2007  \n`\n```\n\nNote: try to not use reserved strings (e.g. `end`,`name`,`start`, etc.) to name your columns. Although PostgreSQL allows you to use them, it is considered a bad practice.\n\nDemo: `db<>fiddle`",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2021-04-05T11:35:57",
      "url": "https://stackoverflow.com/questions/66950911/concatenate-rows-in-function-postgresql"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "plpgsql",
      "question_id": 79231288,
      "title": "Special variable FOUND not set properly?",
      "problem": "When assigning my variable `_variable`, I need to check whether the query returned any rows. \"No row\" is converted to null, but I want to keep the previous value instead in this case. While an actual null value should be assigned.\nI use the special PL/pgSQL variable `FOUND`:\n`do $$\ndeclare\n    _variable bigint = 100;\n    _tmp bigint;\nbegin\n    drop table if exists tTemp;\n    create temp table tTemp(id bigint, value bigint);\n\n    insert into tTemp(id, value) values (1,1), (2, null);\n\n    _tmp := _variable;  -- backup value\n\n/*  -- first variant - doesn't work\n\n    _variable := (select value from tTemp where id = -1);\n    raise notice '%', FOUND;  -- FOUND is true (?!)\n*/\n    -- second variant\n    select value\n    into _variable\n    from tTemp\n    where id = -1;\n\n    raise notice '%', FOUND;  -- FOUND is false\n\n    -- roll back assignment if no row was found\n    if not FOUND then\n        _variable := _tmp;\n    end if;\nend;\n$$;\n`\nWhy is `FOUND` `true` in the first variant?\nIs there a simpler way to avoid overwriting `_variable` with null?\n`COALESCE` would also catch an actual null value, but I only want to catch \"no row\" - which is indistinguishable after being converted to null.",
      "solution": "Explanation\n\nWhy is `FOUND` `true` in the first variant?\n\nA plain assignment with `:=` or `=` does not set the special variable `FOUND`.\nThe manual lists all events that set `FOUND`.\nIn your first variant `FOUND` still reflects the result from the preceding `INSERT` command.\n`GET DIAGNOSTICS` catches more cases (incl. nested `SELECT`). But \"no row\" converted to a null value also counts as \"1 row processed\", so you get a row count of 1 regardless. See:\n\nDynamic SQL (EXECUTE) as condition for IF statement\n\nThere is a hybrid form for assignments, that sets the `DIAGNOSTIC` value as desired: 0 for \"no row\". (But still no effect on `FOUND`.)\n```\n`_var := value FROM tbl WHERE id = -1;\n`\n```\nSee:\n\nStore query result in a PL/pgSQL variable\n\nYou could work with that. But rather stick to the cleaner `SELECT INTO`. Or consider this ...\nSolution\n\nIs there a simpler way to avoid overwriting _variable with null?\n\nYes, you can use the `UNION ALL` \"hack\".\nIt's almost, but not quite, the same as `COALESCE`. This variant distinguishes between a genuine null value and \"no row\":\n`DO\n$do$\nDECLARE\n   _var int := 100;\nBEGIN\n   SELECT value FROM tbl WHERE id = -1\n   UNION ALL\n   SELECT _var\n   INTO _var;\n   \n   RAISE NOTICE '%', _var;\nEND\n$do$;\n`\n`SELECT ... INTO` (without `STRICT`) assigns values from the first returned  row (discarding the rest). If the payload query returns no row, the second leg of the `UNION ALL` query kicks in: `SELECT _var`.\nVoil\u00e1.\nStrictly speaking, there is no formal guarantee without `ORDER BY`. See:\n\nAre results from UNION ALL clauses always appended in order?\n\nSo if you are going to bet your neck on this:\n`DO\n$do$\nDECLARE\n   _var int := 100;\nBEGIN\n   SELECT value, 1 AS rnk FROM tbl WHERE id = -1\n   UNION ALL\n   SELECT _var, 2\n   ORDER BY rnk\n   INTO _var;\n   \n   RAISE NOTICE '%', _var;\nEND\n$do$;\n`\nAdditional columns are discarded in this assignment, just like additional rows.\nfiddle",
      "question_score": 1,
      "answer_score": 3,
      "created_at": "2024-11-27T18:21:48",
      "url": "https://stackoverflow.com/questions/79231288/special-variable-found-not-set-properly"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "plpgsql",
      "question_id": 79048423,
      "title": "How to allow mixed integer and numeric parameters in icase",
      "problem": "Creatinf icase function using\n```\n`CREATE OR REPLACE FUNCTION public.icase(\ncond1 boolean,  res1 anyelement,\ncond2 boolean,  res2 anyelement,\nconddefault anyelement)    \nRETURNS anyelement    LANGUAGE 'sql' IMMUTABLE PARALLEL UNSAFE AS $BODY$ \nSELECT CASE WHEN $1 THEN $2 WHEN $3 THEN $4 ELSE $5 END; \n$BODY$;\n\nselect \n icase( false, 0, false, 0.,0.)\n`\n```\nthrows error\n```\n`function icase(boolean, integer, boolean, numeric, numeric) does not exist\n`\n```\nHow to allow icase to accept mixed integer/numeric parameters?",
      "solution": "Per the link:\nhttps://www.postgresql.org/docs/current/xfunc-sql.html#XFUNC-SQL-POLYMORPHIC-FUNCTIONS\n\nWith make_array declared as above, you must provide two arguments that are of exactly the same data type; the system will not attempt to resolve any type differences.\n\nAn alternative approach is to use the \u201ccommon\u201d family of polymorphic types, which allows the system to try to identify a suitable common type.\n\nSo:\n```\n`CREATE OR REPLACE FUNCTION public.icase(cond1 boolean, res1 anycompatible, cond2 boolean, res2 anycompatible, conddefault anycompatible)\n RETURNS anycompatible\n LANGUAGE sql\n IMMUTABLE\nAS $function$ \nSELECT CASE WHEN $1 THEN $2 WHEN $3 THEN $4 ELSE $5 END; \n$function$\n;\n`\n```\nWhich when run does:\n```\n`select icase(false, 0, false, 0.,0.);                                                                               \n \nicase \n-------\n     0\n\nselect icase(false, 0, false, 0.,0.0);\n \nicase \n-------\n   0.0\n\n`\n```\nThough as you see it will coerce the output to a common suitable type.",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2024-10-02T22:43:41",
      "url": "https://stackoverflow.com/questions/79048423/how-to-allow-mixed-integer-and-numeric-parameters-in-icase"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "plpgsql",
      "question_id": 78981019,
      "title": "Repeated execution of PL/pgSQL function triggers error &quot;could not open relation with OID ...&quot;",
      "problem": "I've hit a snag with the following PL/pgSQL function I wrote:\n`CREATE OR REPLACE FUNCTION swipl_rpc(IN json_in JSONB, OUT json_out JSONB)\nLANGUAGE plpgsql\nAS $code_block$\nBEGIN\nCREATE TEMPORARY TABLE tmp (json_tmp JSONB);\nEXECUTE format($bash$ COPY tmp FROM PROGRAM 'ggp-json ''%s''' $bash$, json_in);\nSELECT json_tmp FROM tmp INTO json_out;\nDROP TABLE tmp;\nEND;\n$code_block$;\n`\nIt works fine if the returned JSON is a couple of thousand characters long. But a 28618 byte JSON return string resulted in:\n\n```\n`ERROR:  could not open relation with OID 2948131\n`\n```\n\nIt seems to be related to the input string size, but I'm a bit in the dark here.",
      "solution": "About the reported error\nNot related to the length of the input string. The problem was with saved query plans. Not for the `COPY` command but for the subsequent `SELECT` & `DELETE`.\nPL/pgSQL can save query plans for nested SQL statements. Since the function body of a PL/pgSQL function is stored as plain string, no dependencies are registered. If you drop and recreate a table used in a saved plan, that can lead to the error message you reported. See:\n\nLANGUAGE SQL vs LANGUAGE plpgsql in PostgreSQL functions\n\nBetter function\n`CREATE OR REPLACE FUNCTION pg_temp.swipl_rpc(IN json_in jsonb, OUT json_out jsonb)\n  LANGUAGE plpgsql STRICT AS\n$func$\nBEGIN\n   CREATE TEMPORARY TABLE IF NOT EXISTS swipl_rpc_tmp (json_tmp jsonb);\n\n   EXECUTE format('COPY pg_temp.swipl_rpc_tmp FROM PROGRAM %L;\n                   SELECT json_tmp FROM pg_temp.swipl_rpc_tmp;'\n                , 'ggp-json ' || quote_literal(json_in))\n   INTO json_out;\n\n   EXECUTE 'DELETE FROM pg_temp.swipl_rpc_tmp';\nEND\n$func$;\n`\nA temporary table is cheaper and more appropriate for your purpose.\nPlus, your current solution is open to SQL injection.\nThe core problem in your question is solved by executing commands involving the (temporary) table dynamically (with `EXECUTE`), which never uses saved query plans. So dropping and re-creating the table can't invalidate saved plans.\n`EXECUTE` can take multiple SQL statements at once. Put the `SELECT` command right after `COPY`. The `INTO` clause moves from `SELECT` to the `EXECUTE` command, `SELECT` must be the final command.\n(Since I don't drop the temp table, the reported error should not normally happen any more anyway.)\nUse a more specific table name, so that conflicts are unlikely. And schema-qualify explicitly, so it can't be resolved to another table of the same name  in the search path by accident if the temp table is missing (`pg_temp.swipl_rpc_tmp`). Both good practice.\nThe temp table only ever has a single row, so use `DELETE` without `WHERE`. Cheaper.\nSQL injection?\nThere are 4 levels of nested quoting in this piece of code:\n\nThe literal representing the JSON value `json_in` must be quoted like any string. (Escaping any nested single quotes!)\n\nSaid string literal is argument to the program `ggp-json`, and the two are nested in single quotes within the `COPY` command.\n\nThe `COPY` command, plus the subsequent `SELECT` form a string as argument to `EXECUTE`.\n\nThe `EXECUTE` command is nested in the function body of the PL/pgSQL function - another quoted string.\n\nYour solution misses level 1 and pastes `json_in` with the format specifier `%s`. Any nested single quotes are not escaped and trigger an error (in the best case) or can be abused for SQL injection (in the worst case).\nI fixed that by first quoting `json_in` with `quote_literal()`. Then concatenate with your program name `ggp-json` and quote for the next level by using the format specifier `%L` to `format()`.\nSee:\n\nSQL injection in Postgres functions vs prepared queries\nInsert text with single quotes in PostgreSQL\n\nDifferent approach?\n(Ab-)using `COPY` to execute an external program is only a workaround. A proper C function would be (much) more efficient and could return directly without a (temp) staging table.\nOr a function with PL/Perl or PL/Python? (If you have one of those installed.) See:\n\nExecute system commands in PostgreSQL",
      "question_score": 1,
      "answer_score": 1,
      "created_at": "2024-09-13T09:00:57",
      "url": "https://stackoverflow.com/questions/78981019/repeated-execution-of-pl-pgsql-function-triggers-error-could-not-open-relation"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "plpgsql",
      "question_id": 77659264,
      "title": "Postgres update Query for Array column",
      "problem": "This is so simple query. I need to update Bookings tables', LinkedBookings column (int array data type) to it's own Id. Below query didn't work for me.\n```\n`update public.\"Booking\" b set \"LinkedBookings\" = {\"Id\"}\n`\n```\nAbove is giving below error,\n```\n`SQL Error [42601]: ERROR: syntax error at or near \"{\"\n`\n```\nI searching thorough the internet and postgres documentation and couldn't find any proper simple query. I can understand there is a way to do with writing a Pg Function. Rather than doing such a big thing, isn't there any simple query for that?",
      "solution": "Assuming `LinkedBookings` is an array column and assuming that you want to set it to an array containing only the `Id` of each row, the correct `SQL` should be >>\n```\n`   UPDATE public.\"Booking\" b \n    SET \"LinkedBookings\" = ARRAY[b.\"Id\"];\n`\n```",
      "question_score": 1,
      "answer_score": 3,
      "created_at": "2023-12-14T11:05:26",
      "url": "https://stackoverflow.com/questions/77659264/postgres-update-query-for-array-column"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "plpgsql",
      "question_id": 77372710,
      "title": "Spring test @Sql throws &quot;Unterminated dollar quote started ... Expected terminating $$&quot;",
      "problem": "I am testing my Spring Boot API with a test class (FoodE2eTest) and I have a separate SQL script file (setup-test-schema.sql) that I want to run before the tests start to set up schema, tables, and data.\nI am using PostgreSQL.\nThe problem\nWhen I run the file by itself then it works fine but when I run the test class then I get the following error:\n`Unterminated dollar quote started at position 3 in SQL\nDO $$ BEGIN IF EXISTS (SELECT 1 FROM pg_namespace WHERE nspname = 'public') THEN DROP SCHEMA public CASCADE.\nExpected terminating $$\n`\nConfiguration file\n```\n`...\nspring.datasource.url=jdbc:postgresql://localhost:5432/testdb\nspring.datasource.username=testuser\nspring.datasource.password=testpass\nspring.datasource.driver-class-name=org.postgresql.Driver\nspring.datasource.hikari.connection-timeout=30000\nspring.datasource.hikari.maximum-pool-size=10\nspring.datasource.hikari.minimum-idle=5\n...\n`\n```\nTest class\n```\n`@SpringBootTest(webEnvironment = SpringBootTest.WebEnvironment.RANDOM_PORT)\n@TestPropertySource(locations = \"/application-test.properties\")\n@AutoConfigureMockMvc\n@Sql(scripts = {\"/migration/setup-test-schema.sql\"}, executionPhase = Sql.ExecutionPhase.BEFORE_TEST_METHOD)\npublic class FoodE2eTest {\n...\n}\n`\n```\nSQL file\n```\n`-- drop everything\nDO\n$$\n    BEGIN\n        IF EXISTS (SELECT 1 FROM pg_namespace WHERE nspname = 'public') THEN\n            DROP SCHEMA public CASCADE;\n        END IF;\n    END\n$$;\n\nCREATE SCHEMA public;\nGRANT ALL ON SCHEMA public TO postgres;\nGRANT ALL ON SCHEMA public TO public;\nGRANT ALL ON SCHEMA public TO testuser;\n\n-- create tables\nCREATE TABLE public.user(\n...\n`\n```\n\nThe problem persisted\n\nWith no changes when I specified the language.\n\n```\n`DO\n$$\nBEGIN\n    IF EXISTS (SELECT 1 FROM pg_namespace WHERE nspname = 'public') THEN\n        DROP SCHEMA public CASCADE;\n    END IF;\nEND;\n$$ LANGUAGE PLPGSQL;\n`\n```\n\nWhen I moved the @Sql annotation to method level.",
      "solution": "Found a solution but I don't know precisely why it works. I guess the Spring framework test environment doesn't support double dollar signs.\nFixed with this change\n```\n`-- drop everything\nDO\n'\nBEGIN\n    IF EXISTS (SELECT 1 FROM pg_namespace WHERE nspname = ''public'') THEN\n        DROP SCHEMA public CASCADE;\n    END IF;\nEND;\n' LANGUAGE PLPGSQL;\n\nCREATE SCHEMA public;\nGRANT ALL ON SCHEMA public TO postgres;\nGRANT ALL ON SCHEMA public TO public;\nGRANT ALL ON SCHEMA public TO testuser;\n\n-- create tables\nCREATE TABLE public.user\n(\n`\n```",
      "question_score": 1,
      "answer_score": 3,
      "created_at": "2023-10-27T10:46:26",
      "url": "https://stackoverflow.com/questions/77372710/spring-test-sql-throws-unterminated-dollar-quote-started-expected-terminat"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "plpgsql",
      "question_id": 77228730,
      "title": "How to list tables of specific database and sort them by size with PostgreSQL?",
      "problem": "I have tried to select a list of databases and select all tables from each database, ordering the first ten tables by their size - including their indexes- through PL/pgSQL, but I have had a problem to join the query for the database with the query for the tables and their sizes included in the specific database.\nI have started like this:\n```\n`    DO $$\nDECLARE\n  database_name pg_database.datname%TYPE;\n  total_table_size pg_tables.tablename%TYPE;\n    rec record;\nBEGIN\n    FOR rec IN SELECT datname\n  FROM pg_database\nLOOP\n  database_name := rec.datname;\n \n  raise notice 'Database name: %', database_name;\n \n  SELECT tablename,\n  pg_total_relation_size(table_name) AS total_table_size\n  FROM pg_tables\n  INTO table_name, total_table_size\n  ORDER BY pg_total_relation_size(relid) DESC\n  LIMIT 10;\n END LOOP;\nEND;\n$$;\n`\n```\nI do not know how to specify that I want the first ten table names and their sizes ordered from the largest to the smallest of the present selected database. Could anyone help me with it, please?\nI thought about joining the queries somehow, but I have not found a column that I could use to make the join.\nI have searched similar problems in the community, but I did not find something so specific.\nThanks in advance.",
      "solution": "You're already limiting the result to just top 10 rows through the addition of `LIMIT 10`. You're just duplicating the same 10 rows as many times as there are databases in your catalog, but still those are results for the current database you're connected to.\n\nYou're probably familiar with all Database Object Size Functions but while it's good to order by the raw byte size, run it through `pg_size_pretty()` in your select section to make it readable.\n\nDon't forget about schemas/namespaces. You can have multiple of those in a single database, and a table of the same name in each one. In `pg_tables` it's under `schemaname`, in `information_schema.tables` it's under `table_schema`. The recommendation is\n\nSince the information schema is SQL-standard whereas the views described here are PostgreSQL-specific, it's usually better to use the information schema if it provides all the information you need.\n\nWhile you can list all databases, roles and tablespaces available on the cluster in system catalogs, there's no cluster-wide catalog for namespaces and tables. There's no native/built-in/standard way in PostgreSQL to switch databases from inside the session, nor is there a way to reference objects in other databases, be it on the same cluster or elsewhere. To be able to do these things from within your session, you need `postgres_fdw` or `dblink` both of which actually start and additional client on your behalf.\nOff-db, you can simply instruct your client to open and close connections wherever and however you want (the idea with `\\c` or `\\connect -reuse-previous=on` in `psql`) as long as you're fine shoveling the data yourself and having to maintain a separate utility.\n\nThere are pre-built external tools you can use to monitor PostgreSQL.\n\n`CREATE VIEW v_pg_total_relation_sizes` on each of your databases (here are examples you can just prepend with `create view ... as`). To make it present by default in databases created in the future, create one in `template1` database as well. Make them visible through `postgres_fdw`, then create a view that's a `union` of the local one and all the linked ones. Depending on how large your databases are, if this gets slow you might want to consider making it a `materialized view` to cache it.\n\nHere's a demo for point 6 above: (db<>fiddle)\n```\n`create table local_table as\nselect generate_series(1,2e5,1), (gen_random_uuid())::text;\n\ncreate view v_pg_total_relation_sizes as \nSELECT nspname || '.' || relname AS \"relation\",\n    pg_size_pretty(pg_total_relation_size(C.oid)) AS \"readable_size\",\n    pg_total_relation_size(C.oid) as size\nFROM pg_class C\nLEFT JOIN pg_namespace N ON (N.oid = C.relnamespace)\nWHERE nspname NOT IN ('pg_catalog', 'information_schema', 'pg_toast')\nORDER BY pg_total_relation_size(C.oid) DESC;\n`\n```\nConnect to your `template1` database (it's always created during initial PostgreSQL setup since it's meant to be just that: a template for all new databases) and create similar objects:\n```\n`create schema neighbour_schema;\n\ncreate table neighbour_schema.neighbour_table as\nselect generate_series(1,3e5,1), (gen_random_uuid())::text;\n\ncreate view neighbour_schema.v_pg_total_relation_sizes as \nSELECT nspname || '.' || relname AS \"relation\",\n    pg_size_pretty(pg_total_relation_size(C.oid)) AS \"readable_size\",\n    pg_total_relation_size(C.oid) as size\n  FROM pg_class C\n  LEFT JOIN pg_namespace N ON (N.oid = C.relnamespace)\n  WHERE nspname NOT IN ('pg_catalog', 'information_schema', 'pg_toast')\n  ORDER BY pg_total_relation_size(C.oid) DESC;\n\ncreate database neighbour_db;\n`\n```\nNow that they are in `template1`, all new databases created with `create database` will have it from the start.\nWhen setting up `postgres_fdw` (and/or `dblink`), note that by default it'll use local Unix domain socket if you don't supply host and user information. For different users and hosts make sure you're also keeping `pg_hba.conf` up to date on all instances you connect/link to.\nOn your main db:\n```\n`create extension postgres_fdw;\n\nCREATE SERVER foreign_server\n        FOREIGN DATA WRAPPER postgres_fdw\n        OPTIONS (dbname 'neighbour_db');\nCREATE USER MAPPING FOR current_user\n        SERVER foreign_server\n        OPTIONS (user 'postgres');\nCREATE FOREIGN TABLE fdw_v_pg_total_relation_sizes (\n        relation text,\n        readable_size text,\n        size bigint )\n  SERVER foreign_server\n  OPTIONS (schema_name 'neighbour_schema', \n           table_name 'v_pg_total_relation_sizes');\n`\n```\nSet up the materialized view and `UNION ALL` the local and linked views:\n```\n`create materialized view f_v_cluster_pg_total_relation_sizes as \nselect current_database() as dbname,* from v_pg_total_relation_sizes\nunion all\nselect 'neighbour_db',* from fdw_v_pg_total_relation_sizes\norder by size desc;\n\nrefresh materialized view f_v_cluster_pg_total_relation_sizes;\n\nselect * from f_v_cluster_pg_total_relation_sizes;\n`\n```\n\ndbname\nrelation\nreadable_size\nsize\n\nneighbour_db\nneighbour_schema.neighbour_table\n22 MB\n23076864\n\npostgres\npublic.local_table\n15 MB\n15736832\n\npostgres\npublic.f_v_cluster_pg_total_relation_sizes\n16 kB\n16384\n\npostgres\npublic.pg_temp_16464\n8192 bytes\n8192\n\npostgres\npublic.dblink_pkey_results\n0 bytes\n0\n\npostgres\npublic.fdw_v_pg_total_relation_sizes\n0 bytes\n0\n\npostgres\npublic.v_pg_total_relation_sizes\n0 bytes\n0\n\nneighbour_db\nneighbour_schema.v_pg_total_relation_sizes\n0 bytes\n0\n\nNote that this view needs to be redefined whenever new databases are added, which can be automated through dynamic SQL in a PL/pgSQL routine, although not fully since neither a regular trigger on `pg_databse` can be created, nor an event trigger on `ddl_statement_start`/`_end` can fire on a `create database`.",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2023-10-04T12:21:41",
      "url": "https://stackoverflow.com/questions/77228730/how-to-list-tables-of-specific-database-and-sort-them-by-size-with-postgresql"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "plpgsql",
      "question_id": 77003447,
      "title": "Error when creating a gin index with table of millions of documents",
      "problem": "I have a table with millions of text documents in a Postgres 14.7 DB. When I try to generate a GIN index I get this error:\n`create index idx__txt_document__doc_text__gin on txt_document using gin(to_tsvector('simple', doc_text));\n`\n`ERROR:  string is too long for tsvector (4040510 bytes, max 1048575 bytes)\nTime: 6221.251 ms (00:06.221)\n`\nIs there a way to find out the offending text id?\nI get the same error when I create a temp table that contains the text id and the length of document's tsvector.\n`create temp table tmp_foo as\nselect id, length(to_tsvector('simple', doc_text))\nfrom txt_document;\n`\n`ERROR:  string is too long for tsvector (4040510 bytes, max 1048575 bytes)\nCONTEXT:  parallel worker\nTime: 1912.090 ms (00:01.912)\n`\nAny idea how to get the text that creates the error?\nI found the largest document which is about 7MB but creating the tsvector works just fine.\n`select length(to_tsvector('simple', doc_text)) from txt_document where id = ID_LARGEST_TEXT;\n`",
      "solution": "You can loop through your table, catch the exception, and `RAISE` a notice (or whatever):\n`DO\n$do$\nDECLARE\n   r record;\nBEGIN\n   FOR r IN\n      SELECT id, doc_text FROM tbl\n   LOOP\n      PERFORM to_tsvector('simple', r.doc_text);\n   END LOOP;\n   \n   -- Force error for debugging:\n   -- PERFORM to_tsvector('simple', string_agg('longwordnr' || g, ' ')) FROM generate_series (1, 100000) g;  \nEXCEPTION\n   WHEN program_limit_exceeded THEN\n-- WHEN SQLSTATE '54000' THEN  -- the same with error code\n      RAISE NOTICE 'Row with this \"id\" exceeds ts_vector length: %', r.id;\nEND\n$do$\n`\nUsing a `DO` command to execute a PL/pgSQL code block quickly.-\nI got the error code from provoking the same error. Postgres error messages by default add this line:\n`...\nSQL state: 54000\n`\nYour client seems to suppress it, or you did not include it in the question.\nAbout Postgres error codes.\nAbout the `EXCEPTION` clause:\n\nHandling EXCEPTION and return result from function\nPerform action before exception call in Postgres trigger function\n\nNote that duplicative lexemes in input strings are stored \"compressed\" in a `tsvector`. Hence, the longest string does not necessarily produce the longest `tsvector`. Consider this demo:\nfiddle",
      "question_score": 1,
      "answer_score": 3,
      "created_at": "2023-08-29T22:26:16",
      "url": "https://stackoverflow.com/questions/77003447/error-when-creating-a-gin-index-with-table-of-millions-of-documents"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "plpgsql",
      "question_id": 75461465,
      "title": "how to iterate and append jsonb in postgres db function",
      "problem": "I have a DB function, say `\"myWrapperFunction\"()`, which returns a JSON array of objects as below:\n```\n`select * from \"myWrapperFunction\"()\n\n>> [{\"a\":\"b\",\"c\":\"d\"},{\"e\":\"f\"}]\n`\n```\nI have a table \"table1\" with column 'column1'.\nI need to pass the values from `table1.column1` into `myFunction()` as argument and build one json array from the results.\nI have created the below db function and it works fine with only one problem: it appends an empty json object at the start:\n```\n`[{},{\"a\":\"b\",\"c\":\"d\"},{\"e\":\"f\"},{\"k\":\"l\"}]\n`\n```\nHow to get rid of that leading empty json object?\n```\n`CREATE OR REPLACE FUNCTION myWrapperFunction()\n    RETURNS SETOF json \n    LANGUAGE 'plpgsql'\n\n    COST 100\n    STABLE STRICT \n    ROWS 1000\nAS $BODY$\nDECLARE\n    _elements INTEGER[];\n    _element INTEGER;\n    _results json;\n    _result json;\n    _all_result jsonb;\n    val json ='{}'::json;\n\nBEGIN\n  SELECT  ARRAY_AGG( DISTINCT column1) into _elements from table1;\n  FOREACH _element IN ARRAY _elements \n  LOOP\n        SELECT * FROM  myFunction(_element) into _results; \n    IF _results IS NOT null THEN\n        val=val::jsonb||_results::jsonb;\n    END IF;\n  END LOOP;\n  RETURN QUERY select val;\n  RETURN  ;\nEND; $BODY$;\n`\n```",
      "solution": "The initial value for `val` is `val json ='{}'::jsonb;` which is later concatenated inside the loop:\n```\n`val=val::jsonb||_results::jsonb;\n`\n```\nFor sample:\n```\n`SELECT '{}'::JSONB || '[{\"a\":\"b\"}]'::JSONB;\n-- [{}, {\"a\": \"b\"}]\n--  ^^\n`\n```\nIt could be replaced with `val json ='[]'::jsonb;`:\n```\n`SELECT '[]'::JSONB || '[{\"a\":\"b\"}]'::JSONB\n-- [{\"a\": \"b\"}]\n`\n```\ndb<>fiddle demo",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2023-02-15T15:44:24",
      "url": "https://stackoverflow.com/questions/75461465/how-to-iterate-and-append-jsonb-in-postgres-db-function"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "plpgsql",
      "question_id": 74741086,
      "title": "Access Record Column Stored in a Variable in PostgreSQL",
      "problem": "I have been looking for a way to access columns returned by a SQL select query in PostgreSQL dynamically. For e.g. see the below sample function:\n```\n`CREATE FUNCTION my_func(col text) RETURNS integer LANGUAGE plpgsql\n$$\nDECLARE\nval text;\nrec record;\nBEGIN\nFOR rec IN\n  EXECUTE 'SELECT ' || col || ' FROM employee_tb WHERE sal > 10000'\nLOOP\n-- trying this but failing\n    val = rec[col];\n-- this is also failing\n    val = rec.col;\n-- What is the syntax for doing this. There should be one I think.\nEND LOOP;\nRETURN 1;\n$$\n\n-- Call function\nselect my_func('emp_code');\n`\n```\nThis is very much possible in any other language. I have been looking for a solution for quite sometime now but still not able to find one.\nThanks in advance.",
      "solution": "Usually, this necessity is signal of overusing of dynamic SQL. On second hand, this necessity can be real. There are more possibilities. The most simply possibility is using of column label.\n```\n`do $$\ndeclare r record;\nbegin\n  execute 'select 1 as x' into r;\n  raise notice '%', r.x;\nend;\n\n$$;\nNOTICE:  1\nDO\n`\n```\nSecond possibility is transformation to jsonb and using `[]` selector syntax:\n```\n`do $$\ndeclare r record; j jsonb;\nbegin\n  execute 'select 1' into r; j := to_jsonb(r);\n  raise notice '%', j['?column?'];\nend;\n\n$$;\nNOTICE:  1\nDO\n`\n```\nAttention - selector returns jsonb value - sometimes is necessary to extract text or bool value from jsonb value.\nPLpgSQL is very static language. The programming style should be more like C than like Python or Perl. Dynamic SQL should not be overused.",
      "question_score": 1,
      "answer_score": 3,
      "created_at": "2022-12-09T10:23:27",
      "url": "https://stackoverflow.com/questions/74741086/access-record-column-stored-in-a-variable-in-postgresql"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "plpgsql",
      "question_id": 70596896,
      "title": "Postgis: create column with srid in procedure",
      "problem": "Hello,\nI wonder how I could create a spatialized column with an srid retrieved from the db.\n`-- OK --\nALTER TABLE POI ADD COORDS GEOMETRY(POINT, 26916);\n\n-- KO (invalid input syntax for type integer: \"sridval\") --\nDO $$\nDECLARE\n    sridval int;\nBEGIN\n    sridval := (select srid FROM project_options);\n    ALTER TABLE POI ADD COORDS GEOMETRY(POINT, sridval);\nEND$$;\n\n-- OK but verbose --\nDO $$\nDECLARE\n    sridval int;\nBEGIN\n    ALTER TABLE POI ADD COORDS GEOMETRY(POINT);\n    sridval := (select srid FROM project_options);\n    PERFORM updategeometrysrid('POI', 'coords', sridval);\nEND $$;\n`\nLast solution doesn't work with generated columns. Ex:\n`ALTER TABLE POI ADD COORDS GEOMETRY(POINT, /*put srid here?*/) generated always as (ST_MakePoint(longitude, latitude)) stored;\nCREATE INDEX COORDS_IDX ON POI USING GIST (COORDS);\n`",
      "solution": "You can use `format()` to create your DDL dynamically.\n```\n`DO $$\nDECLARE\n    sridval int;\nBEGIN\n sridval := (SELECT srid FROM project_options); \n EXECUTE FORMAT('ALTER TABLE poi ADD COLUMN coords geometry(point, %s)',sridval);\nEND $$;\n`\n```\nYou can also skip the variable declaration by passing the query itself as parameter:\n```\n`DO $$\nBEGIN\n  EXECUTE FORMAT('ALTER TABLE poi ADD coords geometry(point, %s)',\n                 (SELECT srid FROM project_options));\nEND $$;\n`\n```\nAnd\n```\n`DO $$\nBEGIN\n EXECUTE FORMAT('ALTER TABLE poi ADD coords geometry(point, %s) \n                 GENERATED ALWAYS AS (ST_MakePoint(longitude, latitude)) STORED;',\n                (SELECT srid FROM project_options));\nEND $$;\n`\n```\n\nThis code assumes that `project_options` has a single row\n\nDemo: `db<>fiddle`",
      "question_score": 1,
      "answer_score": 3,
      "created_at": "2022-01-05T18:12:46",
      "url": "https://stackoverflow.com/questions/70596896/postgis-create-column-with-srid-in-procedure"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "plpgsql",
      "question_id": 68085541,
      "title": "Multiply rows by column value",
      "problem": "I need to transform data from my source-table into a destination-table.\nThe source table has 'content'-columns and 'multiplier'-columns. Based on the multiplier(X), the content of the source should be written X-times into the destination.\neg: if multilpier is '0', nothing will be written, if '1', content is written one time into destination table. Two times, if the multiplier is '2', and so on. I've never done functions in Postgres before.\nMy approach: a nested for-while-loop: for each row, while 'counter' is smaller than 'multiplier', insert content from source-table into destination table.\nExample data:\n```\n`--create source table\ncreate table public.source_tbl(\nid serial, \nmultiplier int, \ncontent varchar,\nprimary key (id)\n);\n--create destination table\ncreate table public.dest_tbl(\nid serial, \nmultiplier int, \ncontent varchar,\nprimary key (id)\n);\n--some content\ninsert into public.source_tbl(multiplier,content)\nvalues(1,'foo'),(1,'bar'),(1,'random'),(2, 'content'),(3,'My'),(4,'creativity'),(3,'is'),(2,'very'),(6,'limited'),(7,'!!!'), (0, 'nothing should be written');\n`\n```\nAnd thats the code I came up with:\n```\n`do\n$$\ndeclare f record;\nbegin \n    for f in    select id, multiplier, content\n                from public.source_tbl;\n    loop\n        do\n        $$\n        declare counter integer counter:=0;\n        begin\n            while counter Needless to say that it does not work, I get an syntax-error with the second 'declare'. So what am I doing wrong?",
      "solution": "You cannot declare a variable in the middle of a `plpgsql` code. It is also not necessary to create another anonymous code block for the second loop. Try this:\n```\n`do\n$$ \ndeclare \n f record;\n counter integer :=0;\nbegin \n    for f in select id, multiplier, content from public.source_tbl  loop\n      while counter Demo: `db<>fiddle`",
      "question_score": 1,
      "answer_score": 1,
      "created_at": "2021-06-22T16:35:30",
      "url": "https://stackoverflow.com/questions/68085541/multiply-rows-by-column-value"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "plpgsql",
      "question_id": 65542057,
      "title": "Postgresql trigger at new row",
      "problem": "I created a trigger\n```\n`CREATE TRIGGER trigger_name\nAFTER INSERT OR UPDATE\nON schema.table\nFOR EACH STATEMENT\nEXECUTE PROCEDURE audit_trigger();\n`\n```\nThat seems to work. Then I have the function:\n```\n`CREATE OR REPLACE FUNCTION audit_trigger()\nRETURNS TRIGGER\nLANGUAGE 'plpgsql'\nAS $$\nBEGIN\n\nUPDATE schema.table SET NEW.changed_at = current_date, NEW.changed_by = current_user ;\n\nEND;\n$$;\n`\n```\nWhen I do an INSERT or an UPDATE at this table, it should automatically insert the time and name of user. The creation of the trigger and the function work properly (without error or warning). But when I do an INSERT at the table, I get this:\n```\n`ERROR: FEHLER:  Spalte \u00bbnew\u00ab von Relation \u00bbtable\u00ab existiert nicht\nLINE 1: UPDATE flug.flugplan SET NEW.changed_at = current_date, NEW....\n                                 ^\nQUERY:  UPDATE schema.tableSET NEW.changed_at = current_date, NEW.changed_by = current_user\nCONTEXT:  PL/pgSQL-Funktion audit_trigger() Zeile 6 bei SQL-Anweisung\n`\n```\nFor the english speakers: It just says that the column >>new",
      "solution": "To update fields in the inserted/updated row you need to do something like this:\n```\n`NEW.changed_at := current_date;   \nNEW.changed_by := current_user;\nreturn NEW;\n`\n```\nYour code will probably not work as expected as the trigger is defined as AFTER trigger and changes done to NEW are not stored.  You need to change it to BEFORE.\nAudit triggers are indeed often after triggers as they are made to record changes done to table rows but in that case they are not used to alter the data in the table being audited.",
      "question_score": 1,
      "answer_score": 3,
      "created_at": "2021-01-02T18:31:41",
      "url": "https://stackoverflow.com/questions/65542057/postgresql-trigger-at-new-row"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "plpgsql",
      "question_id": 79598382,
      "title": "Why is this PostgreSQL function throwing an exception, and how can I tell which input is?",
      "problem": "I have a table which stores attribute value data, the majority of which are numbers but occasionally strings. When the values are numbers, we are interested in a delta. The following PostgreSQL function attempts to capture that\n`CREATE OR REPLACE FUNCTION public.safe_percent_delta(new text, old text)\n RETURNS real\n LANGUAGE plpgsql\n IMMUTABLE\nAS $function$\nbegin \n    if (new :: real = 0 and old :: real <> 0) or (old :: real = 0 and new :: real <> 0) then \n        return null;\n    else\n        return 100 * (new :: real - old :: real) / old :: real;\n    end if;\nexception\n    when others then \n        return null;\nend;\n$function$\n`\nIn the table, we store that as a generated column. However, on a recent insert of new data, I'm getting an exception.\n```\n`SQL Error [22003]: ERROR: value out of range: overflow\n  Where: PL/pgSQL function safe_percent_delta(text,text) while casting return value to function's return type\n`\n```\nI'm trying to insert ~4 million rows, we currently have over 4 billion stored, and this is the first time I've seen this happen. The way we typically insert the data is with a `copy` into a temporary table with just the data we're inserting because it's much faster, then we do the actual insert which relies on some subquerying and also implicitly calls this function for the generated column. When debugging this, I made the temporary table persistent and tried just calling the function after the fact with `select safe_percent_delta(v_new, v_old) from temp_table` and got the same result.\nWhat's going on here? Is there a way to find out which input is causing the issue? Why is my `exception when others` not catching this?",
      "solution": "Your expected output is a real. Casting the outcome of your calculation to this real is done after the code block has finished, which is why your error handler doesn't catch the error.\nYou might be better off using a double precision that can handle a broader range of values. You could/should also validate the content before casting the text to a numeric value. Something like this:\n```\n`CREATE OR REPLACE FUNCTION public.safe_percent_delta(new_text text, old_text text)\nRETURNS double precision\nLANGUAGE plpgsql\nIMMUTABLE\nAS $$\nDECLARE\n    new_val double precision;\n    old_val double precision;\nBEGIN\n    -- Validate content first:\n    IF NOT pg_input_is_valid(new_text, 'double precision') \n       OR NOT pg_input_is_valid(old_text, 'double precision') THEN\n        RETURN NULL;\n    END IF;\n\n    new_val := CAST(new_text AS double precision);\n    old_val := CAST(old_text AS double precision);\n\n    -- Return NULL if exactly one is zero\n    IF (new_val = 0) <> (old_val = 0) THEN\n        RETURN NULL;\n    END IF;\n\n    RETURN 100 * (new_val - old_val) / NULLIF(old_val, 0);\nEND;\n$$;\n`\n```\n\nEdit: If you want to keep the output as real, you can validate the result from the calculation and return NULL when it's not valid:\n```\n`CREATE OR REPLACE FUNCTION public.safe_percent_delta(new_text text, old_text text)\nRETURNS real\nLANGUAGE plpgsql\nIMMUTABLE\nAS $$\nDECLARE\n    new_val double precision;\n    old_val double precision;\n    output double precision;\nBEGIN\n    -- Validate content first:\n    IF NOT pg_input_is_valid(new_text, 'double precision')\n       OR NOT pg_input_is_valid(old_text, 'double precision') THEN\n        RETURN NULL;\n    END IF;\n\n    new_val := CAST(new_text AS double precision);\n    old_val := CAST(old_text AS double precision);\n\n    -- Return NULL if exactly one is zero\n    IF (new_val = 0) <> (old_val = 0) THEN\n        RETURN NULL;\n    END IF;\n\n    output := 100 * (new_val - old_val) / NULLIF(old_val, 0);\n\n    IF pg_input_is_valid(CAST(output AS text), 'real') THEN\n        RETURN output;\n    ELSE\n        RETURN NULL;\n    END IF;\nEND;\n$$;\n`\n```\nThis avoids the need for a costly exception handler.",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2025-04-29T14:33:38",
      "url": "https://stackoverflow.com/questions/79598382/why-is-this-postgresql-function-throwing-an-exception-and-how-can-i-tell-which"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "plpgsql",
      "question_id": 78992204,
      "title": "Retaining and inserting state into a postgres aggregation function",
      "problem": "I have a case where I have a table \"tab\" that stores data (keyed by symbol and time) as well as a second table \"summ\" that stores the exponential weighted running average of the data in the first table. I have it set up so that when data is added to the first table it triggers the calculation of the running average of the corresponding rows in the second table.\nI can't figure out a good way to recover the state however for continuing on the running average. Instead it starts anew each time the trigger is called. How do I save the state of the aggregation at the end of\nI have a minimal example at https://dbfiddle.uk/6dqXCwIQ  which is also reproduced below.\nFirst making a table that will contain the unsmoothed data as well as a summary table that will hold the smoothed data. There is also a trigger to populate the second table given the first.\n```\n`/* Making a table that will contain the unsmoothed data */\ncreate table tab (\n  symbol text,\n  time_to timestamptz,\n  something int4,\n  PRIMARY KEY(time_to, symbol)\n);\nCREATE UNIQUE INDEX index_name2 ON tab USING btree (symbol, time_to);\n\n/* Making a summarisation table too and a trigger to populate it when something is inserted into tab. */\nCREATE OR REPLACE FUNCTION smoother_state(state double precision[],\n                                          newval double precision, frac double precision)\n RETURNS double precision[]\n LANGUAGE plpgsql\n IMMUTABLE PARALLEL SAFE LEAKPROOF\nAS $function$\n  declare\n      resul double precision := case when state[1] is null then newval else state[1] * (1-frac) + newval * frac end;\n  begin\n    return ARRAY[resul, coalesce(state[2] + 1, 1)];\n  END;\n $function$\n;\nCREATE OR REPLACE AGGREGATE smoother(val double precision, frac double precision) (\n    SFUNC = smoother_state,\n    STYPE = double precision[2]\n);\n\ncreate table summ (\n  symbol text,\n  time_to timestamptz,\n  smoothed_something double precision,\n  number_of_periods double precision,\n  PRIMARY KEY(time_to, symbol)\n);\nCREATE UNIQUE INDEX index_name3 ON summ USING btree (symbol, time_to);\n\n/* Making a trigger.\nI guess it should read the smoothed_something and number_of_periods values for the symbol\nand use them as an initial state vector.\n*/\nCREATE OR REPLACE FUNCTION do_update()\n RETURNS trigger\n LANGUAGE plpgsql\n PARALLEL SAFE STRICT LEAKPROOF\nAS $function$\n  DECLARE\n    BEGIN\nwith a as (select symbol, time_to, something,\n  smoother(something, 0.3) over (partition by symbol order by time_to) as smoo\nFROM newtab\n  ), b as (select symbol, time_to, smoo[1] as smoothed_something, smoo[2] as number_of_periods from a)\nINSERT INTO summ (symbol, time_to, smoothed_something, number_of_periods) select * from b;\n        RETURN null;\n    END;\n$function$\n;\n\ncreate trigger update_smoothed after\ninsert on tab\nreferencing new table as newtab\nfor each statement\nexecute function do_update()\n`\n```\nInserting data the first time both tab and summ look correct.\n```\n`insert into tab (symbol, time_to, something) values\n  ('a', '2022-01-01 00:00:15+01:00'::timestamptz, 15),\n  ('b', '2021-01-01 00:00:15+01:00'::timestamptz, 18),\n  ('b', '2022-01-01 00:00:15+01:00'::timestamptz, 13),\n  ('b', '2023-01-01 00:00:15+01:00'::timestamptz, 11),\n  ('b', '2024-01-01 00:00:15+01:00'::timestamptz, 3),\n  ('c', '2022-01-01 00:00:16+01:00'::timestamptz, 15),\n  ('c', '2022-01-01 00:00:17+01:00'::timestamptz, 150);\n`\n```\nInserting data the second time tab looks correct but summ is not as the aggregation starts from a null state.\n```\n`insert into tab (symbol, time_to, something) values\n  ('a', '2022-06-01 00:00:15+01:00'::timestamptz, 150),\n  ('a', '2022-07-01 01:00:15+01:00'::timestamptz, 170),\n  ('b', '2024-08-01 00:00:15+01:00'::timestamptz, 180),\n  ('b', '2024-09-01 00:00:15+01:00'::timestamptz, 130);\n`\n```\nYou can see the resultant data in each table below and also in the fiddle site.\n\nFor this problem:\n\nIs there a way to modify the trigger to input the correct state when running the aggregation the second time? I am happy to impose that we only ever add data that happens  later in time than earlier observations.\nIs there a better way to accomplish this? I have a lot of data so want to avoid having to do a materialised view (as it takes too long to refresh the whole thing).",
      "solution": "An alternative, closer to the idea from the title: the function can accept an `init_state` parameter:\ndemo at db<>fiddle\n`CREATE OR REPLACE FUNCTION smoother_state\n   (state double precision[],\n    newval double precision, \n    frac double precision,\n    init_state double precision[] default null::float[] )\n RETURNS double precision[]\n LANGUAGE sql\n IMMUTABLE PARALLEL SAFE LEAKPROOF\n RETURN ARRAY[ coalesce(state[1] * (1-frac) + newval * frac, \n                        init_state[1] * (1-frac) + newval * frac, \n                        newval)\n              ,coalesce(state[2] + 1, \n                        init_state[2] + 1,\n                        1)];\nCREATE OR REPLACE AGGREGATE smoother(val double precision, \n                                     frac double precision, \n                                     init_state double precision[]) (\n    SFUNC = smoother_state,\n    STYPE = double precision[2]\n);\n`\nYou still need to look up the init state and inject it:\n`CREATE OR REPLACE FUNCTION do_update()\n RETURNS trigger PARALLEL SAFE STRICT LEAKPROOF\nAS $function$ BEGIN\nWITH a AS (\n  SELECT symbol, \n         time_to, \n         smoother(something, 0.3, init_state)OVER w AS smoo\n  FROM newtab\n  LEFT JOIN(SELECT DISTINCT ON(symbol)\n                   symbol,\n                   array[smoothed_something,number_of_periods] AS init_state\n            FROM summ\n            ORDER BY symbol, time_to DESC) AS init_states\n  USING(symbol)\n  WINDOW w AS(PARTITION BY symbol ORDER BY time_to)\n)\nINSERT INTO summ (symbol, time_to, smoothed_something, number_of_periods) \nSELECT symbol, \n       time_to, \n       smoo[1] as smoothed_something, \n       smoo[2] as number_of_periods \nFROM a;\nRETURN null; END $function$ LANGUAGE plpgsql;\n`\n\nsymbol\ntime_to\nsmoothed_something\nnumber_of_periods\n\na\n2021-12-31 23:00:15+00\n15\n1\n\nb\n2020-12-31 23:00:15+00\n18\n1\n\nb\n2021-12-31 23:00:15+00\n16.5\n2\n\nb\n2022-12-31 23:00:15+00\n14.849999999999998\n3\n\nb\n2023-12-31 23:00:15+00\n11.294999999999998\n4\n\nc\n2021-12-31 23:00:16+00\n15\n1\n\nc\n2021-12-31 23:00:17+00\n55.5\n2\n\na\n2022-06-01 00:00:15+01\n55.5\n2\n\na\n2022-07-01 01:00:15+01\n89.85\n3\n\nb\n2024-08-01 00:00:15+01\n61.9065\n5\n\nb\n2024-09-01 00:00:15+01\n82.33455000000001\n6",
      "question_score": 1,
      "answer_score": 1,
      "created_at": "2024-09-17T02:40:49",
      "url": "https://stackoverflow.com/questions/78992204/retaining-and-inserting-state-into-a-postgres-aggregation-function"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "plpgsql",
      "question_id": 78527944,
      "title": "Stored procedure with inserts to two related tables failing on foreign key constraint",
      "problem": "I have a stored procedure which attempts to write to two tables, related via a foreign key constraint.\nSlightly simplified table definitions:\n`CREATE TABLE station_event\n(\n    station_code   VARCHAR(3)   NOT NULL,\n    user_id        INTEGER      NOT NULL,\n    event_dtm      TIMESTAMPTZ  NOT NULL DEFAULT now(),\n\n    CONSTRAINT station_event_pk\n        PRIMARY KEY (station_code, user_id, event_dtm)\n);\n\nCREATE TABLE location_station_event\n(\n    station_code   VARCHAR(3)      NOT NULL,\n    user_id        INTEGER         NOT NULL,\n    event_dtm      TIMESTAMPTZ(0)  NOT NULL DEFAULT now(),\n    location_code  VARCHAR(8)      NOT NULL,\n    location_no    INTEGER         NOT NULL,\n\n    CONSTRAINT location_station_event_pk\n        PRIMARY KEY (station_code, user_id, event_dtm),\n        \n    CONSTRAINT location_station_event_station_event_fk\n        FOREIGN KEY (station_code, user_id, event_dtm)\n        REFERENCES station_event (station_code, user_id, event_dtm)\n);\n`\nSlightly simplified stored procedure definition:\n`CREATE FUNCTION location_station_apply (\n    p_site_code         VARCHAR,\n    p_location_no       INTEGER,\n    p_station_code      VARCHAR\n    )\nRETURNS VOID AS $$\nBEGIN\n    INSERT INTO station_event (\n                station_code,\n                user_id\n        )\n        VALUES (\n                p_station_code,\n                user_id()\n        );\n\n    INSERT INTO location_station_event (\n                station_code,\n                user_id,\n                location_no\n        )\n        VALUES (\n                p_station_code,\n                user_id(),\n                p_location_no\n        );\nEND;\n$$ LANGUAGE plpgsql SECURITY DEFINER;\n`\nThe error I receive is:\n\nERROR:  insert or update on table \"location_station_event\" violates\nforeign key constraint \"location_station_event_station_event_fk\"\nDETAIL:  Key (station_code, user_id, event_dtm)=(CE, 1, 2024-05-24\n10:21:56+01) is not present in table \"station_event\".\n\nThe function is unchanged between DEV and PROD environments, although DEV is running PostgreSQL v14 and PROD is still running v11. It is failing, as above, on DEV but running successfully on PROD.\nIs there something I am missing which might be PostgreSQL version specific? Maybe there's a new config parameter (introduced after v11)?",
      "solution": "Made minor changers in your table definitions, and function. Find below code:\n\nTable Definition:\n\n```\n`CREATE TABLE station_event (\nstation_code   VARCHAR(3)   NOT NULL,\nuser_id        INTEGER      NOT NULL,\nevent_dtm      TIMESTAMPTZ  NOT NULL DEFAULT now(),\n\nCONSTRAINT station_event_pk\n    PRIMARY KEY (station_code, user_id, event_dtm) );\n\nCREATE TABLE location_station_event\n(\n    station_code   VARCHAR(3)      NOT NULL,\n    user_id        INTEGER         NOT NULL,\n    event_dtm      TIMESTAMPTZ  NOT NULL,\n    location_code  VARCHAR(8)      NOT NULL,\n    location_no    INTEGER         NOT NULL,\n\n    CONSTRAINT location_station_event_pk\n        PRIMARY KEY (station_code, user_id, event_dtm),\n        \n    CONSTRAINT location_station_event_station_event_fk\n        FOREIGN KEY (station_code, user_id, event_dtm)\n        REFERENCES station_event (station_code, user_id, event_dtm)\n        \n);\n`\n```\nMade event_dtm consistent in both tables, but changed its default value in location_station_event table\nAs this was causing to round up your micro-seconds in location_station_event\nand values were not matching, and was violating foreign key constraint.\nSecondly made a minor change in your function to make it fool proof, added new variable which gets event_dtm value on inserting in station_event and insert that exact value in location_station_event\n```\n`CREATE OR REPLACE FUNCTION location_station_apply (\n    p_site_code         VARCHAR,\n    p_location_no       INTEGER,\n    p_station_code      VARCHAR\n)\nRETURNS VOID AS\n$$\nDECLARE\n    v_event_dtm TIMESTAMPTZ;\nBEGIN\n    -- Insert into station_event and capture event_dtm\n    INSERT INTO station_event (\n        station_code,\n        user_id\n    )\n    VALUES (\n        p_station_code,\n        user_id()\n    ) RETURNING event_dtm INTO v_event_dtm;\n\n    -- Insert into location_station_event\n    INSERT INTO location_station_event (\n        station_code,\n        user_id,\n        event_dtm,\n        location_code,\n        location_no\n    )\n    VALUES (\n        p_station_code,\n        user_id(),\n        v_event_dtm,\n        p_site_code,\n        p_location_no\n    );\n\nEND;\n$$\nLANGUAGE plpgsql SECURITY DEFINER;\n`\n```",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2024-05-24T12:24:11",
      "url": "https://stackoverflow.com/questions/78527944/stored-procedure-with-inserts-to-two-related-tables-failing-on-foreign-key-const"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "plpgsql",
      "question_id": 78328524,
      "title": "Declaring a cursor in plpgsql fails",
      "problem": "I wrote a db function for plpgsql (postgre) in supabase.\nWhen declaring a cursor I always get `ERROR: 42P01: relation \"cursor1\" does not exist`.\nThis is how I declare it:\n```\n`DECLARE\n    cursor1 CURSOR FOR \n        SELECT public.spending_cap.cap, public.spending_cap.spent, public.spending_cap.image_cap, public.spending_cap.image_spent, public.spending_cap.id, public.spending_cap.created_by FROM public.spending_cap, public.user_profile WHERE is_parent = true AND public.user_profile.id = public.spending_cap.created_by AND organisation is NULL;\n    entry cursor1%rowtype;\n`\n```\nI am just declaring `cursor1` how can it say that the relation does not exist? If I just execute the `SELECT` statement, I get results. I use this declared cursor to loop over the same `SELECT` statement.\nAny hints on what I am doing wrong?",
      "solution": "PL/pgSQL doesn't allow `%ROWTYPE` on cursor variables. The engine of PL/pgSQL is very different from Oracle and it is just out of concept of PL/pgSQL. Probably should not be hard to implement this feature, but it is out of concept. You should to use `RECORD` type instead:\n```\n`DECLARE\n    cursor1 CURSOR FOR \n        SELECT public.spending_cap.cap, public.spending_cap.spent, public.spending_cap.image_cap, public.spending_cap.image_spent, public.spending_cap.id, public.spending_cap.created_by FROM public.spending_cap, public.user_profile WHERE is_parent = true AND public.user_profile.id = public.spending_cap.created_by AND organisation is NULL;\n    entry record;\n`\n```\nTry to read related documentation. Although the syntax is similar sometimes, in details there are lot of differences between  PL/pgSQL cursors and PL/SQL cursors.",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2024-04-15T14:52:29",
      "url": "https://stackoverflow.com/questions/78328524/declaring-a-cursor-in-plpgsql-fails"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "plpgsql",
      "question_id": 78289021,
      "title": "How to query a table whose name is built from a PL/pgSQL function result?",
      "problem": "I've below function that takes a datetime and returns an integer.\n`CREATE OR REPLACE FUNCTION public.get_partition_index(datetime timestamp with time zone)\n RETURNS integer\n LANGUAGE plpgsql\n IMMUTABLE\nAS $function$\nDECLARE\n    partitions integer := 52;\n    datetime_utc timestamp := datetime AT TIME ZONE 'UTC';\n    week_of_year integer := DATE_PART('week', datetime_utc);\n    partition_index integer := week_of_year % partitions;\nBEGIN\n    RETURN partition_index;\nEND;\n$function$\n`\nThere are 52 tables in the database with names `metric_event_1`, `metric_event_2`, ..., `metric_event_10`, ... and `metric_event_51`.\nNow I want query the table based on the result of the `get_partition_index` function.\n`select * from `metric_event_+billing_event_partition_index_new(now())` limit 10;\n`\nBut the above query returns error:\n\n```\n`ERROR:  syntax error at or near \"`\"\nLINE 1: select * from `billing_events_+billing_event_partition_index...\n`\n```",
      "solution": "Better function (still don't use!)\nA simpler SQL version of your function could work like this:\n`CREATE OR REPLACE FUNCTION public.get_partition_index(datetime timestamptz)\n  RETURNS numeric\n  LANGUAGE sql IMMUTABLE PARALLEL SAFE STRICT AS\n$func$\nSELECT EXTRACT(week FROM $1 AT TIME ZONE 'UTC') % 52;\n$func$;\n`\nThat fixes the syntax and improves technicalities. The whole idea is still nonsense on multiple levels.\nNonsense levels\nFor starters, `names metric_event_1` ... `metric_event_51`, that's 51 tables, not 52 like you state. And there are actually 53 ISO weeks. `EXTRACT` returns week numbers from 1 to 53 accordingly. Your `% 52` would make a mess including a week \"0\" ...\nYour try with backticks raised a syntax error as backticks are not used for quoting in Postgres (or standard SQL) at all. See:\n\nInsert text with single quotes in PostgreSQL\n\nBut the attempt is nonsense on a deeper level. You cannot parameterize / interpolate identifiers (incl. table names) in plain SQL. You could concatenate the query string in your client, or use dynamic SQL:\n`CREATE OR REPLACE FUNCTION public.get_rows_from_metric_event(_tstz timestamptz, _limit int = 10)\n  RETURNS SETOF metric_event  -- actual table name!\n  LANGUAGE plpgsql IMMUTABLE PARALLEL SAFE STRICT AS\n$func$\nBEGIN\n-- RAISE NOTICE '%',  -- to debug\n   RETURN QUERY EXECUTE\n   format('SELECT * FROM metric_event_%s LIMIT %s', EXTRACT(week FROM _tstz AT TIME ZONE 'UTC') % 52, _limit);\nEND\n$func$;\n`\nCall:\n```\n`SELECT * FROM public.get_rows_from_metric_event(now());\n`\n```\nRelated:\n\nTable name as a PostgreSQL function parameter\n\nTypically still not worth it.\nWhat you should really do\nIf `metric_event` is a proper partitioned table - and you have `enable_partition_pruning` on (as is default) - simply query the parent table with a filter matching partitioning boundaries exactly:\n`SELECT *\nFROM   metric_event\nWHERE  weeknr = EXTRACT(week FROM timestamptz '2024-04-07 00:00+2' AT TIME ZONE 'UTC')::int  -- cast!\nLIMIT  10;\n`\nAssuming this basic table definition:\n`CREATE TABLE metric_event (weeknr int, data text) PARTITION BY LIST (weeknr);\n\nCREATE TABLE metric_event_1 PARTITION OF metric_event FOR VALUES IN (1);\n...\nCREATE TABLE metric_event_53 PARTITION OF metric_event FOR VALUES IN (53);\n`\nPartition pruning does the job. You get a query plan like:\n`Limit  (cost=0.00..25.88 rows=6 width=36)\n\u2007\u2007->  Seq Scan on metric_event_14 metric_event  (cost=0.00..25.88 rows=6 width=36)\n\u2007\u2007\u2007\u2007\u2007\u2007\u2007\u2007Filter: (weeknr = 14)\n`\nPartition bounds must be matched exactly - including the data type!\nNote that `date_part()` returns `double precision` and `EXTRACT` returns `numeric`! In my example, `weeknr` is `integer`, so cast to `integer`! See:\nfiddle\nAbout `EXTRACT` / `date_part()`\n`EXTRACT` and `date_part()` are basically equivalent. (`EXTRACT` in upper-case because it's really a syntax element rather than a plain function.) If in doubt, use `EXTRACT`. The manual:\n\nThe `date_part` function is modeled on the traditional Ingres\nequivalent to the SQL-standard function `extract`:\n[...]\nFor\nhistorical reasons, the `date_part` function returns values of type\ndouble precision. This can result in a loss of precision in certain\nuses. Using `extract` is recommended instead.\n\nBold emphasis mine.",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2024-04-07T21:20:42",
      "url": "https://stackoverflow.com/questions/78289021/how-to-query-a-table-whose-name-is-built-from-a-pl-pgsql-function-result"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "plpgsql",
      "question_id": 78269696,
      "title": "Validate array of table names",
      "problem": "I have requirement where bash script will pass on the string consisting tables names to anonymous PL/pgSQL block which will process the tables if they exists in the Postgres db.\nBelow is the PL/pgSQL code assuming given_tables variable has the string consisting table names provided through bash script.\n```\n`do $do$\n>\ndeclare\n    tab text;\n    given_tables text[] := ARRAY['ocab1.cust_docu_map','ocsbc2.cust_docu_map'];\n    table_names  text[] := ARRAY(\n                              BEGIN\n                                FOREACH tab IN ARRAY given_tables loop\n                                    IF EXISTS\n                                        ( SELECT 1\n                                          FROM pg_class\n                                          WHERE oid = tab::regclass\n                                        )\n                                    THEN\n                                          SELECT concat_ws('.',relnamespace::regnamespace::text,relname)\n                                          FROM pg_class\n                                          WHERE oid = tab::regclass\n                                          AND relreplident != 'f';\n                                    END IF;\n                                END LOOP;\n                              END\n                            );\nbegin\n  foreach table in array table_names loop\n    raise notice 'here is the table - %',outerblock.table;  \n  end loop;\nend $do$;\n`\n```\nBut code errors out:\n\n```\n`ERROR:  mismatched parentheses at or near \";\"\nLINE 18:             AND relreplident != 'f';\n                                            ^\n`\n```\n\nNot sure where I am going wrong?",
      "solution": "`DO\n$do$\nDECLARE\n   _given_tbls text[] := '{ocab1.cust_docu_map,ocsbc2.cust_docu_map}';\n   _tbl text;\nBEGIN\n   FOR _tbl IN\n      SELECT format('%I.%I', n.nspname, c.relname)         -- properly quoted\n      FROM   unnest(_given_tbls) t(tbl)\n      JOIN   pg_class     c ON c.oid = to_regclass(t.tbl)  -- does not raise exception\n      JOIN   pg_namespace n ON n.oid = c.relnamespace\n      WHERE  c.relreplident != 'f'\n   LOOP\n      RAISE NOTICE 'valid table: %', _tbl;  \n      -- or do something here!\n   END LOOP;\nEND\n$do$;\n`\n`to_regclass(_tab)` does not raise an exception for invalid table names. See:\n\nHow to check if a table exists in a given schema\n\nRemember that identifiers stored in catalog tables may need double-quoting. Quote with `format()` or `quote_ident()` to defend against syntax errors or even SQL injection attacks.\n`regclass` values are quoted automatically when converted to text, but only schema-qualified when necessary with the current `search_path`. You seem to want fully qualified names. See:\n\nTable name as a PostgreSQL function parameter\n\nAlso, it's much cheaper to run a single query and loop through results (if you really need to loop at all?), than to run another query (or even two!) for every element in the array.",
      "question_score": 1,
      "answer_score": 1,
      "created_at": "2024-04-03T20:11:21",
      "url": "https://stackoverflow.com/questions/78269696/validate-array-of-table-names"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "plpgsql",
      "question_id": 77941521,
      "title": "How to create an array from variables in postgresql?",
      "problem": "I want to create an array from two variables p, q.\nHere is my attempt:\n```\n`DO\n  $code$\n    DECLARE\n      p text := 'Hello';\n      q text := 'world!';\n      ta text[];             --text array\n    BEGIN\n      ta := {p, q};\n      RAISE INFO '% %', ta[1], ta[2];\n    END\n  $code$;\n\n`\n```\nbut it doesn't work\n```\n`postgres=# DO\npostgres-#   $code$\npostgres$#     DECLARE\npostgres$#       p text := 'Hello';\npostgres$#       q text := 'world!';\npostgres$#       ta text[];             --text array\npostgres$#     BEGIN\npostgres$#       ta := {p, q};\npostgres$#       RAISE INFO '% %', ta[1], ta[2];\npostgres$#     END\npostgres$#   $code$;\nERROR:  syntax error at or near \"{\"\nLINE 8:       ta := {p, q};\n                    ^\n`\n```\nBest I can get, that does work is this:\n```\n`DO\n  $code$\n    DECLARE\n      p text := 'Hello';\n      q text := 'world!';\n      ta text[];             --text array\n    BEGIN\n      RAISE INFO 'p=[%] q=[%]', p, q;\n      ta := array_append(array_append(ta, p), q);\n      RAISE INFO '% %', ta[1], ta[2];\n    END\n  $code$;\n`\n```\nHow should I do this in pl/pgsql?",
      "solution": "Arrays are documented on this page.\nFor an array of constants, your syntax would almost have worked except it must be written as a string: `'{p, q}'`. However in your case, `p` and `q` are variable and this expression is evaluated as the string `'p'` and the string `'q'` in an array.\nTo include variables and as long as `p` and `q` have the same type, which is the case in your question: `ARRAY[p, q]` (ARRAY constructor syntax) is how you create an array in postgres.IMHO, it is more convenient than the other syntax, overall.\nYour whole corrected code:\n`DO\n  $code$\n    DECLARE\n      p text := 'Hello';\n      q text := 'world!';\n      ta text[];             --text array\n    BEGIN\n      ta := ARRAY[p, q];\n      RAISE INFO '% %', ta[1], ta[2];\n    END\n  $code$;\n`",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2024-02-05T15:05:01",
      "url": "https://stackoverflow.com/questions/77941521/how-to-create-an-array-from-variables-in-postgresql"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "plpgsql",
      "question_id": 77888593,
      "title": "How to return a boolean after inserting within a postgres function?",
      "problem": "I have a table named `member`. I want to create a function that inserts a new email if it doesn't exist in the table, and then return a boolean.\n```\n`CREATE OR REPLACE FUNCTION\n  add_member(member_email TEXT)\nRETURNS BOOLEAN AS\n  $$\n    BEGIN\n        IF NOT EXISTS (SELECT email FROM \"community\".member WHERE email = $1) THEN\n            INSERT INTO \"community\".member (email) VALUES ($1) RETURNING TRUE;\n        ELSE\n            RETURN FALSE;\n        END IF;\n    END;\n  $$\nLANGUAGE PLPGSQL;\n\nSELECT add_member('cheese.head@green.org');\n`\n```\nI get this error `ERROR:  query has no destination for result data`.\nIs the `INSERT` not returning the expected data? Or wrapping it in another data type? Is this how you use the `INSERT` with the returning?",
      "solution": "Using `INSERT ... RETURNING ... INTO` from here:\nhttps://www.postgresql.org/docs/current/plpgsql-statements.html#PLPGSQL-STATEMENTS-SQL-ONEROW\n```\n`CREATE OR REPLACE FUNCTION\n  add_member(member_email TEXT)\nRETURNS BOOLEAN AS\n  $$\n    DECLARE\n        return_val BOOLEAN := FALSE;\n    BEGIN\n        IF NOT EXISTS (SELECT email FROM \"community\".member WHERE email = $1) THEN\n            INSERT INTO \"community\".member (email) VALUES ($1) RETURNING TRUE INTO return_val;\n       \n        END IF;\n    RETURN return_val;\n    END;\n  $$\nLANGUAGE PLPGSQL;\n`\n```\n`DECLARE` the `return_val` variable as `boolean` and with default value of `FALSE`. Then do test for existing value and if present do `INSERT` and update the `return_val` variable with the `TRUE` returning value from the insert. Then `RETURN return_val` which will be `FALSE` if an existing `member_email` is found and `TRUE` if one is not found.",
      "question_score": 1,
      "answer_score": 1,
      "created_at": "2024-01-26T20:28:07",
      "url": "https://stackoverflow.com/questions/77888593/how-to-return-a-boolean-after-inserting-within-a-postgres-function"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "plpgsql",
      "question_id": 77669004,
      "title": "PostgreSQL plpgsql function issues",
      "problem": "I have a function in my PostgreSQL 16.1 database running on Debian 12.2:\n```\n`CREATE OR REPLACE FUNCTION ref.lookup_xxx(\n    in_code character varying,\n    in_description character varying)\n    RETURNS integer\n    LANGUAGE 'plpgsql'\n    COST 100\n    VOLATILE PARALLEL UNSAFE\nAS $BODY$\n  declare id_val integer;\n  begin\n    if in_code is null then -- nothing to do\n      return null;\n    end if;\n    -- check if code is already present in the table:\n    id_val = (select min(id) from ref.xxx where code = in_code);\n    if id_val is null then -- insert new code, desc into reference table:\n      insert into ref.xxx (code, description) values (in_code, in_description) returning id_val;\n    end if;\n    return id_val; -- return id of new or existing row\n  exception\n    when others then\n      raise exception 'lookup_xxx error code, desc = %, %', in_code, in_description;\n  end; \n$BODY$;\n`\n```\nIt returns an error:\n```\n`ERROR:  lookup_xxx error code, desc = 966501, \nCONTEXT:  PL/pgSQL function ref.lookup_xxx(character varying,character varying) line 15 at RAISE \n\nSQL state: P0001\n`\n```\nIf I run the ad hoc query below, it succeeds:\n```\n`insert into ref.xxx (code, description) values ('966501', null);\n`\n```\nI can't get this ad hoc query to run - it may not be possible:\n```\n`do $$\ndeclare x integer;\nbegin\n  insert into ref.xxx (code, description) values ('966501', null) returning x;\n  raise notice 'x is %', x;\nend; \n$$\n`\n```\nI'm looking for any suggestions to correct the function - I've reviewed the postgres docs and can't find anything helpful.  Stepping through the function in the debugger shows it failing at the insert statement.    I've got similar queries in other plpgsql functions that are working correctly.",
      "solution": "Looks like you need this: RETURNING id INTO id_val;\n```\n`CREATE OR REPLACE FUNCTION ref.lookup_xxx(\n    in_code CHARACTER VARYING,\n    in_description CHARACTER VARYING)\n    RETURNS INTEGER\n    LANGUAGE 'plpgsql'\n    COST 10\n    VOLATILE PARALLEL UNSAFE\nAS\n$BODY$\nDECLARE\n    id_val INTEGER;\nBEGIN\n    IF in_code IS NULL THEN -- nothing to do\n        RETURN NULL;\n    END IF;\n    -- check if code is already present in the table:\n    id_val = (SELECT MIN(id) FROM ref.xxx WHERE code = in_code);\n    IF id_val IS NULL THEN -- insert new code, desc into reference table:\n        INSERT INTO ref.xxx (code, description) \n        VALUES (in_code, in_description) \n        RETURNING id \n            INTO id_val; -- <-- this one\n    END IF;\n    RETURN id_val; -- return id of new or existing row\nEXCEPTION -- Why? You will be hiding the real error\n    WHEN OTHERS THEN\n        RAISE EXCEPTION 'lookup_xxx error code, desc = %, %', in_code, in_description;\nEND;\n$BODY$;\n`\n```",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2023-12-15T22:58:20",
      "url": "https://stackoverflow.com/questions/77669004/postgresql-plpgsql-function-issues"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "plpgsql",
      "question_id": 76344627,
      "title": "CREATE FUNCTION statement in an IF block throws error, while running on its own works",
      "problem": "I'm new to PostgreSQL (currently on PostgreSQL 13) and may be confusing things from what other SQL encounters I've had (Microsoft SQL).\nThe goal is to assert there are no values in a table column which would get truncated, then reduce the column length and do the same for the return type of a related function\nAn example of the code giving the error, so it can be reproduced:\n```\n`/* \n CREATE TABLE test_table (id uuid, col_a varchar(100), col_b int); \n INSERT INTO test_table VALUES (gen_random_uuid(), 'asdf', 1);\n **/\n\nDO $$\nBEGIN\n    IF NOT EXISTS (SELECT * FROM test_table WHERE character_length(col_a) > 100) THEN\n        ALTER TABLE test_table ALTER COLUMN col_a TYPE varchar(100);\n        \n        DROP FUNCTION IF EXISTS test_function(varchar(100));\n    \n        CREATE OR REPLACE FUNCTION test_function(test_param varchar(100))\n        RETURNS TABLE (\n            id uuid,\n            col_a varchar(100),\n            col_b int\n        )\n        LANGUAGE plpgsql\n        AS $$\n        BEGIN\n            RETURN QUERY\n            SELECT test_table.id AS id, test_table.col_a AS col_a, test_table.col_b AS col_b\n            FROM test_table\n            WHERE test_table.col_a = test_param;\n        END;\n        $$;\n    ELSE\n        RAISE NOTICE 'Cannot Revert. Rows exist that would be truncated!';\n    END IF;\nEND $$\n\nSELECT id, col_a, col_b FROM test_function ('asdf');\n\n/* DROP TABLE test_table */\n`\n```\nThe error I get is:\n\nSQL Error [42601]: ERROR: syntax error at or near \"BEGIN\" Position: 400\n\nI have tried an attempt at dynamic code, I've run the function statement on its own - sanity check that works; in fact without the `IF` block, running each statement either together or separately works fine too.\nWhat's wrong with my approach and how to fix it?",
      "solution": "The immediate cause of the error is improper dollar-quoting. This would work:\n`DO\n$do$\nBEGIN\n   IF NOT EXISTS (SELECT * FROM test_table WHERE character_length(col_a) > 100) THEN\n      ALTER TABLE test_table ALTER COLUMN col_a TYPE varchar(100);\n\n      DROP FUNCTION IF EXISTS test_function(varchar(100));\n\n      CREATE OR REPLACE FUNCTION test_function(test_param varchar(100))\n        RETURNS TABLE (\n         id uuid,\n         col_a varchar(100),\n         col_b int\n        )\n      LANGUAGE plpgsql AS\n      $func$\n      BEGIN\n         RETURN QUERY\n         SELECT test_table.id, test_table.col_a, test_table.col_b\n         FROM   test_table\n         WHERE  test_table.col_a = test_param;\n      END\n      $func$;\n   ELSE\n      RAISE NOTICE 'Cannot Revert. Rows exist that would be truncated!';\n   END IF;\nEND\n$do$;\n`\nSee:\n\nInsert text with single quotes in PostgreSQL\nWhat are '$$' used for in PL/pgSQL\n\nBut I wouldn't do most of what you are doing there to begin with. Use the data type `text` in table and function and be done with it. See:\n\nAny downsides of using data type \"text\" for storing strings?\nShould I add an arbitrary length limit to VARCHAR columns?\n\nIf you positively need a restriction to a maximum number of characters, still consider `text` and add a `CHECK` constraint.\n```\n`ALTER TABLE test_table ADD CONSTRAINT test_table_col_a_maxlen_200 CHECK (length(col_a) Then, if you want to change that constraint later, all you do is:\n`ALTER TABLE test_table\n  DROP CONSTRAINT test_table_col_a_maxlen_200  -- or whatever it was\n, ADD  CONSTRAINT test_table_col_a_maxlen_100 CHECK (length(col_a) \nPostgres will verify the `CHECK` constraint for you automatically, and fail with an error if any row violates it:\n\nERROR:  check constraint \"test_table_col_a_maxlen_100\" of relation \"test_table\" is violated by some row\n\nIn fairness, you can also just apply the change to `varchar(n)` in modern Postgres. It will check and fail if any existing row is too long:\n\nERROR:  value too long for type character varying(100)\n\nSo you can simplify things even if you stick with `varchar(n)`.",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2023-05-27T01:04:55",
      "url": "https://stackoverflow.com/questions/76344627/create-function-statement-in-an-if-block-throws-error-while-running-on-its-own"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "plpgsql",
      "question_id": 75068081,
      "title": "Use lateral join to loop over all tables from schema",
      "problem": "I want to count certain values in all tables of a schema that contain a column that can contain those values.\nWas hoping to use a LATERAL join to loop over all tables, but I'm running into issues:\n```\n`select\n    fully_qualified_table_name,\n    cnt\nfrom (\n    select\n        'datastore.' || table_name as fully_qualified_table_name\n    from\n        information_schema.columns\n    where\n        table_schema = 'datastore'\n        and column_name = 'dss_current_flag'\n    \n    cross join lateral\n    \n    select\n        count(*) as cnt\n    from\n        information_schema.fully_qualified_table_name\n    );\n`\n```\nIs this possible?",
      "solution": "Based on the answer by @jim-jones my final solution was\n```\n`CREATE TYPE datastore.schema_table_column_counts_type AS (\n    schema_name text,\n    table_name text,\n    column_name text,\n    value text,\n    count_p bigint);\n\nCREATE OR REPLACE FUNCTION datastore.count_records_in_schema_where_column_has_value(_schema_name text, _column_name text, _value text) \nRETURNS setof datastore.schema_table_column_counts_type language plpgsql AS $$\nDECLARE  \n  rec record;\n  result_record datastore.schema_table_column_counts_type;\nBEGIN   \n  FOR rec IN \n    SELECT \n        table_schema AS sch,\n        table_name AS tb, \n        $2 as cn, \n        $3 as v\n    FROM information_schema.columns\n    WHERE table_schema = $1\n    AND column_name = $2\n  LOOP\n    EXECUTE format($ex$ \n        SELECT \n            '%1$s' as schema_name, \n            '%2$s' as table_name, \n            '%3$s' as column_name,\n            '%4$s' as value,\n            count(*) \n        FROM \n            %1$s.%2$s\n        WHERE\n            %3$s = %4$L \n        $ex$\n        , rec.sch, rec.tb, rec.cn, rec.v) \n    INTO result_record;\n    return next result_record;\n  END LOOP;\nEND $$ ;\n\nSELECT * from datastore.count_records_in_schema_where_column_has_value('datastore', 'dss_current_flag', 'P');\n`\n```",
      "question_score": 1,
      "answer_score": 1,
      "created_at": "2023-01-10T10:44:37",
      "url": "https://stackoverflow.com/questions/75068081/use-lateral-join-to-loop-over-all-tables-from-schema"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "plpgsql",
      "question_id": 73772847,
      "title": "Create partition table using execute",
      "problem": "I would like to create N partition tables for the last N days. I have created a table like the following\n```\n`create table metrics.my_table (\n    id bigserial NOT NULL primary key,\n    ...\n    logdate date NOT NULL\n) PARTITION BY LIST (logdate);\n`\n```\nThen I have the following function to create those tables:\n```\n`CREATE OR REPLACE function metrics.create_my_partitions(init_date numeric default 30, current_date_parameter timestamp default current_date)\nreturns void as $$\nDECLARE\n    partition_date TEXT;\n    partition_name TEXT;\nbegin\n    for cnt in 0..init_date loop\n        partition_date := to_char((current_date_parameter - (cnt * interval '1 day')),'YYYY-MM-DD');\n        raise notice 'cnt: %', cnt;\n        raise notice 'partition_date: %', partition_date;\n        partition_name := 'my_table_' || partition_date;\n        raise notice 'partition_name: %', partition_name;\n        EXECUTE format('CREATE table if not exists metrics.%I PARTITION OF metrics.my_table for VALUES IN ($1)', partition_name) using partition_date;\n    end loop;\nEND\n$$\nLANGUAGE plpgsql;\n\nselect metrics.create_my_partitions(30, current_date);\n`\n```\nBut it throws the following error in the EXECUTE format line:\n```\n`SQL Error [42P02]: ERROR: there is no parameter $1\n`\n```\nAny idea on how to create those tables?",
      "solution": "The `EXECUTE ... USING ...` option only works for data values in DML commands (`SELECT`,`INSERT`, etc.). Since `CREATE TABLE` is a DDL command, use a parameter in `format()`:\n`execute format(\n    'create table if not exists metrics.%I partition of metrics.my_table for values in (%L)', \n    partition_name, partition_date);\n`",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2022-09-19T13:41:48",
      "url": "https://stackoverflow.com/questions/73772847/create-partition-table-using-execute"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "plpgsql",
      "question_id": 73717512,
      "title": "Set concatenated string as a application_name",
      "problem": "I'm trying to set application_name with some concatenated string as shown below in the example:\nExample:\n```\n`do\n$$\ndeclare var1 text := 'Text1';\n        var2 text := 'Text2';\n        result text;\nbegin\n    set application_name = var1||'-'||var2;\n    \n    select application_name into result \n    from pg_stat_activity where pid = pg_backend_pid();\n\n    raise info '%',result;\nend;\n$$;\n`\n```\nBut getting an error:\n\nSQL Error [42601]: ERROR: syntax error at or near \"||\"   Position: 116",
      "solution": "Use `format` to concatenate the string and then `execute` it, e.g.\n```\n`do\n$$\ndeclare var1 text := 'Text1';\n        var2 text := 'Text2';\n        result text;\nbegin    \n    execute format('set application_name=%s',quote_ident(var1||'-'||var2));\n    select application_name into result \n    from pg_stat_activity where pid = pg_backend_pid();    \n    raise info '%',result;\nend;\n$$;\n`\n```",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2022-09-14T15:14:38",
      "url": "https://stackoverflow.com/questions/73717512/set-concatenated-string-as-a-application-name"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "plpgsql",
      "question_id": 73374178,
      "title": "oracle %rowtype equivalent in postgresql",
      "problem": "Need to convert the `%rowtype` of oracle to equivalent in postgresql.\nMy try:\n```\n`create table public.test \n(\n    id int,\n    name varchar(20)\n);\n\ninsert into test values (1,'A');\ninsert into test values (2,'B');\ninsert into test values (3,'C');\n`\n```\nNote: I have a variable declare with table `%rowtype` using which we are checking multiple different column condition's as shown below in the example. It's not working in postgres.\n```\n`do \n$$\ndeclare pky public.test%rowtype;\nbegin\n    if pky.id=1\n    then \n        raise info 'id:1';\n    elsif pky.name = 'B'\n    then\n        raise info 'name:B';\n    else\n        raise info 'false';\n    end if;\nend;\n$$;\n`\n```\n`pky` is a input parameter of function in actual code.",
      "solution": "The variable is correctly declared, but you have not assigned any value to it. You can do this with a simple assignment statement\n`do \n$$\ndeclare pky public.test;\nbegin\n    pky:= '(11,something)'::public.test;\n    raise info '%', pky;\nend;\n$$;\n\nINFO:  (11,something)\n`\nor in a query with `into`\n`do \n$$\ndeclare pky public.test;\nbegin\n    select *\n    into pky\n    from public.test\n    where id = 1;\n    raise info '%', pky;\nend;\n$$;\n\nINFO:  (1,A)\n`\nor use it as a loop variable\n`do \n$$\ndeclare pky public.test;\nbegin\n    for pky in\n        select *\n        from public.test\n    loop\n        raise info '%', pky;\n    end loop;\nend;\n$$;\n\nINFO:  (1,A)\nINFO:  (2,B)\nINFO:  (3,C)\n`\nAs you can see, the `%rowtype` is not necessary.",
      "question_score": 1,
      "answer_score": 1,
      "created_at": "2022-08-16T14:40:50",
      "url": "https://stackoverflow.com/questions/73374178/oracle-rowtype-equivalent-in-postgresql"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "plpgsql",
      "question_id": 72817155,
      "title": "Generate matrix of access in source-target table",
      "problem": "I have two tables in PostgreSQL, `class` and `inheritance`.\nEach row in `inheritance` has 2 class IDs `source_id` and `target_id`:\n```\n`CREATE TABLE public.class (\n    id bigint NOT NULL DEFAULT nextval('class_id_seq'::regclass),\n    name character varying(500) COLLATE pg_catalog.\"default\" NOT NULL,\n    CONSTRAINT class_pkey PRIMARY KEY (id)\n)\n\nCREATE TABLE public.inheritance (\n    id bigint NOT NULL DEFAULT nextval('inherited_id_seq'::regclass),\n    source_id bigint NOT NULL,\n    target_id bigint NOT NULL,\n    CONSTRAINT inherited_pkey PRIMARY KEY (id),\n    CONSTRAINT inherited_source_id_fkey FOREIGN KEY (source_id)\n        REFERENCES public.class (id),\n    CONSTRAINT inherited_target_id_fkey FOREIGN KEY (target_id)\n        REFERENCES public.class (id)\n)\n`\n```\nI want to create Access Matrix between all classes based in inheritance relationship in inheritance table.\nI try this code:\n```\n`select * , case when id in (select target_id from inheritance where source_id=1) then 1 else 0 end as \"1\"   \n         , case when id in (select target_id from inheritance where source_id=2) then 1 else 0 end as \"2\"\n         , case when id in (select target_id from inheritance where source_id=3) then 1 else 0 end as \"3\"\n         , case when id in (select target_id from inheritance where source_id=4) then 1 else 0 end as \"4\"\n         , case when id in (select target_id from inheritance where source_id=5) then 1 else 0 end as \"5\"\n         , case when id in (select target_id from inheritance where source_id=6) then 1 else 0 end as \"6\"\n         , case when id in (select target_id from inheritance where source_id=7) then 1 else 0 end as \"7\"\n         , case when id in (select target_id from inheritance where source_id=8) then 1 else 0 end as \"8\"\n         , case when id in (select target_id from inheritance where source_id=9) then 1 else 0 end as \"9\"\nfrom class\n`\n```\nand get the right answer, but it's just for 9 static rows in class.\n\nHow can I get all number of rows in class using a dynamic SQL command?\nIf we can't do it with SQL, how can we do it with PL/pgSQL?",
      "solution": "Static solution\nSQL demands to know name and type of each result column (and consequently their number) at call time. You cannot derive result columns from data dynamically with plain SQL. You can use an array or a document type instead of separate columns:\n`SELECT *\nFROM   class c\nLEFT   JOIN (\n   SELECT target_id AS id, array_agg(source_id) AS sources\n   FROM  (SELECT source_id, target_id FROM inheritance i ORDER BY 1,2) sub\n   GROUP  BY 1\n   ) i USING (id);\n`\n\nid\nname\nsources\n\n1\nc1\n{2,3,4}\n\n2\nc2\n{5}\n\n3\nc3\n{5,6,7}\n\n4\nc4\n{7}\n\n5\nc5\n{8}\n\n6\nc6\n{9}\n\n7\nc7\n{9}\n\n8\nc8\nnull\n\n9\nc9\nnull\n\nDynamic solution\nIf that's not good enough you need dynamic SQL with two round-trips to the DB server: 1. Generate SQL. 2. Execute SQL. Using the `crosstab()` function from the additional module `tablefunc`. If you are unfamiliar, read this first:\n\nPostgreSQL Crosstab Query\n\nGenerate SQL:\n\n`SELECT format(\n$q$SELECT *\nFROM   class c\nLEFT   JOIN crosstab(\n   'SELECT target_id, source_id, 1 FROM inheritance ORDER BY 1,2'\n , 'VALUES (%s)'\n   ) AS ct (id int, %s int)\n     USING (id)\nORDER  BY id;\n$q$\n        , string_agg(c.id::text, '), (')\n        , string_agg('\"' || c.id || '\"', ' int, ')\n      )\nFROM  (SELECT id FROM class ORDER BY 1) c;\n`\nReturns a query of this form, which we ...\n2. Execute:\n`SELECT *\nFROM   class c\nLEFT   JOIN crosstab(\n   'SELECT target_id, source_id, 1 FROM inheritance ORDER BY 1,2'\n , 'VALUES (1), (2), (3), (4), (5), (6), (7), (8), (9)'\n   ) AS ct (id int, \"1\" int, \"2\" int, \"3\" int, \"4\" int, \"5\" int, \"6\" int, \"7\" int, \"8\" int, \"9\" int)\n     USING (id)\nORDER  BY id;\n`\n... to get:\n` id | name | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9\n----+------+---+---+---+---+---+---+---+---+---\n  1 | c1   |   | 1 | 1 | 1 |   |   |   |   |  \n  2 | c2   |   |   |   |   | 1 |   |   |   |  \n  3 | c3   |   |   |   |   | 1 | 1 | 1 |   |  \n  4 | c4   |   |   |   |   |   |   | 1 |   |  \n  5 | c5   |   |   |   |   |   |   |   | 1 |  \n  6 | c6   |   |   |   |   |   |   |   |   | 1\n  7 | c7   |   |   |   |   |   |   |   |   | 1\n  8 | c8   |   |   |   |   |   |   |   |   |  \n  9 | c9   |   |   |   |   |   |   |   |   |  \n`\ndb<>fiddle here\nSee:\n\nDynamic alternative to pivot with CASE and GROUP BY\nDynamically convert hstore keys into columns for an unknown set of keys\n\nDynamic execution with psql\nStill two round-trips to the server, but only a single command.\nBoth of the following solutions use psql meta-commands and only work from within psql!\nWith `\\gexec`\nUsing the standard interactive terminal, you can feed the generated SQL back to the Postgres server for execution directly with `\\gexec`:\n`test=> SELECT format(\n$q$SELECT *\nFROM   class c\nLEFT   JOIN crosstab(\n'SELECT target_id, source_id, 1 FROM inheritance ORDER BY 1,2'\n, 'VALUES (%s)'\n) AS ct (id int, %s int)\n  USING (id)\nORDER  BY id;\n$q$\n     , string_agg(c.id::text, '), (')\n     , string_agg('c' || c.id, ' int, ')\n   )\nFROM  (SELECT id FROM class ORDER BY 1) c\\gexec\n`\nSame result.\nWith `\\crosstabview`\n`test=> SELECT *\ntest-> FROM   class c\ntest-> LEFT   JOIN (\ntest(>    SELECT target_id AS id, source_id, 1 AS val\ntest(>    FROM   inheritance\ntest(>    ) i USING (id)\ntest-> \\crosstabview id source_id val\n id | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 |  \n----+---+---+---+---+---+---+---+---+--\n  1 | 1 | 1 | 1 |   |   |   |   |   | \n  2 |   |   |   | 1 |   |   |   |   | \n  3 |   |   |   | 1 | 1 | 1 |   |   | \n  4 |   |   |   |   |   | 1 |   |   | \n  5 |   |   |   |   |   |   | 1 |   | \n  6 |   |   |   |   |   |   |   | 1 | \n  7 |   |   |   |   |   |   |   | 1 | \n  8 |   |   |   |   |   |   |   |   | \n  9 |   |   |   |   |   |   |   |   | \n(9 rows)\n`\nSee (with related answers for both):\n\nHow do I generate a pivoted CROSS JOIN where the resulting table definition is unknown?\n\nThere are lots of subtleties in these solutions ...\nAside\nAssuming there are some mechanisms in place to disallow duplicates and directly contradicting relationships. Like:\n```\n`CREATE UNIQUE INDEX inheritance_uni_idx\nON inheritance (GREATEST(source_id, target_id), LEAST(source_id, target_id));\n`\n```\nSee:\n\nHow to create a Postgres table with unique combined primary key?",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2022-06-30T15:59:56",
      "url": "https://stackoverflow.com/questions/72817155/generate-matrix-of-access-in-source-target-table"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "plpgsql",
      "question_id": 72659825,
      "title": "Postgres SELECT that doesn&#39;t work with the first function call",
      "problem": "The function does its duty, that is, if there is no hub with that data it adds it and returns its ID otherwise it finds the hub ID with the data passed in input from the JSON.\n```\n`CREATE OR REPLACE FUNCTION public.add_hub(\n    new_hub json,\n    OUT hub_id bigint)\n    RETURNS bigint\n    LANGUAGE 'plpgsql'\n    COST 100\n    VOLATILE PARALLEL UNSAFE\nAS $BODY$\nDECLARE\nhub_name hub.name%TYPE := new_hub->>'name';\nhub_city hub.city%TYPE := new_hub->>'city';\nhub_province hub.province%TYPE := new_hub->>'province';\nhub_region hub.region%TYPE := new_hub->>'region';\nhub_address hub.address%TYPE := new_hub->>'address';\n\nBEGIN\n    SELECT hub.id FROM hub \n        WHERE name = hub_name \n            AND city = hub_city \n            AND province = hub_province\n            AND address = hub_address \n            AND region = hub_region \n            INTO hub_id;\n\n   IF NOT FOUND THEN\n         INSERT INTO public.hub(name, city, province, address, region)\n            VALUES (hub_name, hub_city, hub_province, hub_address, hub_region)\n            RETURNING hub.id INTO hub_id;\n   END IF;\n   \nEND;\n$BODY$;\n\nALTER FUNCTION public.add_hub(json)\n    OWNER TO postgres;\n\n`\n```\nThe problem arises when I run a query like this:\n```\n`SELECT * FROM hub \n  WHERE hub.id = add_hub('\n  {\n   \"name\":\"HUB TEST\",\n   \"city\":\"Los Angeles\",\n   \"province\":\"LA\",\n   \"address\":\"Street 1\",\n   \"region\":\"CA\"\n  }\n');\n`\n```\nIf the hub data is not present in the table they are inserted, however the query with the SELECT returns nothing, if executed again it returns the correct hub.\nThe weird thing is following query always works and always returns hub id, even when data is added the first time:\n```\n`SELECT add_hub('\n  {\n   \"name\":\"HUB TEST\",\n   \"city\":\"Los Angeles\",\n   \"province\":\"LA\",\n   \"address\":\"Street 1\",\n   \"region\":\"CA\"\n  }\n');\n`\n```\nIt always returns a valid value, even in the case of the first iteration, so I really don't understand where I'm wrong, if you have any advice I would be grateful.",
      "solution": "Modifying the table data in the WHERE clause is a very bad idea. First, the function will be called as many times as there are rows in the table. Second, while executing the query, the server cannot modify the queried dataset every time it evaluates the condition.\nIf you want to use the described mechanism, the easiest solution is to change the function so that it returns the entire found or inserted row and use the last query in your question.",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2022-06-17T15:02:43",
      "url": "https://stackoverflow.com/questions/72659825/postgres-select-that-doesnt-work-with-the-first-function-call"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "plpgsql",
      "question_id": 72495975,
      "title": "Postgres: calling function with text[] param fails with array literal",
      "problem": "I have a Postgres function that accepts a text[] as input. For example\n```\n`create function temp1(player_ids text[])\n    returns void\n    language plpgsql\nas\n$$\nbegin\n    update players set player_xp = 0\n    where id in (player_ids);\n    -- the body is actually 20 lines long, updating a lot of tables\nend;\n$$;\n`\n```\nand I'm trying to call it, but I keep getting\n```\n`[42883] ERROR: operator does not exist: text = text[] Hint: No operator matches the given name and argument types. You might need to add explicit type casts. Where: PL/pgSQL function temp1(text[]) line 3 at SQL statement\n`\n```\nI have tried these so far\n```\n`select temp1('{F7AWLJWYQ5BMPKGXLMDNQKQ4NY,AQPBAFKQONGLBKIMCSOD747GY4}');\nselect temp1('{F7AWLJWYQ5BMPKGXLMDNQKQ4NY,AQPBAFKQONGLBKIMCSOD747GY4}'::text[]);\nselect temp1(array['F7AWLJWYQ5BMPKGXLMDNQKQ4NY,AQPBAFKQONGLBKIMCSOD747GY4']);\nselect temp1(array['F7AWLJWYQ5BMPKGXLMDNQKQ4NY,AQPBAFKQONGLBKIMCSOD747GY4']::text[]);\n`\n```\nI have to be missing something obvious...how do I call this function with an array literal?",
      "solution": "Use `= any` instead of `in:`\n`    ...\n    update players set player_xp = 0\n    where id = any(player_ids);\n    ...\n`\nThe IN operator acts on an explicit list of values.\n\nexpression IN (value [, ...])\n\nWhen you want to compare a value to each element of an array, use ANY instead.\n\nexpression operator ANY (array expression)\n\nNote that there are variants of both constructs for subqueries expression IN (subquery) and expression operator ANY (subquery). The first one was properly used in the other answer though a subquery seems excessive in this case.",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2022-06-04T01:17:30",
      "url": "https://stackoverflow.com/questions/72495975/postgres-calling-function-with-text-param-fails-with-array-literal"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "plpgsql",
      "question_id": 71878812,
      "title": "In a Postgres transaction, what is the best way to delete if exists, and fail otherwise?",
      "problem": "In Postgres, I want to do a bunch of deletes and writes in a transaction, but I want to fail the transaction if a row I am intending to delete does not exist. What is the best way to do this?",
      "solution": "Use a PL/pgSQL code block (in a `FUNCTION`, `PROCEDURE` or `DO` statement) and raise an exception if your `DELETE` did not find any rows. You can use the special variable `FOUND`:\n`DO\n$do$\nBEGIN\n   DELETE FROM tbl1 WHERE id = 1;\n   \n   IF NOT FOUND THEN\n      RAISE EXCEPTION 'Failed to delete!';\n   END IF;\n   \n   INSERT INTO tbl2 (col1) VALUES ('foo');\nEND\n$do$;\n`\nRaising an exception rolls back the whole transaction.\nThe manual:\n\nNote in particular that `EXECUTE` changes the output of `GET DIAGNOSTICS`, but does not change `FOUND`.\n\nSee:\n\nDynamic SQL (EXECUTE) as condition for IF statement",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2022-04-15T02:13:23",
      "url": "https://stackoverflow.com/questions/71878812/in-a-postgres-transaction-what-is-the-best-way-to-delete-if-exists-and-fail-ot"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "plpgsql",
      "question_id": 71458498,
      "title": "How to insert declared type variable into table | Postgress",
      "problem": "I have been working on creating a store procedure that will select data from a table, do some modification to that data, and then I need to insert that modified data into the same table. Take an example my table name is `student`. My procedure looks like below:\n```\n`create or replace procedure student_create(p_code varchar)\nlanguage plpgsql\nas $$\ndeclare\n   v_student public.student;\nbegin\n    select * into v_student from student where code = p_code and is_latest_version is true;\n    raise notice 'Value: %', v_student;\n    v_student.id = uuid_generate_v4();\n    v_student.version_created_at = now();\n    v_student.version_updated_at = v_student.version_created_at;\n    raise notice 'Value: %', v_student;\n    INSERT INTO public.student VALUES(v_student);\nend;$$\n`\n```\nI am getting errors while calling this procedure:\n```\n`ERROR:  column \"id\" is of type uuid but expression is of type hotel\nLINE 1: INSERT INTO public.hotel VALUES(v_hotel)\n`\n```\nI know I can make insert statements like I can get each value from the variable and set it like\n```\n`INSERT INTO public.student VALUES(v_student.id, v_student.code, v_student.name);\n`\n```\nBut I don't want to do that because it will become tightly coupled and later if I add another column into the table then I need to add that column into this procedure as well.\nDoes anyone have idea how can I insert the declared type variable directly into table.",
      "solution": "There is no table type, there is only row composite type. Check manual 43.3.4. Row Types.\nuse row type.\n\n```\n`create or replace procedure student_create(p_code text)\nlanguage plpgsql\nas $$\ndeclare\n   v_student public.student\nbegin\n    for v_student in  select *  from student where code = p_code and is_latest_version is true\n    loop\n    v_student.id = uuid_generate_v4();\n    v_student.version_created_at = now();\n    v_student.version_updated_at = v_student.version_created_at;\n    v_student.is_latest_version = true;\n    v_student.code = p_code;\n    INSERT INTO student VALUES(v_student.*);\nend loop;\nend;$$;\n`\n```\ncall it: `call student_create('hello');`\n3. use `update` clause directly.\n```\n`create or replace procedure student_create_1(p_code text)\nlanguage plpgsql as $$\nBEGIN\n    with a  as ( select uuid_generate_v4() as id ,\n                       now() as version_created_at,\n                       now() as version_updated_at,\n                       p_code as \"code\"   from student \n                where code = p_code and is_latest_version is true)\n    \n     INSERT INTO student(id, version_created_at, version_updated_at, code) \n            select a.id, a.version_created_at,a.version_updated_at,a.\"code\" from a;\n    \nend\n$$;\n`\n```\ncall it: `call student_create_1('hello');`\nfiddle code: here",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2022-03-13T17:05:07",
      "url": "https://stackoverflow.com/questions/71458498/how-to-insert-declared-type-variable-into-table-postgress"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "plpgsql",
      "question_id": 71412697,
      "title": "How to perform a GRANT in a stored procedure using a variable?",
      "problem": "As an alternative to Post-create hook for BLOB values in Hibernate I thought an in-database trigger would be a better solution, so I attempted to write the following trigger:\n```\n`CREATE FUNCTION lo_default_grant() RETURNS trigger AS $$\nDECLARE\n    lo_oid lo := NEW[TG_ARGV[0]];\nBEGIN\n    IF TG_OP = 'INSERT' THEN\n        GRANT SELECT ON LARGE OBJECT lo_oid TO reader_role;\n    ELSIF OLD[TG_ARGV[0]] != NEW[TG_ARGV[0]] THEN\n        GRANT SELECT ON LARGE OBJECT lo_oid TO reader_role;\n    END IF;\nEND;\n$$ LANGUAGE plpgsql;\n`\n```\nWhich would be used with a trigger such as:\n```\n`CREATE TRIGGER t_grant AFTER INSERT OR UPDATE ON my_entity\n  FOR EACH ROW EXECUTE PROCEDURE lo_default_grant(file);\n`\n```\nHowever, I cannot work out the syntax for building the `GRANT` statement.\n\n[2022-03-09 16:14:51] [42601] ERROR: syntax error at or near \"lo_oid\"\n[2022-03-09 16:14:51] Position: 279\n\nIt it really not possible to use variables in the same way you can in other statements?",
      "solution": "`GRANT` statements are not optimizable, so you cannot directly insert parameters into them. Nor can you subscript rows with dynamic field names.\nHowever, PL/pgSQL has the `EXECUTE` statement, allowing you to construct a string containing the desired statement:\n```\n`CREATE OR REPLACE FUNCTION lo_default_grant() RETURNS trigger AS $$\nDECLARE\n    old_lo oid;\n    new_lo oid;\nBEGIN\n    EXECUTE format('SELECT $1.%I', TG_ARGV[0]) USING OLD INTO old_lo;\n    EXECUTE format('SELECT $1.%I', TG_ARGV[0]) USING NEW INTO new_lo;\n    IF TG_OP = 'INSERT' OR old_lo != new_lo THEN\n        EXECUTE format('GRANT SELECT ON LARGE OBJECT %s TO reader_role', new_lo);\n    END IF;\n    RETURN NULL;\nEND;\n$$ LANGUAGE plpgsql;\n`\n```",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2022-03-09T17:25:46",
      "url": "https://stackoverflow.com/questions/71412697/how-to-perform-a-grant-in-a-stored-procedure-using-a-variable"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "plpgsql",
      "question_id": 71399551,
      "title": "Extract column values from one table and insert with modifications into another",
      "problem": "I have created a PL/pgSQL function that accepts two column names, a \"relation\", and two table names. It finds distinct rows in one table and inserts them in to a temporary table, deletes any row with a null value, and sets all values of one column to `relation`. I have the first part of the process using this function.\n```\n`create or replace function alt_edger(s text, v text, relation text, tbl text, tbl_src text)\n    returns void\n    language plpgsql as\n$func$\nbegin\n    raise notice 's: %, v: %, tbl: %, tbl_src: %', s,v,tbl,tbl_src;\n    execute ('insert into '||tbl||' (\"source\", \"target\") select distinct \"'||s||'\",\"'||v||'\" from '||tbl_src||'');\n    execute ('DELETE FROM '||tbl||' WHERE \"source\" IS null or \"target\" is null'); \nend\n$func$;\n`\n```\nIt is executed as follows:\n```\n`-- create a temporary table and execute the function twice\ndrop table if exists temp_stack;\ncreate temporary table temp_stack(\"label\" text, \"source\" text, \"target\" text, \"attr\" text, \"graph\" text);\n \nselect alt_edger('x_x', 'y_y', ':associated_with', 'temp_stack','pg_check_table' );\nselect alt_edger('Document Number', 'x_x', ':documents', 'temp_stack','pg_check_table' );\n\nselect * from temp_stack;\n`\n```\nNote that I didn't use `relation`, yet. The `INSERT` shall also assign `relation`, but I can't figure out how to make that happen to get something like:\n\nlabel\nsource\ntarget\nattr\ngraph\n\n:associated_with\n638000\nARAS\n\n:associated_with\n202000\nJASE\n\n:associated_with\n638010\nJASE\n\n:associated_with\n638000\nJASE\n\n:associated_with\n202100\nJASE\n\n:documents\nA\n638010\n\n:documents\nA\n202000\n\n:documents\nA\n202100\n\n:documents\nB\n638000\n\n:documents\nA\n638000\n\n:documents\nB\n124004\n\n:documents\nB\n202100\n\nMy challenges are:\n\nHow to integrate `relation` in  the `INSERT`?  When I try to use  `VALUES` and comma separation I get an \"error near select\".\nHow to allow strings starting with  \":\" in `relation`? I'm anticipating here, the inclusion of the colon has given me challenges in the past.\n\nHow can I do this? Or is there a better approach?\nToy data model:\n```\n`drop table if exists pg_check_table;\ncreate temporary table pg_check_table(\"Document Number\" text, x_x int, y_y text);\ninsert into pg_check_table values ('A',202000,'JASE'),\n('A',202100,'JASE'),\n('A',638010,'JASE'),\n('A',Null,'JASE'),\n('A',Null,'JASE'),\n('A',202100,'JASE'),\n('A',638000,'JASE'),\n('A',202100,'JASE'),\n('B',638000,'JASE'),\n('B',202100,null),\n('B',638000,'JASE'),\n('B',null,'ARAS'),\n('B',638000,'ARAS'),\n('B',null,'ARAS'),\n('B',638000,null),\n('B',124004,null);\nalter table pg_check_table add row_num serial;\nselect * from pg_check_table;\n`\n```",
      "solution": "`-- DROP FUNCTION alt_edger(_s text, _v text, _relation text, _tbl text, _tbl_src text)\nCREATE OR REPLACE FUNCTION alt_edger(_s text, _v text, _relation text, _tbl text, _tbl_src text, OUT row_count int)\n  LANGUAGE plpgsql AS\n$func$\nDECLARE\n   _sql text := format(\n       'INSERT INTO pg_temp.%3$I (label, source, target)\n        SELECT DISTINCT $1, %1$I, %2$I FROM pg_temp.%4$I\n        WHERE (%1$I, %2$I) IS NOT NULL'\n      , _s, _v, _tbl, _tbl_src);\nBEGIN\n   -- RAISE NOTICE '%', _sql;  -- debug\n   EXECUTE _sql USING _relation;\n   GET DIAGNOSTICS row_count = ROW_COUNT;  -- return number of inserted rows\nEND\n$func$;\n`\ndb<>fiddle here\nMost importantly, use `format()` to concatenate your dynamic SQL commands safely. And use the format specifier `%I` for identifiers. This way, SQL injection is not possible and identifiers are double-quoted properly - preserving non-standard names like `Document Number`. That's where your original failed.\nWe could concatenate `_relation` as string to be inserted into `label`, too. But the preferable way to pass values to `EXECUTE` is with the `USING` clause. `$1` inside the SQL string passed to `EXECUTE` is a placeholder for the first `USING` argument. Not to be confused with `$1` referencing function parameters in the context of the function body outside `EXECUTE`! (You can pass any string, leading colon (`:`) does not matter, the string is not interpreted when done right.)\nSee:\n\nFormat specifier for integer variables in format() for EXECUTE?\nTable name as a PostgreSQL function parameter\n\nI replaced the `DELETE` in your original with a `WHERE` clause to the `SELECT` of the `INSERT`. Don't insert rows in the first place, instead of deleting them again later.\n`(%1$I, %2$I) IS NOT NULL` only qualifies when both values are `NOT NULL`.\nAbout that:\n\nCheck if a Postgres composite field is null/empty\n\nDon't use the prefix \"pg_\" for your table names. That's what Postgres uses for system tables. Don't mess with those.\nI schema-qualify known temporary tables with `pg_temp.` That's typically optional as the temporary schema comes first in the `search_path` by default. But that can be changed (maliciously), and then the table name would resolve to any existing regular table of the same name in the `search_path`. So better safe than sorry. See:\n\nHow does the search_path influence identifier resolution and the \"current schema\"\n\nI made the function return the number of inserted rows. That's totally optional!\nSince I do that with an `OUT` parameter, I am allowed to skip the `RETURNS` clause. See:\n\nCan I make a plpgsql function return an integer without using a variable?",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2022-03-08T18:58:36",
      "url": "https://stackoverflow.com/questions/71399551/extract-column-values-from-one-table-and-insert-with-modifications-into-another"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "plpgsql",
      "question_id": 70349374,
      "title": "Escape quotes on psql to avoid SQLInjections",
      "problem": "I'm doing a function to compare JSONs and then insert on a table (trigger) only the differences.\nThe function works perfectly when none of the registers come with a quote. So I would to know how to escape these quotes that may come:\n`CREATE OR REPLACE FUNCTION public.fnc_compare_jsonb(old_reg jsonb, new_reg jsonb)\n RETURNS jsonb\n LANGUAGE plpgsql\n IMMUTABLE STRICT\nAS $function$\ndeclare\n    keys record;\n    jsonb_return jsonb = '{}'::jsonb;\nbegin\n    for keys in\n        select *\n        from jsonb_object_keys($1)\n    loop\n        if $1 -> keys.jsonb_object_keys <> $2 -> keys.jsonb_object_keys then\n            jsonb_return = jsonb_return || format('{\"%s\": \"%s\"}', keys.jsonb_object_keys, $2 ->> keys.jsonb_object_keys)::jsonb;\n        end if;\n    end loop;\n    return jsonb_return;\nend\n$function$\n;\n`\nThe error is happening on line:\n```\n`jsonb_return = jsonb_return || format('{\"%s\": \"%s\"}', keys.jsonb_object_keys, $2 ->> keys.jsonb_object_keys)::jsonb;\n`\n```\n```\n`SQL Error [22P02]: ERROR: invalid input syntax for type json\n  Detail: Token \"@\" is invalid.\n  Where: JSON data, line 1: {\"email\": \"test2\"@...\nPL/pgSQL function ecidadao.fnc_compare_jsonb(jsonb,jsonb) line 11 at assignment\n`\n```",
      "solution": "Your format statement creates some text, not a valid json. Use a json-function to do so:\n```\n`SELECT  json_build_object('foo'::text,'text with double quotes \" \"'::text);\n`\n```\nResult: {\"foo\" : \"text with double quote \" \"\"}",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2021-12-14T14:07:12",
      "url": "https://stackoverflow.com/questions/70349374/escape-quotes-on-psql-to-avoid-sqlinjections"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "plpgsql",
      "question_id": 69805064,
      "title": "postgresql function and trigger execute select current_setting into integer variable causes error",
      "problem": "Postgresql 9.6.x\nI am getting an error with a postgresql function where I am recording a log on every table modification.  This was all working great until I added this functionality where I am recording the current user id using current_setting functionality of postgresql.  I set the current user on transactions in the background like so:\n```\n` select  set_config('myvars.active_user_id', '2123', true)\n`\n```\nAll this functionality works perfectly fine, except when the user is not set.  This occurs when the tables are being updated by back end system queries and in that case the setting 'myvars.active_user_id' is null.\nI want it to be null when it is not set.  The user id field in the log is nullable.\nIt seems to be trying to convert null to an empty string and put that in the integer variable which it doesn't like.\nThis appears to be some kind of weird problem specific to functions with triggers.  I must be doing something wrong because as far as I know assigning a null value through a select...into\"is no issue.\nThe error I get in that case is so:\n```\n`PSQLException: ERROR: invalid input syntax for integer: \"\"\n`\n```\nI have added tracing statements and it is on this line:\n```\n`EXECUTE 'select current_setting(''myvars.active_user_id'', true)  ' into log_user_id;\n`\n```\nI don't understand why in this setting it gets upset about the null value. But it seems limited to this type of trigger function.  Below is essentially the function I am using\n```\n`CREATE OR REPLACE FUNCTION update_log() RETURNS TRIGGER AS $update_log$\nDECLARE\n    logid int;\n   log_user_id int;\nBEGIN\nEXECUTE 'select current_setting(''myvars.active_user_id'', true)  ' into log_user_id;\n\n  IF (TG_OP='DELETE') THEN\n            EXECUTE 'select nextval(''seq_log'') ' into logid;\n--            INSERT INTO log ....\n            RETURN NULL;\n  ELSIF (TG_OP='INSERT') THEN\n            EXECUTE 'select nextval(''seq_log'') ' into logid;\n--            INSERT INTO log ....\n            RETURN NEW;\n\n  ELSIF (TG_OP='UPDATE') THEN\n--            INSERT INTO log ....\n        END IF;\n  END IF;\n        RETURN NULL; \n    END;\n$log$ LANGUAGE plpgsql;\n`\n```\nAny thoughts?",
      "solution": "GUC (Global User Setting) variables like your `myvars.active_user_id` are not nullable internally. It holds text or empty text. These variables cannot to store NULL. So when you store NULL, then empty string is stored, and this empty string is returned from function `current_setting`.\nIn Postgres (and any database without Oracle) NULL is not empty string and empty string is not NULL.\nSo this error is expected:\n```\n`postgres=# do $$\ndeclare x int;\nbegin\n  perform set_config('x.xx', null, false);\n  execute $_$ select current_setting('x.xx', true) $_$ into x;\nend;\n$$;\nERROR:  invalid input syntax for integer: \"\"\nCONTEXT:  PL/pgSQL function inline_code_block line 5 at EXECUTE\n`\n```\nYou need to check result first, and replace empty string by NULL:\n```\n`create or replace function nullable(anyelement)\nreturns anyelement as $$\n  select case when $1 = '' then NULL else $1 end;\n$$ language sql;\n\ndo $$\ndeclare x int;\nbegin\n  perform set_config('x.xx', null, false);\n  execute $_$ select nullable(current_setting('x.xx', true)) $_$ into x;\nend;\n$$;\nDO\n`\n```\n@Laurenz Albe has big true in your comment. Use dynamic SQL (`execute` command) only when it is necessary. It is not this case. So your code should looks like:\n```\n`do $$\ndeclare x int;\nbegin\n  perform set_config('x.xx', null, false);\n  x := nullable(current_setting('x.xx', true)); \nend;\n$$;\nDO\n`\n```\nNote: There is buildin function `nullif`, so your code can looks like (and sure, buildin functionality should be preferred):\n```\n`do $$\ndeclare x int;\nbegin\n  perform set_config('x.xx', null, false);\n  x := nullif(current_setting('x.xx', true), ''); \nend;\n$$;\nDO\n`\n```",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2021-11-02T04:23:06",
      "url": "https://stackoverflow.com/questions/69805064/postgresql-function-and-trigger-execute-select-current-setting-into-integer-vari"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "plpgsql",
      "question_id": 69720607,
      "title": "add parameter to dynamic EXECUTE in postgreSQL",
      "problem": "I have a procedure which contain this piece of code\n```\n`SELECT query INTO query FROM temp_table_test WHERE id = whateverId;\n\nEXECUTE query;\n`\n```\nMy query variable will contain something like\n```\n`CALL Someprocedure (withSomeParameters)\n`\n```\nHow can i make the parameter dynamic in the first proc as they can change time to time and i can't make them static in the variable ?\nin t-SQL there is something with\n```\n`sp_executesql\n`\n```\nBut as I'm very new to postgres I don't know where to start.",
      "solution": "assuming you are using pl/pgsql, you can pass parameters with the USING clause;\n```\n`EXECUTE query USING $1, $2, $3, ...;\n`\n```\nfor example;\n```\n`query = 'SELECT SUM( \"field_a\" ) FROM \"table\" WHERE \"field_b\" = $1 AND \"field_c\" = $2';\nEXECUTE query USING 5, 10;\n`\n```\nthis will be equal to:\n```\n`SELECT SUM( \"field_a\" ) FROM \"table\" WHERE \"field_b\" = 5 AND \"field_c\" = 10;\n`\n```",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2021-10-26T11:29:21",
      "url": "https://stackoverflow.com/questions/69720607/add-parameter-to-dynamic-execute-in-postgresql"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "plpgsql",
      "question_id": 69511796,
      "title": "Dynamic SQL with EXECUTE and nested format()",
      "problem": "I tried doing this with just a normal CTE (no `EXECUTE ... FORMAT`) and I was able to do it.\nI basically have a table that has some 5 columns and some data in each column that I wanted to concatenate and such to manifest/generate some data in a new column.\nI can do something like this and it works:\n```\n`WITH cte AS (SELECT *, case \n                        when var1 = '' then ''\n                        when var2 = '' then ''\n                        else '' end, 'adding_dummy_text_column'\n             FROM some_other_table sot\n             WHERE sot.type = 'java')\n\n             INSERT INTO my_new_table \n                    SELECT *, 'This is the new column I want to make with some data from my CTE' || c.type\n                    FROM cte c;\n`\n```\nSo this works as I said. I'll end up getting a new table that has an extra column which a hardcoded, concatenated string `This is the new column I want to make with some data from my CTE Java`\nOf course, whatever is in the `c.type` column for the corresponding row in the CTE as it loads the `SELECT` is what gets concatenated to that string.\nThe problem is, as soon as I start using the `EXECUTE...FORMAT` to make it cleaner and have more power to concatenate/combined different data pieces from my different columns (I have data kind of scattered around in bad formats and I'm populating a fresh new table), it's as if the FORMAT arguments or the variables cannot detect the CTE table.\nThis is how I'm doing it\n```\n`EXECUTE FORMAT ('WITH cte AS (SELECT *, case \n                        when var1 = %L then %L\n                        when var2 = '' '' then '' ''\n                        else '' '' end, ''adding_dummy_text_column''\n             FROM some_other_table sot\n             WHERE sot.type = ''java'')\n\n             INSERT INTO my_new_table \n                    SELECT *, ''This is the new column I want to make with some data from my CTE %I''\n                    FROM cte c', 'word1', 'word2', c.type\n             );\n`\n```\nOK, so I know I used the empty string `'' ''` in this example and the `%L` but i just wanted to show I had no issues with any of that. Its when I try to reference my CTE columns, so you can see I'm trying to do the same concatenation but by leveraging the `EXECUTE...FORMAT` and using the `%I` identifiers. So, the first 2 args are just fine, its the `c.type` that just no matter what column I try, doesn't work. Also, I removed the `c` alias and didn't get any better luck. It's 100% anytime I reference the columns on the CTE though, as I have removed all that code and it runs just fine without that.\nBut yeah, is there any work around? I really want to transform some data and now have to do the || for concatenation.",
      "solution": "This should do it:\n`EXECUTE format($f$WITH cte AS (SELECT *, CASE\n                        WHEN var1 = %L THEN %L\n                        WHEN var2 = ' ' THEN ' '\n                        ELSE ' ' END, 'adding_dummy_text_column'\n             FROM some_other_table sot\n             WHERE sot.type = 'java')\n\n             INSERT INTO my_new_table\n                    SELECT *, format('This is the new column I want to make with some data from my CTE %%I', c.type)\n                    FROM cte c$f$\n           , 'word1', 'word2');\n`\nThere are two levels. You need a second `format()` that's executed by the dynamic SQL string that's been concatenated by the first `format()`.\nI simplified with dollar-quoting. See:\n\nInsert text with single quotes in PostgreSQL\n\nThe nested `%` character must be escaped by doubling it up: `%%I`. See:\n\nWhat does %% in PL/pgSQL mean?\n\nGenerates and executes this SQL statement:\n`WITH cte AS (SELECT *, CASE\n                        WHEN var1 = 'word1' THEN 'word2'\n                        WHEN var2 = ' ' THEN ' '\n                        ELSE ' ' END, 'adding_dummy_text_column'\nFROM some_other_table sot\nWHERE sot.type = 'java')\n\nINSERT INTO my_new_table\nSELECT *, format('This is the new column I want to make with some data from my CTE %I', c.type)\nFROM cte c\n`\nWhich can be simplified and improved to this equivalent:\n`INSERT INTO my_new_table(co1, col2, ...)  -- provide target column list!\nSELECT *\n     , CASE WHEN var1 = 'word1' THEN 'word2'\n            WHEN var2 = ' ' THEN ' '\n            ELSE ' ' END\n     , 'adding_dummy_text_column'\n     , format('This is the new column I want to make with some data from my CTE %I', sot.type)\nFROM   some_other_table sot\nWHERE  sot.type = 'java'\n`\nAbout the missing target column list:\n\nCannot create stored procedure to insert data: type mismatch for serial column",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2021-10-10T04:06:58",
      "url": "https://stackoverflow.com/questions/69511796/dynamic-sql-with-execute-and-nested-format"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "plpgsql",
      "question_id": 69222259,
      "title": "PostgreSQL - using variable inside quotes",
      "problem": "I am trying to use a variable for an expression like this\n```\n`NOW() - INTERVAL '5 days'\n`\n```\nBut getting errors:\n```\n`CREATE OR REPLACE FUNCTION some.archive() RETURNS VOID AS\n$$\nDECLARE\n    p_archive_depth CONSTANT VARCHAR := '5 days';\nBEGIN\n\n    IF (date(p_table_date) Also tried without success:\n```\n`'' || p_archive_depth || ''\n'' p_archive_depth ''\n`\n```",
      "solution": "You need to define the variable with the data type `interval`\n```\n`DECLARE\n    p_archive_depth CONSTANT interval := interval '5 days';\nBEGIN \n   IF date(p_table_date) < (now() - p_archive_depth)::date\n`\n```",
      "question_score": 1,
      "answer_score": 1,
      "created_at": "2021-09-17T12:57:15",
      "url": "https://stackoverflow.com/questions/69222259/postgresql-using-variable-inside-quotes"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "plpgsql",
      "question_id": 68896735,
      "title": "Postgres select into from string list",
      "problem": "I have list of strings:\n`var ids = [\"aa\", \"cc, \"bb\"];`\nand I want to insert them into temporary table. This is how I try to do it:\n```\n`DROP TABLE IF EXISTS Ids;\nCREATE TEMP TABLE Ids AS\nSELECT Id FROM @ids\n`\n```\nhowever that throws syntax error. How it should be done?",
      "solution": "The parameter is an array, so the query should use array functions\n```\n`CREATE TEMP TABLE Ids AS\nSELECT unnest(ARRAY['a','b','c']);\n`\n```\nor using the .net place holder:\n```\n`CREATE TEMP TABLE Ids AS\nSELECT unnest(@ids);\n`\n```",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2021-08-23T19:29:48",
      "url": "https://stackoverflow.com/questions/68896735/postgres-select-into-from-string-list"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "plpgsql",
      "question_id": 68551877,
      "title": "What is the &quot;lifespan&quot; of a postgres CTE expression? e.g. WITH... AS",
      "problem": "I have a CTE I am using to pull some data from two tables then stick in an intermediate table called `cte_list`, something like\n```\n`  with cte_list as (\n        select pl.col_val from prune_list pl join employees.employee emp on pl.col_val::uuid = emp.id\n        where pl.col_nm = 'employee_ref_id' limit 100\n        )\n`\n```\nThen, I am doing an insert to move records from the `cte_list` to another archive table (if they don't exist) called `employee_arch_test`\n```\n` insert into employees.employee_arch_test (\n      select * from employees.employee where id in (select col_val::uuid from cte_list)\n      and not exists (select 1 from employees.employee_arch_test where employees.employee_arch_test.id=employees.employee.id)\n      );\n`\n```\nThis seems to work fine. The problem is when I add another statement after, to do some deletions from the main `employee` table using this aforementioned `cte_list` - the cte_list apparently no longer exists?\n```\n`SQL Error [42P01]: ERROR: relation \"cte_list\" does not exist\n`\n```\nthe actual delete query:\n```\n`delete from employees.employee where id in (select col_val::uuid from cte_list);\n`\n```\nCan the `cte_list` CTE table only be used once or something? I'm running these statements in a LOOP and I need to run the exact same calls for about 2 or 3 other tables but hit a sticking point here.",
      "solution": "A CTE only exists for the duration of the statement of which it's a part. I gather you have an INSERT statement with the CTE preceding it:\n```\n`with cte_list\n  as (select pl.col_val\n        from prune_list pl\n        join employees.employee emp\n          on pl.col_val::uuid = emp.id\n        where pl.col_nm = 'employee_ref_id'\n        limit 100\n        )\n insert into employees.employee_arch_test\n   (select *\n      from employees.employee\n      where id in (select col_val::uuid from cte_list)\n            and not exists (select 1\n                              from employees.employee_arch_test\n                              where employees.employee_arch_test.id = employees.employee.id)\n   );\n`\n```\nThe CTE is part of the INSERT statement - it is not a separate statement by itself. It only exists for the duration of the INSERT statement.\nIf you need something which lasts longer your options are:\n\nAdd the same CTE to each of your following statements. Note that because data may be changing in your database each invocation of the CTE may return different data.\nCreate a view which performs the same operations as the CTE, then use the view in place of the CTE. Note that because data may be changing in your database each invocation of the view may return different data.\nCreate a temporary table to hold the data from your CTE query, then use the temporary table in place of the CTE. This has the advantage of providing a consistent set of data to all operations.",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2021-07-27T23:16:50",
      "url": "https://stackoverflow.com/questions/68551877/what-is-the-lifespan-of-a-postgres-cte-expression-e-g-with-as"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "plpgsql",
      "question_id": 68207158,
      "title": "How to escape a percent sign in &quot;EXECUTE format(&#39;blah&#39;)&quot;",
      "problem": "I have two tables, e.g.:\n```\n`CREATE TABLE table_1\n(\n    one_column   INTEGER,\n    two_column   INTEGER,\n    three_column INTEGER\n);\n\nCREATE TABLE table_2\n(\n    id        SERIAL,\n    column_1  INTEGER,\n    column_2  INTEGER,\n    column_3  INTEGER,\n    name      TEXT,\n    step      INTEGER\n);\n`\n```\nI also have a stored function which receives a number of parameters. Within the function, I `INSERT` a row into a table using a call to `EXECUTE format()`, something like this...\n```\n`CREATE OR REPLACE PROCEDURE function_p (\n    p_name TEXT DEFAULT '',\n    p_step INT DEFAULT NULL\n    )\nLANGUAGE plpgsql AS $$\nBEGIN\n  EXECUTE format('\n    INSERT INTO table_2 (column_1,column_2,column_3,name,step)\n    SELECT one_column, two_column,three_column,%L,%s\n    FROM table_1',p_name,p_step);   \nEND;\n$$\n`\n```\n(Thanks to @jim-jones)\nHowever, in my 'actual' function, I also have an `INNER JOIN` in the `SELECT` which has something like the following condition...\n```\n`    ON (month_diff(d, now()::DATE) % month_interval) = 0\n`\n```\nI'm getting an error like unrecognized format() type specifier \" \" and I'm thinking do I need to escape the '`%`' sign and, if so, how? is it '`\\%`' or '`%%`' or something else? I've tried these and get the same failure.",
      "solution": "Your problem has to be somewhere else. `%%` does the trick:\n```\n`SELECT\n  format('%s%%',42),\n  format('%1s%%',42),\n  format('%2$s%% - %1$s%%',42,73); -- $ index for customized order\n\n format | format |  format   \n--------+--------+-----------\n 42%    | 42%    | 73% - 42%\n(1 row)\n`\n```\nThis is mentioned in the documentation:\n\nIn addition to the format specifiers described above, the special sequence `%%` may be used to output a literal `%` character.",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2021-07-01T10:49:45",
      "url": "https://stackoverflow.com/questions/68207158/how-to-escape-a-percent-sign-in-execute-formatblah"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "plpgsql",
      "question_id": 68193318,
      "title": "How do I make a function that calculates the correct offset based on a pagesize and a page number in PostgreSQL?",
      "problem": "I'm trying to make a function in PostgreSQL that takes a pageSize and a page Number as params and from that, calculates the offset. This makes paging a lot easier for me.\nThis is what I have so far:\n```\n`CREATE OR REPLACE FUNCTION calculate_paging(page_size INT, page_number INT)\n    RETURNS INT AS $calculate_paging$\nDECLARE\noffset INT;\nmax_int INT = 2147483647;\nBEGIN\n    offset = CASE WHEN CAST(page_size AS BIGINT) * (page_number - 1) > max_int \n                THEN max_int \n                ELSE page_size * (page_number - 1)\n    RETURN offset\nEND;\n$calculate_paging$ LANGUAGE plpgsql;\n`\n```\nThis does not work, I keep getting synthax errors. But the idea is to calculate the offset and return the offset value.",
      "solution": "Without considering the real purpose of the function and just focusing on the syntax errrors:\n```\n`CREATE OR REPLACE FUNCTION calculate_paging(page_size INT, page_number INT)\nRETURNS INT AS $$\nDECLARE max_int INT = 2147483647;\nBEGIN\n    IF page_size::bigint * (page_number - 1) > max_int THEN \n      RETURN max_int;\n    ELSE \n      RETURN page_size * (page_number - 1);\n    END IF;    \nEND;\n$$ LANGUAGE plpgsql;\n`\n```",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2021-06-30T12:46:15",
      "url": "https://stackoverflow.com/questions/68193318/how-do-i-make-a-function-that-calculates-the-correct-offset-based-on-a-pagesize"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "plpgsql",
      "question_id": 67957417,
      "title": "PL/pgSQL restart generated sequence",
      "problem": "I have an initialization script I use on Spring Boot application start and I create the table if it doesn't exist. Then I delete all the data from the table and execute a bunch on inserts.\n```\n`create table if not exists employee (\n    id serial primary key,\n    name varchar(255)\n);\n\ndelete from employee;\n\n-- inserts\n`\n```\nWith each execution of the script, the sequence still continues, so the new rows don't start from one. I have to reset such sequence too, however, it is generated and I dont know its name unless I call this script:\n```\n`select pg_get_serial_sequence('employee', 'id');\n-- returns public.employee_id_seq\n`\n```\nI tried to combine it together and reset the sequence based on the output of this funciton, but with no luck. How to reset the generated sequence without knowing its name? My attempt so far cannot resolve the `seq` sequence from the variable:\n```\n`do $$\n    declare\n        seq varchar(255);\n    begin\n        select pg_get_serial_sequence('employee', 'employee_id') into seq;\n        alter sequence seq restart with 1;\nend; $$;\n`\n```",
      "solution": "The simplest solution is to use `truncate table . . . restart identity` instead of `delete`:\n```\n`truncate table employee restart identity;\n`\n```\nHere is a db<>fiddle.\n`Truncate table` is recommended for other reasons too.  For instance, it reclaims the space the table used immediately.  And it is much faster.  The one difference is that `delete` triggers are not called (although your table doesn't have any triggers).",
      "question_score": 1,
      "answer_score": 1,
      "created_at": "2021-06-13T12:53:26",
      "url": "https://stackoverflow.com/questions/67957417/pl-pgsql-restart-generated-sequence"
    },
    {
      "tech": "postgresql",
      "source": "stackoverflow",
      "tag": "plpgsql",
      "question_id": 66652449,
      "title": "make_date function does not exist in plpgsql",
      "problem": "I've got a plpgsql function. I need to take the date 5 days from today, and then divide month into \"fives\" to takte the start of \"last five\". The problem is thay make_date does not exist in the posgres version that is used on the server....\n```\n`create or replace function getFirstDayOfFive()\nreturns timestamp with time zone as $$\ndeclare\n    firstDay timestamp;\n    startOp timestamp;\nbegin\nstartOp = now() - interval '5 day';\nSELECT\n    make_date(\n        date_part('year', startOp)::int,\n        date_part('month', startOp)::int,\n        greatest(\n            floor(date_part('day', startOp) / 5) * 5,\n            1\n        )::int\n    )\nINTO firstDay;\n\nRETURN firstDay;\n    \nend;\n\n$$\nlanguage plpgsql;\n`\n```\nIt worked fine last week, but now I got an error when I call it\n`ERROR: B\u0141\u0104D:  function make_date(integer, integer, integer) does not exist\nLINE 2:     make_date(\n            ^\nHINT:  There is no function matching provided name and arguments. Maybe you should cast data.\nQUERY:  SELECT\n    make_date(\n        date_part('year', startOp)::int,\n        date_part('month', startOp)::int,\n        greatest(\n            floor(date_part('day', startOp) / 5) * 5,\n            1\n        )::int\n    )\nCONTEXT:  PL/pgSQL function \"getfirstdayoffive\" line 7 at wyra\u017cenie SQL\n\nSQL state: 42883\n`\nWhat happened that earlier it worked and now it gives error?\n[Edit]\nI found out that make_date is available from postgresQL 9.4, but on the server there is posthresQL 9.1 is there any way to do the same in this old version od DB? I'm trying to replace the make_date with something like\n```\n`create or replace function getFirstDayOfFive()\nreturns timestamp with time zone as $$\ndeclare\n    firstDay timestamp;\n    startOp timestamp;\nbegin\nstartOp = now() - interval '5 day';\nSELECT\n    date to_char(startOp, 'YYYY-MM-')||to_char(greatest(\n            floor(date_part('day', startOp) / 5) * 5,\n            1\n        )::int)\nINTO firstDay;\nRETURN firstDay;  \nend;\n$$\nlanguage plpgsql;\n`\n```",
      "solution": "I think you can simplify this by simply adding the desired number of days to the start of the month. Apparently you only want a `date` so I would also recommend to change the return type to `date`\n```\n`create or replace function getfirstdayoffive()\nreturns date\nas\n$$\n   select date_trunc('month', current_date - 5)::date \n          + (greatest(floor(extract(day from current_date - 5) / 5) * 5, 1))::int - 1;\n$$    \nlanguage sql\nstable;\n`\n```",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2021-03-16T10:32:45",
      "url": "https://stackoverflow.com/questions/66652449/make-date-function-does-not-exist-in-plpgsql"
    }
  ]
}