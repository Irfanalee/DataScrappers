{
  "tech": "gcp",
  "count": 331,
  "examples": [
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 72224454,
      "title": "Execution failed for task &#39;:app:mapDebugSourceSetPaths&#39;. &gt; Error while evaluating property &#39;extraGeneratedResDir&#39; of task",
      "problem": "I've checked this answer:\nhttps://stackoverflow.com/a/34834772/13519865\nIt tells us to remove this line\n```\n`apply plugin: 'com.google.gms.google-services'\n`\n```\nRemoving the line as asked completes the build, but I can't use Firebase (ofc!), it caused a new error, which tells me to add the line:\nhttps://stackoverflow.com/a/40085096/13519865\nSo, I'm stuck in a loop here.\nRelated code sample added here https://github.com/Cyberavater/A.Reader",
      "solution": "Edit: April 15rd, 2024\nThe same issue seems to appear with the latest Android Studio Iguana | 2023.2.1 where you need to specify the Gradle version to be `8.3.2` and Google Services `4.4.1`. So here are the working versions:\n```\n`plugins {\n    id 'com.android.application' version '8.3.2' apply false\n    id 'com.android.library' version '8.3.2' apply false\n}\n\ndependencies {\n    classpath 'com.google.gms:google-services:4.4.1'\n}\n`\n```\n\nEdit: October 16rd, 2023\nThe same issue seems to appear with the latest Android Studio Electric Giraffe 2022.3.1 Patch 3 where you need to specify the Gradle version to be `8.1.3` and Google Services `4.4.0`. I have tried to see if the `5.0.0` version works but I got:\n\nPlugin [id: 'com.google.gms.google-services', version: '5.0.0', apply: false] was not found in any of the following sources:\n\nSo here are the working versions:\n```\n`plugins {\n    id 'com.android.application' version '8.1.3' apply false\n    id 'com.android.library' version '8.1.3' apply false\n}\n\ndependencies {\n    classpath 'com.google.gms:google-services:4.4.0'\n}\n`\n```\n\nEdit: August 15rd, 2023\nThe same issue seems to appear with the latest Android Studio Electric Giraffe where you need to specify the Gradle version to be `8.1.0` and Google Services `4.3.15`. I have not tested if it's working with `5.0.0`.\n```\n`plugins {\n    id 'com.android.application' version '8.1.0' apply false\n    id 'com.android.library' version '8.1.0' apply false\n}\n\ndependencies {\n    classpath 'com.google.gms:google-services:4.3.15'\n}\n`\n```\n\nEdit: March 24rd, 2023\nThe same issue seems to appear with the latest Android Studio Electric Eel where you need to specify the Gradle version to be `7.4.0` and Google Services `4.3.15`.\n```\n`plugins {\n    id 'com.android.application' version '7.4.0' apply false\n    id 'com.android.library' version '7.4.0' apply false\n}\n\ndependencies {\n    classpath 'com.google.gms:google-services:4.3.15'\n}\n`\n```\n\nEdit: January 4th, 2023\nI have seen the same issue with the latest Android Studio Dolphin release, where you need to specify the Gradle version to be `7.3.1` and Google Services `4.3.14`.\n```\n`plugins {\n    id 'com.android.application' version '7.3.1' apply false\n    id 'com.android.library' version '7.3.1' apply false\n}\n\ndependencies {\n    classpath 'classpath \"com.google.gms:google-services:4.3.14'\n}\n`\n```\n\nAccording to the new Chipmunk update of Android Studio, if you need to use Google Services, you have to add the following lines of code inside your build.gradle (Project) file:\n```\n`plugins {\n    id 'com.android.application' version '7.3.1' apply false\n    id 'com.android.library' version '7.3.1' apply false\n    id 'com.google.gms.google-services' version '4.3.14' apply false \ud83d\udc48\n}\n\ntask clean(type: Delete) {\n    delete rootProject.buildDir\n}\n`\n```\nAnd inside your build.gradle (Module) file, the following plugin IDs:\n```\n`plugins {\n    id 'com.android.application'\n    id 'com.google.gms.google-services' \ud83d\udc48\n}\n`\n```",
      "question_score": 192,
      "answer_score": 193,
      "created_at": "2022-05-13T06:28:21",
      "url": "https://stackoverflow.com/questions/72224454/execution-failed-for-task-appmapdebugsourcesetpaths-error-while-evaluatin"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 75454425,
      "title": "Access Blocked: &lt;project&gt; has not completed the Google verification process",
      "problem": "I am building a simple script which polls some data and then updates a spreadsheet that I am giving to my client.  (It is a small project and I don't need anything fancy.)\nSo I created a Google Cloud project, enabled the Sheets API, and got a credential for a Desktop app.  When I try to run the quickstart sample, I get an error:\nAccess blocked:  has not completed the Google verification process\nI have tried googling and all the solutions seem to be oriented toward what a user should do if they see this, but I am the developer.   I only need to grant my own self access to this spreadsheet, since my script is the only thing that will be changing it (I will also share it with the client).\nWhat do I do?",
      "solution": "You need to add the account as a test user under the OAuth consent screen:\n1.) From the dashboard go to APIs & Services, click OAuth concent screen, then Audience\n\n2.) Under the Test users, click +Add Users. A menu will prompt on the right panel.\n\n3.) Input the users email\n\n4.) Reload the URL provided.\nReference: https://www.youtube.com/watch?v=bkZns_VOB6I\nNote: I am not affiliated with the video nor the owner of the youtube channel",
      "question_score": 49,
      "answer_score": 122,
      "created_at": "2023-02-15T01:14:24",
      "url": "https://stackoverflow.com/questions/75454425/access-blocked-project-has-not-completed-the-google-verification-process"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 66127933,
      "title": "Cloud Run: &quot;Failed to start and then listen on the port defined by the PORT environment variable.&quot; When I use 8080",
      "problem": "I got this error message when I try to run my container in Google Cloud Run.\n```\n`type: Ready\nstatus: 'False'\nreason: HealthCheckContainerError\nmessage: |-\nCloud Run error: Container failed to start. Failed to start and then listen on the port defined by the PORT environment variable. Logs for this revision might contain more information.\n`\n```\nI already checked the followings but nothing helped to me:\n\nhttps://cloud.google.com/run/docs/troubleshooting\n\nCloud Run error: Container failed to start\n\nMy container is running locally and it's listening on default PORT `8080` with HOST configured as `0.0.0.0`.\nMy Dockerfile:\n```\n`FROM node:10\n\nWORKDIR /usr/src/app\n\nENV PORT 8080\nENV HOST 0.0.0.0\n\nCOPY package*.json ./\n\nRUN npm install --only=production\n\nCOPY . .\n\nRUN npm run build\n\nCMD npm start\n`\n```\nAny idea on why Cloud Run keeps failing to listen on the port?\n\nProject GitHub repo:\nhttps://github.com/fodorpapbalazsdev/ssr-app",
      "solution": "Just to check, are you using the M1 Macbook? I found a solution for myself after facing this issue for some time, might not be the solution for you but just to share some insights I found for other MacBook users.\ntl;dr\n\nbuild your Docker container with the `--platform linux/amd64` flag before deploying the image to Cloud Run\n\n========================================================\nLong story:\nAside from the `container failed to start and listen to the $PORT` error, my logs were showing the following: `Application failed to start: Failed to create init process: failed to load /usr/local/bin/npm: exec format error`. Upon some digging, one of the reasons this happens is that we are trying to run an arm64 image (built on M1 MacBook) on a different host platform.\nGCP does mention on this page here that `Executables in the container image must be compiled for Linux 64-bit. Cloud Run specifically supports the Linux x86_64 ABI format.`\nI guess that explains why building the image on Cloud Build works from the other answer in this post.",
      "question_score": 45,
      "answer_score": 195,
      "created_at": "2021-02-09T23:06:49",
      "url": "https://stackoverflow.com/questions/66127933/cloud-run-failed-to-start-and-then-listen-on-the-port-defined-by-the-port-envi"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 74757406,
      "title": "Can&#39;t deploy Firebase functions (Failed to fetch Run service undefined)",
      "problem": "Can't deploy Firebase functions. I have two project aliases, it's working fine for the first project (dev), but not for the second (prod).\nWhenever I write `firebase deploy --only functions` I get the following message\n```\n`i deploying functions\ni  functions: ensuring required API cloudfunctions.googleapis.com is enabled...\ni  functions: ensuring required API cloudbuild.googleapis.com is enabled...\ni  artifactregistry: ensuring required API artifactregistry.googleapis.com is enabled...\n+  functions: required API cloudbuild.googleapis.com is enabled\n+  artifactregistry: required API artifactregistry.googleapis.com is enabled\n+  functions: required API cloudfunctions.googleapis.com is enabled\ni  functions: preparing codebase default for deployment\n!  functions: package.json indicates an outdated version of firebase-functions. Please upgrade \nusing npm install --save firebase-functions@latest in your functions directory.\n!  functions: Please note that there will be breaking changes when you upgrade.\ni  functions: Loaded environment variables from .env.prod.\n!  functions: You are using an old version of firebase-functions SDK (3.15.7). Please update \nfirebase-functions SDK to >=3.20.0\ni  functions: preparing functions directory for uploading...\ni  functions: packaged E:\\FlutterProjects\\pegasus\\functions (218.61 KB) for uploading\n\nError: Failed to fetch Run service undefined\n`\n```",
      "solution": "I had the same issue and it was fixed for me when installed the latest firebase tools\nnpm install -g firebase-tools",
      "question_score": 39,
      "answer_score": 58,
      "created_at": "2022-12-11T00:53:21",
      "url": "https://stackoverflow.com/questions/74757406/cant-deploy-firebase-functions-failed-to-fetch-run-service-undefined"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 71922754,
      "title": "Google Cloud Platform Logging: How to search wildcard strings in all logs",
      "problem": "How does one do a simple wildcard search in all the logs? I hate GCP's query language and their documentation is just confusing.\nI tried:\n```\n`text:*MY_STRING_TO_SEARCH_FOR*\n`\n```\nDoesn't work.",
      "solution": "Use regex instead: `text=~\".*MY_STRING_TO_SEARCH_FOR.*\"`",
      "question_score": 36,
      "answer_score": 60,
      "created_at": "2022-04-19T11:36:22",
      "url": "https://stackoverflow.com/questions/71922754/google-cloud-platform-logging-how-to-search-wildcard-strings-in-all-logs"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 66281882,
      "title": "How can I get `terraform init` to run on my Apple Silicon Macbook Pro for the Google Provider?",
      "problem": "When I run `terraform init` for my Google Cloud Platform project on my Apple Silicon macbook pro I get this error.\n```\n`Provider registry.terraform.io/hashicorp/google v3.57.0 does not have a package available for your current platform, darwin_arm64.\n`\n```\nHow can I work around this? I thought that the Rosetta2 emulator would check this box, but alas...",
      "solution": "Most providers already have packages available in newer versions.\nYou can update the provider via:\n`terraform init -upgrade`\nIf this route is not acceptable for you or if it does not solve the problem, look at the answer below.\nBuild Terraform's GCP provider from scratch! I modified this walkthrough. https://github.com/hashicorp/terraform/issues/27257#issuecomment-754777716\n```\n`brew install --build-from-source terraform\n`\n```\nThis will install Golang as well (and that appears to be working as of this post)\n```\n`git clone https://github.com/hashicorp/terraform-provider-google.git\ncd terraform-provider-google\ngit checkout v3.22.0\ngo get -d github.com/pavius/impi/cmd/impi\nmake tools\ngo fmt\nmake build\n`\n```\nThe following directory probably does not already exist so lets create it and copy the binary we just built.\n```\n`mkdir -p ${HOME}/.terraform.d/plugins/registry.terraform.io/hashicorp/google/3.22.0/darwin_arm64\ncp ${HOME}/go/bin/terraform-provider-google ${HOME}/.terraform.d/plugins/registry.terraform.io/hashicorp/google/3.22.0/darwin_arm64\n`\n```\nNote that `${HOME}/go` is where your golang install will be located if you don't already have `${GOPATH}` already defined. If you do, then modify the above commands to account for the location of your new build binaries.\n```\n`cp ${GOPATH}/bin/terraform-provider-google ${HOME}/.terraform.d/plugins/registry.terraform.io/hashicorp/google/3.22.0/darwin_arm64\n`\n```\nAfter going back to my project voila!\n```\n`\u279c terraform init\n\nInitializing the backend...\n\nInitializing provider plugins...\n- Finding latest version of hashicorp/google...\n- Installing hashicorp/google v3.22.0...\n- Installed hashicorp/google v3.22.0 (unauthenticated)\n\nTerraform has created a lock file .terraform.lock.hcl to record the provider\nselections it made above. Include this file in your version control repository\nso that Terraform can guarantee to make the same selections by default when\nyou run \"terraform init\" in the future.\n\nTerraform has been successfully initialized!\n\nYou may now begin working with Terraform. Try running \"terraform plan\" to see\nany changes that are required for your infrastructure. All Terraform commands\nshould now work.\n\nIf you ever set or change modules or backend configuration for Terraform,\nrerun this command to reinitialize your working directory. If you forget, other\n\n`\n```",
      "question_score": 33,
      "answer_score": 17,
      "created_at": "2021-02-19T18:00:10",
      "url": "https://stackoverflow.com/questions/66281882/how-can-i-get-terraform-init-to-run-on-my-apple-silicon-macbook-pro-for-the-go"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 68779751,
      "title": "Error publishing source code from cloud build to a bucket using triggers",
      "problem": "I\u00b4m trying to publish the html code from one cloud source repository to a public storage bucket in gcp through a cloud build trigger . However , I get the following error in the build each time I push to the master branch.\n```\n`generic::invalid_argument: generic::invalid_argument: if 'build.service_account' is specified, the build must either (a) specify 'build.logs_bucket' (b) use the CLOUD_LOGGING_ONLY logging option, or (c) use the NONE logging option\n`\n```\nI am using the following cloudbuild.yaml\n```\n`steps:\n  - name: gcr.io/cloud-builders/gsutil\n    args: [\"-m\", \"rsync\", \"-r\", \"-c\", \"-d\", \".\", \"gs://somedomain.com\"]\n`\n```\nI think this is related with the service account associated with the cloud build .\nThe tutorial I\u00b4m following for this solution is here : https://cloud.google.com/community/tutorials/automated-publishing-cloud-build",
      "solution": "The error was solved adding the logs specification at the end of the cloudbuild.yaml and enabling the IAM API .\nThe bucket and the cloud build configuration where in the same project so I didn\u00b4t have the need to grant additional roles to the cloud build service account .\nstraight under the steps put:\n```\n`options:\n  logging: CLOUD_LOGGING_ONLY\n`\n```",
      "question_score": 33,
      "answer_score": 58,
      "created_at": "2021-08-14T03:50:27",
      "url": "https://stackoverflow.com/questions/68779751/error-publishing-source-code-from-cloud-build-to-a-bucket-using-triggers"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 73626841,
      "title": "google-github-actions/auth failed with did not inject $ACTIONS_ID_TOKEN_REQUEST_TOKEN or $ACTIONS_ID_TOKEN_REQUEST_URL",
      "problem": "In github actions I'm running an action that is trying to use github to GCP federated id:\n```\n`     # see https://github.com/marketplace/actions/authenticate-to-google-cloud#setup\n  - id: 'auth'\n    name: 'Authenticate to Google Cloud'\n    uses: 'google-github-actions/auth@v0'\n    with:\n      workload_identity_provider: 'projects/1234/locations/global/workloadIdentityPools/my-github-pool/providers/my-github-oidc-provider'\n      service_account: 'my-github-sa@projxyz.iam.gserviceaccount.com'\n`\n```\nI'm getting:\n```\n`Run google-github-actions/auth@v0\n\nError: google-github-actions/auth failed with: retry function failed after 1 attempt: \ngitHub Actions did not inject $ACTIONS_ID_TOKEN_REQUEST_TOKEN or \n$ACTIONS_ID_TOKEN_REQUEST_URL into this job. \nThis most likely means the GitHub Actions workflow permissions are incorrect, or this job is being run from a fork. \nFor more information, please see https://docs.github.com/en/actions/security-guides/automatic- \ntoken-authentication#permissions-for-the-github_token\n`\n```\nI'm looking at the referenced doc but I'm not seeing anything useful.\nHow to I get GH to inject those values?",
      "solution": "I needed to add:\n```\n`jobs:\n   my_job:\n   # Need to add these 3 lines to add \"id-token\" with the intended permissions.\n   permissions:\n     contents: 'read'\n     id-token: 'write'\n`\n```\nThis is documented here: https://github.com/google-github-actions/auth#usage",
      "question_score": 32,
      "answer_score": 50,
      "created_at": "2022-09-06T21:20:50",
      "url": "https://stackoverflow.com/questions/73626841/google-github-actions-auth-failed-with-did-not-inject-actions-id-token-request"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 72251787,
      "title": "Permission &quot;artifactregistry.repositories.downloadArtifacts&quot; denied on resource",
      "problem": "While the artifact repository was successfully creating, running a docker push to push the image to the google artifact registry fails with a permissions error even after granting all artifact permissions to the accounting I am using on gcloud cli.\nCommand used to push image:\n```\n`docker push us-central1-docker.pkg.dev/project-id/repo-name:v2\n`\n```\nError message:\n```\n`The push refers to repository [us-central1-docker.pkg.dev/project-id/repo-name]\n6f6f4a472f31: Preparing\nbc096d7549c4: Preparing\n5f70bf18a086: Preparing\n20bed28d4def: Preparing\n2a3255c6d9fb: Preparing\n3f5d38b4936d: Waiting\n7be8268e2fb0: Waiting\nb889a93a79dd: Waiting\n9d4550089a93: Waiting\na7934564e6b9: Waiting\n1b7cceb6a07c: Waiting\nb274e8788e0c: Waiting\n78658088978a: Waiting\ndenied: Permission \"artifactregistry.repositories.downloadArtifacts\" denied on resource \"projects/project-id/locations/us-central1/repositories/repo-name\" (or it may not exist)\n\n`\n```",
      "solution": "I was able to recreate your use case. This happens when you are trying to push an image on a `repository` in which its specific hostname (associated with it's repository location) is not yet  added to the credential helper configuration for authentication. You may refer to this Setting up authentication for Docker  as also provided by @DazWilkin in the comments for more details.\nIn my example, I was trying to push an image on a repository that has a location of `us-east1` and got the same error since it is not yet added to the credential helper configuration.\n\nAnd after I ran the authentication using below command (specifically for us-east1 since it is the `location` of my repository), the image was successfully pushed:\n```\n`gcloud auth configure-docker us-east1-docker.pkg.dev\n`\n```\n\nQUICK TIP: You may  get your authentication command specific for your repository when you open your desired repository in the console, and then click on the `SETUP INSTRUCTIONS`.",
      "question_score": 31,
      "answer_score": 63,
      "created_at": "2022-05-15T22:00:57",
      "url": "https://stackoverflow.com/questions/72251787/permission-artifactregistry-repositories-downloadartifacts-denied-on-resource"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 66764890,
      "title": "Google OAuth2.0 allows users NOT in list of test users",
      "problem": "I'm developing a webapp which allows users to log in with their Google accounts, using OAuth2.0.\nI've created an OAuth2.0 client ID, configured the OAuth consent screen with the Publishing status set to 'Testing', and added a test user.\nThe frontend of my app is built with React, and I'm using a package (react-google-login) to handle the flow. I can successfully sign in with the Google account I added as a test user, and retrieve the basic profile information needed.\nThe problem is I can also sign in with other Google accounts, which have not been added to the list of test users. I imagine that Google should simply not issue access tokens for accounts which are not in the list of test users.\nI feel like I've misunderstood something about the OAuth process, or I have configured something incorrectly. I would appreciate if anyone had any pointers?\nThanks.",
      "solution": "It is indeed bugged.\nI was in the same spot as you, assuming I had misunderstood something. After reviewing my code over and over with no luck, I made a Stack Overflow post, in which I was advised to post to Google's bug tracking system. After doing some troubleshooting with Google they confirmed the bug, and they are now working to fix it (for a little while already).\nI included this thread as an example when talking to Google. I meant to post an update here after getting in touch with them, but I forgot, sorry!\nThe buganizer thread with more details:\nhttps://issuetracker.google.com/issues/211370835",
      "question_score": 22,
      "answer_score": 22,
      "created_at": "2021-03-23T15:11:12",
      "url": "https://stackoverflow.com/questions/66764890/google-oauth2-0-allows-users-not-in-list-of-test-users"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 76823978,
      "title": "Error: 10: Developer console is not set up correctly (Not Using Firebase) (One Tap sign-up)",
      "problem": "I'm trying to implement Google's One Tap sign-up for my Android app in development. I am following Google's (quite outdated) official guide and so far everything has gone well, up until display of the sign-up UI where the onFailure listener returns an error when I build and run my app:\n```\n`com.google.android.gms.common.api.ApiException: 10: Developer console is not set up correctly.\n`\n```\nHere's my relevant code:\n`        oneTapClient = Identity.getSignInClient(this);\n        signUpRequest = BeginSignInRequest.builder()\n                .setAutoSelectEnabled(true)\n                .setGoogleIdTokenRequestOptions(BeginSignInRequest.GoogleIdTokenRequestOptions.builder()\n                        .setSupported(true)\n                        .setServerClientId(getString(R.string.web_client_id))\n                        .setFilterByAuthorizedAccounts(false)\n                        .build())\n                .build();\n\n        // ActivityResultLauncher {\n            IntentSenderRequest intentSenderRequest = new IntentSenderRequest.Builder(result.getPendingIntent().getIntentSender()).build();\n            activityResultLauncher.launch(intentSenderRequest);\n        }).addOnFailureListener(this, e -> {\n            Log.d(\"/////\", \"Failed: \" + e);\n        }).addOnCanceledListener(this, () -> {\n            Log.d(\"/////\", \"Canceled.\");\n        });\n`\nNow, as I have mentioned, I'm following the official guide and I haven't skipped a single step so the first thing I did was Google and research the error. It's at this point that I want to mention, I've reviewed every single answer to the following SO questions:\n1 2 3 4 5 6 7 8 9 10\nThere are several, and I mean several duplicated answers that make it such a pain to find one that helps, people make a big deal about duplicated questions but I think these duplicated answers are really what needs moderation. Oh, and a lot of the answers that include code were usually deprecated and for the Legacy Google Sign In, or some other Google API and not One Tap (I still tried to see if the solutions would work regardless but to no avail).\nTo save everyone's time, I will address two answers that came up over and over again during my research.\n\nThe client ID I input is the 'Web' client ID and not the Android client ID.\nSHA-1 was not asked for when making the web client ID but it was when making the android client, I did successfully create and input the correct SHA-1 as per this official guide and this (and more guides for fact-checking).\n\nSome useful information to note: I only have one project, I do not have any variants other than the debug, and I am not using Firebase, this happens every time I run the app on my test device, the application ID is the same as the package name in the manifest, I've also tried regenerating the signed bundle.\nIf you have any ideas why I may be getting this error please let me know! It's been two days and I feel like I'm out of options...",
      "solution": "Context: I put so much time and effort tinkering at my code and Google cloud console, and searching all over the internet to find a damn solution, I asked questions in SO (this one) and GCC and got no response, I then made a new project and copied exactly what I saw on a video tutorial, there was nothing unexpected but the new project did work so I have no idea what went wrong in my project.\nAnswer: I didn't find any solutions so I gave up. I shut down my computer, went to sleep, and when I woke up I noticed my SHA-1 key had reset...\nI updated the cloud console, ran the code, and it just worked. I'm kinda pissed.\nUsually, I'd delete a question like this, but for anyone else who's in dire need like I was, here's what worked for me \"turn it off and on\".",
      "question_score": 19,
      "answer_score": 17,
      "created_at": "2023-08-03T00:58:27",
      "url": "https://stackoverflow.com/questions/76823978/error-10-developer-console-is-not-set-up-correctly-not-using-firebase-one-t"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 66536427,
      "title": "Incorrect Service Networking config for instance: xxxx:SERVICE_NETWORKING_NOT_ENABLED",
      "problem": "I'm trying to replicate a SQL instance in GCP via terraform. The active instance has a public IP, however subnets from a secondary project are shared with the project hosing the SQL instance, and the SQL instance is associated with the secondary project's network.\nI've added the `private_network` setting properly (I think) in the `ip_configuration` section, however I'm getting the following error:\n\nError: Error, failed to create instance xxxx: googleapi: Error 400: Invalid request: Incorrect Service Networking config for instance: xxxx:xxxxx:SERVICE_NETWORKING_NOT_ENABLED., invalid\n\nI can't find much documentation when I google that particular error, and I'm relatively new to Terraform, so I'm hoping someone can point out what I'm missing from either this section of my Terraform config, or another resource altogether.\n```\n`resource \"google_sql_database_instance\" \"cloudsql-instance-qa\" {\n  depends_on       = [google_project_service.project_apis]\n  database_version = \"MYSQL_5_7\"\n  name             = \"${var.env_shorthand}-${var.resource_name}\"\n  project          = var.project_id\n  region           = var.region\n\n  settings {\n    activation_policy = \"ALWAYS\"\n    availability_type = \"ZONAL\"\n\n    backup_configuration {\n      binary_log_enabled             = \"true\"\n      enabled                        = \"true\"\n      point_in_time_recovery_enabled = \"false\"\n      start_time                     = \"15:00\"\n    }\n\n    crash_safe_replication = \"false\"\n    disk_autoresize        = \"true\"\n    disk_size              = \"5003\"\n    disk_type              = \"PD_SSD\"\n\n    ip_configuration {\n      ipv4_enabled    = \"true\"\n      private_network = \"projects/gcp-backend/global/networks/default\"\n      require_ssl     = \"false\"\n    }\n\n    location_preference {\n      zone = var.zone\n    }\n\n    maintenance_window {\n      day  = \"7\"\n      hour = \"4\"\n    }\n\n    pricing_plan     = \"PER_USE\"\n    replication_type = \"SYNCHRONOUS\"\n    tier             = \"db-n1-standard-1\"\n  }\n}\n`\n```",
      "solution": "If you see the following error:\n\nError: Error, failed to create instance xxxx: googleapi: Error 400:\nInvalid request: Incorrect Service Networking config for instance:\nxxxx:xxxxx:SERVICE_NETWORKING_NOT_ENABLED., invalid\n\nEnable the Service Networking API:\n```\n`gcloud services enable servicenetworking.googleapis.com --project=[PSM_PROJECT_NUMBER]\n`\n```\nGetting Started with the Service Networking API",
      "question_score": 19,
      "answer_score": 37,
      "created_at": "2021-03-08T21:07:21",
      "url": "https://stackoverflow.com/questions/66536427/incorrect-service-networking-config-for-instance-xxxxservice-networking-not-en"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 71561730,
      "title": "Authorizing client libraries without access to a web browser - gcloud auth application-default login",
      "problem": "When I use to run either command:\n` gcloud auth application-default login`\nOR for a specific docker container\n`docker exec -it 822c4c491383 /home/astro/google-cloud-sdk/bin/gcloud auth application-default login`.\nMy command line would give me a link to a google response page where I'd copy the code they gave me and write it in the command line.\nFor some reason now, whenever I try to do either command I'm getting the follow error, saying I don't have access to web browser.\n\nYou are authorizing client libraries without access to a web browser. Please run the following command on a machine with a web browser and\ncopy its output back here. Make sure the installed gcloud version is\n372.0.0 or newer.\ngcloud auth application-default login --remote-bootstrap=\"https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=764086051850-6qr4p6gpi6hn506pt8ejuq83di341hur.apps.googleusercontent.com&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Faccounts.reauth&state=FmMFY6gvpOa9xndMXmWiNG3W1jDrCe&access_type=offline&code_challenge=zUI4n_pnYE5V7p0diDQLmL0X0Sk8XpTDzhz_vwtukOo&code_challenge_method=S256&token_usage=remote\"\n\nI've tried copying the link that's inside of this and place it in my web browser but I get a page saying.\n\nError 400: invalid request Missing required parameter:  redirect uri\n\nEdit: Though not sure why this is happening now, I added the option \"--no-launch-browser\" to the end of both commands and it gives me the link to place in my browser now manually and copy code.",
      "solution": "On versions of `gcloud >= 383.0.0` (26 Apr 2022), Google have removed support for the `--console-only` and `--no-launch-browser` flags on their CLI. As far as I can see, they do not give a reason for this, but it is likely security related.\nThe new intended method for authenticating on a machine without a web browser, is to use the `--no-browser` flag and copy the command it gives you onto a machine that has both `gcloud >= 372.0` and a web browser installed. In other words, it is no longer possible to do this purely on a machine with no browser. See the following steps copied directly from their documentation:\n\nFollow these steps:\n\nCopy the long command that begins with `gcloud auth login --remote-bootstrap=\"`.\nPaste and run this command on the command line of a different, trusted machine that has local installations of both a web browser and the gcloud CLI version 372.0 or later.\nCopy the long URL output from the machine with the web browser.\nPaste the long URL back to the first machine under the prompt, `Enter the output of the above command`, and press `Enter` to complete the authorization.",
      "question_score": 17,
      "answer_score": 10,
      "created_at": "2022-03-21T18:30:36",
      "url": "https://stackoverflow.com/questions/71561730/authorizing-client-libraries-without-access-to-a-web-browser-gcloud-auth-appli"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 65821436,
      "title": "Programmatically get current Service Account on GCP",
      "problem": "Is there a way to programmatically access the email of the currently used Service Account on a GCP instance when no `GOOGLE_APPLICATION_CREDENTIALS` is set? (ie. when using the default Service Account)\nI've looked through the GCP documentation, but the only resource I found can't be used with the default Service Account when no `GOOGLE_APPLICATION_CREDENTIALS` is set. I know that it is possible to do so using `gcloud` (see this SO question or documentation), however these solutions aren't applicable when running on a ContainerOptimisedOS.  I've spent a couple of weeks back and forth with the GCP support team, but they concluded with not being able to help me and redirected me to Stack Overflow for help.",
      "solution": "The solution of John works great, on any language without any external library. However, it works only in Google Cloud environment, when a metadata server is deployed. You can't perform this test on your computer.\nI propose just bellow a piece of Python code (with Google OAuth library, but it works in other languages that have this library) to ask the library the current credential. If the credential is a service account (from `GOOGLE_APPLICATION_CREDENTIALS` on your computer, the ADC (Application Default Credential) or from the metadata server), you have the email printed, else you have a warning message because you use your user account credential:\n`import google.auth\n\ncredentials, project_id = google.auth.default()\n\nif hasattr(credentials, \"service_account_email\"):\n    print(credentials.service_account_email)\nelse:\n    print(\"WARNING: no service account credential. User account credential?\")\n`\nNote that if the default service account is used this method will print `default` instead of the entire email address.\nThe Go version:\n`ctx := context.Background()\ncredential,err := google.FindDefaultCredentials(ctx)\ncontent := map[string]interface{}{}\n\njson.Unmarshal(credential.JSON, &content)\nif content[\"client_email\"] != nil {\n    fmt.Println(content[\"client_email\"])\n} else {\n    fmt.Println(\"WARNING: no service account credential. User account credential?\")\n}\n`",
      "question_score": 16,
      "answer_score": 17,
      "created_at": "2021-01-21T06:01:04",
      "url": "https://stackoverflow.com/questions/65821436/programmatically-get-current-service-account-on-gcp"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 73734679,
      "title": "How to generate Google-managed certificates for wildcard hostnames in GCP?",
      "problem": "I would like to use Google-managed certificates for wildcard hostnames\u2014for example, `*.example.com`.\nI know I could use Certificate Manager from the doc to do that in GCP, but I cannot find a concrete example.\nCould you please show me some examples?",
      "solution": "Thanks @James' s answer. I also reference this doc.\nI would like to post a concret example for this question in case anyone have the same doubts as me.\nMost of the steps could reference to the doc, I would like to point out two steps you need to take care of.\nIn `Create a Google-managed certificate referencing the DNS authorization` step, provide your single domain and your wildcard hostname in the `--domains` options.\n`gcloud certificate-manager certificates create \"my-cert\" \\\n    --domains=\"example.com,*.example.com\" \\\n    --dns-authorizations=my-dns-auth\n`\nIn `Create a certificate map entry` step, create two entries for both single hostname and wildcard hostname.\n`gcloud certificate-manager maps entries create \"my-entry1\" \\\n    --map=my-map \\\n    --certificates=my-cert \\\n    --hostname=\"example.com\"\n`\n`gcloud certificate-manager maps entries create \"my-entry2\" \\\n    --map=my-map \\\n    --certificates=my-cert \\\n    --hostname=\"*.example.com\"\n`",
      "question_score": 15,
      "answer_score": 23,
      "created_at": "2022-09-15T18:36:23",
      "url": "https://stackoverflow.com/questions/73734679/how-to-generate-google-managed-certificates-for-wildcard-hostnames-in-gcp"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 71400016,
      "title": "Trying to add support for Docker to existing NextJS project but getting &quot;stat app/.next/standalone: file does not exist&quot; error",
      "problem": "I am following this guide to add Docker support to my existing NextJS + TypeScript project and deploy to Google Cloud Run: https://github.com/vercel/next.js/tree/canary/examples/with-docker.\nHowever, the build your container step:\n```\n`docker build -t nextjs-docker .\n`\n```\nkeeps failing because it's giving me an error that \"stat app/.next/standalone: file does not exist\" error.\nI looked into my .next folder and no standalone file is being generated which is why I'm getting that error. How do I get this .next/standalone file to get created?\nMy next.config.js file looks like this:\n```\n`module.exports = {\n  eslint: {\n    ignoreDuringBuilds: true,\n  },\n  experimental: {\n    outputStandalone: true\n  }\n}\n`\n```\nMy Dockerfile looks like this:\n```\n`# Install dependencies only when needed\nFROM node:16-alpine AS deps\nRUN apk add --no-cache libc6-compat\nWORKDIR /app\n\nCOPY package.json package-lock.json tsconfig.json ./ \nRUN npm ci\n\n# Rebuild the source code only when needed\nFROM node:16-alpine AS builder\nWORKDIR /app\nCOPY --from=deps /app/node_modules ./node_modules\nCOPY . .\n\nRUN npm run build\n\n# Production image, copy all the files and run next\nFROM node:16-alpine AS runner\nWORKDIR /app\n\nENV NODE_ENV production\n\nRUN addgroup --system --gid 1001 nodejs\nRUN adduser --system --uid 1001 nextjs\n\n# You only need to copy next.config.js if you are NOT using the default configuration\nCOPY --from=builder /app/next.config.js ./\nCOPY --from=builder /app/public ./public\nCOPY --from=builder /app/package.json ./package.json\n\n# Automatically leverage output traces to reduce image size \n# https://nextjs.org/docs/advanced-features/output-file-tracing\n\n# Following line is giving me the error\n# NOTE: I can not just comment out this line because it will give an error later that \"Cannot find module '/app/server.js'\"\nCOPY --from=builder --chown=nextjs:nodejs /app/.next/standalone ./\nCOPY --from=builder --chown=nextjs:nodejs /app/.next/static ./.next/static\n\nUSER nextjs\n\nEXPOSE 3000\n\nENV PORT 3000\n\nCMD [\"node\", \"server.js\"]\n`\n```\nMy .next folder looks like:\n```\n`- .next\n  - cache\n  - server\n  - static\n  - traces\n  - BUILD_ID\n  - build-manifest.json\n  - export.marker.json\n  - images-manifest.json\n  - prerender-manifest.json\n  - react-loadasble-manifest.json\n  - required-server-files.json\n  - routes-manifest.json\n  - trace\n`\n```\nMy understanding was that adding:\n```\n`experimental: {\n  outputStandalone: true\n}\n`\n```\nto next.config.js file and then running `npm run build` would create the .next/standalone file but seems to be not working.",
      "solution": "In case this is helpful to anyone else, turns out my version for \"next\" was set to \"^11.1.0\" and the standalone folder only works for \"next\" versions \"^12.1.0\" and above. Updating my package.json fixed the problem!",
      "question_score": 15,
      "answer_score": 8,
      "created_at": "2022-03-08T19:40:46",
      "url": "https://stackoverflow.com/questions/71400016/trying-to-add-support-for-docker-to-existing-nextjs-project-but-getting-stat-ap"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 70674928,
      "title": "Terraform/GCP Error: project: required field is not set",
      "problem": "Problem\nThe google_project document says the project_id is optional.\n\nproject_id - (Optional) The project ID. If it is not provided, the provider project is used.\n\nHowever, Terraform complains it is required.\ngcp.tf\n```\n`data \"google_project\" \"project\" {\n}\n\noutput \"project_number\" {\n  value = data.google_project.project.number\n}\n`\n```\n```\n` Error: project: required field is not set\n\u2502 \n\u2502   with data.google_project.project,\n\u2502   on gcp.tf line 1, in data \"google_project\" \"project\":\n\u2502    1: data \"google_project\" \"project\" {\n`\n```\nQuestion\nPlease help understand if this is a documentation defect and the argument is mandatory actually.\nWorkaround\nSet the GOOGLE_PROJECT environment variable.\n```\n`export GOOGLE_PROJECT=...\nterraform apply\n`\n```",
      "solution": "Your 'Workaround' is functionally equivalent to what the documentation suggests. Namely that the `provider` `project` should be set, i.e.:\n```\n`provider \"google\" {\n  project = \"...\"\n}\n`\n```\nYou don't include your `provider` config but, I assume, it doesn't include the default `project` to be used.\nSo, either|or but, somewhere you need to define the default project.\nOtherwise, you should expect to get the error.",
      "question_score": 15,
      "answer_score": 24,
      "created_at": "2022-01-12T01:15:23",
      "url": "https://stackoverflow.com/questions/70674928/terraform-gcp-error-project-required-field-is-not-set"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 68497046,
      "title": "Error pulling docker image from GCR into GKE &quot;Failed to pull image .... 403 Forbidden&quot;",
      "problem": "Background:\nI have a GKE cluster which has suddenly stopped being able to pull my docker images from GCR; both are in the same GCP project. It has been working well for several months, no issues pulling images, and has now started throwing errors without having made any changes.\n(NB: I'm generally the only one on my team who accesses Google Cloud, though it's entirely possible that someone else on my team may have made changes / inadvertently made changes without realising).\nI've seen a few other posts on this topic, but the solutions offered in others haven't helped. Two of these posts stood out to me in particular, as they were both posted around the same day my issues started ~13/14 days ago. Whether this is coincidence or not who knows..\nThis post has the same issue as me; unsure whether the posted comments helped them resolve, but it hasn't fixed for me. This post seemed to also be the same issue, but the poster says it resolved by itself after waiting some time.\nThe Issue:\nI first noticed the issue on the cluster a few days ago. Went to deploy a new image by pushing image to GCR and then bouncing the pods `kubectl rollout restart deployment`.\nThe pods all then came back with `ImagePullBackOff`, saying that they couldn't get the image from GCR:\n`kubectl get pods`:\n```\n`XXX-XXX-XXX     0/1     ImagePullBackOff   0          13d\nXXX-XXX-XXX     0/1     ImagePullBackOff   0          13d\nXXX-XXX-XXX     0/1     ImagePullBackOff   0          13d\n...\n`\n```\n`kubectl describe pod XXX-XXX-XXX`:\n```\n`Normal   BackOff           20s                kubelet                                Back-off pulling image \"gcr.io//XXX:dev-latest\"\nWarning  Failed            20s                kubelet                                Error: ImagePullBackOff\nNormal   Pulling           8s (x2 over 21s)   kubelet                                Pulling image \"gcr.io//XXX:dev-latest\"\nWarning  Failed            7s (x2 over 20s)   kubelet                                Failed to pull image \"gcr.io//XXX:dev-latest\": rpc error: code = Unknown desc = failed to pull and unpack image \"gcr.io//XXX:dev-latest\": failed to resolve reference \"gcr.io//XXX:dev-latest\": unexpected status code [manifests dev-latest]: 403 Forbidden\nWarning  Failed            7s (x2 over 20s)   kubelet                                Error: ErrImagePull\n`\n```\nTroubleshooting steps followed from other posts:\nI know that the image definitely exists in GCR -\n\nI can pull the image to my own machine (also removed all docker images from my machine to confirm it was really pulling)\nI can see the tagged image if I look on the GCR UI on chrome.\n\nI've SSH'd into one of the cluster nodes and tried to docker pull manually, with no success:\n```\n`docker pull gcr.io//XXX:dev-latest\nError response from daemon: unauthorized: You don't have the needed permissions to perform this operation, and you may have invalid credentials. To authenticate your request, follow the steps in: https://cloud.google.com/container-registry/docs/advanced-authentication\n`\n```\n(Also did a docker pull of a public mongodb image to confirm that was working, and it's specific to GCR).\nSo this leads me to believe it's an issue with the service account not having the correct permissions, as in the cloud docs under the 'Error 400/403' section. This seems to suggest that the service account has either been deleted, or edited manually.\nDuring my troubleshooting, I tried to find out exactly which service account GKE was using to pull from GCR. In the steps outlined in the docs, it says that: `The name of your Google Kubernetes Engine service account is as follows, where PROJECT_NUMBER is your project number:`\n```\n`service-PROJECT_NUMBER@container-engine-robot.iam.gserviceaccount.com\n`\n```\nI found the service account and checked the polices - it did have one for `roles/container.serviceAgent`, but nothing specifically mentioning kubernetes as I would expect from the description in the docs.. 'the Kubernetes Engine Service Agent role' (unless that is the one they're describing, in which case I'm no better off that before anyway..).\nMust not have had the correct roles, so I then followed the steps to re-enable (disable then enable the Kubernetes API). Running `cloud projects get-iam-policy ` again and diffing the two outputs (before/after), the only difference is that a service account for '@cloud-filer...' has been deleted.\nThinking maybe the error was something else, I thought I would try spinning up a new cluster. Same error - can't pull images.\nSend help..\nI've been racking my brains to try to troubleshoot, but I'm now out of ideas! Any and all help much appreciated!",
      "solution": "Have now solved this.\nThe service account had the correct roles/permissions, but for whatever reason stopped working.\nI manually created a key for that service account, added that secret into the kube cluster, and set the service account to use that key.\nStill at a loss as to why it wasn't already doing this, or why it stopped working in the first place all of a sudden, but it's working...\nFix was from this guide, from the section starting 'Create & use GCR credentials'.",
      "question_score": 15,
      "answer_score": 2,
      "created_at": "2021-07-23T11:28:48",
      "url": "https://stackoverflow.com/questions/68497046/error-pulling-docker-image-from-gcr-into-gke-failed-to-pull-image-403-forb"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 75089703,
      "title": "Google is blocking traffic because it detects automated queries",
      "problem": "When I try to access my website deployed on Google Cloud I receive the following error page:\n\nWe're sorry...\n... but your computer or network may be sending automated queries. To\nprotect our users, we can't process your request right now.\n\nHere are the details of my setup:\n\nNextJS web application deployed to Cloud Run (traffic limited to Internal + Load Balancing)\nStrapi Content Management System deployed to Cloud Run (traffic limited to Internal + Load Balancing)\nI have a load balancer in front of each Cloud Run service and am using Google reserved IP's and managed SSL certs\nIdentity Aware Proxy (IAM) is in front of both Load Balancers\nEach page visit makes an API call to the content management system to retrieve content (could this be causing the network to think \"automated queries\" are occurring?)\n\nTroubleshooting:\n\nIt doesn't appear to be related to the actual user visiting the website. This error message appears for all users, different devices, different IP addresses. It was working without issue before, this is the first time I've had this problem.\nNo traffic is reaching the Cloud Run environments. If I switch their ingress to Allow All I am able to hit the Cloud Run URL directly and it works\nDeleting and recreating the load balancer doesn't resolve the issue\n\nBased on this, there seems to be some network configuration/security occurring near the load balancer.\nAny solutions or suggestions to continue debugging?",
      "solution": "So far the two only workarounds available are:\n\ndisable IAP on the load balancer\nMake Public and Access directly the Cloud RUN URL (https://myCloudRun-xxxxx.a.run.com)\n\nBoth are not acceptable on production systems unless you don't care about security. We are trying to work out what are the IAP headers lost between NGINX and Cloud RUN but I guess the issue affects also infrastructure without NGINX\nEDIT\nthe issue tracker post https://issuetracker.google.com/issues/265184232 looks related to the issue.\nThe workaround suggested by the google team is to switch to a Global HTTP Classic Load Balancer\nWe tried and it did not work but we may have a different problem. Hope that helps anyone else for the time being\nEDIT 2\nIn our NGINX configuration we proxy the request to a public Cloud RUN API which at the moment is affected by the issue.\nI was able to proxy the request successfully just by removing the JWT header added by IAP (leaving any other headers with the authenticated user)\nexample of my config\n```\n` location /api/ {\n    #remove iap jwt token header \n    proxy_set_header        X-Goog-Iap-Jwt-Assertion \"\";\n\n    # proxy to the public cloud run instance\n    proxy_pass https://myCloudRun-123456basd-ew.a.run.app/;\n}\n`\n```",
      "question_score": 15,
      "answer_score": 5,
      "created_at": "2023-01-11T23:33:50",
      "url": "https://stackoverflow.com/questions/75089703/google-is-blocking-traffic-because-it-detects-automated-queries"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 68659507,
      "title": "Failed to create function project when deploying firebase cloud function",
      "problem": "I'm trying to deploy the firebase function from my local machine\nI run the following code:\n```\n`firebase deploy --only functions  \n`\n```\nI'm getting this error.\n```\n`\u2714  functions: Finished running predeploy script.\ni  functions: ensuring required API cloudfunctions.googleapis.com is enabled...\ni  functions: ensuring required API cloudbuild.googleapis.com is enabled...\n\u2714  functions: required API cloudbuild.googleapis.com is enabled\n\u2714  functions: required API cloudfunctions.googleapis.com is enabled\ni  functions: preparing functions directory for uploading...\ni  functions: packaged functions (95.46 KB) for uploading\n\u2714  functions: functions folder uploaded successfully\ni  functions: creating Node.js 14 function messageNotification(us-central1)...\n\u26a0  functions: failed to create function projects/xxx/locations/us-central1/functions/messageNotification\n`\n```\nThere isn't any error message. It merely stated that the function failed to be created. Does anyone know why? Thanks in advance!\nEdit\nI followed what John Hanley suggested and ran\n```\n`firebase deploy --debug --only functions\n`\n```\nand got the following error message\n```\n`[2021-08-05T01:49:33.605Z] I realized the way I was writing the function was wrong. I was writing it\n```\n`export const messageNotification = functions.firestore\n    .document(\"channels/{channelId}/{messageId}\")...\n`\n```\nBut I should have written it as shown below instead\n```\n`export const messageNotification = functions.firestore\n    .document(\"channels/{channelId}/messages/{messageId}\")\n`\n```",
      "solution": "To debug deployments try two techniques:\nAdd the command line option --debug\"\n```\n`firebase deploy --debug --only functions\n`\n```\nCheck the logs for messages:\n```\n`firebase functions:log\n`\n```",
      "question_score": 14,
      "answer_score": 36,
      "created_at": "2021-08-05T03:12:05",
      "url": "https://stackoverflow.com/questions/68659507/failed-to-create-function-project-when-deploying-firebase-cloud-function"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 69600097,
      "title": "Firebase Cloud messaging - permissions error for cloud build deployed app",
      "problem": "I have a java spring boot backend app, that I am trying to hook up to Firebase Cloud Messages.\nI have an android app that uses firebase and I am trying to use this backend to push notifications.\nI've generated a private key from firebase console project settings, placed the json file - and the following worked LOCALLY perfectly:\n```\n`try {\n  FirebaseOptions options = new FirebaseOptions.Builder()\n    .setCredentials(GoogleCredentials.fromStream(new ClassPathResource(\"PATH_TO_GENERATED_JSON\").\n  getInputStream())).build();\n  if (FirebaseApp.getApps().isEmpty()) {\n    FirebaseApp.initializeApp(options);\n    logger.info(\"Firebase application has been initialized\");\n  }\n} catch (IOException e) {\n  logger.error(e.getMessage());\n}\n...\nresponse = FirebaseMessaging.getInstance().send(message);\n`\n```\nI have set up google cloud build to automatically trigger and build from github.\nBut I cannot commit the json credentials file (right?), so for cloud deployment I have changed the initialization part to:\n```\n`if (FirebaseApp.getApps().isEmpty()) {\n  FirebaseApp.initializeApp();\n  logger.info(\"Firebase application has been initialized\");\n}\n`\n```\nBut I have received errors information about project id not set, so I have also edited the cloud build trigger inline YAML with:\n`--update-env-vars=GOOGLE_CLOUD_PROJECT=XXXXXXXXXXX`\nBut now I am getting the following error when trying to send the message:\n`com.google.firebase.messaging.FirebaseMessagingException: Permission 'cloudmessaging.messages.create' denied on resource '//cloudresourcemanager.googleapis.com/projects/XXXXXXXXXXX' (or it may not exist).`\n(XXXXXXXXXXX being my project id)\nI've started giving \"Firebase Cloud Messaging Admin\" role left and right on https://console.cloud.google.com/iam-admin/iam?project= but that didn't help :(\nCan anyone help?\nAdding stack trace:\n```\n`com.google.firebase.messaging.FirebaseMessagingException: Permission 'cloudmessaging.messages.create' denied on resource '//cloudresourcemanager.googleapis.com/projects/our-shield-329019' (or it may not exist).\n    at com.google.firebase.messaging.FirebaseMessagingException.withMessagingErrorCode(FirebaseMessagingException.java:51)\n    at com.google.firebase.messaging.FirebaseMessagingClientImpl$MessagingErrorHandler.createException(FirebaseMessagingClientImpl.java:293)\n    at com.google.firebase.messaging.FirebaseMessagingClientImpl$MessagingErrorHandler.createException(FirebaseMessagingClientImpl.java:282)\n    at com.google.firebase.internal.AbstractHttpErrorHandler.handleHttpResponseException(AbstractHttpErrorHandler.java:57)\n    at com.google.firebase.internal.ErrorHandlingHttpClient.send(ErrorHandlingHttpClient.java:108)\n    at com.google.firebase.internal.ErrorHandlingHttpClient.sendAndParse(ErrorHandlingHttpClient.java:72)\n    at com.google.firebase.messaging.FirebaseMessagingClientImpl.sendSingleRequest(FirebaseMessagingClientImpl.java:127)\n    at com.google.firebase.messaging.FirebaseMessagingClientImpl.send(FirebaseMessagingClientImpl.java:113)\n    at com.google.firebase.messaging.FirebaseMessaging$1.execute(FirebaseMessaging.java:137)\n    at com.google.firebase.messaging.FirebaseMessaging$1.execute(FirebaseMessaging.java:134)\n    at com.google.firebase.internal.CallableOperation.call(CallableOperation.java:36)\n    at com.google.firebase.messaging.FirebaseMessaging.send(FirebaseMessaging.java:104)\n    at com.google.firebase.messaging.FirebaseMessaging.send(FirebaseMessaging.java:86)\n    at com.miloszdobrowolski.investobotbackend.InvestobotAPIs.testNotification(InvestobotAPIs.java:120)\n    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n    at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.base/java.lang.reflect.Method.invoke(Method.java:568)\n    at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:205)\n    at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:150)\n    at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:117)\n    at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:895)\n    at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:808)\n    at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)\n    at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1067)\n    at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:963)\n    at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1006)\n    at org.springframework.web.servlet.FrameworkServlet.doGet(FrameworkServlet.java:898)\n    at javax.servlet.http.HttpServlet.service(HttpServlet.java:655)\n    at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:883)\n    at javax.servlet.http.HttpServlet.service(HttpServlet.java:764)\n    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:227)\n    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:162)\n    at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:53)\n    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:189)\n    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:162)\n    at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)\n    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:119)\n    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:189)\n    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:162)\n    at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)\n    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:119)\n    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:189)\n    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:162)\n    at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)\n    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:119)\n    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:189)\n    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:162)\n    at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:197)\n    at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:97)\n    at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:540)\n    at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:135)\n    at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:92)\n    at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:78)\n    at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:357)\n    at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:382)\n    at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:65)\n    at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:893)\n    at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1726)\n    at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:49)\n    at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)\n    at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)\n    at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)\n    at java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: com.google.api.client.http.HttpResponseException: 403 Forbidden\n`\n```",
      "solution": "As listed in the Understanding Roles documentation, Firebase Cloud Messaging Admin does not have a `cloudmessaging.messages.create` permission. In order to add this permission, use one of the following roles:\n\nFirebase Admin (roles/firebase.admin)\nFirebase Grow Admin (roles/firebase.growthAdmin)\nFirebase Admin SDK Administrator Service Agent (roles/firebase.sdkAdminServiceAgent)\nFirebase SDK Provisioning Service Agent (roles/firebase.sdkProvisioningServiceAgent)",
      "question_score": 13,
      "answer_score": 28,
      "created_at": "2021-10-17T00:52:17",
      "url": "https://stackoverflow.com/questions/69600097/firebase-cloud-messaging-permissions-error-for-cloud-build-deployed-app"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 77181996,
      "title": "GCloud Build Failure: ERROR: failed to initialize analyzer, No such object",
      "problem": "My most recent builds with GCloud all started failing recently and I am not really sure why. I keep getting 404 errors from the build attempting to find an image that doesn't exist.\nSample error:\n```\n`Already have image (with digest): us.gcr.io/gae-runtimes/buildpacks/google-gae-22/python/builder:python_20230925_RC00\n===> ANALYZING\nERROR: failed to initialize analyzer: getting previous image: getting config file for image \"us.gcr.io/growth-ops-apps/app-engine-tmp/app/default/ttl-18h:latest\": GET https://storage.googleapis.com/us.artifacts.growth-ops-apps.appspot.com/containers/images/sha256:d13213796512322314e6db634cc57318665191dc5437c121f151cb33310c552e?access_token=REDACTED: unexpected status code 404 Not Found: NoSuchKeyThe specified key does not exist.No such object: us.artifacts.growth-ops-apps.appspot.com/containers/images/sha256:d13213796512322314e6db634cc57318665191dc5437c121f151cb33310c552e\n`\n```\nI can verify that the bucket `us.artifacts.growth-ops-apps.appspot.com` is empty in the console, but I have no idea why. There is no bucket retention policy or anything I can see that is obvious to me.\nFor reference, I have a single GitHub repository set to trigger builds to 2 separate projects, one for development and one for production. Only the production one is failing in this way.\nI am using a cloudbuild.yaml file stored in the repository.\nAny ideas?",
      "solution": "I figured it out! In thinking through one of my own comments, `the build is looking for an artifact that doesn't exist`, I figured there must be some way of forcing a new build. I found the `--no-cache` flag in the docs, which forced a brand new build instead of attempting to use information from a non-existent prior build.",
      "question_score": 12,
      "answer_score": 32,
      "created_at": "2023-09-26T18:50:03",
      "url": "https://stackoverflow.com/questions/77181996/gcloud-build-failure-error-failed-to-initialize-analyzer-no-such-object"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 73891299,
      "title": "Docker push &quot;Missing image name&quot;-error when pushing to GCP Artifact Registry from Github Actions",
      "problem": "I am running the google-github-actions/deploy-cloudrun Action on Github, which fails when trying to push a docker image to Artifact Registry.\n\nI authenticate through an identity pool\nThe docker image builds successfully\nHowever, pushing the image to `Google Artifact Registry` fails with `name invalid: Missing image name. Pushes should be of the form docker push HOST-NAME/PROJECT-ID/REPOSITORY/IMAGE`\n\nGithub action YML\n```\n`# Authenticate Docker to Google Cloud Artifact Registry\n  - name: Docker Auth\n    id: docker-auth\n    uses: 'docker/login-action@v1'\n    with:\n      username: 'oauth2accesstoken'\n      password: '${{ steps.auth.outputs.access_token }}'\n      registry: '${{ env.GAR_LOCATION }}-docker.pkg.dev'\n\n  - name: Build and Push Container\n    run: |-\n      docker build -t \"${{ env.GAR_LOCATION }}-docker.pkg.dev/${{ env.PROJECT_ID }}/${{ env.SERVICE }}:${{ github.sha }}\" .\n      docker push \"${{ env.GAR_LOCATION }}-docker.pkg.dev/${{ env.PROJECT_ID }}/${{ env.SERVICE }}:${{ github.sha }}\"\n`\n```\nLog output\n```\n`Successfully built 2edd636b95c7\nSuccessfully tagged us-central1-docker.pkg.dev/[my-project]/github-actions:ecb28fdf92addae09fe6bd9e86033027b2850de3\nThe push refers to repository [us-central1-docker.pkg.dev/[my-project]/github-actions]\n8189f048f482: Retrying in 5 seconds\n... multiple retries ...\nname invalid: Missing image name. Pushes should be of the form docker push HOST-NAME/PROJECT-ID/REPOSITORY/IMAGE\nError: Process completed with exit code 1.\n`\n```\nI do have Artifact Registry enabled, and created repository with the path `us-central1-docker.pkg.dev/[my-project]/github-actions`\nThe IAM role has following permissions\n\nArtifact Registry Administrator\nCloud Run Admin\nService Account User\n\nI am out of ideas why to the authenticated docker it appears that the registry doesn't exist.",
      "solution": "Turns out, the above notation is only specifying `HOST-NAME/PROJECT-ID/REPOSITORY:tag` but not `/IMAGE`\nReplacing all occurrences by e.g. `${{ env.GAR_LOCATION }}-docker.pkg.dev/${{ env.PROJECT_ID }}/${{ env.SERVICE }}/website:${{ github.sha }}` will use  /`website` as the actual image name within the repository.",
      "question_score": 12,
      "answer_score": 19,
      "created_at": "2022-09-29T08:43:46",
      "url": "https://stackoverflow.com/questions/73891299/docker-push-missing-image-name-error-when-pushing-to-gcp-artifact-registry-fro"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 71462058,
      "title": "Google OAuth 2 Error 400: redirect_uri_mismatch but redirect uri is compliant and already registered in Google Cloud Console",
      "problem": "I am developing a NextJS application using next-auth with Google Oauth 2 as its authentication provider.  The production build is running on Heroku.  When attempting to sign in on my production build, Google OAuth is giving me `\"Error 400: redirect_uri_mismatch\"`.  Normally this would be an easy fix, except the exact uri is already registered in Cloud Console.\n.\nI have also tried added many different permutations of my uri, but this did not help.\nThis issue not solved by 11485271 or 69151061.\nError in question:\n```\n`Error 400: redirect_uri_mismatch\n\nYou can't sign in to this app because it doesn't comply with Google's OAuth 2.0 policy.\n\nIf you're the app developer, register the redirect URI in the Google Cloud Console.\n\nRequest Details\nIf you\u2019re the app developer, make sure that these request details comply with Google policies.\nredirect_uri: https://middcourses2.herokuapp.com/api/auth/callback/google\n`\n```\n\nAnd here is a link to the list of authorized domains in GCP.",
      "solution": "Solved!  So for some reason, Google changed my Client ID and Client Secret after I already set up those env variables.  Once I noticed the change and inputted the new values it worked fine.",
      "question_score": 12,
      "answer_score": 3,
      "created_at": "2022-03-14T01:53:48",
      "url": "https://stackoverflow.com/questions/71462058/google-oauth-2-error-400-redirect-uri-mismatch-but-redirect-uri-is-compliant-an"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 71511498,
      "title": "Is there a way to select only certain fields from Firestore?",
      "problem": "I am working on a performance issue of a function, that takes 15sec to response, which makes a request to firebase for all documents that are\n`\"ErrorID\" \"==\" \"0\"`\nThe problem is that there are many documents and they are kind of very large objects, and I only need TWO FIELDS `(Order and Amount)` of each document, there are any way to request only those two fields that accomplish the condition?\nSomething like :\n```\n`firestore.collection(\"purchases\").where(\"ErrorID\", \"==\", \"0\").get(Order, Amount); \n`\n```\nThe function that im talking about:\n```\n`const totalEarn = async (req, res, next) => {\n  const DAY = 86400000;\n  const WEEK = 604800016;\n  const MONTH = 2629800000;\n\n  try {\n    let snap = await firestore.collection(\"purchases\").where(\"ErrorID\", \"==\", \"0\").get(); // CONDITION\n    let totalToday = 0;\n    let totalYesterday = 0;\n    let totalLastWeek = 0;\n    let totalLastMonth = 0;\n    let now = Date.now();\n    let Yesterday = now - 86400000;\n\n    await snap.forEach((doc) => { // THIS FOR EACH TAKES TOO MUCH TIME\n      let info = doc.data();\n      let time = info.Order.split(\"-\")[2]; // FIRESTORE FIELD -> ORDER\n      let amount = info.AmountEur * 1; // FIRESTORE FIELD -> AMOUNT\n\n      if (time > now - DAY) {\n        totalToday = totalToday + amount;\n      }\n      if (time  Yesterday - DAY) {\n        totalYesterday = totalYesterday + amount;\n      }\n      if (time > now - WEEK) {\n        totalLastWeek = totalLastWeek + amount;\n      }\n      if (time > now - MONTH) {\n        totalLastMonth = totalLastMonth + amount;\n      }\n    });\n\n    res.send({\n      status: true,\n      data: {\n        totalToday: totalToday.toFixed(2),\n        totalYesterday: totalYesterday.toFixed(2),\n        totalLastWeek: totalLastWeek.toFixed(2),\n        totalLastMonth: totalLastMonth.toFixed(2),\n      },\n    });\n  } catch (error) {\n    res.status(410).send({\n      status: false,\n      error: \"some error occured counting the numbers\",\n      e: error.message,\n    });\n  }\n};\n`\n```\nThe document im talking about",
      "solution": "If you use Firestore Node.JS client or Firebase Admin SDK, then you can use `select()` to select fields:\n```\n`import { Firestore } from \"@google-cloud/firestore\";\n\nconst firestore = new Firestore();\n\nconst snap = firestore\n  .collection(\"purchases\")\n  .where(\"ErrorID\", \"==\", \"0\")\n  .select(\"Order\", \"Amount\")\n  .get();\n`\n```",
      "question_score": 12,
      "answer_score": 17,
      "created_at": "2022-03-17T12:25:07",
      "url": "https://stackoverflow.com/questions/71511498/is-there-a-way-to-select-only-certain-fields-from-firestore"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 71320503,
      "title": "Is it possible to update the source code of a GCP Cloud Function in Terraform?",
      "problem": "I use Terraform to manage resources of Google Cloud Functions. But while the inital deployment of the cloud function worked, further deploments with changed cloud function source code (the source archive `sourcecode.zip`) were not redeployed when I use `terraform apply` after updating the source archive.\nThe storage bucket object gets updated but this does not trigger an update/redeployment of the cloud function resource.\nIs this an error of the provider?\nIs there a way to redeploy a function in terraform when the code changes?\nThe simplified source code I am using:\n```\n`resource \"google_storage_bucket\" \"cloud_function_source_bucket\" {\n  name                        = \"${local.project}-function-bucket\"\n  location                    = local.region\n  uniform_bucket_level_access = true\n}\n\nresource \"google_storage_bucket_object\" \"function_source_archive\" {\n  name   = \"sourcecode.zip\"\n  bucket = google_storage_bucket.cloud_function_source_bucket.name\n  source = \"./../../../sourcecode.zip\"\n}\n\nresource \"google_cloudfunctions_function\" \"test_function\" {\n  name                          = \"test_func\"\n  runtime                       = \"python39\"\n  region                        = local.region\n  project                       = local.project\n  available_memory_mb           = 256\n  source_archive_bucket         = google_storage_bucket.cloud_function_source_bucket.name\n  source_archive_object         = google_storage_bucket_object.function_source_archive.name\n  trigger_http                  = true\n  entry_point                   = \"trigger_endpoint\"\n  service_account_email         = google_service_account.function_service_account.email\n  vpc_connector                 = \"projects/${local.project}/locations/${local.region}/connectors/serverless-main\"\n  vpc_connector_egress_settings = \"ALL_TRAFFIC\"\n  ingress_settings              = \"ALLOW_ALL\"\n}\n`\n```",
      "solution": "You can append MD5 or SHA256 checksum of the content of zip to the bucket object's name. That will trigger recreation of cloud function whenever source code changes.\n\n${data.archive_file.function_src.output_md5}\n\n```\n`data \"archive_file\" \"function_src\" {\ntype = \"zip\"\nsource_dir = \"SOURCECODE_PATH/sourcecode\"\noutput_path = \"./SAVING/PATH/sourcecode.zip\"\n}\n\nresource \"google_storage_bucket_object\" \"function_source_archive\" {\nname   = \"sourcecode.${data.archive_file.function_src.output_md5}.zip\"\nbucket = google_storage_bucket.cloud_function_source_bucket.name\nsource = data.archive_file.function_src.output_path\n}\n`\n```\nYou can read more about terraform archive here - terraform archive_file",
      "question_score": 12,
      "answer_score": 13,
      "created_at": "2022-03-02T10:44:23",
      "url": "https://stackoverflow.com/questions/71320503/is-it-possible-to-update-the-source-code-of-a-gcp-cloud-function-in-terraform"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 65632969,
      "title": "Google Picker API Invalid origin value error",
      "problem": "Today Google Picker stopped working in my Google Sheets add-on without any changes to the code. The error in the modal dialogue reads:\n\nInvalid origin value.\n\nThe errors in console are:\n\nFailed to execute 'postMessage' on 'DOMWindow': The target origin provided ('https://docs.google.com') does not match the recipient window's origin ('https://n-a6p4dqsl***d6wq-0lu-script.googleusercontent.com')\ndropping postMessage.. was from unexpected window\ndropping postMessage.. was from unexpected window\nInvalid 'X-Frame-Options' header encountered when loading 'https://docs.google.com/picker?protocol=gadgets&origin=https%3A%2F%2Fdocs.google.com%2F&sdr=true&title&oauth_token=&developerKey=&hostId=n-a6p4dq***d6wq-0lu-script.googleusercontent.com&relayUrl=https%3A%2F%2Fn-a6p4dq***d6wq-0lu-script.googleusercontent.com%2Ffavicon.ico&nav=((%22documents%22%2Cnull%2C%7B%22selectFolder%22%3Atrue%2C%22parent%22%3A%22root%22%7D)%2C(%22documents%22%2Cnull%2C%7B%22dr%22%3Atrue%2C%22includeFolders%22%3Atrue%7D))&rpcService=qhurmoc5w4l7&rpctoken=xssf8g42xc2&thirdParty=true#rpctoken=xssf8g42xc2': 'ALLOW-FROM https://docs.google.com/' is not a recognized directive. The header will be ignored.\n\nIt maybe that the error is linked to this line of code where I do `setOrigin()`:\n`        var picker = new google.picker.PickerBuilder()\n            .addView(driveView)\n            .addView(drivesView)\n            .hideTitleBar()\n            .setOAuthToken(token)\n            .setDeveloperKey(DEVELOPER_KEY)\n            .setCallback(pickerCallback)\n        --> .setOrigin(google.script.host.origin)\n            .setSize(DIALOG_DIMENSIONS.width - 2,\n                DIALOG_DIMENSIONS.height - 2)\n            .build();\n`\nBut this line is directly from the documentation of the Google Picker API and worked properly before. If I change `google.script.host.origin`, that returns `https://docs.google.com` as url to `https://n-a6p4dqsl***6wcd6wq-0lu-script.googleusercontent.com`, I get the same error and a new one, so that is not it.\nI also cannot add this as as an authorized javascript origin in the GCP project as it returns the following error:\n\nInvalid Origin: uses a forbidden domain\n\n(This has been the case of a while)\nThis seems like a new error and I wasn't able to find an answer neither on Google's issues tracker nor on StackOverflow.\nAnyone facing this as well or have an idea how it can be handled?",
      "solution": "Putting an end, the only way to solve this is to remove the trailing slash after\nFrom\ndocs.google.com/\nTo\ndocs.google.com\nContrary,\nThe google.script.host.orgin gives the \"https://docs.google.com/\" which causes the error. Hence you need to hard code as\n\"https://docs.google.com\"\nGoogle has made some changes recently which might have bubbled this issue.\nUPDATE\nYou can use this function - and call -  `...... setOrigin(getOrigin())`\n```\n`function getOrigin() {\n    var url = google.script.host.origin;\n    return url.substr(url.length - 1) === \"/\" ? url.substr(0, url.length - 1) : url;\n}\n`\n```",
      "question_score": 12,
      "answer_score": 13,
      "created_at": "2021-01-08T17:43:21",
      "url": "https://stackoverflow.com/questions/65632969/google-picker-api-invalid-origin-value-error"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 67595318,
      "title": "CloudRun Suddenly got `Improper path /cloudsql/{SQL_CONNECTION_NAME} to connect to Postgres Cloud SQL instance &quot;{SQL_CONNECTION_NAME}&quot;`",
      "problem": "We have been running a service using NestJS and TypeORM on fully managed CloudRun without issues for several months. Yesterday PM we started getting `Improper path /cloudsql/{SQL_CONNECTION_NAME} to connect to Postgres Cloud SQL instance \"{SQL_CONNECTION_NAME}\"` errors in our logs.\nWe didn't make any server/SQL changes around this timestamp. Currently there is no impact to the service so we are not sure if this is a serious issue.\nThis error is not from our code, and our third party modules shouldn't know if we use Cloud SQL, so I have no idea where this errors come from.\nMy assumption is Cloud SQL Proxy or any SQL client used in Cloud Run is making this error. We use --add-cloudsql-instances flag when deploying with \"gcloud run deploy\" CLI command.\nLink to the issue here",
      "solution": "This log was recently added in the Cloud Run data path to provide more context for debugging CloudSQL connectivity issues. However, the original logic was overly aggressive, emitting this message even for properly working CloudSQL connections. Your application is working correctly and should not receive this warning.\nThank you for reporting this issue. The fix is ready and should roll out soon. You should not see this message anymore after the fix is out.",
      "question_score": 12,
      "answer_score": 12,
      "created_at": "2021-05-19T01:57:59",
      "url": "https://stackoverflow.com/questions/67595318/cloudrun-suddenly-got-improper-path-cloudsql-sql-connection-name-to-connect"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 72648022,
      "title": "Deploy SPA application on Google Cloud Storage using Load Balancer and CDN",
      "problem": "I'm investigating a way of deploying an Angular or React web application on Google Cloud using GCS, Load Balancer, and CDN.\nI've set up the LB and the GCS using the urlRewrite, but since the LB does not allow full URL rewrite only pathPrefixRewrite, I cannot redirect all the requests to `/index.html`\nI want to achieve the same functionality as firebase hosting where you can specify the rewrite rules\n```\n`\"rewrites\": [\n      {\n        \"source\": \"**\",\n        \"destination\": \"/index.html\"\n      }\n    ]\n`\n```\nAnother option will be to set the 404 page as `index.html`, but that will result in a `404 status code` return by the server, which I don't like.\nIs this possible with Load Balancer, because they are not supporting a full rewrite?",
      "solution": "Google added rewrite engine in 2020 (see https://serverfault.com/questions/927143/how-to-manage-url-rewrites-with-a-gcp-load-balancer/1045214) and it is only capable `pathPrefixRewrite` (stripping fixed prefix).\nFor SPA you need variable suffix rewrite to `index.html` so you use Nginx or Firebase.\nOr mentioned error handler in bucket's HTTP server (with a downside of HTTP code 404 for your virtual URLs):\n```\n`gsutil web set -m index.html -e index.html gs://web-stage/\n`\n```\nFull instructions to set SPA app is here: Google cloud load balancer create backend service for Firebase Hosting to serve microservices & frontend SPA from the same domain via load balancer?\nDiscussion of LB rewrite engine limitations:\n\nHow can I implement a .htaccess configuration in Firebase Hosting?\nGoogle Cloud Load balancer URL Rewrite not working\nhttps://www.reddit.com/r/googlecloud/comments/8zgg20/static_website_url_rewrite/",
      "question_score": 12,
      "answer_score": 1,
      "created_at": "2022-06-16T17:07:29",
      "url": "https://stackoverflow.com/questions/72648022/deploy-spa-application-on-google-cloud-storage-using-load-balancer-and-cdn"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 71531247,
      "title": "Error while trying to authenticate with `gcloud init`",
      "problem": "I am trying to athenticate to the gcloud sdk using : `gcloud init`.\nI get a URL I'm supposed to access in order to copy a token and return it to the CLI... but instead of a token, I get this error :\n```\n`Erreur d'autorisation\nErreur 400 : invalid_request\nMissing required parameter: redirect_uri\n`\n```\nIs this a bug?\n`gcloud version` info:\n```\n`Google Cloud SDK 377.0.0\nalpha 2022.03.10\nbeta 2022.03.10\nbq 2.0.74\nbundled-python3-unix 3.8.11\ncore 2022.03.10\ngsutil 5.8\n`\n```\nI am running `gcloud init` on wsl2 (Ubuntu 18.04). This error occurs right after the installation of gcloud with `sudo apt install google-cloud-sdk`.",
      "solution": "I had the same problem and gcloud has slightly changed the way their auth flow works.\nRun `gcloud auth login` and then copy the whole output (not just the URL) to a terminal on a computer that has both a web browser and gcloud CLI installed. The command you should copy looks like\n```\n`gcloud auth login --remote-bootstrap=\"https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=****.apps.googleusercontent.com&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fappengine.admin+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcompute+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Faccounts.reauth&state=****&access_type=offline&code_challenge=****&code_challenge_method=S256&token_usage=remote\"\n`\n```\nWhen you run that on your computer that has a web browser, it will open a browser window and prompt you to log in. Once you authorize your app in the web browser you get a new URL in your terminal that looks like\n```\n`https://localhost:8085/?state=****&code=****&scope=email%20openid%20https://www.googleapis.com/auth/userinfo.email%20https://www.googleapis.com/auth/cloud-platform%20https://www.googleapis.com/auth/appengine.admin%20https://www.googleapis.com/auth/compute%20https://www.googleapis.com/auth/accounts.reauth&authuser=0&hd=****&prompt=consent\n`\n```\nPaste this new URL back into the prompt in your headless machine after `Enter the output of the above command:` (in your case, this would be in your WSL2 terminal). Press enter and you get the output\n```\n`You are now logged in as [****].\nYour current project is [None].  You can change this setting by running:\n  $ gcloud config set project PROJECT_ID\n[8]+  Done                    code_challenge_method=S256\n`\n```",
      "question_score": 11,
      "answer_score": 15,
      "created_at": "2022-03-18T18:52:44",
      "url": "https://stackoverflow.com/questions/71531247/error-while-trying-to-authenticate-with-gcloud-init"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 66165652,
      "title": "How to deploy google cloud functions using custom container image",
      "problem": "To enable the webdriver in my google cloud function, I created a custom container using a docker file:\n```\n`FROM python:3.7\nCOPY . /\nWORKDIR /\nRUN pip3 install -r requirements.txt\nRUN apt-get update \nRUN apt-get install -y gconf-service libasound2 libatk1.0-0 libcairo2 libcups2 libfontconfig1 libgdk-pixbuf2.0-0 libgtk-3-0 libnspr4 libpango-1.0-0 libxss1 fonts-liberation libappindicator1 libnss3 lsb-release xdg-utils\n\n#download and install chrome\nRUN wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\nRUN dpkg -i google-chrome-stable_current_amd64.deb; apt-get -fy install\n\n#install python dependencies\nCOPY requirements.txt requirements.txt \nRUN pip install -r ./requirements.txt \n\n# Downloading gcloud package\nRUN curl https://dl.google.com/dl/cloudsdk/release/google-cloud-sdk.tar.gz > /tmp/google-cloud-sdk.tar.gz\n\n# Installing the package\nRUN mkdir -p /usr/local/gcloud \\\n  && tar -C /usr/local/gcloud -xvf /tmp/google-cloud-sdk.tar.gz \\\n  && /usr/local/gcloud/google-cloud-sdk/install.sh\n\n# Adding the package path to local\nENV PATH $PATH:/usr/local/gcloud/google-cloud-sdk/bin\n\n#some envs\nENV PORT 5000\n\n#copy local files\nCOPY . . \n\nCMD exec gunicorn --bind :${PORT} --workers 1 --threads 8 main:app \nENTRYPOINT [\"webcrawler\"]\n`\n```\nI installed gcloud in this docker so that I will be able to use `gcloud deploy` to deploy my cloud functions. Then, I deploy my script using this cloudbuild.yaml:\n```\n`steps:\n  - name: 'us-central1-docker.pkg.dev/$PROJECT_ID/webcrawler-repo/webcrawler:tag1'\n    entrypoint: 'gcloud'\n    args: ['functions', 'deploy', 'MY_FUN', '--trigger-topic=MY_TOPIC', '--runtime=python37', '--entry-point=main', '--region=us-central1', '--memory=512MB', '--timeout=540s']\n    id: 'deploying MY_FUN'\n    dir: 'MY_DIR'\n`\n```\nHowever, I end up getting this error for my deployment:\n```\n`ERROR: (gcloud.functions.deploy) OperationError: code=3, message=Build failed: invalid storage source object \"MY_FUN-ba7acf95-4297-46b3-b76e-1c25ba21ba03/version-14/function-source.zip\" in bucket \"gcf-sources-967732204245-us-central1\": failed to get storage object: Get \"https://storage.googleapis.com/storage/v1/b/gcf-sources-967732204245-us-central1/o/MY_FUN-ba7acf95-4297-46b3-b76e-1c25ba21ba03%2Fversion-14%2Ffunction-source.zip?alt=json&prettyPrint=false\": RPC::UNREACHABLE: gslb: no reachable backends\nERROR\nERROR: build step 0 \"us-central1-docker.pkg.dev/PROJECT_ID/webcrawler-repo/webcrawler:tag1\" failed: step exited with non-zero status: 1\n`\n```\nAny idea how to resolve this issue?\nThanks!",
      "solution": "Cloud functions allows you to deploy only your code. The packaging into a container, with buildpack, is performed automatically for you.\nIf you have already a container, the best solution is to deploy it on Cloud Run. If your webserver listen on the port 5000, don't forget to override this value during the deployment (use `--port` parameter).\nTo plug your PubSub topic to your Cloud Run service, you have 2 solutions\n\nEither manually, you create a PubSub push subscription to your Cloud Run service\nOr you can use EventArc to plug it to your Cloud Run service\n\nIn both cases, you need to take care of the security by using a service account with the role run.invoker on the Cloud Run service that you pass to PubSub push subscription or to EventArc",
      "question_score": 11,
      "answer_score": 22,
      "created_at": "2021-02-12T03:30:46",
      "url": "https://stackoverflow.com/questions/66165652/how-to-deploy-google-cloud-functions-using-custom-container-image"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 74831594,
      "title": "cannot import name &#39;WKBWriter&#39; from &#39;shapely.geos&#39; when import google cloud ai platform",
      "problem": "When I run this code on google colab.\n`from google.cloud import aiplatform`\nThe following error occurred\n`ImportError: cannot import name 'WKBWriter' from 'shapely.geos' (/usr/local/lib/python3.8/dist-packages/shapely/geos.py)`\nDoes anyone know how to solve this problem?\nI was working fine on 2022/12/16, but today it is not working.",
      "solution": "The bug is tracked in: https://github.com/googleapis/python-aiplatform/issues/1852\nThe workaround is to pin `shapely \n```\n`pip install -U google-cloud-aiplatform \"shapely<2\"\n`\n```",
      "question_score": 11,
      "answer_score": 21,
      "created_at": "2022-12-17T04:27:04",
      "url": "https://stackoverflow.com/questions/74831594/cannot-import-name-wkbwriter-from-shapely-geos-when-import-google-cloud-ai-p"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 76380806,
      "title": "Firebase Cloud Functions V2: The request was not authorized to invoke this service",
      "problem": "I'm attempting to call a callable cloud function (which is already deployed) from a client app and getting this error on the GCP logs:\n```\n`{\nhttpRequest: {9}\ninsertId: \"647865c20002422d2d32b259\"\nlabels: {1}\nlogName: \"projects/faker-app-flutter-firebase-dev/logs/run.googleapis.com%2Frequests\"\nreceiveTimestamp: \"2023-06-01T09:32:50.154902339Z\"\nresource: {2}\nseverity: \"WARNING\"\nspanId: \"11982344486849947204\"\ntextPayload: \"The request was not authorized to invoke this service. Read more at https://cloud.google.com/run/docs/securing/authenticating Additional troubleshooting documentation can be found at: https://cloud.google.com/run/docs/troubleshooting#401\"\ntimestamp: \"2023-06-01T09:32:50.138090Z\"\ntrace: \"projects/faker-app-flutter-firebase-dev/traces/ddcb5a4df500af085b7a7f6f89a72ace\"\ntraceSampled: true\n}\n`\n```\n\nThe same function works correctly from the Firebase Local Emulator, so I assume this is a permissions issue related to IAM and service accounts (I still don't understand too well how IAM works).\nHere is my code:\n```\n`import * as admin from \"firebase-admin\"\nimport * as functions from \"firebase-functions/v2\"\nimport * as logger from \"firebase-functions/logger\";\n\n// https://github.com/firebase/firebase-tools/issues/1532\nif (admin.apps.length === 0) {\n  admin.initializeApp()\n}\n\nexport const deleteAllUserJobs = functions.https.onCall(async (context: functions.https.CallableRequest) => {\n  const uid = context.auth?.uid\n  if (uid === undefined) {\n    throw new functions.https.HttpsError(\"unauthenticated\", \"You need to be authenticated to perform this action\")\n  }\n  const firestore = admin.firestore()\n  const collectionRef = firestore.collection(`/users/${uid}/jobs`)\n  const collection = await collectionRef.get()\n  logger.debug(`Deleting ${collection.docs.length} docs at \"/users/${uid}/jobs\"`)\n  // transaction version\n  await firestore.runTransaction(async (transaction) => {\n      for (const doc of collection.docs) {\n          transaction.delete(firestore.doc(`/users/${uid}/jobs/${doc.id}`))\n      }\n  })\n\n  logger.debug(`Deleted ${collection.docs.length} docs at \"/users/${uid}/jobs\"`)\n  return {\"success\": true}\n})\n`\n```\nThe function was deployed with `firebase deploy --only functions`, and I made sure the client app calls this function when the user is already authorized.\nAccording to the docs:\n\nIf you encounter permissions errors when deploying functions, make sure that the appropriate IAM roles are assigned to the user running the deployment commands.\n\nThe docs also link to this page, which says:\n\nCloud Functions for Firebase permissions\nFor a list and descriptions of Cloud Functions permissions, refer to\nthe IAM documentation.\nBe aware that the deployment of functions requires a specific\nconfiguration of permissions that aren't included in the standard\nFirebase predefined roles. To deploy functions, use one of the\nfollowing options:\n```\n`Delegate the deployment of functions to a project Owner.\n\nIf you're deploying only non-HTTP functions, then a project Editor can deploy your functions.\n\nDelegate deployment of functions to a project member who has the following two roles:\n    Cloud Functions Admin role (roles/cloudfunctions.admin)\n    Service Account User role (roles/iam.serviceAccountUser)\n\nA project Owner can assign these roles to a project member using the Google Cloud Console or gcloud CLI. For detailed steps and\n`\n```\nsecurity implications for this role configuration, refer to the IAM\ndocumentation.\n\nBut like I said, I can successfully deploy the function. It's when I try to execute it that I get an error log.\nIn summary, what I'm trying to do is quite basic:\n\nwrite a callable cloud function\ndeploy it\ncall it from the client app\n\nWhen the function runs, it fails with the error above.\nAny advice? Do I need to set a specific IAM role?",
      "solution": "Open `https://console.cloud.google.com/iam-admin/`, find the service account you are using in your firebase project and add the Role \"Cloud Functions Invoker\".\n\nIs like Admin, Editor or Viewer roles are about manipulating the function on GCP (don't allow you to use it) and Invoker allows that account to invoke the function.\nConsider that you may need to initialize again your Firebase project in Firebase cli and that you may need to delete and upload again the cloud functions.",
      "question_score": 11,
      "answer_score": 15,
      "created_at": "2023-06-01T12:07:19",
      "url": "https://stackoverflow.com/questions/76380806/firebase-cloud-functions-v2-the-request-was-not-authorized-to-invoke-this-servi"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 76675683,
      "title": "How do I list all resources in a particular google cloud project?",
      "problem": "I dont see an easy way of listing the resources in a google cloud project from the console. For those familiar with Azure, I can easily navigate over to the resource group and it will displays all the resources within it. I cannot appear to do the same in google cloud.",
      "solution": "You can use the Google Asset Inventory feature to list all the resources in your project.\n\nTo search all resources using the console, complete the following\nsteps:\n\nGo to the Asset Inventory page in the Google Cloud Console.\n\nTo set the scope of your search, open the Projects list box in the menu bar, and then select the organization, folder, or project to\nquery.\n\nSelect the Resource tab.\n\nTo search for resources, enter the query text in the Filter bar. Select the text box, and then a list of searchable fields display. Resource\nsearch supports multiple fields.\n\nSearch results can also be filtered by the pre-defined Asset type, Project, and Location filters in the Filter results pane.\n\nHowever, all the resources aren't supported. You can find the full list of supported asset types here.",
      "question_score": 11,
      "answer_score": 19,
      "created_at": "2023-07-13T05:22:49",
      "url": "https://stackoverflow.com/questions/76675683/how-do-i-list-all-resources-in-a-particular-google-cloud-project"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 67978745,
      "title": "How to solve `CERTIFICATE_VERIFY_FAILED` error when install gcloud?",
      "problem": "I tried to install gcloud in MacOs but failed. I tried two python versions 3.7.4 and 3.9.1 but both have the same issue. how can I install the `gcloud`? Is there any other dependencies I need?\n`$ python --version\nPython 3.9.1\n\n$ sh install.sh --screen-reader=true\nWelcome to the Google Cloud SDK!\n\nTo help improve the quality of this product, we collect anonymized usage data\nand anonymized stacktraces when crashes are encountered; additional information\nis available at . This data is\nhandled in accordance with our privacy policy\n. You may choose to opt in this\ncollection now (by choosing 'Y' at the below prompt), or at any time in the\nfuture by running the following command:\n\n    gcloud config set disable_usage_reporting false\n\nDo you want to help improve the Google Cloud SDK (y/N)?  y\n\nTraceback (most recent call last):\n  File \"/Users/joey/Downloads/google-cloud-sdk/lib/third_party/urllib3/connectionpool.py\", line 667, in urlopen\n    self._prepare_proxy(conn)\n  File \"/Users/joey/Downloads/google-cloud-sdk/lib/third_party/urllib3/connectionpool.py\", line 930, in _prepare_proxy\n    conn.connect()\n  File \"/Users/joey/Downloads/google-cloud-sdk/lib/third_party/urllib3/connection.py\", line 361, in connect\n    self.sock = ssl_wrap_socket(\n  File \"/Users/joey/Downloads/google-cloud-sdk/lib/third_party/urllib3/util/ssl_.py\", line 382, in ssl_wrap_socket\n    return context.wrap_socket(sock, server_hostname=server_hostname)\n  File \"/Users/joey/.pyenv/versions/3.9.1/lib/python3.9/ssl.py\", line 500, in wrap_socket\n    return self.sslsocket_class._create(\n  File \"/Users/joey/.pyenv/versions/3.9.1/lib/python3.9/ssl.py\", line 1040, in _create\n    self.do_handshake()\n  File \"/Users/joey/.pyenv/versions/3.9.1/lib/python3.9/ssl.py\", line 1309, in do_handshake\n    self._sslobj.do_handshake()\nssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self signed certificate in certificate chain (_ssl.c:1123)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/joey/Downloads/google-cloud-sdk/lib/third_party/requests/adapters.py\", line 439, in send\n    resp = conn.urlopen(\n  File \"/Users/joey/Downloads/google-cloud-sdk/lib/third_party/urllib3/connectionpool.py\", line 724, in urlopen\n    retries = retries.increment(\n  File \"/Users/joey/Downloads/google-cloud-sdk/lib/third_party/urllib3/util/retry.py\", line 439, in increment\n    raise MaxRetryError(_pool, url, error or ResponseError(cause))\nurllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='dl.google.com', port=443): Max retries exceeded with url: /dl/cloudsdk/channels/rapid/components-2.json (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self signed certificate in certificate chain (_ssl.c:1123)')))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/joey/Downloads/google-cloud-sdk/bin/bootstrapping/install.py\", line 232, in \n    main()\n  File \"/Users/joey/Downloads/google-cloud-sdk/bin/bootstrapping/install.py\", line 210, in main\n    Install(pargs.override_components, pargs.additional_components)\n  File \"/Users/joey/Downloads/google-cloud-sdk/bin/bootstrapping/install.py\", line 151, in Install\n    _CLI.Execute(['--quiet', 'components', 'list'])\n  File \"/Users/joey/Downloads/google-cloud-sdk/lib/googlecloudsdk/calliope/cli.py\", line 1008, in Execute\n    self._HandleAllErrors(exc, command_path_string, specified_arg_names)\n  File \"/Users/joey/Downloads/google-cloud-sdk/lib/googlecloudsdk/calliope/cli.py\", line 1045, in _HandleAllErrors\n    exceptions.HandleError(exc, command_path_string, self.__known_error_handler)\n  File \"/Users/joey/Downloads/google-cloud-sdk/lib/googlecloudsdk/calliope/exceptions.py\", line 551, in HandleError\n    core_exceptions.reraise(exc)\n  File \"/Users/joey/Downloads/google-cloud-sdk/lib/googlecloudsdk/core/exceptions.py\", line 146, in reraise\n    six.reraise(type(exc_value), exc_value, tb)\n  File \"/Users/joey/Downloads/google-cloud-sdk/lib/third_party/six/__init__.py\", line 693, in reraise\n    raise value\n  File \"/Users/joey/Downloads/google-cloud-sdk/lib/googlecloudsdk/calliope/cli.py\", line 982, in Execute\n    resources = calliope_command.Run(cli=self, args=args)\n  File \"/Users/joey/Downloads/google-cloud-sdk/lib/googlecloudsdk/calliope/backend.py\", line 809, in Run\n    resources = command_instance.Run(args)\n  File \"/Users/joey/Downloads/google-cloud-sdk/lib/surface/components/list.py\", line 102, in Run\n    result = update_manager.List(show_hidden=args.show_hidden,\n  File \"/Users/joey/Downloads/google-cloud-sdk/lib/googlecloudsdk/core/updater/update_manager.py\", line 714, in List\n    to_print, current_version, latest_version = self._GetPrintListWithDiff()\n  File \"/Users/joey/Downloads/google-cloud-sdk/lib/googlecloudsdk/core/updater/update_manager.py\", line 750, in _GetPrintListWithDiff\n    _, diff = self._GetStateAndDiff(command_path='components.list')\n  File \"/Users/joey/Downloads/google-cloud-sdk/lib/googlecloudsdk/core/updater/update_manager.py\", line 641, in _GetStateAndDiff\n    latest_snapshot = self._GetLatestSnapshot(version=version,\n  File \"/Users/joey/Downloads/google-cloud-sdk/lib/googlecloudsdk/core/updater/update_manager.py\", line 624, in _GetLatestSnapshot\n    return snapshots.ComponentSnapshot.FromURLs(\n  File \"/Users/joey/Downloads/google-cloud-sdk/lib/googlecloudsdk/core/updater/snapshots.py\", line 178, in FromURLs\n    data = [\n  File \"/Users/joey/Downloads/google-cloud-sdk/lib/googlecloudsdk/core/updater/snapshots.py\", line 179, in \n    (ComponentSnapshot._DictFromURL(\n  File \"/Users/joey/Downloads/google-cloud-sdk/lib/googlecloudsdk/core/updater/snapshots.py\", line 204, in _DictFromURL\n    response = installers.MakeRequestViaRequests(url, command_path)\n  File \"/Users/joey/Downloads/google-cloud-sdk/lib/googlecloudsdk/core/updater/installers.py\", line 203, in MakeRequestViaRequests\n    return _RawRequestViaRequests(url, headers=headers, timeout=timeout)\n  File \"/Users/joey/Downloads/google-cloud-sdk/lib/googlecloudsdk/core/updater/installers.py\", line 251, in _RawRequestViaRequests\n    return retryer.RetryOnException(\n  File \"/Users/joey/Downloads/google-cloud-sdk/lib/googlecloudsdk/core/util/retry.py\", line 195, in RetryOnException\n    exceptions.reraise(exc_info[1], tb=exc_info[2])\n  File \"/Users/joey/Downloads/google-cloud-sdk/lib/googlecloudsdk/core/exceptions.py\", line 146, in reraise\n    six.reraise(type(exc_value), exc_value, tb)\n  File \"/Users/joey/Downloads/google-cloud-sdk/lib/third_party/six/__init__.py\", line 693, in reraise\n    raise value\n  File \"/Users/joey/Downloads/google-cloud-sdk/lib/googlecloudsdk/core/util/retry.py\", line 176, in TryFunc\n    return func(*args, **kwargs), None\n  File \"/Users/joey/Downloads/google-cloud-sdk/lib/googlecloudsdk/core/updater/installers.py\", line 281, in _ExecuteRequestAndRaiseExceptions\n    response = requests_session.get(\n  File \"/Users/joey/Downloads/google-cloud-sdk/lib/third_party/requests/sessions.py\", line 546, in get\n    return self.request('GET', url, **kwargs)\n  File \"/Users/joey/Downloads/google-cloud-sdk/lib/googlecloudsdk/core/transport.py\", line 243, in WrappedRequest\n    response = orig_request(*modified_args, **modified_kwargs)\n  File \"/Users/joey/Downloads/google-cloud-sdk/lib/googlecloudsdk/core/requests.py\", line 198, in WrappedRequest\n    return orig_request_method(*args, **kwargs)\n  File \"/Users/joey/Downloads/google-cloud-sdk/lib/third_party/requests/sessions.py\", line 533, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/Users/joey/Downloads/google-cloud-sdk/lib/third_party/requests/sessions.py\", line 646, in send\n    r = adapter.send(request, **kwargs)\n  File \"/Users/joey/Downloads/google-cloud-sdk/lib/third_party/requests/adapters.py\", line 514, in send\n    raise SSLError(e, request=request)\nrequests.exceptions.SSLError: HTTPSConnectionPool(host='dl.google.com', port=443): Max retries exceeded with url: /dl/cloudsdk/channels/rapid/components-2.json (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self signed certificate in certificate chain (_ssl.c:1123)')))\n`",
      "solution": "There could be many possible reasons for this error, proxy configuration would be the one but as you mentioned you have no proxy  configured to your network so, the cause could be something else. In order to Authenticate whether this error has occurred  due to proxy or not, you can try running below commands:\n```\n`$ curl https://dl.google.com/\n\nOutput:  Found  \n`\n```\nIf you get a different output from the above mentioned one then, you're behind a proxy or portal.\nYou can refer to  this stackoverflow thread, where a  cloud support user has given an answer by providing various possible reasons that could cause this SSL error, he has also provided the workaround for  resolving this error.\nTry disabling antivirus settings, if there is any such software installed.",
      "question_score": 10,
      "answer_score": 1,
      "created_at": "2021-06-15T02:57:51",
      "url": "https://stackoverflow.com/questions/67978745/how-to-solve-certificate-verify-failed-error-when-install-gcloud"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 76826568,
      "title": "gcloud app deploy complains about missing staging bucket",
      "problem": "I've been used since months or years to delete `staging` and `artifacts` buckets after `gcloud app deploy` command to save on billing costs. These temporary files rest in there forever and I get billed for it.\nIt always worked until this morning when I got this error:\n```\n`ERROR: (gcloud.app.deploy) B instance [staging.my-proj-id.appspot.com] not found: The specified bucket does not exist.\n`\n```\nI tried to recreate the bucket but I'm not allowed since I'm supposed to prove ownership of the `staging.my-proj-id.appspot.com` domain \u2014 which I have not.\n```\n`AccessDeniedException: 403 You must verify site or domain ownership\n`\n```\nI'm stuck and I have important changes to deploy",
      "solution": "I fixed this running this command:\n```\n`gcloud beta app repair\n`\n```\nI got this direction trying to indicate my own staging bucket with\n```\n`gcloud app deploy --bucket=gs://my-own-staging-bucket\n`\n```\nlike suggested in this answer which returned:\n```\n`ERROR: (gcloud.app.deploy) Error Response: [5] Staging bucket staging.my-proj-id.appspot.com is not available. Please refer to https://cloud.google.com/appengine/docs/admin-api/reference/rest/v1/apps/repair to repair your app.\n`\n```",
      "question_score": 10,
      "answer_score": 32,
      "created_at": "2023-08-03T11:13:39",
      "url": "https://stackoverflow.com/questions/76826568/gcloud-app-deploy-complains-about-missing-staging-bucket"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 74836213,
      "title": "`Error 403: Insufficient regional quota to satisfy request: resource &quot;SSD_TOTAL_GB&quot;` when creating kubernetes cluster with terraform",
      "problem": "Hi I am playing around with kubernetes and terraform in a google cloud free tier account (trying to use the free 300$). Here is my terraform resource declaration, it is something very standard I copied from the terraform resource page. Nothing particularly strange here.\n```\n`resource \"google_container_cluster\" \"cluster\" {\n  name = \"${var.cluster-name}-${terraform.workspace}\"\n  location = var.region\n  initial_node_count = 1\n  project = var.project-id\n  remove_default_node_pool = true\n}\n\nresource \"google_container_node_pool\" \"cluster_node_pool\" {\n  name       = \"${var.cluster-name}-${terraform.workspace}-node-pool\"\n  location   = var.region\n  cluster    = google_container_cluster.cluster.name\n  node_count = 1\n\n  node_config {\n    preemptible  = true\n    machine_type = \"e2-medium\"\n    service_account = google_service_account.default.email\n    oauth_scopes    = [\n      \"https://www.googleapis.com/auth/cloud-platform\"\n    ]\n  }\n}\n`\n```\nThis terraform snippet used to work fine. In order to not burn through the 300$ too quickly, at the end of every day I used to destroy the cluster with `terraform destroy`.\nHowever one day the kubernetes cluster creation just stopped working. Here is the error:\n```\n`Error: googleapi: Error 403: Insufficient regional quota to satisfy request: resource \"SSD_TOTAL_GB\": request requires '300.0' and is short '50.0'. project has a quota of '250.0' with '250.0' available. View and manage quotas at https://console.cloud.google.com/iam-admin/quotas?usage=USED&project=xxxxxx., forbidden\n`\n```\nIt looks like something didn't get cleaned up after all the terraform destroy and eventually some quota built up and I am not able to create a cluster anymore. I am still able to create a cluster through the google cloud web interface (I tried only with autopilot, and in the same location). I am a bit puzzled why this is happening. Is my assumption correct? Do I need to delete something that doesn't get deleted automatically with terraform? if yes why? Is there a way to fix this and be able to create the cluster with terraform again?",
      "solution": "I ran into the same issue and I think I figured out what's going. The crucial thing here is to understand the difference between zonal and regional clusters.\ntldr; A zonal cluster operates in only zone, where a regional cluster may be replicated across multiple zones.\nFrom the doc,\n\nBy default, GKE replicates each node pool across three zones of the control plane's region\n\nI think this is why we're seeing the requirement going to 300GB (3 * 100GB), where the `--disk-size` defaults to 100GB.\nThe solution is to set the `location` to a `zone` than a `region`. Of course, here I'm assuming a zonal cluster would satisfy your requirements. E.g.\n```\n`resource \"google_container_cluster\" \"cluster\" {\n  name = \"${var.cluster-name}-${terraform.workspace}\"\n  location = \"us-central1-f\"\n  initial_node_count = 1\n  project = var.project-id\n  remove_default_node_pool = true\n}\n\nresource \"google_container_node_pool\" \"cluster_node_pool\" {\n  name       = \"${var.cluster-name}-${terraform.workspace}-node-pool\"\n  location   = \"us-central1-f\"\n  cluster    = google_container_cluster.cluster.name\n  node_count = 1\n\n  node_config {\n    ...\n  }\n}\n`\n```",
      "question_score": 10,
      "answer_score": 16,
      "created_at": "2022-12-17T18:48:40",
      "url": "https://stackoverflow.com/questions/74836213/error-403-insufficient-regional-quota-to-satisfy-request-resource-ssd-total"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 73026671,
      "title": "How do I now (since June 2022) send an email via Gmail using a Python script?",
      "problem": "I had a Python script which did this. I had to enable something in the Gmail account. For maybe 3 years the script then ran like this:\n```\n`import smtplib, ssl\n...\nsubject = 'some subject message'\nbody = \"\"\"text body of the email\"\"\"\nsender_email = 'my_gmail_account_name@gmail.com'\nreceiver_email = 'some_recipient@something.com'\n\n# Create a multipart message and set headers\nmessage = MIMEMultipart()\nmessage['From'] = 'Mike'\nmessage['To'] = receiver_email\nmessage['Subject'] = subject\n# Add body to email\nmessage.attach(MIMEText(body, 'plain'))\n# Open file in binary mode\nwith open( client_zip_filename, 'rb') as attachment:\n    # Add file as application/octet-stream\n    # Email client can usually download this automatically as attachment\n    part = MIMEBase('application', 'octet-stream')\n    part.set_payload(attachment.read())\n# Encode file in ASCII characters to send by email    \nencoders.encode_base64(part)\n# Add header as key/value pair to attachment part\npart.add_header(\n    'Content-Disposition',\n    f'attachment; filename={subject}',\n)\n# Add attachment to message and convert message to string\nmessage.attach(part)\ntext = message.as_string()\n# Log in to server using secure context and send email\ncontext = ssl.create_default_context()\nwith smtplib.SMTP_SSL('smtp.gmail.com', 465, context=context) as server:\n    print( 'waiting to login...')\n    server.login(sender_email, password)\n    print( 'waiting to send...')\n    server.sendmail(sender_email, receiver_email, text)\nprint( 'email appears to have been sent')\n`\n```\nIn May or so of this year I got a message from Google saying that authority to use emails from scripts would be tightened. \"Oh dear\", I thought.\nSome time in June I found that the above script no longer works, and raises an exception, specifically on the line `server.login(sender_email, password)`:\n```\n`  ...\n  File \"D:\\My documents\\software projects\\operative\\sysadmin_py\\src\\job_backup_routine\\__main__.py\", line 307, in main\n    server.login(sender_email, password)\n  File \"c:\\users\\mike\\appdata\\local\\programs\\python\\python39\\lib\\smtplib.py\", line 745, in login\n    raise last_exception\n  File \"c:\\users\\mike\\appdata\\local\\programs\\python\\python39\\lib\\smtplib.py\", line 734, in login\n    (code, resp) = self.auth(\n  File \"c:\\users\\mike\\appdata\\local\\programs\\python\\python39\\lib\\smtplib.py\", line 657, in auth\n    raise SMTPAuthenticationError(code, resp)\nsmtplib.SMTPAuthenticationError: (535, b'5.7.8 Username and Password not accepted. \nLearn more at\\n5.7.8  https://support.google.com/mail/?p=BadCredentials p14-20020aa7cc8e000000b00435651c4a01sm8910838edt.56 - gsmtp')\n`\n```\n... I was thus not entirely surprised by this, and have now gone looking for a solution.\n\nI have got this idea that the way forward is something called \"OAuth consent\" (I don't have any idea what this is...)\n\nI found this answer and tried to follow the steps there. Here is my account of trying to follow step 1:\n\nI went to this Google configuration page and chose \"my_gmail_account_name\", the account I want to send emails from ...\n\nnew \"project\", name: test-project-2022-07-18\n\nlocation: default (\"No organisation\")\n\nclicked Create\n\nclicked NEXT\n\nclicked ENABLE\n\nclicked the icon to enable the \"Google Developer Console\"\n\nin the hamburger menu (top left) there is an item \"APIs and services\" ... one item there is \"Credentials\" - clicked\n\none item in the left-hand list is \"OAuth consent screen\"\n\nanother item is \"Credentials\". Clicked this: then, at the top, \"+ CREATE CREDENTIALS\"\n\nin the dropdown menu, choose \"OAuth Client ID\"\n\nclicked \"CONFIGURE CONSENT SCREEN\"\n\nradio buttons: \"Internal\" and \"External\". chose latter.\n\nclicked \"CREATE\"\n\nunder \"App information\":\n\n\"App name\": sysadmin_py\n\n\"User support email\": my_gmail_account_name@gmail.com\n\n\"Developer contact information\": my_gmail_account_name@gmail.com\n\nclicked \"SAVE AND CONTINUE\"\n\nthen find myself on a page about \"SCOPES\", with a button \"ADD OR REMOVE SCOPES\"...\n\nAt this point I'm meant to be following \"Step 1\" instruction \"d. Select the application type Other, enter the name \"Gmail API Quickstart\" and click the Create button\"... but nothing of this kind is in view!\nThe update to that answer was done in 2021-04. A year later the interface in Google appears to have changed radically. Or maybe I have taken the wrong path and disappeared down a rabbit hole.\nI have no idea what to do. Can anyone help?",
      "solution": "Google has recently made changes to access of less secure apps (read here: https://myaccount.google.com/lesssecureapps).\nIn order to make your script work again, you'll need to make a new app password for it. Directions to do so are below:\n\nGo to My Account in Gmail and click on Security.\nAfter that, scroll down to choose the Signing into Google option.\nNow, click on App Password. (Note: You can see this option when two-step authentication is enabled). To enable two-step authentication:\n\nFrom the Signing into Google, click on the Two-step Verification option and then enter the password.\nThen Turn ON the two-step verification by entering the OTP code received on the mobile.\n\n(Here's a quick link to the same page: https://myaccount.google.com/apppasswords)\n\nHere, you can see a list of applications, choose the required one.\nNext, pick the Select Device option and click on the device which is being used to operate Gmail.\nNow, click on Generate.\nAfter that, enter the Password shown in the Yellow bar.\nLastly, click on Done.\n\n(Source: https://www.emailsupport.us/blog/gmail-smtp-not-working/)\nSimply switch out the password the script is using for this newly generated app password. This worked for me and I wish the same for you.\nI hope this helps!",
      "question_score": 10,
      "answer_score": 26,
      "created_at": "2022-07-18T20:00:52",
      "url": "https://stackoverflow.com/questions/73026671/how-do-i-now-since-june-2022-send-an-email-via-gmail-using-a-python-script"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 70143131,
      "title": "Application Default Credentials in Google Cloud Build",
      "problem": "Within my code, I am attempting to gather the Application Default Credentials from the associated service account in Cloud Build:\n`from google.auth import default\n\ncredentials, project_id = default()\n`\nThis works fine in my local space because I have set the environment variable `GOOGLE_APPLICATION_CREDENTIALS` appropriately. However, when this line is executed (via a test step in my build configuration) within Cloud Build, the following error is raised:\n```\n`google.auth.exceptions.DefaultCredentialsError: Could not automatically determine credentials. \nPlease set GOOGLE_APPLICATION_CREDENTIALS or explicitly create credentials and re-run the application. \nFor more information, please see https://cloud.google.com/docs/authentication/getting-started\n`\n```\nThis is confusing me because, according to the docs:\n\nBy default, Cloud Build uses a special service account to execute builds on your behalf. This service account is called the Cloud Build service account and it is created automatically when you enable the Cloud Build API in a Google Cloud project.\nRead Here\n\nIf the environment variable GOOGLE_APPLICATION_CREDENTIALS isn't set, ADC uses the service account that is attached to the resource that is running your code.\nRead Here\n\nSo why is the default call not able to access the Cloud Build service account credentials?",
      "solution": "There is a trick: you have to define the network to use in your Docker build. Use the parameter `--network=cloudbuild`, like that\n```\n`steps:\n  - name: gcr.io/cloud-builders/docker\n    entrypoint: 'docker'\n    args: \n      - build\n      - '--no-cache'\n      - '--network=cloudbuild'\n      - '-t'\n      - '$_GCR_HOSTNAME/$PROJECT_ID/$REPO_NAME/$_SERVICE_NAME:$COMMIT_SHA'\n      - .\n      - '-f' \n      - 'Dockerfile'\n...\n`\n```\nYou can find the documentation here",
      "question_score": 10,
      "answer_score": 24,
      "created_at": "2021-11-28T12:53:59",
      "url": "https://stackoverflow.com/questions/70143131/application-default-credentials-in-google-cloud-build"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 67907211,
      "title": "How to resolve &quot;googleapi: Error 403: The caller does not have permission, forbidden&quot;",
      "problem": "I am using terraform to build infra in GCP. I am trying to assign roles to a `service account` using terraform but unable to do so. Below is my code:\nsa.tf:\n```\n`resource \"google_service_account\" \"mojo-terra\" {\n  account_id   = \"mojo-terra\"\n  description  = \"Service account used for terraform script\"\n}\n\nresource \"google_project_iam_member\" \"mojo-roles\" {\n  count = length(var.rolesList)\n  role =  var.rolesList[count.index]\n  member = \"serviceAccount:${google_service_account.mojo-terra.email}\"\n}\n`\n```\ndev.tfvars:\n```\n`rolesList = [\n    \"roles/iam.serviceAccountUser\"\n]\n`\n```\ncloudbuild logs:\n```\n`Step #2: Error: Error when reading or editing Resource \"project \\\"poc-dev\\\"\" with IAM Policy: Error retrieving IAM policy for project \"poc-dev\": googleapi: Error 403: The caller does not have permission, forbidden\nStep #2: \nStep #2: \nStep #2: \nStep #2: Error: Error when reading or editing Resource \"project \\\"poc-dev\\\"\" with IAM Member: Role \"roles/iam.serviceAccountUser\" Member \"serviceAccount:asadsfs@poc-dev-1221.iam.gserviceaccount.com\": Error retrieving IAM policy for project \"poc-dev\": googleapi: Error 403: The caller does not have permission, forbidden\nStep #2: \n`\n```\nBelow are the roles attached to my `cloudbuild service account`:\n`Custom Role cloudbuild, Cloud Build Service Account, Service Account Admin, Create Service Accounts, Delete Service Accounts, Service Account User, Storage Admin`",
      "solution": "The service account providing authorization to Terraform is missing the permission `resourcemanager.projects.getIamPolicy` which is the source of the error message.\nThe service account is also missing the permission `resourcemanager.projects.setIamPolicy` which is required to change IAM policies.\nThose permissions are part of the role `roles/resourcemanager.projectIamAdmin` (Project IAM Admin).\nTo list the roles assigned to the service account:\n```\n`gcloud projects get-iam-policy  \\\n--flatten=\"bindings[].members\" \\\n--format='table(bindings.role)' \\\n--filter=\"bindings.members:\"\n`\n```\nTo list the permissions that a role contains:\n```\n`gcloud iam roles describe roles/resourcemanager.projectIamAdmin\n`\n```\nTo add the required role to the service account:\n```\n`gcloud projects add-iam-policy-binding  \\\n--member=serviceAccount: \\\n--role=roles/resourcemanager.projectIamAdmin\n`\n```",
      "question_score": 10,
      "answer_score": 24,
      "created_at": "2021-06-09T17:25:41",
      "url": "https://stackoverflow.com/questions/67907211/how-to-resolve-googleapi-error-403-the-caller-does-not-have-permission-forbi"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 73108978,
      "title": "google cloud pub sub multiple subscriber for the same message with the same subscription",
      "problem": "I am working on implementing event driven using GCP Pub/Sub.\nI have a topic called orders, and that topic will have a subscription named `orderPlacedSubscription` I have two services that want to listen to all messages for this subscription and perform different actions,\nso I have paymentService and notificationService, the paymentService will listen to each message filtered by `orderPlacedSubscription` and process the payment as well as the notification service will listen to the same message and send a notification.\nMy question\n\ndoes pub-sub support having two subscribers related to one subscription and both receive messages and acknowledge them separately?\nIn case if each subscriber can acknowledge the message separately without affecting the other subscriber, does google cloud pub-sub support retry for different subscribers in case of failure from one subscriber?",
      "solution": "Yes, a subscription can have multiple subscriber clients.\nIn a subscription workflow, if a message is not acknowledged by a subscriber, Pub/Sub will attempt to redeliver the outstanding message. In the process of redelivering the outstanding message, Pub/Sub holds back and tries not to deliver the outstanding message to any other subscriber on the same subscription. Once the outstanding message is acknowledged it can be  delivered to other subscribers.\nYou can refer subscription workflow and this  documentation for more information.",
      "question_score": 10,
      "answer_score": 9,
      "created_at": "2022-07-25T14:08:27",
      "url": "https://stackoverflow.com/questions/73108978/google-cloud-pub-sub-multiple-subscriber-for-the-same-message-with-the-same-subs"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 69568879,
      "title": "Where can I see the PubSub service account?",
      "problem": "The PubSub service account is `service-@gcp-sa-pubsub.iam.gserviceaccount.com`\nThe command described here to create an IAM policy binding succeeds, which shows that the service account exists.\nBut it does not appear in\n\nthe list of service accounts in the console `https://console.cloud.google.com/iam-admin/serviceaccounts?project=` (screenshot below)\nnor in the IAM permissions list `https://console.cloud.google.com/iam-admin/iam?project=` , even when I check \"Include Google-provided role grants\"\nnor in the output of `gcloud iam service-accounts list --project `.\n\nWhere can I see this service account listed?",
      "solution": "The `service-@gcp-sa-pubsub.iam.gserviceaccount.com` is a Google managed service account, therefore, you can't see it in the list of YOUR PROJECT service accounts.\nIn addition, you have granted this service account on a TOPIC resource and not on a PROJECT resource. Therefore, when you go on the iam-admin page, you can't see the service account at the PROJECT resource.\nAnyway, you could be able to view it in the iam-admin page by checking `Include Google-provided role grants`\n\nBecause you have granted the service account at TOPIC resource level, you can see it in the TOPIC page.\nGo to the topic page, check a topic and go to the right-hand panel, in the permission section and look at the role that you grant on the service account. You will find it.",
      "question_score": 10,
      "answer_score": 12,
      "created_at": "2021-10-14T12:12:14",
      "url": "https://stackoverflow.com/questions/69568879/where-can-i-see-the-pubsub-service-account"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 68284263,
      "title": "Google Cloud Function The request was aborted because there was no available instance",
      "problem": "When running some cloud functions, I am getting sometimes the error:\n\nThe request was aborted because there was no available instance.\n\nI have seen other questions being asked with similar error but for Cloud Run where you can specify the number of instances available, but it doesnt look like there is such a thing with Cloud Function. So how to solve this problem?\nI can see on the quotas page that there are limits for background functions but none for HTTP functions. I am calling the lambda function via HTTP, and it is deployed in `us-central1`",
      "solution": "According to GCP dashboard, there's a currently ongoing deployment failures for both region `us-central1` and `europe-west1`. For now there are two reasons I can think of behind the error:\n\nThe ongoing issue has an indirect effect where no instances can currently handle the request.\nThere's an initial bursts of requests coming, to the point where Cloud Functions cannot scale fast enough despite not hitting `max_instances`.\n\nSolution for #1 is to temporarily re-deploy your function to another region. The solution for #2 is to implement some sort of retry logic into your function. Note that automatic retries are not available in HTTP functions, so you will have to implement it with your own logic.\nIf it doesn't work for you, then there's another option to implement a queuing mechanism with Cloud Tasks to handle sudden bursts of traffic and handle retries if the request fails.",
      "question_score": 10,
      "answer_score": 5,
      "created_at": "2021-07-07T12:26:19",
      "url": "https://stackoverflow.com/questions/68284263/google-cloud-function-the-request-was-aborted-because-there-was-no-available-ins"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 75361466,
      "title": "Google Cloud Logging assigns ERROR severity to all Python logging.info calls",
      "problem": "It's the first time I use Google Cloud Platform, so please be understanding!\nI've built a scheduled workflow that simply runs a Batch job. The job runs Python code and uses the standard `logging` library for logging. When the job is executed, I can correctly see all the entries in Cloud Logging, but all the entries have severity ERROR although they're all INFO.\n\nOne possible reason I've been thinking about is that I haven't used the `setup_logging` function as described in the documentation here. The thing is, I didn't want to run the Cloud Logging setup when I run the code locally.\nThe questions I have are:\n\nwhy does logging \"work\" (in the sense that logs end up in Cloud Logging) even if I did not use the `setup_logging` function? What is it's real role?\nwhy do my INFO entries show up with ERROR severity?\nif I include that snippet and that snippet solves this issue, should I include an if statement in my code that detects if I am running the code locally and skips that Cloud Logging setup step?",
      "solution": "According to the documentation, you have to use a setup to send correctly logs to `Cloud Logging`.\nThis setup allows then to use the `Python` logging standard library.\n\nOnce installed, this library includes logging handlers to connect\nPython's standard logging module to Logging, as well as an API client\nlibrary to access Cloud Logging manually.\n\n`# Imports the Cloud Logging client library\nimport google.cloud.logging\n\n# Instantiates a client\nclient = google.cloud.logging.Client()\n\n# Retrieves a Cloud Logging handler based on the environment\n# you're running in and integrates the handler with the\n# Python logging module. By default this captures all logs\n# at INFO level and higher\nclient.setup_logging()\n`\nThen you can use the `Python` standard library to add logs to `Cloud Logging`.\n`# Imports Python standard library logging\nimport logging\n\n# The data to log\ntext = \"Hello, world!\"\n\n# Emits the data using the standard logging module\nlogging.warning(text)\n`\n\nwhy does logging \"work\" (in the sense that logs end up in Cloud Logging) even if I did not use the setup_logging function? What is\nit's real role?\n\nWithout the setup, the log will be added to `Cloud Logging` but not with the correct type and as expected. It's better to use the setup.\n\nwhy do my INFO entries show up with ERROR severity?\n\nThe same reason explained above\n\nif I include that snippet and that snippet solves this issue, should I include an if statement in my code that detects if I am running the\ncode locally and skips that Cloud Logging setup step?\n\nI think no need to add a `if` statement you run the code locally. In this case, the logs should be printed in the console even if the setup is present.",
      "question_score": 10,
      "answer_score": 6,
      "created_at": "2023-02-06T13:44:01",
      "url": "https://stackoverflow.com/questions/75361466/google-cloud-logging-assigns-error-severity-to-all-python-logging-info-calls"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 69904211,
      "title": "Vertex AI prediction - Autoscaling cannot set minimum node to 0",
      "problem": "I am unclear abut Vertex AI pricing for model predictions. In the documentation, under the heading More about automatic scaling of prediction nodes one of the points mentioned is:\n\n\"If you choose automatic scaling, the number of nodes scales\nautomatically, and can scale down to zero for no-traffic durations\"\n\nThe example provided in the documentation later also seems to suggest that during a period with no traffic, zero nodes are in use. However, when I create an Endpoint in Vertex AI, under the Autoscaling heading it says:\n\n\"Autoscaling: If you set a minimum and maximum, compute nodes will scale to meet traffic demand within those boundaries\"\n\nThe  value of 0 under \"Minimum number of compute nodes\" is not allowed so you have to enter 1 or greater, and it is mentioned that:\n\nDefault is 1. If set to 1 or more, then compute resources will\ncontinuously run even without traffic demand. This can increase cost\nbut avoid dropped requests due to node initialization.\n\nMy question is, what happens when I select autoscaling by setting Minimum to 1 and Maximum to, say, 10. Does 1 node always run continuously? Or does it scale down to 0 nodes in no traffic condition as the documentation suggests.\nTo test I deployed an Endpoint with Autoscaling (min and max set to 1) and then when I sent a prediction request the response was almost immediate, suggesting the node was already up. I did that again after about an hour and again the response was immediate suggesting that the node never shut down probably. Also, for high latency requirements, is having autoscale to 0 nodes, if that is indeed possible, even practical, i.e., what latency can we expect for starting up from 0 nodes?",
      "solution": "Are you using an N1 or a non-N1 machine type? If you want to autoscale to zero, you must use non-N1 machines. See second note from node allocation:\n\nNote: Versions that use a Compute Engine (N1) machine type cannot scale down to zero nodes. They can scale down to 1 node, at minimum.\n\nUpdate: AI Platform supports scaling to zero, while Vertex AI currently does not. From the scaling documentation, nodes can scale but there is no mention that it can scale down to zero. Here's a public feature request for people who wants to track this issue.\nWith regards to latency requirements, the actual output will vary. However, one thing to note according to the documentation is that the service may not be able to bring nodes online fast enough to keep up with large spikes of request traffic. If your traffic regularly has steep spikes, and if reliably low latency is important to your application, you may want to consider manual scaling.\nAdditional Reference: https://cloud.google.com/ai-platform/prediction/docs/machine-types-online-prediction#automatic_scaling",
      "question_score": 10,
      "answer_score": 5,
      "created_at": "2021-11-09T20:49:36",
      "url": "https://stackoverflow.com/questions/69904211/vertex-ai-prediction-autoscaling-cannot-set-minimum-node-to-0"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 67510082,
      "title": "Google function Build failed: found incompatible dependencies (Python)",
      "problem": "I have a python 3.8 code deployed with google cloud functions and everything was working smoothly until today, but I made few updates to the code and re-deployed my function. Since then I'm getting the below error.\n\nDeployment failure:\nBuild failed: found incompatible dependencies: \"functions-framework 2.1.2 has requirement click=7.0, but you have click 8.0.0.\"; Error ID: 945b0f01\n\nI also tried to re-deploy the existing the code which was deployed successfully a week ago, but I was still getting the above error. It would be great if anyone can share their thoughts on this error.\nBelow is my requirements.txt file\n```\n`pandas >= 1.1.5 \nfsspec >= 2021.04.0\ngcsfs >= 2021.04.0\nrequests >= 2.23.0\nalpaca-trade-api >= 0.51\ndatapackage >= 1.15\nstatsmodels >= 0.12.1\nsendgrid\n`\n```",
      "solution": "Simply include in your requirements.txt the version functions-framework==2.1.2. It looks like during deploy gcp requires this module and by default is going to use the latest version (functions-framework 2.1.3). Thus the incompatibility of versions. I had several cloud functions that I could deploy partly because of this error...among 7 cloud functions 3 would not deploy because of the missing functions-framework 2.1.2 in the requirements.txt",
      "question_score": 10,
      "answer_score": 3,
      "created_at": "2021-05-12T21:42:53",
      "url": "https://stackoverflow.com/questions/67510082/google-function-build-failed-found-incompatible-dependencies-python"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 70272414,
      "title": "Lots of &quot;Uncaught signal: 6&quot; errors in Cloud Run",
      "problem": "I have a Python (3.x) webservice deployed in GCP. Everytime Cloud Run is shutting down instances, most noticeably after a big load spike, I get many logs like these `Uncaught signal: 6, pid=6, tid=6, fault_addr=0.` together with `[CRITICAL] WORKER TIMEOUT (pid:6)` They are always signal 6.\nThe service is using FastAPI and Gunicorn running in a Docker with this start command\n```\n`CMD gunicorn -w 2 -k uvicorn.workers.UvicornWorker -b 0.0.0.0:8080 app.__main__:app\n`\n```\nThe service is deployed using Terraform with 1 gig of ram, 2 cpu's and the timeout is set to 2 minutes\n```\n`resource \"google_cloud_run_service\"  {\n  name     = \n  location = \n\n  template {\n    spec {\n      service_account_name = \n      timeout_seconds = 120\n      containers {\n        image = var.image\n        env {\n          name = \"GCP_PROJECT\"\n          value = var.project\n        }\n        env {\n          name = \"BRANCH_NAME\"\n          value = var.branch\n        }\n        resources {\n          limits = {\n            cpu = \"2000m\"\n            memory = \"1Gi\"\n          }\n        }\n      }\n    }\n  }\n  autogenerate_revision_name = true\n}\n`\n```\nI have already tried tweaking the resources and timeout in Cloud Run, using the --timeout and --preload flag for gunicorn as that is what people always seem to recommend when googling the problem but all without success. I also dont exactly know why the workers are timing out.",
      "solution": "Unless you have enabled CPU is always allocated,  background threads and processes might stop receiving CPU time after all HTTP requests return. This means background threads and processes can fail, connections can timeout, etc. I cannot think of any benefits to running background workers with Cloud Run except when setting the --cpu-no-throttling flag. Cloud Run instances that are not processing requests, can be terminated.\nSignal 6 means abort which terminates processes. This probably means your container is being terminated due to a lack of requests to process.\nRun more workloads on Cloud Run with new CPU allocation controls\nWhat if my application is doing background work outside of request processing?",
      "question_score": 9,
      "answer_score": 9,
      "created_at": "2021-12-08T10:09:15",
      "url": "https://stackoverflow.com/questions/70272414/lots-of-uncaught-signal-6-errors-in-cloud-run"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 69410134,
      "title": "Gitlab Cloud run deploy successfully but Job failed",
      "problem": "Im having an issue with my CI/CD pipeline ,\nits successfully deployed to GCP cloud run but on Gitlab dashboard the status is failed.\nI tried to replace images to some other docker images but it fails as well .\n```\n` # File: .gitlab-ci.yml\nimage: google/cloud-sdk:alpine\ndeploy_int:\n  stage: deploy\n  environment: integration\n  only:\n  - integration    # This pipeline stage will run on this branch alone\n  script:\n    - echo $GCP_SERVICE_KEY > gcloud-service-key.json # Google Cloud service accounts\n    - gcloud auth activate-service-account --key-file gcloud-service-key.json\n    - gcloud config set project $GCP_PROJECT_ID\n    - gcloud builds submit . --config=cloudbuild_int.yaml\n\n# File: cloudbuild_int.yaml\nsteps:\n    # build the container image\n  - name: 'gcr.io/cloud-builders/docker'\n    args: [ 'build','--build-arg','APP_ENV=int' , '-t', 'gcr.io/$PROJECT_ID/tpdropd-int-front', '.' ]\n    # push the container image\n  - name: 'gcr.io/cloud-builders/docker'\n    args: [ 'push', 'gcr.io/$PROJECT_ID/tpdropd-int-front']\n    # deploy to Cloud Run\n  - name: \"gcr.io/cloud-builders/gcloud\"\n    args: ['run', 'deploy', 'tpd-front', '--image', 'gcr.io/$PROJECT_ID/tpdropd-int-front', '--region', 'us-central1', '--platform', 'managed', '--allow-unauthenticated']\n`\n```\ngitlab build output :\n```\n`ERROR: (gcloud.builds.submit) \nThe build is running, and logs are being written to the default logs bucket.\nThis tool can only stream logs if you are Viewer/Owner of the project and, if applicable, allowed by your VPC-SC security policy.\nThe default logs bucket is always outside any VPC-SC security perimeter.\nIf you want your logs saved inside your VPC-SC perimeter, use your own bucket.\nSee https://cloud.google.com/build/docs/securing-builds/store-manage-build-logs.\nCleaning up project directory and file based variables\n00:01\nERROR: Job failed: exit code 1\n`\n```",
      "solution": "I fix it by using:\n```\n`options:\n  logging: CLOUD_LOGGING_ONLY\n`\n```\nin cloudbuild.yaml",
      "question_score": 9,
      "answer_score": 11,
      "created_at": "2021-10-01T19:51:37",
      "url": "https://stackoverflow.com/questions/69410134/gitlab-cloud-run-deploy-successfully-but-job-failed"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 71781063,
      "title": "GCP workload identity federation - Github provider - &#39;Unable to acquire impersonated credentials&#39;",
      "problem": "I've followed these instructions to the letter to allow me to use the short lived token authentication method to access gcloud resources from our github actions workflow.\nI've created the service account, workload identity pool and github provider pool using the exact instructions above, but it doesn't appear that the auth step is getting the correct token (or any token at all). The GCP service account has the correct IAM permissions.\nOn the `gcloud compute instances list` step, I'm receiving the error:\n```\n`ERROR: (gcloud.compute.instances.list) There was a problem refreshing your current auth tokens: ('Unable to acquire impersonated credentials: No access token or invalid expiration in response.', '{\\n  \"error\": {\\n    \"code\": 403,\\n    \"message\": \"The caller does not have permission\",\\n    \"status\": \"PERMISSION_DENIED\"\\n  }\\n}\\n')\nPlease run:\n\n  $ gcloud auth login\n\nto obtain new credentials.\n`\n```\nMy github actions file is as follows:\n`jobs:\n  backup:\n    permissions:\n      contents: 'read'\n      id-token: 'write'\n    runs-on: ubuntu-latest\n    steps:\n      - name: 'Checkout code'\n        uses: actions/checkout@v3\n\n      - id: 'auth'\n        name: 'Authenticate to Google Cloud'\n        uses: 'google-github-actions/auth@v0'\n        with:\n          workload_identity_provider: 'projects/*******/locations/global/workloadIdentityPools/**REDACTED**/providers/github-provider'\n          service_account: '**REDACTED**@**REDACTED**.iam.gserviceaccount.com'\n\n      # Install gcloud, `setup-gcloud` automatically picks up authentication from `auth`.\n      - name: 'Set up Cloud SDK'\n        uses: 'google-github-actions/setup-gcloud@v0'\n\n      - name: 'Use gcloud CLI'\n        run: gcloud compute instances list\n`\nI enabled logging for the token exchange and I can see it occurring (with no obvious errors) in GCP logs either. So I'm completely stumped.\nAny ideas?",
      "solution": "So I later found out what this was. Despite running:\n```\n`gcloud iam service-accounts add-iam-policy-binding SERVICE_ACCOUNT_EMAIL \\\n    --role=roles/iam.workloadIdentityUser \\\n    --member=\"MEMBER_EXPRESSION\"\n`\n```\nAs per the docs, it had not granted permission - I went into the console and checked the workload identity pool under \"connected service accounts\" menu (to the left) and the service account wasn't in there, so I added it manually.",
      "question_score": 9,
      "answer_score": 5,
      "created_at": "2022-04-07T13:09:27",
      "url": "https://stackoverflow.com/questions/71781063/gcp-workload-identity-federation-github-provider-unable-to-acquire-imperson"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 71498895,
      "title": "Organization Admin somehow doesn&#39;t have access to create a folder in GCP?",
      "problem": "I'm pretty sure this is an actual bug with GCP at the moment. I'm the Organization Admin for the GCP organization (I've quadruple checked this, and that I'm signed in with the correct account).\nBut when I go to Manage Resources, And try to create a new folder, it doesn't let me select the organization as the location, because I \"don't have the required resourcemanager.folders.create permission\". If I try to create the folder in a project that's in the organization, I get \"Unknown error\".\nI'm the user who created the organization and all projects in the first place, and the only G-Suite user that even exists on this domain.",
      "solution": "If you review the permissions that Organization Administrator has, resourcemanager.folders.create is not one of them.\nIAM Roles\nOrg Admin by itself has almost infinite power because it can set IAM policies. This means the Org Admin can grant any IAM permission to any identity.\nGrant yourself the required role such as roles/resourcemanager.folderAdmin.\nNote: I recommend keeping the Org Admin as a separate identity that you lock away and only use to manage the organization. Create separate identities for day-to-day operations, development, and deployment.",
      "question_score": 9,
      "answer_score": 10,
      "created_at": "2022-03-16T15:31:23",
      "url": "https://stackoverflow.com/questions/71498895/organization-admin-somehow-doesnt-have-access-to-create-a-folder-in-gcp"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 66674068,
      "title": "set log severity on google cloud without using google-cloud-logging library",
      "problem": "I am trying to correctly output logs on my service running on google cloud, and for the most part they are correctly identified (`DEBUG` and `INFO` logs, being sent to `stdout`, are marked as info, whereas `WARNING`, `ERROR`, and `CRITICAL` logs are sent to `stderr` and are marked as error). Now, I am trying to get the exact severity out of them, without needing to use the `google-cloud-logging` library. Is there a way where I can accomplish this?\nHere an example of what I currently obtain is shown, with severity (icon on the left) matching whether the log comes from `stdout` or `stderr`.\n\nThis is what I'm trying to obtain, but without using the `google-cloud-logging` library\n\nEdit:\nmy logs are written to the output streams in json format, by using the `python-json-logger` library for python. My google cloud logs have their information stored as in the picture below. We are not using fluentd for log parsing.",
      "solution": "After some research and help from\n@SerhiiRohoza It doesn't seem you can, so in order to set the severity on google cloud you need to add the google-cloud-logging library to your project and set it up as described on the documentation.",
      "question_score": 9,
      "answer_score": 7,
      "created_at": "2021-03-17T14:27:05",
      "url": "https://stackoverflow.com/questions/66674068/set-log-severity-on-google-cloud-without-using-google-cloud-logging-library"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 66015207,
      "title": "TypeError: querySnapshot.forEach is not a function - Firebase Cloud Functions",
      "problem": "I have a data structure made this way:\n\nPosts(collection)\nUserId(document)\nPosts(collection)\npostDoc (document)\n\nAnd I'm setting a cloud function to change all Posts(subcollection) documents upon a certain event, and in order to do so I'm using `collectionGroup` queries:\nThis is how I set it up:\n```\n`exports.changeIsVisibleFieldAfterDay = functions.pubsub\n.schedule(\"every 2 minutes\").onRun((context) => {\n  const querySnapshot = db.collectionGroup(\"Posts\")\n      .where(\"isVisible\", \"==\", true).get();\n  querySnapshot.forEach((doc) => {\n    console.log(doc.data());\n  });\n});\n`\n```\nIn the `Firebase Log` I receive the following error:\n```\n`TypeError: querySnapshot.forEach is not a function\n`\n```\nSearching online I found out that there may be a problem with Firebase SDK version but mine is uptodate, I attach the `package.json` file here:\n```\n`{\n  \"name\": \"functions\",\n  \"description\": \"Cloud Functions for Firebase\",\n  \"scripts\": {\n    \"lint\": \"eslint .\",\n    \"serve\": \"firebase emulators:start --only functions\",\n    \"shell\": \"firebase functions:shell\",\n    \"start\": \"npm run shell\",\n    \"deploy\": \"firebase deploy --only functions\",\n    \"logs\": \"firebase functions:log\"\n  },\n  \"engines\": {\n    \"node\": \"12\"\n  },\n  \"main\": \"index.js\",\n  \"dependencies\": {\n    \"firebase-admin\": \"^9.2.0\",\n    \"firebase-functions\": \"^3.11.0\"\n  },\n  \"devDependencies\": {\n    \"eslint\": \"^7.6.0\",\n    \"eslint-config-google\": \"^0.14.0\",\n    \"firebase-functions-test\": \"^0.2.0\"\n  },\n  \"private\": true\n}\n`\n```",
      "solution": "The `get()` method is asynchronous, so you either need to use `then()` to get the `querySnapshot` when the Promise returned by `get()` is fulfilled, or use `async/await`. More details on how to deal with asynchronous calls in this SO answer.\nWith `then()`\n```\n`const functions = require('firebase-functions');\n\n// The Firebase Admin SDK to access Firestore.\nconst admin = require('firebase-admin');\nadmin.initializeApp();\n\nconst db = admin.firestore();\n\nexports.changeIsVisibleFieldAfterDay = functions.pubsub\n.schedule(\"every 2 minutes\").onRun((context) => {\n  return db.collectionGroup(\"Posts\").where(\"isVisible\", \"==\", true).get()\n  .then(querySnapshot => {\n     querySnapshot.forEach((doc) => {\n      console.log(doc.data());\n     });\n     return null;\n  })\n});\n`\n```\nWith `async/await`\n```\n`const functions = require('firebase-functions');\n\n// The Firebase Admin SDK to access Firestore.\nconst admin = require('firebase-admin');\nadmin.initializeApp();\n\nconst db = admin.firestore();\n\nexports.changeIsVisibleFieldAfterDay = functions.pubsub\n.schedule(\"every 2 minutes\").onRun(async (context) => {\n  const querySnapshot = await db.collectionGroup(\"Posts\").where(\"isVisible\", \"==\", true).get();\n  querySnapshot.forEach((doc) => {\n     console.log(doc.data());\n  });\n  return null;\n});\n`\n```\n\nNote the `return null;` at the end. See this doc item for more details on this key point.\n\nNote also that if you want to update several docs within the `forEach()` loop, you will need to use `Promise.all()`. Many SO answers cover this case.",
      "question_score": 9,
      "answer_score": 11,
      "created_at": "2021-02-02T19:02:56",
      "url": "https://stackoverflow.com/questions/66015207/typeerror-querysnapshot-foreach-is-not-a-function-firebase-cloud-functions"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 71146830,
      "title": "Cloud Functions / Cloud Tasks UNAUTHENTICATED error",
      "problem": "I am trying to get a Cloud Function to create a Cloud Task that will invoke a Cloud Function. Easy.\nThe flow and use case are very close to the official tutorial here.\nI also looked at this article by Doug Stevenson and in particular its security section.\nNo luck, I am consistently getting a `16 (UNAUTHENTICATED)` error in Cloud Task.\n\nIf I can trust what I see in the console it seems that Cloud Task is not attaching the OIDC token to the request:\n\nYet, in my code I do have the `oidcToken` object:\n`const { v2beta3, protos } = require(\"@google-cloud/tasks\");\nimport {\n  PROJECT_ID,\n  EMAIL_QUEUE,\n  LOCATION,\n  EMAIL_SERVICE_ACCOUNT,\n  EMAIL_HANDLER,\n} from \"./../config/cloudFunctions\";\n\nexport const createHttpTaskWithToken = async function (\n  payload: {\n    to_email: string;\n    templateId: string;\n    uid: string;\n    dynamicData?: Record;\n  },\n  {\n    project = PROJECT_ID,\n    queue = EMAIL_QUEUE, \n    location = LOCATION, \n    url = EMAIL_HANDLER, \n    email = EMAIL_SERVICE_ACCOUNT,\n  } = {}\n) {\n  const client = new v2beta3.CloudTasksClient();\n  const parent = client.queuePath(project, location, queue);\n\n  // Convert message to buffer.\n  const convertedPayload = JSON.stringify(payload);\n  const body = Buffer.from(convertedPayload).toString(\"base64\");\n\n  const task = {\n    httpRequest: {\n      httpMethod: protos.google.cloud.tasks.v2.HttpMethod.POST,\n      url,\n      oidcToken: {\n        serviceAccountEmail: email,\n        audience: new URL(url).origin,\n      },\n      headers: {\n        \"Content-Type\": \"application/json\",\n      },\n      body,\n    },\n  };\n\n  try {\n    // Send create task request.\n    const request = { parent: parent, task: task };\n    const [response] = await client.createTask(request);\n    console.log(`Created task ${response.name}`);\n    return response.name;\n  } catch (error) {\n    if (error instanceof Error) console.error(Error(error.message));\n    return;\n  }\n};\n`\nWhen logging the task object from the code above in Cloud Logging I can see that the service account is the one that I created for the purpose of this and that the Cloud Tasks are successfully created.\n\nIAM:\n\nAnd the function that the Cloud Task needs to invoke:\n\nEverything seems to be there, in theory.\nAny advice as to what I would be missing?\nThanks,",
      "solution": "Your audience is incorrect. It must end by the function name. Here, you only have the region and the project `https://-.cloudfunction.net/`. Use the full Cloud Functions URL.",
      "question_score": 9,
      "answer_score": 10,
      "created_at": "2022-02-16T18:48:53",
      "url": "https://stackoverflow.com/questions/71146830/cloud-functions-cloud-tasks-unauthenticated-error"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 72275338,
      "title": "Get access token for a google cloud service account in Golang?",
      "problem": "I'm using a service account on google cloud. For some reason, I want to get the access token programmatically in golang. I can do `gcloud auth application-default print-access-token` on the command line.\nThere is a library by google that seems to allow me to get the token. Here is how I try to use it:\n```\n`    credentials, err := auth.FindDefaultCredentials(ctx)\n    if err == nil {\n        glog.Infof(\"found default credentials. %v\", credentials)\n        token, err2 := credentials.TokenSource.Token()\n        fmt.Printf(\"token: %v, err: %v\", token, err2)\n        if err2 != nil {\n            return nil, err2\n        }\n`\n```\nHowever, I get an error saying `token: , err: oauth2: cannot fetch token: 400 Bad Request`.\nI already have `GOOGLE_APPLICATION_CREDENTIALS` env variable defined and pointing to the json file.",
      "solution": "Running your code as-is, returns an err:\n```\n`Invalid OAuth scope or ID token audience provided\n`\n```\nI added the catch-all Cloud Platform writable scope from Google's OAuth scopes:\n```\n`https://www.googleapis.com/auth/cloud-platform\n`\n```\nDoing so, appears to work. See below:\n```\n`package main\n\nimport (\n    \"context\"\n    \"log\"\n\n    \"golang.org/x/oauth2\"\n    auth \"golang.org/x/oauth2/google\"\n)\n\nfunc main() {\n    var token *oauth2.Token\n    ctx := context.Background()\n    scopes := []string{\n        \"https://www.googleapis.com/auth/cloud-platform\",\n    }\n    credentials, err := auth.FindDefaultCredentials(ctx, scopes...)\n    if err == nil {\n        log.Printf(\"found default credentials. %v\", credentials)\n        token, err = credentials.TokenSource.Token()\n        log.Printf(\"token: %v, err: %v\", token, err)\n        if err != nil {\n            log.Print(err)\n        }\n    }\n}\n`\n```\nI had some challenges using this library recently (to access Cloud Run services which require a JWT audience). On a friend's recommendation, I used `google.golang.org/api/idtoken` instead. The API is very similar.",
      "question_score": 9,
      "answer_score": 9,
      "created_at": "2022-05-17T15:55:53",
      "url": "https://stackoverflow.com/questions/72275338/get-access-token-for-a-google-cloud-service-account-in-golang"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 72213501,
      "title": "using pg_cron extension on Cloud SQL",
      "problem": "I am trying to use pg_cron to schedule calls on stored procedure on several DBs in a Postgres Cloud SQL instance.\nUnfortunately it looks like pg_cron can only be only created on postgres DB\nWhen I try to use pg_cron on a DB different than postgres I get this message :\n```\n`CREATE EXTENSION pg_cron;\n \nERROR: can only create extension in database postgres\nDetail: Jobs must be scheduled from the database configured in \ncron.database_name, since the pg_cron background worker reads job \ndescriptions from this database. Hint: Add cron.database_name = \n'mydb' in postgresql.conf to use the current database. \nWhere: PL/pgSQL function inline_code_block line 4 at RAISE \n\nQuery = CREATE EXTENSION pg_cron;\n`\n```\n... I don't think I have access to postgresql.conf in Cloud SQL ... is there another way ?\nMaybe I could use postgres_fdw to achieve my goal ?\nThank you,",
      "solution": "There's no need to edit any files. All you have to do is set the `cloudsql.enable_pg_cron` flag (see guide) and then create the extension in the postgres database.\nYou need to log onto the postgres database rather than the one you're using for your app. For me that's just replacing the name of my app database with 'postgres' e.g.\n```\n`psql -U -h -p postgres\n`\n```\nThen simply run the create extension command and the cron.job table appears.  Here's one I did a few minutes ago in our cloudsql database.  I'm using the cloudsql proxy to access the remote db:\n```\n`127.0.0.1:2345 admin@postgres=> create extension pg_cron;\nCREATE EXTENSION\nTime: 268.376 ms\n127.0.0.1:2345 admin@postgres=> select * from cron.job;\n jobid | schedule | command | nodename | nodeport | database | username | active | jobname \n\n-------+----------+---------+----------+----------+----------+----------+--------+---------\n(0 rows)\n\nTime: 157.447 ms\n`\n```\nBe careful to specify the correct target database when setting the schedule otherwise it will think that you want the job to run in the postgres database.  The documentation has this example (but it's easily missed)\n```\n`-- Vacuum every Sunday at 4:00am (GMT) in a database other than the one pg_cron is installed in\nSELECT cron.schedule_in_database('weekly-vacuum', '0 4 * * 0', 'VACUUM', 'some_other_database');\n`\n```",
      "question_score": 9,
      "answer_score": 7,
      "created_at": "2022-05-12T11:54:02",
      "url": "https://stackoverflow.com/questions/72213501/using-pg-cron-extension-on-cloud-sql"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 71990570,
      "title": "How to fix 404 error when installing npm package from GCP artifact registry with yarn?",
      "problem": "I'm having an issue with installing an NPM package from GCP.\nI was able to upload the package to the artifact registry of GCP by doing the following steps:\n\nLogin to my google account (`gcloud auth application-default login`)\n\nRun\n`gcloud artifacts print-settings npm \\ --project=[my-project]\\ --repository=[my-repo] \\ --location=us-east1 \\ --scope=@[my-scope]`\n\nPasting the output of the previous step in the `.npmrc` file located in the root of the project.\n\nRefreshing the access token to GCP (`npx google-artifactregistry-auth ./.npmrc`)\n\nRun `yarn publish`\n\nMy `.npmrc` file looks like this:\n```\n`@[my-scope]:registry=https://us-east1-npm.pkg.dev/[my-project]/[my-repo]/\n//us-east1-npm.pkg.dev/[my-project]/[my-repo]/:_authToken=\"[auth-token]\"\n//us-east1-npm.pkg.dev/[my-project]/[my-repo]/:always-auth=true\n`\n```\nHowever, when I try to install the package on another project by:\n\nExecuting steps 1-4 mentioned above\nRun `yarn add @[my-scope]/[my-package]`\n\nI get an 404 error.\nLooks like yarn is looking for the package in the default registry:\n```\n`error An unexpected error occurred: \"https://registry.yarnpkg.com/@[my-scope]/@[my-pacakge]/-/@[my-scope]/[my-package]-0.0.1.tgz: Request failed \\\"404 Not Found\\\"\".\n`\n```\nI simply followed the steps mentioned in the installation instructions in GCP but somehow it's not working.\nI encountered a similar issue in this post: Can't install a scoped package I published to a npm registry in GCP but this not the exact error I get.\nI would appreciate any help regarding this issue.\nThanks in advance!",
      "solution": "I just had this problem for a couple of days and the solution is simple, DO NOT USE YARN when publishing. That's it.\nI don't know which part of yarn causes this but basically it ignores .npmrc resulting in the tarball to point to the wrong repository, you can check it if you run `yarn info`. So when publishing to GCP artifact registry one should use `npm publish` instead.",
      "question_score": 9,
      "answer_score": 9,
      "created_at": "2022-04-24T19:03:14",
      "url": "https://stackoverflow.com/questions/71990570/how-to-fix-404-error-when-installing-npm-package-from-gcp-artifact-registry-with"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 70376512,
      "title": "CloudScheduler 403 Permission denied while creating",
      "problem": "I am trying to create a `Cron` job programmatically in the `CloudScheduler` Google Cloud Platform using the following API explorer.\nReference: Cloud Scheduler Documentation\nEven though I have given the user `Owner` permission and verified it in `Policy Troubleshooter` that it has `cloudscheduler.jobs.create`, I am still getting the following error.\n`{\n  \"error\": {\n    \"code\": 403,\n    \"message\": \"The principal (user or service account) lacks IAM permission \\\"cloudscheduler.jobs.create\\\" for the resource \\\"projects/cloud-monitoring-saurav/locations/us-central\\\" (or the resource may not exist).\",\n    \"status\": \"PERMISSION_DENIED\"\n  }\n}\n`",
      "solution": "I had the same issue. The problem was that the region i specified did not support the cloud scheduler. You seem to have the same issue: \"us-central\" is not suppported. Try \"us-central1\"",
      "question_score": 9,
      "answer_score": 6,
      "created_at": "2021-12-16T10:18:03",
      "url": "https://stackoverflow.com/questions/70376512/cloudscheduler-403-permission-denied-while-creating"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 69128276,
      "title": "What is the point of &quot;Service Account User&quot; role if it&#39;s not for impersonation?",
      "problem": "The documentation for the Service Account User role is a bit confusing.\nhttps://cloud.google.com/iam/docs/service-accounts#user-role\n\nUsers granted the Service Account User role on a service account can use it to indirectly access all the resources to which the service account has access. For example, if a service account has been granted the Compute Admin role (roles/compute.admin), a user that has been granted the Service Account Users role (roles/iam.serviceAccountUser) on that service account can act as the service account to start a Compute Engine instance. In this flow, the user impersonates the service account to perform any tasks using its granted roles and permissions.\n\nBased on this, I assume that by granting my account the Service Account User role on a service account that is owner, I should be able to impersonate that service account from the command line and run `gcloud` commands with the inherited permissions of the service account\n```\n`gcloud init # login to my account that has the user role on the SA\ngcloud set config auth/impersonate_service_account \ngcloud compute instances list\n\n> WARNING: This command is using service account impersonation. All API calls will be executed as [@.iam.gserviceaccount.com].\n\n> ERROR: (gcloud.compute.instances.list) Failed to impersonate [@.iam.gserviceaccount.com]. Make sure the account that's trying to impersonate it has access to the service account itself and the \"roles/iam.serviceAccountTokenCreator\" role.\n`\n```\nSo I removed the User role and assigned myself the Token Creator role. Works as expected. Why does the description for the User role sound like its the role I'm meant to be using but it seems like Token Creator is the only one I need?",
      "solution": "So despite the confusion of the GCP docs, I think I was able to reach a conclusion on the difference between:\n\nService Account User\nService Account Token Creator\n\nAs an example, if I wanted to deploy a GKE cluster but specify a service account for the nodes to use other than the default service account I would add the flag:\n```\n`gcloud containers cluster create my-cluster --service-account= \n`\n```\nFor me to do this I would at a minimum require `Service Account User` on the service account I am attempting to assign to the resources. This role appears to also be used in other cases such as executing code on a VM and using the VMs identity instead(??).\nIf I wanted to deploy the cluster using the service account credentials (ie. Not my own account), I would use impersonation which requires the `Token Creator` role. I might want to do this because my personal account doesn't have permission to deploy clusters but the SA does.\n```\n`gcloud containers cluster create my-cluster --impersonate-service-account=\n`\n```\nThis would build the cluster and log the action as that of the service account, not my personal account.\nPlease correct me if I'm wrong.",
      "question_score": 9,
      "answer_score": 8,
      "created_at": "2021-09-10T08:46:58",
      "url": "https://stackoverflow.com/questions/69128276/what-is-the-point-of-service-account-user-role-if-its-not-for-impersonation"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 68862621,
      "title": "Getting Error 524 while running jupyter lab in google cloud platform",
      "problem": "I am not able to access jupyter lab created on google cloud\n\nI created one notebook using Google AI platform. I was able to start it and work but suddenly it stopped and I am not able to start it now. I tried building and restarting the jupyterlab, but of no use. I have checked my disk usages as well, which is only 12%.\nI tried the diagnostic tool, which gave the following result:\n\nbut didn't fix it.\nThanks in advance.",
      "solution": "The error might be caused by a corrupted disk partition \u201c/dev/sdb\u201d. After connecting to the notebook instance through SSH, run `sudo fsck /dev/sdb` to perform a disk check and repair, and then perform a reboot.\nIn case this does not help, you can download your data after zipping the required content from the \u201c/home/jupyter/\u201d folder and upload it to a new notebook instance.\nBelow is the zip utility command for your reference.\n`cd /home/`\n`sudo zip -r test-1.zip jupyter/`",
      "question_score": 9,
      "answer_score": 5,
      "created_at": "2021-08-20T14:57:57",
      "url": "https://stackoverflow.com/questions/68862621/getting-error-524-while-running-jupyter-lab-in-google-cloud-platform"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 69744746,
      "title": "CloudRun is going over max instance count (of 1)",
      "problem": "I have a cloud run service (receiving no traffic) where I set the max instances and min instances to 1 so that it's always running.\nWhen deploying a new instance, the instance count jumps to 3. This is a problem (I make some requests on instance start that hits a 429 if two instances are simultaneously making these requests).\nWhy is CloudRun instance count going over my max?\nI can confirm my settings are correct and looking at the logs there are two new instances that start up.\nPS: Cloudrun does have this message, which makes me think what I'm trying to do isn't possible. I just figured it would be because of downtime instead of extra instances.\n\nRevisions using a maximum number of instances of 3 or less might experience unexpected downtime.",
      "solution": "Your scenario seems to fit one described in the documentation, in which a new deployment for a Cloud Run service might temporarily create additional instances:\n\nWhen you deploy a new revision, Cloud Run gradually migrates traffic from the old revision to the new one. Because maximum instance limits are set for each revision, you may temporarily exceed the specified limit during the period after deployment.\n\nAdditional instances might be created to handle traffic migration from the previous revision to a new one. Cloud Run offers settings that can alter how migration occurs between revisions. One of these settings is used to instantly serve all new traffic on the new revision. You can test if using this setting helps reduce the number of instances that are created. I tested one of the provided sample services and created multiple revisions, which did not exceed 1 active instance.",
      "question_score": 9,
      "answer_score": 5,
      "created_at": "2021-10-27T21:58:38",
      "url": "https://stackoverflow.com/questions/69744746/cloudrun-is-going-over-max-instance-count-of-1"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 68119561,
      "title": "Stopping and starting a deep learning google cloud VM instance causes tensorflow to stop recognizing GPU",
      "problem": "I am using the pre-built deep learning VM instances offered by google cloud, with an Nvidia tesla K80 GPU attached. I choose to have Tensorflow 2.5 and CUDA 11.0 automatically installed. When I start the instance, everything works great - I can run:\n```\n`Import tensorflow as tf\ntf.config.list_physical_devices()\n`\n```\nAnd my function returns the CPU, accelerated CPU, and the GPU. Similarly, if I run `tf.test.is_gpu_available()`, the function returns True.\nHowever, if I log out, stop the instance, and then restart the instance, running the same exact code only sees the CPU and `tf.test.is_gpu_available()` results in False. I get an error that looks like the driver initialization is failing:\n```\n` E tensorflow/stream_executor/cuda/cuda_driver.cc:355] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\n`\n```\nRunning nvidia-smi shows that the computer still sees the GPU, but my tensorflow can\u2019t see it.\nDoes anyone know what could be causing this? I don\u2019t want to have to reinstall everything when I\u2019m restarting the instance.",
      "solution": "Some people (sadly not me) are able to resolve this by setting the following at the beginning of their script/main:\n`import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n`\nI had to reinstall CUDA drivers and from then on it worked even after restarting the instance. You can configure your system settings on NVIDIAs website and it will provide you the commands you need to follow to install cuda. It also asks you if you want to uninstall the previous cuda version (yes!).This is luckily also very fast.",
      "question_score": 9,
      "answer_score": 3,
      "created_at": "2021-06-24T18:32:47",
      "url": "https://stackoverflow.com/questions/68119561/stopping-and-starting-a-deep-learning-google-cloud-vm-instance-causes-tensorflow"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 75542954,
      "title": "Deploy Cloud Storage Triggers in Multi-Region Buckets",
      "problem": "I am trying to deploy a Cloud Storage trigger by following the guide here:\nhttps://cloud.google.com/functions/docs/tutorials/storage#object_finalize\nI want to listen for any finalization in my default \"us multi-region\" bucket. Unfortunately, when I try to deploy the function trigger via gcloud cli as described in the link above:\n```\n`gcloud functions deploy **** \\\n--gen2 \\\n--region=us-central1 \\\n--project=**** \\\n--trigger-event-filters=\"type=google.cloud.storage.object.v1.finalized\" \\\n--trigger-event-filters=\"bucket={my-multi-region-bucket}\"\n`\n```\nI get the following error:\n```\n`INVALID_ARGUMENT: Cannot create trigger ****: \nBucket '****' is in location 'us', but the trigger location is 'us-central1'.\nThe trigger must be in the same location as the bucket. \nTry redeploying and changing the trigger location to 'us'.\n`\n```",
      "solution": "I finally got it working by simply adding the `--trigger-location=us` as shown below:\n```\n`gcloud functions deploy **** \\\n--gen2 \\\n--region=us-central1 \\\n--project=**** \\\n--trigger-event-filters=\"type=google.cloud.storage.object.v1.finalized\" \\\n--trigger-event-filters=\"bucket={my-multi-region-bucket}\" \\\n--trigger-location=us\n`\n```",
      "question_score": 8,
      "answer_score": 18,
      "created_at": "2023-02-23T10:37:41",
      "url": "https://stackoverflow.com/questions/75542954/deploy-cloud-storage-triggers-in-multi-region-buckets"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 70297884,
      "title": "&quot;TypeError: db._checkNotDeleted is not a function&quot; When creating a Storage Ref on Firebase / React",
      "problem": "I'm creating an app on React that uses Firebase for backend data and want to be able to upload pictures. When trying to get a Storage Ref through firebase, I get the following error:\n\nEvery time I try to get a ref to the Storage, I get this error. I also use the realtime-databse on firebase, but I don't think that would interfere at all.\nBelow is the code that is called:\n```\n`import database, { auth, provider, storage } from '../../components/utils/firebase.js';\nimport { uploadBytes } from '@firebase/storage';\n\n...\n        handleSubmit(e) {\n        e.preventDefault();\n\n        const file = e.target[0].files[0];\n        console.log(\"File: \", file);\n        \n        if (!file) {\n            console.log(\"No file!\");\n          } else {\n            const storageRef = ref(storage, `images/${file.name}`);\n            uploadBytes(storageRef, file);\n            console.log(\"Uploaded file!\");\n            console.log(\"File: \" + file);\n          }\n        \n    }\n`\n```\nfirebase.js (my config file):\n```\n`import { initializeApp } from 'firebase/app';\nimport { getDatabase } from 'firebase/database';\nimport  { getAuth, GoogleAuthProvider } from 'firebase/auth';\nimport { getStorage } from 'firebase/storage';\n\nconst firebaseConfig = {\n  apiKey: process.env.REACT_APP_APIKEY,\n  authDomain: process.env.REACT_APP_AUTHDOMAIN,\n  databaseURL: process.env.REACT_APP_DATABASEURL,\n  projectId: process.env.REACT_APP_PROJECTID,\n  storageBucket: process.env.REACT_APP_STORAGEBUCKET,\n  messagingSenderId: process.env.REACT_APP_MESSAGINGSENDERID,\n  appId: process.env.REACT_APP_APPID,\n  measurementId: process.env.REACT_APP_MEASUREMENTID\n};\n\n// Initialize Firebase\nconst app = initializeApp(firebaseConfig);\nconst database = getDatabase(app);\n\nexport const storage = getStorage(app);\nexport default database;\nexport const auth = getAuth(app);\n\nexport const provider = new GoogleAuthProvider();\n`\n```\nDoes anyone have any ideas as to why this is happening? Thank you!",
      "solution": "I found the issue while watching a video of someone setting up their storage bucket.\nEssentially, because I am already using `ref` for my realtime database, and import that command from the realtime database folder, my storage ref was actually a realtime database ref. To fix this, I imported my storage `ref` function like so:\n`import { ref as sRef } from 'firebase/storage';`\nthen used `sRef` instead of `ref`.\nHope this helps anyone who had the same issue! It was frustrating to hunt down",
      "question_score": 8,
      "answer_score": 15,
      "created_at": "2021-12-10T00:10:26",
      "url": "https://stackoverflow.com/questions/70297884/typeerror-db-checknotdeleted-is-not-a-function-when-creating-a-storage-ref-o"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 69849117,
      "title": "gcloud app deploy eror The &quot;vpcaccess.connectors.use&quot; permission is required",
      "problem": "I have GO application and tried to deploy it to the google app engine with google command line sdk from a local machine with a custom service account.\nI create a custom service account with roles:\n\nApp Engine Admin\nCloud Build Service Account\nCloud Build Service\nAgent Serverless VPC Access User\n\nAnd authorized locally with this account:\n```\n`gcloud auth activate-service-account account_name@project-name.iam.gserviceaccount.com --key-file=key.json\n`\n```\nAlso, I configure the service account in my app.yaml:\n```\n`service_account: account_name@project-name.iam.gserviceaccount.com\n`\n```\nBut when i try to deploy\n```\n`gcloud app deploy -v 1   \n`\n```\nI get the error:\n```\n`Beginning deployment of service [default]...\n\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n\u2560\u2550 Uploading 0 files to Google Cloud Storage                \u2550\u2563\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\nFile upload done.\nERROR: (gcloud.app.deploy) PERMISSION_DENIED: Operation is not allowed. The \"vpcaccess.connectors.use\" permission is required.\n`\n```\nI can not understand why vpcaccess.connectors.use permission still required even I have Serverless VPC Access User role? I found the role on this google docs page\nMaybe someone has met with a similar difficulty? Please help me)\nUPD even i give Owner role i steel have the error",
      "solution": "One way to fix this error is by removing `vpc_access_connector:` configuration from app.yaml, however that means your app engine needs to connect via the internet to reach the resources on your subnet.\nIf you do need vpc_access_connector configured, add\n`Serverless VPC Access User` and `Compute Viewer` IAM roles to the service account doing the deployment. Add the roles to the project where the VPC Access connector is configured.\nThis error will also appear when the connector does not exist (or exists somewhere else).\nCheck the project-id, region and connector name in app.yaml are correct and refer to a connector that exists in\nhttps://console.cloud.google.com/networking/connectors/list?project=\n```\n`vpc_access_connector:\n  name: projects/project-id/locations/the-zone/connectors/my-connector-name\n`\n```\nIf you have a Shared VPC, the serverless access connector can either be in the host project or the service project. See: Configure connectors in Shared VPC service projects\nIf the connector is on a host project of a Shared VPC you must give the  deploying service account `Serverless VPC Access Viewer` (vpcaccess.viewer) role on the host project.\n`Compute Network Viewer` (compute.networkViewer) on the host is also recommended.",
      "question_score": 8,
      "answer_score": 12,
      "created_at": "2021-11-05T06:48:43",
      "url": "https://stackoverflow.com/questions/69849117/gcloud-app-deploy-eror-the-vpcaccess-connectors-use-permission-is-required"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 67104574,
      "title": "Terraform: &quot;known only after apply&quot; ISSUE",
      "problem": "I'm creating an `aws_subnet` and referencing it in another resource.\nExample:\n```\n`resource \"aws_subnet\" \"mango\" {\n     vpc_id     = aws_vpc.mango.id\n     cidr_block = \"${var.subnet_cidr}\"\n  }\n`\n```\nThe reference\n```\n` network_configuration {\n    subnets          = \"${aws_subnet.mango.id}\"\n  }\n`\n```\nWhen planning it I get\n`aws_subnet.mango.id is a string, known only after apply`\nerror. I'm new to Terraform. Is there something similar to Cloudformation's `DependsOn` or `Export/Import`?",
      "solution": "This is a case of explicit dependency.\nThe argument `depends_on` similar to CloudFormation's `DependsOn` solves such dependencies.\nNote: \"Since Terraform will wait to create the dependent resource until after the specified resource is created, adding explicit dependencies can increase the length of time it takes for Terraform to create your infrastructure.\"\nExample:\n```\n`depends_on = [aws_subnet.mango]\n`\n```",
      "question_score": 8,
      "answer_score": 8,
      "created_at": "2021-04-15T10:06:56",
      "url": "https://stackoverflow.com/questions/67104574/terraform-known-only-after-apply-issue"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 66248421,
      "title": "Terragrunt import resource created via for_each loop",
      "problem": "I'm creating a GCP buckets with for_each loop, and wanted to import the existing buckets into my terraform state\n`An execution plan has been generated and is shown below.\nResource actions are indicated with the following symbols:\n  + create\n\nTerraform will perform the following actions:\n\n  # google_storage_bucket.buckets[\"a-test-test-test\"] will be created\n  + resource \"google_storage_bucket\" \"buckets\" {\n      + bucket_policy_only          = (known after apply)\n      + force_destroy               = false\n      + id                          = (known after apply)\n      + location                    = \"US\"\n      + name                        = \"a-test-test-test\"\n      + project                     = \"xxx\"\n      + self_link                   = (known after apply)\n      + storage_class               = \"STANDARD\"\n      + uniform_bucket_level_access = false\n      + url                         = (known after apply)\n\n      + versioning {\n          + enabled = true\n        }\n    }\n\nPlan: 1 to add, 0 to change, 0 to destroy.\n\nChanges to Outputs:\n  ~ urls = [\n      - \"gs://a-test-test-test\",\n      + (known after apply),\n    ]\n\nDo you want to perform these actions?\n  Terraform will perform the actions described above.\n  Only 'yes' will be accepted to approve.\n\n  Enter a value: yes\n\ngoogle_storage_bucket.buckets[\"a-test-test-test\"]: Creating...\n\nError: googleapi: Error 409: You already own this bucket. Please select another name., conflict\n`\nThe resource already exists but that's fine, I can just import it, question is how\nbecause running it like this\n`MacBook-Pro% terragrunt import google_storage_bucket.buckets a-test-test-test\n...\nAcquiring state lock. This may take a few moments...\ngoogle_storage_bucket.buckets: Importing from ID \"a-test-test-test\"...\ngoogle_storage_bucket.buckets: Import prepared!\n  Prepared google_storage_bucket for import\ngoogle_storage_bucket.buckets: Refreshing state... [id=a-test-test-test]\n\nImport successful!\n`\nseems to work, but it imported it 'wrongly'\n`terragrunt state list\n...\ngoogle_storage_bucket.buckets\n`\nits show in my tfstate, but it should be like this\n`google_storage_bucket.buckets[\"a-test-test-test\"]\n`\nbecause if I run now apply - it says it wants to delete the `google_storage_bucket.buckets` and create `google_storage_bucket.buckets[\"a-test-test-test\"]`\n`google_storage_bucket.buckets: Refreshing state... [id=a-test-test-test]\n\nAn execution plan has been generated and is shown below.\nResource actions are indicated with the following symbols:\n  + create\n  - destroy\n\nTerraform will perform the following actions:\n\n  # google_storage_bucket.buckets will be destroyed\n  - resource \"google_storage_bucket\" \"buckets\" {\n      - bucket_policy_only          = false -> null\n      - default_event_based_hold    = false -> null\n      - force_destroy               = false -> null\n      - id                          = \"a-test-test-test\" -> null\n      - labels                      = {} -> null\n      - location                    = \"US\" -> null\n      - name                        = \"a-test-test-test\" -> null\n      - project                     = \"xxx\" -> null\n      - requester_pays              = false -> null\n      - self_link                   = \"https://www.googleapis.com/storage/v1/b/a-test-test-test\" -> null\n      - storage_class               = \"STANDARD\" -> null\n      - uniform_bucket_level_access = false -> null\n      - url                         = \"gs://a-test-test-test\" -> null\n\n      - versioning {\n          - enabled = true -> null\n        }\n    }\n\n  # google_storage_bucket.buckets[\"a-test-test-test\"] will be created\n  + resource \"google_storage_bucket\" \"buckets\" {\n      + bucket_policy_only          = (known after apply)\n      + force_destroy               = false\n      + id                          = (known after apply)\n      + location                    = \"US\"\n      + name                        = \"a-test-test-test\"\n      + project                     = \"xxx\"\n      + self_link                   = (known after apply)\n      + storage_class               = \"STANDARD\"\n      + uniform_bucket_level_access = false\n      + url                         = (known after apply)\n\n      + versioning {\n          + enabled = true\n        }\n    }\n\nPlan: 1 to add, 0 to change, 1 to destroy.\n\nChanges to Outputs:\n  + urls = [\n      + (known after apply),\n    ]\n\nDo you want to perform these actions?\n  Terraform will perform the actions described above.\n  Only 'yes' will be accepted to approve.\n`\nAny thoughts how to import into a for_each in terragrunt ?\nI tried the\n`terragrunt import google_storage_bucket.buckets a-test-test-test\nterragrunt import google_storage_bucket.buckets.a-test-test-test a-test-test-test\nterragrunt import google_storage_bucket.buckets[\"a-test-test-test\"] a-test-test-test\nterragrunt import google_storage_bucket.buckets[\\\"a-test-test-test\\\"] a-test-test-test\n`\nand noone work just leaving me with error\n`zsh: no matches found: google_storage_bucket.buckets[\"a-test-test-test\"]\n`\nwhile the first option `terragrunt import google_storage_bucket.buckets a-test-test-test` , imported (aka. worked) but not the right way\n\nThe terraform code is like so:\n`inputs = {\n  project_id  = \"${local.project_id}\"\n    {\n      name                        = \"a-test-test-test\"\n      location                    = \"US\"\n    }\n}\n\nlocals {\n  buckets        = {for b in jsondecode(var.buckets) : b.name => b }\n}\n\nvariable \"buckets\" {\n  description = \"The name of the bucket.\"\n}\n\nresource \"google_storage_bucket\" \"buckets\" {\n  for_each      = local.buckets\n  name          = each.key\n  project       = var.project_id\n  location      = each.value.location\n`",
      "solution": "Importing into the full instance address (including the instance key index part) is the right approach, but the trick here is to determine the best way to work around the grammar of your shell so that the necessary characters can reach Terraform.\nFor Unix-style shells I typically recommend putting the address in single quotes to disable metacharacter interpretation, like this:\n```\n`terragrunt import 'google_storage_bucket.buckets[\"a-test-test-test\"]' a-test-test-test\n`\n```\nI don't have much experience with `zsh` in particular but from referring to a copy of part of its documentation I get the impression that the above is valid `zsh` syntax too. If the above doesn't work, it might be worth trying with a different shell such as `bash` to see if you get a different result.\nAlthough you specifically mentioned `zsh`, for completeness I'll also note that on Windows the rules are a little different: single quotes aren't supported under the conventional Windows command line syntax, and so unfortunately we must escape the quotes instead when running Terraform from the Windows command prompt:\n```\n`terragrunt import google_storage_bucket.buckets[\\\"a-test-test-test\\\"] a-test-test-test\n`\n```\nThe important thing is that the quote characters `\"` in the address make it through the shell to Terraform, so that Terraform can then successfully parse the argument as resource address syntax.",
      "question_score": 8,
      "answer_score": 14,
      "created_at": "2021-02-17T20:04:48",
      "url": "https://stackoverflow.com/questions/66248421/terragrunt-import-resource-created-via-for-each-loop"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 74673148,
      "title": "BigQuery Client using Python | Timeout and Polling issues",
      "problem": "I am trying to execute a SQL statement on BQ Database by initializing the BQ Client. This has been running smoothly for a while but lately seeing an issue.\nMy code specifically fails when its trying to wait for the results from the DB\n```\n`query_job = client.query(QUERY)  # API request to start the query    \ndb_rslt = query_job.result()     # Wait for the query to return results\n`\n```\nHere is error message:\n`  File \"/opt/conda/default/lib/python3.7/site-packages/google/cloud/bigquery/job.py\", line xxx, in result super(QueryJob, self).result(retry=retry, timeout=timeout) File \"/opt/conda/default/lib/python3.7/site-packages/google/cloud/bigquery/job.py\", line xxx, in result return super(_AsyncJob, self).result(timeout=timeout) File \"/opt/conda/default/lib/python3.7/site-packages/google/api_core/future/polling.py\", line xxx, in result self._blocking_poll(timeout=timeout, retry=retry, polling=polling) TypeError: _blocking_poll() got an unexpected keyword argument 'retry'`\nAdded the timeout parameter to the result method but did not help",
      "solution": "7 days ago Google released an update to `google-api-core==1.34.0` for bug fixes. I guess this introduced some breaking changes if you are not using the later versions of `google-cloud-bigquery`. You may either:\n\nPin `google-api-core` to the previous working version (eg. 1.33.2)\nUpgrade `google-cloud-bigquery` to the latest versions.",
      "question_score": 8,
      "answer_score": 5,
      "created_at": "2022-12-04T06:39:03",
      "url": "https://stackoverflow.com/questions/74673148/bigquery-client-using-python-timeout-and-polling-issues"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 72705417,
      "title": "Use GOOGLE_APPLICATION_CREDENTIALS env var with GitHub Actions secret",
      "problem": "My problem is simple: I want to run code that depends on `GOOGLE_APPLICATION_CREDENTIALS` being set up correctly from a GitHub Actions secret.\nThe problem is that `GOOGLE_APPLICATION_CREDENTIALS` is expect to contain a path to a service account file, whether the secret contain the actual service account file content.\nWhat's the best practice for that?\n\nEDIT\nEssentially I run a NodeJS script which connect to multiple GCP resources using client libraries (PubSub, BigQuery, etc). To my understanding, they can most easily work if `GOOGLE_APPLICATION_CREDENTIALS` env var is correctly defined.",
      "solution": "The problem is that GOOGLE_APPLICATION_CREDENTIALS is expect to\ncontain a path to a service account file, whether the secret contain\nthe actual service account file content.\n\nThere isn't one. The environment variable must point to a file which is the location for a service account JSON key. You could write the contents to an artifact, but that is dangerous. There are other methods, but your question does not provide details on what your GitHub action does or the commands it runs. However, I still would not use those methods.\nThe correct solution is to use Google Cloud Workload Identity Federation. That is both the solution and the best practice solution.\nGitHub provides an OAuth identity provider. Google supports federating credentials from one OAuth provider to another.\nEnabling keyless authentication from GitHub Actions\ngoogle-github-actions/auth",
      "question_score": 8,
      "answer_score": 6,
      "created_at": "2022-06-21T20:13:57",
      "url": "https://stackoverflow.com/questions/72705417/use-google-application-credentials-env-var-with-github-actions-secret"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 70024078,
      "title": "Connecting to Cloud SQL from Cloud Run via cloud-sql-proxy with IAM login enabled",
      "problem": "I would like to connect to a Cloud SQL instance from Cloud Run, using a service account. The connection used to be created within the VPC and we would just provide a connection string with a `user` and a `password` to our PostgreSQL client. But now we want the authentication to be managed by Google Cloud IAM, with the service account associated with the Cloud Run service.\nOn my machine, I can use the `enable_iam_login` argument to use my own service account. The command to run the Cloud SQL proxy would look like this:\n```\n`./cloud_sql_proxy -dir=/cloudsql -instances=[PROJECT-ID]:[REGION]:[INSTANCE] \\\n-enable_iam_login -credential_file=${HOME}/.config/gcloud/application_default_credentials.json\n`\n```\nThe problem is that I can't seem to find a way to use the IAM authentication method to run the Cloud SQL Proxy from Cloud Run, I can just provide an instance name. Has anyone face this problem before?",
      "solution": "Unfortunately, there isn't a way to configure Cloud Run's use of the Cloud SQL proxy to do this for you.\nIf you are using Java, Python, or Go, there are language specific connectors you can use from Cloud Run. These all have the option to use IAM DB AuthN as part of them.",
      "question_score": 8,
      "answer_score": 10,
      "created_at": "2021-11-18T18:17:46",
      "url": "https://stackoverflow.com/questions/70024078/connecting-to-cloud-sql-from-cloud-run-via-cloud-sql-proxy-with-iam-login-enable"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 74853090,
      "title": "Is it OK to duplicate document Ids in different Firestore collections?",
      "problem": "Suppose I have a collection called `savedGamesFull`. Whenever a document is created in this collection, it receives the Firebase-generated ID; I then reuse this ID and give it to another document in a parallel collection `savedGamesLight`.\nIn another example, for each document in `players` collection, there would be a document with the same id in `playerPublicProfiles` and `playerStats` collections.\nBasically, all this naming method does is that it saves a few lines of code here and there, and there is no need to keep the `userId` field in those parallel documents.\nBut after reading about hotspotting (although it doesn't seem to apply here) I got concerned that there may be issues with this naming.\nSo are there any downsides to having the same document IDs in several different collections?",
      "solution": "Is it OK to duplicate document IDs in different Firestore collections?\n\nYes. It is actually a quite common practice when it comes to identifying users across your application, no matter what the collection is.\n\nI then reuse this id and give it to another document in a parallel collection savedGamesLight.\n\nIn the NoSQL world, it's always a good practice to denormalize data. This means that it totally makes sense to display data from lighter documents rather than from heavier documents.\n\nBut after reading about hot-spotting (although it doesn't seem to apply here) I got concerned that there may be issues with this naming.\n\nHot-spotting doesn't refer to having the same document IDs in multiple collections or sub-collections. It's about having sequential document IDs.\n\nSo are there any downsides to having the same document ids in several different collections?\n\nNot at all. It's quite normal.",
      "question_score": 8,
      "answer_score": 8,
      "created_at": "2022-12-19T17:09:22",
      "url": "https://stackoverflow.com/questions/74853090/is-it-ok-to-duplicate-document-ids-in-different-firestore-collections"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 65683004,
      "title": "Google API Gateway: Provide API key in header",
      "problem": "I'm trying to setup Google API Gateway to use an API key that callers send in the header.\nMy api config yaml looks like this:\n`...\nsecurityDefinitions:\n  api_key_header:\n    type: apiKey\n    name: key\n    in: header\n  api_key_query:\n    type: apiKey\n    name: key\n    in: query\npaths:\n  /foo-header:\n    get:\n      summary: Test foo endpoint\n      operationId: testGet-header\n      x-google-backend:\n        address: \"\"\n        protocol: h2\n        path_translation: APPEND_PATH_TO_ADDRESS\n      security:\n        - api_key_header: []\n      responses:\n        204:\n          description: A successful response\n  /foo-query:\n    get:\n      summary: Test foo endpoint\n      operationId: testGet-header\n      x-google-backend:\n        address: \"\"\n        protocol: h2\n        path_translation: APPEND_PATH_TO_ADDRESS\n      security:\n        - api_key_query: []\n      responses:\n        204:\n          description: A successful response \n`\nI expect both calls, `/foo-header` and `/foo-query` to fail with 401 status if a valid API key is not provided via header or query parameter.\nBut in a fact only `/foo-query` behaves as expected.\nRequests to `/foo-header` pass to the backend even when the API key is not provided in request header.\nDo I have issue with the config, or is it the Google API Gateway that doesn't work properly when API key is provided in request header?",
      "solution": "When `in` is `header`, the `name` should be `x-api-key`.\nhttps://cloud.google.com/endpoints/docs/openapi/openapi-limitations#api_key_definition_limitations",
      "question_score": 8,
      "answer_score": 10,
      "created_at": "2021-01-12T12:27:02",
      "url": "https://stackoverflow.com/questions/65683004/google-api-gateway-provide-api-key-in-header"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 70797574,
      "title": "(Terraform, Cloud Run) Error: Forbidden Your client does not have permission to get URL / from this server",
      "problem": "I'm trying to run a docker image on Cloud Run with the Terraform code below:\n`provider \"google\" {\n  credentials = file(\"myCredentials.json\")\n  project     = \"myproject-214771\"\n  region      = \"asia-northeast1\"\n}\n\nresource \"google_cloud_run_service\" \"default\" {\n  name     = \"hello-world\"\n  location = \"asia-northeast1\"\n\n  template {\n    spec {\n      containers {\n        image = \"gcr.io/myproject-214771/hello-world:latest\"\n      }\n    }\n  }\n\n  traffic {\n    percent         = 100\n    latest_revision = true\n  }\n}\n`\nThen, it was successful to run the docker image:\n\nBut when I access the URL, it shows this:\n\nError: Forbidden Your client does not have permission to get URL /\nfrom this server\n\nAre there any mistakes in my Terraform code?",
      "solution": "Add(Copy & paste) this code below to your Terraform code to allow unauthenticated invocations for public API or website:\n`data \"google_iam_policy\" \"noauth\" {\n  binding {\n    role = \"roles/run.invoker\"\n    members = [\n      \"allUsers\",\n    ]\n  }\n}\n\nresource \"google_cloud_run_service_iam_policy\" \"noauth\" {\n  location    = google_cloud_run_service.default.location\n  project     = google_cloud_run_service.default.project\n  service     = google_cloud_run_service.default.name\n\n  policy_data = data.google_iam_policy.noauth.policy_data\n}\n`\nSo this is the full code:\n`provider \"google\" {\n  credentials = file(\"myCredentials.json\")\n  project     = \"myproject-214771\"\n  region      = \"asia-northeast1\"\n}\n\nresource \"google_cloud_run_service\" \"default\" {\n  name     = \"hello-world\"\n  location = \"asia-northeast1\"\n\n  template {\n    spec {\n      containers {\n        image = \"gcr.io/myproject-214771/hello-world:latest\"\n      }\n    }\n  }\n\n  traffic {\n    percent         = 100\n    latest_revision = true\n  }\n}\n\ndata \"google_iam_policy\" \"noauth\" {\n  binding {\n    role = \"roles/run.invoker\"\n    members = [\n      \"allUsers\",\n    ]\n  }\n}\n\nresource \"google_cloud_run_service_iam_policy\" \"noauth\" {\n  location    = google_cloud_run_service.default.location\n  project     = google_cloud_run_service.default.project\n  service     = google_cloud_run_service.default.name\n\n  policy_data = data.google_iam_policy.noauth.policy_data\n}\n`\nFinally, your URL shows your website properly:\n\nMoreover, now \"Authentication\" is \"Allow unauthenticated\":\n\nDon't forget to add the role \"Cloud Run Admin\" to your service account:\n\nOtherwise, you cannot allow unauthenticated invocations for public API or website then you will get this error below:\n\nError setting IAM policy for cloudrun service\n\"v1/projects/myproject-214771/locations/asia-northeast1/services/hello-world\":\ngoogleapi: Error 403: Permission 'run.services.setIamPolicy' denied on\nresource\n'projects/myproject-214771/locations/asia-northeast1/services/hello-world'\n(or resource may not exist).\n\nMoreover, with these roles below, you cannot allow unauthenticated invocations for public API or website:\n\nOnly the role \"Cloud Run Admin\" can allow unauthenticated invocations for public API or website.",
      "question_score": 8,
      "answer_score": 11,
      "created_at": "2022-01-21T08:06:25",
      "url": "https://stackoverflow.com/questions/70797574/terraform-cloud-run-error-forbidden-your-client-does-not-have-permission-to"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 70756708,
      "title": "Unable to create a new Cloud Function - cloud-client-api-gae",
      "problem": "I'm unable to create a Cloud Function in my GCP project using GUI, but have admin roles for GCF, SA and IAM.\nHere is the error message:\n\nMissing necessary permission iam.serviceAccounts.actAs for\ncloud-client-api-gae on the service account\nserviceaccountname@DOMAIN.iam.gserviceaccount.com. Grant the role\n'roles/iam.serviceAccountUser' to cloud-client-api-gae on the service\naccount serviceaccountname@DOMAIN.iam.gserviceaccount.com.\n\n`cloud-client-api-gae` is not an SA nor User on my IAM list. It must be a creature living underneath Graphical User Interfrace.\nI have Enabled API for GCF, AppEngine and I have `Service Account Admin` role.\nI had literally 0 search results when googling for `cloud-client-api-gae`.",
      "solution": "I've contacted GCP support and it seems my user was missing single role:\n`Service Account User` - that's it.\nPS: Person from support didn't know what this thing called \"cloud-client-api-gae\" is.",
      "question_score": 8,
      "answer_score": 7,
      "created_at": "2022-01-18T14:53:18",
      "url": "https://stackoverflow.com/questions/70756708/unable-to-create-a-new-cloud-function-cloud-client-api-gae"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 66152733,
      "title": "BigQuery TypeError: to_pandas() got an unexpected keyword argument &#39;timestamp_as_object&#39;",
      "problem": "Environment details\n\nOS type and version: 1.5.29-debian10\nPython version: 3.7\n`google-cloud-bigquery` version: 2.8.0\n\nI'm provisioning a dataproc cluster which gets the data from BigQuery into a pandas dataframe.\nAs my data is growing I was looking to boost the performance and heard about using the BigQuery storage client.\nI had the same problem in the past and this was solved by setting the google-cloud-bigquery to version 1.26.1.\nIf I use that version I get the following message.\n```\n`/opt/conda/default/lib/python3.7/site-packages/google/cloud/bigquery/client.py:407: UserWarning: Cannot create BigQuery Storage client, the dependency google-cloud-bigquery-storage is not installed.\n \"Cannot create BigQuery Storage client, the dependency \" \n`\n```\nThe code snippet executes but at a way slower rate. If I do not specify the pip version, I encounter this error.\nSteps to reproduce\n\nCluster creation on dataproc\n\n```\n`gcloud dataproc clusters create testing-cluster  --region=europe-west1  --zone=europe-west1-b  --master-machine-type n1-standard-16  --single-node  --image-version 1.5-debian10  --initialization-actions gs://dataproc-initialization-actions/python/pip-install.sh  --metadata 'PIP_PACKAGES=elasticsearch google-cloud-bigquery google-cloud-bigquery-storage pandas pandas_gbq'\n`\n```\n\nExecute the Following script on the cluster\n\n```\n`bqclient = bigquery.Client(project=project)\njob_config = bigquery.QueryJobConfig(\n    query_parameters=[\n        bigquery.ScalarQueryParameter(\"query_start\", \"STRING\", str('2021-02-09 00:00:00')),\n        bigquery.ScalarQueryParameter(\"query_end\", \"STRING\", str('2021-02-09 23:59:59.99')),\n    ]\n)\ndf = bqclient.query(query, job_config=job_config).to_dataframe(create_bqstorage_client=True)\n`\n```\n```\n`2021-02-11 10:10:14,069 - preprocessing logger initialized\n2021-02-11 10:10:14,069 - arguments = [file, arg1, arg2, arg3, arg4, project_id, arg5, arg6]\nTraceback (most recent call last):\n  File \"/tmp/782503bcc80246258560a07d2179891f/immo_preprocessing-pageviews_kyero.py\", line 104, in \n    df = bqclient.query(base_query, job_config=job_config).to_dataframe(create_bqstorage_client=True)\n  File \"/opt/conda/default/lib/python3.7/site-packages/google/cloud/bigquery/job/query.py\", line 1333, in to_dataframe\n    date_as_object=date_as_object,\n  File \"/opt/conda/default/lib/python3.7/site-packages/google/cloud/bigquery/table.py\", line 1793, in to_dataframe\n    df = record_batch.to_pandas(date_as_object=date_as_object, **extra_kwargs)\n  File \"pyarrow/array.pxi\", line 414, in pyarrow.lib._PandasConvertible.to_pandas\nTypeError: to_pandas() got an unexpected keyword argument 'timestamp_as_object'\n`\n```\nUsing the pandas-gbq version gives exaclty the same error\n```\n`query_config = {\n    'query': {\n        'parameterMode': 'NAMED',         \n        'queryParameters': [\n            {\n                'name': 'query_start',\n                'parameterType': {'type': 'STRING'},\n                'parameterValue': {'value': str('2021-02-09 00:00:00')}\n            },\n            {\n                'name': 'query_end',\n                'parameterType': {'type': 'STRING'},\n                'parameterValue': {'value': str('2021-02-09 23:59:59.99')}\n            },\n        ]\n    }\n}\ndf = pd.read_gbq(base_query, \n                 configuration=query_config, \n                 progress_bar_type='tqdm',\n                 use_bqstorage_api=True)\n`\n```\n```\n`2021-02-11 09:21:19,532 - preprocessing logger initialized\n2021-02-11 09:21:19,532 - arguments = [file, arg1, arg2, arg3, arg4, project_id, arg5, arg6]\nstarted\nDownloading: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3107858/3107858 [00:14\n    use_bqstorage_api=True)\n  File \"/opt/conda/default/lib/python3.7/site-packages/pandas/io/gbq.py\", line 193, in read_gbq\n    **kwargs,\n  File \"/opt/conda/default/lib/python3.7/site-packages/pandas_gbq/gbq.py\", line 977, in read_gbq\n    dtypes=dtypes,\n  File \"/opt/conda/default/lib/python3.7/site-packages/pandas_gbq/gbq.py\", line 536, in run_query\n    user_dtypes=dtypes,\n  File \"/opt/conda/default/lib/python3.7/site-packages/pandas_gbq/gbq.py\", line 590, in _download_results\n    **to_dataframe_kwargs\n  File \"/opt/conda/default/lib/python3.7/site-packages/google/cloud/bigquery/table.py\", line 1793, in to_dataframe\n    df = record_batch.to_pandas(date_as_object=date_as_object, **extra_kwargs)\n  File \"pyarrow/array.pxi\", line 414, in pyarrow.lib._PandasConvertible.to_pandas\nTypeError: to_pandas() got an unexpected keyword argument 'timestamp_as_object'\n\n`\n```\nhttps://github.com/googleapis/python-bigquery/issues/519",
      "solution": "Dataproc installs by default pyarrow 0.15.0 while the bigquery-storage-api needs a more recent version. Manually setting pyarrow to 3.0.0 at install solved the issue.\nThat being said, PySpark has a compability setting for Pyarrow >= 0.15.0\nhttps://spark.apache.org/docs/3.0.0-preview/sql-pyspark-pandas-with-arrow.html#apache-arrow-in-spark\nI've taken a look at the release notes of dataproc and this env variable is set as default since May 2020.",
      "question_score": 8,
      "answer_score": 4,
      "created_at": "2021-02-11T11:17:38",
      "url": "https://stackoverflow.com/questions/66152733/bigquery-typeerror-to-pandas-got-an-unexpected-keyword-argument-timestamp-as"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 66456548,
      "title": "Document AI: google.api_core.exceptions.InvalidArgument: 400 Request contains an invalid argument",
      "problem": "I am getting this error when trying to implement the Document OCR from google cloud in python as explained here: https://cloud.google.com/document-ai/docs/ocr\nWhen I run\n```\n`   result = client.process_document(request=request)\n`\n```\nI get this error\n```\n`Traceback (most recent call last):\n  File \"/Users/Niolo/Desktop/untitled/Desktop/lib/python3.8/site-packages/google/api_core/grpc_helpers.py\", line 73, in error_remapped_callable\n    return callable_(*args, **kwargs)\n  File \"/Users/Niolo/Desktop/untitled/Desktop/lib/python3.8/site-packages/grpc/_channel.py\", line 923, in __call__\n    return _end_unary_response_blocking(state, call, False, None)\n  File \"/Users/Niolo/Desktop/untitled/Desktop/lib/python3.8/site-packages/grpc/_channel.py\", line 826, in _end_unary_response_blocking\n    raise _InactiveRpcError(state)\ngrpc._channel._InactiveRpcError: \nThe above exception was the direct cause of the following exception:\nTraceback (most recent call last):\n  File \"\", line 1, in \n  File \"/Users/Niolo/Desktop/untitled/Desktop/lib/python3.8/site-packages/google/cloud/documentai_v1beta3/services/document_processor_service/client.py\", line 327, in process_document\n    response = rpc(request, retry=retry, timeout=timeout, metadata=metadata,)\n  File \"/Users/Niolo/Desktop/untitled/Desktop/lib/python3.8/site-packages/google/api_core/gapic_v1/method.py\", line 145, in __call__\n    return wrapped_func(*args, **kwargs)\n  File \"/Users/Niolo/Desktop/untitled/Desktop/lib/python3.8/site-packages/google/api_core/retry.py\", line 281, in retry_wrapped_func\n    return retry_target(\n  File \"/Users/Niolo/Desktop/untitled/Desktop/lib/python3.8/site-packages/google/api_core/retry.py\", line 184, in retry_target\n    return target()\n  File \"/Users/Niolo/Desktop/untitled/Desktop/lib/python3.8/site-packages/google/api_core/grpc_helpers.py\", line 75, in error_remapped_callable\n    six.raise_from(exceptions.from_grpc_error(exc), exc)\n  File \"\", line 3, in raise_from\ngoogle.api_core.exceptions.InvalidArgument: 400 Request contains an invalid argument.\n\n  \n`\n```\nMy full code:\n```\n`import os\n# Import the base64 encoding library.\n\n  \nproject_id= 'your-project-id'\nlocation = 'eu' # Format is 'us' or 'eu'\nprocessor_id = 'your-processor-id' # Create processor in Cloud Console\nfile_path = '/file_path/invoice.pdf'\nos.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/full_path/your_credentials.json\"\n\ndef process_document_sample(\n        project_id: str, location: str, processor_id: str, file_path: str\n):\n    from google.cloud import documentai_v1beta3 as documentai\n\n    # Instantiates a client\n    client = documentai.DocumentProcessorServiceClient()\n\n    # The full resource name of the processor, e.g.:\n    # projects/project-id/locations/location/processor/processor-id\n    # You must create new processors in the Cloud Console first\n    name = f\"projects/{project_id}/locations/{location}/processors/{processor_id}\"\n\n    with open(file_path, \"rb\") as image:\n        image_content = image.read()\n\n    # Read the file into memory\n    document = {\"content\": image_content, \"mime_type\": \"application/pdf\"}\n\n    # Configure the process request\n    request = {\"name\": name, \"document\": document}\n\n    # Recognizes text entities in the PDF document\n    result = client.process_document(request=request)\n\n    document = result.document\n\n    print(\"Document processing complete.\")\n\n    # For a full list of Document object attributes, please reference this page: https://googleapis.dev/python/documentai/latest/_modules/google/cloud/documentai_v1beta3/types/document.html#Document\n\n    document_pages = document.pages\n\n    # Read the text recognition output from the processor\n    print(\"The document contains the following paragraphs:\")\n    for page in document_pages:\n        paragraphs = page.paragraphs\n        for paragraph in paragraphs:\n            paragraph_text = get_text(paragraph.layout, document)\n            print(f\"Paragraph text: {paragraph_text}\")\n`\n```",
      "solution": "`client = documentai.DocumentProcessorServiceClient()` points to US end point by default.\n```\n`in: client = documentai.DocumentProcessorServiceClient()\nin: print(client.DEFAULT_ENDPOINT)\nout: us-documentai.googleapis.com\n`\n```\nYou need to override the api_endpoint to EU for this to work.\n```\n`from google.api_core.client_options import ClientOptions\n    # Set endpoint to EU \n    options = ClientOptions(api_endpoint=\"eu-documentai.googleapis.com:443\")\n    # Instantiates a client\n    client = documentai.DocumentProcessorServiceClient(client_options=options)\n`\n```\nHere is the full code:\n```\n`import os\n\n# TODO(developer): Uncomment these variables before running the sample.\nproject_id= 'your-project-id'\nlocation = 'eu' # Format is 'us' or 'eu'\nprocessor_id = 'your-processor-id' # Create processor in Cloud Console\nfile_path = '/file_path/invoice.pdf'\nos.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/full_path/your_credentials.json\"\n\ndef process_document_sample(\n    project_id: str, location: str, processor_id: str, file_path: str\n):\n    from google.cloud import documentai_v1beta3 as documentai\n    from google.api_core.client_options import ClientOptions\n\n    # Set endpoint to EU\n    options = ClientOptions(api_endpoint=\"eu-documentai.googleapis.com:443\")\n    # Instantiates a client\n    client = documentai.DocumentProcessorServiceClient(client_options=options)\n\n    # The full resource name of the processor, e.g.:\n    # projects/project-id/locations/location/processor/processor-id\n    # You must create new processors in the Cloud Console first\n    name = f\"projects/{project_id}/locations/{location}/processors/{processor_id}\"\n\n    with open(file_path, \"rb\") as image:\n        image_content = image.read()\n\n    # Read the file into memory\n    document = {\"content\": image_content, \"mime_type\": \"application/pdf\"}\n\n    # Configure the process request\n    request = {\"name\": name, \"document\": document}\n\n    # Recognizes text entities in the PDF document\n    result = client.process_document(request=request)\n\n    document = result.document\n\n    print(\"Document processing complete.\")\n\n    # For a full list of Document object attributes, please reference this page: https://googleapis.dev/python/documentai/latest/_modules/google/cloud/documentai_v1beta3/types/document.html#Document\n\n    document_pages = document.pages\n\n    # Read the text recognition output from the processor\n    print(\"The document contains the following paragraphs:\")\n    for page in document_pages:\n        paragraphs = page.paragraphs\n        for paragraph in paragraphs:\n            paragraph_text = get_text(paragraph.layout, document)\n            print(f\"Paragraph text: {paragraph_text}\")\n`\n```\nHere is a snippet of the output:",
      "question_score": 8,
      "answer_score": 9,
      "created_at": "2021-03-03T12:52:42",
      "url": "https://stackoverflow.com/questions/66456548/document-ai-google-api-core-exceptions-invalidargument-400-request-contains-an"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 68736659,
      "title": "Credential implementation provided to initializeApp() via the &quot;credential&quot; property failed to fetch a valid Google OAuth2",
      "problem": "I have a NodeJS Firebase Function that uses firebase-admin sdk. It has been working alright since last year. In the last 2 weeks, the following error started showing in the log:\n`Credential implementation provided to initializeApp() via the \"credential\" property failed to fetch a valid Google OAuth2 access token with the following error: \"Error fetching access token: 404 page not found`\nCould it be an issue with the version of `\"firebase-admin\": \"^7.3.0\"` I am using?\nHave anyone faced the same issue and can you offer some direction on what might have been the cause?",
      "solution": "This looks like a problem with NodeJS, the version 8 is no longer supported as per the documentation. Deployment of functions to the Node.js 8 runtime was disabled in the Firebase CLI on December 15, 2020 and that is most likely why you are facing the 404 error.\nTo migrate to a newer supported version of the NodeJS runtime, use the documentation.\nAlso, I do not suspect any issue with your firebase-admin version 7.3.0.",
      "question_score": 8,
      "answer_score": 4,
      "created_at": "2021-08-11T07:56:17",
      "url": "https://stackoverflow.com/questions/68736659/credential-implementation-provided-to-initializeapp-via-the-credential-prope"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 77565749,
      "title": "Use process.env variables inside next.config.js",
      "problem": "I have an application with NextJS deployed on GCP. Right now I'm setting up the CD for the application and i have three CDs because there's three different deploys. Let's call it cd-one.yaml, cd-two.yaml and cd-three.yaml. For each CD I have an app.yml (app-one.yml, app-two.yml and app-three.yml).\nthis is the code for an app.yml for example:\n\nthe only thing that changes for each app.yml is the service and GCP_SERVICE, for example: one, two and three. The CDs are working and being deployed successfully. But I'm having a problem when I try to use this variables inside next.config.js. For example this is my next-i18next.config.js\n\nI'm using the variable from app.yml and it works just fine. I'm getting the behavior that I wanted. But when I try to do the same for my next.config.js it looks like it's not receiving the variables there because it will always fall on the last condition that is the default. This is my next.config.js\n\nI don't know what to try anymore. Is there any other way to achieve this?",
      "solution": "`next-i18next.config.js` is a runtime configuration and `next.config.js` is a build-time configuration. If your build process occurs in an environment where the app.yaml is not used (like a CI/CD pipeline that doesn't apply these configurations), then the environment variables will not be present. The app.yaml file is specific to Google App Engine and is only applied when the app is being deployed, not during the build.\nSo you need to add the env variables in Cloud Build Trigger:",
      "question_score": 8,
      "answer_score": 4,
      "created_at": "2023-11-28T17:55:53",
      "url": "https://stackoverflow.com/questions/77565749/use-process-env-variables-inside-next-config-js"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 69834735,
      "title": "Programmatically Connecting a GitHub repo to a Google Cloud Project",
      "problem": "I'm working on a Terraform project that will set up all the GCP resources needed for a large project spanning multiple GitHub repos. My goal is to be able to recreate the cloud infrastructure from scratch completely with Terraform.\nThe issue I'm running into is in order to setup build triggers with Terraform within GCP, the GitHub repo that is setting off the trigger first needs to be connected. Currently, I've only been able to do that manually via the Google Cloud Build dashboard. I'm not sure if this is possible via Terraform or with a script but I'm looking for any solution I can automate this with. Once the projects are connected updating everything with Terraform is working fine.\nTLDR; How can I programmatically connect a GitHub project with a GCP project instead of using the dashboard?",
      "solution": "Currently there is no way to programmatically connect a GitHub repo to a Google Cloud Project. This must be done manually via Google Cloud.\nMy workaround is to manually connect an \"admin\" project, build containers and save them to that project's artifact registry, and then deploy the containers from the registry in the programmatically generated project.",
      "question_score": 8,
      "answer_score": 3,
      "created_at": "2021-11-04T06:09:22",
      "url": "https://stackoverflow.com/questions/69834735/programmatically-connecting-a-github-repo-to-a-google-cloud-project"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 68979914,
      "title": "Keyring authentication to Artifact Repository not working (GCP)",
      "problem": "In order to set up keyring authentication to a private pip repository, I followed the steps on Setting up authentication to Python package repositories - Authenticating with keyring.\nI chose to use an environment variable for the authentication. I verified it was set correctly:\n```\n`abc@def:~/PycharmProjects/ghi$ echo $GOOGLE_APPLICATION_CREDENTIALS\n/home/jkl/.googlekeys/serviceaccount.json\n`\n```\nI also logged in my user using `gcloud auth login`.\nThe user that is logged in is the owner of a service account that has write permission on the Artifact Registry.\nUnfortunately, I get the following output when trying to install a package\uff1a\n```\n`abc@def:~/PycharmProjects/ghi$ pip install stringcase\nLooking in indexes: https://pypi.org/simple, https://us-west1-python.pkg.dev/mno-415182/pqr/simple/\nUser for us-west1-python.pkg.dev:\n`\n```\nI expected the keyring to handle the authentication at this step. Any suggestions for determining the cause of this problem?",
      "solution": "As per our discussion in the comments, the problem is that you followed both the `Keyring authentication with user credentials` and `Keyring authentication with service account credentials` steps of the documentation. Those are both different methods of doing the final step in the authentication process.\nTo fix it all you need to do it is revoke the authentication with `gcloud auth revoke --all` and it will work.",
      "question_score": 8,
      "answer_score": 3,
      "created_at": "2021-08-30T08:45:43",
      "url": "https://stackoverflow.com/questions/68979914/keyring-authentication-to-artifact-repository-not-working-gcp"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 77540093,
      "title": "Cloud Run Trigger execution failed when creating a service",
      "problem": "I am having problems creating a service in cloud run. I am following this tutorial, using the Go template that GCP provides.\nBut I get this error: `Trigger execution failed: source code could not be built or deployed, no logs are found`.\nI tried with the template that they provide, and with my own application in Go. The app works fine when running with docker.\nWhat could be happening? I don't have more logs than that.\n\nEdit: In log explorer more logs are shown and there was an error which showed the IAM API disabled. Enabling this api, the creation of the service works fine",
      "solution": "Might sound redundant but have you checked the logs on Log Explorer / Stackdriver? The tutorial you just shared should be pretty straightforward. Maybe there (Log Explorer) we can have some more information",
      "question_score": 8,
      "answer_score": 2,
      "created_at": "2023-11-24T00:56:11",
      "url": "https://stackoverflow.com/questions/77540093/cloud-run-trigger-execution-failed-when-creating-a-service"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 76434349,
      "title": "Firebase Cloud Functions V2 - Error when deploying",
      "problem": "Followed the docs and did the setup as needed, whenever trying to deploy a function, the debug error says\n```\n`Cannot create trigger projects/***********/locations/us-central1/triggers/******-967155: \nInvalid resource state for \\\"\\\": Permission denied while using the Eventarc Service Agent. If you recently started to use Eventarc, it may take a few minutes before all necessary permissions are propagated to the Service Agent. Otherwise, verify that it has Eventarc Service Agent role.\"\n\nCould not create Cloud Run service onrelationadded. spec.template.spec.containers.resources.limits.cpu: Invalid value specified for cpu. For the specified value, maxScale may not exceed 10.\\nConsider running your workload in a region with greater capacity, decreasing your requested cpu-per-instance, or requesting an increase in quota for this region if you are seeing sustained usage near this limit\n`\n```\nComing from the V1 I never saw this error before, after looking a lil bit I found its related to IAM & ADMIN . checked the service default manager permission  and tried to enable permission as Functions Editor/Admin but still without success\n\nCommand : `firebase deploy --only functions`\nResponse:\n```\n`\ni  deploying functions\nRunning command: npm --prefix \"$RESOURCE_DIR\" run build\n\n> build\n> tsc\n\n\u2714  functions: Finished running predeploy script.\ni  functions: preparing codebase default for deployment\ni  functions: ensuring required API cloudfunctions.googleapis.com is enabled...\ni  functions: ensuring required API cloudbuild.googleapis.com is enabled...\ni  artifactregistry: ensuring required API artifactregistry.googleapis.com is enabled...\n\u2714  functions: required API cloudbuild.googleapis.com is enabled\n\u2714  artifactregistry: required API artifactregistry.googleapis.com is enabled\n\u2714  functions: required API cloudfunctions.googleapis.com is enabled\ni  functions: preparing functions directory for uploading...\ni  functions: packaged /User/ ******/******/**** (125.52 KB) for uploading\ni  functions: ensuring required API run.googleapis.com is enabled...\ni  functions: ensuring required API eventarc.googleapis.com is enabled...\ni  functions: ensuring required API pubsub.googleapis.com is enabled...\ni  functions: ensuring required API storage.googleapis.com is enabled...\n\u2714  functions: required API pubsub.googleapis.com is enabled\n\u2714  functions: required API eventarc.googleapis.com is enabled\n\u2714  functions: required API storage.googleapis.com is enabled\n\u2714  functions: required API run.googleapis.com is enabled\ni  functions: generating the service identity for pubsub.googleapis.com...\ni  functions: generating the service identity for eventarc.googleapis.com...\n\u2714  functions: functions folder uploaded successfully\ni  functions: creating Node.js 18 (2nd Gen) function onRelationAdded(us-central1)...\n\u26a0  functions: Your current project quotas don't allow for the current max instances setting of 100. Either reduce this function's maximum instances, or request a quota increase on the underlying Cloud Run service at https://cloud.google.com/run/quotas.\n\u26a0  functions: You can adjust the max instances value in your function's runtime options:\n        setGlobalOptions({maxInstances: 10})\nFailed to create function projects/****/locations/us-central1/functions/Added/user\n\nFunctions deploy had errors with the following functions:\n        AddedUser(us-central1)\ni  functions: cleaning up build files...\n\nError: There was an error deploying functions ```\n`\n```",
      "solution": "To explicitly reduce the default number of instances for all functions in V2,\nneed to use  the `setGlobalOptions` function from the `firebase-functions/v2` package.\n```\n`import { setGlobalOptions } from \"firebase-functions/v2\";\n\n// Set the maximum instances to 10 for all functions\nsetGlobalOptions({ maxInstances: 10 });\n`\n```\n\nUse below code if you are not using ES module\n```\n`const {setGlobalOptions} = require(\"firebase-functions/v2\");\nsetGlobalOptions({maxInstances: 10});\n`\n```",
      "question_score": 7,
      "answer_score": 26,
      "created_at": "2023-06-08T19:31:22",
      "url": "https://stackoverflow.com/questions/76434349/firebase-cloud-functions-v2-error-when-deploying"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 76829663,
      "title": "Error when deploying 2nd gen Firebase Cloud Function",
      "problem": "This Firebase blog post says that Gen 2 functions are available from 7/31/23, but when I try to `firebase deploy` one of them, I get this error:\n\nError: [callGroupRTDBSetup(us-central1)] Upgrading from GCFv1 to GCFv2 is not yet supported. Please delete your old function or wait for this feature to be ready.\n\nA few days ago I recreated my project from scratch, doing `firebase init` all over again. I think the only thing I might not have updated is Node.js. Today I ran `npm update -g firebase-tools` because it told me there was an update available. I also restarted my computer. My `firebase --version` is now 12.4.7.\n```\n`import {onCall} from \"firebase-functions/v2/https\";\n\nexports.callGroupRTDBSetup = onCall({cors: true}, async (req) => {\n  try {\n    const groupID = req.data.groupID;\n    const adminUID = req.data.adminUID;\n    const adminUsername = req.data.adminUsername;\n    const createdTimestamp = req.data.createdTimestamp;\n\n    await setGroupAdminRTDB(groupID, adminUID, adminUsername, createdTimestamp);\n\n    await addMemberRTDB(groupID, adminUID, adminUsername, createdTimestamp);\n\n    return {\n      returnVal: 1,\n      returnMsg: \"callGroupRTDBSetup success!\",\n    };\n  } catch (error) {\n    throw new HttpsError(\"invalid-argument\", \"Error: \" + error);\n  }\n});\n`\n```",
      "solution": "As explained in the documentation:\n\nIt is not possible to upgrade a function from 1st to 2nd gen with the\nsame name and run firebase deploy. Doing so will result in the error:\nUpgrading from GCFv1 to GCFv2 is not yet supported. Please delete your\nold function or wait for this feature to be ready.",
      "question_score": 7,
      "answer_score": 6,
      "created_at": "2023-08-03T17:41:17",
      "url": "https://stackoverflow.com/questions/76829663/error-when-deploying-2nd-gen-firebase-cloud-function"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 72391357,
      "title": "gcloud auth does not work out with windows WSL",
      "problem": "I tried to authenticate with gcp using my personal account, but I can able to make it work entirely.\nso I tried doing it using\n\nwhen I copy the same url and paste it into the browser, it is not working, see this\n\nNot sure why this is now happening, as it is the WSL - ubuntu machine on windows does not have web browser attached to terminal, so I had to do it manually paste into web which is not working.",
      "solution": "I missed, instead of `gcloud auth` I need to do this -\n```\n`gcloud init --console-only\n`\n```\nwhich works out the way I expected. (I copied the url from console paste it into console where verification code)",
      "question_score": 7,
      "answer_score": 3,
      "created_at": "2022-05-26T13:50:52",
      "url": "https://stackoverflow.com/questions/72391357/gcloud-auth-does-not-work-out-with-windows-wsl"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 71612593,
      "title": "GKE vs Cloud run",
      "problem": "I have a python Flask APIs deployed on cloud run. Autoscaling, CPU, concurrency everything is configurable in cloud run.\nNow the problem is with the actual load testing with around 40k concurrent users hitting APIs continuously.\nDoes cloud run handle these huge volumes or should we port our app to GKE?\nWhat are the factors decide Cloud run vs GKE?",
      "solution": "Cloud Run is designed to handle exactly what you're talking about. It's very performant and scalable. You can set things like concurrency per container/service as well which can be handy where payloads might be larger.\nWhere you would use GKE is when you need to customise your platform, perform man in the middle or environment complexity, or to handle potentially long running compute, etc. You might find this in large enterprise or highly regulated environments. Kubernetes is almost like a private cloud though, it's very complex, has its own way of working, and requires ongoing maintenance.\nThis is obviously opinionated but if you can't think of a reason why you need Kubernetes/GKE specifically, Cloud Run wins for an API.\nTo provide more detail though; see Cloud Run Limits and Quotas.\nThe particularly interesting limit is the 1000 container instances but note that it can be increased at request.",
      "question_score": 7,
      "answer_score": 15,
      "created_at": "2022-03-25T06:34:28",
      "url": "https://stackoverflow.com/questions/71612593/gke-vs-cloud-run"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 71556562,
      "title": "Google Cloud Platform adding OAuth Client ID says Requested entity already exists",
      "problem": "I created a OAuth 2 client Id in Google Cloud Platform(GCP) in our production application. However this was only for internal use, so I removed it and tried to add it again in our development GCP project.\nHowever when trying to add it, it says\n```\n`Save failed\nRequested entity already exists\n\nTracking number: xxx\n`\n```\nWhat am I doing wrong? Do I need to do some extra steps to completely remove the OAuth 2 client id? I removed them around a month ago already, so it really should be gone by now.",
      "solution": "It seems after 1 month the problem has automatically resolved itself. I assume it just soft deletes when you press delete, and then hard deletes one month later. Pretty annoying system.\nYou can also remove the entire project to get rid of unwanted ghost clients, but obviously you then lose all configuration.",
      "question_score": 7,
      "answer_score": 4,
      "created_at": "2022-03-21T12:05:57",
      "url": "https://stackoverflow.com/questions/71556562/google-cloud-platform-adding-oauth-client-id-says-requested-entity-already-exist"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 67951166,
      "title": "GCP - access from API Gateway to Google Cloud Run backend",
      "problem": "In my GCP project, I have a python API running in a docker container (using connexion). I want to expose the API (with an API key) using API Gateway.\nWhen I deploy the docker container with `--ingress internal`, I get `Access is forbidden.` on API calls over the Gateway. So the API gateway cannot access the Google Run container.\nWhen I use `--ingress all`, all works as expected, but then my internal API is accessible from the web, which is not what I want.\nI created a service account for this:\n```\n`gcloud iam service-accounts create $SERVICE_ACCOUNT_ID \\\n#  --description=\"the api gateway user\" \\\n#  --display-name=\"api gateway user\"\n`\n```\n... gave the account `run.invoker` permissions:\n```\n`gcloud projects add-iam-policy-binding $PROJECT_ID \\\n  --role=roles/run.invoker --member \\\n  serviceAccount:$SERVICE_ACCOUNT_EMAIL\n`\n```\n... and used the service account to create the API Config:\n```\n`gcloud api-gateway api-configs create $CONFIG_ID \\\n  --api=$API_ID --openapi-spec=$API_DEFINITION \\\n  --project=$PROJECT_ID --backend-auth-service-account=$SERVICE_ACCOUNT_EMAIL\n`\n```\nBut I can't access the docker API from API Gateway. What am I missing here? How can I secure my API, so API Gateway can connect internally.\nUpdate1:\nAlso applied the role to my run service:\n```\n`gcloud run services add-iam-policy-binding $SERVICE_ID \\\n  --region $REGION --member=\"serviceAccount:$SERVICE_ACCOUNT_EMAIL\" \\\n  --role=\"roles/run.invoker\"\n`\n```\nUpdate2:\nSome extra info as requested by John Hanley:\nMy gateway yml looks like this:\n```\n`swagger: '2.0'\n\ninfo:\n  title: \"title\"\n  description: \"description\"\n  version: \"0.1\"\n\nschemes:\n- https\n\nx-google-backend:\n  address: \n\npaths:\n  /api:\n    post:\n      operationId: api\n      consumes:\n        - application/json\n      produces:\n        - application/json\n      security:\n        - api_key: []\n      parameters:\n        - in: body\n          name: request\n          description: request\n          required: true\n          schema:\n            $ref: '#/definitions/Request'\n      responses:\n        200:\n          description: \"success\"\n        400:\n          description: \"bad data\"\n        503:\n          description: \"internal error\"\n\ndefinitions:\n  Request:\n    properties:\n      parameter1:\n        type: string\n      parameter1:\n        type: string\n    required:  \n      - parameter1\n\nsecurityDefinitions:\n  api_key:\n    type: \"apiKey\"\n    name: \"key\"\n    in: \"query\"\n`\n```\n```\n`gcloud api-gateway api-configs describe api-config --api api-api\ncreateTime: '2021-06-12T15:02:27.382098034Z'\ndisplayName: api-config\ngatewayServiceAccount: projects/-/serviceAccounts/apigatewayuser@projectid.iam.gserviceaccount.com\nname: projects/722514052893/locations/global/apis/api-api/configs/api-config\nserviceConfigId: api-config-3hytlxf4gfvzj\nstate: ACTIVE\nupdateTime: '2021-06-12T15:05:09.778404414Z'\n`\n```\n```\n`gcloud api-gateway gateways describe api-gateway --location europe-west1\napiConfig: projects/722514052893/locations/global/apis/api-api/configs/api-config\ncreateTime: '2021-06-12T15:06:03.383002459Z'\ndefaultHostname: api-gateway-97x27n6l.ew.gateway.dev\ndisplayName: api-gateway\nname: projects/projectid/locations/europe-west1/gateways/api-gateway\nstate: ACTIVE\nupdateTime: '2021-06-12T15:07:37.590520122Z'\n`\n```\n```\n`gcloud run services describe api --region europe-west1\n\u2714 Service api in region europe-west1\n \nURL:     https://api-o3rf5h4boa-ew.a.run.app\nIngress: internal\nTraffic:\n  100% LATEST (currently api-00010-lig)\n \nLast updated on 2021-06-12T17:42:49.913232Z by myemail@gmail.com:\n  Revision api-00010-lig\n  Image:         gcr.io/projectid/api\n  Port:          8080\n  Memory:        512Mi\n  CPU:           1000m\n  Concurrency:   80\n  Max Instances: 100\n  Timeout:       300s\n`\n```\nTried debugging directly on Cloud Run:\n```\n`gcloud iam service-accounts keys create $KEY_FILE --iam-account=$SERVICE_ACCOUNT_EMAIL\ngcloud auth activate-service-account $SERVICE_ACCOUNT_EMAIL --key-file $KEY_FILE\nBEARER=$(gcloud auth print-identity-token $SERVICE_ACCOUNT_EMAIL)\n\ncurl --header \"Content-Type: application/json\" \\\n  --header \"Authorization: bearer $BEARER\" \\\n  --request POST \\\n  --data '{\"parameter1\":\"somedata\"}' \\\n  $SERVICE_URL/api\n`\n```\nThe result is still a Forbidden:\n```\n`\n\n  \n  \n  Error 403 (Forbidden)!!1\n  \n    *{margin:0;padding:0}html,code{font:15px/22px arial,sans-serif}html{background:#fff;color:#222;padding:15px}body{margin:7% auto 0;max-width:390px;min-height:180px;padding:30px 0 15px}* > body{background:url(//www.google.com/images/errors/robot.png) 100% 5px no-repeat;padding-right:205px}p{margin:11px 0 22px;overflow:hidden}ins{color:#777;text-decoration:none}a img{border:0}@media screen and (max-width:772px){body{background:none;margin-top:0;max-width:none;padding-right:0}}#logo{background:url(//www.google.com/images/branding/googlelogo/1x/googlelogo_color_150x54dp.png) no-repeat;margin-left:-5px}@media only screen and (min-resolution:192dpi){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat 0% 0%/100% 100%;-moz-border-image:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) 0}}@media only screen and (-webkit-min-device-pixel-ratio:2){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat;-webkit-background-size:100% 100%}}#logo{display:inline-block;height:54px;width:150px}\n  \n  \n  403. That\u2019s an error.\n  Access is forbidden.  That\u2019s all we know.\n`\n```\nSo the problem lies in the Cloud Run application not being accessible by the service account. I'm not sure why this does not work, since the `run.invoker` role was added to the Run service.",
      "solution": "Ingress internal means \"Accept only the requests coming from the project's VPC or VPC SC perimeter\".\nWhen you use API Gateway, you aren't in your VPC, it's serverless, it's in Google Cloud managed VPC. Therefore, your query are forbidden.\nAnd because API Gateway can't be plugged to a VPC Connector (for now) and thus can't route the request to your VPC, you can't use this ingress=internal mode.\n\nThus, the solution is to set an ingress to all, which is not a concern is you authorize only the legit accounts to access it.\nFor that, check in Cloud Run service is there is allUsers granted with the roles/run.invoker in your project.\n\nIf yes, remove it\n\nThen, create a service account and grant it the roles/run.invoker on the Cloud Run service.\nFollow this documentation\n\nStep 4: update the x-google-backend in your OpenAPI spec file to add the correct authentication audience when you call your Cloud Run (it's the base service URL)\nStep 5: create a gateway with a backend service account; set the service account that you created previously\n\nAt the end, only the account authenticated and authorized will be able to reach your Cloud Run service\nAll the unauthorized access are filtered by Google Front End and discarded before reaching your service. Therefore, your service isn't invoked for nothing and therefore your pay nothing!\nOnly API Gateway (and the potential other accounts that you let on the Cloud Run service) can invoke to the Cloud Run service.\nSo, OK, your URL is public, reachable from the wild internet, but protected with Google Front End and IAM.",
      "question_score": 7,
      "answer_score": 14,
      "created_at": "2021-06-12T19:06:16",
      "url": "https://stackoverflow.com/questions/67951166/gcp-access-from-api-gateway-to-google-cloud-run-backend"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 67204755,
      "title": "Firebase Authentication unable to enable Google auth method - &quot;Error updating Google&quot;",
      "problem": "I am trying to enable the Firebase authentication with the Google Auth sign-in method, but enabling it and clicking \"save\" shows the error \"Error updating Google\".\nIn the Google Cloud Console activity logs, it shows:\n\nFailed:google.internal.firebase.v1.FirebaseInternalProductService.EnableGoogleSignIn\n\nWith the error message \"Not found (HTTP 404): Operation failed with error code NOT_FOUND.\"\nHowever, when I tried this in a new Google Cloud project, it worked perfectly. I have tried removing and recreating the Firebase Admin SDK, removing and creating a new app, and removing the OAuth credentials.\nI cannot seem to find any solution to this problem other than creating a new project, but I would prefer to keep my existing project ID.\nAlternatively, if there is any way to reset my GCP project or remake it with the same ID, that would also be fine.",
      "solution": "This issue is caused by deleting the OAuth client autogenerated by Firebase by default.\nTo solve it, you need to first create a new OAuth 2 client ID, and set the necessary redirect URIs for your Firebase app (they should default to something like `https://{PROJECT_ID}.web.app/__/auth/handler`).\nThen, call this API - the request should look something like this, using the client ID and client secret from the credentials generated above:\n`PATCH` `https://identitytoolkit.googleapis.com/admin/v2/projects/{PROJECT_ID}/defaultSupportedIdpConfigs/google.com`\n`{\n  \"name\": \"projects/{PROJECT_ID}/defaultSupportedIdpConfigs/google.com\",\n  \"enabled\": true,\n  \"clientId\": \"{YOUR_CLIENT_ID}\",\n  \"clientSecret\": \"{YOUR_CLIENT_SECRET}\"\n}\n`\nAfter making this API call, the Google authentication provider should be enabled.",
      "question_score": 7,
      "answer_score": 13,
      "created_at": "2021-04-22T01:31:46",
      "url": "https://stackoverflow.com/questions/67204755/firebase-authentication-unable-to-enable-google-auth-method-error-updating-go"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 68533094,
      "title": "How do I access mounted secrets when using Google Cloud Run?",
      "problem": "I have two questions:\n\nWhy can't I mount two cloud secrets in the same directory?\nI have attempted to mount two secrets, `FIREBASE_AUTH_SERVICE_ACCOUNT` and `PURCHASE_VALIDATION_SERVICE_ACCOUNT` in the directory:\n`flask_app/src/services/firebase/service_accounts/`\nHowever I get this error, when attempting to do this:\n`spec.template.spec.containers[0].volume_mounts[1].mount_path, Duplicate volume mount paths are forbidden` Why is this?\n\nHow do I access a mounted secret using python?\nI'm really not sure how to do this as I couldn't find any documentation on how to actually access the secret itself. This is the only thing I found. I am using python just for context. Would the secret be mounted as a .txt and is that mount path the folder that it is stored in or does it also specify the file name?",
      "solution": "With Cloud Run and Secret manager you can load a secret in 2 manners:\n\nLoad a secret in a environment variable, use `--set-secrets=ENV_VAR_NAME=secretName:version`\nLoad a secret in a file, use `--set-secrets=/path/to/file=secretName:version`\n\nTherefore, you can read a secret as you read\n\nAn environment variable (something like `os.getenv()`)\nA file (something like `fs.open('/path/to/file','r')`)\n\nSo, your first question about directory is not clear. If you mount 2 secrets in 2 files in the same directory, no problem!\nIf it doesn't solve your question, please, clarify.",
      "question_score": 7,
      "answer_score": 12,
      "created_at": "2021-07-26T18:02:33",
      "url": "https://stackoverflow.com/questions/68533094/how-do-i-access-mounted-secrets-when-using-google-cloud-run"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 65921120,
      "title": "GCP Load Balancer - Host and path rules not working",
      "problem": "I have a VM that has 3 applications hosted on it. All the apps are working fine with the VM's public IP and port (e.g. 34.44.55.66:{port})\n\nReact JS website 1 (port 3001)\nReact JS website 2 (port 3002)\nExpress JS API 1 (port 3003)\n\nI now want to have all these behind a Cloud Load Balancer. I've done the following for it.\n\nCreated three backend services (one for each application)\nCreated Host and Path rules as below\n\nThe load balancer is created without any issues. However, only the default path works fine. Refer to the details below (consider the LB IP as 55.66.77.88).\n\n55.66.77.88: Website-1 opens without any issue (default path)\n\n55.66.77.88/website-1: Website-1 shows error messages in the browser console\n\n55.66.77.88/website-2: Website-2 shows error messages in the browser console\n\n55.66.77.88/api-1: API-1 shows an error message in the browser console\n`GET http://55.66.77.88/api-1 404 (Not Found)`\n\nWhenever I map any of the above backend services with the default path, they work fine. However, they do not work as expected when the path is entered in the browser.\nAny advice will be appreciated.",
      "solution": "Keep in mind that, in your configuration, there isn't URL rewrite action. That means your request\n```\n`55.66.77.88/website-1/index.html\n`\n```\nwill reach the backend on this path\n```\n`# Backend bucket example\ngs://my-bucket/website-1/index.html\n\n# Instance group backend\n/website-1/index.html\n\n# Network Endpoint Group backend\n/website-1/index.html\n`\n```\nThe base path provided to the load balancer is forwarded. When you haven't additional path in the forward, it works (case of the root path `55.66.77.88/`)\nYou can override the behavior in the advanced mode, and you can define URL rewrite for each of your rules.\n\nYou can define rewrite rule like this in the console\n\nClick on advanced configuration\n\nSet up your default configuration (any domain, any path)\n\nThen click on add host and path rule\n\nPut * in the host as you did\n\nThen click on the pencil of the path rule section to define the rule by default. Here again, select your backend by default. no special rewrite, it's the default path.\n\nNow click on add path rule.\n\nAdd your matching paths. And only / in the rewrite URL part (you can rewrite your host if the backend rely on the host name, but it doesn't seem your case)\n\nDo this for all your backend and test it (let 3 - 5 minutes to propagate your update to the edge node)",
      "question_score": 7,
      "answer_score": 11,
      "created_at": "2021-01-27T15:25:30",
      "url": "https://stackoverflow.com/questions/65921120/gcp-load-balancer-host-and-path-rules-not-working"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 70561720,
      "title": "Google Cloud Platform not letting me delete project",
      "problem": "3 new projects suddenly appeared in my Google Cloud.  I don't know where they came from, and I can't delete them.\nFollowing the delete instructions: https://cloud.google.com/go/getting-started#delete-the-project\nWhen I select each of the projects to delete, the delete button disables, and shows the float over message:\n```\n`You need permissions for this action.\nRequired permission(s): resourcemanager.projects.delete\n`\n```\nAll my other projects are fine, and I can delete them.\nThe three new projects that suddenly appeared are:\n```\n`My Project 64342    brave-watch-314519\nMy Project 82497    beaming-light-336511\nYou can see this project    you-can-see-this-project\n`\n```\nScreenshot:\n\nHow do I give myself permission to delete them?\nNote 1: I am the only owner/user of this Google Cloud account.\nNote 2: I also tried going to IAM -> Roles, but got the error:",
      "solution": "It seems to be a known issue with GCP. Leaving \u201cGoogle Groups\u201d related to GCP is a fix to this issue. You can track this Public Issue for more information.\nYou might have been added into a project through a group, so it appears in the project list. However, you have not been granted permission to modify the IAM of that project, so you can't remove the group from the permission list.\nAs a workaround, you can leave \"Google Groups\" related to GCP and reload the GCP console webpage so that all your unknown/inaccessible projects will disappear from the projects list. You can find what groups you're a member of, using this Google Groups link.\nNOTE : You can leave the groups in order to lose the access, but there could be a situation where your email is added to a single role/permission and you would not be able to remove yourself from the IAM list.",
      "question_score": 7,
      "answer_score": 9,
      "created_at": "2022-01-03T05:37:39",
      "url": "https://stackoverflow.com/questions/70561720/google-cloud-platform-not-letting-me-delete-project"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 69366431,
      "title": "How to access cloud run environment variables in Dockerfile",
      "problem": "I have built a containerised python application which runs without issue locally using a .env file and and a docker-compose.yml file compiled with compose build.\nI am then able to use variables within the Dockerfile like this.\n```\n`ARG APP_USR\nENV APP_USR ${APP_USR}\n\nARG APP_PASS\nENV APP_PASS ${APP__PASS}\n\nRUN pip install https://${APP_USR}:${APP_PASS}@github.org/*****/master.zip\n`\n```\nI am deploying to cloud run via a synced bitbucket repository, and have defined  under \"REVISIONS\" > \"SECRETS AND VARIABLES\",(as described here: https://cloud.google.com/run/docs/configuring/environment-variables)\nbut I can not work out how to access these variables in the Dockerfile during build.\nAs I understand it, I need to create a cloudbuild.yaml file to define the variables, but I haven't been able to find a clear example of how to set this up using the Environment variables defined in cloud run.",
      "solution": "My understanding is that it is not possible to directly use a Cloud Run revision's environment variables in the Dockerfile because the build is managed by Cloud Build, which doesn't know about Cloud Run revision before the deployment.\nBut I was able to use Secret Manager's secrets in the Dockerfile.\nSources:\n\nPassing secrets from Secret Manager to `cloudbuild.yaml`: https://cloud.google.com/build/docs/securing-builds/use-secrets\nPassing an environment variable from `cloudbuild.yaml` to `Dockerfile`: https://vsupalov.com/docker-build-pass-environment-variables/\n\nQuick summary:\nIn your case, for `APP_USR` and `APP_PASS`:\n\nGrant the Secret Manager Secret Accessor (roles/secretmanager.secretAccessor) IAM role for the secret to the Cloud Build service account (see first source).\n\nAdd an `availableSecrets` block at the end of the `cloudbuild.yaml` file (out of the `steps` block):\n\n`availableSecrets:\n  secretManager:\n  - versionName: \n    env: 'APP_USR'\n  - versionName: \n    env: 'APP_PASS'\n`\n\nPass the secrets to your build step (depends on how you summon `docker build`, Google's documentation uses 'bash', I use Docker directly):\n\n`  - id: Build\n    name: gcr.io/cloud-builders/docker\n    args:\n      - build\n      - '-f=Dockerfile'\n      - '.'\n\n      # Add these two `--build-arg` params:\n\n      - '--build-arg'\n      - 'APP_USR=$$APP_USR'\n\n      - '--build-arg'\n      - 'APP_PASS=$$APP_PASS'\n\n    secretEnv: ['APP_USR', 'APP_PASS'] # \n\nUse these secrets as standard environment variables in your `Dockerfile`:\n\n```\n`ARG APP_USR\nENV APP_USR=$APP_USR\n\nARG APP_PASS\nENV APP_PASS=$APP_PASS\n\nRUN pip install https://$APP_USR:$APP_PASS@github.org/*****/master.zip\n`\n```",
      "question_score": 7,
      "answer_score": 5,
      "created_at": "2021-09-28T19:57:22",
      "url": "https://stackoverflow.com/questions/69366431/how-to-access-cloud-run-environment-variables-in-dockerfile"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 65657720,
      "title": "Does &quot;Domain Restricted Sharing&quot; in GCP prevent service accounts from getting IAM permissions?",
      "problem": "If I turn on the Organization Policy constraint \"Domain Restricted Sharing\" (doc) and set it to allow only my org domain `foo.com`, will this prevent the slew of platform service accounts from getting their IAM permissions granted?  For instance, accounts in the domain `@iam.gserviceaccount.com` or `@developer.gserviceaccount.com`.  These service accounts get provisioned and given permissions all over the place.  My worry is that enabling \"Domain Restricted Sharing\" will block these accounts from having IAM access.\nAnother way to ask this is: does \"Domain Restricted Sharing\" ignore these sorts of platform-based service accounts?  If it doesn't, I feel like it would be difficult to maintain a list of exceptions.\nA more fundamental question - does \"Domain Restricted Sharing\" only apply to Cloud Identity / Google Workspace accounts, and is hence not relevant when it comes to service accounts?",
      "solution": "In this answer I am using the term `Google Cloud Identities` meaning identities such as service accounts, service agents, etc. that are created by Google Cloud and not by other Google services such as Gmail.\n\nIf turn on the Organization Policy constraint \"Domain Restricted\nSharing\" ...\n\nYes, if the IAM service accounts are children of an organization resource associated with the given Google Workspace domain.\n\nA more fundamental question - does \"Domain Restricted Sharing\" only\napply to Cloud Identity / Google Workspace accounts, and is hence not\nrelevant when it comes to service accounts?\n\nDomain Restricted Sharing applies to all non Google Cloud Identities such a Google Workspace, Cloud Identity and Gmail style accounts. You can define members of a domain managed/controlled by Google Workspace as being allowed (me@example.com) while identities that are not part of that domain (me@gmail.com) are blocked.\nAt this time, only domains managed by Google Workspace are supported. Cloud Identity is not supported for specifying an allowed domain unless the domain name is also the organization name. (Note: I cannot find an authoritative reference for this statement and this may change in the future).",
      "question_score": 7,
      "answer_score": 9,
      "created_at": "2021-01-10T20:44:27",
      "url": "https://stackoverflow.com/questions/65657720/does-domain-restricted-sharing-in-gcp-prevent-service-accounts-from-getting-ia"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 77277981,
      "title": "Error while running apt-get update with GCSFuse installed",
      "problem": "I have GCSFuse installed on my machine and I am seeing the following error while running sudo apt-get update command.\n```\n`E: Repository 'http://packages.cloud.google.com/apt gcsfuse-focal InRelease' changed its 'Origin' value from 'gcsfuse-jessie' to 'namespaces/gcs-fuse-prod/repositories/gcsfuse-focal'\nE: Repository 'http://packages.cloud.google.com/apt gcsfuse-focal InRelease' changed its 'Label' value from 'gcsfuse-jessie' to 'namespaces/gcs-fuse-prod/repositories/gcsfuse-focal'\nN: This must be accepted explicitly before updates for this repository can be applied. See apt-secure(8) manpage for details.\n`\n```\nI was wondering if this is intentional or something changed?",
      "solution": "The origin and label were changed because the backend used to host these packages changed. This is an intentional change. Please use one of the following commands to run apt-get update:\n```\n`sudo apt-get update --allow-releaseinfo-change\n`\n```\nOR\n```\n`sudo apt update -y && sudo apt-get update\n`\n```\nThis should be a one-time problem. Any following apt-get commands should work fine.",
      "question_score": 7,
      "answer_score": 8,
      "created_at": "2023-10-12T07:46:52",
      "url": "https://stackoverflow.com/questions/77277981/error-while-running-apt-get-update-with-gcsfuse-installed"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 75547631,
      "title": "Overwrite single file in a Google Cloud Storage bucket, via Python code",
      "problem": "I have a `logs.txt` file at certain location, in a Compute Engine VM Instance. I want to periodically backup (i.e. overwrite) `logs.txt` in a Google Cloud Storage bucket. Since `logs.txt` is the result of some preprocessing made inside a Python script, I want to also use that script to upload / copy that file, into the Google Cloud Storage bucket (therefore, the use of `cp` cannot be considered an option). Both the Compute Engine VM instance, and the Cloud Storage bucket, stay at the same GCP project, so \"they see each other\". What I am attempting right now, based on this sample code, looks like:\n`from google.cloud import storage\n\nbucket_name = \"my-bucket\"\ndestination_blob_name = \"logs.txt\"\nsource_file_name = \"logs.txt\"  # accessible from this script\n\nstorage_client = storage.Client()\nbucket = storage_client.bucket(bucket_name)\nblob = bucket.blob(destination_blob_name)\n\ngeneration_match_precondition = 0\nblob.upload_from_filename(source_file_name, if_generation_match=generation_match_precondition)\n\nprint(f\"File {source_file_name} uploaded to {destination_blob_name}.\")\n`\nIf `gs://my-bucket/logs.txt` does not exist, the script works correctly, but if I try to overwrite, I get the following error:\n```\n`Traceback (most recent call last):\n  File \"/usr/local/lib/python3.8/dist-packages/google/cloud/storage/blob.py\", line 2571, in upload_from_file\n    created_json = self._do_upload(\n  File \"/usr/local/lib/python3.8/dist-packages/google/cloud/storage/blob.py\", line 2372, in _do_upload\n    response = self._do_multipart_upload(\n  File \"/usr/local/lib/python3.8/dist-packages/google/cloud/storage/blob.py\", line 1907, in _do_multipart_upload\n    response = upload.transmit(\n  File \"/usr/local/lib/python3.8/dist-packages/google/resumable_media/requests/upload.py\", line 153, in transmit\n    return _request_helpers.wait_and_retry(\n  File \"/usr/local/lib/python3.8/dist-packages/google/resumable_media/requests/_request_helpers.py\", line 147, in wait_and_retry\n    response = func()\n  File \"/usr/local/lib/python3.8/dist-packages/google/resumable_media/requests/upload.py\", line 149, in retriable_request\n    self._process_response(result)\n  File \"/usr/local/lib/python3.8/dist-packages/google/resumable_media/_upload.py\", line 114, in _process_response\n    _helpers.require_status_code(response, (http.client.OK,), self._get_status_code)\n  File \"/usr/local/lib/python3.8/dist-packages/google/resumable_media/_helpers.py\", line 105, in require_status_code\n    raise common.InvalidResponse(\ngoogle.resumable_media.common.InvalidResponse: ('Request failed with status code', 412, 'Expected one of', )\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/my_folder/upload_to_gcs.py\", line 76, in \n    blob.upload_from_filename(source_file_name, if_generation_match=generation_match_precondition)\n  File \"/usr/local/lib/python3.8/dist-packages/google/cloud/storage/blob.py\", line 2712, in upload_from_filename\n    self.upload_from_file(\n  File \"/usr/local/lib/python3.8/dist-packages/google/cloud/storage/blob.py\", line 2588, in upload_from_file\n    _raise_from_invalid_response(exc)\n  File \"/usr/local/lib/python3.8/dist-packages/google/cloud/storage/blob.py\", line 4455, in _raise_from_invalid_response\n    raise exceptions.from_http_status(response.status_code, message, response=response)\ngoogle.api_core.exceptions.PreconditionFailed: 412 POST https://storage.googleapis.com/upload/storage/v1/b/production-onementor-dt-data/o?uploadType=multipart&ifGenerationMatch=0: {\n  \"error\": {\n    \"code\": 412,\n    \"message\": \"At least one of the pre-conditions you specified did not hold.\",\n    \"errors\": [\n      {\n        \"message\": \"At least one of the pre-conditions you specified did not hold.\",\n        \"domain\": \"global\",\n        \"reason\": \"conditionNotMet\",\n        \"locationType\": \"header\",\n        \"location\": \"If-Match\"\n      }\n    ]\n  }\n}\n: ('Request failed with status code', 412, 'Expected one of', )\n`\n```\nI have checked the documentation for `upload_from_filename`, but it seems there is no flag to \"enable overwritting\".\nHow to properly overwrite a file existing in a Google Cloud Storage Bucket, using Python language?",
      "solution": "It's because of if_generation_match\n\nAs a special case, passing 0 as the value for if_generation_match\nmakes the operation succeed only if there are no live versions of the\nblob.\n\nThis is what is meant by the return message \"At least one of the pre-conditions you specified did not hold.\"\nYou should pass `None` or leave out that argument altogether.",
      "question_score": 7,
      "answer_score": 8,
      "created_at": "2023-02-23T17:28:27",
      "url": "https://stackoverflow.com/questions/75547631/overwrite-single-file-in-a-google-cloud-storage-bucket-via-python-code"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 69145410,
      "title": "cloud run not auto deploying?",
      "problem": "I have a strange problem. I setup continous delivery for cloud run when I commit to a specific dev branch of my github code.\nWhen I commit, I see it notices the change and builds the code but it does not automatically deploy it to the cloud run instance.\nI have to manually select edit and deploy new revision then select the image that was created.\nWhat can I do to automatically deploy?",
      "solution": "I figured it out. Documentation here\nI think the problem was that cloud build continuous deployment setup automates the building when a change is detected in my github repo but it didn't actually deploy the image it built.\nI had to create a file called `cloudbuild.yaml` and save this file(updated for my cloud run instance)\n```\n` steps:\n # Build the container image\n - name: 'gcr.io/cloud-builders/docker'\n   args: ['build', '-t', 'gcr.io/$PROJECT_ID/SERVICE-NAME:$COMMIT_SHA', '.']\n # Push the container image to Container Registry\n - name: 'gcr.io/cloud-builders/docker'\n   args: ['push', 'gcr.io/$PROJECT_ID/SERVICE-NAME:$COMMIT_SHA']\n # Deploy container image to Cloud Run\n - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'\n   entrypoint: gcloud\n   args:\n   - 'run'\n   - 'deploy'\n   - 'SERVICE-NAME'\n   - '--image'\n   - 'gcr.io/$PROJECT_ID/SERVICE-NAME:$COMMIT_SHA'\n   - '--region'\n   - 'REGION'\n images:\n - 'gcr.io/$PROJECT_ID/SERVICE-NAME:$COMMIT_SHA'\n`\n```",
      "question_score": 7,
      "answer_score": 8,
      "created_at": "2021-09-11T19:47:07",
      "url": "https://stackoverflow.com/questions/69145410/cloud-run-not-auto-deploying"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 73502275,
      "title": "Is there a way to fetch Google Cloud Monitoring Incident metrics via the API?",
      "problem": "When reviewing the documentation here (https://cloud.google.com/monitoring/alerts/incidents-events#incident) and using the general product, it appears that the open incidents and associated details are only displayed via the Google Cloud Console.\nThe CLI and API appear to only support management of alerting policies, but I cannot find a way to retrieve a list of open incidents. For example, I'd like to send an alert if more than 5 incidents are open for 12 hours on a specific alerting policy. It would seem that the data exists (as provided from the Google Cloud Console), but the API is not public.",
      "solution": "I think it is not possible.\nIncidents appear to not be part of Google's public API for Cloud Monitoring.\nThere are a couple of ways to verify this:\n\nAPIs Explorer documents Cloud Monitoring API and there are no Incident resource types nor methods.\nUsing e.g. Chrome Developer Console while browsing Cloud Console: Incidents doesn't (appear to) include any public API endpoints/methods\n\nThere's an existing feature request (FR) on Google's public issue tracker for this.\nI encourage you to \"Star this issue\" by clicking the star icon to the left of the issue title Manage Incidents via API to both \"+1\" the FR and to subscribe to updates on it.",
      "question_score": 7,
      "answer_score": 7,
      "created_at": "2022-08-26T16:05:58",
      "url": "https://stackoverflow.com/questions/73502275/is-there-a-way-to-fetch-google-cloud-monitoring-incident-metrics-via-the-api"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 72309492,
      "title": "How do I install the same pip dependencies locally as are installed in my Cloud Composer Airflow environment on GCP?",
      "problem": "I'm trying to set up a local development environment in VS Code where I'd get code completion for the packages Cloud Composer/Apache Airflow uses. I've been successful so far using a virtual environment (created with `python -m venv .venv`) and a very minimal `requirements.txt` file that contains just the Airflow package, installed into the local environment.\nThe file is like this:\n```\n`apache-airflow==1.10.15\n`\n```\nAnd I can install it into my virtual environment by running `pip install -r requirements.txt` after activating my virtual environment in VS Code, after which I get code completion in VS Code for the quickstart DAG in their docs, the `BashOperator`:\n\nI wanted to get more code completion as I followed more tutorials. For example, following the `KubernetesPodOperator` tutorial (https://cloud.google.com/composer/docs/how-to/using/using-kubernetes-pod-operator), I get this error, and VS Code doesn't recognize the import:\n```\n\nImport \"airflow.providers.cncf.kubernetes.operators.kubernetes_pod\" could not be resolved Pylance(reportMissingImports)\n\n```\nI figured that a good next step would be to install exactly the same PyPI packages into my virtual environment as are running in the Cloud Composer environment. I used the page https://cloud.google.com/composer/docs/concepts/versioning/composer-versions to see which packages were installed:\n\nSo my `requirements.txt` file then looked like this:\n```\n`absl-py==1.0.0\nalembic==1.5.7\namqp==2.6.1\napache-airflow==1.10.15+composer\napache-airflow-backport-providers-apache-beam==2021.3.13\napache-airflow-backport-providers-cncf-kubernetes==2021.3.3\napache-airflow-backport-providers-google==2022.4.1+composer\napache-beam==2.37.0\napispec==1.3.3\nappdirs==1.4.4\nargcomplete==1.12.2\nastunparse==1.6.3\nattrs==20.3.0\nBabel==2.9.0\nbcrypt==3.2.0\nbilliard==3.6.3.0\ncached-property==1.5.2\ncachetools==4.2.1\ncattrs==1.1.2\ncelery==4.4.7\ncertifi==2020.12.5\ncffi==1.14.5\nchardet==4.0.0\nclick==6.7\ncloudpickle==2.0.0\ncolorama==0.4.4\ncolorlog==4.0.2\nconfigparser==3.5.3\ncrcmod==1.7\ncroniter==0.3.37\ncryptography==3.4.6\ndefusedxml==0.7.1\ndill==0.3.1.1\ndistlib==0.3.1\ndnspython==2.1.0\ndocopt==0.6.2\ndocutils==0.16\nemail-validator==1.1.2\nfastavro==1.3.4\nfasteners==0.17.3\nfilelock==3.0.12\nFlask==1.1.2\nFlask-Admin==1.5.4\nFlask-AppBuilder==2.3.4\nFlask-Babel==1.0.0\nFlask-Bcrypt==0.7.1\nFlask-Caching==1.3.3\nFlask-JWT-Extended==3.25.1\nFlask-Login==0.4.1\nFlask-OpenID==1.3.0\nFlask-SQLAlchemy==2.5.1\nflask-swagger==0.2.14\nFlask-WTF==0.14.3\nflower==0.9.7\nfuncsigs==1.0.2\nfuture==0.18.2\ngast==0.3.3\ngoogle-ads==7.0.0\ngoogle-api-core==1.31.5\ngoogle-api-python-client==1.12.8\ngoogle-apitools==0.5.31\ngoogle-auth==1.28.0\ngoogle-auth-httplib2==0.1.0\ngoogle-auth-oauthlib==0.4.3\ngoogle-cloud-aiplatform==1.12.1\ngoogle-cloud-automl==2.7.2\ngoogle-cloud-bigquery==1.28.0\ngoogle-cloud-bigquery-datatransfer==3.6.1\ngoogle-cloud-bigquery-storage==2.6.3\ngoogle-cloud-bigtable==1.7.0\ngoogle-cloud-build==2.0.0\ngoogle-cloud-container==1.0.1\ngoogle-cloud-core==1.6.0\ngoogle-cloud-datacatalog==3.7.1\ngoogle-cloud-dataplex==0.2.1\ngoogle-cloud-dataproc==3.3.1\ngoogle-cloud-dataproc-metastore==1.5.0\ngoogle-cloud-datastore==1.15.3\ngoogle-cloud-dlp==1.0.0\ngoogle-cloud-kms==2.11.1\ngoogle-cloud-language==1.3.0\ngoogle-cloud-logging==2.2.0\ngoogle-cloud-memcache==1.3.1\ngoogle-cloud-monitoring==2.0.0\ngoogle-cloud-os-login==2.6.1\ngoogle-cloud-pubsub==2.12.0\ngoogle-cloud-pubsublite==1.4.1\ngoogle-cloud-redis==2.8.0\ngoogle-cloud-resource-manager==1.4.1\ngoogle-cloud-secret-manager==1.0.0\ngoogle-cloud-spanner==1.19.1\ngoogle-cloud-speech==1.3.2\ngoogle-cloud-storage==1.36.2\ngoogle-cloud-tasks==2.8.1\ngoogle-cloud-texttospeech==1.0.1\ngoogle-cloud-translate==1.7.0\ngoogle-cloud-videointelligence==1.16.1\ngoogle-cloud-vision==1.0.0\ngoogle-cloud-workflows==1.6.1\ngoogle-crc32c==1.1.2\ngoogle-pasta==0.2.0\ngoogle-resumable-media==1.2.0\ngoogleapis-common-protos==1.53.0\ngraphviz==0.16\ngrpc-google-iam-v1==0.12.3\ngrpcio==1.44.0\ngrpcio-gcp==0.2.2\ngrpcio-status==1.44.0\ngunicorn==20.0.4\nh5py==2.10.0\nhdfs==2.6.0\nhttplib2==0.17.4\nhumanize==3.3.0\nidna==2.8\nimportlib-metadata==2.1.1\nimportlib-resources==1.5.0\niso8601==0.1.14\nitsdangerous==1.1.0\nJinja2==2.11.3\njson-merge-patch==0.2\njsonschema==3.2.0\nKeras-Preprocessing==1.1.2\nkombu==4.6.11\nkubernetes==11.0.0\nlazy-object-proxy==1.4.3\nlibcst==0.3.17\nlockfile==0.12.2\nMako==1.1.4\nMarkdown==2.6.11\nMarkupSafe==1.1.1\nmarshmallow==2.21.0\nmarshmallow-enum==1.5.1\nmarshmallow-sqlalchemy==0.23.1\nmock==2.0.0\nmonotonic==1.5\nmypy-extensions==0.4.3\nmysqlclient==1.3.14\nnatsort==7.1.1\nnumpy==1.19.5\noauth2client==4.1.3\noauthlib==3.1.0\nopt-einsum==3.3.0\norjson==3.6.8\noverrides==6.1.0\npackaging==20.9\npandas==1.1.5\npandas-gbq==0.14.1\npbr==5.8.1\npendulum==1.4.4\npip==20.1.1\npipdeptree==1.0.0\nprison==0.1.3\nprometheus-client==0.8.0\nproto-plus==1.18.1\nprotobuf==3.15.6\npsutil==5.8.0\npsycopg2-binary==2.8.6\npyarrow==2.0.0\npyasn1==0.4.8\npyasn1-modules==0.2.8\npycparser==2.20\npydata-google-auth==1.1.0\npydot==1.4.2\nPygments==2.8.1\nPyJWT==1.7.1\npymongo==3.11.3\npyOpenSSL==20.0.1\npyparsing==2.4.7\npyrsistent==0.17.3\npython-daemon==2.3.0\npython-dateutil==2.8.1\npython-editor==1.0.4\npython-http-client==3.3.4\npython-nvd3==0.15.0\npython-slugify==4.0.1\npython3-openid==3.2.0\npytz==2021.1\npytzdata==2020.1\nPyYAML==5.4.1\nredis==3.5.3\nrequests==2.25.1\nrequests-oauthlib==1.3.0\nrsa==4.7.2\nscipy==1.4.1\nsendgrid==5.6.0\nsetproctitle==1.2.2\nsetuptools==57.5.0\nsix==1.15.0\nSQLAlchemy==1.3.20\nSQLAlchemy-JSONField==0.9.0\nSQLAlchemy-Utils==0.36.8\nstatsd==3.3.0\ntabulate==0.8.9\ntenacity==4.12.0\ntensorboard==2.2.2\ntensorboard-plugin-wit==1.8.1\ntensorflow==2.2.0\ntensorflow-estimator==2.2.0\ntermcolor==1.1.0\ntext-unidecode==1.3\nthrift==0.13.0\ntornado==5.1.1\ntyping-extensions==3.7.4.3\ntyping-inspect==0.6.0\ntyping-utils==0.1.0\ntzlocal==1.5.1\nunicodecsv==0.14.1\nuritemplate==3.0.1\nurllib3==1.26.4\nvine==1.3.0\nvirtualenv==20.4.3\nwebsocket-client==0.58.0\nWerkzeug==0.16.1\nwheel==0.37.1\nwrapt==1.12.1\nWTForms==2.3.3\nzipp==3.4.1\nzope.deprecation==4.4.0\n`\n```\nWhen I tried running `pip install -r requirements.txt` again, I get the following error:\n```\n\nERROR: Could not find a version that satisfies the requirement apache-airflow==1.10.15+composer (from versions: 1.10.9-bin, 1.8.1, 1.8.2rc1, 1.8.2, 1.9.0, 1.10.0, 1.10.1b1, 1.10.1rc2, 1.10.1, 1.10.2b2, 1.10.2rc1, 1.10.2rc2, 1.10.2rc3, 1.10.2, 1.10.3b1, 1.10.3b2, 1.10.3rc1, 1.10.3rc2, 1.10.3, 1.10.4b2, 1.10.4rc1, 1.10.4rc2, 1.10.4rc3, 1.10.4rc4, 1.10.4rc5, 1.10.4, 1.10.5rc1, 1.10.5, 1.10.6rc1, 1.10.6rc2, 1.10.6, 1.10.7rc1, 1.10.7rc2, 1.10.7rc3, 1.10.7, 1.10.8rc1, 1.10.8, 1.10.9rc1, 1.10.9, 1.10.10rc1, 1.10.10rc2, 1.10.10rc3, 1.10.10rc4, 1.10.10rc5, 1.10.10, 1.10.11rc1, 1.10.11rc2, 1.10.11, 1.10.12rc1, 1.10.12rc2, 1.10.12rc3, 1.10.12rc4, 1.10.12, 1.10.13rc1, 1.10.13, 1.10.14rc1, 1.10.14rc2, 1.10.14rc3, 1.10.14rc4, 1.10.14, 1.10.15rc1, 1.10.15, 2.0.0b1, 2.0.0b2, 2.0.0b3, 2.0.0rc1, 2.0.0rc2, 2.0.0rc3, 2.0.0, 2.0.1rc1, 2.0.1rc2, 2.0.1, 2.0.2rc1, 2.0.2, 2.1.0rc1, 2.1.0rc2, 2.1.0, 2.1.1rc1, 2.1.1, 2.1.2rc1, 2.1.2, 2.1.3rc1, 2.1.3, 2.1.4rc1, 2.1.4rc2, 2.1.4, 2.2.0b1, 2.2.0b2, 2.2.0rc1, 2.2.0, 2.2.1rc1, 2.2.1rc2, 2.2.1, 2.2.2rc1, 2.2.2rc2, 2.2.2, 2.2.3rc1, 2.2.3rc2, 2.2.3, 2.2.4rc1, 2.2.4, 2.2.5rc1, 2.2.5rc2, 2.2.5rc3, 2.2.5, 2.3.0b1, 2.3.0rc1, 2.3.0rc2, 2.3.0)\nERROR: No matching distribution found for apache-airflow==1.10.15+composer\n\n```\nWhen I looked at the PyPI website, I noticed that some of the packages that have \"+composer\" in their name in `requirements.txt` don't exist in PyPI. For example, `apache-airflow==1.10.15+composer` and `apache-airflow-backport-providers-google==2022.4.1+composer` don't exist there. Does this mean that those packages are not publicly available? I'm relatively new to Python and Airflow, so these are just some ideas I've been thinking of since I encountered this issue. I may be on the wrong track.\nI'd appreciate any help I can get here in installing these packages into my local virtual environment, or installing some other packages that would achieve my goal of being able to do local development, with code completion, on DAGs.\nHere's the script I used to create my environment for this test, for reference:\n`#!/bin/bash\n\ngcloud composer environments create my-environment \\\n    --location us-central1 \\\n    --image-version composer-1.18.8-airflow-1.10.15 # uses Python 3.8.12\n\n`",
      "solution": "So the two incompatibilities in Cloud Composer dependencies as listed on the official website are `apache-airflow` and `apache-airflow-providers-google` (or `apache-airflow-backport-providers-google` if you are using Cloud Composer v1).\n\nWhat you need to do is to replace these two dependencies with the correct pins.\nFor example, if you are running `composer-2.0.16-airflow-2.2.5` version that specifies the two dependencies as\n```\n`apache-airflow==2.2.5+composer\napache-airflow-providers-google==2022.5.18+composer\n`\n```\nYou need to replace them with\n```\n`apache-airflow==2.2.5\napache-airflow-providers-google==7.0.0\n`\n```\n\nIf you are wondering how I came up with the  specific version for `apache-airflow-providers-google` then what you need to do is head the page containing the list of commits included in each release.\nAt the top of each release, you can see the date of the latest commit. Then the specific package version will be the one with the latest 'Latest change' prior to the date specified in the original listing on Cloud Composer version page (in this example that'd be `2022.5.18`).\n\nNote that for some specific composer versions, the `apache-ariflow-providers-google` dependency is specified explicitly (.e.g `6.7.0` or `6.8.0`). Not sure if the date convention is there by mistake or perhaps a convention that we are not aware of (?)",
      "question_score": 7,
      "answer_score": 7,
      "created_at": "2022-05-19T20:29:53",
      "url": "https://stackoverflow.com/questions/72309492/how-do-i-install-the-same-pip-dependencies-locally-as-are-installed-in-my-cloud"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 68259141,
      "title": "Using proxy with Chromedriver within Google Cloud Engine",
      "problem": "I'm trying to use a proxy within Google Cloud Engine with chromedriver.\nI've tried many solutions suggested (see below) but everytime the IP was the one on Google server.\nAttempt 1:\n```\n`from selenium import webdriver\nfrom selenium.webdriver.chrome.options import Options\n\nchrome_options = Options()\nchrome_options.add_argument(\"--headless\")\nchrome_options.add_argument(\"--no-sandbox\")\nchrome_options.add_argument(\"--window-size=1920x1080\")\nchrome_options.add_argument(\"--ignore-certificate-errors\")\n\nmyproxy = '207.157.25.44:80'\nprox = Proxy()\nprox.proxy_type = ProxyType.MANUAL\nprox.http_proxy = myproxy\nprox.ssl_proxy = myproxy\n\ncapabilities = webdriver.DesiredCapabilities.CHROME\nprox.add_to_capabilities(capabilities)\n\ndriver = webdriver.Chrome(options=chrome_options, \n    executable_path=\"/user/sebastien/chromedriver\", \n    desired_capabilities=capabilities)\ndriver.get(\"https://www.whatismyip.com/\")\nget_location()\n\n`\n```\nAttempt 2:\n```\n`from selenium import webdriver\nfrom selenium.webdriver.chrome.options import Options\n\nchrome_options = Options()\nchrome_options.add_argument(\"--headless\")\nchrome_options.add_argument(\"--no-sandbox\")\nchrome_options.add_argument(\"--window-size=1920x1080\")\nchrome_options.add_argument(\"--ignore-certificate-errors\")\n\nmyproxy = '207.157.25.44:80'\nprefs = {}\nprefs[\"network.proxy.type\"] = 1\nprefs[\"network.proxy.http\"] = myproxy\nprefs[\"network.proxy.ssl\"] = myproxy\n\nchrome_options.add_experimental_option('prefs', prefs)\n\ndriver = webdriver.Chrome(options=chrome_options, \n    executable_path=\"/user/sebastien/chromedriver\")\ndriver.get(\"https://www.whatismyip.com/\")\nget_location()\n\n`\n```\nAttempt 3:\n```\n`from selenium import webdriver\nfrom selenium.webdriver.chrome.options import Options\n\nchrome_options = Options()\nchrome_options.add_argument(\"--headless\")\nchrome_options.add_argument(\"--no-sandbox\")\nchrome_options.add_argument(\"--window-size=1920x1080\")\nchrome_options.add_argument(\"--ignore-certificate-errors\")\n\nmyproxy = '207.157.25.44:80'\nchrome_options.add_argument(\"--proxy-server=http://%s\" % myproxy)\n\ndriver = webdriver.Chrome(options=chrome_options,\n    executable_path=\"/user/sebastien/chromedriver\")\ndriver.get(\"https://www.whatismyip.com/\")\nget_location()\n`\n```\nNone of them would reach the website with the desired IP.\nAgain, this issue is happening when running the code on GCP Compute Engine, Canonical, Ubuntu, 16.04 LTS, amd64 xenial.\nBelow the function to test the IP:\n```\n`import json\nfrom urllib.request import urlopen\n\ndef get_location(ip=False):\n    if ip:\n        html = urlopen(f\"http://ipinfo.io/{str(ip).split(':')[0]}/json\")\n    else:\n        html = urlopen(\"http://ipinfo.io/json\")\n\n    data = json.loads(html.read().decode('utf-8'))\n    IP = data['ip']\n    org = data['org']\n    city = data['city']\n    country = data['country']\n    region = data['region']\n\n    print('IP detail')\n    print('IP : {4} \\nRegion : {1} \\nCountry : {2} \\nCity : {3} \\nOrg : {0}'.format(org, region, country, city, IP))\n\n`\n```\nThanks for reading !",
      "solution": "I don't think the issue that you're having is related to your code implementation.  I'm sure that the issue that you're having is related to your usage of a free proxy.  These type of proxies\nare notorious for having connections issues, such as timeouts related to latency. Plus these sites can also be intermittent, which means that they can go down at anytime.  And sometimes these sites are being abused, so they can get blocked.\nYour proxy is `207.157.25.44:80`, which is shown in the image below.\n\nWhen I tested this code:\n```\n`from selenium import webdriver\nfrom selenium.webdriver.chrome.options import Options\n\nproxy_server = '207.157.25.44:80'\n\nchrome_options = Options()\nchrome_options.add_argument(\"--disable-infobars\")\nchrome_options.add_argument(\"start-maximized\")\nchrome_options.add_argument(\"--disable-extensions\")\nchrome_options.add_argument(\"--disable-popup-blocking\")\nchrome_options.add_argument('--proxy-server=%s' % proxy_server)\n\n# disable the banner \"Chrome is being controlled by automated test software\"\nchrome_options.add_experimental_option(\"useAutomationExtension\", False)\nchrome_options.add_experimental_option(\"excludeSwitches\", ['enable-automation'])\n\ndriver = webdriver.Chrome('/usr/local/bin/chromedriver', options=chrome_options)\n\ndriver.get('https://www.whatismyip.com/')\n`\n```\nThe Chrome browser opens, but it does not display any content.\n\nIf I check the address `207.157.25.44:80` via an online proxy checker service, I get mixed results.\nThis image below shows that the proxy is not responding to any query types (HTTP, HTTPS, SOCKS4, SOCKS5).\n\nWhen I do the same check 5 minutes later the proxy is up on HTTP, but has latency issues.\n\nIf I selected another proxy from the free proxy website:\n```\n`from selenium import webdriver\nfrom selenium.webdriver.chrome.options import Options\nfrom selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n\nproxy_server = '47.184.133.79:3128'\n\nchrome_options = Options()\nchrome_options.add_argument(\"--disable-infobars\")\nchrome_options.add_argument(\"start-maximized\")\nchrome_options.add_argument(\"--disable-extensions\")\nchrome_options.add_argument(\"--disable-popup-blocking\")\nchrome_options.add_argument('--proxy-server=%s' % proxy_server)\n\n# disable the banner \"Chrome is being controlled by automated test software\"\nchrome_options.add_experimental_option(\"useAutomationExtension\", False)\nchrome_options.add_experimental_option(\"excludeSwitches\", ['enable-automation'])\n\ndriver = webdriver.Chrome('/usr/local/bin/chromedriver', options=chrome_options)\n\ndriver.get('https://www.whatismyip.com/')\n\n`\n```\nI get a `CloudFlare` challenge page when connecting to the website `whatismyip.`\n\nBut if I try the same proxy on the website `nordvpn.com/what-is-my-ip` I get the proxy's IP address.\n\nI would highly recommend testing any free proxy IP address multiple times to see if the address has any types of issues.  Additionally, you need to add some error handling in your code to catch issues when a proxy goes offline, because they can drop at anytime.\nIf you need to use a proxy, I would strongly recommend using a commercial proxy service, because they are more reliable than the free proxy services.\n\noxylabs.io\nbright data",
      "question_score": 7,
      "answer_score": 6,
      "created_at": "2021-07-05T18:09:41",
      "url": "https://stackoverflow.com/questions/68259141/using-proxy-with-chromedriver-within-google-cloud-engine"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 69605402,
      "title": "Firebase phone auth failing on app already in production with status code 17028",
      "problem": "We are using firebase phone authentication to authenticate our users. We've been using it for over a year now.\nToday at 13:00 GMT, new users are receiving 17028 An invalid safety_net_token was passed after entering their phone number.\nI tried to verify each of the causes I found on the firebase docs:\n\nVerifying the SHA1 and SHA256 on the firebase console (We use the fingerprints that are on the play store console)\nVerifying the package name\n\nThe last app update was on October 1st and since then thousands of users created an account with Firebase Auth and there were no configuration changes on the firebase console.\nUpon looking at other StackOverflow questions, the error we are getting is not the same as 17028 A safety_net_token was passed, but no matching SHA-256 was registered in the Firebase console. Please make sure that this application's packageName/SHA256 pair is registered in the Firebase Console Even though it is the same error code, in our case, it says an invalid token was passed.\nWe are using firebase with react-native-firebase module.\nEDIT: After disabling Android device verification API from Google Cloud Console the verification is now working but with no device verification. (Users have to verify they're not robots with a CAPTCHA).",
      "solution": "Update: It is working fine now, and the outage has been fixed.\n\nIt looks to be a firebase internal issue, and the only solution now is to disable \"Android Device Verification\" and all your users will see the Recaptcha page.\ncheck\nhttps://status.firebase.google.com/\nhttps://status.firebase.google.com/incidents/TYeQBVB4kkzyk2kE8vbP",
      "question_score": 7,
      "answer_score": 5,
      "created_at": "2021-10-17T16:38:42",
      "url": "https://stackoverflow.com/questions/69605402/firebase-phone-auth-failing-on-app-already-in-production-with-status-code-17028"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 73852611,
      "title": "Serverless VPC access connector is in a bad shape",
      "problem": "Our project is using a Serverless VPC access connector to allow access to DB over private IP from cloud functions and cloud runs. It was working flawlessly for a few months, but today I tried to deploy one of the functions that use such a connector and I got the message:\n\nVPC connector\nprojects/xxxx/locations/us-central1/connectors/vpc-connector is not\nready yet or does not exist. Please visit\nhttps://cloud.google.com/functions/docs/troubleshooting for in-depth\ntroubleshooting documentation.\n\nI went to the Serverless VPC access view and found out that indeed the connector has a red marking on it. When I hover on it it says\n\nConnector is in a bad state, manual deletion recommended\n\nbut I don't know for what reason, Link to logs doesn't show anything for the past 3 months.\n\nI tried to google about the such error but without success.\nI also tried to search through logs but also didn't find anything relevant.\nI'm looking for any hints:\n\nWhy it happened?\nHow to fix it? I don't want to recreate the connector, it is related to many functions, and cloud runs",
      "solution": "As the issue was blocking us from the deployment of cloud functions I was forced to recreate the connector.\nBut this time API returned an error:\n```\n`Error: Error waiting to create Connector: Error waiting for Creating Connector: Error code 7, message: Operation failed: Google APIs Service Agent (@cloudservices.gserviceaccount.com) needs editor role in the project.\n`\n```\nAfter adding such permission old connector started to work again...\nBefore there was no such requirement, but it changed in meantime.\nSpooky, one time something works other not.",
      "question_score": 7,
      "answer_score": 4,
      "created_at": "2022-09-26T12:09:15",
      "url": "https://stackoverflow.com/questions/73852611/serverless-vpc-access-connector-is-in-a-bad-shape"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 69538854,
      "title": "Authorized Javascript Origins wildcard alternative",
      "problem": "We've recently introduced Google Single Sign-on to our platform. It works well, except for one issue. All our development branches are automatically assigned a url that looks something like `https://{branch-name}.ourdomain.com`. As of right now, we have to manually add the authorized origin for each environment, which is not scalable for us.\nIs there a solution, such as an API we can use in our deployment process, that doesn't require us to authorize from the same origin for all our branches and doing a redirect dance? The ideal solution would be the wildcard solution where we could add `https://*.ourdomain.com` as an authorized origin, but that doesn't seem to be allowed in the Google Cloud Platform.",
      "solution": "There is no API for adding authorized origin dynamically in the Google console; it must be done manually. The OAuth engineering team is still evaluating the best way an API could be deployed as this carries many security risks that need to be properly assessed. JavaScript origins cannot contain certain characters including: Wildcard characters ('*') to ensure the security and privacy of accounts. You need to add the exact URIs the application is gonna use as JavaScript origins. Unfortunately, there is no good alternative workaround in regards to your use case, the only workaround is that you need to add each environment manually.\nNote : There are several feature requests like Can't update Google Cloud Javascript Origin domains via API for this, but unlikely that will be implemented soon.\nRefer Google API: Authorized JavaScript Origins for information.",
      "question_score": 7,
      "answer_score": 5,
      "created_at": "2021-10-12T12:21:38",
      "url": "https://stackoverflow.com/questions/69538854/authorized-javascript-origins-wildcard-alternative"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 69209115,
      "title": "Cloud Run: 429: The request was aborted because there was no available instance",
      "problem": "We (as a company) experience large spikes every day. We use Pub/Sub -> Cloud Run combination.\nThe issue we experience is that when high traffic hits, Pub/Sub tries to push messages to Cloud/Run all at the same time without any flow control. The result?\n429: The request was aborted because there was no available instance.\nAlthough this is marked as a warning, every 4xx HTTP response results in the message retry delivery.\nMessages, therefore, come back to the queue and wait. If a message repeats this process and the instances are still taken, Cloud Run returns 429 again, and the message is sent back to the queue. This process repeats x times (depends on what value we set in Maximum delivery attempts). After that, the message goes to the dead-letter queue.\nWe want to avoid this and ideally don't get any 429, so the message won't travel back and forth, and it won't end up in the dead-letter subscription because it is not one of the application errors we want to keep there, but rather a warning caused by Pub/Sub not controlling the flow and coordinating with Cloud Run.\nNeither Pub/Sub nor a push subscription (which is required to use for Cloud Run) have any flow control feature.\n\nIs there any way to control how many messages are sent to Cloud Run to avoid getting the 429 response? And also, why does Pub/Sub even try to deliver when it is obvious that Cloud Run hit the limit of instances. The best would be to keep the messages in a queue until the instances free up.\n\nMost of the answers would probably suggest increasing the limit of instances. We already set 1000. This would not be scalable because even if we set the limit to 1500 and a huge spike comes, we would pass the limit and get the 429 messages again.\nThe only option I can think of is some flow control. So far, we have read about Cloud Tasks, but we are not sure if this can help us. Ideally, we don't want to introduce any new service, but if necessary, we will do.\nThank you for all your tips and time! :)",
      "solution": "Here are some options:\n\nUse a 1st gen event-driven Cloud Function https://stackoverflow.com/a/75763414/10720618\nUse Cloud Tasks to rate limit. But then you don't get dead lettering\nDisable dead lettering and have pubsub continuously attempt delivery. May want to set an end condition to avoid infinite retry loops. Also need to setup some alerting to ensure messages don't expire (if the service cannot keep up with the load).\nHandle in application code by tracking attempt count for each messageId (IE using Redis), and publishing to a dead letter topic when attempt count has exceeded a threshold. This is easy to abstract, doesn't introduce a new service, and checks all the boxes but definitely adds overhead of state management.",
      "question_score": 7,
      "answer_score": 4,
      "created_at": "2021-09-16T15:13:05",
      "url": "https://stackoverflow.com/questions/69209115/cloud-run-429-the-request-was-aborted-because-there-was-no-available-instance"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 65611063,
      "title": "Docker don&#39;t copy files in .gitignore",
      "problem": "In my project, I have to use `docker-in-docker` to upload from GitLab a Google CloudRun using CLoudBuild.\nI'm running into a strange error. I have a local version of the `.env`  file that I don't want to be excluded from git revisioning. So I put it in the `.gitignore` file.\nBut in the CloudBuild upload process, I create a file named `.env` using the variables set into CI/CD of GitLab. I use this `.gitlab-ci.yml`.\n```\n`image: docker:latest\n\nstages:\n    - deploy\n\ndeploy:\n    stage: deploy\n    image: google/cloud-sdk\n    services:\n        - docker:dind\n    script:\n        - echo CLOUDSQL_PASSWORD=$CLOUDSQL_PASSWORD >> app/.env\n        - echo CLOUDSQL_USERNAME=$CLOUDSQL_USERNAME >> app/.env\n        - echo CLOUDSQL_HOST=$CLOUDSQL_HOST >> app/.env\n        - echo MONGODB_PASSWORD=$MONGODB_PASSWORD >> app/.env\n        - echo MONGODB_USERNAME=$MONGODB_USERNAME >> app/.env\n        ...\n`\n```\nThe problem is that this file is not copied by docker to the new container and I can't access it in my Node.js app. It seems like docker is incorporating also the lines of `.gitignore` file and excluding the `.env` the GitLab CI has just created.\nI've tried to set up the `.dockerignore` file in a whitelist approach.\n```\n`*\n#white list\n!app/.env\n!app/index.js\n!app/package.json\n!app/service-account.json\n!app/yarn.lock\n`\n```\nand this is the `Dockerfile` being used.\n```\n`FROM node:14.12-alpine3.12\n\nENV NODE_ENV production\nENV GOOGLE_APPLICATION_CREDENTIALS service-account.json\n\nEXPOSE 8080\n\nWORKDIR /app\n\nCOPY app/ ./\n\nRUN yarn\n\nCMD [\"yarn\", \"start\"]\n`\n```",
      "solution": "The official docs don't say, but from personal experience, if you do not have a `.gcloudignore` file, then it defaults to follow the rules in your `.gitignore`\nThere is the option to disable this default behaviour globally:\n```\n`gcloud config set gcloudignore/enabled false\n`\n```\nbut that does not sound like a good option. Best is to explicitly add a .gcloudignore file:\n```\n`.gcloudignore\n.git\n.gitignore\n\n!app/.env\n`\n```\nThe last line would not make a difference in your case, but just including it to show how to explicitly include a missing file.",
      "question_score": 7,
      "answer_score": 6,
      "created_at": "2021-01-07T11:57:34",
      "url": "https://stackoverflow.com/questions/65611063/docker-dont-copy-files-in-gitignore"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-platform",
      "question_id": 66994050,
      "title": "How do I get root access to AI Platform notebook within GCP?",
      "problem": "I am logged as user `jupyter` in the terminal in my GCP AI Platform notebook instance.\nI want to just install a few things (cannot be installed by pip) and I am unable to ssh as root user using the gcloud command.\nIt would be great if someone could share a pointer of the right way to get permissions to perform these actions.\nHere is a snippet of an example package installation that is causing problems -\n```\n`(base)~/datascience$ sudo cp ./pip-bash-completion/pip /etc/bash_completion.d/\n\nWe trust you have received the usual lecture from the local System\nAdministrator. It usually boils down to these three things:\n\n    #1) Respect the privacy of others.\n    #2) Think before you type.\n    #3) With great power comes great responsibility.\n\n[sudo] password for jupyter: \n`\n```\nI am supposed to have `sudo` access based on the information I see -\n`Zone - us-central1-b\nEnvironment  - Python 3 (with Intel\u00ae MKL)\nMachine type - n1-standard-4 (4 vCPUs, 15 GB RAM)\nGPU - None\nBoot disk - 100 GB disk\nData disk - 100 GB disk\nBackup - Not specified\nPermission mode - Service account\nSudo access - Enabled\n`",
      "solution": "If you are in the Jupyter Terminal you should use `sudo`.\nSome IT admins restrict root access by using the `notebook-disable-root=true` metadata. You can verify this by accessing the Notebooks page in Cloud Console.\nExample of creating a new instance with root disabled:\n```\n`gcloud notebooks instances create $INSTANCE_NAME \\\n  --vm-image-project=deeplearning-platform-release \\\n  --vm-image-family=tf-latest-gpu \\\n  --metadata=\"notebook-disable-root=true,proxy-mode=mail,proxy-user-mail=user@company.com\" \\\n  --location=us-west1-a\n`\n```\nGoogle Cloud AI Platform Notebooks support SSH via OSLogin. https://cloud.google.com/compute/docs/oslogin/manage-oslogin-in-an-org\nyou need to SSH with your IAM account and then perform a sudo.",
      "question_score": 7,
      "answer_score": 1,
      "created_at": "2021-04-07T23:01:33",
      "url": "https://stackoverflow.com/questions/66994050/how-do-i-get-root-access-to-ai-platform-notebook-within-gcp"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 74233349,
      "title": "How do I install gke-gcloud-auth-plugin on a Mac M1 with zsh",
      "problem": "I try to install `gke-gcloud-auth-plugin` on a Mac M1 with zsh, following the gcloud docs.\nThe installation ran without issue and trying to re-run `gcloud components install gke-gcloud-auth-plugin` I get the `All components are up to date.` message.\nHowever, `gke-gcloud-auth-plugin --version` returns `zsh: command not found: gke-gcloud-auth-plugin`. `kubectl`, installed the same way, works properly.\nI tried to install `kubectl` using `brew`, with no more success.",
      "solution": "Not sure if it is the same on macOS. Can you try the following:\n`export USE_GKE_GCLOUD_AUTH_PLUGIN=True\n`\nThen reload the Cluster with\n`gcloud container clusters get-credentials clustername\n`\nGuess it is installed but just not used.\nMaybe you just need to add the directory where to find your `gke-gcloud-auth-plugin` file to your `PATH`.\nIs it working when you call it wirh absolute path?\n`path/to/gke-gcloud-auth-plugin --version\n`\nto find the file use the following command:\n```\n`sudo find / -name gke-gcloud-auth-plugin\n`\n```",
      "question_score": 50,
      "answer_score": 12,
      "created_at": "2022-10-28T11:34:15",
      "url": "https://stackoverflow.com/questions/74233349/how-do-i-install-gke-gcloud-auth-plugin-on-a-mac-m1-with-zsh"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 72274548,
      "title": "How to remove warning in kubectl with gcp auth plugin?",
      "problem": "When I run any kubectl command I get following WARNING:\n```\n`W0517 14:33:54.147340   46871 gcp.go:120] WARNING: the gcp auth plugin is deprecated in v1.22+, unavailable in v1.25+; use gcloud instead.\nTo learn more, consult https://cloud.google.com/blog/products/containers-kubernetes/kubectl-auth-changes-in-gke\n`\n```\nI have followed the instructions in the link several times but the WARNING keeps appearing making kubectl output uncomfortable to read.\nOS:\n```\n`cat /etc/lsb-release \nDISTRIB_ID=Ubuntu\nDISTRIB_RELEASE=22.04\nDISTRIB_CODENAME=jammy\nDISTRIB_DESCRIPTION=\"Ubuntu 22.04 LTS\"\n`\n```\nkubectl version:\n```\n`Client Version: v1.24.0\nKustomize Version: v4.5.4\n`\n```\ngke-gcloud-auth-plugin:\n```\n`Kubernetes v1.23.0-alpha+66064c62c6c23110c7a93faca5fba668018df732\n`\n```\ngcloud version:\n```\n`Google Cloud SDK 385.0.0\nalpha 2022.05.06\nbeta 2022.05.06\nbq 2.0.74\nbundled-python3-unix 3.9.12\ncore 2022.05.06\ngsutil 5.10\n`\n```\nI \"login\" with:\n```\n`gcloud init\n`\n```\nand then:\n```\n`gcloud container clusters get-credentials cluster_name --region my-region\n`\n```\nfinally:\n```\n`myyser@mymachine:/$ k get pods -n madeupns\nW0517 14:50:10.570103   50345 gcp.go:120] WARNING: the gcp auth plugin is deprecated in v1.22+, unavailable in v1.25+; use gcloud instead.\nTo learn more, consult https://cloud.google.com/blog/products/containers-kubernetes/kubectl-auth-changes-in-gke\nNo resources found in madeupns namespace.\n`\n```\nHow can I remove the WARNING or fix the problem?\nRemoving my `.kube/config` and re-running get-credentials didn't work.",
      "solution": "I fixed this problem by adding the correct export in `.bashrc`\n```\n`export USE_GKE_GCLOUD_AUTH_PLUGIN=True\n`\n```\nAfter sourcing `.bashrc` with `. ~/.bashrc` and reloading cluster config with:\n```\n`gcloud container clusters get-credentials clustername\n`\n```\nthe warning dissapeared:\n```\n`user@laptop:/$ k get svc -A\nNAMESPACE     NAME                   TYPE           CLUSTER-IP     EXTERNAL-IP    \nkube-system   default-http-backend   NodePort       10.10.13.157            \nkube-system   kube-dns               ClusterIP      10.10.0.10              \nkube-system   kube-dns-upstream      ClusterIP      10.10.13.92             \nkube-system   metrics-server         ClusterIP      10.10.2.191             \n`\n```",
      "question_score": 39,
      "answer_score": 48,
      "created_at": "2022-05-17T15:01:40",
      "url": "https://stackoverflow.com/questions/72274548/how-to-remove-warning-in-kubectl-with-gcp-auth-plugin"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 74106823,
      "title": "Working Poetry project with private dependencies inside Docker",
      "problem": "I have a Python library hosted in Google Cloud Platform Artifact Registry. Besides, I have a Python project, using Poetry, that depends on the library.\nThis is my project file `pyproject.toml`:\n`[tool.poetry]\nname = \"Test\"\nversion = \"0.0.1\"\ndescription = \"Test project.\"\nauthors = [\n    \"Me \"\n]\n\n[tool.poetry.dependencies]\npython = \">=3.8,=1.1.0\"]\nbuild-backend = \"poetry.core.masonry.api\"\n\n[[tool.poetry.source]]\nname = \"my-lib\"\nurl = \"https://us-east4-python.pkg.dev/my-gcp-project/my-lib/simple/\"\nsecondary = true\n\n`\nTo enable using my private repository, I installed gcloud CLI and authenticated with my credentials. So when I run this command, I see proper results, like this:\n`$ gcloud auth list\nACTIVE  ACCOUNT\n...\n*       @appspot.gserviceaccount.com\n...\n`\nAdditionally, I'm using Python keyring togheter with keyrings.google-artifactregistry-auth, as you can see in the project file.\nSo, with this setup, I can run `poetry install`, the dependency gets downloaded from my private artifact registry, using the authentication from GCP.\n\nThe issue comes when I try to apply the same principles inside a Docker container.\nI created a Docker file like this:\n```\n`# syntax = docker/dockerfile:1.3\nFROM python:3.9\n\n# Install Poetry\nRUN curl -sSL https://install.python-poetry.org | python3 -\nENV PATH \"${PATH}:/root/.local/bin\"\n\n# Install Google Cloud SDK CLI\nARG GCLOUD_VERSION=\"401.0.0-linux-x86_64\"\nRUN wget -q https://dl.google.com/dl/cloudsdk/channels/rapid/downloads/google-cloud-cli-${GCLOUD_VERSION}.tar.gz && \\\n    tar -xf google-cloud-cli-*.tar.gz && \\\n    ./google-cloud-sdk/install.sh --quiet && \\\n    rm google-cloud-cli-*.tar.gz\nENV PATH \"${PATH}:/google-cloud-sdk/bin\"\n\n# install Google Artifact Rrgistry keyring integration\nRUN pip install keyrings.google-artifactregistry-auth\nRUN --mount=type=secret,id=GOOGLE_APPLICATION_CREDENTIALS ${GOOGLE_APPLICATION_CREDENTIALS} gcloud auth activate-service-account --key-file=/run/secrets/GOOGLE_APPLICATION_CREDENTIALS\nRUN gcloud auth list\nRUN keyring --list-backends\n\nWORKDIR /app\n\n# copy Poetry project files and install dependencies\nCOPY ./.env* ./\nCOPY ./pyproject.toml ./poetry.lock* ./\nRUN poetry install\n\n# copy source files\nCOPY ./app /app/app\n\n# run the program\nCMD poetry run python -m app.main\n\n`\n```\nAs you can see, I injected the Google credentials file, following this documentation. This works. I used Docker BuildKit secrets, as exposed here (security concerns are not a matter of this question). So, when I try to build the image, I got an authentication error (`GOOGLE_APPLICATION_CREDENTIALS` is properly set pointing to a valid key file):\n`$ DOCKER_BUILDKIT=1 docker image build --secret id=GOOGLE_APPLICATION_CREDENTIALS,src=${GOOGLE_APPLICATION_CREDENTIALS} -t app-test .\n\n...\n#19 66.68 Source (my-lib): Authorization error accessing https://us-east4-python.pkg.dev/my-gcp-project/my-lib/simple/mylib/\n#19 68.21\n#19 68.21   RuntimeError\n#19 68.21\n#19 68.22   Unable to find installation candidates for mylib (0.1.1)\n...\n`\nIf I execute, line by line, all the commands in the Dockerfile, using the same Google credentials key file outside Docker, I got it working.\nI even tried to debug inside the image, not executing `poetry install`, nor `poetry run...` commands, and I saw this, if it helps to debug:\n`# gcloud auth list\n                  Credentialed Accounts\nACTIVE  ACCOUNT\n*       @appspot.gserviceaccount.com\n\n`\n`# keyring --list-backends\nkeyrings.gauth.GooglePythonAuth (priority: 9)\nkeyring.backends.chainer.ChainerBackend (priority: -1)\nkeyring.backends.fail.Keyring (priority: 0)\n`\nFinally, I even tried following this approach: Using Keyring on headless Linux systems in a Docker container, with the same results:\n`# apt update\n...\n# apt install -y gnome-keyring\n...\n# dbus-run-session -- sh\nGNOME_KEYRING_CONTROL=/root/.cache/keyring-MEY1T1\nSSH_AUTH_SOCK=/root/.cache/keyring-MEY1T1/ssh\n# poetry install\n...\n  \u2022 Installing mylib (0.1.1): Failed\n\n  RuntimeError\n\n  Unable to find installation candidates for mylib (0.1.1)\n\n  at ~/.local/share/pypoetry/venv/lib/python3.9/site-packages/poetry/installation/chooser.py:103 in choose_for\n       99\u2502\n      100\u2502             links.append(link)\n      101\u2502\n      102\u2502         if not links:\n    \u2192 103\u2502             raise RuntimeError(f\"Unable to find installation candidates for {package}\")\n      104\u2502\n      105\u2502         # Get the best link\n      106\u2502         chosen = max(links, key=lambda link: self._sort_key(package, link))\n      107\u2502\n...\n\n`\nI even tried following the advices of this other question. No success.\n`gcloud` CLI works inside the container, testing other commands. My guess is that the integration with Keyring is not working properly, but I don't know how to debug it.\nHow can I get my dependency resolved inside a Docker container?",
      "solution": "Finally, I found a solution that worked in my use case.\nThere are two main parts:\n\nInstalling keyrings.google-artifactregistry-auth as a Poetry plugin, using this command:\n\n```\n`poetry self add keyrings.google-artifactregistry-auth\n`\n```\n\nAuthenticating inside the container using a service account key file:\n\n```\n`gcloud auth activate-service-account --key-file=key.json\n`\n```\nIn my case, I use BuildKit secrets to handle it.\nThen, for instance, the Dockerfile would like this:\n```\n`FROM python:3.9\n\n# Install Poetry\nRUN curl -sSL https://install.python-poetry.org | python3 -\nENV PATH \"${PATH}:/root/.local/bin\"\n\n# install Google Artifact Registry tools for Python as a Poetry plugin\nRUN poetry self add keyrings.google-artifactregistry-auth\n\n# Install Google Cloud SDK CLI\nARG GCLOUD_VERSION=\"413.0.0-linux-x86_64\"\nRUN wget -q https://dl.google.com/dl/cloudsdk/channels/rapid/downloads/google-cloud-cli-${GCLOUD_VERSION}.tar.gz && \\\n    tar -xf google-cloud-cli-*.tar.gz && \\\n    ./google-cloud-sdk/install.sh --quiet && \\\n    rm google-cloud-cli-*.tar.gz\nENV PATH \"${PATH}:/google-cloud-sdk/bin\"\n\n# authenticate with gcloud using a BuildKit secret\nRUN --mount=type=secret,id=gac.json \\\n    gcloud auth activate-service-account --key-file=/run/secrets/gac.json\n\nCOPY ./pyproject.toml ./poetry.lock* /\nRUN poetry install\n\n# deauthenticate with gcloud once the dependencies are already installed to clean the image\nRUN gcloud auth revoke --all\n\nCOPY ./app /app\n\nWORKDIR /app\n\nCMD [\"whatever\", \"command\", \"you\", \"use\"]\n`\n```\nAnd the Docker build command, providing the secret:\n`DOCKER_BUILDKIT=1 docker image build \\\n        --secret id=gac.json,src=${GOOGLE_APPLICATION_CREDENTIALS} \\\n        -t ${YOUR_TAG} .\n`\nAnd with Docker Compose, a similar approach:\n`services:\n  yourapp:\n    build:\n      context: .\n      secrets:\n        - key.json\n    image: yourapp:yourtag\n    ...\n`\n```\n`COMPOSE_DOCKER_CLI_BUILD=1 DOCKER_BUILDKIT=1 docker compose up --build\n`\n```",
      "question_score": 12,
      "answer_score": 5,
      "created_at": "2022-10-18T08:52:44",
      "url": "https://stackoverflow.com/questions/74106823/working-poetry-project-with-private-dependencies-inside-docker"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 76387734,
      "title": "Error on gcloud artifact registry: INVALID_ARGUMENT: Maven config is not supported for format &quot;DOCKER&quot;",
      "problem": "I am trying to create an artifact using gcloud artifact registry and I ran the `gcloud init` command which was fine and then I ran the following command\n`(venv) PS C:\\Users\\abc\\Python VS code projects\\project-name\\server> gcloud artifacts repositories create repo-name --repository-format=docker --location=us-central1 --description=\"Docker repo description\"`\nin doing so I came across the following error\n`ERROR: (gcloud.artifacts.repositories.create) INVALID_ARGUMENT: Maven config is not supported for format \"DOCKER\"`\nI have been using the same list of gcloud commands for creating artifact registries and using them and many other apis on google cloud console for some time but I have not come across this error. I have tried googling it but no avail. Any help would be appreciated. Thanks. Also my Docker file:\n```\n`\n# run command to install all dependencies\nRUN pip install Flask gunicorn flask_wtf pyrebase4 firebase_admin httplib2==0.15.0 pycryptodome==3.10.1 \n# RUN pip install google-auth google-auth-oauthlib google-auth-httplib2 google-api-python-client\n\n# now to copy our sourcre code to a folder in the container\nCOPY src/ app/\n# and set this container folder to working directory\nWORKDIR /app\n# here set environment variable for a port which we set in app.run command in app.py\nENV PORT 8080\n# run a command for gunicorn where we bind the ports, \n# set workers to 1, \n# set threads to 8,\n# bind the app \nCMD exec gunicorn --bind :$PORT --workers 1 --threads 8 app:app```\n`\n```",
      "solution": "I found the solution to my problem here on github. It does not say what the reason for the problem was but the solution was to update gcloud components.\nI updated my Cloud SDK CLI by running following command in administrator mode.\n```\n`gcloud components update\n`\n```",
      "question_score": 8,
      "answer_score": 12,
      "created_at": "2023-06-02T08:43:32",
      "url": "https://stackoverflow.com/questions/76387734/error-on-gcloud-artifact-registry-invalid-argument-maven-config-is-not-support"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 66901497,
      "title": "Including a subfolder from an excluded folder in .gcloudignore",
      "problem": "I'm trying to deploy a Node project with a Dockerfile to Google Cloud Run with the `gcloud beta run deploy` command.\nLong story short, I would like to copy my local `node_modules/my-module` after running `RUN npm install` in the Dockerfile:\n```\n`COPY node_modules/my-module /app/node_modules/my-module/\n`\n```\n(I only do this while in development, to avoid committing and pushing every change from `my-module` for testing).\nUnfortunately, Docker cannot copy this directory since, apparently, `node_modules` is not uploaded to Cloud Build by default.\nSo I created this `.gcloudignore` file to override the default:\n```\n`.gcloudignore\n.git\n.gitignore\n\nnode_modules/\n!node_modules/my-module/\n`\n```\nI've tried a lot of other syntaxes but none allowed me to exclude `node_modules` while including `node_modules/my-module`.\nHowever, I can include the whole `node_modules` directory by omitting it from the `.gcloudignore` file, but this obviously takes forever to upload.\nDo you know how I could upload my local module to Cloud Build?",
      "solution": "After reading this Gist's comments, I realized you have to also include the parent directories, like so:\n```\n`node_modules/**\n\n!node_modules/\n!node_modules/my-module/**\n`\n```\nThis will exclude all subfolders from `node_modules` except `my-module` and its content (the `**` is important, otherwise only the empty folder would be included).",
      "question_score": 8,
      "answer_score": 9,
      "created_at": "2021-04-01T11:00:56",
      "url": "https://stackoverflow.com/questions/66901497/including-a-subfolder-from-an-excluded-folder-in-gcloudignore"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 72122744,
      "title": "Google Cloud Vertex AI with Golang: rpc error: code = Unimplemented desc = unexpected HTTP status code received from server: 404 (Not Found)",
      "problem": "I have a Vertex AI model deployed on an endpoint and want to do some prediction from my app in Golang.\nTo do this I create code inspired by this example : https://cloud.google.com/go/docs/reference/cloud.google.com/go/aiplatform/latest/apiv1?hl=en\n```\n`const file = \"MY_BASE64_IMAGE\"\n\nfunc main() {\n\n    ctx := context.Background()\n\n    c, err := aiplatform.NewPredictionClient(cox)\n    if err != nil {\n        log.Printf(\"QueryVertex NewPredictionClient - Err:%s\", err)\n    }\n    defer c.Close()\n\n    parameters, err := structpb.NewValue(map[string]interface{}{\n        \"confidenceThreshold\": 0.2,\n        \"maxPredictions\":      5,\n    })\n    if err != nil {\n        log.Printf(\"QueryVertex structpb.NewValue parameters - Err:%s\", err)\n    }\n\n    instance, err := structpb.NewValue(map[string]interface{}{\n        \"content\": file,\n    })\n    if err != nil {\n        log.Printf(\"QueryVertex structpb.NewValue instance - Err:%s\", err)\n    }\n\n    reqP := &aiplatformpb.PredictRequest{\n        Endpoint:   \"projects/PROJECT_ID/locations/LOCATION_ID/endpoints/ENDPOINT_ID\",\n        Instances:  []*structpb.Value{instance},\n        Parameters: parameters,\n    }\n\n    resp, err := c.Predict(cox, reqP)\n    if err != nil {\n        log.Printf(\"QueryVertex Predict - Err:%s\", err)\n    }\n\n    log.Printf(\"QueryVertex Res:%+v\", resp)\n}\n`\n```\nI put the path to my service account JSON file on GOOGLE_APPLICATION_CREDENTIALS environment variable.\nBut when I run my test app I obtain this error message:\n```\n`QueryVertex Predict - Err:rpc error: code = Unimplemented desc = unexpected HTTP status code received from server: 404 (Not Found); transport: received unexpected content-type \"text/html; charset=UTF-8\"\nQueryVertex Res:\n`\n```",
      "solution": "As @DazWilkin suggested, configure the client option to specify the specific regional endpoint with a port 443:\n```\n`option.WithEndpoint(\"-aiplatform.googleapis.com:443\")\n`\n```\nTry like below:\n```\n`func main() {\n \n   ctx := context.Background()\n   c, err := aiplatform.NewPredictionClient(\n       ctx,\n       option.WithEndpoint(\"-aiplatform.googleapis.com:443\"),\n   )\n   if err != nil {\n       log.Printf(\"QueryVertex NewPredictionClient - Err:%s\", err)\n   }\n   defer c.Close()\n       .\n       .\n`\n```",
      "question_score": 7,
      "answer_score": 9,
      "created_at": "2022-05-05T08:20:16",
      "url": "https://stackoverflow.com/questions/72122744/google-cloud-vertex-ai-with-golang-rpc-error-code-unimplemented-desc-unexp"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 73661666,
      "title": "dev_appserver.py BadArgumentError: app must not be empty",
      "problem": "Hey all,\nFor context: I had this `dev_appserver` setup working late last year in 2021, and upon trying to set it up again, I'm getting odd errors.\n`BadArgumentError: app must not be empty.`\nI've solved quite a lot of errors up to this point, and this is where I'm at:\n\nJDK 1.11+ installed (for `Cloud Datastore Emulator`)\nGolang 1.15+ installed (for `gops` & `dev_appserver.py - go build`)\nGcloud Components:\n\nI run my `dev_appserver` like this:\n`export DATASTORE_DATASET=dev8celbux\nexport DATASTORE_PROJECT_ID=dev8celbux\nexport DATASTORE_USE_PROJECT_ID_AS_APP_ID=true\n`\n`dev_appserver.py --enable_console --admin_port=8001 --port=8081 --go_debugging=true --support_datastore_emulator=true --datastore_path=./datastore/local_db.bin setuptables-app.yaml\n\nINFO     2022-09-09 13:26:30,233 devappserver2.py:317] Skipping SDK update check.\nINFO     2022-09-09 13:26:30,250 datastore_emulator.py:156] Starting Cloud Datastore emulator at: http://localhost:58946\nINFO     2022-09-09 13:26:32,381 datastore_emulator.py:162] Cloud Datastore emulator responded after 2.131000 seconds\nINFO     2022-09-09 13:26:32,381 :384] Starting API server at: http://localhost:59078\nINFO     2022-09-09 13:26:32,384 :374] Starting gRPC API server at: http://localhost:59079\nINFO     2022-09-09 13:26:32,394 instance_factory.py:184] Building with dependencies from go.mod.\nINFO     2022-09-09 13:26:32,397 dispatcher.py:280] Starting module \"setuptables\" running at: http://localhost:8081\nINFO     2022-09-09 13:26:32,397 admin_server.py:70] Starting admin server at: http://localhost:8001\nWARNING  2022-09-09 13:26:32,398 devappserver2.py:414] No default module found. Ignoring.\n2022/09/09 13:26:35 STARTING\nINFO     2022-09-09 13:26:37,220 instance.py:294] Instance PID: 9656\n`\nThis error appears when I try & view the contents within the local datastore at `localhost:8001/datastore`.\n`Traceback (most recent call last):\n  File \"C:\\Users\\user\\AppData\\Local\\Google\\Cloud SDK\\google-cloud-sdk\\platform\\google_appengine\\lib\\webapp2\\webapp2\\__init__.py\", line 1526, in __call__\n    rv = self.handle_exception(request, response, e)\n  File \"C:\\Users\\user\\AppData\\Local\\Google\\Cloud SDK\\google-cloud-sdk\\platform\\google_appengine\\lib\\webapp2\\webapp2\\__init__.py\", line 1520, in __call__\n    rv = self.router.dispatch(request, response)\n  File \"C:\\Users\\user\\AppData\\Local\\Google\\Cloud SDK\\google-cloud-sdk\\platform\\google_appengine\\lib\\webapp2\\webapp2\\__init__.py\", line 1270, in default_dispatcher\n    return route.handler_adapter(request, response)\n  File \"C:\\Users\\user\\AppData\\Local\\Google\\Cloud SDK\\google-cloud-sdk\\platform\\google_appengine\\lib\\webapp2\\webapp2\\__init__.py\", line 1094, in __call__\n    return handler.dispatch()\n  File \"C:\\Users\\user\\AppData\\Local\\Google\\Cloud SDK\\google-cloud-sdk\\platform\\google_appengine\\google\\appengine\\tools\\devappserver2\\admin\\admin_request_handler.py\", line 88, in dispatch\n    super(AdminRequestHandler, self).dispatch()\n  File \"C:\\Users\\user\\AppData\\Local\\Google\\Cloud SDK\\google-cloud-sdk\\platform\\google_appengine\\lib\\webapp2\\webapp2\\__init__.py\", line 588, in dispatch\n    return self.handle_exception(e, self.app.debug)\n  File \"C:\\Users\\user\\AppData\\Local\\Google\\Cloud SDK\\google-cloud-sdk\\platform\\google_appengine\\lib\\webapp2\\webapp2\\__init__.py\", line 586, in dispatch\n    return method(*args, **kwargs)\n  File \"C:\\Users\\user\\AppData\\Local\\Google\\Cloud SDK\\google-cloud-sdk\\platform\\google_appengine\\google\\appengine\\tools\\devappserver2\\admin\\datastore_viewer.py\", line 661, in get\n    kinds = self._get_kinds(namespace)\n  File \"C:\\Users\\user\\AppData\\Local\\Google\\Cloud SDK\\google-cloud-sdk\\platform\\google_appengine\\google\\appengine\\tools\\devappserver2\\admin\\datastore_viewer.py\", line 597, in _get_kinds\n    return sorted([x.kind_name for x in q.run()])\n  File \"C:\\Users\\user\\AppData\\Local\\Google\\Cloud SDK\\google-cloud-sdk\\platform\\google_appengine\\google\\appengine\\ext\\db\\__init__.py\", line 2077, in run\n    raw_query = self._get_query()\n  File \"C:\\Users\\user\\AppData\\Local\\Google\\Cloud SDK\\google-cloud-sdk\\platform\\google_appengine\\google\\appengine\\ext\\db\\__init__.py\", line 2482, in _get_query\n    _app=self._app)\n  File \"C:\\Users\\user\\AppData\\Local\\Google\\Cloud SDK\\google-cloud-sdk\\platform\\google_appengine\\google\\appengine\\api\\datastore.py\", line 1371, in __init__\n    self.__app = datastore_types.ResolveAppId(_app)\n  File \"C:\\Users\\user\\AppData\\Local\\Google\\Cloud SDK\\google-cloud-sdk\\platform\\google_appengine\\google\\appengine\\api\\datastore_types.py\", line 238, in ResolveAppId\n    ValidateString(app, 'app', datastore_errors.BadArgumentError)\n  File \"C:\\Users\\user\\AppData\\Local\\Google\\Cloud SDK\\google-cloud-sdk\\platform\\google_appengine\\google\\appengine\\api\\datastore_types.py\", line 186, in ValidateString\n    raise exception('%s must not be empty.' % name)\nBadArgumentError: app must not be empty.\n`\nI figured out that it is somewhat related to the `APPLICATION_ID` environment variable being missing. Upon setting it, I am able to view the Database page. HOWEVER. When getting no errors writing my data to the emulator (line by line debugged to confirm & `local_db.bin` is created), upon looking at the data, nothing is there. I successfully write 15 entities from the code's point of view. However none appear on the admin page. I think it's due to the manual set of the `APPLICATION_ID` as I did not do this before. Perhaps should be automatic somehow. Was thinking that this environment variable could maybe do that: `export DATASTORE_USE_PROJECT_ID_AS_APP_ID=true` but doesn't seem to change anything.\nBefore calling creation of entities:\n\nAfter calling creation of entities:\n\nI write the data like this, no doubt this works correctly.\n`ctx, err := appengine.Namespace(appengine.BackgroundContext(), \"celbux101\")\n...\nuserKeyOut, err := datastore.Put(ctx, userKey, &F1_4{...})\n`\nAlso, looked in both the `default` & the designated namespace (`celbux101`):\n\nSuper stumped. :( Help appreciated!\nI really think it may somehow be related to `APPLICATION_ID`",
      "solution": "Yes!\n... I managed to come to a solution! As suspected, the data was getting written correctly, as confirmed by the line-by-line debug & the creation of the `local_db.bin`. The issue is that the `dev_appserver`'s UI is not able to show the database entities due to the incorrect or missing `APPLICATION_ID`, as deducted.\nI figured out that the `dev_appserver`'s UI uses both the `APPLICATION_ID` & `namespace` to determine where to look for your entities. Also, the `dev_appserver` has it's own default `APPLICATION_ID`.\nSolution\nThe fix is to export this environment variable BEFORE running your `dev_appserver.py`.\n\n`export APPLICATION_ID=dev~None`\n\nThis magic export allows everything work as expected. You can view the `APPLICATION_ID` that the UI is trying to use on the top-left of the interface.\nEDIT: I just came back to running this on a new computer, and want to add this for future reference:\nIf you are getting `IOError: emulator did not respond within 10s`\nINSTALL Python27 & add to your path! (alongside your bundled python)",
      "question_score": 6,
      "answer_score": 17,
      "created_at": "2022-09-09T13:45:54",
      "url": "https://stackoverflow.com/questions/73661666/dev-appserver-py-badargumenterror-app-must-not-be-empty"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 75032978,
      "title": "GCloud auth login: Ports 8085 and 8184 possible blocked",
      "problem": "I have installed and uninstalled and reinstalled GCloud on MACOS Monterey (Chipset M1) and I'm facing the next situation: When I run in Terminal gcloud auth login, it displays the next message:\n```\n`WARNING: Failed to start a local webserver listening on any port between 8085 and 8184. Please check your firewall settings or locally running programs that may be blocking or using those ports.\nWARNING: Defaulting to --no-browser mode.\nYou are authorizing gcloud CLI without access to a web browser. Please run the following command on a machine with a web browser and copy its output back here. Make sure the installed gcloud version is 372.0.0 or newer.\n`\n```\nI have tried in many ways to install: The last one was this:\n```\n`curl https://sdk.cloud.google.com | bash\n\nexec -l $SHELL #restart shell\n`\n```\nBut I still facing that message.\nAnybody couls help me with this?",
      "solution": "This happens because my Internet provider has blocked these ports. There will be to make some fixes to the router.\nPatch solution for this:\n```\n`gcloud auth login --no-launch-browser\n`\n```\nFollow the instructions given on Terminal",
      "question_score": 6,
      "answer_score": 11,
      "created_at": "2023-01-06T16:36:23",
      "url": "https://stackoverflow.com/questions/75032978/gcloud-auth-login-ports-8085-and-8184-possible-blocked"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 67523920,
      "title": "Orchestrating many instances from a local - Too frequent operations from the source resource",
      "problem": "I have a local linux machine of the following flavor:\n```\n`NAME=\"Ubuntu\"\nVERSION=\"20.04.2 LTS (Focal Fossa)\"\nID=ubuntu\nID_LIKE=debian\nPRETTY_NAME=\"Ubuntu 20.04.2 LTS\"\nVERSION_ID=\"20.04\"\nHOME_URL=\"https://www.ubuntu.com/\"\nSUPPORT_URL=\"https://help.ubuntu.com/\"\nBUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\nPRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\nVERSION_CODENAME=focal\nUBUNTU_CODENAME=focal\n`\n```\nOn that machine I have gcloud installed with the following version:\n```\n`$ gcloud -v\nGoogle Cloud SDK 334.0.0\nalpha 2021.03.26\nbeta 2021.03.26\nbq 2.0.66\ncore 2021.03.26\ngsutil 4.60\n`\n```\nOn that machine I have the following shell script:\n```\n`#!/bin/bash\n\nclient_name=$1\ninstance_name_arg=imageclient${client_name}\nz=us-east1-b\n\ngcloud beta compute --project=foo-123 instances create $instance_name_arg --zone=${z} --machine-type=g1-small --subnet=default --network-tier=PREMIUM --source-machine-image projects/foo-123/global/machineImages/envNameimage2\n\ngcloud compute instances start $instance_name_arg --zone=${z}\nsleep 10s\n\ngcloud compute ssh --zone=${z} usr@${instance_name_arg} --  '/home/usr/miniconda3/envs/minienvName/bin/python /home/usr/git/bitstamp/envName/client_run.py  --payload=client_packages_'${client_name}\n\ngcloud compute instances stop $instance_name_arg --zone=${z}\n`\n```\nAs I am scaling this project, I am needing to launch this task many times at exactly the same time but with different settings.\nAs I am launching more and more of these bash scripts, I am starting to get the following error message:\n```\n`ERROR: (gcloud.beta.compute.instances.create) Could not fetch resource:\n - Operation rate exceeded for resource 'projects/foo-123/global/machineImages/envNameimage2'. Too frequent operations from the source resource.\n`\n```\nHow do I work around this issue?\nAs my solution is architected, I have a \"one-machine to one-launch-setting\" solution and would ideally want to persist this.\nThere must be other, large-scale gcloud customers who may or may not need a large number of machines spawned in parallel.\nThank you very much",
      "solution": "As you are using the machine image `envNameimage2` for creating the new instance, this is seen as a snapshot of the disk.\nYou can snapshot your disks at most once every 10 minutes. If you want to issue a burst of requests to snapshot your disks, you can issue at most 6 requests in 60 minutes.\nReference:\n\nSnapshot frequency limits\nCreating disks from snapshots\nAnothers API rate limits\n\nA workaround could be to follow the rate limits, or to create an instance using an existing (available) disk with the `--disk` flag.\n\n--disk=name=clone-disk-1,device-name=clone-disk-1,mode=rw,boot=yes",
      "question_score": 6,
      "answer_score": 4,
      "created_at": "2021-05-13T19:52:52",
      "url": "https://stackoverflow.com/questions/67523920/orchestrating-many-instances-from-a-local-too-frequent-operations-from-the-sou"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 71102253,
      "title": "gcloud auth login throwing error: gcloud crashed (ConnectionError): HTTPSConnectionPool(host=&#39;oauth2.googleapis.com&#39;, port=443): Max retries exceeded",
      "problem": "`gcloud builds submit` failed for me yesterday.\nThen I tried `gcloud config set project`, that also failed.\nSo I thought login might have expired so tried `gcloud auth login`.\n\nIn all cases, it always throws following error:\n```\n`ERROR: gcloud crashed (ConnectionError): HTTPSConnectionPool(host='oauth2.googleapis.com', port=443): Max retries exceeded with url: /token (Caused by NewConnectionError(': Failed to establish a new connection: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond'))\n\nIf you would like to report this issue, please run the following command:\n  gcloud feedback\n\nTo check gcloud for common problems, please run the following command:\n  gcloud info --run-diagnostics\n`\n```\nOn running diagnostics as suggested, everything seems fine.\n```\n`>  gcloud info --run-diagnostics\nNetwork diagnostic detects and fixes local network connection issues.\nChecking network connection...done.\nReachability Check passed.\nNetwork diagnostic passed (1/1 checks passed).\n\nProperty diagnostic detects issues that may be caused by properties.\nChecking hidden properties...done.\nHidden Property Check passed.\nProperty diagnostic passed (1/1 checks passed).\n`\n```\nI have also tried with my other google account, threw same error. It was working fine 1 day before. Have tried all Stackoverflow solutions, nothing seems to work.",
      "solution": "This problem is with Airtel India ISP and not Google Cloud CLI. Use a VPN, and it will work.",
      "question_score": 5,
      "answer_score": 13,
      "created_at": "2022-02-13T16:44:35",
      "url": "https://stackoverflow.com/questions/71102253/gcloud-auth-login-throwing-error-gcloud-crashed-connectionerror-httpsconnect"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 78161775,
      "title": "Organisational Policy Permissions Google Cloud for Microsoft Migration",
      "problem": "I am attempting to shift a small business (5 email inboxes) over to Microsoft 365 business. I made sure to organise all the DNS prerequisites on both sides before performing the migration. During the automated process, while it completed no JSON file was created on the Google side. I attempted to download the API key for the service account, but  I get the error that the Service account key creation is disabled.\n\nTracing that, I found under the organisational policies that the service account key creation (iam.disableServiceAccountKeyCreation) was enforced. There is only one account with access to the cloud, policies etc. the account is supposed to have all organisational permissions, however checking on the cloud shell, I continue to get access errors, and neither the service account or the main account appear to have permissions to make any changes.\nI am brand new to this account/network/business side of IT so I am a bit unsure what the issue might be. I have attempted to make changes according to the documentation but I have had no success.\nLooking at the `gcloud organizations describe organizationurl.com` in the return there is no depicted `owner:`. using `gcloud projects describe projectnametmpz' I get a do not have permission to access projects error. Similarly for\n```\n`gcloud iam service-accounts keys create file.json \\\n--iam-account serviceaccountkey@projectid-tpmz.iam.gserviceaccount.com\n\nERROR: (gcloud.iam.service-accounts.keys.create) FAILED_PRECONDITION: Key creation is not allowed on this service account.\n`\n```\nIf anyone could give me some pointers on how to enable the correct permissions I would be immensely grateful.\nI have attempted to trace the problem on the Google end, and I have narrowed it down to being\nMicrosoft support could not help and for Google cloud support a subscription was needed, and all the documentation seems to be sending me in cirlces.",
      "solution": "I had the same problem after completing the EOP wizard for Workspace migration. Took me a couple of hours to figure out 8-/\nWith a Workspace super admin, login to https://console.cloud.google.com.\nMake sure you're working in the root org.\n\nSelect IAM and admin.\nIn IAM on left-hand menu, edit permissions for organisation.\nAdd Organisation Policy Administrator and save.\nGo to Organisation policies in left-hand menu.\nSearch for 'Disable service account key creation'.\nEdit policy, set Enforcement to Off and save.\nChange workspace from root org to the project the 365 wizard created. Mine was called projectnamempij.\nIn IAM and admin, go to Service accounts in left-hand menu.\nIn the 3 dots menu besides the service account, select Manage keys.\nWhen in service account, Add key -> Create new key.\nThe json file is created and downloaded.\nCreate a new endpoint in Exchange Online and use the downloaded json\n\nHope this helps.",
      "question_score": 5,
      "answer_score": 9,
      "created_at": "2024-03-14T16:50:51",
      "url": "https://stackoverflow.com/questions/78161775/organisational-policy-permissions-google-cloud-for-microsoft-migration"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 68523074,
      "title": "Firebase functions build error when referencing multiple c# projects",
      "problem": "I was able to deploy my firebase c# function with no issues, however, when i referenced another c# project so that i could utilize another object i get error saying project doesn't exist.\nSo was able to deploy following with no problem:\n```\n`namespace CloudFunctions\n{\n    public class Login : IHttpFunction\n    {\n        public async Task HandleAsync(HttpContext context) {\n            await context.Response.WriteAsync(\"Hello World!\");\n        }\n    }\n}\n`\n```\nThis class lives in a project called CloudFunctions. I added a project reference to a project called Services so that i could call the login service and i get the following error:\n```\n`The referenced project '../Services/Services.csproj' does not exist\n`\n```\nThis is how i am deploying:\n```\n`gcloud functions deploy login --entry-point CloudFunctions.Login --runtime dotnet3 --trigger-http --allow-unauthenticated\n`\n```\nI can't imagine we would be required to have everything in one project in order to deploy?",
      "solution": "You need to make all of the projects available to the buildpack (i.e. deploy from the parent directory) but specify the project that contains the entry point as well, using the `GOOGLE_BUILDABLE` build-time environment variable.\nFrom the deployment documentation in the Functions Framework GitHub repo:\n\nReal world functions are often part of a larger application which will usually contain common code for data types and common business logic. If your function depends on another project via a local project reference (a `` element in your .csproj file), the source code for that project must also be included when deploying to Google Cloud Functions. Additionally, you need to specify which project contains the function you wish to deploy.\nIn a typical directory structure where all projects sit side-by-side within one top-level directory, this will mean you need to deploy from that top-level directory instead of from the function's project directory. You also need to use the `--set-build-env-vars` command line flag to specify the `GOOGLE_BUILDABLE` build-time environment variable. This tells the Google Cloud Functions deployment process which project to build and deploy. Note that the `GOOGLE_BUILDABLE` environment variable value is case-sensitive, and should match the directory and file names used.\nWhen deploying a function with multiple projects, it's important to make sure you have a suitable `.gcloudignore` file, so that you only upload the code that you want to. In particular, you should almost always include `bin/` and `obj/` in the `.gcloudignore` file so that you don't upload your locally-built binaries.\n\nSample deployment command line from the `examples` directory:\n`gcloud functions deploy multi-project \\\n  --runtime dotnet3 \\\n  --trigger-http \\\n  --entry-point=Google.Cloud.Functions.Examples.MultiProjectFunction.Function \\\n  --set-build-env-vars=GOOGLE_BUILDABLE=Google.Cloud.Functions.Examples.MultiProjectFunction\n`",
      "question_score": 5,
      "answer_score": 7,
      "created_at": "2021-07-26T00:58:20",
      "url": "https://stackoverflow.com/questions/68523074/firebase-functions-build-error-when-referencing-multiple-c-projects"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 69032860,
      "title": "Is there a way to impersonate a service account with the cloudsql_proxy executable?",
      "problem": "https://github.com/GoogleCloudPlatform/cloudsql-proxy\nI have found this is possible by setting impersonation system wide with this command: `gcloud config set auth/impersonate_service_account `.\nThe proxy exe seems to read the gcloud config.\nBut that is really clunky. I want to start the proxy and specify a specific user to impersonate without having to change it system wide. Also, I'm not resorting to generating non-expiring json keys- I want to use impersonation.\nMany Gcloud commands now support a specific switch for this, but the proxy exe does not. See this GitHub issue (with no response from google): https://github.com/GoogleCloudPlatform/cloudsql-proxy/issues/417\nCan I run `gcloud auth print-access-token --impersonate-service-account=` and set an env var the proxy exe will pick up or something?\nI can't find anything in the code except this mention of gcloud: https://github.com/GoogleCloudPlatform/cloudsql-proxy/blob/eca37935e7cd54efcd612c170e46f45c1d8e3556/cmd/cloud_sql_proxy/cloud_sql_proxy.go#L160\n\nWhen the gcloud command-line tool is installed on the local\nmachine, the\n\"active account\" is used for authentication. Run 'gcloud auth list' to see\nwhich accounts are installed on your local machine and\n'gcloud config list account' to view the active account.\n\nwhich is funny because when running `auth/impersonate_service_account` `gcloud config list account` doesn't say anything about it.\nIs there a way to have Gcloud do impersonation on a per session basis?\nEDIT: just to follow up, per the answer the `--token` totally works, so now I can run the proxy with IAM auth and impersonation a gsa simultaneously:\n```\n`# start proxy with IAM login as a GSA with a cloud sql service account setup\n./cloud_sql_proxy \\\n    -enable_iam_login \\\n    -dir=/var/run/cloudsql \\\n    -instances=project_id:region:instance_name \\\n    --token=$(gcloud auth print-access-token --impersonate-service-account='my-gsa@myco.iam.gserviceaccount.com')\n\n# now can auth through proxy as cloud sql federated user \npsql \"sslmode=disable \\\n    host='/var/run/cloudsql/project_id:region:instance_name' \\\n    user=my-gsa@myco.iam dbname=mydb\"\n`\n```",
      "solution": "Newer versions of Cloud SQL Proxy (\u2265 2.0) have explicit support for impersonation, so things now work a bit differently from the previous answers:\n```\n`cloud-sql-proxy --impersonate-service-account= \n`\n```\nor if you prefer:\n```\n`CSQL_PROXY_IMPERSONATE_SERVICE_ACCOUNT= cloud-sql-proxy \n`\n```",
      "question_score": 4,
      "answer_score": 4,
      "created_at": "2021-09-02T17:08:44",
      "url": "https://stackoverflow.com/questions/69032860/is-there-a-way-to-impersonate-a-service-account-with-the-cloudsql-proxy-executab"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 68263948,
      "title": "gcloud util installation crashed on Windows 10",
      "problem": "I want to install gcloud ssh component on Windows 10 Home in order to ssh GCE instances. But it failed showing the following message.\n```\n`Your current Cloud SDK version is: 347.0.0\nInstalling components from version: 347.0.0\n\nThese components will be installed.\n\nName: gcloud Beta Commands\nVersion: 2019.05.17\nSize: Here is the gcloud version installed.\n```\n`$ gcloud version\nGoogle Cloud SDK 347.0.0\nbq 2.0.69\ncore 2021.06.25\ngsutil 4.64\n`\n```",
      "solution": "i suggest you once uninstall cloud sdk and reinstall it again,sometimes most of the errors will get resolved through reinstalling .refer this documentation to uninstall.you can refer this documentation for installing it. and use any of this methods to ssh",
      "question_score": 4,
      "answer_score": 4,
      "created_at": "2021-07-06T04:51:33",
      "url": "https://stackoverflow.com/questions/68263948/gcloud-util-installation-crashed-on-windows-10"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 77678578,
      "title": "Google Cloud Run: &quot;FAILED_PRECONDITION: due to quota restrictions&quot; but real quota not reached yet",
      "problem": "I am encountering an issue while deploying my NodeJS server using Google Cloud CLI and would appreciate some guidance. Here is the process I followed:\n\nExecuted `npm run build` for my project, which completed successfully.\nSet up Google Cloud service account and authenticated it.\nConfigured Docker with Google Cloud using `gcloud auth configure-docker`.\nSet the Google Cloud project ID and the region for Cloud Run.\nTried deploying my service to Cloud Run using `gcloud run deploy`.\n\nHowever, the deployment fails with the following error:\n```\n`ERROR: (gcloud.run.deploy) FAILED_PRECONDITION: failed precondition: due to quota restrictions, cannot run builds in this region. Please contact support\n`\n```\nHere are some details about my setup:\n\nNodeJS version: 18.12.0\nGoogle Cloud CLI version: latest 457.0.0\nRegion: `europe-west3`\n\nI suspect the issue might be related to quota restrictions in the `europe-west3` region, but I'm unsure how to confirm or resolve this. I have checked my account quota restrictions and it's okay, haven't reached it yet. Has anyone else faced a similar issue, or does anyone know how to work around?",
      "solution": "Update (25/01/24)\nA few users have upgraded to v460.0.0, and confirmed that this issue is now fixed.\nProblem\nOn the Google Cloud CLI version 456.0.0 it was added the following:\n\nNetwork Security\n\nAdd `--billing-project` required flag to `gcloud network-security firewall-endpoints create` command to provide Google Cloud project ID\nfor API enablement check, quota, and endpoint uptime billing.\nAdd `--update-billing-project` flag to `gcloud network-security firewall-endpoints update` command to update the Google Cloud project\nused for API enablement check, quota, and endpoint uptime billing. So\nit might be related.\n\nAnd the --billing-account command seems like a beta feature:\n\n(BETA) This command sets or updates the billing account associated\nwith a project.\n--billing-project=BILLING_PROJECTThe Google Cloud project that will be charged quota for operations performed in gcloud. If you need to\noperate on one project, but need quota against a different project,\nyou can use this flag to specify the billing project. If both\nbilling/quota_project and --billing-project are specified,\n--billing-project takes precedence. Run $ gcloud config set --help to see more information about billing/quota_project.\n\nSo I would recommend to set this value to your correct billing project, but even so, it feels like something is wrong from their side.\nFor more info: https://cloud.google.com/sdk/gcloud/reference/beta/billing/projects/link\nSolution\nI simply downgraded to 455.0.0 again and it deployed without a problem.\nHow to downgrade?\nIf you're deploying via GitHub actions:\n```\n` - name: Cloud run deploy\n    uses: \"google-github-actions/deploy-cloudrun@v2\"\n    with:\n      ...\n      gcloud_version: \"455.0.0\"\n`\n```\nWith gcloud CLI:\n```\n`gcloud components update --version=445.0.0\n`\n```\nWith Gitlab deploy (.gitlab-ci.yml):\n```\n`image: google/cloud-sdk:455.0.0\n`\n```",
      "question_score": 4,
      "answer_score": 5,
      "created_at": "2023-12-18T11:53:44",
      "url": "https://stackoverflow.com/questions/77678578/google-cloud-run-failed-precondition-due-to-quota-restrictions-but-real-quot"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 77520719,
      "title": "How to pass secure parameters to onSchedule firebase function?",
      "problem": "I am writing a firebase function using the `onSchedule` handlers from the firebase sdk.\nThe thing is that the function I am writing requires the use of sensitive information, and I don't want this into the codebase.\nLooking at the firebase functions documentation for Configuring Your Environment, I need to be able to use the `runWith` parameters from the `v2` firebase functions api, because those environment / parametrized variables needs to be bound to the function that is going to use them.\nIt seems the `onSchedule` handler cannot be used when using the `runWith` parameter from `firebase-functions`, I can only use the `https`, `pubsub` etc... handlers with it.\nIs this not yet supported or is there another way to achieve the same thing?\nThanks.",
      "solution": "You can create a `.env` file and put all of your sensitive information there. Then access the environment variables as shown below.\n```\n`const { onSchedule } = require(\"firebase-functions/v2/scheduler\");\n\nconst PRIVATE_CREDENTIAL = process.env.PRIVATE_CREDENTIAL\n\nexports.scheduledFunction = onSchedule(\"5 11 * * *\", async (event) => {\n// ...\n});\n`\n```\nor if you prefer using secrets:\n```\n`const { onSchedule } = require(\"firebase-functions/v2/scheduler\");\nconst { defineSecret } = require('firebase-functions/params');\nconst ApiKey = defineSecret('API_KEY');\n\nexports.schedule = onSchedule({secrets:[ApiKey], schedule:\"5 11 * * *\" },\nasync (event) => {\n// ...\n})\n`\n```\nCheck out the documentation for further information.",
      "question_score": 4,
      "answer_score": 6,
      "created_at": "2023-11-21T07:26:26",
      "url": "https://stackoverflow.com/questions/77520719/how-to-pass-secure-parameters-to-onschedule-firebase-function"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 75129987,
      "title": "GCP - User ... does not have permission to access projects instance",
      "problem": "I was using `gcloud` with a service account to try to figure out why my API Gateway endpoint didn't work when I ran into another problem. First I ran this `export GOOGLE_APPLICATION_CREDENTIALS=/path/to/credential/fils/PROJECTNAME-hash.json`. Then I ran `gcloud services list --available` and I got this in my terminal:\n```\n`ERROR: (gcloud.services.list) User [@.iam.gserviceaccount.com] does not have permission to access projects instance [] (or it may not exist): Permission denied to list services for consumer container [projects/]\nHelp Token: \n- '@type': type.googleapis.com/google.rpc.PreconditionFailure\n  violations:\n  - subject: ?error_code=110002&service=cloudresourcemanager.googleapis.com&permission=serviceusage.services.list&resource=projects/\n    type: googleapis.com\n- '@type': type.googleapis.com/google.rpc.ErrorInfo\n  domain: serviceusage.googleapis.com\n  metadata:\n    permission: serviceusage.services.list\n    resource: projects/\n    service: cloudresourcemanager.googleapis.com\n  reason: AUTH_PERMISSION_DENIED\n`\n```\nI believe I have the correct permissions enabled in my service account:\n\nSo why am I getting this error and how do I get `gcloud services list --available` to work with the selected service account?",
      "solution": "This problem seemed to stem from the fact that I needed to set the service account to have the role of `serviceusage.serviceUsageViewer`. In order to do that I need to run the `add-iam-policy-binding` command but this command needs to be run with an account that has account owner/editor permissions.\nStep 1 was to switch the account in gcloud to the master gmail account with which I signed up for GCP services.\nI set my gcloud \"account\" to my master Gmail account with `gcloud config set account `. Then I ran:\n```\n`gcloud projects add-iam-policy-binding  \\\n    --member \"serviceAccount:@.iam.gserviceaccount.com\" \\\n    --role \"roles/serviceusage.serviceUsageViewer\"\n`\n```\nThat command succeeded. I set the gcloud account back to the service account with `gcloud config set account ` and then ran `gcloud services list --available`. This command worked this time.",
      "question_score": 4,
      "answer_score": 6,
      "created_at": "2023-01-16T04:25:01",
      "url": "https://stackoverflow.com/questions/75129987/gcp-user-does-not-have-permission-to-access-projects-instance"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 74622670,
      "title": "BIGQUERY PERMISSION | What is bigquery.readsession can do to bigquery dataset?",
      "problem": "I do not understand about the `BigQuery Read Session User` permission. I wonder if I got assigned this role. Can I query the data set in the Bigquery via python SDK?\n\nI tried:\n```\n`from google.cloud import bigquery\nimport os\n\nos.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'Path/xxx.json'\nproject_id = 'Project_ID'\nclient = bigquery.Client()\nclient.query(`SQL`)\n`\n```\nGot:\n```\n`Forbidden: 403 Access Denied: Table <> User does not have permission to query table <>, or perhaps it does not exist in location <>.\n\nLocation: <>\nJob ID: <>\n`\n```\nTo be clear, I want to know what the `read session` means in Bigquery.",
      "solution": "When the `Storage Read API` is used, structured data is sent in a binary serialization format which allows parallelism. Storage Read API provides fast access to managed BigQuery storage using RPC protocol.For the usage of Storage Read API, `ReadSession` has to be created.\nThe ReadSession message  contains the information about  maximum number of streams, the snapshot time, the set of columns to return, and the predicate filter  which is provided to CreateReadSession RPC. A ReadSession response contains the set of Stream identifiers which is used by Storage API. The Stream identifier that is returned from the ReadSession response is used to read all the data from the table. For more information, you can check this documentation.",
      "question_score": 4,
      "answer_score": 5,
      "created_at": "2022-11-30T04:47:23",
      "url": "https://stackoverflow.com/questions/74622670/bigquery-permission-what-is-bigquery-readsession-can-do-to-bigquery-dataset"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 71076548,
      "title": "Google artifact regitsry NPM + github action",
      "problem": "I'm trying to publish a npm package on GAR (Google Artifact Registry) through github using `google-github-actions/auth@v0` and `google-artifactregistry-auth`\nFor the authentication to google from github here is what I did to use the Federation Workload Identity:\n`export PROJECT_ID=\"my-project-id\"\n\ngcloud iam service-accounts create \"gh-deploy-service-account\" --project \"${PROJECT_ID}\"\n\ngcloud iam workload-identity-pools create \"github-pool\" --project=\"${PROJECT_ID}\" --location=\"global\" --display-name=\"Github pool\"\n\ngcloud iam workload-identity-pools describe github-pool\" --project=\"${PROJECT_ID}\" --location=\"global\" --format=\"value(name)\"\n\nexport WORKLOAD_IDENTITY_POOL_ID=projects/my-custom-id-number/locations/global/workloadIdentityPools/github-pool\n\ngcloud iam workload-identity-pools providers create-oidc \"github-provider\" \\\n  --project=\"${PROJECT_ID}\" \\\n  --location=\"global\" \\\n  --workload-identity-pool=\"github-pool\" \\\n  --display-name=\"Github provider\" \\\n  --attribute-mapping=\"google.subject=assertion.sub,attribute.actor=assertion.actor,attribute.repository=assertion.repository\" \\\n  --issuer-uri=\"https://token.actions.githubusercontent.com\"\n\nexport REPO=\"@example/my-package\"\n\ngcloud iam service-accounts add-iam-policy-binding \"gh-deploy-service-account@${PROJECT_ID}.iam.gserviceaccount.com\" \\\n  --project=\"${PROJECT_ID}\" \\\n  --role=\"roles/iam.workloadIdentityUser\" \\\n  --member=\"principalSet://iam.googleapis.com/${WORKLOAD_IDENTITY_POOL_ID}/attribute.repository/${REPO}\"\n`\nThen I created my artifact repository on google :\n`gcloud artifacts repositories create npm-repository --repository-format=npm --location=asia-east2\n`\nHere is my github workflows :\n`name: Publish Package\non:\n  push:\n    branches:\n      - main\n\njobs:\n  publish:\n    timeout-minutes: 10\n    runs-on: ubuntu-latest\n    permissions:\n      contents: \"read\"\n      id-token: \"write\"\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v2\n\n      - uses: actions/setup-node@v2\n        with:\n          node-version: 16\n\n      - name: Install\n        run: npm ci\n\n      - id: \"auth\"\n        name: \"Authenticate to Google Cloud\"\n        uses: \"google-github-actions/auth@v0\"\n        with:\n          workload_identity_provider: ${{ secrets.WORKLOAD_IDENTITY_PROVIDER }}\n          service_account: ${{ secrets.SERVICE_ACCOUNT }}\n          create_credentials_file: true\n\n      - name: \"Set up Cloud SDK\"\n        uses: \"google-github-actions/setup-gcloud@v0\"\n\n      - name: Create .npmrc\n        run: |\n          cat  .npmrc\n            @example:registry=https://asia-east2-npm.pkg.dev/${{ secrets.GCP_PROJECT_ID }}/npm-repository/\n            //asia-east2-npm.pkg.dev/${{ secrets.GCP_PROJECT_ID }}/npm-repository/:_authToken=\"\"\n            //asia-east2-npm.pkg.dev/${{ secrets.GCP_PROJECT_ID }}/npm-repository/:always-auth=true\n          EOF\n\n      - name: Artifact login\n        run: |\n          #export GOOGLE_APPLICATION_CREDENTIALS=${{ steps.auth.outputs.credentials_file_path }}\n          npx google-artifactregistry-auth@v3 --repo-config=[./.npmrc] --credential-config=[./.npmrc]\n\n`\nBut on this workflow, I got an error on the step `Artifact login`. Telling me :\n```\n`npm WARN exec The following package was not found and will be installed: google-artifactregistry-auth\nRetrieving application default credentials...\nRetrieving credentials from gcloud...\nError: Fail to get credentials. Please run: \n`gcloud auth application-default login`, `gcloud auth login`, or \n`export GOOGLE_APPLICATION_CREDENTIALS=`\n    at Object.getCreds (/home/runner/.npm/_npx/64aef35f3ba01c7c/node_modules/google-artifactregistry-auth/src/auth.js:40:9)\n    at processTicksAndRejections (node:internal/process/task_queues:96:5)\n    at async main (/home/runner/.npm/_npx/64aef35f3ba01c7c/node_modules/google-artifactregistry-auth/src/main.js:66:19)\nError: Process completed with exit code 1.\n`\n```\nThe full workflow is available here\nI don't know where is my mistake here. Does my service account need more right ? or is it an issue on the `google-artifactregistry-auth` ? I really don't know :/\nThx in advance for you help !\nEDIT 1 : I tried to follow this documentation and I added to my service account some right :\n`gcloud artifacts repositories add-iam-policy-binding npm-repository \\\n--location asia-east2 --member=serviceAccount:my-service-account --role=roles/artifactregistry.writer\n`",
      "solution": "I finally find out !!!\nBUT I'm not sure in term of security if there is any risk or not so if anyone can advice I'll edit the answer !\nWhat is changing but I'm not sure in term of security is here :\n`gcloud iam service-accounts add-iam-policy-binding \"gh-deploy-service-account@${PROJECT_ID}.iam.gserviceaccount.com\" \\\n  --project=\"${PROJECT_ID}\" \\\n  --role=\"roles/iam.serviceAccountTokenCreator\" \\\n  --member=\"principalSet://iam.googleapis.com/projects/MY_PROJECT_NUMBER/locations/global/workloadIdentityPools/github-pool-2/*\"\n\ngcloud iam service-accounts add-iam-policy-binding \"gh-deploy-service-account@${PROJECT_ID}.iam.gserviceaccount.com\" \\\n  --project=\"${PROJECT_ID}\" \\\n  --role=\"roles/iam.workloadIdentityUser\" \\\n  --member=\"principalSet://iam.googleapis.com/projects/MY_PROJECT_NUMBER/locations/global/workloadIdentityPools/github-pool-2/*\"\n\n`\nI think I don't really get the `principalSet` option and all the attribute possible so if anyone can advice also on this, I'll be grateful !\nThen don't forget to bind your repo to your service account :\n`gcloud artifacts repositories add-iam-policy-binding npm-repository \\\n--location asia-east2 --member=serviceAccount:gh-deploy-service-account@${PROJECT_ID}.iam.gserviceaccount.com --role=roles/artifactregistry.writer\n`\nAnd for the github workflow I remove the `google-artifactregistry-auth`  and i use the `access_token` in the `.npmrc` file.\nHere is the full workflow :\n`name: Publish Package\non:\n  push:\n    branches:\n      - main\n\njobs:\n  publish:\n    timeout-minutes: 10\n    runs-on: ubuntu-latest\n    permissions:\n      contents: \"read\"\n      id-token: \"write\"\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v2\n\n      - uses: actions/setup-node@v2\n        with:\n          node-version: 16\n\n      - name: Install\n        run: npm ci\n\n      - id: \"auth\"\n        name: \"Authenticate to Google Cloud\"\n        uses: \"google-github-actions/auth@v0\"\n        with:\n          workload_identity_provider: ${{ secrets.WORKLOAD_IDENTITY_PROVIDER }}\n          service_account: ${{ secrets.SERVICE_ACCOUNT }}\n          token_format: 'access_token'\n\n      - name: \"Set up Cloud SDK\"\n        uses: \"google-github-actions/setup-gcloud@v0\"\n\n      - name: Create .npmrc\n        run: |\n          cat  .npmrc\n            @example:registry=https://asia-east2-npm.pkg.dev/${{ secrets.GCP_PROJECT_ID }}/npm-repository/\n            //asia-east2-npm.pkg.dev/${{ secrets.GCP_PROJECT_ID }}/npm-repository/:_authToken=\"${{ steps.auth.outputs.access_token }}\"\n            //asia-east2-npm.pkg.dev/${{ secrets.GCP_PROJECT_ID }}/npm-repository/:always-auth=true\n          EOF\n\n      - name: Artifact login\n        run: |\n          npm publish\n`",
      "question_score": 4,
      "answer_score": 5,
      "created_at": "2022-02-11T08:35:06",
      "url": "https://stackoverflow.com/questions/71076548/google-artifact-regitsry-npm-github-action"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 66953774,
      "title": "GCP cloud functions with &quot;gcloud functions deploy&quot; fails with --source as file rather than directory",
      "problem": "I'm trying to structure my GCP gcloud functions (in python) as a separate file per function. It sounds like from the documentation it should be supported but when I try to use --source as a file name instead of a directory it fails.  Is there something I'm doing wrong?\nHere is the command I use:\n\ngcloud functions deploy createAccount --runtime python38 --trigger-http --source=postgres/createAccount.py --region us-central1\n\nand the error I get back is:\n\nERROR: (gcloud.functions.deploy) argument `--source`: Provided path does not point to a directory\n\nBut if I put my \"createAccount\" python function in `main.py` inside the postgres directory and use this command the function deploys perfectly:\n\ngcloud functions deploy createAccount --runtime python38 --trigger-http --source=postgres --region us-central1\n\nHere it loojs as though it should accept file names in the `--source` option:\nhttps://cloud.google.com/functions/docs/first-python\nSee this section:\n\nAny ideas if there is a way to not make main.py one big monolith of all my cloud functions?",
      "solution": "If we look at the documentation of the gcloud command for deploying functions and the `--source` flag within:\nhttps://cloud.google.com/sdk/gcloud/reference/functions/deploy#--source\nWe find that it unambiguously says that it wants a directory as a parameter and not a source file.  The link you gave which seems to say that we can specify a file looks to be in error.  I think that is a mistake and it can only be a directory that is supplied with `--source`.\nThis would seem to imply that you can create multiple directories ... where each directory contains just the function you wish to deploy.",
      "question_score": 4,
      "answer_score": 5,
      "created_at": "2021-04-05T15:23:04",
      "url": "https://stackoverflow.com/questions/66953774/gcp-cloud-functions-with-gcloud-functions-deploy-fails-with-source-as-file-r"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 78664414,
      "title": "Error migrating Google Cloud Container Registry to Artifact Registry: RESOURCE_EXHAUSTED",
      "problem": "We are following Google's instructions on migrating Container Registry to Artifact Registry for several App Engine Standard services that we have. We don't want to do this, but we have to, as Container Registry is shutting down.\nPer Google's guide we are executing the command,\n`gcloud artifacts docker upgrade migrate --projects=projectid`\nIt succeeded for one project.\nWhen we went to do the next project, it started failing with:\n`ERROR: (gcloud.artifacts.docker.upgrade.migrate) RESOURCE_EXHAUSTED: Resource has been exhausted (e.g. check quota).`\nI waited several hours and tried again but it still fails with the same error.\nSo, I went to the quotas page and checked: we have not exceeded any quotas. I checked both the project quotas and the organization's quotas. Nada.\nWhat can I do to fix this error so we can continue the migration? We have many services to migrate and this is NOT a good experience for migrating production services\nHere's the http call that is failing with a 429 status code (over quota):\n```\n`==== request start ====\nuri: https://cloudasset.googleapis.com/v1/organizations/xxxx:analyzeIamPolicy?alt=json&analysisQuery.accessSelector.permissions=storage.objects.get&analysisQuery.accessSelector.permissions=storage.objects.list&analysisQuery.accessSelector.permissions=storage.objects.create&analysisQuery.accessSelector.permissions=storage.objects.delete&analysisQuery.resourceSelector.fullResourceName=%2F%2Fcloudresourcemanager.googleapis.com%2Fprojects%2Fxxxxx\nmethod: GET\n`\n```",
      "solution": "I am pretty sure the issue comes from the Policy Analyzer free tier limitation.\nBy default the Policy Analyzer is limited to 20 Queries per day: https://cloud.google.com/policy-intelligence/docs/policy-analyzer-overview#pricing\nAnd if you look at the logs of the migration command it use the policy analyzer to validate some of your policy on the repos:\n```\n`DEBUG: https://cloudasset.googleapis.com:443 \"GET /v1/projects/:analyzeIamPolicy?alt=json&analysisQuery.accessSelector.permissions=storage.objects.get&analysisQuery.accessSelector.permissions=storage.objects.list&analysisQuery.accessSelector.permissions=storage.objects.create&analysisQuery.accessSelector.permissions=storage.objects.delete&analysisQuery.resourceSelector.fullResourceName=%2F%2Fstorage.googleapis.com%2Fartifacts..appspot.com HTTP/1.1\" 429 None\n`\n```\nUsing this error log you can recreate the query in the Policy Analyzer and you get the error message:\n```\n`Upgrade to run analysis\nYou can run 20 queries per day\nTry again tomorrow or upgrade to Security Comand Center Premium for unlimited queries.\n`\n```",
      "question_score": 4,
      "answer_score": 3,
      "created_at": "2024-06-24T22:02:43",
      "url": "https://stackoverflow.com/questions/78664414/error-migrating-google-cloud-container-registry-to-artifact-registry-resource-e"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 75214633,
      "title": "suppress bindings output from gcloud projects add-iam-policy-binding command",
      "problem": "When we execute gcp project add-iam-policy-binding, it returns a very verbose output of current bindings. Sometimes, there could be more than 15-20 bindings.\nIs there a way to suppress the output to only success or failure?\nI tried to use the `--verbose` flag with value `error`. It did not help.\nCommand:\n```\n`gcloud projects add-iam-policy-binding  --member=serviceAccount: --role \n`\n```\nOutput:\n```\n`Updated IAM policy for project [].\nbindings:\n\n`\n```\nAny help would be much appreciated.",
      "solution": "Try to use \"--no-user-output-enabled\"\nreference https://fig.io/manual/gcloud/projects/add-iam-policy-binding",
      "question_score": 4,
      "answer_score": 3,
      "created_at": "2023-01-23T21:18:28",
      "url": "https://stackoverflow.com/questions/75214633/suppress-bindings-output-from-gcloud-projects-add-iam-policy-binding-command"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 69865503,
      "title": "&quot;No CMAKE_CXX_COMPILER could be found&quot; errror while deploying flask app on gcloud",
      "problem": "I have a flask application that I'm deploying on google cloud run. The app using a library 'face_recognition' that requires Cmake. I'm installing the CMake by running a command in DockerFile but getting an error. I don't know what it mean.\nHere is my Dockerfile\n```\n`# Use the official lightweight Python image.\n# https://hub.docker.com/_/python\nFROM python:3.9-slim\n\n# Allow statements and log messages to immediately appear in the Knative logs\nENV PYTHONUNBUFFERED True\n\n# Copy local code to the container image.\nENV APP_HOME /app\nWORKDIR $APP_HOME\nCOPY . ./\n\n# Install production dependencies.\nRUN apt-get update && apt-get install -y cmake\nRUN pip install --no-cache-dir -r requirements.txt\nRUN pip install gunicorn\n\n# Run the web service on container startup. Here we use the gunicorn\n# webserver, with one worker process and 8 threads.\n# For environments with multiple CPU cores, increase the number of workers\n# to be equal to the cores available.\n# Timeout is set to 0 to disable the timeouts of the workers to allow Cloud Run to handle instance scaling.\nCMD exec gunicorn --bind :$PORT --workers 1 --threads 8 --timeout 0 main:app\n`\n```\nHere is the error\n```\n`CMake Error at CMakeLists.txt:14 (project):\n      No CMAKE_CXX_COMPILER could be found.\n    \n      Tell CMake where to find the compiler by setting either the environment\n      variable \"CXX\" or the CMake cache entry CMAKE_CXX_COMPILER to the full path\n      to the compiler, or to the compiler name if it is in the PATH.\n    \n    \n    -- Configuring incomplete, errors occurred!\n    See also \"/tmp/pip-install-2m1peq73/dlib_d6f82528b68745578021b2f234f89d7c/build/temp.linux-x86_64-3.9/CMakeFiles/CMakeOutput.log\".\n    See also \"/tmp/pip-install-2m1peq73/dlib_d6f82528b68745578021b2f234f89d7c/build/temp.linux-x86_64-3.9/CMakeFiles/CMakeError.log\".\n    Traceback (most recent call last):\n      File \"\", line 1, in \n      File \"/tmp/pip-install-2m1peq73/dlib_d6f82528b68745578021b2f234f89d7c/setup.py\", line 222, in \n        setup(\n      File \"/usr/local/lib/python3.9/site-packages/setuptools/__init__.py\", line 153, in setup\n        return distutils.core.setup(**attrs)\n      File \"/usr/local/lib/python3.9/distutils/core.py\", line 148, in setup\n        dist.run_commands()\n      File \"/usr/local/lib/python3.9/distutils/dist.py\", line 966, in run_commands\n        self.run_command(cmd)\n      File \"/usr/local/lib/python3.9/distutils/dist.py\", line 985, in run_command\n        cmd_obj.run()\n      File \"/usr/local/lib/python3.9/site-packages/setuptools/command/install.py\", line 61, in run\n        return orig.install.run(self)\n      File \"/usr/local/lib/python3.9/distutils/command/install.py\", line 546, in run\n        self.run_command('build')\n      File \"/usr/local/lib/python3.9/distutils/cmd.py\", line 313, in run_command\n        self.distribution.run_command(command)\n      File \"/usr/local/lib/python3.9/distutils/dist.py\", line 985, in run_command\n        cmd_obj.run()\n      File \"/usr/local/lib/python3.9/distutils/command/build.py\", line 135, in run\n        self.run_command(cmd_name)\n      File \"/usr/local/lib/python3.9/distutils/cmd.py\", line 313, in run_command\n        self.distribution.run_command(command)\n      File \"/usr/local/lib/python3.9/distutils/dist.py\", line 985, in run_command\n        cmd_obj.run()\n      File \"/tmp/pip-install-2m1peq73/dlib_d6f82528b68745578021b2f234f89d7c/setup.py\", line 134, in run\n        self.build_extension(ext)\n      File \"/tmp/pip-install-2m1peq73/dlib_d6f82528b68745578021b2f234f89d7c/setup.py\", line 171, in build_extension\n        subprocess.check_call(cmake_setup, cwd=build_folder)\n      File \"/usr/local/lib/python3.9/subprocess.py\", line 373, in check_call\n        raise CalledProcessError(retcode, cmd)\n`\n```",
      "solution": "The container base python:3.9-slim is very stripped down. If your application requires CMake which often implies the gcc compiler as well,\nyou have at least two options:\n\nUse a more feature rich base container such as debian:buster\nChoose a container with those tools already configured.\n\nExample Dockerfile to build a base container:\n```\n`FROM debian:buster\nRUN apt update && apt install -y gcc clang clang-tools cmake python3\n`\n```\nYou can then use that container as the base for future containers or modify the Dockerfile to include your application.\nDocker debian:buster",
      "question_score": 4,
      "answer_score": 3,
      "created_at": "2021-11-06T16:48:00",
      "url": "https://stackoverflow.com/questions/69865503/no-cmake-cxx-compiler-could-be-found-errror-while-deploying-flask-app-on-gclou"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 71335311,
      "title": "gcloud Firestore import: PERMISSION_DENIED",
      "problem": "I am trying to import a bucket containing a Firestore database export into antoher Firebase project. I have been following this guide on how to do this.\nWhen running the `gcloud firestore import`, I run into the following issue:\n\nERROR: (gcloud.firestore.import) PERMISSION_DENIED: Service account does not have access to Google Cloud Storage file: /bucket/EXPORT_PREFIX.overall_export_metadata. See https://cloud.google.com/datastore/docs/export-import-entities#permissions for a list of permissions needed. Error details: service-XXX@gcp-sa-firestore.iam.gserviceaccount.com does not have storage.buckets.get access to the Google Cloud Storage bucket.\n\nI did however use the following command to grant access to the bucket:\n```\n`gsutil iam ch serviceAccount:SOURCE_PROJECTID@appspot.gserviceaccount.com:admin gs://bucket_name\n`\n```\nThis did not give me any error whatsoever, so I assume it ran as expected. I triple checked and believe I was working in the correct projects while using these commands.\nI think that perhaps the `import` command is ran with another service account than `SOURCE_PROJECTID@appspot.gserviceaccount.com:admin`, but am unsure about this or on how to ensure the correct service account is being used.\nAny help on resolving this would be highly appreciated! :)",
      "solution": "The error message appears to include the Service Account in question:\n```\n`Error details:\nservice-XXX@gcp-sa-firestore.iam.gserviceaccount.com\ndoes not have storage.buckets.get access to the Google Cloud Storage bucket.\n`\n```\nI think you need to:\n`gsutil iam ch \\\nserviceAccount:[service-XXX]@gcp-sa-firestore.iam.gserviceaccount.com:objectViewer \\\ngs://[bucket-name]\n`\nReplacing `[service-XXX]` and `[bucket-name]`",
      "question_score": 4,
      "answer_score": 2,
      "created_at": "2022-03-03T11:06:06",
      "url": "https://stackoverflow.com/questions/71335311/gcloud-firestore-import-permission-denied"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 70293779,
      "title": "The command `gcloud container clusters get-credentials ` will not create a kubeconfig",
      "problem": "When I type `gcloud container clusters get-credentials`, I get response `entry generated for ***.`. and it looks like it is generated, but when I hit `kubectl config view`, there is nothing.\nReference of `gcloud container clusters get-credentials` says,\n\ngcloud container clusters get-credentials updates a kubeconfig file with appropriate credentials and endpoint information to point kubectl at a specific cluster in Google Kubernetes Engine.\n\nSo I thought the problem was that `~/.kube/config` did not exist, but creating an empty file did not change it.",
      "solution": "Thanks to @DazWilkin in the comments for suggesting running the command with the `--verbosity=debug` flag, which then outputted where it was writing the file.\nThe reason was that the `PATH` of WSL included the `PATH` of the Windows side by default, so it was calling the `gcloud` on Windows (installed by scoop).\nI solved the problem by excluding the `PATH` of Windows, referring to this gists.",
      "question_score": 4,
      "answer_score": 2,
      "created_at": "2021-12-09T17:57:43",
      "url": "https://stackoverflow.com/questions/70293779/the-command-gcloud-container-clusters-get-credentials-will-not-create-a-kubec"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 66596277,
      "title": "Modifying gcloud VM service account permissions without stopping the VM",
      "problem": "I'm trying to upload a file from a Google Cloud VM into a cloud storage bucket.\nExpectedly it fails because the service account associated with the VM doesn't have permissions:\n```\n`$ gsutil cp file.png gs://bucket/\nCopying file://file.png [Content-Type=image/png]...\nAccessDeniedException: 403 Insufficient Permission\n`\n```\nFrom what I understand there are two ways to fix this:\n\nmodify the scopes from the VM web admin panel\nchange the permissions of the bucket and add the service account with write access (I'd prefer this because the other option seems to give access to all buckets in the same project)\n\nHowever, it seems that both solutions require the VM to be stopped, which is problematic as it is a production server.\nIs there any way to fix this without stopping the VM?",
      "solution": "There are two methods of controlling permissions granted to a Compute Engine VM.\n\nAccess Scopes\nService Account assigned to the instance.\n\nBoth of these methods work together. The total permissions available to a Compute Engine instance is controlled by the service account. Access Scopes then limit the permissions assigned to the VM.\nYou must shutdown a VM to change the Access Scopes. Changing the service account roles does not require rebooting the VM.\nFor this question regarding Cloud Storage Access:\nIf the service account has a Cloud Storage role granting access to cloud storage but the Access Scope for Storage is set to None, then the VM will not have access to Cloud Storage even though the service account has the required role. In this case you must shutdown the VM to change the Access Scope to enable access to Cloud Storage.\nIf the VM Access Scope has Storage enabled, but the service account does not have a Cloud Storage role, the VM will not be able to access Cloud Storage. In this case, adding a Cloud Storage role to the service account will grant access to Cloud Storage without requiring a VM reboot.\nAccess Scopes (OAuth Scopes) are a legacy mechanism that existed prior to Google Cloud IAM. Given that you are using this VM in a production environment and shutting down the instance is not desired, I recommend the following:\n\nSet the VM Access Scopes to \"Allow full access to all Cloud APIs\".\nCreate a new service account with the required roles and assign that service account to the Compute Engine VM instance.",
      "question_score": 4,
      "answer_score": 2,
      "created_at": "2021-03-12T09:06:25",
      "url": "https://stackoverflow.com/questions/66596277/modifying-gcloud-vm-service-account-permissions-without-stopping-the-vm"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 66133553,
      "title": "How to deploy Laravel 8 google cloud run with google cloud database",
      "problem": "Iam looking for help to containerize a laravel application with docker, running it locally and make it deployable to gcloud Run, connected to a gcloud database.\nMy application is an API, build with laravel, and so far i have just used the docker-compose/sail package, that comes with laravel 8, in the development.\nHere is what i want to achieve:\n\nLaravel app running on gcloud Run.\nDatabase in gcloud, Mysql, PostgreSQL or SQL server. (prefer Mysql).\nEnviroment stored in gcloud.\n\nMy problem is can find any info if or how to use/rewrite the docker-composer file i laravel 8, create a Dockerfile or cloudbuild file, and build it for gcloud.\nMaybe i could add something like this in a cloudbuild.yml file:\n```\n`#cloudbuild.yml\n  steps:\n  # running docker-compose\n  - name: 'docker/compose:1.26.2'\n    args: ['up', '-d']\n`\n```\nAny help/guidanceis is appreciated.",
      "solution": "As mentioned in the comments to this question you can check this video that explains how you can use docker-composer, laravel to deploy an app to Cloud Run with a step-by-step tutorial.\nAs per database connection to said app, the Connecting from Cloud Run (fully managed)  to Cloud SQL documentation is quite complete on that matter and for secret management I found this article that explains how to implement secret manager into Cloud Run.\nI know this answer is basically just links to the documentation and articles, but I believe all the information you need to implement your app into Cloud Run is in those.",
      "question_score": 4,
      "answer_score": 1,
      "created_at": "2021-02-10T09:47:48",
      "url": "https://stackoverflow.com/questions/66133553/how-to-deploy-laravel-8-google-cloud-run-with-google-cloud-database"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 73348750,
      "title": "ERROR: gcloud failed to load: No module named &#39;googlecloudsdk.third_party.apis.binaryauthorization.v1alpha2.binaryauthorization_v1alpha2_messages&#39;",
      "problem": "I am trying to install gcloud CLI on macOS 64-bit(x86_64) by following these instructions: https://cloud.google.com/sdk/docs/install\nWhen I run `./google-cloud-sdk/install.sh` I get this error:\n```\n`Welcome to the Google Cloud CLI!\nTraceback (most recent call last):\n  File \"/Users/benkula/./google-cloud-sdk/bin/bootstrapping/install.py\", line 30, in \n    from googlecloudsdk import gcloud_main\n  File \"/Users/benkula/google-cloud-sdk/lib/googlecloudsdk/gcloud_main.py\", line 37, in \n    from googlecloudsdk.command_lib.util.apis import yaml_command_translator\n  File \"/Users/benkula/google-cloud-sdk/lib/googlecloudsdk/command_lib/util/apis/yaml_command_translator.py\", line 39, in \n    from googlecloudsdk.command_lib.iam import iam_util\n  File \"/Users/benkula/google-cloud-sdk/lib/googlecloudsdk/command_lib/iam/iam_util.py\", line 85, in \n    binaryauthorization_message_v1alpha2 = core_apis.GetMessagesModule(\n  File \"/Users/benkula/google-cloud-sdk/lib/googlecloudsdk/api_lib/util/apis.py\", line 339, in GetMessagesModule\n    return __import__(api_def.apitools.messages_full_modulepath,\nModuleNotFoundError: No module named 'googlecloudsdk.third_party.apis.binaryauthorization.v1alpha2.binaryauthorization_v1alpha2_messages'\n`\n```\n`./google-cloud-sdk/bin/gcloud init` shows a similar error:\n```\n`ERROR: gcloud failed to load: No module named 'googlecloudsdk.third_party.apis.binaryauthorization.v1alpha2.binaryauthorization_v1alpha2_messages'\n    gcloud_main = _import_gcloud_main()\n    import googlecloudsdk.gcloud_main\n    from googlecloudsdk.command_lib.util.apis import yaml_command_translator\n    from googlecloudsdk.command_lib.iam import iam_util\n    binaryauthorization_message_v1alpha2 = core_apis.GetMessagesModule(\n    return __import__(api_def.apitools.messages_full_modulepath,\n\nThis usually indicates corruption in your gcloud installation or problems with your Python interpreter.\n\nPlease verify that the following is the path to a working Python 2.7 or 3.5+ executable:\n    /Applications/Xcode.app/Contents/Developer/usr/bin/python3\n\nIf it is not, please set the CLOUDSDK_PYTHON environment variable to point to a working Python 2.7 or 3.5+ executable.\n\nIf you are still experiencing problems, please reinstall the Cloud SDK using the instructions here:\n    https://cloud.google.com/sdk/\n`\n```\nPython executable seems to be where it should be, so I don't think that's the problem. I can see it when I run `ls /Applications/Xcode.app/Contents/Developer/usr/bin/`.\n`python3 -V` shows `Python 3.8.9`.\nWhat can I do to install gcloud CLI successfully?",
      "solution": "I used this command instead, and it worked for me:\n```\n`curl https://sdk.cloud.google.com | bash\n`\n```\nFound here: https://cloud.google.com/sdk/docs/downloads-interactive.",
      "question_score": 3,
      "answer_score": 16,
      "created_at": "2022-08-14T04:16:56",
      "url": "https://stackoverflow.com/questions/73348750/error-gcloud-failed-to-load-no-module-named-googlecloudsdk-third-party-apis-b"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 74049351,
      "title": "gcloud functions deploy go runtime error &quot;undefined: unsafe.Slice; Error ID: 2f5e35a0&quot;",
      "problem": "While deploying to google cloud function, I am getting this error:\n```\n`ERROR: (gcloud.functions.deploy) OperationError: code=3, message=Build failed: # projectname/vendor/golang.org/x/sys/unix\nsrc/projectname/vendor/golang.org/x/sys/unix/syscall.go:83:16: undefined: unsafe.Slice\nsrc/projectname/vendor/golang.org/x/sys/unix/syscall_linux.go:2255:9: undefined: unsafe.Slice\nsrc/projectname/vendor/golang.org/x/sys/unix/syscall_unix.go:118:7: undefined: unsafe.Slice\nsrc/projectname/vendor/golang.org/x/sys/unix/sysvshm_unix.go:33:7: undefined: unsafe.Slice; Error ID: 2f5e35a0\n`\n```\nHere's my command:\n```\n`gcloud functions deploy servicename --region=us-central1 --entry-point=gofunctionname --runtime=go116 --source=.\n`\n```\nI am using vendoring to package my dependencies. It's been a while I have updated this function. And first time I noticed this error.\nAny help would be much appreciated.",
      "solution": "As DazWilkin suggested above, `unsafe.Slice` was added as part of Go 1.17 and GCP Functions support Go 1.16 as of now.\nI had to revert back the `golang.org/x/sys` module in the `go.mod` file and it worked for me.\nFrom\n```\n`golang.org/x/sys v0.0.0-20221010170243-090e33056c14 // indirect\n`\n```\nTo\n```\n`golang.org/x/sys v0.0.0-20220811171246-fbc7d0a398ab // indirect\n`\n```\nWith this change, I am able to build and deploy the code to Google Cloud Functions.",
      "question_score": 3,
      "answer_score": 13,
      "created_at": "2022-10-13T02:47:30",
      "url": "https://stackoverflow.com/questions/74049351/gcloud-functions-deploy-go-runtime-error-undefined-unsafe-slice-error-id-2f5"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 77397920,
      "title": "Python, Poetry, cant publish to Google Artifact Registry repository | &#39;The request does not have valid authentication credentials",
      "problem": "I have a proprietary Python project that I manage using Poetry. I stored the code in the Google Artifact Registry in the past, but now I switched to a different dev environment (Arch Linux in a VM -> Windows + Ubuntu WSL) and I can't upload it the AR, while getting this error (this is on my new WSL machine):\n```\n`$ poetry publish --build --repository all-mpm                                                                                                      \nThere are 2 files ready for publishing. Build anyway? (yes/no) [no] yes\nBuilding mpm (0.1.54)\n  - Building sdist\n  - Built mpm-0.1.54.tar.gz\n  - Building wheel\n  - Built mpm-0.1.54-py3-none-any.whl\n\nPublishing mpm (0.1.54) to all-mpm\n - Uploading mpm-0.1.54-py3-none-any.whl FAILED\n\nHTTP Error 401: Unauthorized | b'The request does not have valid authentication credentials.\\n'\n`\n```\nHere's the catch: This is only happens to this repository. If I create a new repository on the same machine using the same project, I don't get an error and am able to upload my package successfully.\n\nI believe this is a keyring issue. Listing all my keyring backends:\n```\n`$ keyring --list-backends   \nkeyrings.gauth.GooglePythonAuth (priority: 9)\nkeyring.backends.chainer.ChainerBackend (priority: 10)\nkeyring.backends.SecretService.Keyring (priority: 5)\nkeyring.backends.fail.Keyring (priority: 0)\n`\n```\nI used gsutil for its creation:\n```\n`gcloud artifacts repositories create my-repo \\\n    --repository-format=python \\\n    --location=europe-west3\n`\n```\nAfter that I set for my Python project which I manage using poetry.\n```\n`poetry config repository.myrepo \"https://europe-west3-python.pkg.dev/my-project/my-repo/\n`\n```\nMy usual build process:\n```\n`poetry publish --build --repository my-repo\n`\n```\nIf I run this command with the verbose option (-vvv), I get the following details:\n```\n`$ poetry publish --build --repository all-mpm -vvv          \nLoading configuration file /home/til/.config/pypoetry/config.toml\nLoading configuration file /home/til/.config/pypoetry/auth.toml\nThere are 2 files ready for publishing. Build anyway? (yes/no) [no] yes\nUsing virtualenv: /home/til/.cache/pypoetry/virtualenvs/mpm-W0UszHAu-py3.11\nBuilding mpm (0.1.54)\n  - Building sdist\nIgnoring: tests/__pycache__/test_plytix.cpython-311-pytest-7.4.2.pyc\n********redacted********\n  - Adding: /home/til/code/aplus-automation/src/mpm/__init__.py\n********redacted********\n  - Adding: pyproject.toml\n  - Adding: README.md\n  - Built mpm-0.1.54.tar.gz\n  - Building wheel\nIgnoring: htmlcov/d_a44f0ac069e85531_test_plytix_py.html\n********redacted********\n  - Adding: /home/til/code/aplus-automation/src/mpm/__init__.py\n********redacted********\nSkipping: /home/til/code/aplus-automation/LICENSE\nSkipping: /home/til/code/aplus-automation/COPYING\n  - Built mpm-0.1.54-py3-none-any.whl\n\n[keyring.backend] Loading KWallet\n[keyring.backend] Loading SecretService\n[keyring.backend] Loading Windows\n[keyring.backend] Loading chainer\n[keyring.backend] Loading libsecret\n[keyring.backend] Loading macOS\n[keyring.backend] Loading Google Auth\nFound authentication information for all-mpm.\nPublishing mpm (0.1.54) to all-mpm\n - Uploading mpm-0.1.54-py3-none-any.whl 0%[urllib3.connectionpool] Starting new HTTPS connection (1): europe-west3-python.pkg.dev:443\n - Uploading mpm-0.1.54-py3-none-any.whl 100%[urllib3.connectionpool] https://europe-west3-python.pkg.dev:443 \"POST /mpm99-398708/all-mpm HTTP/1.1\" 401 60\n - Uploading mpm-0.1.54-py3-none-any.whl FAILED\n\n  Stack trace:\n\n  1  ~/.pyenv/versions/3.11.6/lib/python3.11/site-packages/poetry/publishing/uploader.py:265 in _upload_file\n      263\u2502                     bar.display()\n      264\u2502                 else:\n    \u2192 265\u2502                     resp.raise_for_status()\n      266\u2502             except (requests.ConnectionError, requests.HTTPError) as e:\n      267\u2502                 if self._io.output.is_decorated():\n\n  HTTPError\n\n  401 Client Error: Unauthorized for url: https://europe-west3-python.pkg.dev/mpm99-398708/all-mpm\n\n  at ~/.local/lib/python3.11/site-packages/requests/models.py:1021 in raise_for_status\n      1017\u2502                 f\"{self.status_code} Server Error: {reason} for url: {self.url}\"\n      1018\u2502             )\n      1019\u2502 \n      1020\u2502         if http_error_msg:\n    \u2192 1021\u2502             raise HTTPError(http_error_msg, response=self)\n      1022\u2502 \n      1023\u2502     def close(self):\n      1024\u2502         \"\"\"Releases the connection back to the pool. Once this method has been\n      1025\u2502         called the underlying ``raw`` object must not be accessed again.\n\nThe following error occurred when trying to handle this error:\n\n  Stack trace:\n\n  11  ~/.local/lib/python3.11/site-packages/cleo/application.py:327 in run\n       325\u2502 \n       326\u2502             try:\n     \u2192 327\u2502                 exit_code = self._run(io)\n       328\u2502             except BrokenPipeError:\n       329\u2502                 # If we are piped to another process, it may close early and send a\n\n  10  ~/.pyenv/versions/3.11.6/lib/python3.11/site-packages/poetry/console/application.py:190 in _run\n       188\u2502         self._load_plugins(io)\n       189\u2502 \n     \u2192 190\u2502         exit_code: int = super()._run(io)\n       191\u2502         return exit_code\n       192\u2502 \n\n   9  ~/.local/lib/python3.11/site-packages/cleo/application.py:431 in _run\n       429\u2502             io.input.interactive(interactive)\n       430\u2502 \n     \u2192 431\u2502         exit_code = self._run_command(command, io)\n       432\u2502         self._running_command = None\n       433\u2502 \n\n   8  ~/.local/lib/python3.11/site-packages/cleo/application.py:473 in _run_command\n       471\u2502 \n       472\u2502         if error is not None:\n     \u2192 473\u2502             raise error\n       474\u2502 \n       475\u2502         return terminate_event.exit_code\n\n   7  ~/.local/lib/python3.11/site-packages/cleo/application.py:457 in _run_command\n       455\u2502 \n       456\u2502             if command_event.command_should_run():\n     \u2192 457\u2502                 exit_code = command.run(io)\n       458\u2502             else:\n       459\u2502                 exit_code = ConsoleCommandEvent.RETURN_CODE_DISABLED\n\n   6  ~/.local/lib/python3.11/site-packages/cleo/commands/base_command.py:119 in run\n       117\u2502         io.input.validate()\n       118\u2502 \n     \u2192 119\u2502         status_code = self.execute(io)\n       120\u2502 \n       121\u2502         if status_code is None:\n\n   5  ~/.local/lib/python3.11/site-packages/cleo/commands/command.py:62 in execute\n        60\u2502 \n        61\u2502         try:\n     \u2192  62\u2502             return self.handle()\n        63\u2502         except KeyboardInterrupt:\n        64\u2502             return 1\n\n   4  ~/.pyenv/versions/3.11.6/lib/python3.11/site-packages/poetry/console/commands/publish.py:82 in handle\n        80\u2502         )\n        81\u2502 \n     \u2192  82\u2502         publisher.publish(\n        83\u2502             self.option(\"repository\"),\n        84\u2502             self.option(\"username\"),\n\n   3  ~/.pyenv/versions/3.11.6/lib/python3.11/site-packages/poetry/publishing/publisher.py:86 in publish\n        84\u2502         )\n        85\u2502 \n     \u2192  86\u2502         self._uploader.upload(\n        87\u2502             url,\n        88\u2502             cert=resolved_cert,\n\n   2  ~/.pyenv/versions/3.11.6/lib/python3.11/site-packages/poetry/publishing/uploader.py:107 in upload\n       105\u2502 \n       106\u2502         try:\n     \u2192 107\u2502             self._upload(session, url, dry_run, skip_existing)\n       108\u2502         finally:\n       109\u2502             session.close()\n\n   1  ~/.pyenv/versions/3.11.6/lib/python3.11/site-packages/poetry/publishing/uploader.py:191 in _upload\n       189\u2502     ) -> None:\n       190\u2502         for file in self.files:\n     \u2192 191\u2502             self._upload_file(session, url, file, dry_run, skip_existing)\n       192\u2502 \n       193\u2502     def _upload_file(\n\n  UploadError\n\n  HTTP Error 401: Unauthorized | b'The request does not have valid authentication credentials.\\n'\n\n  at ~/.pyenv/versions/3.11.6/lib/python3.11/site-packages/poetry/publishing/uploader.py:271 in _upload_file\n      267\u2502                 if self._io.output.is_decorated():\n      268\u2502                     self._io.overwrite(\n      269\u2502                         f\" - Uploading {file.name} FAILED\"\n      270\u2502                     )\n    \u2192 271\u2502                 raise UploadError(e)\n      272\u2502             finally:\n      273\u2502                 self._io.write_line(\"\")\n      274\u2502 \n      275\u2502     def _register(self, session: requests.Session, url: str) -> requests.Response:\n`\n```\nOther measures I took:\n\nreinstallation of gsutils\nconsulted the AR docs, setting up using a) ADC, b) a service account, c) .pypirc file\nadded the keyring to poetry\n\nMore debug info:\nPoetry plugins:\n```\n`$ poetry self show plugins    \n\n  \u2022 poetry-plugin-export (1.5.0) Poetry plugin to export the dependencies to various formats\n      1 application plugin\n\n      Dependencies\n        - poetry (>=1.5.0,=1.6.0,After removing .config/gcloud/ and setting up using gcloud init again (still getting the bad credentials error)\n```\n`$ gcloud init\nWelcome! This command will take you through the configuration of gcloud.\n\nYour current configuration has been set to: [default]\n\nYou can skip diagnostics next time by using the following flag:\n  gcloud init --skip-diagnostics\n\nNetwork diagnostic detects and fixes local network connection issues.\nChecking network connection...done.                                                                                                                                                           \nReachability Check passed.\nNetwork diagnostic passed (1/1 checks passed).\n\nYou must log in to continue. Would you like to log in (Y/n)?  y\n\nGo to the following link in your browser:\n\n    ****redacted****\n\nEnter authorization code: ****redacted****\n\nUpdates are available for some Google Cloud CLI components.  To install them,\nplease run:\n  $ gcloud components update\n\nYou are logged in as: [***redacted****].\n\nPick cloud project to use: \n****redacted****\nPlease enter numeric choice or text value (must exactly match list item):  3\n\nYour current project has been set to: [****redacted****].\n\n`\n```\nThere is a slight difference when uploading to any other registry:\n```\n`[keyring.backend] Loading KWallet\n[keyring.backend] Loading SecretService\n[keyring.backend] Loading Windows\n[keyring.backend] Loading chainer\n[keyring.backend] Loading libsecret\n[keyring.backend] Loading macOS\n[keyring.backend] Loading Google Auth\n### this is different! vvvvv\n[google.auth._default] Checking None for explicit credentials as part of auth process...\n[google.auth._default] Checking Cloud SDK credentials as part of auth process...\n[google.auth.transport.requests] Making request: POST https://oauth2.googleapis.com/token\n[urllib3.connectionpool] Starting new HTTPS connection (1): oauth2.googleapis.com:443\n[urllib3.connectionpool] https://oauth2.googleapis.com:443 \"POST /token HTTP/1.1\" 200 None\nFound authentication information for all-mpm2.\n`\n```\nI'm at a loss at what to try next.",
      "solution": "As @robert-g commented, the solution can be found in https://github.com/python-poetry/poetry/issues/7545\nFor posterity, this fixed my issue:\n```\n`poetry config http-basic.my-repo oauth2accesstoken $(gcloud auth print-access-token)\n`\n```",
      "question_score": 3,
      "answer_score": 8,
      "created_at": "2023-10-31T18:15:48",
      "url": "https://stackoverflow.com/questions/77397920/python-poetry-cant-publish-to-google-artifact-registry-repository-the-reque"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 76983714,
      "title": "How can I get gcloud auth to open the windows browser via WSL?",
      "problem": "When trying to authenticate with google cloud CLI, you are prompted to the OAuth2 flow that asks you to login with your google account and approve.\n```\n`gcloud auth application-default login\n`\n```\nHowever, this does not work via WSL on windows.\nIt seems that the google CLI tool can't identify the browser that is configured to run on the host.\nThe flow itself can still work, as the CLI prints a URL that can be copied to the browser, but as a developer, I always prefer being lazy and save a bit more time whenever possible :)",
      "solution": "TL;DR\nInstall wslu and run the command with `DISPLAY='ANYTHING'`\n```\n`DISPLAY='X' gcloud auth application-default login\n`\n```\nWhy does this work?\nThere are two steps to solving this problem:\n\nWe want to set a default browser to the linux distribution. This is one approach - https://superuser.com/a/1368878\nWe want to get `gcloud` to work with our setting\n\nAfter digging around the python CLI implementation, I found this piece of code inside `check_browser.py`\n```\n`# These are environment variables that can indicate a running compositor on\n# Linux.\n_DISPLAY_VARIABLES = ['DISPLAY', 'WAYLAND_DISPLAY', 'MIR_SOCKET']\n\n....\n\ncurrent_os = platforms.OperatingSystem.Current()\nif (current_os is platforms.OperatingSystem.LINUX and\n    not any(encoding.GetEncodedValue(os.environ, var) for var\n            in _DISPLAY_VARIABLES)):\n  launch_browser = False\n`\n```\nAn easy solution that works for me is running the gcloud CLI with the environment variable `DISPLAY='X'`:\n```\n`DISPLAY='X' gcloud auth application-default login\n`\n```\nYou can also set an alias:\n```\n`alias gauth='DISPLAY=\"X\" gcloud auth application-default login'\n`\n```\nTroubleshooting\nHowever, this might now work on all systems. Also the code might change in the future. If Anyone else is interested in tweaking with it in a different manner, I located the relevant file under:\n```\n`/usr/lib/google-cloud-sdk/lib/googlecloudsdk/command_lib/util/check_browser.py\n`\n```\nAnd you can playaround with it in python\n```\n`In [21]: from googlecloudsdk.command_lib.util.check_browser import *\n\nIn [22]: ShouldLaunchBrowser(True)\nOut[22]: False\n`\n```",
      "question_score": 3,
      "answer_score": 6,
      "created_at": "2023-08-26T17:48:09",
      "url": "https://stackoverflow.com/questions/76983714/how-can-i-get-gcloud-auth-to-open-the-windows-browser-via-wsl"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 76773971,
      "title": "how to paginate in drive.permissions.list using google-api-python-client",
      "problem": "My Problem\nI have a GDrive folder of which I want to list the users having access to it. Unfortunately the API doesn't return more than 100 users and the documented solutions to paginate don't work.\n(!) Maybe this is related to testing on shared drives - currently I cannot reproduce on non-shared drive folders unfortunately.\nWhat I Tried so far:\nI tried the pagination solution as described in the google docs, e.g. I tried:\n```\n`file_id = \"12abcde\"  # my folder ID\nrequest = service.permissions().list(\n  fileId=file_id, fields=\"permissions(type,emailAddress)\", supportsAllDrives=True\n)\nwhile request is not None:\n    activities_doc = request.execute()\n    request = service.permissions().list_next(request, activities_doc)\n`\n```\nthe request in the loop is always None.\nThe returned body from the initial request as well doesn't match the one described in the docs.\nDoes somebody have experience on making this work?",
      "solution": "to get the full list, you need to use the `nextPageToken` and place it under the `pageToken` parameter.\nYou can get the nextPageToken from the response body by adding it to the requested fields.\nYou can use the following sample code as reference:\n```\n`    try:\n\n        service = build('drive', 'v3', credentials=creds)\n        file_id = \"asdfasdfas\"  # my folder ID\n        permissions_list = []\n        first_request = service.permissions().list(\n                fileId=file_id, \n                pageSize=100, \n                supportsAllDrives=True, \n                fields=\"nextPageToken, permissions(type,emailAddress)\"\n            ).execute()\n        \n        permissions_list.append(first_request)\n\n        next_Page_Token = first_request.get('nextPageToken')\n\n        \n\n        while next_Page_Token:\n\n            request = service.permissions().list(\n                pageToken = next_Page_Token,\n                fileId=file_id,\n                pageSize=100, \n                supportsAllDrives=True, \n                fields=\"nextPageToken, permissions(type,emailAddress)\"\n            ).execute()\n            permissions_list.append(request)\n            next_Page_Token = request.get('nextPageToken')\n\n            \n\n    except HttpError as error:\n        # TODO(developer) - Handle errors from drive API.\n        print(f'An error occurred: {error}')\n`\n```\nUpdate:\nThis one is a sample using `list_next()` base in the documentation here:\n```\n`    try:\n        service = build('drive', 'v3', credentials=creds)\n\n        permission_list = []\n        token = 'test'\n\n        fileID='folderID'\n\n        request_made =  service.permissions().list(\n            fileId=fileID,\n            pageSize=100, \n            fields=\"nextPageToken, permissions(type,emailAddress)\",\n            supportsAllDrives=True\n            )\n\n        request = request_made.execute()\n        permission_list.append(request)\n\n        while token:\n            nextpage = service.permissions().list_next(\n                previous_request=request_made, \n                previous_response=request)\n            request = nextpage.execute()\n            token = request.get('nextPageToken')\n            permission_list.append(request)\n            request_made = nextpage\n\n    except HttpError as error:\n        # TODO(developer) - Handle errors from drive API.\n        print(f'An error occurred: {error}')\n`\n```",
      "question_score": 3,
      "answer_score": 4,
      "created_at": "2023-07-26T20:04:28",
      "url": "https://stackoverflow.com/questions/76773971/how-to-paginate-in-drive-permissions-list-using-google-api-python-client"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 66228193,
      "title": "How can I specify the ABI of my emulator to x86_64 when using Firebase Test Lab?",
      "problem": "Background:\nSome people are having this issue where their UI tests fail when their emulators aren't using ABI x86_64. Correct me if I'm wrong, but this seems to be a problem when running automated tests via Firebase Test Lab (via `gcloud firebase test android run`), because their emulators do not seem to be using ABI x86_64.\nWhen using `gcloud firebase test android run`, is it possible to set the ABI of the emulator being used?",
      "solution": "Currently Firebase Test Lab supports `x86` ABIs on virtual devices, but not `x86_64` yet. However, `x86_64` support is expected to come later this year when Android S devices are released.\nAlso, the `gcloud firebase test android models describe MODEL_ID` command will list which ABIs each Android device supports.",
      "question_score": 3,
      "answer_score": 6,
      "created_at": "2021-02-16T17:19:21",
      "url": "https://stackoverflow.com/questions/66228193/how-can-i-specify-the-abi-of-my-emulator-to-x86-64-when-using-firebase-test-lab"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 78421956,
      "title": "gcloud run jobs deploy is crashing w/ can only concatenate str (not &quot;tuple&quot;) to str?",
      "problem": "I started getting this error today when running gcloud to do source deployment to Cloud Run:\n\nThis command is equivalent to running `gcloud builds submit --tag [IMAGE] .` and `gcloud run jobs deploy surfey-bin --image [IMAGE]`\nBuilding using Dockerfile and deploying container to Cloud Run job [surfey-bin] in project [surfey] region [us-central1]\nX  Building and updating job... Building Container.\nOK Uploading sources...\nOK Building Container... Logs are available at [https://console.cloud.google.com/cloud-build/builds/b650028c-ac14-4c3b-b5b8-f2e06a2ebdd5?project=270402655279].\nJob failed to deploy\nERROR: gcloud crashed (TypeError): can only concatenate str (not \"tuple\") to str\nIf you would like to report this issue, please run the following command:\ngcloud feedback\nTo check gcloud for common problems, please run the following command:\ngcloud info --run-diagnostics\n\nHere's the full command I'm running:\n`gcloud run jobs deploy \"surfey-bin\" \\\n  --cpu 4 \\\n  --memory 8Gi \\\n  --max-retries 2 \\\n  --project surfey \\\n  --region us-central1 \\\n  --set-env-vars \"OPENAI_ORGANIZATION=$OPENAI_ORGANIZATION\" \\\n  --set-secrets=OPENAI_API_KEY=OPENAI_API_KEY:latest \\\n  --source . \\\n  --task-timeout 1200\n`\nHere's the gcloud version dump from my local machine:\n```\n`$ gcloud --version\nGoogle Cloud SDK 474.0.0\nalpha 2024.04.26\nbeta 2024.04.26\nbq 2.1.4\nbundled-python3-unix 3.11.8\ncore 2024.04.26\ngcloud-crc32c 1.0.0\ngke-gcloud-auth-plugin 0.5.8\ngsutil 5.27\nkubectl 1.26.15\nminikube 1.32.0\nskaffold 2.11.1\n`\n```\nI also get the same thing with this command running on Google Cloud Shell and the `gcloud info --run-diagnostics` reports no issues.\nThe build actually succeeds and generates a new image in the artifact registry that is properly tagged with `latest`. However, it doesn't actually switch the image used by the Cloud Run Jobs entry and due to my cleanup rules that image gets removed and then Cloud Run Jobs begin failing with \"image not found\".\nFinally, if I lookup the built image from the artifact registry and then run `gcloud run jobs deploy surfey-bin --image` ... it works!?",
      "solution": "I was facing the same issue.\nI downgraded to version 472 and it worked.\n```\n`gcloud components update --version 472.0.0\n`\n```\nSeems a bug in their latest(474 as of now) version",
      "question_score": 3,
      "answer_score": 6,
      "created_at": "2024-05-03T00:20:58",
      "url": "https://stackoverflow.com/questions/78421956/gcloud-run-jobs-deploy-is-crashing-w-can-only-concatenate-str-not-tuple-to"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 74490465,
      "title": "Github Actions Failing for setup-gcloud",
      "problem": "My previous deployments with same github workflow file were successful.\nSuddenly today, I get this error in Github Actions while trying to deploy.\nMay I know how to fix this?\n```\n`Run google-github-actions/setup-gcloud@v0\n24\n/usr/bin/tar xz --warning=no-unknown-keyword --overwrite -C /home/runner/work/_temp/fa0cd935-fe7e-4593-8662-69259b4b00a0 -f /home/runner/work/_temp/52901a76-e32d-4cdf-92e4-83836f8c5362\n25\nWarning: \"service_account_key\" has been deprecated. Please switch to using google-github-actions/auth which supports both Workload Identity Federation and Service Account Key JSON authentication. For more details, see https://github.com/google-github-actions/setup-gcloud#authorization\n26\nError: google-github-actions/setup-gcloud failed with: failed to execute command `gcloud --quiet auth activate-service-account *** --key-file -`: /opt/hostedtoolcache/gcloud/270.0.0/x64/lib/googlecloudsdk/core/console/console_io.py:544: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n27\n  if answer is None or (answer is '' and default is not None):\n28\n/opt/hostedtoolcache/gcloud/270.0.0/x64/lib/third_party/ipaddress/__init__.py:1106: SyntaxWarning: 'str' object is not callable; perhaps you missed a comma?\n29\n  raise TypeError(\"%s and %s are not of the same version\" (a, b))\n30\nERROR: gcloud failed to load: module 'collections' has no attribute 'MutableMapping'\n31\n    gcloud_main = _import_gcloud_main()\n32\n    import googlecloudsdk.gcloud_main\n33\n    from googlecloudsdk.calliope import base\n34\n    from googlecloudsdk.calliope import display\n35\n    from googlecloudsdk.calliope import display_taps\n36\n    from googlecloudsdk.core.resource import resource_printer_base\n37\n    from googlecloudsdk.core.resource import resource_projector\n38\n    from google.protobuf import json_format as protobuf_encoding\n39\n    from google.protobuf import symbol_database\n40\n    from google.protobuf import message_factory\n41\n    from google.protobuf import reflection\n42\n    from google.protobuf.internal import python_message as message_impl\n43\n    from google.protobuf.internal import containers\n44\n    MutableMapping = collections.MutableMapping\n45\n\n46\nThis usually indicates corruption in your gcloud installation or problems with your Python interpreter.\n47\n\n48\nPlease verify that the following is the path to a working Python 2.7 executable:\n49\n    /usr/bin/python\n50\n\n51\nIf it is not, please set the CLOUDSDK_PYTHON environment variable to point to a working Python 2.7 executable.\n52\n\n53\nIf you are still experiencing problems, please reinstall the Cloud SDK using the instructions here:\n54\n    https://cloud.google.com/sdk/\n`\n```",
      "solution": "I fixed it by using below lines in workflow yml before `uses: google-github-actions/setup-gcloud@v0`\n```\n`- run: |\n    sudo apt-get install python2.7\n    export CLOUDSDK_PYTHON=\"/usr/bin/python2\"\n`\n```",
      "question_score": 3,
      "answer_score": 3,
      "created_at": "2022-11-18T14:33:15",
      "url": "https://stackoverflow.com/questions/74490465/github-actions-failing-for-setup-gcloud"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 71716824,
      "title": "GCP Workflows: Easy way to re-run failed execution",
      "problem": "Given failed workflow execution (identified by workflow ID and execution ID),\nI need a relatively quick and easy way to re-run it using CLI tools (`gcloud`), with the same input payload.\nAsking Google and searching over stackoverflow didn't bring me any easy way to do this.",
      "solution": "While the UI offers easy ways to rerun an execution, `gcloud` doesn't have a shortcut (yet). You could build one with a bit of shell scripting, retrieving the previous arguments and passing them to a new execution:\n`#/bin/sh\n# Usage: ./rerun.sh LOCATION WORKFLOW EXECUTION_ID\nDATA=`gcloud workflows executions describe $3 --location $1 --workflow $2 | grep \"^argument:\" | cut -f2 -d' ' | sed -e \"s/^'//\" -e \"s/'$//\"`\ngcloud workflows run $2 --location $1 --data=$DATA\n`",
      "question_score": 3,
      "answer_score": 6,
      "created_at": "2022-04-02T12:39:00",
      "url": "https://stackoverflow.com/questions/71716824/gcp-workflows-easy-way-to-re-run-failed-execution"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 70778304,
      "title": "&quot;ERROR: (gcloud.functions.call) ResponseError: status=[404], code=[Ok], message=[Function XYZ in region us-central1 in project ABC does not exist]&quot;",
      "problem": "I am testing a cloud function.\nWhen I run it from the \"Testing\" tab, online, and I paste the json dictionary as the argument to be passed as the request variable of the cloud function in the \"Triggering event\" editor as:\n```\n`{\"api_key\":\"MY_API_KEY\"}\n`\n```\n(with MY_API_KEY to be replaced with a plain text key) then the function runs through.\nFrom bash, using `gcloud` instead, the same function fails with:\n```\n`gcloud functions call MY_CLOUD_FUNCTION --data '{\"api_key\":\"$API_KEY\"}'\n`\n```\n\n`gcloud functions call MY_CLOUD_FUNCTION --data '{\"api_key\":\"$API_KEY\"}' \nERROR: (gcloud.functions.call) ResponseError:\nstatus=[404], code=[Ok], message=[Function MY_CLOUD_FUNCTION in region\nus-central1 in project MY_CLOUD_PROJECT does not exist]\n`\n\nAnd this is most likely not a problem of the triggering event which just happens to be part of the args but should not play a role here. It is a problem of the different regions. `gcloud` expects the default region `us-central1` while my function is `europe-west3` (from the \"Details\" tab).\nHow can I tell `gcloud` to use the region of the function, and not the default region? Or what would be another way to fix this?",
      "solution": "`gcloud` assumes defaults (that may be get|set using `gcloud config`)\nIn this case, you should add the flag `--region=europe-west3` to your command to specify the intended region, i.e.:\n`REGION=\"europe-west3\"\n\ngcloud functions call your-function \\\n--region=${REGION} \\\n--data=\"{\\\"api_key\\\":\\\"${API_KEY}\\\"}\"\n`\nYou can verify this behavior (hopefully) by:\n`gcloud config list\n[core]\naccount = your@email.com\n\nYour active configuration is: [default]\n`\n\nNOTE The above command does not include `functions/region`; I suspect this is because `us-central1` is the default (!) default value.\n\n`gcloud config get-value functions/region\nus-central1\n`\nI discourage storing global state in `gcloud config` (partly because it results in this type of confusion) but, you may also:\n`gcloud config set functions/region europe-west3\nUpdated property [functions/region].\n\ngcloud config get-value functions/region\neurope-west3\n\ngcloud functions call your-function \\\n--data=\"{\\\"api_key\\\":\\\"${API_KEY}\\\"}\"\n`",
      "question_score": 3,
      "answer_score": 4,
      "created_at": "2022-01-19T23:00:32",
      "url": "https://stackoverflow.com/questions/70778304/error-gcloud-functions-call-responseerror-status-404-code-ok-message"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 67472755,
      "title": "Gcloud command for listing notification channels not working with filter",
      "problem": "I am trying to filter the existing channels created for stack-driver alert notifications using `gcloud` command based on the displayName. The channels are of type emails and webhook and below is structure of email notification channel:\n```\n`creationRecord:\n  mutateTime: '2021-03-16T14:28:59.926805618Z'\ndisplayName: 'Test Notifications Channel'\nenabled: true\nlabels:\n  email_address: 1234fcb0.XXXPortal.onmicrosoft.com@ayd.teams.ms\nmutationRecords:\n- mutateTime: '2021-03-16T14:28:59.926805618Z'\nname: projects/xxx/notificationChannels/13657854696054677020\ntype: email\n`\n```\nI am using the following Gcloud command to list this channel to find out whether it actually exists.\n```\n`gcloud alpha monitoring channels list --filter='displayName=\"Test Notifications Channel\"' --format='value(name)' --project=xxx\n`\n```\nThe output is:\n\nWARNING: The following filter keys were not present in any resource : displayName\n\nAlso the beta version of the command gives the same result. I need to find out if the channel exists and by the displayName.\nNote: in the --filter='type=\"email\"' is working, but I don't require that.\nWhich gcloud command and filter can I use to solve this issue?\nUpdate\nThanks for your responses below, i find that the filter is indeed working for the above code, as rightly pointed out there is some trailing space. What I have been actually trying is the\nthe displayName is consisting of Test Notifications Channel Default.\nBut in the filter I have given only, omitting the Default:\n```\n`gcloud alpha monitoring channels list --filter='displayName=\"Test Notifications Channel\"' --format='value(name)' --project=xxx\n`\n```\nbut my requirement is to print all the channels starting with the displayName Test Notifications Channel so I want something like this:\n```\n`gcloud alpha monitoring channels list --filter='displayName=\"Test Notifications Channel*\"' --format='value(name)' --project=xxx\n`\n```",
      "solution": "```\n`gcloud alpha monitoring channels list --filter='displayName:\"Test Notifications Channel\"' --format='value(name)' --project=xxx\n`\n```\nThis command gives all the Test Notifications like:\nTest Notifications Channel Default\nTest Notifications Channel Non-Default",
      "question_score": 3,
      "answer_score": 6,
      "created_at": "2021-05-10T16:55:34",
      "url": "https://stackoverflow.com/questions/67472755/gcloud-command-for-listing-notification-channels-not-working-with-filter"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 73399536,
      "title": "GCloud Coud SQL Postgres: &quot;ERROR: permission denied for table &lt;table&gt;&quot;",
      "problem": "I have a postgres database in Google Cloud SQL which was created by the user \"api\". Now I would like to read some of the data on that database using my own user. I can connect to the database but when I run\n```\n`SELECT * FROM data;\n`\n```\nI get the error:\n```\n`ERROR:  permission denied for table data\n`\n```\n\nEDIT\nI tried `GRANT ALL PRIVILEGES ON DATABASE website TO sev;` but that didn't work. The SQL defining the databse looks like this:\n```\n`CREATE DATABASE website\n    WITH \n    OWNER = cloudsqlsuperuser\n    ENCODING = 'UTF8'\n    LC_COLLATE = 'en_US.UTF8'\n    LC_CTYPE = 'en_US.UTF8'\n    TABLESPACE = pg_default\n    CONNECTION LIMIT = -1;\n\nGRANT ALL ON DATABASE website TO cloudsqlsuperuser;\n\nGRANT TEMPORARY, CONNECT ON DATABASE website TO PUBLIC;\n\nGRANT ALL ON DATABASE website TO sev;\n`\n```",
      "solution": "The `GRANT` must be run by the owning user `api`.\nGranting privileges on the database is not enough. To access a table, you must have privileges on the table itself as well as privileges on the schema that contains the table.",
      "question_score": 3,
      "answer_score": 4,
      "created_at": "2022-08-18T10:10:03",
      "url": "https://stackoverflow.com/questions/73399536/gcloud-coud-sql-postgres-error-permission-denied-for-table-table"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 69302528,
      "title": "How to pass environment variables to gcloud beta ai custom-jobs create with custom container (Vertex AI)",
      "problem": "I'm running custom training jobs in google's Vertex AI. A simple `gcloud` command to execute a custom job would use something like the following syntax (complete documentation for the command can be seen here):\n```\n`gcloud beta ai custom-jobs create --region=us-central1 \\\n--display-name=test \\\n--config=config.yaml\n`\n```\nIn the `config.yaml` file, it is possible to specify the machine and accelerator (GPU) types, etc., and in my case, point to a custom container living in the Google Artifact Registry that executes the training code (specified in the `imageUri` part of the `containerSpec`). An example config file may look like this:\n```\n`# config.yaml\nworkerPoolSpecs:\n  machineSpec:\n    machineType: n1-highmem-2\n    acceleratorType: NVIDIA_TESLA_P100\n    acceleratorCount: 2\n  replicaCount: 1\n  containerSpec:\n    imageUri: {URI_FOR_CUSTOM_CONATINER}\n    args:\n    - {ARGS TO PASS TO CONTAINER ENTRYPOINT COMMAND}\n`\n```\nThe code we're running needs some runtime environment variables (that need to be secure) passed to the container. In the API documentation for the `containerSpec`, it says it is possible to set environment variables as follows:\n```\n`# config.yaml\nworkerPoolSpecs:\n  machineSpec:\n    machineType: n1-highmem-2\n    acceleratorType: NVIDIA_TESLA_P100\n    acceleratorCount: 2\n  replicaCount: 1\n  containerSpec:\n    imageUri: {URI_FOR_CUSTOM_CONATINER}\n    args:\n    - {ARGS TO PASS TO CONTAINER ENTRYPOINT COMMAND}\n    env:\n    - name: SECRET_ONE\n      value: $SECRET_ONE\n    - name: SECRET_TWO\n      value: $SECRET_TWO\n`\n```\nWhen I try and add the `env` flag to the `containerSpec`, I get an error saying it's not part of the container spec:\n```\n`ERROR: (gcloud.beta.ai.custom-jobs.create) INVALID_ARGUMENT: Invalid JSON payload received. Unknown name \"env\" at 'custom_job.job_spec.worker_pool_specs[0].container_spec': Cannot find field.\n- '@type': type.googleapis.com/google.rpc.BadRequest\n  fieldViolations:\n  - description: \"Invalid JSON payload received. Unknown name \\\"env\\\" at 'custom_job.job_spec.worker_pool_specs[0].container_spec':\\\n      \\ Cannot find field.\"\n    field: custom_job.job_spec.worker_pool_specs[0].container_spec\n`\n```\nAny idea how to securely set runtime environment variables in Vertex AI custom jobs using custom containers?",
      "solution": "There are two versions of the REST API - \u201cv1\u201d and \u201cv1beta1\u201d where \"v1beta1\" does not have the `env` option in `ContainerSpec` but \"v1\" does. The `gcloud ai custom-jobs create` command without the `beta` parameter doesn\u2019t throw the error as it uses version \u201cv1\u201d to make the API calls.\nThe environment variables from the yaml file can be passed to the custom container in the following way:\nThis is the docker file of the sample custom training application I used to test the requirement. Please refer to this codelab for more information about the training application.\n`FROM gcr.io/deeplearning-platform-release/tf2-cpu.2-3\nWORKDIR /root\n\nWORKDIR /\n\n# Copies the trainer code to the docker image.\nCOPY trainer /trainer\n\n# Copies the bash script to the docker image.\nCOPY commands.sh /scripts/commands.sh\n\n# Bash command to make the script file an executable\nRUN [\"chmod\", \"+x\", \"/scripts/commands.sh\"]\n\n# Command to execute the file\nENTRYPOINT [\"/scripts/commands.sh\"]\n\n# Sets up the entry point to invoke the trainer.\n# ENTRYPOINT \"python\" \"-m\" $SECRET_TWO \u21d2 To use the environment variable  \n# directly in the docker ENTRYPOINT. In case you are not using a bash script, \n# the trainer can be invoked directly from the docker ENTRYPOINT.\n`\n\nBelow is the `commands.sh` file used in the docker container to test whether the environment variables are passed to the container.\n`#!/bin/bash\nmkdir /root/.ssh\necho $SECRET_ONE\npython -m $SECRET_TWO\n`\n\nThe example `config.yaml` file\n`# config.yaml\nworkerPoolSpecs:\n  machineSpec:\n    machineType: n1-highmem-2\n  replicaCount: 1\n  containerSpec:\n    imageUri: gcr.io/infosys-kabilan/mpg:v1\n    env:\n    - name: SECRET_ONE\n      value: \"Passing the environment variables\"\n    - name: SECRET_TWO\n      value: \"trainer.train\"\n`\nAs the next step, I built and pushed the container to Google Container Repository. Now, the `gcloud ai custom-jobs create --region=us-central1  --display-name=test --config=config.yaml` can be run to create the custom training job and the output of the `commands.sh` file can be seen in the job logs as shown below.",
      "question_score": 3,
      "answer_score": 4,
      "created_at": "2021-09-23T17:03:34",
      "url": "https://stackoverflow.com/questions/69302528/how-to-pass-environment-variables-to-gcloud-beta-ai-custom-jobs-create-with-cust"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 72062448,
      "title": "Service Account Unable to Push Docker Image to Google Artifact Registry",
      "problem": "I am trying to `push` a Docker image to Google Artifact Registry (GAR) while impersonating a Service Account (`$SERV_ACCT_EMAIL`):\n\ndenied: Permission \"artifactregistry.repositories.downloadArtifacts\" denied on resource \"projects/$GCP_PROJECT_ID/locations/us-west1/repositories/$GAR_REPOSITORY\" (or it may not exist)\n\n`$SERV_ACCT_EMAIL` has Artifact Registry Writer (`roles/artifactregistry.writer`) and Artifact Registry Reader (`roles/artifactregistry.reader`) roles; the latter of which has the permission `artifactregistry.repositories.downloadArtifacts`. Thus, if the resource is granted access to `$SERV_ACCT_EMAIL`, I believe I will indeed be able to `push` those artifacts.\nHow do I `push` to GAR while impersonating `$SERV_ACCT_EMAIL`?",
      "solution": "You may need to `configure-docker` while impersonating your service account (`$SERVICE_ACCOUNT_EMAIL`):\n\nCheck to make sure you are still impersonating `$SERVICE_ACCOUNT_EMAIL`:\n`gcloud auth list\n\n#=>\n\nCredentialed Accounts\nACTIVE  ACCOUNT\n        . . .\n*       $SERVICE_ACCOUNT_EMAIL\n        . . .\n`\nif not:\n`gcloud auth activate-service-account \\\n\"$SERVICE_ACCOUNT_EMAIL\" \\\n--key-file=$SERVICE_ACCOUNT_JSON_FILE_PATH\n\n#=>\n\nActivated service account credentials for: [$SERVICE_ACCOUNT_EMAIL]\n`\n\nRun the `configure-docker` command against the `auth` group:\n`gcloud auth configure-docker us-west1-docker.pkg.dev\n\n#=>\n\n. . .\nAdding credentials for: us-west1-docker.pkg.dev\n. . .\n`\n\nFinally, try `push`ing the Docker image again.",
      "question_score": 3,
      "answer_score": 3,
      "created_at": "2022-04-29T20:43:37",
      "url": "https://stackoverflow.com/questions/72062448/service-account-unable-to-push-docker-image-to-google-artifact-registry"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 70743101,
      "title": "Unable to build in parallel using the Googles Gcloud Bazel builder",
      "problem": "In GCP building bazel based stuff on a `cloudbuild.yaml`, I am using the `waitFor: ['-']` keyword to build in parallel and am using the bazel  gcr.io/cloud-builders/bazel -builder. When I am trying to build multiple steps with the above builder and using the `waitFor: ['-']` This would create a parallel build. The issue is when trying this way, I get the error as shown below and the build fails, however when I remove the `waitFor: ['-']` keyword, the building occurs sequentially and the build goes through successfully. Is there any Bazel configuration I must change in the Gcloud's Bazel builder? Error is shown below while building in parallel:\n```\n`Another command holds the client lock: \npid=12\nowner=client\ncwd=/workspace\n\nWaiting for it to complete...\nAnother command holds the client lock: \npid=13\nowner=client\ncwd=/workspace\n\nWaiting for it to complete...\nStarting local Bazel server and connecting to it...\n`\n```\nMy cloudbuild.yaml looks like this below:\n```\n`steps:\n\n-   name: \"gcr.io/cloud-builders/bazel\"\n    id: \"Building Bazel components and Uploading the component manifest for ml_cmp_1 \"\n    entrypoint: \"bash\"\n    args:\n        - \"-c\"\n        - |\n            cmp_dir=components/train_test_split_1\n            cmp_bazel_file=\"$cmp_dir/BUILD.bazel\"\n            bazel run --remote_cache=${_BAZEL_CACHE_URL} --google_default_credentials --define=PROJ_ID=${_PROJECT_ID} //$cmp_dir:container_push\n\n    waitFor: [\"-\"]\n\n-   name: \"gcr.io/cloud-builders/bazel\"\n    id: \"Building Bazel components and Uploading the component manifest for ml_cmp_2 \"\n    entrypoint: \"bash\"\n    args:\n        - \"-c\"\n        - |\n            cmp_dir=components/train_test_split_2\n            cmp_bazel_file=\"$cmp_dir/BUILD.bazel\"\n            bazel run --remote_cache=${_BAZEL_CACHE_URL} --google_default_credentials --define=PROJ_ID=${_PROJECT_ID} //$cmp_dir:container_push\n\n    waitFor: [\"-\"]\n\ntimeout: 86399s\nlogsBucket: gs://some_project_id_cloudbuild/logs\noptions:\n  machineType: 'N1_HIGHCPU_8'\n  substitution_option: 'ALLOW_LOOSE'\n\nsubstitutions:\n    _PROJECT_ID: \"some_project_id\"\n    _BAZEL_CACHE_URL: \"https://storage.googleapis.com/some_project_id_cloudbuild/bazel-cache\"\n`\n```",
      "solution": "If you want to run multiple bazel commands in parallel, they each need their own --output_base flag. That will make each build independent.\nIf you want them to share intermediate outputs within the same build machine, a shared --disk_cache is the simplest approach. You could also set up a full remote cache which would cache outputs across build machines. If you want parallel builds to deduplicate common actions fully, you'll need to set up remote execution.",
      "question_score": 3,
      "answer_score": 3,
      "created_at": "2022-01-17T15:38:33",
      "url": "https://stackoverflow.com/questions/70743101/unable-to-build-in-parallel-using-the-googles-gcloud-bazel-builder"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 67716846,
      "title": "ERROR: (gcloud.app.deploy) Error Response: [9] Flex operation projects/.../regions/us-central1/operations/... error [FAILED_PRECONDITION]",
      "problem": "I'm pretty new on Google Cloud, and I just wanted to deploy my first streamlit webapp. I'm on Windows in command line. I already did the Google Cloud \"Hello World\" Example, which worked without any error.\nWhen I deploy the streamlit webapp, I got after 3-4 minutes waiting \"Updating Server\" the following error:\n```\n`ERROR: (gcloud.app.deploy) Error Response: [9] Flex operation projects/XXXX/regions/us-central1/operations/f0c89d22-2d09-410d-bf99-fc49ad337800 error [FAILED_PRECONDITION]: An internal error occurred while processing task /app-engine-flex/flex_await_healthy/flex_await_healthy>2021-05-27T06:13:50.278Z10796.jc.0: 2021-05-27 06:15:32.787 An update to the [server] config option section was detected. To have these changes be reflected, please restart streamlit.\n`\n```\nThat's my app.yaml file:\n```\n`service: default\nruntime: custom\nenv: flex\nmanual_scaling: \n  instances: 1\nresources:\n  cpu: 1\n  memory_gb: 0.5\n  disk_size_gb: 10\n`\n```",
      "solution": "Posting my comment as an answer for better visibility and to summarize.\nIn this particular case, the error was caused by a mistake in the Dockerfile.\nHere are some steps you can follow to fix or narrow down the error:\n\nTry to deploy a test app to see the differences in configuration. Example.\nTry deploying your app after updating the gcloud with `gcloud components update` command.\nMake sure you run the SDK as an Admin.\nIf the error recurs, run the `gcloud app deploy app.yaml --verbosity=debug` to try getting more specified error.",
      "question_score": 3,
      "answer_score": 2,
      "created_at": "2021-05-27T08:34:49",
      "url": "https://stackoverflow.com/questions/67716846/error-gcloud-app-deploy-error-response-9-flex-operation-projects-regio"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 67411615,
      "title": "Port forwarding to connect to Memorystore for Redis from local machine failing",
      "problem": "I am trying to connect to my Redis instance from my from my local machine by following this guide, where you create a Compute Engine instance to use for port forwarding to the Redis instance.\nI was able to create the Compute Engine instance using:\n`gcloud compute instances create redis-port-forward-vm --machine-type=f1-micro --zone=us-east1-d`.\nWhen I try to create an SSH tunnel that port forwards traffic through the Compute Engine VM using: \n`gcloud compute ssh redis-port-forward-vm --zone=us-east1-d -- -N -L 6379:REDIS_INSTANCE_IP_ADDRESS:6379`.\nI get the following error:\n`channel 2: open failed: connect failed: Connection timed out`.\nI don't understand what could be the issue, I am able to successfully SSH into the Compute Engine instance, but port forwarding is not working.",
      "solution": "Follow the below steps,\n\nInstall redis-cli on the Compute Engine VM by running the following command from the redis-port-forward-vm SSH terminal:\n`sudo apt-get install redis-server`\nCreate a Redis instance if not created already and check the port number of the Redis instance you created (it is 6379 by default but for me it was 6378).\nRun the following commands on your local machine terminal\n`gcloud compute ssh redis-port-forward-vm --zone=us-east1-d`\n`redis-cli -h REDIS INSTANCE IP -p PORT NUMBER`\nTo test the connection, open a new terminal window and run the following command:\n`redis-cli ping`",
      "question_score": 3,
      "answer_score": 3,
      "created_at": "2021-05-06T05:53:31",
      "url": "https://stackoverflow.com/questions/67411615/port-forwarding-to-connect-to-memorystore-for-redis-from-local-machine-failing"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 66565360,
      "title": "Set HTTPS flag in a Cloud Function with in a gcloud deploy instrucction",
      "problem": "I'm trying to deploy a GCP Cloud Function\n\ngcloud functions deploy MyFuntion --set-env-vars\nEXPRESSPORT=0000,CODEENV=PRE --region=europe-west1 --entry-point MyPoint\n--runtime nodejs12 --trigger-http --allow-unauthenticated --ingress-settings=all --memory=256 --timeout=30\n\nBut I can't find the flag to set the \"Required HTTPS\" that you can set in the Web Console:\n\nDoes anyone know the instruction?, I can't find it in de Google Cloud documentation.\nThanks in advance.",
      "solution": "https://cloud.google.com/functions/docs/writing/http#security_levels\n`gcloud beta functions deploy \\\n--security-level=secure-always \\\n...\n`\n\nNOTE `gcloud beta`\nhttps://cloud.google.com/sdk/gcloud/reference/beta/functions/deploy#--security-level",
      "question_score": 3,
      "answer_score": 3,
      "created_at": "2021-03-10T14:04:00",
      "url": "https://stackoverflow.com/questions/66565360/set-https-flag-in-a-cloud-function-with-in-a-gcloud-deploy-instrucction"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 76731322,
      "title": "How can I find available ip ranges in a gcloud VPC?",
      "problem": "I have to specify a subnet in order to allow Google's Datastream to connect with a source database but every single subnet I specify gives me the error:\n```\n`Error: Error creating PrivateConnection: googleapi: Error 400: The IP range specified (10...0/29) overlaps with a reserved IP range. Provide a different IP range.\ncom.google.apps.framework.request.StatusException:  generic::INVALID_ARGUMENT: The IP range specified (10...0/29) overlaps with a reserved IP range. Provide a different IP range.\n`\n```\nIt would be helpful to find out all the reserved ip ranges so I can avoid them but I don't think that is even possible?",
      "solution": "The Google Cloud Console GUI shows networking console which shows details on IP address allocation to each subnet.\nThe CLI command will show you details on networking documentation:\n```\n`gcloud compute networks subnets list\n`\n```\nSee this answer for more details on reserved IP addresses that also might affect you.",
      "question_score": 3,
      "answer_score": 2,
      "created_at": "2023-07-20T17:12:19",
      "url": "https://stackoverflow.com/questions/76731322/how-can-i-find-available-ip-ranges-in-a-gcloud-vpc"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 76316307,
      "title": "Issue deploying my Node app to GCP (Google Cloud): Cannot find module and version has no instances",
      "problem": "Context:\nI'm having an issue when I deploy my node app to GCP app engine.\nThe deployment has worked in the past, but I've not touched the code for a few months.\nThe deployment doesn't error, but when I click on the GCP url I get a 500 server error.\nI noticed that after I deployed my version didn't have any instances and I believe it should of had two instances.\nMessage:\n\nError: Server Error The server encountered an error and could not\ncomplete your request. Please try again in 30 seconds.\n\nI've taken a look at the logs which show the following:\n\nTech Stack:\nNodeJs, NestJs, Express, GCP, Typescript\nError Message:\nError: Cannot find module '/workspace/dist/main'\nHere is some more of the logs:\n\nI'm not sure if theirs some permissions issue in i am that I need to add.\nI did receive an email about moving my app from Container Registry to Artifact Registry.\nSo I'm not sure if thats effecting it or what I need to do.\nOutcome:\nIdeally like to understand why the server has a 500 error. The log messages are not helpful.\nDist folder after a nest build command run:\n\nDeployment Steps:\n\nI run npm run set-project:staging to select the correct app engine to deploy to.\n\nThen I do nest build.\n\nThen I do gcloud app deploy.\n\nMy app.yaml file:\n```\n`runtime: nodejs16\nenv: standard\ninstance_class: F2\n\nenv_variables:\n  GOOGLE_NODE_RUN_SCRIPTS: ''\n`\n```\nPackage json:\n```\n`{\n  \"name\": \"backend-app\",\n  \"version\": \"0.0.2\",\n  \"description\": \"\",\n  \"author\": \"\",\n  \"private\": true,\n  \"license\": \"UNLICENSED\",\n  \"main\": \"dist/main.js\",\n  \"scripts\": {\n    \"gcp-build\": \"\",\n    \"prebuild\": \"rimraf dist\",\n    \"local-build\": \"nest build\",\n    \"clean\": \"gts clean\",\n    \"format\": \"prettier --write \\\"src/**/*.ts\\\" \\\"test/**/*.ts\\\"\",\n    \"start\": \"nest start\",\n    \"start:prod\": \"node dist/main\",\n    \"start:dev\": \"nest start --watch\",\n    \"lint\": \"eslint \\\"{src,apps,libs,test}/**/*.ts\\\" --fix\",\n    \"set-project:staging\": \"gcloud config set project hidden-staging\",\n    \"deploy:staging\": \"npm run set-project:staging && npm run local-build && gcloud app deploy\",\n    \"deploy:functions:staging\": \"firebase use staging && firebase deploy --only functions\",\n    \"gcloud-login\": \"gcloud auth application-default login\"\n  },\n  \"engines\": {\n    \"node\": \"16.x\"\n  },\n  \"dependencies\": {\n    \"@google-cloud/storage\": \"^6.2.0\",\n    \"@google-cloud/text-to-speech\": \"^4.0.0\",\n    \"@nestjs/cli\": \"^8.0.0\",\n    \"@nestjs/common\": \"^8.0.0\",\n    \"@nestjs/core\": \"^8.0.0\",\n    \"@nestjs/jwt\": \"^8.0.0\",\n    \"@nestjs/passport\": \"^8.2.1\",\n    \"@nestjs/platform-express\": \"^8.0.0\",\n    \"@nestjs/swagger\": \"^5.2.0\",\n    \"@nestjs/throttler\": \"^2.0.1\",\n    \"@ntegral/nestjs-sendgrid\": \"^1.0.0\",\n    \"@sendgrid/mail\": \"^7.6.1\",\n    \"@types/passport\": \"^1.0.7\",\n    \"cookie-parser\": \"^1.4.6\",\n    \"csurf\": \"^1.11.0\",\n    \"firebase\": \"^9.6.10\",\n    \"firebase-admin\": \"^10.0.2\",\n    \"firebase-functions\": \"^3.18.1\",\n    \"passport\": \"^0.5.2\",\n    \"passport-firebase-jwt\": \"^1.2.1\",\n    \"pdf-puppeteer\": \"^1.1.10\",\n    \"puppeteer\": \"^15.5.0\",\n    \"raygun\": \"^0.13.2\",\n    \"reflect-metadata\": \"^0.1.13\",\n    \"request\": \"^2.88.2\",\n    \"rimraf\": \"^3.0.2\",\n    \"rxjs\": \"^7.2.0\",\n    \"swagger-ui-express\": \"^4.3.0\"\n  },\n  \"devDependencies\": {\n    \"@nestjs/schematics\": \"^8.0.0\",\n    \"@nestjs/testing\": \"^8.0.0\",\n    \"@types/cookie-parser\": \"^1.4.2\",\n    \"@types/express\": \"^4.17.13\",\n    \"@types/jest\": \"^27.0.1\",\n    \"@types/node\": \"^16.0.0\",\n    \"@types/supertest\": \"^2.0.11\",\n    \"@typescript-eslint/eslint-plugin\": \"^4.28.2\",\n    \"@typescript-eslint/parser\": \"^4.28.2\",\n    \"eslint\": \"^7.30.0\",\n    \"eslint-config-prettier\": \"^8.3.0\",\n    \"eslint-plugin-prettier\": \"^3.4.0\",\n    \"jest\": \"^27.0.6\",\n    \"prettier\": \"^2.3.2\",\n    \"supertest\": \"^6.1.3\",\n    \"ts-jest\": \"^27.0.3\",\n    \"ts-loader\": \"^9.2.3\",\n    \"ts-node\": \"^10.0.0\",\n    \"tsconfig-paths\": \"^3.10.1\",\n    \"typescript\": \"^4.3.5\"\n  },\n  \"jest\": {\n    \"moduleFileExtensions\": [\n      \"js\",\n      \"json\",\n      \"ts\"\n    ],\n    \"rootDir\": \"src\",\n    \"testRegex\": \".*\\\\.spec\\\\.ts$\",\n    \"transform\": {\n      \"^.+\\\\.(t|j)s$\": \"ts-jest\"\n    },\n    \"collectCoverageFrom\": [\n      \"**/*.(t|j)s\"\n    ],\n    \"coverageDirectory\": \"../coverage\",\n    \"testEnvironment\": \"node\"\n  }\n}\n`\n```\nproject.toml\n```\n`[[build.env]]\nname = \"GOOGLE_NODE_RUN_SCRIPTS\"\nvalue = \"\"\n`\n```\nCurrent Error:\n\nSeems to only say 2 files for upload in the terminal?\n\n500 error in browser and no instance in version :",
      "solution": "Try the following command to the package json script:\n```\n`\"gcp-build\": \"nest build\"\n`\n```\nThe documentation states to have gcp-build as an empty string.\nHowever, with a nest build we must add nest build inside the empty string.\nOriginal resource from the chat above: https://cloud.google.com/appengine/docs/standard/nodejs/release-notes#April_11_2023\nThe app.yaml file:\n```\n`runtime: nodejs16\nenv: standard\ninstance_class: F2\n`\n```",
      "question_score": 3,
      "answer_score": 2,
      "created_at": "2023-05-23T17:17:45",
      "url": "https://stackoverflow.com/questions/76316307/issue-deploying-my-node-app-to-gcp-google-cloud-cannot-find-module-and-versio"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 74219123,
      "title": "Property &#39;value&#39; does not exist on type &#39;SecretParam&#39; of gcloud param defineSecret in typescript",
      "problem": "I am writing a firestore function for which I want to set some secrets.\nI read in the firebase documentation how to set up a secret param:\n```\n`https://firebase.google.com/docs/functions/config-env#secret_parameters\n`\n```\nso I tried it that way:\n```\n`import * as functions from \"firebase-functions\";\nimport {defineSecret} from 'firebase-functions/params';\nconst someToken = defineSecret(\"TOKEN_NAME\");\n\nimport * as express from \"express\";\nimport {bootstrap, services} from \"./bootstrap\";\n\nexport const app = express();\n\napp.use(express.json());\n\napp.get(\"/\", (req, res) => {\n  res.sendStatus(200);\n});\n\napp.post(\"/\", async (req, res) => {\n  if (!req.body) {\n    res.send(\"json body missing!\");\n    return;\n  }\n\n  await bootstrap(someToken.value());\n\n  services.useGraphQL?.getEntity(req.body).then(\n      (response) => {\n        res.status(200).send(response);\n      },\n      () => {\n        console.error(`Error while ${req.body.command}`);\n        res.sendStatus(500);\n      });\n}\n);\n\nexport default functions\n    .runWith({secrets: [\"TOKEN_NAME\"]})\n    .https.onRequest(app);\n`\n```\nMy problem / question is, that \"someToken.value()\" throws the error \"Property 'value' does not exist on type 'SecretParam' \". So what am I doing wrong? What can I do to do it correctly?\nI tried to read the value of the Object differently, but everything else resolved in more type errors.",
      "solution": "When accessing secrets they are added as environment variables for the cloud function. They can be accessed through `process.env.MY_SECRET` in this example.\nAlso, have you JSON serialised the `someToken` object and logged it? That would enabled you to see what the object structure was and potentially reveal the object key useable if the above doesn't work.",
      "question_score": 3,
      "answer_score": 2,
      "created_at": "2022-10-27T10:40:27",
      "url": "https://stackoverflow.com/questions/74219123/property-value-does-not-exist-on-type-secretparam-of-gcloud-param-definesecr"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 70939685,
      "title": "(gcloud.dataproc.batches.submit.spark) unrecognized arguments: --subnetwork=",
      "problem": "I am trying to submit google dataproc batch job. As per documentation Batch Job, we can pass `subnetwork` as parameter. But when use, it give me\n\nERROR: (gcloud.dataproc.batches.submit.spark) unrecognized arguments:\n--subnetwork=\n\nHere is gcloud command I have used,\n`gcloud dataproc batches submit spark \\\n    --region=us-east4 \\\n    --jars=file:///usr/lib/spark/examples/jars/spark-examples.jar \\\n    --class=org.apache.spark.examples.SparkPi \\\n     --subnetwork=\"https://www.googleapis.com/compute/v1/projects/myproject/regions/us-east4/subnetworks/network-svc\" \\\n    -- 1000\n`",
      "solution": "According to dataproc batches docs, the subnetwork URI needs to be specified using argument `--subnet`.\nTry:\n`gcloud dataproc batches submit spark \\\n    --region=us-east4 \\\n    --jars=file:///usr/lib/spark/examples/jars/spark-examples.jar \\\n    --class=org.apache.spark.examples.SparkPi \\\n    --subnet=\"https://www.googleapis.com/compute/v1/projects/myproject/regions/us-east4/subnetworks/network-svc\" \\\n    -- 1000\n`",
      "question_score": 3,
      "answer_score": 2,
      "created_at": "2022-02-01T12:17:57",
      "url": "https://stackoverflow.com/questions/70939685/gcloud-dataproc-batches-submit-spark-unrecognized-arguments-subnetwork"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 70670084,
      "title": "Configuring the Health Check of a Kubernetes Ingress with Terraform",
      "problem": "We are using an ingress (`kubernetes_ingress.db_admin_ingress`) to expose the service (`kubernetes_service.db_admin`) of a deployment (`kubernetes_deployment.db_admin`) in Google Kubernetes Engine (GKE) with Terraform.\nWhen Terraform creates the ingress, a Level 7 Load Balancer is automatically created with a default health check:\n\nport: 80\npath: `/`\nprotocol: HTTP(S)\n\nOur deployment (`kubernetes_deployment.db_admin`) does not respond to the path `/` with a `200`, so the health check fails.\nHow can we change the path in the health check configuration?\n`resource \"google_compute_managed_ssl_certificate\" \"db_admin_ssl_certificate\" {\n  provider = google-beta\n\n  name = \"db-admin-ssl-certificate\"\n\n  managed {\n    domains = [\"db.${var.domain}.\"]\n  }\n}\n\nresource \"kubernetes_deployment\" \"db_admin\" {\n  metadata {\n    name = \"db-admin\"\n    labels = {\n      App = \"db-admin\"\n    }\n  }\n\n  spec {\n    replicas = 1\n    selector {\n      match_labels = {\n        App = \"db-admin\"\n      }\n    }\n    template {\n      metadata {\n        labels = {\n          App = \"db-admin\"\n        }\n      }\n      spec {\n        container {\n          image = \"dpage/pgadmin4:2022-01-10-1\"\n          name  = \"db-admin\"\n          env {\n            name = \"PGADMIN_DEFAULT_EMAIL\"\n            value = \"test@test.com\"\n          }\n          env {\n            name = \"PGADMIN_DEFAULT_PASSWORD\"\n            value = \"test\"\n          }      \n\n          port {\n            container_port = 80\n          }\n\n          resources {}\n        }\n      }\n    }\n  }\n}\n\nresource \"kubernetes_service\" \"db_admin\" {\n  metadata {\n    name = \"db-admin\"\n  }\n  spec {\n    selector = {\n      App = kubernetes_deployment.db_admin.spec.0.template.0.metadata[0].labels.App\n    }\n    port {\n      protocol    = \"TCP\"\n      port        = 80\n      target_port = 80\n    }\n\n    type = \"NodePort\"\n  }\n}\n\nresource \"kubernetes_ingress\" \"db_admin_ingress\" {\n  wait_for_load_balancer = true\n  \n  metadata {\n    name = \"db-admin-ingress\"\n    annotations = {\n      \"ingress.gcp.kubernetes.io/pre-shared-cert\"   = google_compute_managed_ssl_certificate.db_admin_ssl_certificate.name\n    }\n  }\n\n  spec {\n\n    rule {\n      http {\n        path {\n          backend {\n            service_name = \"db-admin\"\n            service_port = 80\n          }\n\n          path = \"/*\"\n        }\n\n      }\n    }\n\n  }\n}\n`",
      "solution": "According to Google Kubernetes Engine (GKE) official documentation here, you are able to customize `ingress`/Level 7 Load Balancer health checks through either:\n\nthe `readinessProbe` for the `container` within the `pod` your `ingress` is serving traffic to\nWarning: this method comes with warnings here\n\na `backendconfig` resource\n\nI would highly recommend creating a `backendconfig` resource.\nUnfortunately, the `kubernetes` Terraform provider does not seem to support the `backendconfig` resource based on this GitHub issue.  This means that you can either:\n\nuse the `kubernetes-alpha` provider (found here) to transcribe a YAML `backendconfig` manifest to HCL with the `manifest` argument for the only `kubernetes-alpha` resource: `kubernetes-manifest` (more on that here)\nuse an unofficial provider (such as `banzaicloud/k8s` found here)\ncheck the `backendconfig` manifest (as either JSON or YAML) into SCM\n\nA sample `backendconfig` YAML manifest:\n`apiVersion: cloud.google.com/v1\nkind: BackendConfig\nmetadata:\n  name: db-admin\n  namespace: default\nspec:\n  healthCheck:\n    checkIntervalSec: 30\n    timeoutSec: 5\n    healthyThreshold: 1\n    unhealthyThreshold: 2\n    type: HTTP\n    requestPath: /v1/some/path\n    port: 80\n`\nNote: a `service` is needed to associate a `backendconfig` with an `ingress`/Level 7 Load Balancer:\n`apiVersion: v1\nkind: Service\nmetadata:\n  name: db-admin-ingress-backend-config\n  labels:\n    app: db-admin\n  annotations:\n    cloud.google.com/backend-config: '{\"ports\": {\"80\":\"db-admin\"}}'\n    cloud.google.com/neg: '{\"ingress\": true}'\nspec:\n  type: NodePort\n  selector:\n    app: db-admin\n  ports:\n  - port: 80\n    protocol: TCP\n    targetPort: 80\n`\nYou can learn more about the `backendconfig` resource and the `service` it requires here.",
      "question_score": 3,
      "answer_score": 2,
      "created_at": "2022-01-11T17:22:01",
      "url": "https://stackoverflow.com/questions/70670084/configuring-the-health-check-of-a-kubernetes-ingress-with-terraform"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 74784729,
      "title": "Error in submitting a pig job to Google Dataproc with properties file",
      "problem": "I'm new to Dataproc and trying to submit a pig job to google dataproc via gcloud\n```\n`   gcloud config set project PROJECT\n\n  gcloud dataproc jobs submit pig   --cluster=cluster-workaround   --region=us-east4   --verbosity=debug   --properties-file=gs://bucket/cvr_gcs_one.properties --file=gs://bucket-temp/intellibid-intermediat-cvr.pig \n`\n```\nwith below property file\n```\n`jarLocation=gs://bucket-data-science/emr/jars/pig.jar\npigScriptLocation=gs://bucket-data-science/emr/pigs\nlogLocation=gs://bucket-data-science/prod/logs\nudf_path=gs://bucket-data-science/emr/jars/udfs.jar\ncsv_dir=gs://bucket-db-dump/prod\ncurrdate=2022-12-13\ntrain_cvr=gs://bucket-temp/{2022-12-09}\noutput_dir=gs://analytics-bucket/outoout\n`\n```\nand below is the sample of pig script which is uploaded to GCS\n```\n` register $udf_path;\n\n SET default_parallel 300;\n SET pig.exec.mapPartAgg true; -- To remove load on combiner\n\n SET pig.tmpfilecompression TRUE          -- To make Compression true between \n MapReduce Job Mainly when using Joins\n SET pig.tmpfilecompression.codec gz     -- To Specify the type of compression between MapReduce Job\n SET mapreduce.map.output.compress TRUE      --To make Compression true between Map and Reduce\n SET mapreduce.map.output.compress.codec org.apache.hadoop.io.compress.GzipCodec\n set mapred.map.tasks.speculative.execution false\n SET mapreduce.task.timeout 10800000\n set mapreduce.output.fileoutputformat.compress true\n set mapreduce.output.fileoutputformat.compress.codec \n org.apache.hadoop.io.compress.GzipCodec\n SET mapreduce.map.maxattempts 16\n SET mapreduce.reduce.maxattempts 16\n SET mapreduce.job.queuename HIGH_PRIORITY\n\n define GSUM com.java.udfs.common.javaSUM();\n define get_cvr_key com.java.udfs.common.ALL_CTR_MODEL('$csv_dir', 'variableList.ini')\n define multiple_file_generator com.java.udfs.common.CVR_KEY_GENERATION('$csv_dir','newcampaignToKeyMap')\n\n  train_tmp1 = load '$train_cvr/' using PigStorage('\\t','-noschema') as (cookie,AdvID,nviews,ls_dst,ls_src,ls_di,ls_ft,ls_np,tos,nsess,e100_views,e200_views,e300_views,e400_views,e100_tos,e200_tos,e300_tos,e400_tos,uniq_prod,most_seen_prod_freq,uniq_cat,uniq_subcat,search_cnt,click_cnt,cart_cnt,HSDO,os,bwsr,dev,hc_c_v,hc_c_tp,hc_c_up,hc_c_ls,hc_s_v,hc_s_tp,hs_s_up,hc_s_ls,hc_clk_pub,hc_clk_cnt,hc_clk_lm,hp_ls_v,hp_ls_c,hp_ls_s,hp_ms_v,hp_ms_c,hp_ms_s,hu_v,hu_c,hu_s,purchase_flag,hp_ls_cvr,hp_ls_crr,hp_ms_cvr,hp_ms_crr,mpv,gc_c_tp,gc_clk_cnt,gc_c_up,gc_clk_lm,gc_c_v,gc_c_ls,gc_s_v,gc_s_lsts,gc_s_tp,gc_s_up,gc_clk_pub,epoch_ms,gc_ac_s,gc_ac_clk,gc_ac_vclk,udays,hc_vclk_cnt,gc_vclk_cnt,e205_view,e205_tos,AdvID_copy,hc_p_ms_p,hc_c_ms_p,most_seen_cat_freq,hc_p_ls_p,currstage,hc_c_city);\n`\n```\nGetting below error\n```\n`INFO  org.apache.pig.impl.util.Utils - Default bootup file /root/.pigbootup not found\nERROR org.apache.pig.impl.PigContext - Undefined parameter : udf_path\n2022-12-13 11:58:51,504 [main] \nERROR org.apache.pig.Main - ERROR 2997: Encountered IOException. \norg.apache.pig.tools.parameters.ParameterSubstitutionException: Undefined parameter : udf_path\n`\n```\nTried most of the methods using console as well, doesn't get good documentation to go through.\nAnd Whats Exactly the difference between Query parameters Field(Specify the parameter names and values to insert in place of parameter entries in the query file. The query uses those values when it runs.) and Property Field(A list of key-value pairs to configure the job.\n) in UI\nCan somone guide me here on what im doing wrong and how can i run a pig script in Dataproc",
      "solution": "Pass it like below ,\n```\n`  gcloud config set project PROJECT\n\n  gcloud dataproc jobs submit pig   --cluster=cluster-workaround   --region=us-east4   --verbosity=debug   --properties-file=gs://bucket/cvr_gcs_one.properties --file=gs://bucket-temp/your_pig.pig --params udf_path=gs://your_udfs.jar\n`\n```",
      "question_score": 3,
      "answer_score": 1,
      "created_at": "2022-12-13T13:16:35",
      "url": "https://stackoverflow.com/questions/74784729/error-in-submitting-a-pig-job-to-google-dataproc-with-properties-file"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 70946280,
      "title": "Unable to deploy Google Cloud Function due to mysterious --production=false flag",
      "problem": "For some reason, I can no longer deploy existing google functions from my local machine or from github actions. Whenever I deploy using the `gcloud functions deploy` command, I get the following error in the console: `ERROR: (gcloud.functions.deploy) OperationError: code=3, message=Build failed: Unknown Syntax Error: Invalid option name (\"--production=false\").` I am not using a --production=false option in my gcloud deploy command, so I don't really understand where that is coming from.\nBuild logs always failing on:\n`Step #1 - \"build\": Unable to delete previous cache image: DELETE https://us.gcr.io/v2/{{projectId}}/gcf/{{region}}/{{guid}}/cache/manifests/sha256:{{imageId}}: GOOGLE_MANIFEST_DANGLING_TAG: Manifest is still referenced by tag: latest`.\nDeploy command:\n```\n`gcloud functions deploy --runtime=nodejs16 --region=us-central1 {{function_name}} --entry-point={{node_function}} --trigger-topic={{topic_name}}\n`\n```\nAttempted with the following gcloud versions and got the same result each time:\n370, 371, 369, 360\nI am not sure where this is coming from. I did not have this problem when I deployed just yesterday and it is not specific to my local machine.",
      "solution": "This was due to a regression issue on Google's part. They released a fix for it today and deploys are working again now.\nIssue: https://github.com/GoogleCloudPlatform/buildpacks/issues/175#issuecomment-1030519240",
      "question_score": 3,
      "answer_score": 1,
      "created_at": "2022-02-01T20:18:18",
      "url": "https://stackoverflow.com/questions/70946280/unable-to-deploy-google-cloud-function-due-to-mysterious-production-false-flag"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 69662127,
      "title": "How to add a SSL postgres connection using CLI for Cloud Composer?",
      "problem": "using airflow locally, I was able to add a SSL Postgres connection using this :\n```\n`./airflow.sh connections add connection_name --conn-uri 'postgres://user:@host:port/db?sslmode=verify-ca&sslcert=<>.crt&sslca=<>.crt&sslkey=<>.key.pk8'\n`\n```\nNow I'm using Cloud Composer, and I want to add this connection again, but I can't find how. I tried using gcloud CLI as explained here but I can't find the right method or the right arguments to use.\nDoes anyone know how to do it ?\nThanks in advance !",
      "solution": "As per our discussion in comments, you can configure PostgreSQL connection in Cloud Composer using :\nCLI (Linux)\nCloud SDK supports Airflow CLI subcommand to run. When specifying the connection as URI, extras (ie. sslmode,sslcert etc) are passed as parameters of the URI.\nFor example refer the following command:\n```\n`gcloud composer environments run \\\n  ENVIRONMENT_NAME \\\n  --location LOCATION \\\n  connections -- --add \\\n  --conn_id=CONNECTION_ID \\\n  --conn-uri 'postgresql://postgres_user:XXXXXXXXXXXX@1.1.1.1:5432/postgresdb?sslmode=verify-ca&sslcert=%2Ftmp%2Fclient-cert.pem&sslkey=%2Ftmp%2Fclient-key.pem&sslrootcert=%2Ftmp%2Fserver-ca.pem'\n`\n```\nFor more information refer to this Airflow Documentation.\nCloud Composer Airflow Console\n\nIn the Airflow webserver console generated by Cloud Composer,\nnavigate to Admin > Connection > Create\nSpecify Connection Id,Connection Type (Postgres)\nFill the required parameters ie. Host,Login, Password. (Refer)\nExtra: extra parameters (as json) ie.sslmode,sslcert, sslca, sslkey",
      "question_score": 3,
      "answer_score": 1,
      "created_at": "2021-10-21T14:38:32",
      "url": "https://stackoverflow.com/questions/69662127/how-to-add-a-ssl-postgres-connection-using-cli-for-cloud-composer"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 72549138,
      "title": "How do I use gcloud sql operation wait?",
      "problem": "I am looking at the documentation for this command.\nIt says that it requires the operation identifier...\n\nAn identifier that uniquely identifies the operation\n\nBut it does not give an example of how to obtain an identifier. What am I supposed to provide for this value? I am a getting an error when creating an sql instance using gcloud because I did not wait, and I don't know how to use this command from the docs description of it.",
      "solution": "I was having the same issue. Here is essential how you use this guy:\n```\n`gcloud sql import sql qa-1 gs://db_dumps/staging/Cloud_SQL_Export_$(date +%F).sql --database=qa-1 --async --quiet\nsleep 10\nPENDING_OPERATION=$(gcloud sql operations list --instance=qa-1 --filter=\"TYPE:IMPORT AND NOT STATUS:DONE\" --format='value(name)')\ngcloud sql operations wait \"$PENDING_OPERATION\" --timeout=unlimited\n`\n```\nNotice the use of async. If you wait for that command to fail your in trouble because its going to. Instead, run the command asynchronously, look up the operation id using filters, store it, and then use the wait command with that id.\nThen, Party!",
      "question_score": 2,
      "answer_score": 9,
      "created_at": "2022-06-08T18:22:37",
      "url": "https://stackoverflow.com/questions/72549138/how-do-i-use-gcloud-sql-operation-wait"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 69423435,
      "title": "Problem with Fedora SSH connection on Google Cloud",
      "problem": "I am having problem with connecting to my new instance directly from the browser. This is literally a new instance of Fedora OS so I have not configured anything. It doesn't work out of the box.\nI have other servers with OS like Debian 10 (Buster) and they seem to work fine when connecting through SSH.\nHere are the server specs that I am using for the VM:\n```\n`Machine type: e2-medium (2 vCPUs, 4 GB memory)\nCPU platform: Intel Broadwell\nZone: us-central1-a\nOS Image: fedora-coreos-34-20210904-3-0-gcp-x86-64\nDisk Size: 30GB\nDisk Type: SSD\n`\n```\nHere are the logs from the serial port: (Link to entire log from serial port)\n```\n`[  545.747496] audit: type=2404 audit(1633250527.525:300): pid=1892 uid=0 auid=4294967295 ses=4294967295 subj=system_u:system_r:sshd_t:s0-s0:c0.c1023 msg='op=destroy kind=server fp=SHA256:80:d3:1d:38:a5:96:e3:02:50:e1:55:11:ec:61:1b:65:89:6e:08:ad:4d:50:09:82:2d:a6:cb:c8:fa:35:6c:c7 direction=? spid=1893 suid=74  exe=\"/usr/sbin/sshd\" hostname=? addr=? terminal=? res=success'\n[  545.780996] audit: type=1109 audit(1633250527.525:301): pid=1892 uid=0 auid=4294967295 ses=4294967295 subj=system_u:system_r:sshd_t:s0-s0:c0.c1023 msg='op=PAM:bad_ident grantors=? acct=\"?\" exe=\"/usr/sbin/sshd\" hostname=74.125.73.141 addr=74.125.73.141 terminal=ssh res=failed'\n[  545.806261] audit: type=2404 audit(1633250527.526:302): pid=1892 uid=0 auid=4294967295 ses=4294967295 subj=system_u:system_r:sshd_t:s0-s0:c0.c1023 msg='op=destroy kind=server fp=SHA256:80:d3:1d:38:a5:96:e3:02:50:e1:55:11:ec:61:1b:65:89:6e:08:ad:4d:50:09:82:2d:a6:cb:c8:fa:35:6c:c7 direction=? spid=1892 suid=0  exe=\"/usr/sbin/sshd\" hostname=? addr=? terminal=? res=success'\n[  545.839942] audit: type=1112 audit(1633250527.526:303): pid=1892 uid=0 auid=4294967295 ses=4294967295 subj=system_u:system_r:sshd_t:s0-s0:c0.c1023 msg='op=login acct=\"(unknown)\" exe=\"/usr/sbin/sshd\" hostname=? addr=74.125.73.141 terminal=ssh res=failed'\n[  564.968011] audit: type=2404 audit(1633250546.749:304): pid=1895 uid=0 auid=4294967295 ses=4294967295 subj=system_u:system_r:sshd_t:s0-s0:c0.c1023 msg='op=destroy kind=server fp=SHA256:80:d3:1d:38:a5:96:e3:02:50:e1:55:11:ec:61:1b:65:89:6e:08:ad:4d:50:09:82:2d:a6:cb:c8:fa:35:6c:c7 direction=? spid=1895 suid=0  exe=\"/usr/sbin/sshd\" hostname=? addr=? terminal=? res=success'\n[  565.344660] audit: type=2407 audit(1633250547.122:305): pid=1894 uid=0 auid=4294967295 ses=4294967295 subj=system_u:system_r:sshd_t:s0-s0:c0.c1023 msg='op=start direction=from-server cipher=aes128-ctr ksize=128 mac=hmac-sha2-256 pfs=diffie-hellman-group-exchange-sha256 spid=1895 suid=74 rport=32883 laddr=10.128.15.203 lport=22  exe=\"/usr/sbin/sshd\" hostname=? addr=74.125.17.13 terminal=? res=success'\n[  565.382463] audit: type=2407 audit(1633250547.122:306): pid=1894 uid=0 auid=4294967295 ses=4294967295 subj=system_u:system_r:sshd_t:s0-s0:c0.c1023 msg='op=start direction=from-client cipher=aes128-ctr ksize=128 mac=hmac-sha2-256 pfs=diffie-hellman-group-exchange-sha256 spid=1895 suid=74 rport=32883 laddr=10.128.15.203 lport=22  exe=\"/usr/sbin/sshd\" hostname=? addr=74.125.17.13 terminal=? res=success'\n[  566.988544] audit: type=2404 audit(1633250548.769:307): pid=1894 uid=0 auid=4294967295 ses=4294967295 subj=system_u:system_r:sshd_t:s0-s0:c0.c1023 msg='op=destroy kind=session fp=? direction=both spid=1895 suid=74 rport=32883 laddr=10.128.15.203 lport=22  exe=\"/usr/sbin/sshd\" hostname=? addr=74.125.17.13 terminal=? res=success'\n[  567.021621] audit: type=2404 audit(1633250548.800:308): pid=1894 uid=0 auid=4294967295 ses=4294967295 subj=system_u:system_r:sshd_t:s0-s0:c0.c1023 msg='op=destroy kind=server fp=SHA256:80:d3:1d:38:a5:96:e3:02:50:e1:55:11:ec:61:1b:65:89:6e:08:ad:4d:50:09:82:2d:a6:cb:c8:fa:35:6c:c7 direction=? spid=1895 suid=74  exe=\"/usr/sbin/sshd\" hostname=? addr=? terminal=? res=success'\n[  567.057403] audit: type=1109 audit(1633250548.800:309): pid=1894 uid=0 auid=4294967295 ses=4294967295 subj=system_u:system_r:sshd_t:s0-s0:c0.c1023 msg='op=PAM:bad_ident grantors=? acct=\"?\" exe=\"/usr/sbin/sshd\" hostname=74.125.17.13 addr=74.125.17.13 terminal=ssh res=failed'\n[  567.082647] audit: type=2404 audit(1633250548.800:310): pid=1894 uid=0 auid=4294967295 ses=4294967295 subj=system_u:system_r:sshd_t:s0-s0:c0.c1023 msg='op=destroy kind=server fp=SHA256:80:d3:1d:38:a5:96:e3:02:50:e1:55:11:ec:61:1b:65:89:6e:08:ad:4d:50:09:82:2d:a6:cb:c8:fa:35:6c:c7 direction=? spid=1894 suid=0  exe=\"/usr/sbin/sshd\" hostname=? addr=? terminal=? res=success'\n[  567.116466] audit: type=1112 audit(1633250548.801:311): pid=1894 uid=0 auid=4294967295 ses=4294967295 subj=system_u:system_r:sshd_t:s0-s0:c0.c1023 msg='op=login acct=\"(unknown)\" exe=\"/usr/sbin/sshd\" hostname=? addr=74.125.17.13 terminal=ssh res=failed'\n`\n```\nHere's what I have tried so far:\nFollowing this question, I tried to manually add SSH key to my instance meta data but that doesn't seem to work as well. When I try to connect through SSH, I get the following error:\n```\n`Permission denied (publickey,gssapi-keyex,gssapi-with-mic).\n`\n```\nI have also tried to connect through Google's OS Login console and it still doesn't connect for some reason. Here's the console output for that:\n```\n`gcloud beta compute ssh --zone \"us-central1-a\" \"instance-1\"  --project \"XXX\"\nWarning: Permanently added 'compute.178891790600165087' (ECDSA) to the list of known hosts.\nXXX@123.456.789.123: Permission denied (publickey,gssapi-keyex,gssapi-with-mic).\nERROR: (gcloud.beta.compute.ssh) [/usr/bin/ssh] exited with return code [255].\n`\n```",
      "solution": "Adding public key to instance metadata by the username `core` seems to solve the issue. Any other username is rejected.\n\nGenerate key pair: `ssh-keygen -t ed25519`\nCopy public key to instance metadata - link.\nRestart the instance.\nConnect using new key: `ssh -i  core@`\n\nMore information about this issue can also be found here.",
      "question_score": 2,
      "answer_score": 9,
      "created_at": "2021-10-03T10:58:36",
      "url": "https://stackoverflow.com/questions/69423435/problem-with-fedora-ssh-connection-on-google-cloud"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 70433003,
      "title": "GCP Cloud Run Cannot Pull Image from Artifact Registry in Other Project",
      "problem": "I have a parent project that has an artifact registry configured for docker.\nA child project has a cloud run service that needs to pull its image from the parent.\nThe child project also has a service account that is authorized to access the repository via an IAM role roles/artifactregistry.writer.\nWhen I try to start my service I get an error message:\n\nGoogle Cloud Run Service Agent must have permission to read the image,\neurope-west1-docker.pkg.dev/test-parent-project/docker-webank-private/node:custom-1.\nEnsure that the provided container image URL is correct and that the\nabove account has permission to access the image. If you just enabled\nthe Cloud Run API, the permissions might take a few minutes to\npropagate. Note that the image is from project [test-parent-project], which\nis not the same as this project [test-child-project]. Permission must be\ngranted to the Google Cloud Run Service Agent from this project.\n\nI have tested manually connecting with docker login and using the service account's private key and the docker pull command works perfectly from my PC.\n```\n`cat $GOOGLE_APPLICATION_CREDENTIALS | docker login -u _json_key --password-stdin https://europe-west1-docker.pkg.dev\n> Login succeeded\ndocker pull europe-west1-docker.pkg.dev/bfb-cicd-inno0/docker-webank-private/node:custom-1\n> OK\n`\n```\nThe service account is also attached to the cloud run service:",
      "solution": "You have 2 types of service account used in Cloud Run:\n\nThe Google Cloud Run API service account\nThe Runtime service account.\n\nIn your explanation, and your screenshot, you talk about the runtime service account, the identity that will be used by the service when it runs and call Google Cloud API.\nBUT before running, the service must be deployed. This time, it's a Google Cloud Run internal process that run to pull the container, create a revision and do all the required internal stuff. To do that job, a service account also exist, it's named \"service agent\".\nIn the IAM console, you can find it: the format is the following\n```\n`service-@serverless-robot-prod.iam.gserviceaccount.com\n`\n```\nDon't forget to tick the checkbox in the upper right corner to include the Google Managed service account\n\nIf you want that this deployment service account be able to pull image in another project, grant on it the correct permission, not on the runtime service account.",
      "question_score": 2,
      "answer_score": 7,
      "created_at": "2021-12-21T10:14:09",
      "url": "https://stackoverflow.com/questions/70433003/gcp-cloud-run-cannot-pull-image-from-artifact-registry-in-other-project"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 70172425,
      "title": "Gcloud and Kubectl see me logged in as two different users",
      "problem": "Mac here, in case it makes a difference. I am on 2 separate GCP/gcloud/GKE/Kubernetes projects and have two different gmails for each of them:\n\nProject 1: flim-flam, where my email is myuser1@gmail.example.com (pretend its a gmail)\nProject 2: foo-bar, where my email is myuser2@gmail.example.com\n\nI log into my `myuser1@gmail.example.com` account via `gcloud auth login` and confirm I am logged in as that account. For instance, I go to the GCP console and verify (in the UI) that I am in fact logged in as `myuser1@gmail.example.com`. Furthermore, when I run `gcloud config configurations list` I get:\n```\n`NAME       IS_ACTIVE  ACCOUNT                    PROJECT        COMPUTE_DEFAULT_ZONE  COMPUTE_DEFAULT_REGION\nflim-flam  True       myuser1@gmail.example.com  flim-flam\nfoo-bar    False      myuser2@gmail.example.com  foo-bar\n`\n```\nFrom my `flim-flam` project, when I run `kubectl delete ns flimflam-app` I get permission errors:\n```\n`Error from server (Forbidden): namespace \"flimflam-app\" is forbidden: User \"myuser2@gmail.example.com\" cannot delete resource \"namespaces\" in API group \"\" in the namespace \"flimflam-app\": requires one of [\"container.namespaces.delete\"] permission(s).\n`\n```\nSo gcloud thinks I'm logged in as `myuser1` but kubectl thinks I'm logged in as `myuser2`. How do I fix this?",
      "solution": "`gcloud` and `kubectl` share user identities but their configuration is in different files.\nUsing `gcloud auth login` does not update (!) existing (!) `kubectl` configurations. The former (on Linux) are stored in `${HOME}/.config/gcloud` and the latter in `${HOME}/.kube/config`.\nI don't have a copy on hand but, if you check `${HOME}/.kube/config`, it likely references the other Google account. You can either duplicate the `users` entry and reference it from the `context`. Or you could edit the existing `users` entry.\nActually, better yet use `gcloud container clusters get-credentials` to update `kubectl`'s configuration with the currently-active `gcloud` user. This command updates `${HOME}/.kube/config` for you.",
      "question_score": 2,
      "answer_score": 6,
      "created_at": "2021-11-30T17:05:29",
      "url": "https://stackoverflow.com/questions/70172425/gcloud-and-kubectl-see-me-logged-in-as-two-different-users"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 69551010,
      "title": "Google Cloud Shell gcloud commands output not pretty printed anymore",
      "problem": "I was using GCloud Shell a few weeks ago and got pretty printed outputs from gcloud commands, like so:\n```\n`DISPLAY NAME                            EMAIL                                                 DISABLED\nCompute Engine default service account  XXXXXXXXXXXX-compute@developer.gserviceaccount.com    False\nsa-xxxxxxxxx                            sa-xxxxxxxxx@my-project.iam.gserviceaccount.com       False\n`\n```\nSince a few days, output is not anymore pretty printed:\n```\n`DISPLAY NAME: Compute Engine default service account\nEMAIL: XXXXXXXXXXXX-compute@developer.gserviceaccount.com\nDISABLED: False\n\nDISPLAY NAME: sa-xxxxxxxxx\nEMAIL: sa-xxxxxxxxx@my-project.iam.gserviceaccount.com\nDISABLED: False\n`\n```\nI checked embedded gcloud SDK version:\n```\n`$ gcloud -v\nGoogle Cloud SDK 360.0.0\nalpha 2021.10.04\napp-engine-go 1.9.71\napp-engine-java 1.9.91\napp-engine-python 1.9.95\napp-engine-python-extras 1.9.95\nbeta 2021.10.04\nbigtable\nbq 2.0.71\ncbt 0.10.1\ncloud-build-local 0.5.2\ncloud-datastore-emulator 2.1.0\ncore 2021.10.04\ndatalab 20190610\ngsutil 5.3\nkind 0.7.0\nkpt 1.0.0-beta.5\nlocal-extract 1.3.1\nminikube 1.23.2\npubsub-emulator 0.5.0\nskaffold 1.32.0\n`\n```\nI also checked the documentation on output formats, which isn\u2019t of any help. Tried several outputs without being able to have a pretty one like before.\nI tried installing the SDK 360.0.0 on Cloud Shell, which gives me the pretty output as before\u2026\nAnyone else having this issue? Or knowing how to get the pretty print as before (without having to manually install gcloud SDK)?\nEdit:\nAs asked by John Hanley, here is the output of `gcloud config list`:\n```\n`[accessibility]\nscreen_reader = True\n[component_manager]\ndisable_update_check = True\n[compute]\ngce_metadata_read_timeout_sec = 30\n[core]\naccount = nicolas@mydomain.com\ndisable_usage_reporting = True\nproject = my-project\n[metrics]\nenvironment = devshell\n\nYour active configuration is: [cloudshell-25102]\n`\n```\nColumn width as given by `tput cols` is 267.",
      "solution": "Thanks to @JohnHanley for the insight of `gcloud config list`, I compared the configurations between embedded `gcloud` and the downloaded version, then read some documentation to find that this behavior is only due to an accessibility option which is now set to `true` by default.\nFor anyone having this issue, here is the command to get the good ol' pretty print output back:\n```\n`gcloud config set accessibility/screen_reader false\n`\n```\nIf you want it to persist between Cloud Shell reboots, add the `--installation` flag and use `sudo`.",
      "question_score": 2,
      "answer_score": 6,
      "created_at": "2021-10-13T09:00:14",
      "url": "https://stackoverflow.com/questions/69551010/google-cloud-shell-gcloud-commands-output-not-pretty-printed-anymore"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 70655166,
      "title": "How to patch GKE Managed Instance Groups (Node Pools) for package security updates?",
      "problem": "I have a GKE cluster running multiple nodes across two zones. My goal is to have a job scheduled to run once a week to run `sudo apt-get upgrade` to update the system packages. Doing some research I found that GCP provides a tool called \"OS patch management\" that does exactly that. I tried to use it but the Patch Job execution raised an error informing\nFailure reason: Instance is part of a Managed Instance Group.\nI also noticed that during the creation of the GKE Node pool, there is an option for enabling \"Auto upgrade\". But according to its description, it will only upgrade the version of the Kubernetes.",
      "solution": "According to the Blog Exploring container security: the shared responsibility model in GKE:\n\nFor GKE, at a high level, we are responsible for protecting:\n\nThe nodes\u2019 operating system, such as Container-Optimized OS (COS) or Ubuntu. GKE promptly makes any patches to these images available. If you have auto-upgrade enabled, these are automatically deployed. This is the base layer of your container\u2014it\u2019s not the same as the operating system running in your containers.\n\nConversely, you are responsible for protecting:\n\nThe nodes that run your workloads. You are responsible for any extra software installed on the nodes, or configuration changes made to the default. You are also responsible for keeping your nodes updated. We provide hardened VM images and configurations by default, manage the containers that are necessary to run GKE, and provide patches for your OS\u2014you\u2019re just responsible for upgrading. If you use node auto-upgrade, it moves the responsibility of upgrading these nodes back to us.\n\nThe node auto-upgrade feature DOES patch the OS of your nodes, it does not just upgrade the Kubernetes version.",
      "question_score": 2,
      "answer_score": 3,
      "created_at": "2022-01-10T16:48:09",
      "url": "https://stackoverflow.com/questions/70655166/how-to-patch-gke-managed-instance-groups-node-pools-for-package-security-updat"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 68457214,
      "title": "gcloud &#39;no such file or directory&#39;",
      "problem": "On Ubuntu 20.04, gcloud installed with `snap install google-cloud-sdk --classic`...\nToday it no longer works. Yesterday there was an auto update.\n```\n`$ kubectl get all\nUnable to connect to the server: error executing access token command \"/snap/google-cloud-sdk/188/bin/gcloud config config-helper --format=json\": err=fork/exec /snap/google-cloud-sdk/188/bin/gcloud: no such file or directory output= stderr=\n`\n```\nVersion 188 it is referencing is gone, and it is now at 190. (Version 189 is also present.)\nI've uninstalled and deleted the .config/gcloud, and reinstalled, but still have the same error.\nAny tips on where to look for that stale path?",
      "solution": "The problem is that gcloud stores the absolete reference to the gcloud binary to ~/.kube/config. The solution is to replace `/snap/google-cloud-sdk/.*/gcloud` -> `/snap/bin/gcloud` in `~/.kube/config`.\nExample of accomplishing this with `perl` on the command line:\n```\n`perl -i -p -e 's/\\/snap\\/google-cloud-(sdk|cli)\\/.*?\\/gcloud/\\/snap\\/bin\\/gcloud/' ~/.kube/config\n`\n```",
      "question_score": 2,
      "answer_score": 5,
      "created_at": "2021-07-20T17:04:57",
      "url": "https://stackoverflow.com/questions/68457214/gcloud-no-such-file-or-directory"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 68072744,
      "title": "Ignore pubsub topic if it already created",
      "problem": "I have a simple script to deploy a pubsub application.\nThis script will run on every deploy of my Cloud Run service and I have a line with:\n```\n`gcloud pubsub topics create some-topic\n`\n```\nI want to improve my script if the topic already exist, currently if I run my script, the output will be:\n\nERROR: Failed to create topic [projects/project-id/topics/some-topic]: Resource already exists in the project (resource=some-topic).\n\nERROR: (gcloud.pubsub.topics.create) Failed to create the following: [some-topic].\n\nI tried the flag `--no-user-output-enabled` but no success.\nIs there a way to ignore if the resource already exists, or a way to check before create?",
      "solution": "Yes.\nYou can repeat the operation knowing that, if the topic didn't exist beforehand, it will if the command succeeds.\nYou can swallow stderr (with `2>/dev/null`) and then check whether the previous command (`$?`) succeeded (`0`):\n`gcloud pubsub topic create do-something 2>/dev/null\nif [ $? -eq 0 ]\nthen\n  # Command succeeded, topic did not exist\n  echo \"Topic ${TOPIC} did not exist, created.\"\nelse\n  # Command did not succeed, topic may (!) not have existed\n  echo \"Failure\"\nfi\n`\n\nNOTE This approach misses the fact that, the command may fail and the topic didn't exist (i.e. some other issue).\n\nAlternatively (more accurately and more expensively!) you can enumerate the topics first and then try (!) to create it if it doesn't exist:\n`TOPIC=\"some-topic\"\nRESULT=$(\\\n  gcloud pubsub topics list \\\n  --filter=\"name.scope(topics)=${TOPIC}\" \\\n  --format=\"value(name)\" 2>/dev/null)\n\nif [ \"${RESULT}\" == \"\" ]\nthen\n  echo \"Topic ${TOPIC} does not exist, creating...\"\n  gcloud pubsub topics create ${TOPIC}\n  if [ $? -eq 0 ]\n  then\n    # Command succeeded, topic created\n  else\n    # Command did not succeed, topic was not created\n  fi\nfi\n`\nDepending on the complexity of your needs, you can automate using:\n\nany of Google's (Pub/Sub) libraries which provide better error-handling and retry capabilities.\nTerraform e.g. google_pubsub_topic",
      "question_score": 2,
      "answer_score": 5,
      "created_at": "2021-06-21T20:06:42",
      "url": "https://stackoverflow.com/questions/68072744/ignore-pubsub-topic-if-it-already-created"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 66385850,
      "title": "How to increase the cloud build timeout when using ```gcloud run deploy```?",
      "problem": "When attempting to deploy to Cloud Run using the `gcloud run deploy` I am hitting the 10m Cloud Build timeout limit.  `gcloud run deploy` is working well as long as the build step does not exceed 10m.  When the build step exceeds 10m, the build fails with the \"Timed out\" status as shown in below screenshot.  AFAIK there are no arguments to `gcloud run deploy` that can set the Cloud Build timeout limit.  `gcloud run deploy` docs are here: https://cloud.google.com/sdk/gcloud/reference/run/deploy\nI've attempted to increase the Cloud Build timeout limit using `gcloud config set builds/timeout 20m` and `gcloud config set container/build_timeout 20m`, but these settings are not reflected in the execution details of the cloud build process when using `gcloud run deploy`.\nIn the GUI, this is the setting I want to change:\n\nIs it possible to increase the Cloud Build timeout limit using `gcloud run deploy`?",
      "solution": "How about splitting the command into (more easily configured) constituents?\n[I've not tried this]\nBuild the container image specifying the timeout\n:\n`gcloud builds submit --source=.... --timeout=...\n`\nThen reference the image that results when you `gcloud run deploy`:\n`gcloud run deploy ... --image=...\n`",
      "question_score": 2,
      "answer_score": 2,
      "created_at": "2021-02-26T13:27:42",
      "url": "https://stackoverflow.com/questions/66385850/how-to-increase-the-cloud-build-timeout-when-using-gcloud-run-deploy"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 66054484,
      "title": "Gcloud functions deploy with specific python file and specific function name",
      "problem": "I'm trying to deploy several Cloud Functions which are all in the same repository but in different python files.\nBasically, I have those two functions\n```\n`my_repo\n- function_1_folder\n  - function_1.py\n  - requirements.txt\n  - .gcloudignore\n- function_2_folder  \n  - function_2.py\n  - requirements.txt\n  - .gcloudignore\n`\n```\nInside `function_1.py` I have `function_1()` which I want to deploy to my cloud function called `function1` (note there is no underscore here), and same for `function_2`.\nI go to `function_1_folder` and I have specified an entry point (`--entry-point function_1`) but I get a \"missing main.py\" error.\nHow can I specify both python filename and function name (if possible) ? Will gcloud also deploy the `requirements.txt` which is needed to install the packages my function depends on ?\nThank you",
      "solution": "You can't name the source file arbitrarily. It's part of the structuring requirements that your Python source file should always be named `main.py`. This is how your directory should look like:\n```\n`my_repo\n - function_1_folder\n   - main.py\n   - requirements.txt\n   - .gcloudignore\n - function_2_folder  \n   - main.py\n   - requirements.txt\n   - .gcloudignore\n`\n```\nThe flag `--entry-point` is used to specify the name of the function on your source file. For example (as HTTP):\nmain.py\n```\n`def function_1(request):\n    return 'Hello World'\n`\n```\nRun this command inside `function_1_folder`:\n```\n`gcloud functions deploy function1 --entry-point function_1 --runtime python37 --trigger-http \n`\n```\nTo answer your final question, requirements.txt is included along with your main.py and is a valid configuration. Those dependencies will be installed during build time.\nAs additional reference, see: https://github.com/GoogleCloudPlatform/python-docs-samples/tree/master/functions",
      "question_score": 2,
      "answer_score": 5,
      "created_at": "2021-02-04T23:00:18",
      "url": "https://stackoverflow.com/questions/66054484/gcloud-functions-deploy-with-specific-python-file-and-specific-function-name"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 65610480,
      "title": "gcloud compute backend-services provides a &quot;not found&quot; error",
      "problem": "For some unclear reason the gcloud compute backend-services does not find a resource even it is listed.\nI run the command:\n```\n`gcloud compute backend-services list\n`\n```\nAnd get the output:\n```\n`NAME                                                     BACKENDS                                                                                     PROTOCOL\nk8s1-48550837-defaul-reverse-proxy-captcha-s-8-9edaabba  us-central1-c/networkEndpointGroups/k8s1-48550837-defaul-reverse-proxy-captcha-s-8-9edaabba  HTTP\nk8s1-48550837-default-admission-service-80-ccc76f21      us-central1-c/networkEndpointGroups/k8s1-48550837-default-admission-service-80-ccc76f21      HTTP\nk8s1-48550837-default-agent-service-80-46302173          us-central1-c/networkEndpointGroups/k8s1-48550837-default-agent-service-80-46302173          HTTP\nk8s1-48550837-default-authproxy-service-80-f50b4f37      us-central1-c/networkEndpointGroups/k8s1-48550837-default-authproxy-service-80-f50b4f37      HTTP\n\n`\n```\nBut for any one of the backends I am trying to get description or edit, I get error.\nFor example, this command:\n```\n`gcloud compute backend-services describe k8s1-48550837-default-agent-service-80-46302173\n`\n```\nreturns error:\n```\n`ERROR: (gcloud.compute.backend-services.describe) Could not fetch resource:\n - The resource 'projects/radware-cto/regions/us-central1/backendServices/k8s1-48550837-default-agent-service-80-46302173' was not found\n`\n```",
      "solution": "OP confirmed --global works in a comment.\nThe reason this works is because the 'describe' command will by default describe the resource in the default compute/region (gcloud config set compute/region...)\nYou can see this in your error message:\n` - The resource .../regions/us-central1/backendServices/...`\nHowever, as this resource is global, we need to specify `--global` flag to override the default region.\nAdditional information here",
      "question_score": 2,
      "answer_score": 5,
      "created_at": "2021-01-07T11:20:43",
      "url": "https://stackoverflow.com/questions/65610480/gcloud-compute-backend-services-provides-a-not-found-error"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 79816569,
      "title": "&#39;str&#39; object is not callable in Google SDK when trying to fetch a spreadsheet",
      "problem": "I'm trying to fetch a spreadsheet with Google SDK. I'm logged into an account with a `Service Account Token Creator` role and the service account has access rights to the spreadsheet. This is my code:\n`def build_gcp_sheets_service():\n    target_scopes = ['https://www.googleapis.com/auth/spreadsheets.readonly']\n\n    source_credentials, _ = google.auth.default()\n\n    target_credentials = impersonated_credentials.Credentials(\n        source_credentials=source_credentials,\n        target_principal=config.gcp.local_service_account,\n        target_scopes=target_scopes,\n        lifetime=60\n    )\n\n    service = build('sheets', 'v4', credentials=target_credentials)\n    return service\n\nservice = build_gcp_sheets_service()\n\nsheet = service.spreadsheets()\nrequest = sheet.values().get(spreadsheetId=file_id, range=sheet_range)\nresult = request.execute() # Error occurs here\n`\nCalling `request.execute()` results in this error:\n```\n`Traceback (most recent call last):\n  File \"/aircraft-detection/ml/gcp.py\", line 69, in _fetch_spreadsheet\n    result = request.execute()\n             ^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/googleapiclient/_helpers.py\", line 130, in positional_wrapper\n    return wrapped(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/googleapiclient/http.py\", line 923, in execute\n    resp, content = _retry_request(\n                    ^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/googleapiclient/http.py\", line 191, in _retry_request\n    resp, content = http.request(uri, method, *args, **kwargs)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/google_auth_httplib2.py\", line 207, in request\n    self.credentials.before_request(self._request, method, uri, request_headers)\n  File \"/usr/local/lib/python3.11/site-packages/google/auth/credentials.py\", line 228, in before_request\n    self._blocking_refresh(request)\n  File \"/usr/local/lib/python3.11/site-packages/google/auth/credentials.py\", line 191, in _blocking_refresh\n    self.refresh(request)\n  File \"/usr/local/lib/python3.11/site-packages/google/auth/credentials.py\", line 365, in refresh\n    self._refresh_token(request)\n  File \"/usr/local/lib/python3.11/site-packages/google/auth/impersonated_credentials.py\", line 289, in _refresh_token\n    self._source_credentials._refresh_token(request)\nTypeError: 'str' object is not callable\n`\n```\nI suspect that something is wrong with my gcloud configuration, not the code, because:\n\nThis code used to work in the past and the error started to occur despite no changes in the code.\n\nThe code works on other machines (other developers)\n\nI reinstalled gcloud sdk and configured it again but it didn't help.",
      "solution": "Apparently this is a recent issue with the newest SDK version, reverting to 2.41.1 fixes it.",
      "question_score": 2,
      "answer_score": 4,
      "created_at": "2025-11-11T12:20:03",
      "url": "https://stackoverflow.com/questions/79816569/str-object-is-not-callable-in-google-sdk-when-trying-to-fetch-a-spreadsheet"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 75796183,
      "title": "How to add a `billing_project` to a group in GCP Terraform?",
      "problem": "I am unable to make user groups in GCP using terraform: https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/cloud_identity_group\nThere is a warning about using the application default credentials:\n\nIf you are using User ADCs (Application Default Credentials) with this resource, you must specify a billing_project and set user_project_override to true in the provider configuration. Otherwise the Cloud Identity API will return a 403 error. Your account must have the serviceusage.services.use permission on the billing_project you defined.\n\nI am using the Application Default Credentials.\nHere is the error I get when I try to terraform apply my code:\n```\n`Error: Error creating Group: googleapi: Error 403: Your application has authenticated using end user credentials from the Google Cloud SDK or Google Cloud Shell which are not supported by the cloudidentity.googleapis.com. We recommend configuring the billing/quota_project setting in gcloud or using a service account through the auth/impersonate_service_account setting. For more information about service accounts and how to use them in your application, see https://cloud.google.com/docs/authentication/. If you are getting this error with curl or similar tools, you may need to specify 'X-Goog-User-Project' HTTP header for quota and billing purposes. For more information regarding 'X-Goog-User-Project' header, please check \nhttps://cloud.google.com/apis/docs/system-parameters.\n\u2502 Details:\n\u2502 [\n\u2502   {\n\u2502     \"@type\": \"type.googleapis.com/google.rpc.ErrorInfo\",\n\u2502     \"domain\": \"googleapis.com\",\n\u2502     \"metadata\": {\n\u2502       \"consumer\": \"projects/764086051850\",\n\u2502       \"service\": \"cloudidentity.googleapis.com\"\n\u2502     },\n\u2502     \"reason\": \"SERVICE_DISABLED\"\n\u2502   }\n\u2502 ]\n\u2502\n\u2502   with google_cloud_identity_group.group,\n\u2502   on groups.tf line 10, in resource \"google_cloud_identity_group\" \"group\":\n\u2502   10: resource \"google_cloud_identity_group\" \"group\" {\n\u2502\n`\n```\nAs you can see, it is a 403 error, just like in the warning. I had also already tried enabling the api by following these instructions: https://cloud.google.com/identity/docs/how-to/setup\nSo I'm fairly certain the enabling of the API is not the actual issue, I believe the issue is what the terraform documentation warned about.\nThe warning says to specify `billing_project` and set `user_project_override` to true, but I don't know where to do that. As a guess I tried putting them as arguments to my terraform script but it didn't work (kind of expected because they weren't listed as arguments in the terraform documentation)\n```\n`$ terraform apply\n\u2577\n\u2502 Error: Unsupported argument\n\u2502 \n\u2502   on groups.tf line 14, in resource \"google_cloud_identity_group\" \"group\":\n\u2502   14:   billing_project = var.project_id\n\u2502 \n\u2502 An argument named \"billing_project\" is not expected here.\n\u2575\n\u2577\n\u2502 Error: Unsupported argument\n\u2502 \n\u2502   on groups.tf line 15, in resource \"google_cloud_identity_group\" \"group\":\n\u2502   15:   user_project_override = true\n\u2502 \n\u2502 An argument named \"user_project_override\" is not expected here.\n`\n```\nMy code:\n```\n`variable \"domain_name\"{\n    type = string\n    default = \"martiantower.com\"\n}\nvariable \"customer_id\"{\n    type = string\n    default = \"C00yc5oid\" # See: https://apps.google.com/supportwidget/articlehome?hl=en&article_url=https%3A%2F%2Fsupport.google.com%2Fa%2Fanswer%2F10070793%3Fhl%3Den&assistant_id=generic-unu&product_context=10070793&product_name=UnuFlow&trigger_context=a\n}\n\nresource \"google_cloud_identity_group\" \"group\" {\n  display_name = \"my-identity-group\"\n\n  parent = \"customers/${var.customer_id}\"\n  # billing_project = var.project_id # Not an actual argument\n  # user_project_override = true # Not an actual argument\n\n  group_key {\n    id = \"my-identity-group@${var.domain_name}\"\n  }\n\n  labels = {\n    \"cloudidentity.googleapis.com/groups.discussion_forum\" = \"\"\n  }\n}\n`\n```\nI assume I'm supposed to set the `billing_project` and `user_project_override` via the `gcloud` cli commands, but I don't know the commands for it.\nAny idea how to set the `billing_project` and `user_project_override`?",
      "solution": "You have to set those values in Google Provider Configuration, not in `google_cloud_identity_group`.",
      "question_score": 2,
      "answer_score": 2,
      "created_at": "2023-03-21T00:28:17",
      "url": "https://stackoverflow.com/questions/75796183/how-to-add-a-billing-project-to-a-group-in-gcp-terraform"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 67593807,
      "title": "ERROR: gcloud crashed (ModuleNotFoundError): No module named &#39;distutils.spawn&#39;",
      "problem": "I have been deploying my service on App Engine for a long time now and never had an issue until today.\nCommand to Deploy\n```\n`gcloud app deploy app.yaml\n`\n```\nOutput\n```\n`Beginning deployment of service [default]...\nBuilding and pushing image for service [default]\nERROR: gcloud crashed (ModuleNotFoundError): No module named 'distutils.spawn'\n`\n```\nI just deployed this morning with no issues and randomly when I tried to redeploy now I get the above error. Hopefully someone can help figure out what caused this issue.\nFor info:\napp.yaml\n```\n`runtime: custom\nenv: flex\nmanual_scaling:\n  instances: 1\nresources:\n  cpu: 1\n  memory_gb: 4\n  disk_size_gb: 10\n`\n```\nGcloud version\n```\n`$ gcloud --version\nGoogle Cloud SDK 341.0.0\nalpha 2021.05.14\nbeta 2021.05.14\nbq 2.0.68\ncore 2021.05.14\ngsutil 4.62\nminikube 1.20.0\nskaffold 1.23.0\n`\n```",
      "solution": "Same exact problem.\nBuilding and pushing image for service [default]\nERROR: gcloud crashed (ModuleNotFoundError): No module named 'distutils.spawn'\nThis issue seemed to be in the snap install of google-cloud-sdk in Ubuntu 20.04.2 LTS (You can select it pre-installed during the ISO setup.. DONT)\nI was getting this in 18.04 as well\nFINALLY solved it..\nBut.. I had to make sure I did not snap install google-cloud-sdk\nI also..\nsudo apt update\nsudo apt upgrade\nThen I made sure the snap install was not installed.  (After a fresh install of Ubuntu).  Sense I use dockerfiles it's easy for me to zap a dev environment and get it back.\nBut Id imagine if you can't zap your os and make sure not to let the OS put it's snap install of google-cloud-sdk.. You could snap remove google-cloud-sdk  and then hunt for all it's configuration files.. And remove them.\nAt that point\nhttps://cloud.google.com/sdk/docs/install#deb\nFollow that... I did so exactly... FINALLY seemed to work.  I used the apt install route they explain.. NOT the snap.\nI tried all the pip install sudo apt-get install python3-distutils  Till I was blue in the face... NADA.\nsomehow.. The Snap being present puts PATH settings that use the wrong distutils.\nOn my box now that I search for it.. In Totally fresh OS state... No Snap install and going through exactly the cloud.google.com/sdk/docs/install#deb work..\nHere is distutils everywhere on my box in Ubuntu 20.03.2 LTS\n```\n`$ sudo find / -name distutils\n\n/snap/lxd/19188/lib/python2.7/distutils\n/snap/core18/1944/usr/lib/python3.6/distutils\n/snap/core18/1944/usr/lib/python3.7/distutils\n/snap/core18/1944/usr/lib/python3.8/distutils\n/usr/lib/python3.8/distutils\n/usr/lib/python3.9/distutils\n/usr/lib/python2.7/distutils\n`\n```\nNote.. There's no google-cloud-sdk in the snap!!\nThe gcloud app deploy FINALLY works!!   Passes the part where it starts to deploy.\nBut as the others in this.. It happened completly random.\nAll I can guess is......  Something clobered distutils as an update somewhere and started pointing to a garbage path.\nMake sure you search for distutils find out where it is.. what's referencing it.. Somewhere in that mess you can fix it.\n\nOne thing I was able to discover is this problem will come default from 20.04.2.\nI downloaded the most recent iso.. thinking it was an 18.04 issue.\nInstalled it fresh into Virtual Box.. And got exactly this same issue.  So my solution fix (no SNAP).. Is against a totally clean 20.04.2 brand spanking new Ubuntu LTS VM.  Default everything.\n===============\nRegarding the random one day it worked.. the next it didn't...\nHere's the thing about snaps in Ubuntu:\nhttps://www.google.com/search?q=Do+snap+packages+update+automatically%3F&rlz=1C1CHBF_enUS834US834&ei=ygynYJGRIo3f-gSLzb3YDg&oq=Do+snap+packages+update+automatically%3F&gs_lcp=Cgdnd3Mtd2l6EAMyCAghEBYQHRAeUJ-TCVifkwlgj5kJaABwAXgAgAFziAHVAZIBAzEuMZgBAKABAqABAaoBB2d3cy13aXrAAQE&sclient=gws-wiz&ved=0ahUKEwiRnrfXz9nwAhWNr54KHYtmD-sQ4dUDCA4&uact=5\n\"Do snap packages update automatically?\nSnaps update automatically, and by default, the snapd daemon checks for updates 4 times a day. Each update check is called a refresh.\"\nso that's how it randomly broke if you used a snap",
      "question_score": 2,
      "answer_score": 1,
      "created_at": "2021-05-18T22:53:50",
      "url": "https://stackoverflow.com/questions/67593807/error-gcloud-crashed-modulenotfounderror-no-module-named-distutils-spawn"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 79756233,
      "title": "`google.auth` python SDK not reading granted scopes from credentials file",
      "problem": "I've run:\n```\n`gcloud auth application-default login --client-id-file google_oauth_client_id.json --scopes=\"https://www.googleapis.com/auth/cloud-platform,https://www.googleapis.com/auth/calendar.calendars.readonly\"\n`\n```\nsuccessfully. My browser opened, I granted the calendar and cloud-platform permissions to my test app, and the results were saved to disk:\n```\n`Credentials saved to file:[/home/*****/.config/gcloud/application_default_credentials.json]\n`\n```\nHowever, running the following snippet leads to a 403 error:\n```\n`from google.auth import default\nfrom google.auth.transport.requests import Request\nfrom googleapiclient.discovery import build\n\nSCOPES = [\"https://www.googleapis.com/auth/calendar.calendars.readonly\"]\n\ncredentials, project_id = default(scopes=SCOPES, quota_project_id='my-project-id')\ncredentials.refresh(Request())\naccess_token = credentials.token\nservice = build(\"calendar\", \"v3\", credentials=credentials)\nevents = service.events().list(calendarId=\"My Calendar Id\", maxResults=10, singleEvents=True, orderBy=\"startTime\").execute()\n`\n```\nAt first I thought maybe I wasn't using the correct `calendarId`, but when I was in the debugger, I noticed that the `credentials` object has no scopes defined:\n```\n`>>> (credentials.scopes, credentials.default_scopes, credentials.granted_scopes)\n(None, None, None)\n`\n```\nHowever, if I delete the `application_default_credentials.json` file the `default` method throws an appropriate error, so it does seem like it's reading from the file properly-- it's just not realizing that the permissions have been granted...\nLooking at the `application_default_credentials.json`, I'm not seeing any mention of scopes: `dict_keys(['account', 'client_id', 'client_secret', 'refresh_token', 'type', 'universe_domain'])`\nThis leads me to believe that either:\n\nThe scopes are saved server-side, and I need to properly request them when refreshing the token\nThe `gcode` client isn't saving this information properly.\n\nOption 1 seems more likely, since the CLI is properly displaying the scopes and passing them to the OAuth session....",
      "solution": "You should use `calendar.events.readonly` for `service.events().list`.\nThis is documented by APIs Explorer for Calendar API for `events.list` under Authorization\nYour issue is that the scopes granted by `gcloud auth application-default login` to your user credentials do not include this scope. You may verify using e.g. this link:\n`https://www.googleapis.com/oauth2/v1/tokeninfo?access_token={ACCESS_TOKEN}`\nWhen you login, you can request the additional scopes:\n`gcloud auth application-default login \\\n--scopes=\\\nhttps://www.googleapis.com/auth/cloud-platform,\\\nhttps://www.googleapis.com/auth/calendar.events.readonly\n`",
      "question_score": 2,
      "answer_score": 3,
      "created_at": "2025-09-04T23:22:07",
      "url": "https://stackoverflow.com/questions/79756233/google-auth-python-sdk-not-reading-granted-scopes-from-credentials-file"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 75628553,
      "title": "Google Artifact Registry (GAR) summary of space used by images",
      "problem": "I'd like to be able to identify how much space is used by all images with a certain tag. The idea for this, is to be able to tell which dev team is consuming \"too much\" space.\nEach dev team, will use a particular container TAG that will help identify them.\nI tried using gcloud cli for this matter but I was unable to find a way to achieve such thing.\nAny ideas?",
      "solution": "As of the moment, you can only determine the size of Google Artifact Registry per image. You can determine the size in two ways:\n\nThrough the Repositories page\nOr by entering the code below through gcloud CLI:\n```\n`gcloud artifacts docker images list\n`\n```\n\nYou may check this documentation on listing images for additional information.\nYou may also want to file this as a feature request through this link. There's no specific ETA on this but you can check its progress by following the ticket once it's created.",
      "question_score": 2,
      "answer_score": 3,
      "created_at": "2023-03-03T15:50:08",
      "url": "https://stackoverflow.com/questions/75628553/google-artifact-registry-gar-summary-of-space-used-by-images"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 71022367,
      "title": "Pass contents of file as a string to an argument for another command",
      "problem": "I have a json file like this:\n```\n`{\"my_string\": \"apple\", \"my_int\": 456}\n`\n```\nAnd a Bash program that works OK like this:\n```\n`my_program --message='{\"my_string\": \"apple\", \"my_int\": 456}'\n`\n```\nBut when my JSON is a file I cannot seem to get it to run properly:\n```\n`my_program --message=$(cat test_message.json) \n`\n```\nI've been searching SO looking at similar questions and have tried lots of things like tr, awk, and various combinations of echo and cat but to no avail.\nThis is the error I receive that leads me to believe this is something I am doing wrong in Bash:\n```\n`ERROR: (gcloud.beta.pubsub.schemas.validate-message) unrecognized arguments:\n  \"apple\",\n  \"my_int\":\n  456}\n  To search the help text of gcloud commands, run:\n  gcloud help -- SEARCH_TERMS\n`\n```\nIf you see it look like it has broken the JSON up, only taking the characters before the first space. Any ideas? I have a feeling this is very basic but I can't seem to figure it out. Thank you in advance.",
      "solution": "The immediate fix is to add quotes around the command substitution:\n```\n`my_program --message=\"$(cat test_message.json)\"\n`\n```\nSee also When to wrap quotes around a shell variable?\nHowever, a much better fix if you have control over `my_program` is to have it read standard input instead.\n```\n`my_program --stdin If the `cat` is just a placeholder for something more complex, combine with a pipe:\n```\n`jq .fnord test_message.json |\nmy_program --stdin\n`\n```\n(Maybe still better to not require an explicit option for this common base case.)\nIn case it's not obvious, the pipe is a direct line of communication between the producer and the consumer, whereas the command substitution requires the shell to read and buffer into memory all the output before passing it on. This is inefficient, inelegant, and slow and error-prone for any nontrivial amounts of output.",
      "question_score": 2,
      "answer_score": 3,
      "created_at": "2022-02-07T17:59:09",
      "url": "https://stackoverflow.com/questions/71022367/pass-contents-of-file-as-a-string-to-an-argument-for-another-command"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 69852863,
      "title": "Why is @google-cloud/profiler throwing &#39;permission_denied&#39; errors when running inside a GKE instance?",
      "problem": "I have been following the instructions on https://cloud.google.com/profiler/docs/profiling-nodejs#gke\nto use the gcloud profiler for my nodejs application:\nI added `RUN npm install @google-cloud/profiler` to my dockerfile.\nAnd I added\n```\n`require('@google-cloud/profiler').start({\n    serviceContext: {\n        service: 'your-service',\n        version: '1.0.1',\n    },\n});\n`\n```\nto my app.js file.\nI then ran the container on a GKE cluster and got the following error:\n\n@google-cloud/profiler Failed to create profile, waiting 8.6s to try again: Error: generic::permission_denied\n\nBy my understanding of the documentation I do not need explicit authentication or permissions to create profiles when running the code from within a gcloud hosted instance.\nThe error itself isn't very helpful and I am a bit out of my depth here.\nI already tried if creating the cluster with `--(autoprovisioning-)scopes \"https://www.googleapis.com/auth/cloud-platform\"` might do the trick, but had no luck either.\nAny ideas what might be the issue here?",
      "solution": "You're correct that the documentation suggests you don't need credentials but I think that you do (and that the documentation is incomplete).\nWhen an app (e.g. Profiler) uses Google's Application Default Credentials and runs on e.g. Compute Engine (App Engine, Cloud Run etc.) it is able to get credentials automatically from the environment (on GCP using Metadata service; locally using by the developer exporting `GOOGLE_APPLICATION_CREDENTIALS` to a key).\nOn GKE, I think this isn't true (unless Google's doing some magic somewhere) and that the app won't be able to access the credentials automatically unless you represent them in GKE. I think (!?) you'll need to create a Service Account and:\n\neither create a Kubernetes Secret from the Service Account's key\nor using GKE's Workload Identity\n\nNOTE Workload Identity is a neat feature but it's only useful if you only ever plan to use GKE (and not some other Kubernetes implementation).\n\nBefore proceeding, let's see whether other folks reply to tell me that I'm wrong.\nI wrote about using what was then called Stackdriver Profiler for GKE deployed apps. The article is dated but it should provide an outline of what you need to do.",
      "question_score": 2,
      "answer_score": 3,
      "created_at": "2021-11-05T12:53:36",
      "url": "https://stackoverflow.com/questions/69852863/why-is-google-cloud-profiler-throwing-permission-denied-errors-when-running-i"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 68981804,
      "title": "Calling gcloud with different service accounts in parallel and automatically using its project ID",
      "problem": "I know this has been asked many times because of the complete mess Google have made with authentication but I can't find an answer. I'm trying to create a CI pipeline that can use service account credentials from a file. I want to be able to run it locally or from a server. From what I've read gcloud inexplicably ignores the `GOOGLE_APPLICATION_CREDENTIALS` env var so I have to globally set my creds with the following, meaning I can kiss goodbye to any kind of parallelisation:\n```\n`gcloud auth activate-service-account --key-file=$(GOOGLE_APPLICATION_CREDENTIALS)\n`\n```\nSurely it must be possible to run multiple commands in parallel with different SA credentials?\nAlso, the above approach ignores the project ID specified in the key file, so gcloud tries to target the last project ID I personally set for myself.\nIs there a solution to this ridiculousness? I'm looking for a non-interactive, non-destructive (i.e. won't trash my personal creds) way of calling gcloud in parallel with different service accounts and automatically using their project IDs. Is this possible?",
      "solution": "Well it actually is possible with this:\n```\n`    CLOUDSDK_AUTH_CREDENTIAL_FILE_OVERRIDE=$(GOOGLE_CREDENTIALS_FILE) \\\n    CLOUDSDK_CORE_PROJECT=$(GCP_PROJECT) \\\n    gcloud run deploy --allow-unauthenticated $(CLOUD_RUN_CONFIG) --image $(GCR_DOCKER_IMAGE)\n`\n```\nIt's a shame the docs are so poor it's taken me forever to find this info. Why gcloud doesn't just use the same env vars as all the libraries will remain a mystery to everyone outside Google...",
      "question_score": 2,
      "answer_score": 3,
      "created_at": "2021-08-30T11:29:30",
      "url": "https://stackoverflow.com/questions/68981804/calling-gcloud-with-different-service-accounts-in-parallel-and-automatically-usi"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 67016651,
      "title": "Why Gcloud Pub/Sub is used?",
      "problem": "Why Pub/Sub is used?\nUsecase: There is a http triggered \"Cloud function\" on which some Data is sent.\nThis function after processing data, publishes Data to a Pub/Sub topic.\nThen there is another Cloud function, which is triggered based on publishing to that Pub/sub topic.\nSo, This Cloud function takes published data from pub/sub and  inserts it to a BigQuery table.\nSo in this usecase why pub sub is used, why can't we just have  one cloud function which takes data from http hit, and inserts it to BigQuery.\nWhat cam be the design thought given for choosing pub/sub here?\nAlso generally why Pub/sub architecture is used?",
      "solution": "There could be several reasons to use Cloud Pub/Sub in this architecture. One reason would be if there is any fan out or plans to fan out, where the published data ends up not only in BigQuery, but in some other place. Without Pub/Sub, the http-triggered Cloud Function would have to know about all interested receivers of the data and send it to each one of them. With Pub/Sub, any additional service that was interested in the incoming data could create a separate subscription on the data and consume it independently of BigQuery.\nAnother reason would be to be able to batch data inserts into BigQuery without increasing latency on the http requests to the initial Cloud Function. This may be done for efficiency or for doing some kind of cross-event pre-processing on the data coming in. By using Pub/Sub, the first Cloud Function can respond to the request as soon as the publish to Cloud Pub/Sub succeeds, not having to wait for any other requests, and can be sure that the request will ultimately be processed.\nIn the absence of these two either now or in the future, it may make sense to write to BigQuery directly.",
      "question_score": 2,
      "answer_score": 3,
      "created_at": "2021-04-09T09:14:36",
      "url": "https://stackoverflow.com/questions/67016651/why-gcloud-pub-sub-is-used"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 65739044,
      "title": "Cloud Run remove revision tag through CLI",
      "problem": "Is there a way to remove a tag(through CLI) that I've set on a cloud run revision when doing a deployment? I can see how to do it through the UI, but I need to include this in my deployment pipeline so it should be through the CLI \nMy use case is the following:\n\ndeploy a new version of my service with `--no-traffic` flag and `--tag` option in order to make the version accessible\nrun test suite on the newly deployed version\nupdate traffic to point to the new version\nremove the tag from the old/new version\n\nThe reason I want to remove those is that all versions that contain tag are kept accessible which is a problem for me since I'm also using the `min-instances` option.\nEdit: I was using incorrectly labels instead of tags here.",
      "solution": "The `--remove-tags` can be used with the `update-traffic` command. Since every tag can be assigned only to a single run revision this is the correct way to remove a tag from a revision.\n** Always using the same tag for deployment preview/testing also works in the above use case since using the same tag on the latest revision effectively removes this tag from older revisions.",
      "question_score": 2,
      "answer_score": 3,
      "created_at": "2021-01-15T16:39:57",
      "url": "https://stackoverflow.com/questions/65739044/cloud-run-remove-revision-tag-through-cli"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 78248527,
      "title": "Permission &#39;storage.buckets.get&#39; denied on resource (or it may not exist)",
      "problem": "I have created a new service account and attached a custom role. This custom role has the `storage.buckets.get` permission. While using the new service account, I am still getting the following error:\n\ngoogle.api_core.exceptions.Forbidden: 403 GET https://storage.googleapis.com/storage/v1/b/[bucket-name]?projection=noAcl&prettyPrint=false: [service-account-name]@[project-id].iam.gserviceaccount.com does not have storage.buckets.get access to the Google Cloud Storage bucket. Permission 'storage.buckets.get' denied on resource (or it may not exist).\n\nThe python code:\n```\n`storage_client = storage.Client()\nbucket_name = os.getenv('BUCKET_NAME')\nself.bucket = storage_client.get_bucket(bucket_name)\n`\n```\nRoles of the service account:\n```\n`ROLE\nprojects/[ProjectID]/roles/[CustomRole]\nroles/storage.objectCreator\nroles/storage.objectUser\nroles/storage.objectViewer\nroles/viewer\n`\n```\nPermissions of `projects/[ProjectID]/roles/[CustomRole]` role:\n```\n`description: [some-description]\netag: [some-etag]\nincludedPermissions:\n- storage.buckets.get\nname: projects/[ProjectID]/roles/[CustomRole]\nstage: ALPHA\ntitle: [Custom Role Title]\n`\n```\nWhat could be the reason? I am making a query to get a particular bucket using this service account.\nPS: I have waited 15 minutes and can confirm that the role is properly attached to the service account.",
      "solution": "The bucket did not exist and the second part of the error mentions non-existence of the resource, which is the bucket in this case.",
      "question_score": 2,
      "answer_score": 1,
      "created_at": "2024-03-30T16:43:09",
      "url": "https://stackoverflow.com/questions/78248527/permission-storage-buckets-get-denied-on-resource-or-it-may-not-exist"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 77112881,
      "title": "google cloud function times out, apparently during database access",
      "problem": "I've written a 2nd gen Google Cloud Function in Python that pulls data from an external API and should write it to a PostgreSQL database hosted in Google Cloud SQL. When I try to run the function, it times out without changing anything in the database. I added some print() statements to see where it was failing. The sqlalchemy \"engine.create()\" seems to go through without issue, but it seems to not get past the first session.execute(stmt). Running the same function locally (while connecting to the cloud db) using function-framework works without issue, however. I'm using the Python cloud.sql.connector library (with the pg8000 driver) to connect to the database.\nI'm honestly at a loss to explain why this won't work, or what the issue could even be. I've tried using a service account for the Function that has the global Owner role; the database credentials work when I'm running it locally, as well as in another cloud function; I've tried running the function with `curl  -H \"Authorization: Bearer $(gcloud auth print-identity-token)\" https://FUNCTION_URL` from the cloud shell, as well as with `gcloud function call` from my local machine, same result. I'd expect the function to either complete or crash, but instead it just sits there until it times out. The only differences that I perceive between this and another function that connects to the same instance is that this one is a 2nd gen Cloud Function and uses the connector library instead of unix sockets to connect to the database. Same project, same region, same db instance, same GCP service account, same SQL login.\nRelevant (I think) bits of code; bit where engine is created:\n`connector = Connector()\n\ndef getconn() -> pg8000.dbapi.Connection:\n    conn: pg8000.dbapi.Connection = connector.connect(\n        instance_connection_name,\n        \"pg8000\",\n        user=username,\n        password=password,\n        db=database,\n        ip_type=IPTypes.PUBLIC,\n    )\n    return conn\nprint(\"creating db connection...\")\n# create a connection to the database\nengine = create_engine(\"postgresql+pg8000://\",\n                       creator=getconn,\n                       echo=False)\nSession = sessionmaker(bind=engine)\nsession = Session()\nprint(\"db con ok\")\n`\nThe method that won't complete (never prints \"ok\"):\n`def wipe_table(name):\n    print(f\"Wiping staging table {name}...\")\n    stmt = text(f\"TRUNCATE {name};\")\n    session.execute(stmt)\n    print(\"ok\")\n\n`",
      "solution": "This is most likely same issue as Cloud Function cannot connect to Cloud SQL with \"upstream request timeout\" (I recommend updating post with full Cloud Function code for allowing reproduction of error).\nTLDR; the Cloud SQL Python `Connector` object or SQLAlchemy connection pool should be lazy initialized when used with Cloud Functions because it runs background tasks.\nIt turns out global variables that run background tasks can cause issues when run out side of the Cloud Function request context (because Cloud Functions only allocates compute during a request). So Cloud Functions recommends lazy initializing this type of global variable so that the variable is initialized within the request context.\nExample of lazy initializing connection pool is as follows:\n`import functions_framework\nimport sqlalchemy\nfrom google.cloud.sql.connector import Connector, IPTypes\nimport pg8000\n\ndef connect_to_instance() -> sqlalchemy.engine.base.Engine:\n    connector = Connector()\n\n    def getconn() -> pg8000.dbapi.Connection:\n        return connector.connect(\n            \"...\", # the PostgreSQL's instance connection name here\n            \"pg8000\",\n            user     = \"xyz\",\n            password = 'supersecret',\n            db       = \"db_name\",\n            ip_type  = IPTypes.PUBLIC\n        )\n    \n    return sqlalchemy.create_engine(\n        \"postgresql+pg8000://\",\n        creator      = getconn,\n    )\n\n# lazy initialization of global db\ndb = None\n\n@functions_framework.http\ndef hello_http(request):\n    # lazy init within request context\n    global db\n    if not db:\n        db = connect_to_instance()\n    with db.connect() as conn:\n        # ... run queries\n`\nMore details on this same issue can be found here: https://github.com/GoogleCloudPlatform/cloud-sql-python-connector/issues/830",
      "question_score": 2,
      "answer_score": 2,
      "created_at": "2023-09-15T15:44:24",
      "url": "https://stackoverflow.com/questions/77112881/google-cloud-function-times-out-apparently-during-database-access"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 76740460,
      "title": "How do I remove &quot;Allow unauthenticated&quot; from a HTTP onRequest Gen 2 Firebase Function?",
      "problem": "When deploying functions with `gcloud` we can use the flag `--no-allow-unauthenticated` to stop functions from being accessed by anyone.\nThat flag does not exist for Firebase. For example, this does NOT work:\n`firebase deploy --only functions --no-allow-unauthenticated`\nSo currently when I deploy a HTTP onRequest Gen 2 Firebase Function it automatically deploys with \"Allow unauthenticated\" authentication status. This is bad. How do I stop that from happening?\nI want to either remove the \"Allow unauthenticated\" status after deploying, or avoid deploying like that in the first place.\nNote: This is for Gen 2 Firebase Functions. I can't find anything applicable to that. With Gen 1 all I have to do is remove \"allUsers\" role but Gen 2 functions do not have that role and they are still marked as \"Allow unauthenticated\".",
      "solution": "The Firebase CLI can't do it. The default is to allow access because most Firebase developers use Cloud Functions to deploy web APIs that should be accessible publicly as part of their applications.\nYou can use gcloud to modify the permissions after deployment.  The documentation covers adding permissions, with specific instructions for 2nd gen functions.  You can remove the permission by using `remove-iam-policy-binding` instead of `add-iam-policy-binding`.\n```\n`gcloud run services remove-iam-policy-binding FUNCTION_NAME \\\n  --member=\"allUsers\" \\\n  --role=\"roles/run.invoker\"\n`\n```",
      "question_score": 2,
      "answer_score": 2,
      "created_at": "2023-07-21T20:38:12",
      "url": "https://stackoverflow.com/questions/76740460/how-do-i-remove-allow-unauthenticated-from-a-http-onrequest-gen-2-firebase-fun"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 74564789,
      "title": "How to run locally Docker Container with cloud-run service account &amp; GOOGLE_APPLICATION_CREDENTIALS?",
      "problem": "I am trying to run locally a Docker Container of a Ktor app that connects to a Cloud SQL database.\nThis container is later to be pushed to Cloud Run, so I'm using an IAM service account in my environment variables for the authentication. I've been researching this problem for a week and haven't found an answer.\nThe steps I used are the following:\n\nCreate IAM service account \"foo-cloudrun@foo-foo-121212.iam.gserviceaccount.com\nSet the needed Roles for Cloud Run and Cloud SQL:\n\n```\n`   Cloud Build Service Agent\n   Cloud Run Admin\n   Cloud Run Service Agent\n   Cloud SQL Client\n   Cloud SQL Instance User\n   Viewer\n`\n```\n\nCreate a new key from the IAM account and save it to desktop.\nActivate and login to the service account by running the following command on CMD:\n\n```\n` gcloud auth activate-service-account --key-file=\"C:\\Users\\user\\Desktop\\cloudrun-key.json\"\n`\n```\n\nSet the GOOGLE_APPLICATION_CREDENTIALS environment variable to the key file location:\n\n```\n`  set GOOGLE_APPLICATION_CREDENTIALS=\"C:\\Users\\user\\Desktop\\cloudrun-key.json\"\n`\n```\n\nNavigate to my application folder and build the docker image:\n\n```\n` docker build -t my-application .\n`\n```\n\nStart the image:\n\n```\n` docker run -p 8080:8080 my-application\n`\n```\n\nThis is where I'm getting a vague exception: \"Exception in thread \"main\" java.lang.NullPointerException\".\nTo find what's going on I tried to use docker compose to see if I get a better idea. So I run the gradle shadowJar to create a .jar file in the build/libs directory, and then run \"docker-compose up\".\nThis is where I'm getting this exception that baffles me, since I have set the environment variable to the proper location:\n\nCaused by: java.io.IOException: The Application Default Credentials\nare not available. They are available if running in Google Compute\nEngine. Otherwise, the environment variable GOOGLE_APPLICATION_\nCREDENTIALS must be defined pointing to a file defining the\ncredentials. See\nhttps://developers.google.com/accounts/docs/application-default-credentials\nfor more information.\n\nDockerfile as instructed here:\n```\n`FROM gradle:7-jdk11 AS build\n# Copy local code to the container image.\nCOPY --chown=gradle:gradle . /home/gradle/src\nWORKDIR /home/gradle/src\n# Build a release artifact.\nRUN gradle shadowJar --no-daemon \n\nFROM openjdk:11\nEXPOSE 8080:8080\nRUN mkdir /app\n# Copy the jar to the production image from the builder stage.\nCOPY --from=build /home/gradle/src/build/libs/*.jar /app/my-application-docker.jar\n# Run the web service on container startup.\nENTRYPOINT  [ \"java\", \"-jar\", \"/app/my-application-docker.jar\" ]\n`\n```\ndocker-compose.yml file:\n```\n`services:\n  web:\n    build: .\n    ports:\n      - \"8080:8080\"\n    environment:\n      INSTANCE_CONNECTION_NAME: ${INSTANCE_CONNECTION_NAME}\n      DB_IAM_USER: ${DB_IAM_USER}\n      DB_USER: ${DB_USER}\n      DB_NAME: ${DB_NAME}\n      PORT: ${PORT}\n      JDBC_DRIVER: ${JDBC_DRIVER}\n      GOOGLE_APPLICATION_CREDENTIALS: ${GOOGLE_APPLICATION_CREDENTIALS}\n`\n```\nenv. file:\n```\n`JDBC_DRIVER=org.postgresql.Driver\nDB_IAM_USER=foo-cloudrun@foo-foo-121212.iam\nDB_NAME=my-database\nINSTANCE_CONNECTION_NAME=foo-foo-121212:us-central1:database-sql-instance\nPORT=8080\nGOOGLE_APPLICATION_CREDENTIALS=C:\\Users\\user\\Desktop\\cloudrun-key.json\n`\n```\nDatabaseFactory.kt file like this reference:\n```\n`object DatabaseFactory {\n\n    private val INSTANCE_CONNECTION_NAME = System.getenv(\"INSTANCE_CONNECTION_NAME\");\n    private val DB_IAM_USER = System.getenv(\"DB_IAM_USER\")\n    private val DB_NAME = System.getenv(\"DB_NAME\")\n\n    fun init() {\n        Database.connect(hikari())\n\n        transaction {\n            SchemaUtils.create(FirstTable)\n            SchemaUtils.create(SecondTable)\n        }\n    }\n\n    private fun hikari(): HikariDataSource {\n\n        val config = HikariConfig()\n\n        // IAM AUTHENTICATION\n        config.driverClassName = System.getenv(\"JDBC_DRIVER\")\n        config.jdbcUrl = String.format(\"jdbc:postgresql:///%s\", DB_NAME)\n        config.addDataSourceProperty(\"socketFactory\", \"com.google.cloud.sql.postgres.SocketFactory\")\n        config.addDataSourceProperty(\"cloudSqlInstance\", INSTANCE_CONNECTION_NAME)\n        config.addDataSourceProperty(\"enableIamAuth\", \"true\")\n        config.addDataSourceProperty(\"user\", DB_IAM_USER)\n        config.addDataSourceProperty(\"password\", \"password\")\n        config.addDataSourceProperty(\"sslmode\", \"disable\")\n\n        config.connectionTimeout = 10000 // 10s\n        config.maximumPoolSize = 5\n        config.minimumIdle = 3\n        config.maxLifetime = 1800000; // 30 minutes\n        config.isAutoCommit = false\n        config.transactionIsolation = \"TRANSACTION_REPEATABLE_READ\"\n        config.validate()\n\n        // Initialize the connection pool using the configuration object.\n        return HikariDataSource(config);\n\n    }\n\n    suspend fun  dbQuery(block: () -> T): T =\n        withContext(Dispatchers.IO) {\n            transaction {\n                addLogger(StdOutSqlLogger)\n                block()\n            }\n        }\n\n}\n`\n```\nThings that I have tried unsuccessfully:\n\nTesting a Cloud Run service locally . Getting NullPointerException\n\ndocker run -p 8080:8080 -v\n${GOOGLE_APPLICATION_CREDENTIALS}:/tmp/keys/cloudrun-key.json:ro\nmy-application-api\ndocker run -p 8080:8080 -e GOOGLE_APPLICATION_CREDENTIALS=/tmp/keys/cloudrun-key.json\n-v ${GOOGLE_APPLICATION_CREDENTIALS}:/tmp/keys/cloudrun-key.json:ro my-application-api\n\nUsing my gmail user account to run the docker-compose up command by:\n\nUn-setting the environment variable (set GOOGLE_APPLICATION_CREDENTIALS= )\ngcloud auth login (to gmail account)\ngcloud auth application-default login (to set the default credentials to known location)\nrunning again the docker run command or docker-compose up\n\nwhere I'm getting:\n\" Error reading credential file from environment variable GOOGLE_APPLICATION_CREDENTIALS, value 'C:\\Users\\user\\Desktop\\cloudrun-key.json': File does not exist.\"\n\nSetting the GOOGLE_APPLICATION_CREDENTIALS in Windows system environment variables manually through system properties. Again same error.\n\nAny help would be great, thanks.",
      "solution": "Wow that's a lot of question :-)\nFirst The following commands are mutually exclusive-ish:\nThis authenticates `gcloud` and its commands with the Service Account. When you run `gcloud something`, it will use the Service Account as its identity.\n```\n`gcloud auth activate-service-account \\\n--key-file=\"C:\\Users\\user\\Desktop\\cloudrun-key.json\"\n`\n```\nThis enables code built with Google's client libraries to use Application Default Credentials (ADC) to obtain the runtime identity for the code. It does not affect `gcloud`. It does provide an identity to your code.\n\nI assume `set` is a Windows environment variable command\n\n```\n`set GOOGLE_APPLICATION_CREDENTIALS=\"C:\\Users\\user\\Desktop\\cloudrun-key.json\"\n`\n```\nSecond and I'll admit I scanned the question, you need to mount the service account into the container when it runs.\nUsing `docker` or `podman` etc.:\n`docker run \\\n--interactive --tty --rm\n--publish=8080:8080 \\\n--volume=c:\\Users\\user\\Desktop\\cloudrun-key.json:/secrets/key.json \\\n--env=GOOGLE_APPLICATION_CREDENTIALS=/secrets/key.json \\\nmy-application\n`\n\nThe path `c:\\Users\\user\\Desktop\\cloudrun-key.json` is probably incorrect when using Docker. You may need to replace `\\` with `/`. You may need to do something with the `c:`. I don't know how this gets mapped on Windows.\n\nThe `--volume` flag maps the location of the key on your host to (`:`) a location in the container.\nIn this case, in the container, the key will be `/secrets/key.json` and so this is the value to be used for `GOOGLE_APPLICATION_CREDENTIALS`.\nUsing `docker-compose` it's going to be something like:\n```\n`services:\n  web:\n    build: .\n    ports:\n      - \"8080:8080\"\n    volumes:\n    - \"c:\\Users\\user\\Desktop\\cloudrun-key.json:/secrets/key.json\"\n    environment:\n      INSTANCE_CONNECTION_NAME: ${INSTANCE_CONNECTION_NAME}\n      DB_IAM_USER: ${DB_IAM_USER}\n      DB_USER: ${DB_USER}\n      DB_NAME: ${DB_NAME}\n      PORT: ${PORT}\n      JDBC_DRIVER: ${JDBC_DRIVER}\n      GOOGLE_APPLICATION_CREDENTIALS: /secrets/key.json\n`\n```\nThird when you run the container on Cloud Run, it will leverage ADC and automatically use the identity of the Cloud Run service that you deploy. Cloud Run services have a default identity but you can specify an identity at deployment with `gcloud run deploy ... --service-account=${SERVICE_ACCOUNT}`.\nThanks to ADC, you don't need to worry about mounting the key into the container or specifying an environment variable, it will just work.",
      "question_score": 2,
      "answer_score": 2,
      "created_at": "2022-11-24T19:09:40",
      "url": "https://stackoverflow.com/questions/74564789/how-to-run-locally-docker-container-with-cloud-run-service-account-google-appl"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 72005187,
      "title": "Can you use the Google Cloud emulator with Terraform?",
      "problem": "When I setup the emulator and Terraform correctly, will I be able to run terraform with the results inside the emulator and not inside my project in Google Cloud?\nI could not find an answer on the web and cannot start before I know.\nThanks in advance!",
      "solution": "Yes you can! We use it to bootstrap the Google PubSub emulator by reusing our existing production environment configuration.\nThe trick is that you need to override the API Endpoints in the provider configuration:\n```\n`terraform {\n  required_providers {\n    google = {\n      source  = \"hashicorp/google\"\n      version = \"4.33.0\"\n    }\n  }\n}\n\nprovider \"google\" {\n  project = \"some-project-id\"\n  pubsub_custom_endpoint = \"http://localhost:8085/v1/\"\n}\n`\n```\nTo apply this then, I start the emulator like this:\n`$ gcloud beta emulators pubsub start --project=some-project-id\n`\nNote:\n\nThe project-id is specified via the argument and must match the project-id you configure in the terraform provider\nPort 8085 is the default port the emulator starts on\n\nDrawbacks\nSince you're overriding only specific endpoint, you must be careful which resources you create. For example, creating a `google_service_account` will sent that request to the actual Google endpoint.\nThere are emulators for some Google service, but not all! Alternatively, you can override all endpoints in the Terraform Provider to non-existing local ports. This will then fail to apply the plan instead of silently creating resources on prem that you might not expect.",
      "question_score": 2,
      "answer_score": 2,
      "created_at": "2022-04-25T22:06:25",
      "url": "https://stackoverflow.com/questions/72005187/can-you-use-the-google-cloud-emulator-with-terraform"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 70602244,
      "title": "Google Cloud Function Build failed. Error ID: 99f2b037",
      "problem": "Build failed when I try to update code and re-deploy the Google Cloud Function.\nDeploy Script:\n```\n`gcloud functions deploy  --entry-point  \\\n--runtime python37 \\\n--trigger-http \\\n--region=asia-east2 \\\n--memory=8192 \\\n--timeout=540\n`\n```\nError Message:\n```\n`ERROR: (gcloud.functions.deploy) OperationError: code=3, message=Build failed: \n/layers/google.python.pip/pip/bin/python3: Error while finding module specification for 'pip' \n(AttributeError: module '__main__' has no attribute '__file__'); Error ID: 99f2b037\n`\n```\nSource code structure:\n```\n`.\n\u251c\u2500\u2500 lib\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2514\u2500\u2500 azsync.py\n\u251c\u2500\u2500 main.py\n\u2514\u2500\u2500 requirements.txt\n`\n```\nRequirements.txt:\n```\n`flask==1.0\ngcsfs==0.4.0\npandas==0.25.0\nazure-storage-blob==12.0.0\n`\n```",
      "solution": "The release of setuptools  60.3.0 caused AttributeError because of a bug and now Setuptools 60.3.1 is available. You can refer to the GitHub Link here.\nFor more information you can refer to the stackoverflow answer as :\n\nIf you run into this pip error in a Cloud Function, you might consider updating pip in the \"`requirements.txt`\" but if you are in such an unstable Cloud Function the better workaround seems to be to create a new Cloud Function and copy everything in there.\nThe pip error probably just shows that the source script, in this case the `requirements.txt`, cannot be run since the source code is not fully embedded anymore or has lost some embedding in the Google Storage.\nor you give that Cloud Function a second chance and edit, go to Source tab, click on Dropdown Source code to choose Inline Editor and add main.py and requirements.txt manually (Runtime: Python).\u201d",
      "question_score": 2,
      "answer_score": 1,
      "created_at": "2022-01-06T04:51:04",
      "url": "https://stackoverflow.com/questions/70602244/google-cloud-function-build-failed-error-id-99f2b037"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 69436175,
      "title": "Metadata, content-length for GCS objects",
      "problem": "Two things:\n\nI am trying to set custom metadata on a GCS object signed URL.\nI am trying to set a maximum file size on a GCS object signed URL.\n\nUsing the following code:\n```\n`    Map headers = new HashMap<>();\n    headers.put(\"x-goog-meta-\" + usernameKey, username);\n\n    if (StringUtils.hasText(purpose)) {\n      headers.put(\"x-goog-meta-\" + purposeKey, purpose);\n    }\n\n    if (maxFileSizeMb != null) {\n      headers.put(\"x-goog-content-length-range\", String.format(\"0,%d\", maxFileSizeMb * 1048576));\n    }\n\n    List options = new ArrayList<>();\n    options.add(Storage.SignUrlOption.httpMethod(HttpMethod.POST));\n    options.add(Storage.SignUrlOption.withExtHeaders(headers));\n\n    String documentId = documentIdGenerator.generateDocumentId().getFormatted();\n    StorageDocument storageDocument =\n        StorageDocument.builder().id(documentId).path(getPathByDocumentId(documentId)).build();\n    storageDocument.setFormattedName(documentId);\n\n    SignedUrlData.SignedUrlDataBuilder builder =\n        SignedUrlData.builder()\n            .signedUrl(storageInterface.signUrl(gcpStorageBucket, storageDocument, options))\n            .documentId(documentId)\n            .additionalHeaders(headers);\n`\n```\nFirst of all the generated signed URL works and I can upload a document.\nNow I am expecting to see the object metadata through the console view. There is no metadata set though. Also the `content-length-range` is not respected. I can upload a 1.3 MB file when the `content-length-range` is set to `0,1`.\nSomething happens when I upload a bigger file (~ 5 MB), but within the `content-length-range`. I receive an error message: `Metadata part is too large.`.",
      "solution": "As you can see here `content-length-range` requires both a minimum and maximum size. The unit used for the range is bytes, as you can see in this example.\nI also noticed that you used `x-goog-content-length-range`, I found this documentation for it, when using this header take into account:\n\nUse a `PUT` request, otherwise it will be silently ignored.\nIf the size of the request's content is outside the specified range, the request fails and a 400 Bad Request code is returned in the response.\nYou have to set the minimum and maximum size in bytes.",
      "question_score": 2,
      "answer_score": 2,
      "created_at": "2021-10-04T14:42:42",
      "url": "https://stackoverflow.com/questions/69436175/metadata-content-length-for-gcs-objects"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 69172696,
      "title": "gcloud logging with regular expression",
      "problem": "I'm trying to use `gcloud logging` along with regular expression. My query works in the console but I can't get it going via the CLI.\n```\n`gcloud logging read \"resource.type=gce_instance AND protoPayload.authenticationInfo.principalEmail=~'.*@example.com.au'\" --limit=10 --format=json\n`\n```\nI get the error:\n```\n`ERROR: (gcloud.logging.read) INVALID_ARGUMENT: Unparseable filter: unrecognized node at token 'MEMBER'\n`\n```\nI've tried with and without various quotes `'' \"\" \"\\\"\\`\nI also have the same trouble when doing timestamp dates as well:\n```\n`gcloud logging read \"resource.type=gce_instance AND timestamp > '2021-06-15T00:00:00.000000Z'\"\n`\n```\nI get the error:\n```\n`ERROR: (gcloud.logging.read) INVALID_ARGUMENT: Unparseable filter: syntax error at line 1, column 112, token ':';\n`\n```",
      "solution": "Your first `gcloud` expression should look like this:\n```\n`cloud logging read \"resource.type=gce_instance AND protoPayload.authenticationInfo.principalEmail:'.*@example.com.au'\"\n`\n```\nI changed `=` sign to `:`.\nAnd the second one like this:\n```\n`gcloud logging read 'resource.type=gce_instance AND timestamp > \"2021-08-15T00:00:00.000000Z\"'\n`\n```\nI exchanged single with double quotes (literally).\nIt's best to have a quick look at the gcloud logging read command documentation (I figured out a proper syntax this way).",
      "question_score": 2,
      "answer_score": 2,
      "created_at": "2021-09-14T08:18:58",
      "url": "https://stackoverflow.com/questions/69172696/gcloud-logging-with-regular-expression"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 68766580,
      "title": "Google Cloud cron.yaml Intervals",
      "problem": "I'm trying to customise a cronjob to schedule run a function in Google Cloud.\nI've been reading the documentation\nand as per title, one thing confuses me.\nThe documentation mentions that `every` prefix must be used if the function is to be repeated at a `DAILY` interval. But it is not clear on how it works if you want a function to be repeated at a `WEEKLY`, `MONTHLY`, or in my specific case, `YEARLY` intervals.\nI've added `schedule: 1 of jan 00:10` to my cron.yaml, am I correct to assume that this will be repeated every 1st of January, 10 minutes after midnight or will it only run once? Do I need to change it to `schedule: every 1 of jan 00:10` ?\nFor the record, I found a similar question here on SO, but the problem is said question was asked and answered 10 years ago, so I dont know how applicable it still is.",
      "solution": "Yes, it is like you said, `schedule: 1 of jan 00:10` will work for you. It will be repeated on 1st of Jan every year at 12:10 am.\nSimilarly, to run weekly: `schedule: every monday 00:00`\nMonthly: `schedule: 1 of month 09:00`\nFor more such examples and verification you can refer to \u201cCustom interval\u201d section in this documentation",
      "question_score": 2,
      "answer_score": 2,
      "created_at": "2021-08-13T05:58:18",
      "url": "https://stackoverflow.com/questions/68766580/google-cloud-cron-yaml-intervals"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 68354145,
      "title": "Existing host keys found error when ssh in gcp vm instance",
      "problem": "When running `gcloud beta compute ssh --zone xxx --internal-ip --project xxx xxx`\nI am getting the following error but some of my colleagues work just fine and ssh in without any issue.\n```\n`Existing host keys found in /Users/xxx/.ssh/google_compute_known_hosts\nERROR: (gcloud.beta.compute.ssh) [/usr/bin/ssh] exited with return code [255].\n`\n```\nMay I ask what can be the reason causing this and what can I do to resolve the problem?",
      "solution": "This means that a host with the same IP address but with a different fingerprint was found in the known hosts file. This can happen when you create and delete instances and the same external public IP address is used for the VM instance. The fingerprint will be different for the new host (VM instance).\nEdit the file `/Users/xxx/.ssh/google_compute_known_hosts`, find the entry for the VM's IP address and remove the entry.",
      "question_score": 2,
      "answer_score": 2,
      "created_at": "2021-07-12T23:39:21",
      "url": "https://stackoverflow.com/questions/68354145/existing-host-keys-found-error-when-ssh-in-gcp-vm-instance"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 68069971,
      "title": "How to use private IP based backends with google cloud API gateway?",
      "problem": "So I am trying to make Google cloud's API gateway serve requests from a private IP based backend. Currently, the backend is a Kubernetes based service. However, I couldn't find it explicitly being mentioned whether its possible or not.\nHas anyone else encountered such an issue given that its a pretty common use case? It seems possible only when the API gateway infrastructure has a link to the VPC network(route table) or an explicit private connection.",
      "solution": "After looking for a while I think that the best way to do what you are asking is to use Private service connect, this allows private consumption of services across VPC networks that belong to different groups, teams, projects, or organizations and also lets you connect to service producers using endpoints with internal IP addresses in your VPC network.\nHere is a guide of how to use Private Service Connect to access Google APIs.\nthe Google API gateways exist only for serverless product and is intended to be use only against serverless backends(s). It is possible to configure it against public IP\u2019s that are hosted on our Google backends because they leverage the same x-google-backend configuration key-value pairs in the openapi.yaml for API Gateway, but more niche features like authorization on behalf of backend services, or limiting access to backed services hosted on non-serverless platforms like GKE are currently not supported. a possible workaround could be to set up endpoints directly with your GKE cluster you, this documentation could help you first, second, third\nBest regards.",
      "question_score": 2,
      "answer_score": 2,
      "created_at": "2021-06-21T16:43:00",
      "url": "https://stackoverflow.com/questions/68069971/how-to-use-private-ip-based-backends-with-google-cloud-api-gateway"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 67040036,
      "title": "Stop BigQuery from truncating output",
      "problem": "I need to export a query into a json using the `bq` cli (on bash b.t.w.)\nI am using this command:\n```\n`bq --format=json query --use_legacy_sql=false 'SELECT text FROM `..`' | jq -c '.[].text'\n`\n```\nThis works just fine at giving me an output:\n```\n`\"Lorem ipsum dolor sit amet, consectetur adipiscing elit.\"\n\"Mauris vehicula mauris eu tortor blandit pulvinar.\"\n\"Cras semper lacus vitae tellus laoreet porttitor.\"\n\"Duis fringilla dolor ut justo porttitor faucibus.\"\n\"etc.\"\n`\n```\nJust like I want. The only problem is that this stops at around 100 lines, and I want all of the data in the sql table, not just a sample size. I know how to do this through the Google Cloud console - simply being a matter of executing the query and clicking export - but I cannot figure out how to do it through the cli as necessary.\nI imagine this is something that people would like to do pretty often, so if you have any information it would be of extraordinarily great use to me, and I'm sure anyone else that sees this tread.\nThank you",
      "solution": "per Command-line tool reference\n```\n`--max_rows or -n\nAn integer specifying the number of rows to return in the query results. \nThe default value is 100.\n`\n```",
      "question_score": 2,
      "answer_score": 2,
      "created_at": "2021-04-11T01:01:03",
      "url": "https://stackoverflow.com/questions/67040036/stop-bigquery-from-truncating-output"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 66824075,
      "title": "Google App Engine Flexible deploy throwing ERROR: (gcloud.app.deploy) HttpError accessing",
      "problem": "Hi I am getting this error when deploying nodejs application to flexible engine. I am unable to figure out where the issue is happening.\nThe error message I am getting\n```\n`ERROR: (gcloud.app.deploy) HttpError accessing : response: , content \nThis may be due to network connectivity issues. Please check your network settings, and the status of the service you are trying to reach.\n`\n```\napp.yaml\n```\n`runtime: nodejs\nenv: flex\nservice: testws\nmanual_scaling:\n  instances: 1\nnetwork:\n  session_affinity: true\nresources:\n  cpu: 1\n  memory_gb: 1\n  disk_size_gb: 2\nenv_variables:\n  KEY: \"dev\"\n  PRIVATE_KEY_URL: \"key\"\nskip_files:\n  - ^node_modules/.*$\n`\n```\nGcloud SDK version\n```\n`Google Cloud SDK 333.0.0\nbq 2.0.65\ncore 2021.03.19\ngsutil 4.60\nkubectl 1.17.17\n`\n```\nI have added the debug logs by running gcloud app deploy --verbosity=\"debug\"\ndebug logs\n```\n`>\nThis may be due to network connectivity issues. Please check your network settings, and the status of the service you are trying to reach.\nTraceback (most recent call last):\n  File \"/Users/shashi/google-cloud-sdk/lib/googlecloudsdk/calliope/cli.py\", line 982, in Execute\n    resources = calliope_command.Run(cli=self, args=args)\n  File \"/Users/shashi/google-cloud-sdk/lib/googlecloudsdk/calliope/backend.py\", line 809, in Run\n    resources = command_instance.Run(args)\n  File \"/Users/shashi/google-cloud-sdk/lib/surface/app/deploy.py\", line 130, in Run\n    use_legacy_apis=args.use_legacy_apis)\n  File \"/Users/shashi/google-cloud-sdk/lib/googlecloudsdk/command_lib/app/deploy_util.py\", line 678, in RunDeploy\n    ignore_file=args.ignore_file)\n  File \"/Users/shashi/google-cloud-sdk/lib/googlecloudsdk/command_lib/app/deploy_util.py\", line 455, in Deploy\n    extra_config_settings)\n  File \"/Users/shashi/google-cloud-sdk/lib/googlecloudsdk/api_lib/app/appengine_api_client.py\", line 172, in DeployService\n    extra_config_settings)\n  File \"/Users/shashi/google-cloud-sdk/lib/googlecloudsdk/api_lib/app/appengine_api_client.py\", line 249, in _CreateVersion\n    return self.client.apps_services_versions.Create(create_request)\n  File \"/Users/shashi/google-cloud-sdk/lib/googlecloudsdk/third_party/apis/appengine/v1/appengine_v1_client.py\", line 830, in Create\n    config, request, global_params=global_params)\n  File \"/Users/shashi/google-cloud-sdk/lib/third_party/apitools/base/py/base_api.py\", line 729, in _RunMethod\n    http, http_request, **opts)\n  File \"/Users/shashi/google-cloud-sdk/lib/third_party/apitools/base/py/http_wrapper.py\", line 350, in MakeRequest\n    check_response_func=check_response_func)\n  File \"/Users/shashi/google-cloud-sdk/lib/third_party/apitools/base/py/http_wrapper.py\", line 406, in _MakeRequestNoRetry\n    check_response_func(response)\n  File \"/Users/shashi/google-cloud-sdk/lib/googlecloudsdk/api_lib/util/apis.py\", line 267, in _CheckResponseForApiEnablement\n    http_wrapper.CheckResponse(response)\n  File \"/Users/shashi/google-cloud-sdk/lib/third_party/apitools/base/py/http_wrapper.py\", line 223, in CheckResponse\n    raise exceptions.BadStatusCodeError.FromResponse(response)\napitools.base.py.exceptions.BadStatusCodeError: HttpError accessing : response: , content \n`\n```",
      "solution": "I found the reason for the error and the solution to fix it if anyone is facing this issue.\nReason - The App engine Flexible Service Account was accidentally deleted from the google cloud project. As mentioned in this link - service-account\n```\n`The App Engine flexible environment service agent has the App Engine Flexible Environment Service Agent role. The role includes a set of permissions needed by Node.js flexible environment to manage your flexible environment apps. For example, this role includes permissions to perform the following tasks:\n\n 1. Deploying a new version.\n 2. Stopping or deleting existing versions.\n 3. Automatic weekly restarts and system updates.\n\n`\n```\nSolution - Add a service account by going to IAM and adding an account of name mentioned below with role App Engine Flexible Environment Service Agent. As mentioned in this link - restore-service-account\n\nservice-[YOUR_PROJECT_NUMBER]@gae-api-prod.google.com.iam.gserviceaccount.com\n\nI have raised an issue with google tracker to change the error message. You can track the issue here - issue-tracker\nUpdate - adding the below image for further clarity. if you try to add the deleted account, it will be suggested by the system.",
      "question_score": 2,
      "answer_score": 1,
      "created_at": "2021-03-26T21:43:41",
      "url": "https://stackoverflow.com/questions/66824075/google-app-engine-flexible-deploy-throwing-error-gcloud-app-deploy-httperror"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 66639920,
      "title": "gsutil acl ch -u AllUsers:R gs://&lt;bucket&gt;/file.js Not working",
      "problem": "I am trying to use command `gsutil acl ch -u AllUsers:R gs:///file.js` to give read access to `file.js` It giving an error as `CommandException: AllUsers:R gs://file.js is an invalid change description.`",
      "solution": "`allUsers` not `AllUsers`:\nhttps://cloud.google.com/storage/docs/gsutil/commands/iam\nSo:\n```\n`gsutil acl ch -u allUsers:R gs:///file.js\n`\n```",
      "question_score": 2,
      "answer_score": 2,
      "created_at": "2021-03-15T15:31:05",
      "url": "https://stackoverflow.com/questions/66639920/gsutil-acl-ch-u-allusersr-gs-bucket-file-js-not-working"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 79456180,
      "title": "How to get the latest key version in Google Cloud KMS?",
      "problem": "Assuming you have a key name what is the best way to find the latest version of that key?\nI had assumed:\n`    masterKeyVersionIterator := client.ListCryptoKeyVersions(ctx, &kmspb.ListCryptoKeyVersionsRequest{\n        Parent:   masterKeyName,\n        PageSize: 1,\n        OrderBy:  \"createTime desc\", // Can't order by \"name desc\" because it is probably string sort\n    })\n`\nBut it seems like only ordering by name is allowed.\nSo, is the only solution something like this?\n`    masterKeyVersionIterator := client.ListCryptoKeyVersions(ctx, &kmspb.ListCryptoKeyVersionsRequest{\n        Parent:   masterKeyName,\n        PageSize: 2147483647, // int32 max\n    })\n\n    masterKeyLatest := &kmspb.CryptoKeyVersion{CreateTime: timestamppb.New(time.Time{})}\n    for {\n        versionCursor, err := masterKeyVersionIterator.Next()\n        if err == iterator.Done {\n            break\n        }\n        if err != nil {\n            log.Fatalln(err)\n        }\n        if versionCursor.CreateTime.AsTime().After(masterKeyLatest.CreateTime.AsTime()) {\n            masterKeyLatest = versionCursor\n        }\n    }\n`\n(But I believe that is actually not totally correct either because there is a maximum page size that gcloud will send I believe, meaning that you would need to wrap this in another loop iterating the page token)\nThis is a very inelegant approach, is there no better way?",
      "solution": "`// Can't order by \"name desc\" because it is probably string sort\n`\nThis isn't true. `name` is an output only field (documentation) and it's always set to an increasing number.\nWhen sorted, it's done numerically.\nAn example:\n`        listCryptoKeyVersionsReq := &kmspb.ListCryptoKeyVersionsRequest{\n                Parent: cryptoKey,\n                OrderBy: \"name desc\",\n        }\n\n        it := client.ListCryptoKeyVersions(ctx, listCryptoKeyVersionsReq)\n\n        for {\n                resp, err := it.Next()\n                if err == iterator.Done {\n                        break\n                }   \n                if err != nil {\n                        log.Fatalf(\"Failed to list key rings: %v\", err)\n                }   \n\n                version, _ := strings.CutPrefix(resp.Name, cryptoKey)\n                fmt.Printf(\"key version: %s\\n\", version)\n        }\n`\nOutput:\n`$ go run main.go \nkey version: /cryptoKeyVersions/10\nkey version: /cryptoKeyVersions/9\nkey version: /cryptoKeyVersions/8\nkey version: /cryptoKeyVersions/7\nkey version: /cryptoKeyVersions/6\nkey version: /cryptoKeyVersions/5\nkey version: /cryptoKeyVersions/4\nkey version: /cryptoKeyVersions/3\nkey version: /cryptoKeyVersions/2\nkey version: /cryptoKeyVersions/1\n`",
      "question_score": 2,
      "answer_score": 1,
      "created_at": "2025-02-21T02:26:14",
      "url": "https://stackoverflow.com/questions/79456180/how-to-get-the-latest-key-version-in-google-cloud-kms"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 78153565,
      "title": "Error 403 Forbidden while uploading to a gcloud signed url from the client side with next js",
      "problem": "I have set up a gcloud bucket with the right permissions and created a service account that has the role storage creator. I have also written a go backend that generates the signed url and passes it to next. The signed url is working as I have tried it with a curl command and it uploaded everything correctly.  The problem arises when I try to upload a file from my nextjs application. I have a simple form and a onSubmit handler that gets the file from the form, passes it to formData and uploads the whole thing. For some reason I get the 403 Forbidden error message. I tried looking at some bucket logs but there was nothing. (maybe i didn't set up the logging correctly).I tried to do the same with some vanilla js still didn't work.\nHere is my nextjs code.\n`\"use client\"\nimport { getSignedURLs } from \"../../upload/upload\"\n\nexport default function Images(props) {\n  const submitImages = (e) => {\n    e.preventDefault()\n    const form = e.currentTarget\n    const data = new FormData(form)\n    const file = data.get(\"file\")\n\n    getSignedURLs(1).then((r) => {\n      console.log(r[0])\n      let signed = r[0]\n      console.log(`${file.type}`)\n      fetch(signed, {\n        headers: {\n          \"Content-Type\": `${file.type}`,\n        },\n        method: \"PUT\",\n        body: file,\n      })\n        .then((response) => {\n          if (!response.ok) {\n            throw new Error(\"Network response was not ok\")\n          }\n          return response.json()\n        })\n        .then((data) => {\n          console.log(\"Response data:\", data)\n        })\n        .catch((error) => {\n          console.error(\"There was a problem with your fetch operation:\", error)\n        })\n    })\n  }\n  return (\n    \n      \n      Upload\n    \n  )\n}\n`\nNote that I have tested the getSignedUrl function and I'm sure it returns the correct url.\nI've also played around with the content type but with no luck.\nThis is now my vanilla js.\n`document\n  .getElementById(\"uploadForm\")\n  .addEventListener(\"submit\", function (event) {\n    event.preventDefault();\n\n    const formData = new FormData();\n    formData.append(\"file\", this.file.files[0]);\n    getSignedURLs(1).then((r) => {\n      let signed = r[0];\n      fetch(signed, {\n        method: \"PUT\",\n        body: formData,\n      })\n        .then((response) => {\n          if (!response.ok) {\n            throw new Error(\"Network response was not ok\");\n          }\n          return response.json();\n        })\n        .then((data) => {\n          console.log(\"Response data:\", data);\n        })\n        .catch((error) => {\n          console.error(\n            \"There was a problem with your fetch operation:\",\n            error\n          );\n        });\n    });\n  });\n`\nUPDATE:\nTrying out a few things I noticed that while using sites like https://restninja.io/ to make a request everything works fine. It doesn't work tho when I specify a content-type, only when I leave it empty. This doesn't solve my problem as i still cannot reproduce the desired behaviour in my js.",
      "solution": "Try these to override browser's default headers:\n```\n`headers: {\n  \"Content-Length\": file.size,\n  \"Content-Type\": file.type,\n},\n`\n```",
      "question_score": 2,
      "answer_score": 1,
      "created_at": "2024-03-13T12:51:54",
      "url": "https://stackoverflow.com/questions/78153565/error-403-forbidden-while-uploading-to-a-gcloud-signed-url-from-the-client-side"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 78000747,
      "title": "How to pull docker image from Google Artifact Registry in k8s deployment.yaml via imagePullSecrets",
      "problem": "I have a pod deployment in my helm chart.\nI want my pod deployment to pull the docker image from google artifact registry.\nI created a service account in gcloud, gave artifact registry reader permissions to this service account then created a json key under the service account which I want to use to login to artifact registry from any machine.\nI'm installing pod on openshift.\nI want to use gcloud service account json key only as a mode of authentication to google artifact registry.\nThis works:\nI converted my service account json key to base64 and passed to docker login which gave me successful login to artifact registry.\n`cat service_account_key_base64.json | docker login -u _json_key_base64 --password-stdin `\nNow I can pull any docker image under google artifact registry.\n`docker pull  `\nThis doesn't work:\nI created a secret using my service account json key like this:\n```\n`apiVersion: v1 \nkind: Secret \nmetadata:\n  name: gcloud-secret\ndata:\n  .dockerconfigjson: \ntype: kubernetes.io/dockerconfigjson\n\n`\n```\n`oc create -f `\nAdded glcoud-secret as imagePullSecrets in my pod deployment file\n```\n`imagePullSecrets:\n- name: gcloud-secret\n`\n```\nWhen I try to install my helm chart, pod goes to ImagePullBackOff state and describing pod gives me this error:\n`Requesting bearer token: invalid status code from registry 403 (Forbidden)`",
      "solution": "Found the only way to do which is to create secret as mentioned below so that kubernetes can pull the image from google artifactory registry\nfirst docker login manually on your machine\n```\n`cat service_account_key_base64.json | docker login -u _json_key_base64 --password-stdin \n`\n```\nthen:\n```\n`cat ~/.docker/config.json | base64 -w 0\n`\n```\ncopy the long string which you get in your secret.yaml file as shown below.\nsecret.yaml file\n```\n`apiVersion: v1\n  kind: Secret\n  metadata:\n    name: glcoud_secret\n  data:\n    .dockerconfigjson: \n  type: kubernetes.io/dockerconfigjson\n`\n```\nNow create the secret:\n```\n`$ kubectl create -f Provide the secret name in your imagePullSecrets.",
      "question_score": 2,
      "answer_score": 1,
      "created_at": "2024-02-15T13:22:22",
      "url": "https://stackoverflow.com/questions/78000747/how-to-pull-docker-image-from-google-artifact-registry-in-k8s-deployment-yaml-vi"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 74236700,
      "title": "Is `gcloud storage` billing us for transfers of public data?",
      "problem": "We're switching over our scripts from using `gsutil` to the reportedly faster `gcloud storage`. However we access a significant amount of public data, for example from `gs://gcp-public-data--broad-references`.\nWe do NOT want to pay to download this public data. However it appears that `gcloud storage` is automatically setting the `X-Goog-User-Project` header for public transfers while `gsutil` does not.\nIs my understanding of the various documentation correct that `glcoud storage` is instructing GCS to bill us and not the public bucket for transfers?\n\nRun `gcloud version`\n\nOn my machine this outputs `Google Cloud SDK 407.0.0` and `gsutil 5.15`\n\nRun `gcloud init`\n\nLog in\nSelect a google project\n\nRun `gcloud config list`\n\nVerify the project you selected before has been configured\n\nRun `gsutil -d ls gs://gcp-public-data--broad-references`\n\nVerify that the request `Headers:` do NOT contain `X-Goog-User-Project`\n\nRun `gcloud --log-http storage ls gs://gcp-public-data--broad-references`\n\nVerify that under `== headers start ==` your default project has been included as the `X-Goog-User-Project`\n\nAccording to all the documentation I've been able to find one should not set that header by default.\nVia https://cloud.google.com/storage/docs/requester-pays:\n\nImportant: Buckets that have Requester Pays disabled still accept requests that include a billing project, and charges are applied to the billing project supplied in the request. Consider any billing implications prior to including a billing project in all of your requests.\n\nVia https://cloud.google.com/storage/docs/xml-api/reference-headers#xgooguserproject:\n\nThe project specified in the header is billed for charges associated with the request. This header is used, for example, when making requests to buckets that have Requester Pays enabled.\n\nBonus:\n\nRun `gsutil ls gs://gnomad-public-requester-pays`\n\nYou should receive an error `BadRequestException: 400 Bucket is a requester pays bucket but no user project provided.`\n\nRun `gcloud storage ls gs://gnomad-public-requester-pays`\n\nThe bucket contents should be listed\n\nThe latter above doesn't seem correct to me as I never intentionally told `gcloud storage` which project to bill for the request.",
      "solution": "Update: This behavior seems to have been fixed as of the Google Cloud  SDK 411.0.0 released 2022-12-06. As of that version running the setup specified in the original question no longer sends the `X-Goog-User-Project` header.\nThanks @carbocation for the heads up about the fix!\n\nHeard back from a support member after this was reposted to the Google Cloud Community Forums.\nErnestoC said:\n\nThe default behavior of the Cloud CLI gcloud is to use the current project for all quota and billing operations. This is why you automatically see your project ID passed in X-Goog-User-Project. This behavior can be overridden though by adding the global --billing-project flag to any command.\nIf you set this flag to an empty string, no project is passed in the request. I tested this with gcloud storage and confirmed that requester pays buckets return the expected error message (\u201c400: Bucket is a requester pays bucket but no user project provided.\u201d). Non-requester pays buckets allow operations as well.",
      "question_score": 2,
      "answer_score": 1,
      "created_at": "2022-10-28T16:17:59",
      "url": "https://stackoverflow.com/questions/74236700/is-gcloud-storage-billing-us-for-transfers-of-public-data"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 71380482,
      "title": "Creating signed url by impersonating service account from google function",
      "problem": "I was successfully able to create signed urls from my gcloud instance by running these commands :\n```\n`gcloud iam service-accounts add-iam-policy-binding \\\n   service-account-name@project-id.iam.gserviceaccount.com \\\n --member=\"serviceAccount:instance-name@project-name.iam.gserviceaccount.com\" \\\n --role=\"roles/iam.serviceAccountTokenCreator\"\n\ngsutil -i service-account-name@project-name.iam.gserviceaccount.com signurl -d 40m -r region --use-service-account gs://project-name/file-name\n`\n```\nNow I'd like to do the same in my google function, where I'm not supposed to store the private keys and create signed urls like the one suggested in the gcloud docs.\nI tried working with this : gsalrashid123/gcs_signedurl but I'm running into some errors.\nCode :\n```\n`import subprocess\nimport shlex\nimport google.auth\nfrom google.auth import impersonated_credentials\nfrom google.auth import compute_engine\nfrom datetime import datetime, timedelta\nfrom google.cloud import storage\ndef hello_world(request):\n  bucket_name = \"\"\n  sa_email =  \"\"\n  credentials, project = google.auth.default()\n  print(\"credentials : {}\\nproject : {}\".format(credentials, project))\n  storage_client = storage.Client()\n  data_bucket = storage_client.bucket(bucket_name)\n  blob = data_bucket.get_blob(\"audio.wav\")\n  expires_at_ms = datetime.now() + timedelta(minutes=30)\n  signing_credentials = impersonated_credentials.Credentials(\n    source_credentials=credentials,\n    target_principal=sa_email,\n    target_scopes = 'https://www.googleapis.com/auth/devstorage.read',\n    lifetime=2)\n  signed_url = blob.generate_signed_url(expires_at_ms, credentials=signing_credentials)\n  return signed_url, 200, {'Content-Type': 'text/plain'}\n`\n```\nError :\n```\n`Traceback (most recent call last): File \"/layers/google.python.pip/pip/lib/python3.9/site-packages/flask/app.py\", line 2073, in wsgi_app response = self.full_dispatch_request() File \"/layers/google.python.pip/pip/lib/python3.9/site-packages/flask/app.py\", line 1518, in full_dispatch_request rv = self.handle_user_exception(e) File \"/layers/google.python.pip/pip/lib/python3.9/site-packages/flask/app.py\", line 1516, in full_dispatch_request rv = self.dispatch_request() File \"/layers/google.python.pip/pip/lib/python3.9/site-packages/flask/app.py\", line 1502, in dispatch_request return self.ensure_sync(self.view_functions[rule.endpoint])(**req.view_args) File \"/layers/google.python.pip/pip/lib/python3.9/site-packages/functions_framework/__init__.py\", line 99, in view_func return function(request._get_current_object()) File \"/workspace/main.py\", line 25, in hello_world signed_url = blob.generate_signed_url(expires_at_ms, credentials=signing_credentials) File \"/layers/google.python.pip/pip/lib/python3.9/site-packages/google/cloud/storage/blob.py\", line 620, in generate_signed_url return helper( File \"/layers/google.python.pip/pip/lib/python3.9/site-packages/google/cloud/storage/_signing.py\", line 396, in generate_signed_url_v2 signed_query_params = get_signed_query_params_v2( File \"/layers/google.python.pip/pip/lib/python3.9/site-packages/google/cloud/storage/_signing.py\", line 81, in get_signed_query_params_v2 signature_bytes = credentials.sign_bytes(string_to_sign.encode(\"ascii\")) File \"/layers/google.python.pip/pip/lib/python3.9/site-packages/google/auth/impersonated_credentials.py\", line 296, in sign_bytes raise exceptions.TransportError( google.auth.exceptions.TransportError: Error calling sign_bytes: {'error': {'code': 400, 'message': 'Request contains an invalid argument.', 'status': 'INVALID_ARGUMENT'}}\n`\n```\nI'm guessing I have to add something related to my gfunc as a `--member` in the first command, since now I only have the instance as a member. But I'm unaware as to how I can do that. I also used the `--impersonate-service-account` option along with the gcloud deploy command, but it didn't work.",
      "solution": "Posting John Hanley's comment and Tony Stark's comment as community wiki for visibility.\nThe error occurred because the `--impersonate-service-account` which OP used is only having the scope `devstorage.read` which is not enough to sign data.\nThe following article from John Hanley helped in troubleshooting and resolving the issue.",
      "question_score": 2,
      "answer_score": 1,
      "created_at": "2022-03-07T12:37:17",
      "url": "https://stackoverflow.com/questions/71380482/creating-signed-url-by-impersonating-service-account-from-google-function"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 71364907,
      "title": "Cannot create google cloud sql instance using gcloud cli because it is taking too long",
      "problem": "I am running the following command using the gcloud cli tool...\n```\n`gcloud sql instances create sql-db-1 --database-version=MYSQL_8_0 --region=us-central --tier=db-f1-micro\n`\n```\nIt sits in the terminal for a long time with the following output...\n```\n`Creating new cloud sql instance...\n`\n```\nand then finally fails with...\n```\n`ERROR: (gcloud.sql.instances.create) Operation https://sqladmin.googleapis.com/sql/v1beta4/projects/my-project/operations/0d9534c4-9c70-4a77-86a9-ae5c6d3b5fd8 is taking longer than expected. You can continue waiting for the operation by running `gcloud beta sql operations wait --project my-project 0d9534c4-9c70-4a77-86a9-ae5c6d3b5fd8`\nStatus : FAIL 1 b''\n`\n```\nThis same command was working for me reliably, and then all of a sudden it just started doing this.",
      "solution": "When you execute the command line, it creates the Cloud SQL instance, but the CLI throws an ERROR making you think that this wasn\u2019t created; so I opened an issue tracker report for you.\nMeanwhile you can also run after the error:\n`gcloud beta sql operations wait --project my-project xxxxxxxxxxxxxxxxxxxxxx`\nAnd wait for the operation to complete. After that, you must see STATUS as DONE:\n```\n`NAME                                  TYPE    START                          END                            ERROR  STATUS\nXXXXXXXXXXXXXXXXXXXXXXXXX             CREATE  2022-03-09T21:53:40.532+00:00  2022-03-09T22:06:53.389+00:00  -      DONE\n`\n```\nWhen you open this link in the ERROR:\n`https://sqladmin.googleapis.com/sql/v1beta4/projects/my-project/operations/0d9534c4-9c70-4a77-86a9-ae5c6d3b5fd8`\nYou will get:\n`{\n  \"error\": {\n    \"code\": 401,\n    \"message\": \"Request is missing required authentication credential. Expected OAuth 2 access token, login cookie or other valid authentication credential. See https://developers.google.com/identity/sign-in/web/devconsole-project.\",\n    \"errors\": [\n      {\n        \"message\": \"Login Required.\",\n        \"domain\": \"global\",\n        \"reason\": \"required\",\n        \"location\": \"Authorization\",\n        \"locationType\": \"header\"\n      }\n    ],\n    \"status\": \"UNAUTHENTICATED\",\n    \"details\": [\n      {\n        \"@type\": \"type.googleapis.com/google.rpc.ErrorInfo\",\n        \"reason\": \"CREDENTIALS_MISSING\",\n        \"domain\": \"googleapis.com\",\n        \"metadata\": {\n          \"service\": \"sqladmin.googleapis.com\",\n          \"method\": \"google.cloud.sql.v1beta4.SqlOperationsService.Get\"\n        }\n      }\n    ]\n  }\n}\n`",
      "question_score": 2,
      "answer_score": 1,
      "created_at": "2022-03-05T19:32:58",
      "url": "https://stackoverflow.com/questions/71364907/cannot-create-google-cloud-sql-instance-using-gcloud-cli-because-it-is-taking-to"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 71354512,
      "title": "Google Cloud Tasks - Maximum number of tasks in a single queue?",
      "problem": "Is there a limit set for how many tasks can be created in a single queue? I can't seem to find this info anywhere.\nI will probably not execute more than 100 at a time, but I will need to have a lot more waiting in the queue.",
      "solution": "No, there's none. The only limit mentioned is for the number of Queues that can be added (1000 default, can be increased on your quotas page).\nSee full details:\nhttps://cloud.google.com/tasks/docs/quotas",
      "question_score": 2,
      "answer_score": 1,
      "created_at": "2022-03-04T17:43:09",
      "url": "https://stackoverflow.com/questions/71354512/google-cloud-tasks-maximum-number-of-tasks-in-a-single-queue"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 70755904,
      "title": "Google cloud object detection model training error",
      "problem": "I have a problem training a computer vision Model in google could, I am sure that the problem is related to GPU. I know that google say be default you have 1 GPU put the training fails with this message error :\n\"The request for 8 K80 accelerators exceeds the allowed maximum of 0 A100, 0 K80, 0 P100, 0 P4, 0 T4, 0 TPU_V2, 0 TPU_V2_POD, 0 TPU_V3, 0 TPU_V3_POD, 0 V100 accelerators.\"\nyou can se i have 0  from all accelerators\nhere is my full command i am trying to run :\n```\n`gcloud ai-platform jobs submit training segmentation_maskrcnn_test_0 ^\n--runtime-version 2.1 ^\n--python-version 3.7 ^\n--job-dir=gs://image-segmentation-b/training-process ^\n--package-path ./object_detection ^\n--module-name object_detection.model_main_tf2 ^\n--region us-central1 ^\n--scale-tier CUSTOM ^\n--master-machine-type n1-highcpu-32 ^\n--master-accelerator count=8,type=nvidia-tesla-k80 ^\n-- ^\n--model_dir=gs://image-segmentation-b/training-process ^\n--pipeline_config_path=gs:gs://image-segmentation-b/mask_rcnn_inception_resnet_v2_1024x1024_coco17_gpu-8 - cloud.config\n`\n```\nand here is the full error :\n```\n`ERROR: (gcloud.ai-platform.jobs.submit.training) HttpError accessing : response: , content \nThis may be due to network connectivity issues. Please check your network settings, and the status of the service you are trying to reach.\n`\n```\nHow can I fix this error? Do I have to go somewhere and enable GPU for the project?",
      "solution": "You need to raise your GPU quota before you can train your models.\nEither your project, or your account does not have enough GPU quota to fulfill your request.\nYou can check your quotas here: API Quotas",
      "question_score": 2,
      "answer_score": 1,
      "created_at": "2022-01-18T13:52:31",
      "url": "https://stackoverflow.com/questions/70755904/google-cloud-object-detection-model-training-error"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 70742448,
      "title": "gcloud Dataflow Drain Command Wait Until Job Finished Draining",
      "problem": "I'm currently building cd pipeline that replace existing Google Cloud Dataflow streaming pipeline with the new one with bash command. The old and new has the same name job. And I write bash command like this\n`gcloud dataflow jobs drain \"${JOB_ID}\" --region asia-southeast2 && \\\ngcloud dataflow jobs run NAME --other-flags\n`\nThe problem with this command is that the first command doesn't wait until the job finish draining so that the second command throw error because duplicated job name.\nIs there a way to wait until dataflow job finish draining? Or is there any better way?\nThanks!",
      "solution": "Seeing as this post hasn't garnered any attention, I will be posting my comment as a post:\nDataflow jobs are asynchronous to the command `gcloud dataflow jobs run`, so when you use `&&` the only thing that you'll be waiting on will be for the command to finish and since that command is just to get the process started (be it draining a job or running one) it finishes earlier than the job/drain does.\nThere are a couple of ways you could wait for the job/drain to finish, both having some added cost:\n\nYou could use a Pub/Sub step as part of a larger Dataflow job (think of it as a parent to the jobs you are draining and running, with the jobs you are draining or running sending a message to Pub/Sub about their status once it changes) - you may find the cost of Pub/Sub [here].\nYou could set up some kind of loop to repeatedly check the status of the job you're draining/running, likely inside of a bash script, though that can be a bit more tedious and isn't as neat as a listener, and it would require one's own computer/connection to be maintained or a GCE instance.",
      "question_score": 2,
      "answer_score": 1,
      "created_at": "2022-01-17T14:49:32",
      "url": "https://stackoverflow.com/questions/70742448/gcloud-dataflow-drain-command-wait-until-job-finished-draining"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 69406883,
      "title": "Removing &quot;child said into stdout/stderr&quot; prefix from logs in GAE php74 standard environment",
      "problem": "I'm running a php74 standard environment in Google App Engine and outputting logs to `php://stdout`. Unfortunately, all the logs are prefixed with `child X said into stdout` and all the PHP errors are prefixed with `child X said into stderr`. Outside of GAE, I've addressed this by modifying the php-fpm runtime config to set `catch_workers_output = yes` but on the standard environment, you can't change the runtime config, you can only set `php.ini` changes.\nIs there a way to remove this prefix on the standard environment?",
      "solution": "As per the App Engine's Standard configuration reference where all elements are listed, there is no option to remove the log prefix.\nSince it's a full list of options, such a feature would be mentioned there if it was supported.\nIn this case, you may want to create a new Feature request for GCP to consider it.",
      "question_score": 2,
      "answer_score": 1,
      "created_at": "2021-10-01T15:41:02",
      "url": "https://stackoverflow.com/questions/69406883/removing-child-said-into-stdout-stderr-prefix-from-logs-in-gae-php74-standard"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "gcloud",
      "question_id": 67421917,
      "title": "Google Cloud Functions - How to import a Python package (via PIP) from a GCP Repository in another project?",
      "problem": "I need to have private Python packages in GCP usable in multiple projects. I haven't tried the Artifact Registry since that's still in alpha, so right now I've been trying with simple repositories, but I'm open to alternatives.\nI have a Python package source code in a GCP Repository in Project A, and I have a cloud function in a repository also in Project A. In this cloud function I import the mentioned package by adding `git+https://source.developers.google.com/p/project-a/r/my-python package` in my `requirements.txt` file.\nIf I deploy this cloud function in Project A via `gcloud functions` in my terminal, specifying `--source=https://source.developers.google.com/projects/project-a/repos/my-cloud-function` and `--project=project-a`, it works fine, and the function can successfully import the elements from the package when I call it, but if I deploy this function in Project B instead, I get the following error:\n```\n`Deploying function (may take a while - up to 2 minutes)...failed.                                                                             \nERROR: (gcloud.functions.deploy) OperationError: code=3, message=Build failed: `pip_download_wheels` had stderr output:\n  Running command git clone -q https://source.developers.google.com/p/project-a/r/my-python-package /tmp/pip-req-build-f_bcp4y9\n  remote: PERMISSION_DENIED: The caller does not have permission\n  remote: [type.googleapis.com/google.rpc.RequestInfo]\n  remote: request_id: \"abe4(...)\"\n  fatal: unable to access 'https://source.developers.google.com/p/project-a/r/my-python-package/': The requested URL returned error: 403\nERROR: Command errored out with exit status 128: git clone -q https://source.developers.google.com/p/project-a/r/my-python-package /tmp/pip-req-build-f_bcp4y9 Check the logs for full command output.\n`\n```\nThis seems like a permissions issue. However, if I remove the package dependency from `requirements.txt`, it deploys fine, which means that Project B indeed has access to repos from Project A, so it seems like a issue inside Pip. However, Pip has no problems if I deploy to Project A, so I'm a little lost.\nMany thanks in advance.",
      "solution": "Artifact Registry is GA and no longer on Alpha/Beta since last year.\nI replicated your issue. The error is indeed due to permissions, it didn't happened on the deployment when you remove the line on the `requirements.txt`, probably because the credentials had access to both projects.\nIn order to make the deployment correct you have to add the permissions on the repository to the service account that makes the deployment (which is the CF service account) that can be found on `Cloud Functions` - `(select your Cloud Function)` - `Details`, it should be something like `project@appspot.gserviceaccount.com`\nOnce you have located the service account add it to the Cloud Repository by clicking on `Settings` - `Permissions` and add it with at least the `Source Repository Reader` role",
      "question_score": 2,
      "answer_score": 1,
      "created_at": "2021-05-06T18:05:09",
      "url": "https://stackoverflow.com/questions/67421917/google-cloud-functions-how-to-import-a-python-package-via-pip-from-a-gcp-rep"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 68611817,
      "title": "Can&#39;t deploy Cloud Functions because of &quot;Unhandled error cleaning up build images&quot;",
      "problem": "I've deployed hundreds of function and this is the first time I encounter this issue. Simply, it stops deploying function process, saying:\n\nUnhandled error cleaning up build images. This could result in a small monthly bill if not corrected. You can attempt to delete these images by redeploying or you can delete them manually at https://console.cloud.google.com/gcr/images/[project-name]/us/gcf\n\nThe way I deploy is through Firebase CLI command: firebase deploy --only functions:nameOfFunction\nQuestion is what are those images I have to delete? Why? How can I solve it?",
      "solution": "Cloud Functions uses another product called Cloud Build to build the server images that actually get deployed.  Those images are stored in Cloud Storage, and that storage is billed to your account.\nRead more about it:\n\nhttps://github.com/firebase/firebase-tools/issues/3404\nhttps://krasimirtsonev.com/blog/article/firebase-gcp-saving-money\n\nWatch:\n\nhttps://www.youtube.com/watch?v=aHaI0jZ5rwM\n\nYou should be able to locate and delete the files manually in the Google Cloud console.  But it sounds like there is a bug here with the files not being cleaned up automatically, so you contact Firebase support directly.",
      "question_score": 42,
      "answer_score": 17,
      "created_at": "2021-08-01T17:22:18",
      "url": "https://stackoverflow.com/questions/68611817/cant-deploy-cloud-functions-because-of-unhandled-error-cleaning-up-build-image"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 69897000,
      "title": "Parsing error: Cannot read file &#39;\\tsconfig.json&#39; eslint after following Firebase Cloud Functions initialization instructions",
      "problem": "Problem\nRight after my TypeScript project initialization in VSCode using firebase tools for composing Firebase Cloud Functions following the official documentation the very first line of the `index.ts` file displays an error:\n`Parsing error: Cannot read file '\\tsconfig.json'    eslint [1,1]`\n\nand the `.eslintrc.js` displays an error:\n`File is a CommonJS module; it may be converted to an ES6    module.ts(80001)`\n\nSince all files are auto-generated these errors are a complete surprise and I want to get rid of them.\nVersions\nFor the record, here are the versions installed:\n```\n`npm      --version 8.1.3\ntsc      --version 4.4.4\nnode     --version 17.0.1\nfirebase --version 9.22.0\n`\n```\nInstallation process\nThese are the commands I used in the powershell in VSCode with some info/warnings:\n```\n`>npm install -g firebase-tools\nnpm WARN deprecated har-validator@5.1.5: this library is no longer supported\nnpm WARN deprecated uuid@3.4.0: Please upgrade  to version 7 or higher.  Older versions may use Math.random() in certain circumstances, which is known to be problematic.  See https://v8.dev/blog/math-random for details.\nnpm WARN deprecated request@2.88.2: request has been deprecated, see https://github.com/request/request/issues/3142\n...\n\n>firebase init\n...\nnpm WARN EBADENGINE Unsupported engine {\nnpm WARN EBADENGINE   package: undefined,\nnpm WARN EBADENGINE   required: { node: '14' },\nnpm WARN EBADENGINE   current: { node: 'v17.0.1', npm: '8.1.3' }\nnpm WARN EBADENGINE }\n...\n\n>npm install firebase-functions@latest firebase-admin@latest --save\n...\nnpm WARN deprecated har-validator@5.1.5: this library is no longer supported\nnpm WARN deprecated uuid@3.4.0: Please upgrade  to version 7 or higher.  Older versions may use Math.random() in certain circumstances, which is known to be problematic.  See https://v8.dev/blog/math-random for details.\nnpm WARN deprecated request@2.88.2: request has been deprecated, see https://github.com/request/request/issues/3142\n...\n\n>firebase deploy\n...\nC:\\Users\\SAMS\\firebase_dogout\\functions\\src\\index.ts\n  1:13  warning  'functions' is defined but never used  @typescript-eslint/no-unused-vars\n...\nError: functions predeploy error: Command terminated with non-zero exit code2\n`\n```\nFiles\ntsconfig.json\n```\n`{\n  \"compilerOptions\": {\n    \"module\": \"commonjs\",\n    \"noImplicitReturns\": true,\n    \"noUnusedLocals\": true,\n    \"outDir\": \"lib\",\n    \"sourceMap\": true,\n    \"strict\": true,\n    \"target\": \"es2017\"\n  },\n  \"compileOnSave\": true,\n  \"include\": [\n    \"src\"\n  ]\n}\n`\n```\n.eslintrc.js\n```\n`module.exports = {\n  root: true,\n  env: {\n    es6: true,\n    node: true,\n  },\n  extends: [\n    \"eslint:recommended\",\n    \"plugin:import/errors\",\n    \"plugin:import/warnings\",\n    \"plugin:import/typescript\",\n    \"google\",\n    \"plugin:@typescript-eslint/recommended\",\n  ],\n  parser: \"@typescript-eslint/parser\",\n  parserOptions: {\n    project: [\"tsconfig.json\", \"tsconfig.dev.json\"],\n    sourceType: \"module\",\n  },\n  ignorePatterns: [\n    \"/lib/**/*\", // Ignore built files.\n  ],\n  plugins: [\n    \"@typescript-eslint\",\n    \"import\",\n  ],\n  rules: {\n    \"quotes\": [\"error\", \"double\"],\n    \"import/no-unresolved\": 0,\n  },\n};\n\n`\n```\nindex.ts\n```\n`import * as functions from \"firebase-functions\";\n\nexport const helloWorld = functions.https.onRequest((request, response) => {\n  functions.logger.info(\"Hello logs!\", {structuredData: true});\n  response.send(\"Hello from Firebase!\");\n});\n`\n```\nWhat I tried\n\nReinitialization\nCreating the `.vscode` folder with `settings.json` file with\n\n```\n` {\n   \"eslint.workingDirectories\": [\n       \"src\" // and \"functions\"\n   ]\n }\n`\n```\n\nUpdating your eslintrc.json file with the following line: `\"project\":\"PROJECT_NAME/tsconfig.json\"`\nUpdating `.eslintrc.js` by setting `tsconfigRootDir: __dirname` in `parserOptions`\nDeleting the ESLint extension. The error was gone, but `firebase deploy` command didn't allow the code deployement.\n\nSo, the related thread didn't really help",
      "solution": "Ok, I have solved the problem with a great help of this github thread\nFalse positive Error - TS6133 error (declared but its value is never read) report.\nI have changed `\"noUnusedLocals\"` setting in the `tsconfig.json` file from the default `true` to `false`, so the file becomes:\n```\n`\"compilerOptions\": {\n    ...\n    \"noUnusedLocals\": false,\n    ...\n  }\n`\n```\nHowever, the strange thing is that after successfully deploying functions to `Firebase Cloud Fuctions` using this setting and afterwards reverting the changes, the redeployment doesn\u00b4t show any error and is successful as well.\nUpdate\nAfter retrying it a few time I must admit that the solution is different.\nTurns out that the related stackoverflow thread did help, in particular @cherryblossom solution. Setting `tsconfigRootDir: __dirname` in the `.eslintrc.js` file and restarting the VSCode solves the problem.\n`.eslintrc.js`:\n```\n`module.exports = {\n  // ...\n  parserOptions: {\n    ...\n    tsconfigRootDir: __dirname,\n    ...\n  },\n  // ...\n}\n`\n```",
      "question_score": 35,
      "answer_score": 70,
      "created_at": "2021-11-09T12:04:44",
      "url": "https://stackoverflow.com/questions/69897000/parsing-error-cannot-read-file-tsconfig-json-eslint-after-following-firebase"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 76990628,
      "title": "Firebase Functions: Could not create or update Cloud Run service, Container Healthcheck failed",
      "problem": "On `deploy` i got this error\n`i  functions: creating Node.js 18 (2nd Gen) function addCourseData(us-central1)...\nCould not create or update Cloud Run service addcoursedata, Container Healthcheck failed. Revision 'addcoursedata-00001-cup' is not ready and cannot serve traffic. The user-provided container failed to start and listen on the port defined provided by the PORT=8080 environment variable. Logs for this revision might contain more information.\n\nLogs URL: https://console.cloud.google.com/logs/viewer?project=PROJECT_ID&resource=cloud_run_revision/service_name/addcoursedata/revision_name/addcoursedata-00001-cup&advancedFilter=resource.type%3D%22cloud_run_revision%22%0Aresource.labels.service_name%3D%22addcoursedata%22%0Aresource.labels.revision_name%3D%22addcoursedata-00001-cup%22\nFor more troubleshooting guidance, see https://cloud.google.com/run/docs/troubleshooting#container-failed-to-start\n\nFunctions deploy had errors with the following functions:\n        addCourseData(us-central1)\ni  functions: cleaning up build files...\n`\nwith `--debug` I got this log\n`Total Function Deployment time: 67749\n[] 1 Functions Deployed\n[] 1 Functions Errored\n[] 0 Function Deployments Aborted\n[] Average Function Deployment time: 67748\n\nFunctions deploy had errors with the following functions:\n        addCourseData(us-central1)\n[] Not printing URL for HTTPS function. Typically this means it didn't match a filter or we failed deployment\n\nFunctions deploy failed.\n[] {\n  \"endpoint\": {\n    \"id\": \"addCourseData\",\n    \"project\": \"PROJECT_ID\",\n    \"region\": \"us-central1\",\n    \"entryPoint\": \"addCourseData\",\n    \"platform\": \"gcfv2\",\n    \"runtime\": \"nodejs18\",\n    \"httpsTrigger\": {},\n    \"labels\": {\n      \"deployment-tool\": \"cli-firebase\"\n    },\n    \"serviceAccount\": null,\n    \"ingressSettings\": null,\n    \"availableMemoryMb\": null,\n    \"timeoutSeconds\": null,\n    \"maxInstances\": null,\n    \"minInstances\": null,\n    \"concurrency\": 80,\n    \"vpc\": null,\n    \"environmentVariables\": {\n      \"FIREBASE_CONFIG\": \"{\\\"projectId\\\":\\\"PROJECT_ID\\\",\\\"databaseURL\\\":\\\"https://PROJECT_ID-default-rtdb.asia-southeast1.firebasedatabase.app\\\",\\\"storageBucket\\\":\\\"PROJECT_ID.appspot.com\\\"}\",\n      \"GCLOUD_PROJECT\": \"PROJECT_ID\",\n      \"EVENTARC_CLOUD_EVENT_SOURCE\": \"projects/PROJECT_ID/locations/us-central1/services/addCourseData\"\n    },\n    \"codebase\": \"default\",\n    \"securityLevel\": \"SECURE_ALWAYS\",\n    \"cpu\": 1,\n    \"targetedByOnly\": true,\n    \"hash\": \"38475170b79b25f455db5cacbdc1d6c36adc4679\"\n  },\n  \"op\": \"update\",\n  \"original\": {\n    \"name\": \"FirebaseError\",\n    \"children\": [],\n    \"exit\": 1,\n    \"message\": \"Could not create or update Cloud Run service addcoursedata, Container Healthcheck failed. Revision 'addcoursedata-00001-sox' is not ready and cannot serve traffic. The user-provided container failed to start and listen on the port defined provided by the PORT=8080 environment variable. Logs for this revision might contain more information.\\n\\nLogs URL: https://console.cloud.google.com/logs/viewer?project=PROJECT_ID&resource=cloud_run_revision/service_name/addcoursedata/revision_name/addcoursedata-00001-sox&advancedFilter=resource.type%3D%22cloud_run_revision%22%0Aresource.labels.service_name%3D%22addcoursedata%22%0Aresource.labels.revision_name%3D%22addcoursedata-00001-sox%22 \\nFor more troubleshooting guidance, see https://cloud.google.com/run/docs/troubleshooting#container-failed-to-start\",\n    \"status\": 3,\n    \"code\": 3\n  }\n}\n[] Error: Failed to update function addcourseData in region us-central1\n    at C:\\Users\\USER_ABC\\AppData\\Roaming\\npm\\node_modules\\firebase-tools\\lib\\deploy\\functions\\release\\fabricator.js:51:11\n    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)\n    at async Fabricator.updateV2Function (C:\\Users\\USER_ABC\\AppData\\Roaming\\npm\\node_modules\\firebase-tools\\lib\\deploy\\functions\\release\\fabricator.js:380:32)\n    at async Fabricator.updateEndpoint (C:\\Users\\USER_ABC\\AppData\\Roaming\\npm\\node_modules\\firebase-tools\\lib\\deploy\\functions\\release\\fabricator.js:153:13)\n    at async handle (C:\\Users\\USER_ABC\\AppData\\Roaming\\npm\\node_modules\\firebase-tools\\lib\\deploy\\functions\\release\\fabricator.js:88:17)\n\nError: There was an error deploying functions\n`\nHow to fix it? Any help!\nMy code is\n`const calledFunctionName = process.env.K_SERVICE;\nif (!calledFunctionName || calledFunctionName === \"addCourseData\") {\n  const {onRequest} = require(\"firebase-functions/v2/https\");\n\n  // Take the email and adds an entry to Firestore with Course data\n  exports.addCourseData = onRequest(async (request, response) => {\n    return await (await require(\"./my-functions/course/add-course-data-function\"))\n        .addCourseData(request, response);\n  });\n}\n`\nEverything was working fine before introducing the `process.env.K_SERVICE`, so is there anything else to do too, to use the environment variable? As I just added in code to save the cold start and unnecessary file loading.\nWhy server not able to create a container for the function?",
      "solution": "I need to change this line\n`if (!calledFunctionName || calledFunctionName === \"addCourseData\")\n`\nto\n`if (!calledFunctionName || calledFunctionName === \"addcoursedata\")\n`\nOR (better approach would be)\n`if (!calledFunctionName || calledFunctionName === \"addCourseData\".toLowerCase())\n`\nas `process.env.K_SERVICE` returns the lower alphabets ONLY, instead of camelCase or name-with-hyphens.\nFinally, I found the answer to my issue after 2 days of researching and debugging. Maybe it saves someone's time!",
      "question_score": 22,
      "answer_score": 7,
      "created_at": "2023-08-28T09:02:16",
      "url": "https://stackoverflow.com/questions/76990628/firebase-functions-could-not-create-or-update-cloud-run-service-container-heal"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 67723703,
      "title": "Firebase function: Callable request verification passed",
      "problem": "I have defined a firebase function like this:\n```\n`exports.getTestResults = functions.region(\"europe-west3\").https.onCall((data, context) => {\n    return {\n        test: 'Test 123'\n    }\n})\n`\n```\nIf I call this function as follows\n```\n`var getTestResults = firebase.app().functions('europe-west3').httpsCallable('getTestResults');\ngetTestResults({ }).then(response => {\n    console.log(response);\n}).catch(ex => {\n    console.log('EXC: ' + ex);\n})\n`\n```\nI get this error/warning/whatever in the firebase function log\n```\n`Callable request verification passed {\"verifications\":{\"app\":\"MISSING\",\"auth\":\"VALID\"}}\n`\n```\nWhat is the cause of this?",
      "solution": "You are not using Firebase App Check, are you?\nUse Firebase App Check to verify that the request for your callable function is coming from your app, not from any other client.\nIf you use Firebase App Check, Cloud Functions won't throw this error.\nCheck this for more info.",
      "question_score": 22,
      "answer_score": 9,
      "created_at": "2021-05-27T15:58:12",
      "url": "https://stackoverflow.com/questions/67723703/firebase-function-callable-request-verification-passed"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 70487806,
      "title": "Must use import to load ES Module .eslintrc.js",
      "problem": "I am trying to fix this problem for hours. I've read nearly every post about this, but still, I came to no solution.\nI am trying to deploy a firebase-function with the \"https got-library\" dependency, but no matter what I do, nothing works. I am not the best with node-js or typescript (usually a kotlin frontend-dev), so I have no clue what the error wants from me.\nTsconfig.json\n```\n`{\n  \"compilerOptions\": {\n    \"module\": \"commonjs\",\n    \"noImplicitReturns\": true,\n    \"noUnusedLocals\": false,\n    \"esModuleInterop\": true,\n    \"outDir\": \"lib\",\n    \"sourceMap\": true,\n    \"strict\": true,\n    \"target\": \"es2017\"\n  },\n  \"compileOnSave\": true,\n  \"include\": [\n    \"src\"\n  ],\n}\n`\n```\n.eslintrc.js\n```\n`module.exports = {\n  root: true,\n  env: {\n    es6: true,\n    node: true,\n  },\n  extends: [\n    \"eslint:recommended\",\n    \"plugin:import/errors\",\n    \"plugin:import/warnings\",\n    \"plugin:import/typescript\",\n    \"google\",\n    \"plugin:@typescript-eslint/recommended\",\n  ],\n  parser: \"@typescript-eslint/parser\",\n  parserOptions: {\n    project: [\"tsconfig.json\", \"tsconfig.dev.json\"],\n    sourceType: \"module\",\n  },\n  ignorePatterns: [\n    \"/lib/**/*\", // Ignore built files.\n  ],\n  plugins: [\n    \"@typescript-eslint\",\n    \"import\",\n  ],\n  rules: {\n    \"quotes\": [\"error\", \"double\"],\n    \"import/no-unresolved\": 0,\n    \"linebreak-style\": [\"error\", \"windows\"],\n    \"indent\": \"off\",\n    \"object-curly-spacing\": \"off\",\n    \"no-tabs\": 0,\n    \"max-len\": \"off\",\n    \"require-jsdoc\": 0,\n    \"no-empty\": [0, \"allow-empty-functions\", \"allow-empty-catch\"],\n    \"@typescript-eslint/no-explicit-any\": [\"off\"],\n    \"@typescript-eslint/naming-convention\": [\"off\"],\n    \"@typescript-eslint/explicit-function-return-type\": \"off\",\n    \"@typescript-eslint/explicit-module-boundary-types\": \"off\",\n    \"@typescript-eslint/no-var-requires\": \"off\",\n    \"no-mixed-spaces-and-tabs\": 0,\n    \"camelcase\": 0,\n  },\n};\n`\n```\npackage.json\n```\n`{\n  \"name\": \"functions\",\n  \"scripts\": {\n    \"lint\": \"eslint --ext .js,.ts .\",\n    \"build\": \"tsc\",\n    \"serve\": \"npm run build && firebase emulators:start --only functions\",\n    \"shell\": \"npm run build && firebase functions:shell\",\n    \"start\": \"npm run shell\",\n    \"deploy\": \"firebase deploy --only functions\",\n    \"logs\": \"firebase functions:log\"\n  },\n  \"engines\": {\n    \"node\": \"14\"\n  },\n  \"type\": \"module\",\n  \"main\": \"lib/index.js\",\n  \"dependencies\": {\n    \"@google-cloud/functions-framework\": \"^2.1.0\",\n    \"@types/stripe\": \"^8.0.417\",\n    \"firebase-admin\": \"^10.0.1\",\n    \"firebase-functions\": \"^3.14.1\",\n    \"firebase-tools\": \"^10.0.1\",\n    \"form-data\": \"^4.0.0\",\n    \"got\": \"^12.0.0\",\n    \"iso3166-alpha-converter\": \"^1.0.0\",\n    \"mailgun.js\": \"^4.1.0\",\n    \"stripe\": \"^8.193.0\"\n  },\n  \"devDependencies\": {\n    \"@typescript-eslint/eslint-plugin\": \"^5.8.0\",\n    \"@typescript-eslint/parser\": \"^5.8.0\",\n    \"eslint\": \"^8.5.0\",\n    \"eslint-config-google\": \"^0.14.0\",\n    \"eslint-plugin-import\": \"^2.22.0\",\n    \"firebase-functions-test\": \"^0.3.3\",\n    \"typescript\": \"^4.5.4\"\n  },\n  \"private\": true\n}\n`\n```\nError\n```\n`Error [ERR_REQUIRE_ESM]: Must use import to load ES Module: C:\\Users\\ImMor\\Documents\\FirebaseFunctions\\functions\\.eslintrc.js\nrequire() of ES modules is not supported.\nrequire() of C:\\Users\\ImMor\\Documents\\FirebaseFunctions\\functions\\.eslintrc.js from C:\\Users\\ImMor\\Documents\\FirebaseFunctions\\functions\\node_modules\\@eslint\\eslintrc\\dist\\eslintrc.cjs is an ES module file as it is a .js file whose nearest parent package.json contains \"type\": \"module\" which defines all .js files in that package scope as ES modules.\nInstead rename .eslintrc.js to end in .cjs, change the requiring code to use import(), or remove \"type\": \"module\" from C:\\Users\\ImMor\\Documents\\FirebaseFunctions\\functions\\package.json.\n\n    at Object.Module._extensions..js (internal/modules/cjs/loader.js:1080:13)\n    at Module.load (internal/modules/cjs/loader.js:928:32)\n    at Function.Module._load (internal/modules/cjs/loader.js:769:14)\n    at Module.require (internal/modules/cjs/loader.js:952:19)\n    at Object.module.exports [as default] (C:\\Users\\ImMor\\Documents\\FirebaseFunctions\\functions\\node_modules\\import-fresh\\index.js:32:59)\n    at loadJSConfigFile (C:\\Users\\ImMor\\Documents\\FirebaseFunctions\\functions\\node_modules\\@eslint\\eslintrc\\dist\\eslintrc.cjs:2531:47)\n    at loadConfigFile (C:\\Users\\ImMor\\Documents\\FirebaseFunctions\\functions\\node_modules\\@eslint\\eslintrc\\dist\\eslintrc.cjs:2615:20)\n    at ConfigArrayFactory.loadInDirectory (C:\\Users\\ImMor\\Documents\\FirebaseFunctions\\functions\\node_modules\\@eslint\\eslintrc\\dist\\eslintrc.cjs:2808:34)\n    at CascadingConfigArrayFactory._loadConfigInAncestors (C:\\Users\\ImMor\\Documents\\FirebaseFunctions\\functions\\node_modules\\@eslint\\eslintrc\\dist\\eslintrc.cjs:3772:46)\n    at CascadingConfigArrayFactory.getConfigArrayForFile (C:\\Users\\ImMor\\Documents\\FirebaseFunctions\\functions\\node_modules\\@eslint\\eslintrc\\dist\\eslintrc.cjs:3693:18)\n`\n```\nFunction I want to deploy\n```\n`// THIS IS MAKING THE PROBLEM\nimport got from \"got\";\n\nexport async function doOnDeletedUser(\n    // SOME OTHER STUFF\n) {\n    const uid = user.uid;\n    // SOME OTHER STUFF\n}\n`\n```\nEdit\nChanging `.eslintrc.js` to `.eslintrc.cjs` solves this problem, but then I get the following error:\n```\n`ReferenceError: exports is not defined\n    at file:///C:/Users/ImMor/Documents/FirebaseFunctions/functions/lib/index.js:24:23\n    at ModuleJob.run (internal/modules/esm/module_job.js:152:23)\n    at async Loader.import (internal/modules/esm/loader.js:166:24)\n    at async loadModule (C:\\Users\\ImMor\\AppData\\Roaming\\npm\\node_modules\\firebase-tools\\lib\\deploy\\functions\\runtimes\\node\\triggerParser.js:16:20)\n    at async C:\\Users\\ImMor\\AppData\\Roaming\\npm\\node_modules\\firebase-tools\\lib\\deploy\\functions\\runtimes\\node\\triggerParser.js:34:15\n`\n```\nEdit 2\nRemoving \"type\":\"module\" then gives me this error again:\n```\n`Error [ERR_REQUIRE_ESM]: Must use import to load ES Module: C:\\Users\\ImMor\\Documents\\FirebaseFunctions\\functions\\node_modules\\ky\\distribution\\index.js\nrequire() of ES modules is not supported.\nrequire() of C:\\Users\\ImMor\\Documents\\FirebaseFunctions\\functions\\node_modules\\ky\\distribution\\index.js from C:\\Users\\ImMor\\Documents\\FirebaseFunctions\\functions\\lib\\auth\\onDelete\\onDeletedUser.f.js is an ES module file as it is a .js file whose nearest parent package.json contains \"type\": \"module\" which defines all .js files in that package scope as ES modules.\nInstead rename index.js to end in .cjs, change the requiring code to use import(), or remove \"type\": \"module\" from C:\\Users\\ImMor\\Documents\\FirebaseFunctions\\functions\\node_modules\\ky\\package.json.\n\n    at Object.Module._extensions..js (internal/modules/cjs/loader.js:1080:13)\n    at Module.load (internal/modules/cjs/loader.js:928:32)\n    at Function.Module._load (internal/modules/cjs/loader.js:769:14)\n    at Module.require (internal/modules/cjs/loader.js:952:19)\n    at require (internal/modules/cjs/helpers.js:88:18)\n    at Object. (C:\\Users\\ImMor\\Documents\\FirebaseFunctions\\functions\\lib\\auth\\onDelete\\onDeletedUser.f.js:27:30)\n    at Module._compile (internal/modules/cjs/loader.js:1063:30)\n    at Object.Module._extensions..js (internal/modules/cjs/loader.js:1092:10)\n    at Module.load (internal/modules/cjs/loader.js:928:32)\n    at Function.Module._load (internal/modules/cjs/loader.js:769:14)\n    at Module.require (internal/modules/cjs/loader.js:952:19)\n    at require (internal/modules/cjs/helpers.js:88:18)\n    at Object. (C:\\Users\\ImMor\\Documents\\FirebaseFunctions\\functions\\lib\\index.js:39:27)\n    at Module._compile (internal/modules/cjs/loader.js:1063:30)\n    at Object.Module._extensions..js (internal/modules/cjs/loader.js:1092:10)\n    at Module.load (internal/modules/cjs/loader.js:928:32)\n`\n```\nEdit 3\nFollowing changes have been made:\n\nChange \"module\" : \"commonjs\" -> \"module\": \"es6\"\nChange \"target\" : \"es2017\" -> \"target\": \"es6\"\nAdd \"moduleResolution\": \"node\" in tsconfig.js (inside compileroptions).\nAdd \"type\":\"module\" to package.json\n\nNow I am getting the following error:\n```\n`Error [ERR_MODULE_NOT_FOUND]: Cannot find module 'C:\\Users\\ImMor\\Documents\\FirebaseFunctions\\functions\\lib\\utils\\constants.f' imported from C:\\Users\\ImMor\\Documents\\FirebaseFunctions\\functions\\lib\\index.js\n    at finalizeResolution (internal/modules/esm/resolve.js:276:11)\n    at moduleResolve (internal/modules/esm/resolve.js:699:10)\n    at Loader.defaultResolve [as _resolve] (internal/modules/esm/resolve.js:810:11)\n    at Loader.resolve (internal/modules/esm/loader.js:86:40)\n    at Loader.getModuleJob (internal/modules/esm/loader.js:230:28)\n    at ModuleWrap. (internal/modules/esm/module_job.js:56:40)\n    at link (internal/modules/esm/module_job.js:55:36)\n`\n```\nHere is the code (constants.f.ts)\nexport const constants = {\n/**\n* Default firebase-functions region\n*/\nregion: \"europe-west1\",\n} as const;\nEdit 4\nLooks like I've fixed all my problems. When using \"module\": \"es6\", one has to import its module like this:\n```\n`import { constants } from \"./utils/constants.js\";\n`\n```\nand not like this:\n```\n`import { constants } from \"./utils/constants\";\n`\n```\nThe ending \".js\" is important",
      "solution": "Quoting from the ESLint documentation:\n\nuse `.eslintrc.cjs` when running ESLint in JavaScript packages that specify `\"type\":\"module\"` in their `package.json`. Note that ESLint does not support ESM configuration at this time.\n\nSince you have `\"type\": \"module\"` in your package.json file, you should be fine by renaming \".eslintrc.js\" to \".eslintrc.cjs\".",
      "question_score": 18,
      "answer_score": 37,
      "created_at": "2021-12-26T16:51:47",
      "url": "https://stackoverflow.com/questions/70487806/must-use-import-to-load-es-module-eslintrc-js"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 74362083,
      "title": "Cloud Functions Puppeteer cannot open browser",
      "problem": "My setup in GCF:\n\ninstall `npm install --save puppeteer` from project cloud shell\n\nedit package.json like so:\n`{ \"dependencies\": { \"puppeteer\": \"^19.2.2\" } }`\n\npaste code from medium.com into index.js:\nhttps://gist.githubusercontent.com/Alezco/b9b7ce4ec7ee7f208818e395225fcbbe/raw/8554acc8b311a10e272f5d1b98dce3400945bb00/index.js\n\ndeploy with 2 GB RAM, 0-3 instances, max 500s timeout\n\nI get these errors after building or opening the URL:\n\nInternal Server Error\nCould not find Chromium (rev. 1056772). This can occur if either 1. you did not perform an installation before running the script (e.g. `npm install`) or 2. your cache path is incorrectly configured (which is: /workspace/.cache/puppeteer). For (2), check out our guide on configuring puppeteer at https://pptr.dev/guides/configuration.\n\nWhen I run `npm list` both webdriver and puppeteer are installed. I suspect there is an issue this Path but I cannot figure out where it should lead.\nI could then provide puppeteer.launch() with argument `executablePath` which might solve the problem.\nI tried reinstalling puppeteer and changing configuration. No luck.",
      "solution": "The following example runs on Cloud Functions Gen 2 with Node.js 16 (I did not manage to get Node.js 18 to work).\nJS file with puppeteer function:\n\r\n\r\n`const puppeteer = require('puppeteer')\n\nlet browserPromise = puppeteer.launch(\n    {\n    args: [\n        '--no-sandbox'\n    ]\n}\n);\n\nexports.productads = async (req, res) => {\n  /* Your function goes here*/\n}`\r\n\r\n\r\n\nYou need to have .puppeteerrc.cjs:\n\r\n\r\n`const {join} = require('path');\nmodule.exports = {\n  cacheDirectory: join(__dirname, '.cache', 'puppeteer')\n};`\r\n\r\n\r\n\nAnd package.json similar to this:\n\r\n\r\n`{\n  \"name\": \"puppeteer\",\n  \"version\": \"1.0.0\"\n  \"description\": \"\",\n  \"main\": \"index.js\",\n  \"scripts\": {\n    \"gcp-build\": \"node node_modules/puppeteer/install.js\"\n  },\n  \"devDependencies\": {\n    \"@google-cloud/functions-framework\": \"^3.1.2\"\n  },\n  \"dependencies\": {\n    \"puppeteer\": \"^19.2.2\"\n  }\n}`\r\n\r\n\r\n\nBoth files should placed be next to the .js file:\nSee the image",
      "question_score": 17,
      "answer_score": 7,
      "created_at": "2022-11-08T15:06:07",
      "url": "https://stackoverflow.com/questions/74362083/cloud-functions-puppeteer-cannot-open-browser"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 68814180,
      "title": "&#39;darwin-arm64v8&#39; binaries cannot be used on the &#39;darwin-x64&#39; platform",
      "problem": "I am trying to deploy functions to firebase using my Mac M1, for which it was required to do an `npm install` to install packages in `node_modules/`.\nI am getting this error:\n```\n`Error: 'darwin-arm64v8' binaries cannot be used on the 'darwin-x64' platform. Please remove the 'node_modules/sharp' directory and run 'npm install' on the 'darwin-x64' platform.\n    at Object.hasVendoredLibvips (/Users/ali/Desktop/tajir/backend-mvp/appengine/back-end-flex/node_modules/sharp/lib/libvips.js:80:13)\n    at Object. (/Users/ali/Desktop/tajir/backend-mvp/appengine/back-end-flex/node_modules/sharp/lib/constructor.js:7:22)\n    at Module._compile (internal/modules/cjs/loader.js:1136:30)\n    at Module._compile (pkg/prelude/bootstrap.js:1394:32)\n    at Object.Module._extensions..js (internal/modules/cjs/loader.js:1156:10)\n    at Module.load (internal/modules/cjs/loader.js:984:32)\n    at Function.Module._load (internal/modules/cjs/loader.js:877:14)\n    at Module.require (internal/modules/cjs/loader.js:1024:19)\n    at Module.require (pkg/prelude/bootstrap.js:1338:31)\n    at require (internal/modules/cjs/helpers.js:72:18)\n\n`\n```",
      "solution": "Usually someone having a Mac M1 would have this issue. The Mac M1 processor is `arm64`. There was a solution posted here which requires to change terminal architecture to `arch -x86_64 zsh` which I did not want to do.\nSo, that's the workaround I was able to discover (some of the steps also mentioned in the error):\n```\n`rm -rf node_modules/sharp\nnpm install --arch=x64 --platform=darwin sharp\n`\n```",
      "question_score": 15,
      "answer_score": 26,
      "created_at": "2021-08-17T10:37:49",
      "url": "https://stackoverflow.com/questions/68814180/darwin-arm64v8-binaries-cannot-be-used-on-the-darwin-x64-platform"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 69853038,
      "title": "firebase function with realtime database error",
      "problem": "I am new to firebase function and trying to use firebase function with Realtime database (Emulator suite).But when i try to set the value in firebase using the firebase function,it gives an error and doesn't set the value in database.\nError:\n```\n`17:33:14\nI\nfunction[us-central1-textToLength]\n[2021-11-05T12:03:14.194Z]  @firebase/database: FIREBASE WARNING: wss:// URL used, but browser isn't known to support websockets.  Trying anyway. \n17:34:18\nI\nfunction[us-central1-textToLength]\n[2021-11-05T12:04:18.762Z]  @firebase/database: FIREBASE WARNING: wss:// URL used, but browser isn't known to support websockets.  Trying anyway. \n17:35:06\nI\nfunction[us-central1-textToLength]\n[2021-11-05T12:05:06.473Z]  @firebase/database: FIREBASE WARNING: wss:// URL used, but browser isn't known to support websockets.  Trying anyway. \n17:35:54\nI\nfunction[us-central1-textToLength]\n[2021-11-05T12:05:54.409Z]  @firebase/database: FIREBASE WARNING: wss:// URL used, but browser isn't known to support websockets.  Trying anyway. \n`\n```\nfirebase function code :\n```\n`const functions = require('firebase-functions');\nvar admin = require(\"firebase-admin\");\nadmin.initializeApp(); \n\nvar database = admin.database();\n\nexports.helloWorld = functions.https.onRequest((request, response) => {\n    response.send(\"Hello from Firebase!\");\n});\n\nexports.textToLength = functions.https.onRequest((request, response) => {\n    var tex = request.query.text;\n    var textLength = tex.length;\n    console.log(textLength);\n    database.ref().child('test').set(\"op\");\n    response.send(String(textLength));\n});\n`\n```\ndependencies :\n```\n`{\n  \"name\": \"functions\",\n  \"description\": \"Cloud Functions for Firebase\",\n  \"scripts\": {\n    \"serve\": \"firebase emulators:start --only functions\",\n    \"shell\": \"firebase functions:shell\",\n    \"start\": \"npm run shell\",\n    \"deploy\": \"firebase deploy --only functions\",\n    \"logs\": \"firebase functions:log\"\n  },\n  \"engines\": {\n    \"node\": \"14\"\n  },\n  \"main\": \"index.js\",\n  \"dependencies\": {\n    \"firebase-admin\": \"^9.8.0\",\n    \"firebase-functions\": \"^3.14.1\",\n    \"@firebase/database-compat\": \"0.1.2\" \n  },\n  \"devDependencies\": {\n    \"firebase-functions-test\": \"^0.2.0\"\n  },\n  \"private\": true\n}\n`\n```\nnpm installed packages\n```\n`+-- @firebase/app-compat@0.1.7\n| +-- @firebase/app@0.7.6\n| | +-- @firebase/component@0.5.8\n| | | +-- @firebase/util@1.4.1 deduped\n| | | `-- tslib@2.3.1 deduped\n| | +-- @firebase/logger@0.3.1\n| | | `-- tslib@2.3.1 deduped\n| | +-- @firebase/util@1.4.1\n| | | `-- tslib@2.3.1 deduped\n| | `-- tslib@2.3.1 deduped\n| +-- @firebase/component@0.5.8\n| | +-- @firebase/util@1.4.1 deduped\n| | `-- tslib@2.3.1 deduped\n| +-- @firebase/logger@0.3.1\n| | `-- tslib@2.3.1 deduped\n| +-- @firebase/util@1.4.1\n| | `-- tslib@2.3.1 deduped\n| `-- tslib@2.3.1\n+-- @firebase/database-compat@0.1.2\n| +-- @firebase/component@0.5.7\n| | +-- @firebase/util@1.4.0 deduped\n| | `-- tslib@2.3.1 deduped\n| +-- @firebase/database@0.12.2\n| | +-- @firebase/auth-interop-types@0.1.6\n| | +-- @firebase/component@0.5.7 deduped\n| | +-- @firebase/logger@0.3.0 deduped\n| | +-- @firebase/util@1.4.0 deduped\n| | +-- faye-websocket@0.11.4\n| | | `-- websocket-driver@0.7.4\n| | |   +-- http-parser-js@0.5.3\n| | |   +-- safe-buffer@5.2.1 deduped\n| | |   `-- websocket-extensions@0.1.4\n| | `-- tslib@2.3.1 deduped\n| +-- @firebase/database-types@0.9.1\n| | +-- @firebase/app-types@0.7.0\n| | `-- @firebase/util@1.4.0 deduped\n| +-- @firebase/logger@0.3.0\n| | `-- tslib@2.3.1 deduped\n| +-- @firebase/util@1.4.0\n| | `-- tslib@2.3.1 deduped\n| `-- tslib@2.3.1 deduped\n`-- firebase-admin@10.0.0\n  +-- @firebase/database-compat@0.1.2 deduped\n  +-- @firebase/database-types@0.7.3\n  | `-- @firebase/app-types@0.6.3\n  +-- @google-cloud/firestore@4.15.1\n  | +-- fast-deep-equal@3.1.3\n  | +-- functional-red-black-tree@1.0.1\n  | +-- google-gax@2.28.0\n`\n```",
      "solution": "In the meantime, if you are on the latest Admin SDK version, you can pin @firebase/database-compat to version 0.1.2 in your package.json file as a temporary fix.\n`\"dependencies\": { \"@firebase/database-compat\": \"0.1.2\" }`\nThis works for me.\n\nRef: https://github.com/firebase/firebase-admin-node/issues/1487\nI referred this example and it worked but for this to work rebuild your package-lock.json file by removing node_modules folder and package_lock.json file and running `npm install --package-lock-only`",
      "question_score": 15,
      "answer_score": 5,
      "created_at": "2021-11-05T13:09:30",
      "url": "https://stackoverflow.com/questions/69853038/firebase-function-with-realtime-database-error"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 72693593,
      "title": "Firebase Cloud Functions Deploy Error- SyntaxError: Unexpected token &#39;?&#39;",
      "problem": "I just updated to:\n```\n`npm: 8.11.0\nnode: v16.15.1\n`\n```\nNew Edit:\nI just updated again `sudo n latest`:\n```\n`npm: 8.12.1\nnode: v18.4.0\n`\n```\nI'm trying to deploy a new cloud function `firebase deploy --only functions:deleteUser`\nbut I keep getting a cli error:\n\nFunction failed on loading user code. This is likely due to a bug in\nthe user code. Error message: Error: please examine your function logs\nto see the error cause\n\nWhen I look at the log:\n\n```\n`deleteUser\n\nDetailed stack trace: /workspace/node_modules/firebase-admin/lib/app/firebase-namespace.js:84\n\nthis.INTERNAL = new FirebaseNamespaceInternals(appStore ?? new lifecycle_1.AppStore());\n\nProvided module can't be loaded. \n\nIs there a syntax error in your code?\n\nSyntaxError: Unexpected token '?'\n\nat wrapSafe (internal/modules/cjs/loader.js:915:16)\nat Module._compile (internal/modules/cjs/loader.js:963:27)\nat Object.Module._extensions..js (internal/modules/cjs/loader.js:1027:10)\nat Module.load (internal/modules/cjs/loader.js:863:32)\nat Function.Module._load (internal/modules/cjs/loader.js:708:14)\nat Module.require (internal/modules/cjs/loader.js:887:19)\nat require (internal/modules/cjs/helpers.js:74:18)\nat Object. (/workspace/node_modules/firebase-admin/lib/default-namespace.js:19:30)\nat Module._compile (internal/modules/cjs/loader.js:999:30)\nat Object.Module._extensions..js (internal/modules/cjs/loader.js:1027:10)\n\nCould not load the function, shutting down.\n`\n```\n\nIndex.js:\n```\n`const functions = require('firebase-functions');\nconst admin = require('firebase-admin');\nadmin.initializeApp();\n\nexports.deleteUser = functions.https.onCall((data, context) => {\n\n    const userID = data.userID;\n\n    admin.auth().deleteUser(userID)\n    .then(() => {\n        console.log('Successfully deleted userID: ', userID);\n        return true // without this Return I get a different error: Each then() should return a value or throw  promise/always-return\n    })\n    .catch((error) => {\n        console.log('Error deleting user: ', error);\n    });\n});\n`\n```\nNew Edit Again\nPackage.json:\n```\n`{\n  \"name\": \"functions\",\n  \"description\": \"Cloud Functions for Firebase\",\n  \"scripts\": {\n    \"lint\": \"eslint .\",\n    \"serve\": \"firebase emulators:start --only functions\",\n    \"shell\": \"firebase functions:shell\",\n    \"start\": \"npm run shell\",\n    \"deploy\": \"firebase deploy --only functions\",\n    \"logs\": \"firebase functions:log\"\n  },\n  \"engines\": {\n    \"node\": \"12\"\n  },\n  \"main\": \"index.js\",\n  \"dependencies\": {\n    \"@google-cloud/logging\": \"^8.1.1\",\n    \"firebase-admin\": \"^11.0.0\",\n    \"firebase-functions\": \"^3.11.0\",\n    \"save\": \"^2.4.0\"\n  },\n  \"devDependencies\": {\n    \"eslint\": \"^5.12.0\",\n    \"eslint-plugin-promise\": \"^4.0.1\",\n    \"firebase-functions-test\": \"^0.2.0\"\n  },\n  \"private\": true\n}\n`\n```",
      "solution": "Thanks to the tip by @raina77ow in the comments. I had to go inside my `package.json` file and simply change the node version from 12 to 16\nold:\n```\n`\"engines\": {\n    \"node\": \"12\" // causes error\n  }\n`\n```\nnew:\n```\n`\"engines\": {\n    \"node\": \"16\" // error is now gone\n  }\n`\n```\nUpdate: I posted this a while back, Firebase is now on `\"node\": \"20\"` look here",
      "question_score": 14,
      "answer_score": 26,
      "created_at": "2022-06-21T00:28:10",
      "url": "https://stackoverflow.com/questions/72693593/firebase-cloud-functions-deploy-error-syntaxerror-unexpected-token"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 68140196,
      "title": "How to fix Lint problems with --fix in Firebase Functions",
      "problem": "I'ved got a firebase Function code that has quite a lot of Lint problems. How can I fix these with --fix. It was shown at the end of the error message.\n\nfirebase deploy\n\n```\n`/Users/xxx/firebase/functions/index.js\n  16:1   error  Unexpected tab character                             no-tabs\n  16:1   error  Expected indentation of 6 spaces but found 1 tab     indent\n  .....\n  69:1   error  Unexpected tab character                             no-tabs\n  69:1   error  Expected indentation of 6 spaces but found 1 tab     indent\n  69:5   error  Trailing spaces not allowed                          no-trailing-spaces\n  69:5   error  Block must not be padded by blank lines              padded-blocks\n  71:1   error  Trailing spaces not allowed                          no-trailing-spaces\n\n\u2716 157 problems (157 errors, 0 warnings)\n  45 errors and 0 warnings potentially fixable with the `--fix` option.\n`\n```",
      "solution": "The error message probably is coming from eslint. You can run eslint directly:\n`$ (cd functions && npx eslint . --fix)\n# or\n$ (cd functions && node_modules/eslint/bin/eslint.js . --fix)\n`",
      "question_score": 14,
      "answer_score": 28,
      "created_at": "2021-06-26T09:32:09",
      "url": "https://stackoverflow.com/questions/68140196/how-to-fix-lint-problems-with-fix-in-firebase-functions"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 65981036,
      "title": "Firebase emulator for Cloud Functions not updating the code",
      "problem": "I'm deploying the following cloud function using `firebase deploy --only functions` :\n```\n`export const testFunction = functions.https.onCall((data, context) => {\n  return {\"data\": \"hello\"};\n});\n`\n```\nand when call it from the client app using the code\n```\n`var testFunction = firebase.functions().httpsCallable(\"testFunction\");\n      testFunction().then((result) => {\n        // Read result of the Cloud Function.\n        this.data = result.data;\n});\n`\n```\nit works as expected.\nNow, I want to continue developing the function testing it on the local emulator so, following the documentation, I added this line to the web app code (before the function)\n```\n`firebase.functions().useEmulator(\"localhost\", 5001); // for local simulator\n`\n```\nand I run the local emulator with:\n```\n`firebase emulators:start --only functions\n`\n```\nIf I run the client app now, I correctly see the call going through the local emulator instead of the remote cloud function.\nProblem: If modify the code the local function doesn't get updated. I need to run firebase deploy again in order to see the change in the response. How can I just deploy locally?",
      "solution": "This stackoverflow solves the issue.\nSolution in short:\n\nAfter every change in your code, run `npm run build` to recomplie the code.\n\nTo auto-compile for every change in your code.\nChange `\"build\": \"tsc\"` to `\"build\": \"tsc -w\"` in your `package.json`\nfile, and run the emulator in an another terminal window.",
      "question_score": 14,
      "answer_score": 21,
      "created_at": "2021-01-31T16:45:14",
      "url": "https://stackoverflow.com/questions/65981036/firebase-emulator-for-cloud-functions-not-updating-the-code"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 66477703,
      "title": "Error: 16 UNAUTHENTICATED: Failed to retrieve auth metadata with error: Could not refresh access token",
      "problem": "The complete error is as follows:\n```\n`Unhandled error Error: 16 UNAUTHENTICATED: Failed to retrieve auth metadata with error: Could not refresh access token: Unsuccessful response status code. Request failed with status code 500\nat Object.callErrorFromStatus (/workspace/node_modules/@grpc/grpc-js/build/src/call.js:31:26)\nat Object.onReceiveStatus (/workspace/node_modules/@grpc/grpc-js/build/src/client.js:327:49)\nat Object.onReceiveStatus (/workspace/node_modules/@grpc/grpc-js/build/src/client-interceptors.js:299:181)\nat /workspace/node_modules/@grpc/grpc-js/build/src/call-stream.js:145:78\nat processTicksAndRejections (internal/process/task_queues.js:79:11)\nCaused by: Error\nat Query._get (/workspace/node_modules/@google-cloud/firestore/build/src/reference.js:1449:23)\nat Query.get (/workspace/node_modules/@google-cloud/firestore/build/src/reference.js:1438:21)\nat Object.getTeacherDataWithFilters (/workspace/lib/teachers/methods.js:168:81)\nat /workspace/lib/teachers/callable.js:22:36\nat func (/workspace/node_modules/firebase-functions/lib/providers/https.js:273:32)\nat processTicksAndRejections (internal/process/task_queues.js:97:5) {\ncode: 16,\ndetails: 'Failed to retrieve auth metadata with error: Could not refresh access token: Unsuccessful response status code. Request failed with status code 500',\nmetadata: Metadata { internalRepr: Map {}, options: {} }\n`\n```\nI recently deployed to the firebase functions and now I am getting this error. I dont know what has happened and I could not find anything specific to this",
      "solution": "I have figured out the actual reason and that's why I am answering my own question. The reason why I was getting this error is that the default service account attached to this project which is mostly like `@appspot.gserviceaccount.com` was disabled. After enabling, everything worked fine.",
      "question_score": 13,
      "answer_score": 7,
      "created_at": "2021-03-04T16:03:26",
      "url": "https://stackoverflow.com/questions/66477703/error-16-unauthenticated-failed-to-retrieve-auth-metadata-with-error-could-no"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 76306434,
      "title": "Unpredictable URLs with Firebase Cloud Functions 2nd gen",
      "problem": "I am used to Firebase Functions and now trying to set up a project with Firebase Cloud Functions 2nd Gen.\nWhen I deploy my functions they do not get the predictable URLs that I know from 1st gen (like: `https://europe-west1-[project-slug].cloudfunctions.net/[function-name]`), but rather some weird URLs like this one: `https://[function-name]-kbnn7zbc7q-uc.a.run.app`).\nI can't find any mentions of this in the documentation and I expect there may be a setting to get a more predictable scheme for functions URLs?",
      "solution": "As of now at least, there is also a deterministic URL for functions.\nThe tutorial adds this:\n\nBoth 1st gen and 2nd gen functions have assigned URLs that have the following format. These URLs have a deterministic format, meaning that you can predict what the URL will be before you deploy the function:\n\n```\n`https://REGION-PROJECT_ID.cloudfunctions.net/FUNCTION_NAME\n`\n```\nFirebase (unhelpfully) shows the non-deterministic URL. You can see the deterministic URL in the GCP console for functions if you are like me and don't completely trust the URL pattern above.",
      "question_score": 11,
      "answer_score": 21,
      "created_at": "2023-05-22T14:56:29",
      "url": "https://stackoverflow.com/questions/76306434/unpredictable-urls-with-firebase-cloud-functions-2nd-gen"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 71937121,
      "title": "Error: HTTP Error: 400, Invalid JSON payload received. Unknown name &quot;skipLog&quot;: Cannot find field",
      "problem": "I'm trying to setup firebase functions for the first time. I followed the steps in the docs, but when I run `firebase init functions` I run into this error:\n\nError: HTTP Error: 400, Invalid JSON payload received. Unknown name\n\"skipLog\": Cannot find field.\n\nHere is the extract from the `firebase-debug.log` file:\n`[debug] [2022-04-20T08:53:35.659Z] /services/cloudfunctions.googleapis.com:enable {\"error\":{\"code\":400,\"message\":\"Invalid JSON payload received. Unknown name \\\"skipLog\\\": Cannot find field.\",\"status\":\"INVALID_ARGUMENT\",\"details\":[{\"@type\":\"type.googleapis.com/google.rpc.BadRequest\",\"fieldViolations\":[{\"description\":\"Invalid JSON payload received. Unknown name \\\"skipLog\\\": Cannot find field.\"}]}]}}\n[error] \n[error] Error: HTTP Error: 400, Invalid JSON payload received. Unknown name \"skipLog\": Cannot find field.\n[debug] [2022-04-20T08:53:35.670Z] Error Context: {\n  \"body\": {\n    \"error\": {\n      \"code\": 400,\n      \"message\": \"Invalid JSON payload received. Unknown name \\\"skipLog\\\": Cannot find field.\",\n      \"status\": \"INVALID_ARGUMENT\",\n      \"details\": [\n        {\n          \"@type\": \"type.googleapis.com/google.rpc.BadRequest\",\n          \"fieldViolations\": [\n            {\n              \"description\": \"Invalid JSON payload received. Unknown name \\\"skipLog\\\": Cannot find field.\"\n            }\n          ]\n        }\n      ]\n    }\n  },\n  \"response\": {\n    \"statusCode\": 400\n  }\n}\n`",
      "solution": "I had the same problem, managed to init functions with older version (10.6.0) of firebase-tools",
      "question_score": 11,
      "answer_score": 12,
      "created_at": "2022-04-20T11:18:30",
      "url": "https://stackoverflow.com/questions/71937121/error-http-error-400-invalid-json-payload-received-unknown-name-skiplog-c"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 68785472,
      "title": "Passing parameters to httpsCallable cloud function from Flutter code",
      "problem": "Using this code snippet to call `httpsCallable` cloud function on firebase:\n`@override\n  Future initialize(\n    ClientEntity client,\n    PositionEntity location,\n    PositionEntity destination, {\n    required bool isVehicleEmpty,\n  }) async {\n    final data = InitializePickupRequestCommand.from(\n      client,\n      location,\n      destination,\n      isVehicleEmpty: isVehicleEmpty,\n    ).toJson();\n\n    final name = describeEnum(CloudFunctionNames.initializePickupRequest);\n\n    final initializePickupRequest = backend.httpsCallable(name);\n\n    final result = await initializePickupRequest.call(data);\n\n    return InitializePickupRequestCommandResult.from(\n      result.data as Map,\n    );\n  }\n`\n`data` object holds all required data for the CF to perform the operation, it is of type `Map`.\n` Map toJson() => {\n        \"clientId\": clientId,\n        \"clientLat\": clientLat,\n        \"clientLng\": clientLng,\n        \"vehicleType\": vehicleType,\n        \"isVehicleEmpty\": isVehicleEmpty,\n        \"location\": {\n          \"lat\": clientLat,\n          \"lng\": clientLng,\n        },\n        \"destination\": {\n          \"placeId\": destination.id,\n          \"zip\": destination.zip,\n          \"city\": destination.city,\n          \"searchString\": destination.searchString,\n          \"lat\": destination.lat,\n          \"lng\": destination.lng,\n        },\n      };\n`\nProblem\nevery time when trying to call the CF, it throws this exception:\n\n_AssertionError ('package:cloud_functions/src/https_callable.dart': Failed assertion: line 33 pos 12: '_debugIsValidParameterType(parameters)': is not true.)\n\nWhat i tried\nusing these as params:\n\n`data as Map`\n`{...data}`\n`{...data}`\n\nTried `{\"dummy\": \"data\"}` as a param and the CF was executed normally. don't know why!\n\nSo how parameters should be passed to https callable cloud function?",
      "solution": "The problem was in `toJson()` specifically in `\"vehicleType\": vehicleType,` because the value is an enum property and that what was throwing the invalid parameters exception.\nNow using`\"vehicleType\": vehicleType.index,`",
      "question_score": 11,
      "answer_score": 12,
      "created_at": "2021-08-14T19:13:03",
      "url": "https://stackoverflow.com/questions/68785472/passing-parameters-to-httpscallable-cloud-function-from-flutter-code"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 68129683,
      "title": "firebase - App Check fails when accessing database from callable cloud function",
      "problem": "I recently enabled App Check for my firebase app and enforced it on both my cloud functions and database. The cloud function workflow is behaving correctly. However, I can no longer access the database from the function. A minimal version of my callable function looks something like this:\n`exports.myFunc = functions.https.onCall(async (data, context) => {\n  const adminApp = admin.initializeApp();\n  const ref = adminApp.database().ref('some-node');\n  \n  if (context.app === undefined) {\n    throw new functions.https.HttpsError(\n      'failed-precondition',\n      'The function must be called from an App Check verified app.',\n    );\n  }\n\n  try {\n    return (await ref.orderByKey().equalTo('foo').get()).exists()\n  } catch(exc) {\n    console.error(exc);\n  }\n});\n`\nThis used to work before App Check, but now it fails and I see this in the logs:\n\n@firebase/database: FIREBASE WARNING: Invalid appcheck token (https://my-project-default-rtdb.firebaseio.com/)\nError: Error: Client is offline.\n\nSeems like I need to do something extra to get the `admin` app to pass App Check verification down to the database, but I haven't been able to find any documentation on this yet. I also tried using the app instance from `functions.app.admin` instead of initializing a new one, but this didn't help.\nI have the latest version of the packages:\n```\n`\"firebase-admin\": \"^9.10.0\"\n\"firebase-functions\": \"^3.14.1\"\n`\n```",
      "solution": "firebaser here\nThe behavior you're seeing is not how it's supposed to work, and we've been able to reproduce it. Thanks for the clear report, and sorry you encountered this.\nIf you (re)install the Firebase Admin SDK today, you won't be experiencing this same problem as we've fixed the problem in the `@firebase/database` dependency (in this PR).\nIf you're (still) experiencing the problem, you can check if you have the correct `@firebase/database` dependency by running:\n```\n`npm ls @firebase/database\n`\n```\nresults look something like this:\n```\n`temp-admin@1.0.0 /Users/you/repos/temp-admin\n\u2514\u2500\u252c firebase-admin@9.11.0\n  \u2514\u2500\u2500 @firebase/database@0.10.8\n`\n```\nIf your `@firebase/database` version is lower than `0.10.8`, you'll have to reinstall the Admin SDK, for example by deleting your `node_modules` directory and your `package-lock.json` file and running `npm install` again. This may also update other dependencies.",
      "question_score": 11,
      "answer_score": 9,
      "created_at": "2021-06-25T12:43:57",
      "url": "https://stackoverflow.com/questions/68129683/firebase-app-check-fails-when-accessing-database-from-callable-cloud-function"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 77337076,
      "title": "Firestore update method throw error: &quot;Error: 13 INTERNAL: Received RST_STREAM with code 1&quot;",
      "problem": "While using Cloud Functions, we've encountered the following error:\nTimestamp: 2023-10-21 18:50:18.281 EEST\nFunction: v8-function\n`---updateUserByID finish update---\n\nCaused by: Error \n    at WriteBatch.commit (/workspace/node_modules/firebase-admin/node_modules/@google-cloud/firestore/build/src/write-batch.js:433:23) \n    at DocumentReference.update (/workspace/node_modules/firebase-admin/node_modules/@google-cloud/firestore/build/src/reference.js:433:14) \n    at Object.updateUserByID (/workspace/dist/src/DB/db.js:74:14) \n    at createSpecialistOrderListService (/workspace/dist/src/crud/specialist/services/specialistList/createSpecialistOrderListService.js:38:29) \n    at runMicrotasks () \n    at processTicksAndRejections (node:internal/process/task_queues:96:5) \n    at async getRecommendedSpecialistsListController (/workspace/dist/src/crud/specialist/controllers/getRecommendedSpecialistsListController.js:25:44) \n    at async /workspace/dist/src/framework/express/middlewares/express-handler.js:18:36 \n\nError Details:\n- Code: `13`\n- Description: `Received RST_STREAM with code 1`\n- Metadata: `{ internalRepr: Map(0) {}, options: {} }`\n- Note: `Exception occurred in retry method that was not classified as transient`\n`\nThis error seems to pop up when we execute the following command in our update function:\n```\n`const writeResult = await admin\n      .firestore()\n      .collection(FirestoreCollections.Users)\n      .doc(userID)\n      .update(fieldsToUpdate);\n`\n```\nExample of `fieldsToUpdate`:\n`[\n  {\n    \"boolean\": true,\n    \"number\": 100,\n    \"id\": \"some_id\"\n  }\n]\n`\nHowever, what's puzzling is that this method seems to work flawlessly in our other cloud functions. In certain situations, even if an error is thrown during the update, the data in Firestore might still get updated.\n\nThe issue persists even when tested locally.\nUpon creating a new cloud function with the same method, everything operates smoothly.",
      "solution": "I had the same problem last week and it seems to be something inside firebase / grpc implementation related to long time delays between firebase calls.\nAlso, firebase library seems to be moving away from RPC but it still keeps it as a default option if you don't set `preferRest: true` (see docs)\nFor me it works when I call init right before performing an operation or changing firebase config to prefer using rest comunication.\nHere is my code snipet:\n```\n`function getFbDb() {\n  const mainFirebaseApp = firebaseAdmin.initializeApp({\n      credential: firebaseAdmin.credential.applicationDefault()\n    }, uuid.v4());\n\n  const db = mainFirebaseApp.firestore();\n  const settings = {\n    preferRest: true,\n    timestampsInSnapshots: true\n  };\n  db.settings(settings);\n  return { db, firebaseApp: mainFirebaseApp };\n}\nconst { db, firebaseApp } = getFbDb();\n`\n```\nUse `const { db, firebaseApp } = getFbDb();` everytime you have to execute an operation.\nAnother \"dirty option\" seems to be using a retry approach after the first fail.\nIf this doesn't work for you keep an eye on issue 2345 and see what comes up from there.",
      "question_score": 11,
      "answer_score": 5,
      "created_at": "2023-10-21T19:23:46",
      "url": "https://stackoverflow.com/questions/77337076/firestore-update-method-throw-error-error-13-internal-received-rst-stream-wi"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 73171905,
      "title": "Chromium executable doesn&#39;t exist for running Playwright within a deployed Google Cloud Function",
      "problem": "TL;DR - Does anyone know of a way to run `npx playwright install chromium` after installing Node.js dependencies, and in the Google Cloud Function's production environment?\nIs it possible to run Playwright within a deployed Google Cloud Function? I have a deployed function that works fine locally, but when deployed, consistently fails with this error:\n```\n`browserType.launch: Executable doesn't exist at /root/.cache/ms-playwright/chromium-1015/chrome-linux/chrome\n`\n```\nFun fact, before using Playwright, I successfully used Puppeteer to successfully deploy the Google Cloud Function. Puppeteer also uses Chromium and had no problems locating and/or installing the Chromium executable (and I am also using only Chromium with Playwright).\nI'm pretty sure that running `npx playwright install chromium` after `npm install` would fix the problem, but I'm not sure how to run that command in the Google Cloud Function's environment. I tried doing the following:\n```\n`// package.json\n\n...\n\"scripts\": {\n  ...\n  \"postinstall\": \"npx playwright install chromium\",\n  \"postci\": \"npx playwright install chromium\"\n}\n...\n`\n```\nBut that didn't work since I'm getting the same error (and I'm not sure if either post script was executed). Running `npx playwright install chromium` works locally though.\nHas anyone successfully ran Playwright in a deployed Google Cloud Function?\nUpdate\nDoing this approach works when running remotely, but not locally (via an emulated Google Cloud Function). Another problem with that approach is that we're fixed on specific versions of `playwright-core` and `chrome-aws-lambda`.\nDoes anyone have a better solution?",
      "solution": "This isn't ideal since this is happening at function execution time, but I got this working by calling spawnSync in the function's body.\n```\n`import { spawnSync } from \"child_process\";\n\n...\n// in the function's body\nspawnSync(\"npx\", [\"playwright\", \"install\", \"chromium\"]);\n`\n```\nThis works locally (via emulators) and in production.",
      "question_score": 11,
      "answer_score": 2,
      "created_at": "2022-07-30T01:44:36",
      "url": "https://stackoverflow.com/questions/73171905/chromium-executable-doesnt-exist-for-running-playwright-within-a-deployed-googl"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 65606902,
      "title": "Did Firebase Cloud Functions ESLint change recently?",
      "problem": "I created a cloud function project with firebase a few months ago, and used linting.\nI recently created a new cloud function project with linting, and now the linter is complaining about random rules I never set. I don't remember it enforcing nearly the amount of style rules a few months ago.\nThings like:\n```\n`This line has a length of 95. Maximum allowed is 80\nMissing JSDoc comment\nMissing Trailing comma\nexpected indentation of 2 spaces but found 4\nStrings must use singlequote\n`\n```\nIt's also not letting me use async/await.\nI found out I can individually set these rules in my .eslintrc.js file, but that's annoying and I don't want to do that. By default, why aren't these rules disabled? I just want basic rules that make sure my code won't fail when run, not random style preferences like single/double quotes and max line length.\nIs there any way to use just basic linting functionality with firebase functions?",
      "solution": "I ran into the same issue as you. The new, more strict linting rules seem to come from the fact that Firebase functions use the \"google\" eslint base configuration plugin by default now. Read more about configuring ESLint plugins in the docs. My older Firebase functions were using tslint without issue.\nHere's what my .eslintrc.js file looked like while I was getting style errors from eslint:\n```\n`module.exports = {\n    env: {\n        es6: true,\n        node: true,\n    },\n    extends: [\n        'eslint:recommended',\n        'plugin:import/errors',\n        'plugin:import/warnings',\n        'plugin:import/typescript',\n        'google',\n    ],\n    parser: '@typescript-eslint/parser',\n    parserOptions: {\n        project: ['tsconfig.json', 'tsconfig.dev.json'],\n        sourceType: 'module',\n    },\n    ignorePatterns: [\n        '/lib/**/*', // Ignore built files.\n    ],\n    plugins: ['@typescript-eslint', 'import'],\n    rules: {\n        quotes: ['error', 'double'],\n    },\n};\n`\n```\nI deleted 'google' from the extends property, which seemed to resolve almost all of the style linting issues.\nNow it looks like this:\n```\n`module.exports = {\n    env: {\n        es6: true,\n        node: true,\n    },\n    extends: [\n        'eslint:recommended',\n        'plugin:import/errors',\n        'plugin:import/warnings',\n        'plugin:import/typescript',\n    ],\n    parser: '@typescript-eslint/parser',\n    parserOptions: {\n        project: ['tsconfig.json', 'tsconfig.dev.json'],\n        sourceType: 'module',\n    },\n    ignorePatterns: [\n        '/lib/**/*', // Ignore built files.\n    ],\n    plugins: ['@typescript-eslint', 'import'],\n    rules: {\n        quotes: ['error', 'double'],\n    },\n};\n`\n```",
      "question_score": 10,
      "answer_score": 23,
      "created_at": "2021-01-07T06:04:55",
      "url": "https://stackoverflow.com/questions/65606902/did-firebase-cloud-functions-eslint-change-recently"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 75597907,
      "title": "Build failed: npm ERR! Missing a lot of unknown packages",
      "problem": "I initialized a new firebase project in my project. I added functions to my project and I am trying to deploy the default helloWorld function.\n```\n`const functions = require(\"firebase-functions\");\n\n// // Create and deploy your first functions\n// // https://firebase.google.com/docs/functions/get-started\n//\nexports.helloWorld = functions.https.onRequest((request, response) => {\n  functions.logger.info(\"Hello logs!\", {structuredData: true});\n  response.send(\"Hello from Firebase!\");\n});\n`\n```\nWhen I do this I get the following error saying it is missing a lot of packages I have never heard of.\n```\n`i  functions: updating Node.js 16 function helloWorld(us-central1)...\nBuild failed: ...c-fn@2.1.0 from lock file\nnpm ERR! Missing: error-ex@1.3.2 from lock file\nnpm ERR! Missing: json-parse-even-better-errors@2.3.1 from lock file\nnpm ERR! Missing: lines-and-columns@1.2.4 from lock file\nnpm ERR! Missing: is-arrayish@0.2.1 from lock file\nnpm ERR! Missing: find-up@4.1.0 from lock file\nnpm ERR! Missing: ansi-styles@5.2.0 from lock file\nnpm ERR! Missing: react-is@18.2.0 from lock file\nnpm ERR! Missing: kleur@3.0.3 from lock file\nnpm ERR! Missing: sisteransi@1.0.5 from lock file\nnpm ERR! Missing: is-core-module@2.11.0 from lock file\nnpm ERR! Missing: path-parse@1.0.7 from lock file\nnpm ERR! Missing: supports-preserve-symlinks-flag@1.0.0 from lock file\nnpm ERR! Missing: resolve-from@5.0.0 from lock file\nnpm ERR! Missing: buffer-from@1.1.2 from lock file\nnpm ERR! Missing: escape-string-regexp@2.0.0 from lock file\nnpm ERR! Missing: char-regex@1.0.2 from lock file\nnpm ERR! Missing: glob@7.2.3 from lock file\nnpm ERR! Missing: is-number@7.0.0 from lock file\nnpm ERR! Missing: picocolors@1.0.0 from lock file\nnpm ERR! Missing: convert-source-map@1.9.0 from lock file\nnpm ERR! Missing: makeerror@1.0.12 from lock file\nnpm ERR! Missing: tmpl@1.0.5 from lock file\nnpm ERR! Missing: yallist@3.1.1 from lock file\nnpm ERR! Missing: ansi-styles@3.2.1 from lock file\nnpm ERR! Missing: escape-string-regexp@1.0.5 from lock file\nnpm ERR! Missing: supports-color@5.5.0 from lock file\nnpm ERR! Missing: color-convert@1.9.3 from lock file\nnpm ERR! Missing: color-name@1.1.3 from lock file\nnpm ERR! Missing: has-flag@3.0.0 from lock file\nnpm ERR! Missing: locate-path@5.0.0 from lock file\nnpm ERR! Missing: argparse@1.0.10 from lock file\nnpm ERR! Missing: sprintf-js@1.0.3 from lock file\nnpm ERR! Missing: p-locate@4.1.0 from lock file\nnpm ERR! Missing: p-limit@2.3.0 from lock file\nnpm ERR! Missing: p-try@2.2.0 from lock file\nnpm ERR! Missing: cliui@8.0.1 from lock file\nnpm ERR! Missing: yargs-parser@21.1.1 from lock file\nnpm ERR! Missing: locate-path@5.0.0 from lock file\nnpm ERR! Missing: p-locate@4.1.0 from lock file\nnpm ERR! Missing: p-limit@2.3.0 from lock file\nnpm ERR!\nnpm ERR! Clean install a project\nnpm ERR!\nnpm ERR! Usage:\nnpm ERR! npm ci\nnpm ERR!\nnpm ERR! Options:\nnpm ERR! [-S|--save|--no-save|--save-prod|--save-dev|--save-optional|--save-peer|--save-bundle]\nnpm ERR! [-E|--save-exact] [-g|--global] [--global-style] [--legacy-bundling]\nnpm ERR! [--omit  [--omit  ...]]\nnpm ERR! [--strict-peer-deps] [--no-package-lock] [--foreground-scripts]\nnpm ERR! [--ignore-scripts] [--no-audit] [--no-bin-links] [--no-fund] [--dry-run]\nnpm ERR! [-w|--workspace  [-w|--workspace  ...]]\nnpm ERR! [-ws|--workspaces] [--include-workspace-root] [--install-links]\nnpm ERR!\nnpm ERR! aliases: clean-install, ic, install-clean, isntall-clean\nnpm ERR!\nnpm ERR! Run \"npm help ci\" for more info\n\nnpm ERR! A complete log of this run can be found in:\nnpm ERR!     /www-data-home/.npm/_logs/2023-02-28T21_49_46_615Z-debug-0.log; Error ID: beaf8772\n\nFunctions deploy had errors with the following functions:\n    helloWorld(us-central1)\n`\n```\nI installed the firebase-functions package, also let firebase install it's packages in the setup when initializing.\nI tried removing node_modules, remove cache, remove lock, reinstalling.\nI tried removing firebase from my project and reinitializing it.\nI tried changing my Node version.\nI am really clueless.\nHope someone can help me!\nThanks!",
      "solution": "FIXED! I fixed this by removing the lockfile after installing the packages and then deploy to firebase without lockfile.",
      "question_score": 10,
      "answer_score": 24,
      "created_at": "2023-02-28T23:24:31",
      "url": "https://stackoverflow.com/questions/75597907/build-failed-npm-err-missing-a-lot-of-unknown-packages"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 70027316,
      "title": "Firebase function failing to deploy",
      "problem": "I'm trying to create a Firebase Function but I'm running into a deploy error, even when deploying the default `helloworld` function.\nThe firebase-debug.log file mentions this:\n`Could not find image for function projects/picci-e030e/locations/us-central1/functions/helloWorld.`\nI have been trying to debug and so far have not been able to solve it...\nfirebase-debug.log\n```\n`[info] Functions deploy had errors with the following functions:\n    helloWorld(us-central1)\n[debug] [2021-11-18T21:54:08.946Z] Missing URI for HTTPS function in printTriggerUrls. This shouldn't happen\n[info] i  functions: cleaning up build files... \n[debug] [2021-11-18T21:54:08.948Z] >>> [apiv2][query] GET https://us.gcr.io/v2/picci-e030e/gcf/us-central1/tags/list [none]\n[debug] [2021-11-18T21:54:09.407Z] index.js:\n```\n`const functions = require(\"firebase-functions\");\n\n// Create and Deploy Your First Cloud Functions\n// https://firebase.google.com/docs/functions/write-firebase-functions\n\nexports.helloWorld = functions.https.onRequest((request, response) => {\n  functions.logger.info(\"Hello logs!\", {structuredData: true});\n  response.send(\"Hello from Firebase!\");\n});\n\n// const getBlurhash = require(\"./get_blurhash\");\n// exports.getBlurhash = getBlurhash.generateHash;\n`\n```\nPackage.json:\n```\n`{\n  \"name\": \"functions\",\n  \"description\": \"Cloud Functions for Firebase\",\n  \"scripts\": {\n    \"lint\": \"eslint\",\n    \"serve\": \"firebase emulators:start --only functions\",\n    \"shell\": \"firebase functions:shell\",\n    \"start\": \"npm run shell\",\n    \"deploy\": \"firebase deploy --only functions\",\n    \"logs\": \"firebase functions:log\"\n  },\n  \"engines\": {\n    \"node\": \"14\"\n  },\n  \"main\": \"index.js\",\n  \"dependencies\": {\n    \"firebase-admin\": \"^9.8.0\",\n    \"firebase-functions\": \"^3.14.1\",\n    \"blurhash\": \"^1.1.4\"\n  },\n  \"devDependencies\": {\n    \"eslint\": \"^7.6.0\",\n    \"eslint-config-google\": \"^0.14.0\",\n    \"firebase-functions-test\": \"^0.2.0\"\n  },\n  \"private\": true\n}\n`\n```\nMy version of node:\n`v14.16.0`\nAppreciate your help.",
      "solution": "Could not find image for function projects/picci-e030e/locations/us-central1/functions/helloWorld.\n\nThe Firebase Function deployment failed because it cannot find the image built based on your function app. There might be a problem building in your app, it could be your dependencies or files.\nI replicated your issue, received the same error and solved it. There's a problem with the `package.json` file and `package-lock.json`. If you just add(without installing) your dependency in `package.json` you should delete or remove your `package-lock.json` that will be found in function directory before you deploy it again using the deployment command:\n```\n`firebase deploy --only functions\n`\n```\nor you can just install your dependency to make sure it will be added to your `package.json` and `package-lock.json` file, deploy again. For example:\n```\n`npm install --save blurhash\n`\n```",
      "question_score": 10,
      "answer_score": 14,
      "created_at": "2021-11-18T23:03:02",
      "url": "https://stackoverflow.com/questions/70027316/firebase-function-failing-to-deploy"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 66592146,
      "title": "Add middleware to all firebase functions in one line / function",
      "problem": "In `express` you can add middleware such as `app.use(cors())` which adds it to all of the endpoints, however I can't find something similar in firebase examples. Here is the example (see below) of how to apply it in every function. However I want to apply the middleware (cors or other) globally, as I have many functions.\n```\n`import * as cors from 'cors';\nconst corsHandler = cors({origin: true});\n\nexport const exampleFunction= functions.https.onRequest(async (request, response) => {\n       corsHandler(request, response, () => { return handler(req, res) });\n});\n`\n```\nWhat is the equivalent of `app.use()` in firebase? Is adding and express server the only option?",
      "solution": "Use currying to create a handler, you have to repeat it across all the functions, but it's easier than writing the middleware each time:\n```\n`const applyMiddleware = handler => (req, res) => {\n  return cors(req, res, () => {\n    return handler(req, res)\n  })\n}\nexports.handler = functions.https.onRequest(applyMiddleware(yourHandler))\n`\n```\nEdit, an example of a more complex middleware:\n`const applyMiddleware =\n  (handler, { authenticatedRoute = false } = {}) =>\n  (req, res) => {\n    if (authenticatedRoute) {\n      const isAuthorized = isAuthenticated(req)\n      if (!isAuthorized) {\n        return res.status(401).send('Unauthorized')\n      }\n    }\n    return cors(req, res, () => {\n      return handler(req, res)\n    })\n  }\nexports.handler = functions.https.onRequest(\n   applyMiddleware(yourHandler, { authenticatedRoute: true })\n)\n`",
      "question_score": 10,
      "answer_score": 15,
      "created_at": "2021-03-12T00:41:11",
      "url": "https://stackoverflow.com/questions/66592146/add-middleware-to-all-firebase-functions-in-one-line-function"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 68865573,
      "title": "Firebase Cloud Function finished with status: &#39;response error&#39;",
      "problem": "I have a cloud function that is returning a lot of data (50'000 documents) as objects. When I run it I get the error `finished with status: 'response error'`.\nThis only happens when I export all of the data, when a limit (up to 20'000) is applied it works without problem. This lets me think that the response might be too big, but there is no info in the logs about this at all. Also adding try / catch does not work. In the console I only get the above message without any further indication.\nI know that functions normally log when timeout is hit or the memory exceeded, so I am wondering what else could be the source of error.\n```\n`exports.run = functions.runWith({ timeoutSeconds: 540, memory: '8GB' }).https.onRequest(async (req, res) => {\n  try {\n    const querySnap = await db.collection(\"myData\").get();\n    const data = querySnap.docs.map(doc => doc.data());\n    return res.status(200).json({\n      data: data\n    }).end();\n\n  } catch (err) {\n    console.log(err);\n    return res.status(400).end();\n  }\n});\n`\n```\nEDIT: It is indeed the size of the response that causes this error. You can reproduce this if you simply return data of given size (with `Buffer.alloc(bytes)`).",
      "solution": "I thins you hit the max HTTP response size which is 10 MB for HTTP functions\nReference : https://cloud.google.com/functions/quotas#resource_limits with the screenshot below take from that ref.",
      "question_score": 10,
      "answer_score": 14,
      "created_at": "2021-08-20T18:48:01",
      "url": "https://stackoverflow.com/questions/68865573/firebase-cloud-function-finished-with-status-response-error"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 65608860,
      "title": "Connect and query from a BigQuery database via a Google Cloud Function (Python)",
      "problem": "I have\n\nA google Cloud Function (main.py + requirements.txt)\nA bigQuery Database\nWorking query\n\nCan someone help me with a link/tutorial/code to connect to this bigquery database using my Google Cloud Function in Python and simply query some data from the database and display it.\nI tried the https://cloud.google.com/bigquery/docs/reference/libraries but it was related to connecting to big query from a normal deployment and not a Google Cloud Function.\nThis is what I have so far. It is deploying without an error, but upon testing, it is giving a 500 error\nmain.py (sample public query\n```\n`from google.cloud import bigquery\n\ndef query_stackoverflow(request):\n client = bigquery.Client()\n query_job = client.query(\n    \"\"\"\n    SELECT\n      CONCAT(\n        'https://stackoverflow.com/questions/',\n        CAST(id as STRING)) as url,\n      view_count\n    FROM `bigquery-public-data.stackoverflow.posts_questions`\n    WHERE tags like '%google-bigquery%'\n    ORDER BY view_count DESC\n    LIMIT 10\"\"\"\n)\n\nresults = query_job.result()  # Waits for job to complete.\nreturn Response(\"{'message':'successfully connected'}\", status=200, mimetype='application/json')\n`\n```\nrequirements.txt\n```\n`google-cloud-bigquery\n`\n```\nError log:\n```\n`\n500 Internal Server Error\nInternal Server Error\nThe server encountered an internal error and was unable to complete your \nrequest. Either the server is overloaded or there is an error in the \napplication.\n`\n```",
      "solution": "Create a service account and grant the necessary role.\n\n```\n`gcloud iam service-accounts create connect-to-bigquery\ngcloud projects add-iam-policy-binding your-project --member=\"serviceAccount:connect-to-bigquery@your-project.iam.gserviceaccount.com\" --role=\"roles/owner\"\n`\n```\n\nCreate a cloud function using using the service account you just created as identity\n\nEdit the main.py and requirements.txt\n\nDeploy and Test the function\n\nSUCCESS!",
      "question_score": 10,
      "answer_score": 9,
      "created_at": "2021-01-07T09:22:33",
      "url": "https://stackoverflow.com/questions/65608860/connect-and-query-from-a-bigquery-database-via-a-google-cloud-function-python"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 68886279,
      "title": "Firebase Function deploy throwing typescript errors",
      "problem": "I am using typescript with Firebase Functions. When I try to deploy using `npm run deploy` inside functions directory. It throws too many typescript errors.\n```\n`../../../../node_modules/@types/react/index.d.ts:3208:13 - error TS2717: Subsequent property declarations must have the same type.  Property 'feFuncB' must be of type 'SVGProps', but here has type 'SVGProps'.\n\n3208             feFuncB: React.SVGProps;\n                 ~~~~~~~\n\n  ../node_modules/@types/react/index.d.ts:3108:13\n    3108             feFuncB: React.SVGProps;\n                     ~~~~~~~\n    'feFuncB' was also declared here.\n\n../../../../node_modules/@types/react/index.d.ts:3209:13 - error TS2717: Subsequent property declarations must have the same type.  Property 'feFuncG' must be of type 'SVGProps', but here has type 'SVGProps'.\n\n3209             feFuncG: React.SVGProps;\n                 ~~~~~~~\n\n  ../node_modules/@types/react/index.d.ts:3109:13\n    3109             feFuncG: React.SVGProps;\n                     ~~~~~~~\n    'feFuncG' was also declared here.\n\n../../../../node_modules/@types/react/index.d.ts:3210:13 - error TS2717: Subsequent property declarations must have the same type.  Property 'feFuncR' must be of type 'SVGProps', but here has type 'SVGProps'.\n\n3210             feFuncR: React.SVGProps;\n                 ~~~~~~~\n\n \n../../../../node_modules/@types/react/index.d.ts:3222:13 - error TS2717: Subsequent property declarations must have the same type.  Property 'filter' must be of type 'SVGProps', but here has type 'SVGProps'.\n\n3222             filter: React.SVGProps;\n`\n```\ntsconfig.json\n```\n`  \"compilerOptions\": {\n    \"module\": \"commonjs\",\n    \"noImplicitReturns\": true,\n    \"noUnusedLocals\": true,\n    \"outDir\": \"lib\",\n    \"sourceMap\": true,\n    \"strict\": true,\n    \"target\": \"es2017\"\n  },\n  \"compileOnSave\": true,\n  \"include\": [\n    \"src\"\n  ]\n}\n`\n```\nSeems like typescript is going through the node modules folder.",
      "solution": "```\n`{ \n  {\n    \"compilerOptions\": {\n      \"module\": \"commonjs\",\n      \"noImplicitReturns\": true,\n      \"noUnusedLocals\": true,\n      \"outDir\": \"lib\", \n      \"sourceMap\": true,\n      \"strict\": true,\n      \"target\": \"es2017\",\n      \"typeRoots\": [\"node_modules/@types\"]  // <-- here\n    },  \n    \"compileOnSave\": true,\n    \"include\": [\"src\"]\n  }\n}\n`\n```",
      "question_score": 10,
      "answer_score": 8,
      "created_at": "2021-08-23T03:06:51",
      "url": "https://stackoverflow.com/questions/68886279/firebase-function-deploy-throwing-typescript-errors"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 69044315,
      "title": "Module not found: Can&#39;t resolve &#39;firebase&#39; in",
      "problem": "After: npm i firebase\n\nI'am importing firebase from firebase itself & not from a file\n\nimport firebase from 'firebase';   >in firebase.js file\nerror in terminal>>\n./src/firebase.js\nModule not found: Can't resolve 'firebase' in 'C:\\Users\\Home\\Documents\\dsn\\e\\Documents..........'",
      "solution": "`npm i firebase` now installs v9 Modular SDK so you cannot used the old imports. Try refactoring your code to this:\n```\n`import { initializeApp } from 'firebase/app';\n\nconst firebaseConfig = {\n  //...\n};\n\nconst app = initializeApp(firebaseConfig);\n`\n```\nIf you want to use older syntax then change your imports to compat libraries:\n```\n`import firebase from \"firebase/compat/app\"\nimport \"firebase/compat/auth\"\nimport \"firebase/compat/firestore\"\n// other services is needed\n`\n```\nYou can read more about it in the documentation",
      "question_score": 9,
      "answer_score": 33,
      "created_at": "2021-09-03T13:39:49",
      "url": "https://stackoverflow.com/questions/69044315/module-not-found-cant-resolve-firebase-in"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 75840698,
      "title": "I cannot run firebase emulators on fresh install. Getting &#39; SyntaxError: Unexpected string: re&quot;use strict&quot;; &#39;",
      "problem": "I followed the tutorial to set up Firebase Functions with Typescript at\nhttps://firebase.google.com/docs/functions/typescript\nHowever, when I run `npm run serve` or `firebase emulators:start`, emulators won't start, but I'm getting the following error:\n```\n`[2023-03-25T09:04:16.875Z] /Users/username/.cache/firebase/emulators/ui-v1.11.4/server/server.js:569\n        re\"use strict\";\n          ^^^^^^^^^^^^\n\n`\n```\nI have tried deleting the cache folder, with no effect. In the file server.js, the relevant lines are:\n```\n`function createDebug(namespace) {\n    function debug2() {\n      if (!debug2.enabled)\n        re\"use strict\";\n`\n```\nInstalled versions of the relevant libraries are:\n```\n`firebase --version\n11.25.1\n\nnode -v\nv16.19.1\n\nnpm -v\n8.19.3\n\njava --version\nopenjdk 19.0.2 2023-01-17\nOpenJDK Runtime Environment Homebrew (build 19.0.2)\nOpenJDK 64-Bit Server VM Homebrew (build 19.0.2, mixed mode, sharing)\n\ntsc --version\nVersion 5.0.2\n`\n```\nI'm out of ideas what to try, or what to reinstall. Any suggestions what I'm doing wrong?\nI tried reinstalling all libraries, set up a new firebase project, installed the correct node version, reinstalled jdk. Was expecting emulators to run.",
      "solution": "I had the same issue as you. I raised the following ticket on Firebase. Was facing this on MacOS (and Linux), but I assumed it's an issue with the Node version I was using (not being LTS).\nIssue:\nAfter some investigation the following elements are the cause of the issue.\n\nNode Version - using an incompatible version of Node. Change to Node LTS version.\nJava Version - using an incompatible version of Java. Change to use compatible version e.g. Java version 11\n\nResolution:\nTo resolve it I amended the host environment to use compatible versions as described above. The Node + Java components are the ones that are important.\nOn my device, the following setting allowed the Emulator UI to run successfully.\n```\n`node - v16.19.1\nnpm - 8.19.3\nfirebase - 11.25.1\njava openjdk - 11.0.18\n`\n```\nI then deleted the machine cache located:\n```\n`~/.cache/firebase\n`\n```\nThen restart the emulator\n```\n`firebase emulators:start\n`\n```\n\nThe firebase emulator components should download again\nThe emulator starts up without the error.",
      "question_score": 9,
      "answer_score": 12,
      "created_at": "2023-03-25T10:15:19",
      "url": "https://stackoverflow.com/questions/75840698/i-cannot-run-firebase-emulators-on-fresh-install-getting-syntaxerror-unexpec"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 76360867,
      "title": "Google Secrets in Firebase Function V2 Document Trigger",
      "problem": "I have a question about Firebase Functions V2, with regards to how to include Google Secrets Manager into the document trigger function.\nI know how to do it with OnRequest, the below works fine.\n```\n`exports.myWebhook = onRequest({secrets: [MY_WEBHOOK_SECRET, MY_TEST_SECRET_KEY]}, async (req, res) => {\n    const safe = STRIPE_TEST_SECRET_KEY.value();\n}\n`\n```\nHowever I can't seem to get it to work with onDocumentCreated...\n```\n`exports.myUpdate = onDocumentCreated(\"/orders/{documentId}\", async (event ) => {\n const myAPIKey = MY_OTHER_API.value()\n}\n`\n```\nAny time I place {secrets: [MY_OTHER_API]} anywhere in this function, it creates an error. If I exclude it then the API key value is just blank.\nAny help would be appreciated. Thanks.\nTried placing {secrets: [MY_OTHER_API_KEY]} into the function to allow it access to the Google Secret value.",
      "solution": "Per the API reference, the first parameter is either a document path or a `firestore.DocumentOptions` object (which extends `GlobalOptions` where `secrets` is defined):\n`exports.myUpdate = onDocumentCreated(\n  {\n    document: \"/orders/{documentId}\",\n    secrets: [MY_OTHER_API]\n  },\n  async (event) => {\n    const myAPIKey = MY_OTHER_API.value()\n    // other steps\n  }\n)\n`",
      "question_score": 9,
      "answer_score": 24,
      "created_at": "2023-05-30T01:50:22",
      "url": "https://stackoverflow.com/questions/76360867/google-secrets-in-firebase-function-v2-document-trigger"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 72097157,
      "title": "Turning on the &#39;esModuleInterop&#39; flag",
      "problem": "I'm using Firebase Functions for my App. I installed these firebase functions on my PC but I can't use the following command:\n\nFirebase deploy --only functions\n\nI get the following error:\n```\n`node_modules/google-gax/build/protos/iam_service.d.ts:17:23 - error TS2497: This module \ncan only be referenced with ECMAScript imports/exports by turning on the \n'esModuleInterop' flag and referencing its default export.\n\n17 import * as Long from 'long';\n                       ~~~~~~\n\nnode_modules/google-gax/build/protos/operations.d.ts:17:23 - error TS2497: This module \ncan only be referenced with ECMAScript imports/exports by turning on the \n'esModuleInterop' flag and referencing its default export.\n\n17 import * as Long from 'long';\n                         ~~~~~~\n\nFound 2 errors in 2 files.\n\nErrors  Files\n 1  node_modules/google-gax/build/protos/iam_service.d.ts:17\n 1  node_modules/google-gax/build/protos/operations.d.ts:17\n\nError: functions predeploy error: Command terminated with non-zero exit code2\n`\n```\nDoes anyone know how to turn on this flag?",
      "solution": "just add `\"skipLibCheck\": true` to your `tsconfig.json` as shown below:\n```\n`\"compilerOptions\": {\n    \"module\": \"commonjs\",\n    \"noImplicitReturns\": true,\n    \"noUnusedLocals\": true,\n    \"outDir\": \"lib\",\n    \"sourceMap\": true,\n    \"strict\": true,\n    \"target\": \"es2017\",\n    \"skipLibCheck\": true\n  },\n`\n```",
      "question_score": 9,
      "answer_score": 12,
      "created_at": "2022-05-03T11:12:01",
      "url": "https://stackoverflow.com/questions/72097157/turning-on-the-esmoduleinterop-flag"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 67630558,
      "title": "Getting error &quot;Path&quot; argument must be string while deploying React - Loadable components sample code in cloud functions",
      "problem": "I'm trying to integrate Loadable components in my SSR project and that is working when I executed in localhost:3000, then I tried to deploy it in cloud function I am getting this error\n\n[ERR_INVALID_ARG_TYPE]: The \"path\" argument must be of type string. Received undefined\n\nAfter that I tried to deploy loadable components server side rendering example, sample code also giving the same error.\nI did some changes in package.json, server main.js and app.js file to work in Cloud functions\nThis is my server main.js file\n```\n`import path from 'path'\nimport express from 'express'\nimport React from 'react'\nimport { renderToString } from 'react-dom/server'\nimport { ChunkExtractor } from '@loadable/server'\nimport App from '../client/App'\nconst functions = require('firebase-functions');\nconst app = express()\n\n//app.use(express.static(path.join(__dirname, '../../public')))\n\nconst nodeStats = path.resolve(\n  __dirname,\n  '../../public/dist/async-node/loadable-stats.json',\n)\n\nconst webStats = path.resolve(\n  __dirname,\n  '../../public/dist/web/loadable-stats.json',\n)\n\napp.get('*', (req, res) => {\n  const nodeExtractor = new ChunkExtractor({ statsFile: nodeStats })\n  const { default: App } = nodeExtractor.requireEntrypoint()\n\n  const webExtractor = new ChunkExtractor({ statsFile: webStats })\n  const jsx = webExtractor.collectChunks()\n\n  const html = renderToString(jsx)\n\n  res.set('content-type', 'text/html')\n  res.send(`\n      \n      \n        \n        ${webExtractor.getLinkTags()}\n        ${webExtractor.getStyleTags()}\n        \n        \n          ${html}\n          ${webExtractor.getScriptTags()}\n        \n      \n    `)\n})\n\n// eslint-disable-next-line no-console\nexports.supercharged = functions.https.onRequest(app);\n`\n```\nIn this file I did these changes from example code `app.listen` to `exports.supercharged = functions.https.onRequest(app);` and I imported the `const functions = require('firebase-functions');`\nThis is my package.json file\n```\n`{\n  \"private\": true,\n  \"scripts\": {\n    \"dev\": \"nodemon src/server/main.js\",\n    \"build\": \"NODE_ENV=production yarn build:webpack && yarn build:functions\",\n    \"build:webpack\": \"webpack\",\n    \"build:functions\": \"babel -d functions src\",\n    \"start\": \"NODE_ENV=production node functions/server/main.js\",\n    \"deploy\": \"firebase deploy --only functions,hosting\",\n    \"serve\": \"firebase serve --only functions,hosting\"\n  },\n  \"devDependencies\": {\n    \"@babel/cli\": \"^7.4.4\",\n    \"@babel/core\": \"^7.6.2\",\n    \"@babel/node\": \"^7.0.0\",\n    \"@babel/preset-env\": \"^7.6.2\",\n    \"@babel/preset-react\": \"^7.0.0\",\n    \"@loadable/babel-plugin\": \"^5.10.3\",\n    \"@loadable/component\": \"^5.10.3\",\n    \"@loadable/server\": \"^5.10.3\",\n    \"@loadable/webpack-plugin\": \"^5.7.1\",\n    \"babel-loader\": \"^8.0.6\",\n    \"css-loader\": \"^2.1.1\",\n    \"mini-css-extract-plugin\": \"^0.6.0\",\n    \"nodemon\": \"^1.19.0\",\n    \"webpack\": \"^5.0.0-beta.16\",\n    \"webpack-cli\": \"^3.3.2\",\n    \"webpack-dev-middleware\": \"^3.6.2\",\n    \"webpack-node-externals\": \"^1.7.2\"\n  },\n  \"dependencies\": {\n    \"core-js\": \"^3.0.1\",\n    \"express\": \"^4.16.4\",\n    \"firebase-admin\": \"^9.2.0\",\n    \"firebase-functions\": \"^3.11.0\",\n    \"react\": \"^16.8.6\",\n    \"react-dom\": \"^16.8.6\"\n  }\n}\n`\n```\nIn package file I replaced babel script to copy files to functions folder instead of lib\n```\n`\"build:functions\": \"babel -d functions src\", \n`\n```\nHere is my app.js\n```\n`import React from 'react'\n// eslint-disable-next-line import/no-extraneous-dependencies\nimport loadable from '@loadable/component'\n\nconst App = () => (\n  \n  Hello world\n  \n)\n\nexport default App\n`\n```\nLoadable componets has lot of code in app.js so I just replaced app.js with simple hello world text\nHere is my firebase.json file\n```\n`{\n  \"hosting\": {\n    \"public\": \"public\",\n    \"ignore\": [\n      \"firebase.json\",\n      \"**/.*\",\n      \"**/node_modules/**\"\n    ],\n    \"rewrites\": [\n      {\n        \"source\": \"**\",\n        \"function\": \"supercharged\"\n      }\n    ]\n  }\n}\n`\n```\nThese all are the changes I made from the the loadable-components server side rendering async node example\nI don't know what I missed here, Please assist me if I missed or need to add anything",
      "solution": "Check by downgrading webpack to 4.31.0, Loadable components has some issues with webpack 5\n```\n`\"webpack\": \"^4.31.0\",\n\"webpack-cli\": \"^3.3.2\",\n\"webpack-dev-middleware\": \"^3.6.2\",\n\"webpack-node-externals\": \"^1.7.2\"\n`\n```",
      "question_score": 9,
      "answer_score": 1,
      "created_at": "2021-05-21T05:45:10",
      "url": "https://stackoverflow.com/questions/67630558/getting-error-path-argument-must-be-string-while-deploying-react-loadable-co"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 68220281,
      "title": "Google Cloud Platform / Firebase Function not triggering with onWrite",
      "problem": "My application makes use of Firestore Function Triggers to perform background actions based on changes in the Firestore, e.g. user profile changes. For example, if they change their mobile number, a verification code is sent.\nI have a trigger that should run when an onWrite() event happens on a specific collection. `onWrite()` runs when any of the following actions occur in Firebase on a specific collection:\n\nonCreate()\nonUpdate()\nonDelete()\n\nIn my usecase, I need it to run for `onCreate()` and `onUpdate()`, thus I use `onWrite()`\nFor Firebase Triggers to work, a specific format is expected in addition to a document id/wildcard representing a document that was created/changed/deleted.\nConstants:\n```\n`const collections = {\n    ...\n    conversations: \"conversations\",\n    ...\n}\n`\n```\nCallable Function (updates firestore):\n```\n`/**\n * Add an conversation to conversations collection\n * @type {HttpsFunction & Runnable}\n */\nexports.addConversations = functions.https.onCall(async (data, context) =>  {\n    // expects conversation & interested state\n    const {valid, errors} = validateRequiredBody(data, [\n        \"conversation\",\n    ]);\n    if (!valid) {\n        return {\n            status: false,\n            message: \"Missing or invalid parameters\",\n            errors: errors,\n            jwtToken: \"\",\n        };\n    }\n\n    // get conversation item\n    const conversation = {\n        id: data[\"conversation\"][\"id\"],\n        name: data[\"conversation\"][\"name\"],\n    }\n\n    // create conversation with empty counter\n    // let writeResult = await collectionRefs.conversationsRef.doc(conversation.id).set({\n    let writeResult = await admin.firestore().collection(collections.conversations).doc(conversation.id).set({\n        id: conversation.id,\n        name: conversation.name,\n        count: 0\n    });\n    console.log(`[function-addConversations] New Conversation [${conversation.name}] added`);\n\n    return {\n        status: true,\n        message: \"\"\n    }\n});\n`\n```\nFirestore Trigger (not triggering):\n```\n`/**\n * On conversations updated/removed, update corresponding counter\n * @type {CloudFunction>}\n */\nexports.onConversationProfileCollectionCreate = functions.firestore.document(`${collections.conversations}/{id}`)\n    .onWrite(async snapshot => {\n        console.log(\"Conversation Collection Changed\");\n        // let conversation = collectionRefs.conversationsRef.doc(snapshot.id);\n        // await conversation.update({count: FieldValue.increment(1)});\n    });\n`\n```\nIn my mobile application, the user (calls) the `addConversations()` firebase function, this adds the new conversation to Firestore which is clearly visible, but the counter trigger (trigger function) doesn't run.\nEmulator output:\n```\n`...\n{\"verifications\":{\"app\":\"MISSING\",\"auth\":\"MISSING\"},\"logging.googleapis.com/labels\":{\"firebase-log-type\":\"callable-request-verification\"},\"severity\":\"INFO\",\"message\":\"Callable request verification passed\"}\n[function-addConversations] New Conversation [Test Conversation Topic] added\nProfile updated \n(print profile data)\n...\n`\n```\nWhat I SHOULD expect to see:\n```\n`...\n{\"verifications\":{\"app\":\"MISSING\",\"auth\":\"MISSING\"},\"logging.googleapis.com/labels\":{\"firebase-log-type\":\"callable-request-verification\"},\"severity\":\"INFO\",\"message\":\"Callable request verification passed\"}\n[function-addConversations] New Conversation [Test Conversation Topic] added\nConversation Collection Changed      // this is output by the trigger\nProfile updated \n(print profile data)\n...\n`\n```\nDid I do something wrong?",
      "solution": "The issue was one closer to home.\nI am developing using the firebase emulators and connecting to them using the emulators addition in Flutter's firebase packages built in emulator feature.\nFirebase emulator setup e.g. Firebase Functions can be found here\nTL;DR:\nWhen starting your Firebase emulator, you should see something similar to:\n```\n`C:\\Users\\User\\MyApp\\awesome-app-server>firebase emulators:start --only functions\ni  emulators: Starting emulators: functions\n!  functions: You are running the functions emulator in debug mode (port=9229). This means that functions will execute in sequence rather than in parallel.\n!  functions: The following emulators are not running, calls to these services from the Functions emulator will affect production: auth, firestore, database, hosting, pubsub, storage\n!  Your requested \"node\" version \"12\" doesn''t match your global version \"14\"\ni  ui: Emulator UI logging to ui-debug.log\ni  functions: Watching \"C:\\Users\\User\\MyApp\\awesome-app-server\\functions\" for Cloud Functions...\n>  Debugger listening on ws://localhost:9229/03dc1d62-f2a3-418e-a343-bb0b357f7329\n>  Debugger listening on ws://localhost:9229/03dc1d62-f2a3-418e-a343-bb0b357f7329\n>  For help, see: https://nodejs.org/en/docs/inspector\n!  functions: The Cloud Firestore emulator is not running, so calls to Firestore will affect production.\n!  functions: The Realtime Database emulator is not running, so calls to Realtime Database will affect production.\n\n...\n`\n```\nBUT then you see this - THIS is very important!\n```\n`i  functions[us-central1-api-user-onConversationProfileCollectionCreate ]: function ignored because the database emulator does not exist or is not running.\n`\n```\nThis, since Firestore & (Realtime) Database use triggers which are found in functions, functions expects to find a local firestore/database emulator.\nSince no firestore/database emulators were running locally\n```\n`!  functions: The Cloud Firestore emulator is not running, so calls to Firestore will affect production.\n!  functions: The Realtime Database emulator is not running, so calls to Realtime Database will affect production.\n`\n```\nand these functions don't automagically attach to production Firestore/Database(s) (that would be potentially devestating), these triggers didn't run when I expected them to while emulating locally.\nSolution:\n\nEmulate firestore & database locally (see this to import your firestore data to a local emulator) with `firebase emulators:start --only functions,...,firestore,database`\n\nUpload functions to work with Firestore/Database(s) (please do this with care)\n\nMore details:\nBelow I provide details what lead me to the problem, and how I figured out the issue:\nWhat I was doing:\n```\n`firebase emulators:start --inspect-functions --only functions,auth\n`\n```\n\n[local] Firebase Functions I was developing and testing the backend for my mobile app using Firebase Functions for various interactivity.\nSince Firebase Auth is handled locally through the use of custom tokens on my firebase functions app, I used local auth for testing\n\nI had prepopulated my Firestore with data, thus I intended to use Firestore data while emulating locally which had worked as expected, BUT the firebase firestore & database triggers won't work due to emulating them locally.\nI knew some of my triggers DID infact trigger correctly, thus it must be a configuration error of some kind.\nExtended Solution (import firestore data to local emulator)\nfirebase emulators:start --inspect-functions --only functions,auth,firestore,database\nnotice the last 2, firestore & database - this should be added for triggers to work locally\n\nI noticed when reviewing the logs at startup, the text indicating some functions won't run. This made me realize I was making a crucial mistake - the cause.\n\nImport Data from production firestore\nThe reason for using production (during development) firestore is to not recreate all the data for each test. The solution is to export from firestore and import you data to the emulators.\nSee this for details - I wasn't able to export from the terminal, I thus went to Google Console to export to a storage bucket, and download it via the console from there.\nTo start the emulator (and allow debugging of firebase functions), I use:\n```\n`firebase emulators:start --inspect-functions --import ./functions/{path/to/data} --only functions,auth,firestore,database\n`\n```\ndetails:\n\n`firebase emulators:start` - start emulator\n`--inspect-functions` allow debugging functions via websockets - e.g. Webstorm NodeJs debugger on port 9229\n`--import ./functions/{path/to/data}` - import data to firestore emulator using project root as `./`\n`--only functions,auth,firestore,database` specify the modules to emulate",
      "question_score": 9,
      "answer_score": 1,
      "created_at": "2021-07-02T08:00:49",
      "url": "https://stackoverflow.com/questions/68220281/google-cloud-platform-firebase-function-not-triggering-with-onwrite"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 67635311,
      "title": "How to remove data on logout when using firebase and react query?",
      "problem": "When my user logs out, I want to remove all user data from the app, but I'm having trouble implementing this.\nI have a custom useUserData() hook that gets the user's data. getUser() is a callable cloud function. This is my code so far.\n```\n`import { useEffect, useState } from \"react\"\nimport { useQuery, useQueryClient } from \"react-query\"\nimport { getUser } from \"Services/firebase/functions\"\nimport firebase from \"firebase/app\"\n\nexport default function useUserData(){\n    const [ enabled, setEnabled] = useState(false)\n    const queryClient = useQueryClient()\n\n    useEffect(_ => {\n        const unsubscribe = firebase.auth().onAuthStateChanged(user => {\n            setEnabled(Boolean(user))\n            if (!user){\n                // remove data\n            }\n            else queryClient.invalidateQueries(\"user\", { refetchActive: true, refetchInactive: true })\n        })\n\n        return unsubscribe()\n    }, [])\n\n    return useQuery(\n        \"user\", \n        () => getUser().then(res => res.data),\n        {\n            enabled\n        }\n    )\n}\n\n`\n```\nEdit:\nIt seemed that I was handling my effect cleanup wrong. This seems to be working.\n```\n`import { useEffect, useState } from \"react\"\nimport { useQuery, useQueryClient } from \"react-query\"\nimport { getUser } from \"Services/firebase/functions\"\nimport firebase from \"firebase/app\"\n\nexport default function useUserData(){\n    const [ enabled, setEnabled] = useState(false)\n    const queryClient = useQueryClient()\n    useEffect(_ => {\n        const unsubscribe = firebase.auth().onAuthStateChanged(user => {\n            setEnabled(Boolean(user))    \n            if (!user) {\n                queryClient.removeQueries(\"user\")\n            } \n        })\n        \n        return _ => unsubscribe()\n    }, [])\n\n    return useQuery(\n        \"user\", \n        () => getUser().then(res => res.data),\n        {\n            enabled\n        }\n    )\n}\n\n`\n```\nWeirdly enough, the query still fetches once after logging out, when the query should already be disabled.",
      "solution": "Here's my current implementation which seems to work fine.\n```\n`import { useEffect, useState } from \"react\"\nimport { useQuery, useQueryClient } from \"react-query\";\nimport firebase from \"firebase/app\"\n\nexport default function useAuthenticatedQuery(key, func, options){\n    const [ enabled, setEnabled] = useState(false)\n    const queryClient = useQueryClient()\n    useEffect(_ => {\n        const unsubscribe = firebase.auth().onAuthStateChanged(user => {\n            setEnabled(Boolean(user))    \n            if (!user){\n                queryClient.setQueryData(key, _ => undefined)\n                queryClient.removeQueries(key, _ => undefined)\n            }else \n                queryClient.invalidateQueries(key, { refetchActive: true, refetchInactive: true })\n        })\n        return _ => unsubscribe()\n        // eslint-disable-next-line\n    }, [])\n\n    return useQuery(\n        key,\n        func,\n        {\n            ...options,\n            enabled\n        }\n    )\n}\n`\n```\nI use it just like the regular useQuery hook:\n```\n`import useAuthenticatedQuery from \"Hooks/useAuthenticatedQuery\"\n\nexport default function useUserData(){\n\n    return useAuthenticatedQuery(\n        \"user\", \n        () => getUser().then(res => res.data)\n    )\n}\n`\n```",
      "question_score": 8,
      "answer_score": 1,
      "created_at": "2021-05-21T12:41:42",
      "url": "https://stackoverflow.com/questions/67635311/how-to-remove-data-on-logout-when-using-firebase-and-react-query"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 73079648,
      "title": "Cloud Functions FieldValue increment TypeError in Firestore",
      "problem": "I am testing cloud functions in the firebase emulator and getting an error when trying to increment a field in the cloud firestore. Please check my code & error message below. Thanks for any help!\n```\n`import * as functions from \"firebase-functions\";\nimport * as admin from \"firebase-admin\";\n\nadmin.initializeApp();\n\nconst db = admin.firestore();\n\nexport const newUserIncrementStat = functions.firestore.document(\"users/{uid}\").onCreate((snap, context) => {\n  const docRef = db.doc(\"stats/users\");\n\n  try {\n    return docRef.set({\n      totalUsers: admin.firestore.FieldValue.increment(1),\n    }, {merge: true});\n\n  } catch (e) {\n    console.log(\"Something is wrong: \", e);\n    return Promise.reject(e);\n  }\n});\n`\n```\nError Message from Firebase Logs:\nTypeError: Cannot read properties of undefined (reading 'increment')\nDependencies\nfirebase-admin: \"^11.0.0\"\nfirebase-functions: \"^3.22.0\"\nfirebase: 11.3.0",
      "solution": "as @Min commented the error was with the firebase emulator, deploying the function directly to google cloud works without any error.",
      "question_score": 8,
      "answer_score": 3,
      "created_at": "2022-07-22T13:30:04",
      "url": "https://stackoverflow.com/questions/73079648/cloud-functions-fieldvalue-increment-typeerror-in-firestore"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 75969901,
      "title": "Firebase function deployment fails with &#39;missing&#39; dependencies error",
      "problem": "I am encountering an error when attempting to deploy a Firebase function. Despite deleting `node_modules` and `package-lock.json`, running `npm install`, and executing `firebase deploy --only functions` many times, the build process still fails with the following error:\n```\n`Build failed: ...regex-range@5.0.1 from lock file\nnpm ERR! Missing: brace-expansion@1.1.11 from lock file\nnpm ERR! Missing: balanced-match@1.0.2 from lock file\nnpm ERR! Missing: concat-map@0.0.1 from lock file\nnpm ERR! Missing: mimic-fn@2.1.0 from lock file\nnpm ERR! Missing: p-limit@2.3.0 from lock file\nnpm ERR! Missing: error-ex@1.3.2 from lock file\nnpm ERR! Missing: json-parse-even-better-errors@2.3.1 from lock file\nnpm ERR! Missing: lines-and-columns@1.2.4 from lock file\nnpm ERR! Missing: is-arrayish@0.2.1 from lock file\nnpm ERR! Missing: ansi-styles@5.2.0 from lock file\nnpm ERR! Missing: react-is@18.2.0 from lock file\nnpm ERR! Missing: kleur@3.0.3 from lock file\nnpm ERR! Missing: sisteransi@1.0.5 from lock file\nnpm ERR! Missing: is-core-module@2.11.0 from lock file\nnpm ERR! Missing: path-parse@1.0.7 from lock file\nnpm ERR! Missing: supports-preserve-symlinks-flag@1.0.0 from lock file\nnpm ERR! Missing: shebang-regex@3.0.0 from lock file\nnpm ERR! Missing: buffer-from@1.1.2 from lock file\nnpm ERR! Missing: escape-string-regexp@2.0.0 from lock file\nnpm ERR! Missing: char-regex@1.0.2 from lock file\nnpm ERR! Missing: has-flag@4.0.0 from lock file\nnpm ERR! Missing: is-number@7.0.0 from lock file\nnpm ERR! Missing: picocolors@1.0.0 from lock file\nnpm ERR! Missing: convert-source-map@1.9.0 from lock file\nnpm ERR! Missing: makeerror@1.0.12 from lock file\nnpm ERR! Missing: tmpl@1.0.5 from lock file\nnpm ERR! Missing: isexe@2.0.0 from lock file\nnpm ERR! Missing: ms@2.1.2 from lock file\nnpm ERR! Missing: yallist@3.1.1 from lock file\nnpm ERR! Missing: ansi-styles@3.2.1 from lock file\nnpm ERR! Missing: escape-string-regexp@1.0.5 from lock file\nnpm ERR! Missing: supports-color@5.5.0 from lock file\nnpm ERR! Missing: color-convert@1.9.3 from lock file\nnpm ERR! Missing: color-name@1.1.3 from lock file\nnpm ERR! Missing: has-flag@3.0.0 from lock file\nnpm ERR! Missing: ms@2.1.2 from lock file\nnpm ERR! Missing: ms@2.1.2 from lock file\nnpm ERR! Missing: cliui@8.0.1 from lock file\nnpm ERR! Missing: yargs-parser@21.1.1 from lock file\nnpm ERR! Missing: p-try@2.2.0 from lock file\nnpm ERR!\nnpm ERR! Clean install a project\nnpm ERR!\nnpm ERR! Usage:\nnpm ERR! npm ci\nnpm ERR!\nnpm ERR! Options:\nnpm ERR! [-S|--save|--no-save|--save-prod|--save-dev|--save-optional|--save-peer|--save-bundle]      \nnpm ERR! [-E|--save-exact] [-g|--global] [--global-style] [--legacy-bundling]\nnpm ERR! [--omit  [--omit  ...]]\nnpm ERR! [--strict-peer-deps] [--no-package-lock] [--foreground-scripts]\nnpm ERR! [--ignore-scripts] [--no-audit] [--no-bin-links] [--no-fund] [--dry-run]\nnpm ERR! [-w|--workspace  [-w|--workspace  ...]]\nnpm ERR! [-ws|--workspaces] [--include-workspace-root] [--install-links]\nnpm ERR!\nnpm ERR! aliases: clean-install, ic, install-clean, isntall-clean\nnpm ERR!\nnpm ERR! Run \"npm help ci\" for more info\n\nnpm ERR! A complete log of this run can be found in:\nnpm ERR!     /www-data-home/.npm/_logs/2023-04-09T09_55_08_623Z-debug-0.log; Error ID: beaf8772      \n\nFunctions deploy had errors with the following functions:\n        update(asia-east2)\ni  functions: cleaning up build files...\n\nError: There was an error deploying functions\n`\n```\n`package.json`\n```\n`{\n  \"name\": \"functions\",\n  \"description\": \"Cloud Functions for Firebase\",\n  \"scripts\": {\n    \"serve\": \"firebase emulators:start --only functions\",\n    \"shell\": \"firebase functions:shell\",\n    \"start\": \"npm run shell\",\n    \"deploy\": \"firebase deploy --only functions\",\n    \"logs\": \"firebase functions:log\"\n  },\n  \"main\": \"index.js\",\n  \"dependencies\": {\n    \"firebase-admin\": \"^10.0.0\",\n    \"firebase-functions\": \"^4.0.0\",\n    \"stripe\": \"^8.0.0\"\n  },\n  \"devDependencies\": {\n    \"firebase-functions-test\": \"^3.0.0\"\n  },\n  \"private\": true,\n  \"engines\": {\n    \"node\": \"16\"\n  }\n}\n`\n```\n`firebase.json`\n```\n`{\n  \"functions\": [\n    {\n      \"source\": \"functions\",\n      \"codebase\": \"default\",\n      \"ignore\": [\n        \"node_modules\",\n        \".git\",\n        \"firebase-debug.log\",\n        \"firebase-debug.*.log\"\n      ]\n    }\n  ]\n}\n`\n```\nI don't have `.gcloudignore` but `.gitignore`:\n```\n`node_modules/\n`\n```\nI have manually `npm` installed each of them but anytime I execute `firebase deploy --only functions`, it keeps asking for more. Now my `package-lock.json` has hundreds of packages.\nThe code I'm testing:\n```\n`exports.helloWorld = functions\n  .region(\"asia-east2\")\n  .https.onRequest((request, response) => {\n    functions.logger.info(\"Hello logs!\", { structuredData: true });\n    response.send(\"Hello from Firebase!\");\n  });\n`\n```\nThe steps I took:\n\n`npm install -g firebase-tools`\n\n`firebase login`\n\n`firebase init functions` - Use an existing project ...\n\n`cd functions`\n\n`npm i stripe`\n\nedit `index.js`\n\n`firebase deploy --only functions`\n\nOn the Firebase Dashboard, this function is indicated by a red triangle with the message 'Function deployment failed. Please try again.'\nI have also tried updating the affected packages in my `package.json` file, but the error persists.\nI am unsure what could be causing this issue, and I would greatly appreciate any guidance or suggestions on how to fix it. Thank you for your help.",
      "solution": "I have checked `firebase-debug.log` in this issue and it contains `Could not find functions.yaml. Must use http discovery` :\n```\n`...\n[debug] [2023-04-12T00:48:22.311Z] Building nodejs source\n[debug] [2023-04-12T00:48:22.312Z] Could not find functions.yaml. Must use http discovery\n[debug] [2023-04-12T00:48:22.319Z] Found firebase-functions binary at 'C:\\Users\\munic\\Desktop\\programming\\penvie\\testfunc\\functions\\node_modules\\.bin\\firebase-functions'\n[debug] [2023-04-12T00:48:22.460Z] Serving at port 8008\n\n[debug] [2023-04-12T00:48:22.666Z] Got response from /__/functions.yaml {\"endpoints\":{\"helloWorld\":{\"platform\":\"gcfv1\",\"availableMemoryMb\":null,\"timeoutSeconds\":null,\"minInstances\":null,\"maxInstances\":null,\"ingressSettings\":null,\"serviceAccountEmail\":null,\"vpc\":null,\"httpsTrigger\":{},\"entryPoint\":\"helloWorld\"}},\"specVersion\":\"v1alpha1\",\"requiredAPIs\":[]}\n[debug] [2023-04-12T00:48:22.672Z] shutdown requested via /__/quitquitquit\n\n[info] i  functions: preparing functions directory for uploading... \n[info] i  functions: packaged C:\\Users\\munic\\Desktop\\programming\\penvie\\testfunc\\functions (63.59 KB) for uploading \n...\n`\n```\nAfter reading a similar issue, I see that they recommend downgrading `firebase-tools` to `10.0.0`, `10.8.0`, `11.17.0`, `11.18.0`, `11.20.0`, or `11.22.0`. Mostly they prefer to downgrade it to `11.17.0`.\n```\n`npm i -g firebase-tools@11.17.0\n`\n```",
      "question_score": 8,
      "answer_score": 6,
      "created_at": "2023-04-09T12:10:52",
      "url": "https://stackoverflow.com/questions/75969901/firebase-function-deployment-fails-with-missing-dependencies-error"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 72553425,
      "title": "Why firebase emulator function says request body is missing data?",
      "problem": "I'm running my local emulator suite and I'm constantly getting error messages for my curl requests. The following command:\n```\n`curl -X POST http://localhost:5001/my-project/us-central1/myFunction \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\"productId\": 123456, \"quantity\": 100}'  \n`\n```\nIs always showing this in the emulator CLI:\n```\n`>  {\"productId\":123456,\"quantity\":100,\"severity\":\"WARNING\",\"message\":\"Request body is missing data.\"}\n>  {\"severity\":\"ERROR\",\"message\":\"Invalid request, unable to process.\"}\n`\n```\nNone of the code was executed in the function as it starts with a console log which is never printed here. Any thoughts?",
      "solution": "That error occurs when you use the `onCall` method. So, I would assume that you are using `functions.https.onCall`. As explained in this documentation:\n\nIt's important to keep in mind that HTTPS callable functions are similar but not identical to HTTP functions. To use HTTPS callable functions you must use the client SDK for your platform together with the functions.https backend API (or implement the protocol).\n\nIf you want to directly call the function via its endpoint then you should follow the protocol specification for `https.onCall`. One of its requirements is the request body should be a `JSON` object with `data` as its main key.\nAn example request `JSON` object should be like this instead:\n```\n`{\n    \"data\": {\n        \"productId\": 123456,\n        \"quantity\": 100\n    }\n}\n`\n```\nFor reference, here's the full curl request:\n```\n`curl -X POST http://localhost:5001/my-project/us-central1/myFunction \\\n    -H \"Content-Type: application/json\" \\\n    -d '{ \"data\": { \"productId\": 123456, \"quantity\": 100 } }' \n`\n```\n\nFor more information, you may check out this documentation:\n\nProtocol specification for https.onCall",
      "question_score": 8,
      "answer_score": 13,
      "created_at": "2022-06-09T02:06:30",
      "url": "https://stackoverflow.com/questions/72553425/why-firebase-emulator-function-says-request-body-is-missing-data"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 69843556,
      "title": "How do I get access token inside Node.JS Google Cloud function?",
      "problem": "I have a cloud function in Node.JS on Google Cloud, I need to make GET request to Google and it requires an auth token. Using `curl` you can generate one using `$(gcloud auth application-default print-access-token)`. But this doesn't work in Cloud instance so how I can generate one?\nSome of the function:\n```\n`exports.postTestResultsToSlack = functions.testLab\n  .testMatrix()\n  .onComplete(async testMatrix => {\n\n    if (testMatrix.clientInfo.details['testType'] != 'regression') {\n      // Not regression tests\n      return null;\n    }\n\n    const { testMatrixId, outcomeSummary, resultStorage } = testMatrix;\n\n    const projectID = \"project-feat1\"\n    const executionID = resultStorage.toolResultsExecutionId\n    const historyID = resultStorage.toolResultsHistoryId\n\n    const historyRequest = await axios.get(`https://toolresults.googleapis.com/toolresults/v1beta3/projects/${projectID}/histories/${historyID}/executions/${executionID}/environments`, {\n      headers: {\n        'Authorization': `Bearer $(gcloud auth application-default print-access-token)`,\n        'X-Goog-User-Project': projectID\n      }\n    });\n`\n```",
      "solution": "After countless hours spent, I stumbled across the answer scrolling through auto complete suggestions. Google has documentation about authentication but none mention this what you need for Cloud Functions to make API requests:\n```\n`const {GoogleAuth} = require('google-auth-library');\n\nconst auth = new GoogleAuth();\nconst token = await auth.getAccessToken()\n\nconst historyRequest = await axios.get(\n`https://toolresults.googleapis.com/toolresults/v1beta3/projects/${projectID}/histories/${historyID}/executions/${executionID}/environments`, \n      {\n        headers: {\n          'Authorization': `Bearer ${token}`,\n          'X-Goog-User-Project': projectID\n        }\n    });\n`\n```",
      "question_score": 8,
      "answer_score": 9,
      "created_at": "2021-11-04T18:39:54",
      "url": "https://stackoverflow.com/questions/69843556/how-do-i-get-access-token-inside-node-js-google-cloud-function"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 69731946,
      "title": "Firebase functions parsing request body before I can handle it in Express",
      "problem": "I'm trying to handle invalid requests in Firebase functions, so making a post request with invalid JSON, with the intent of handling it in express. but I get 400 error 'SyntaxError: Unexpected token a in JSON at position 20' before it even reaches express layer, and the worst thing is the function runs for 60 seconds untill it hits timeout error.\nmy function\n`import * as functions from 'firebase-functions';\nimport * as express from 'express';\nimport * as admin from 'firebase-admin';\n\nadmin.initializeApp();\n\nconst app = express();\n\napp.use((err: any, req: any, res: any, next: any) => {\n  res.json({ error: 'invalid request' });\n  next(err);\n});\n\napp.post('/test', (req: any, res: any) => {\n  res.json({ error: 'invalid request' });\n  res.end();\n  return;\n});\n\nconst server = functions.runWith({ maxInstances: 100 }).https.onRequest(app);\n\nexport { server as api };\n`\nthe invalid json,\n```\n`{\n    \"es\":\"adfasdf\"asdf\n}\n`\n```\nI suspect this has something to do with the https://firebase.google.com/docs/functions/http-events#read_values_from_the_request at \"This parsing is done by the following body parsers:\"\n\nAlso to note, at the time of asking this question, it was 1st gen of functions.",
      "solution": "Firebase Functions are built on top of the `functions-framework-nodejs` package (or at least, an internal variant of the same code).\nWithin that package, the body parsers you mention are injected. As you correctly surmised, these are indeed added before your code even gets the chance to execute.\nAs the error is internal to Firebase's operations, you will need to reach out to Firebase support directly.",
      "question_score": 8,
      "answer_score": 7,
      "created_at": "2021-10-27T04:06:45",
      "url": "https://stackoverflow.com/questions/69731946/firebase-functions-parsing-request-body-before-i-can-handle-it-in-express"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 76385289,
      "title": "Is there a way to use onSchedule and also set a custom &#39;timeoutSeconds&#39; and &#39;memory&#39; using Firebase functions V2?",
      "problem": "I have had to revert back to using Firebase functions V1 in order to schedule the running of my functions and also specify the runtime options including timeoutSeconds and memory in my code (written in TypeScript):\n```\n`const runtimeOpts = {\n    timeoutSeconds: 540,\n    memory: \"1GB\" as const,\n};\nexports.cleanupEvents = functions\n    .runWith(runtimeOpts)\n    .pubsub.schedule(\"0 0 * * *\")\n    .timeZone(\"Europe/Berlin\")\n    .onRun(async () => {\n        await cleanupOldEvents(adminDb);\n        logger.log(\"Event cleanup finished\");\n    });\n\n`\n```\nDoes anyone know if it is possible with Firebase functions V2 using the onSchedule syntax to also specify these runtimeOpts in code? Without needing to go into the google cloud console and manually setting it there.\nI have tried chaining'onSchedule' and 'runWith' together and seeing what other possibilities Emmet suggests, so far but had no luck.",
      "solution": "The API documentation for onSchedule suggests that you can pass an object as the first parameter, which is a ScheduleOptions object, an extension of GlobalOptions:\n`onSchedule({\n    schedule: \"your-schedule-here\",\n    timeoutSeconds: your-timeout,\n    memory: your-memory,\n    // include other options here from SchedulerOptions or GlobalOptions\n}, (event) => { ... })\n`",
      "question_score": 7,
      "answer_score": 14,
      "created_at": "2023-06-01T21:57:26",
      "url": "https://stackoverflow.com/questions/76385289/is-there-a-way-to-use-onschedule-and-also-set-a-custom-timeoutseconds-and-mem"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 72661065,
      "title": "ERR_PACKAGE_PATH_NOT_EXPORTED with firebase-admin 11.0.0 and firebase-functions 3.21.2",
      "problem": "I'm trying to update to the most recent versions of `firebase-admin` (11.0.0) and `firebase-functions` (3.21.2). I'm using `firebase-tools` 11.1.0. I get this error when try to deploy my functions:\n```\n`Error: Failed to load function definition from source: Failed to generate manifest from function source: Error [ERR_PACKAGE_PATH_NOT_EXPORTED]: Package subpath './lib' is not defined by \"exports\" in /Users/myuser/Documents/myproject/node_modules/firebase-functions/package.json\n`\n```\nI've seen similar errors in StackOverflow like this one or this one but this error is slightly different. The problem is not in `firebase` or `firebase-admin` dependencies but with `firebase-functions`.\nUsing `firebase-functions` 3.14.1 works (I get some warnings though) but I'd like to update to the latest version so I can hopefully get rid of warnings and get the latest updates.\nHow can I fix this?\nThanks!",
      "solution": "As the error described, the problem was that I had imports referencing the lib folder of firebase-functions like this:\n```\n`import { HttpsError } from 'firebase-functions/lib/providers/https'\n...\nthrow new HttpsError('failed-precondition', 'An error')\n`\n```\nThe problem was gone after removing all of them and replacing with something like the following:\n```\n`import { https } from 'firebase-functions'\n...\nthrow new https.HttpsError('failed-precondition', 'An error')\n`\n```\nThe first approach worked until 3.14.1. Above that, it looks like we can't reference the lib folder in the from. Not ideal because I wanted to avoid the namespace when using these types, but at least it works.",
      "question_score": 7,
      "answer_score": 10,
      "created_at": "2022-06-17T16:40:00",
      "url": "https://stackoverflow.com/questions/72661065/err-package-path-not-exported-with-firebase-admin-11-0-0-and-firebase-functions"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 65955328,
      "title": "Deployment error. Build failed: Build error details not available. Firebase Cloud Functions",
      "problem": "I reinstalled multiple times `NPM` and `Node` on my pc.\n(npm version 7.4.3)\n(node version v15.7.0)\nI followed the procedure for configuring the `Firebase CLI` with:\n```\n`npm install -g firebase-tools\n`\n```\nand `firebase init` and `firebase deploy` and the configuration seems to work fine.\nThe problem I'm facing happens when I open the `index.js` file and I uncomment the stock helloWorld function which looks like this:\n```\n`exports.helloWorld = functions.https.onRequest((request, response) => {\nfunctions.logger.info(\"Hello logs!\", {structuredData: true});\nresponse.send(\"Hello from Firebase!\");\n});\n`\n```\nI run `firebase deploy` and I receive this error\n```\n`functions[helloWorld(us-central1)]: Deployment error.\nBuild failed: Build error details not available. Please check the logs at https://console.    {urlStuff}\n\nFunctions deploy had errors with the following functions:\nhelloWorld\n\n To try redeploying those functions, run:\nfirebase deploy --only \"functions:helloWorld\"\n\n To continue deploying other features (such as database), run:\nfirebase deploy --except functions\n\n Error: Functions did not deploy properly.\n`\n```\nI honestly don't know what to do now.\nI tried multiple times to re install node and npm and re doing the Firebase CLI procedure but nothing seems to solve this problem, I still receive this Error when deploying.\nThe log error I receive is this :\n```\n`textPayload: \"ERROR: error fetching storage source: generic::unknown: retry budget exhausted (3 attempts): fetching gcs source: unpacking source from gcs: source fetch container exited with non-zero status: 1\"\n`\n```",
      "solution": "As suggested by this link provided by @Muthu Thavamani :\nGCP Cloud Function - ERROR fetching storage source during build/deploy\n`Firebase CLI` uses NodeJS `version 12` while on my device I had `version 15` installed.\nJust use this guide to downgrade your version of NodeJS and everything works fine.",
      "question_score": 7,
      "answer_score": 10,
      "created_at": "2021-01-29T14:23:49",
      "url": "https://stackoverflow.com/questions/65955328/deployment-error-build-failed-build-error-details-not-available-firebase-clou"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 69945493,
      "title": "Google Scheduled functions: There was an error deploying functions?",
      "problem": "I have a fresh project but was looking to test scheduled functions. Am I missing anything?\n```\n`$ firebase deploy\n\n=== Deploying to 'testing-db'...\n\ni  deploying functions\ni  functions: ensuring required API cloudfunctions.googleapis.com is enabled...\ni  functions: ensuring required API cloudbuild.googleapis.com is enabled...\n!  functions: missing required API cloudbuild.googleapis.com. Enabling now...\n+  functions: required API cloudfunctions.googleapis.com is enabled\n+  functions: required API cloudbuild.googleapis.com is enabled\ni  functions: preparing functions directory for uploading...\ni  functions: packaged functions (24.45 KB) for uploading\ni  functions: ensuring required API pubsub.googleapis.com is enabled...\ni  functions: ensuring required API cloudscheduler.googleapis.com is enabled...\n!  functions: missing required API cloudscheduler.googleapis.com. Enabling now...\n+  functions: required API pubsub.googleapis.com is enabled\n+  functions: required API cloudscheduler.googleapis.com is enabled\n+  functions: functions folder uploaded successfully\ni  functions: creating Node.js 14 function scheduledFunction(us-central1)...\n\nFunctions deploy had errors with the following functions:\n        scheduledFunction(us-central1)\ni  functions: cleaning up build files...\n\nError: There was an error deploying functions\n`\n```\nIndex.js\n```\n`const functions = require('firebase-functions');\n\nexports.scheduledFunction = functions.pubsub\n  .schedule('every 1 minutes')\n  .onRun((context) => {\n    \n    return console.log('This will be run every 1 minutes!');\n  });\n`\n```\nFirebase log shows:\n```\n`Error: Failed to upsert schedule function scheduledFunction in region europe-west1\n`\n```",
      "solution": "When you are using scheduled functions in Firebase Functions, an App Engine instance is created that is needed for Cloud Scheduler to work. You can read about it here.They use the location that has been set by default for resources. I think that you are getting that error because there is a difference between the default GCP resource location you specified and the region of your scheduled cloud function. If you click on the cogwheel next to project-overview in Firebase you can see where your resources are located.\nCheck your Cloud Scheduler function details and see which region it has been deployed to. By default, functions run in the us-central1 region. Check this link to see how we can change the region of the function.",
      "question_score": 7,
      "answer_score": 1,
      "created_at": "2021-11-12T16:43:46",
      "url": "https://stackoverflow.com/questions/69945493/google-scheduled-functions-there-was-an-error-deploying-functions"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 72454571,
      "title": "Deploying firebase cloud functions using github actions",
      "problem": "I'm trying to deploy my firebase cloud functions app using github actions:\n```\n`name: Deploy\n\n'on':\n  push:\n    branches:\n      - main\n\njobs:\n  deploy_to_production:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2\n      - name: install dependencies\n        run: cd functions/ && npm install\n      - name: deploy to production\n        uses: w9jds/firebase-action@master\n        with:\n          args: deploy --only functions\n        env:\n          FIREBASE_TOKEN: ${{ secrets.FIREBASE_TOKEN }}\n          \n`\n```\nThe step \"deploy to production\" is not successful. I activate debug mode and I don't actually receive any useful information:\n```\n`##[debug]Evaluating: secrets.FIREBASE_TOKEN\n##[debug]Evaluating Index:\n##[debug]..Evaluating secrets:\n##[debug]..=> Object\n##[debug]..Evaluating String:\n##[debug]..=> 'FIREBASE_TOKEN'\n##[debug]=> '***'\n##[debug]Result: '***'\n##[debug]Evaluating condition for step: 'deploy to production'\n##[debug]Evaluating: success()\n##[debug]Evaluating success:\n##[debug]=> true\n##[debug]Result: true\n##[debug]Starting: deploy to production\n##[debug]Loading inputs\n##[debug]Loading env\nRun w9jds/firebase-action@master\n  with:\n    args: deploy --only functions\n  env:\n    FIREBASE_TOKEN: ***\n/usr/bin/docker run --name w9jdsfirebaseactionv212_2c5197 --label 08450d --workdir /github/workspace --rm -e FIREBASE_TOKEN -e INPUT_ARGS -e HOME -e GITHUB_JOB -e GITHUB_REF -e GITHUB_SHA -e GITHUB_REPOSITORY -e GITHUB_REPOSITORY_OWNER -e GITHUB_RUN_ID -e GITHUB_RUN_NUMBER -e GITHUB_RETENTION_DAYS -e GITHUB_RUN_ATTEMPT -e GITHUB_ACTOR -e GITHUB_WORKFLOW -e GITHUB_HEAD_REF -e GITHUB_BASE_REF -e GITHUB_EVENT_NAME -e GITHUB_SERVER_URL -e GITHUB_API_URL -e GITHUB_GRAPHQL_URL -e GITHUB_REF_NAME -e GITHUB_REF_PROTECTED -e GITHUB_REF_TYPE -e GITHUB_WORKSPACE -e GITHUB_ACTION -e GITHUB_EVENT_PATH -e GITHUB_ACTION_REPOSITORY -e GITHUB_ACTION_REF -e GITHUB_PATH -e GITHUB_ENV -e GITHUB_STEP_SUMMARY -e RUNNER_DEBUG -e RUNNER_OS -e RUNNER_ARCH -e RUNNER_NAME -e RUNNER_TOOL_CACHE -e RUNNER_TEMP -e RUNNER_WORKSPACE -e ACTIONS_RUNTIME_URL -e ACTIONS_RUNTIME_TOKEN -e ACTIONS_CACHE_URL -e GITHUB_ACTIONS=true -e CI=true -v \"/var/run/docker.sock\":\"/var/run/docker.sock\" -v \"/home/runner/work/_temp/_github_home\":\"/github/home\" -v \"/home/runner/work/_temp/_github_workflow\":\"/github/workflow\" -v \"/home/runner/work/_temp/_runner_file_commands\":\"/github/file_commands\" -v \"/home/runner/work/personalsite-backend/personalsite-backend\":\"/github/workspace\" w9jds/firebase-action:v2.1.2 deploy --only functions\n##[debug]Docker Action run completed with exit code 2\n##[debug]Finishing: deploy to production\n`\n```\nAm I missing something?\nNote: Locally I can deploy without any problem.",
      "solution": "By using `w9jds/firebase-action`, there's a known issue wherein if you use `uses: w9jds/firebase-action@master`, it tries to store what the CLI spits out and if it errors out and ends the action before it can echo it this might stop it from printing out the response. More information from the repository owner here.\nStarting with version `v2.1.2`, you must replace this line:\n```\n`uses: w9jds/firebase-action@master\n`\n```\nto this:\n```\n`uses: docker://w9jds/firebase-action:master\n`\n```\nMore information here.\n\nMoreover, there is also an alternate solution to this by using `actions/checkout` instead. See `yaml` configuration below:\n```\n`name: Deploy to Firebase Functions via github action\n\"on\":\n  push:\n    branches:\n      - main\nenv:\n  CI: false\n\njobs:\n  build_and_deploy:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2\n\n      - name: Download deps\n        working-directory: functions\n        run: npm install\n\n      - name: Deploy\n        run: npx firebase-tools deploy\n        env:\n          FIREBASE_TOKEN: ${{ secrets.FIREBASE_TOKEN }}\n`\n```\nNote: I've used `actions/checkout@v2` on the sample yaml above, but `v3` is now available.",
      "question_score": 7,
      "answer_score": 10,
      "created_at": "2022-06-01T00:10:26",
      "url": "https://stackoverflow.com/questions/72454571/deploying-firebase-cloud-functions-using-github-actions"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 65554303,
      "title": "unable to test google assistant action on web simulator (Error Cloud function deployment failed. Dismiss)",
      "problem": "Hey Respected Community!\nI started learning to create google actions.\nso i created very basic google action. which invokes by 'hey google talk to Doctor Strange'\nand after adding 1 more transition which displays suggestion. I saved it and trying to test it.\nbut continously getting error.\ncloud function deployment failed.\n\ni am continously trying to test it but getting error.\nCan anyone help me what i am missing?\nthanks in advance",
      "solution": "As you surmise in your comments, using the Inline Editor for the webhook fulfillment requires you to have billing enabled for the cloud project it is attached to. (This is because it uses Cloud Functions for Firebase under the hood, and this requires billing to be enabled, even if you limit yourself to the free tier.)\nYou don't need to use the Inline Editor, or even Google Cloud Functions, for your webhook. All you need is a public HTTPS server that can accept and respond with JSON.",
      "question_score": 7,
      "answer_score": 8,
      "created_at": "2021-01-03T20:55:32",
      "url": "https://stackoverflow.com/questions/65554303/unable-to-test-google-assistant-action-on-web-simulator-error-cloud-function-de"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 74083042,
      "title": "NPM Error: Fix the upstream dependency conflict installing NPM Packages (Cloud Function)",
      "problem": "I just updated firebase functions with this function `npm i firebase-functions@latest`, and updated `npm install -g firebase-tools`. And suddenly I have unabled to deploy all of my functions at `firebase deploy --only functions`. I got all of these errors:\n\n```\n`Build failed: npm ERR! code ERESOLVE\nnpm ERR! ERESOLVE could not resolve\nnpm ERR!\nnpm ERR! While resolving: firebase-functions-test@0.2.3\nnpm ERR! Found: firebase-functions@4.0.0-rc.0\nnpm ERR! node_modules/firebase-functions\nnpm ERR!   firebase-functions@\"^4.0.0-rc.0\" from the root project\nnpm ERR!\nnpm ERR! Could not resolve dependency:\nnpm ERR! peer firebase-functions@\">=2.0.0\" from firebase-functions-test@0.2.3\nnpm ERR! node_modules/firebase-functions-test\nnpm ERR!   dev firebase-functions-test@\"^0.2.0\" from the root project\nnpm ERR!\nnpm ERR! Conflicting peer dependency: firebase-functions@3.24.1\nnpm ERR! node_modules/firebase-functions\nnpm ERR!   peer firebase-functions@\">=2.0.0\" from firebase-functions-test@0.2.3\nnpm ERR!   node_modules/firebase-functions-test\nnpm ERR!     dev firebase-functions-test@\"^0.2.0\" from the root project\nnpm ERR!\nnpm ERR! Fix the upstream dependency conflict, or retry\nnpm ERR! this command with --force, or --legacy-peer-deps\nnpm ERR! to accept an incorrect (and potentially broken) dependency resolution.\nnpm ERR!\nnpm ERR! See /www-data-home/.npm/eresolve-report.txt for a full report.\n`\n```\n\nAnyone know what's happening? I have tried this function on stackoverflow, but no luck at all. `npm install --legacy-peer-deps` Please help me! I have been like this for two days!",
      "solution": "The issue that you're encountering is that `firebase-functions-test` specifies that it needs firebase-functions at at version `>=2.0.0`. npm interprets this as \"stable versions greater than or equal to 2.0.0\". That means versions like `3.24.1`, `3.0.0`, `2.3.1` would all be valid, but things like `4.0.0-rc.0` or a hypothetical `5.9.3-beta` would not.\nWhen you ran `npm i firebase-functions@latest`, it grabbed the version that the devs had tagged as \"latest\", which was `4.0.0-rc-0`, which does not satisfy the aforementioned constraint.\nI'd recommend explicitly installing a stable version (`npm install firebase-functions@3.24.1`), or -if you have access to `firebase-functions-test`- modify that package's `package.json` to specify `\"firebase-functions\": \">=2.0.0 || 4.0.0-rc.0\"`.\nCheck out the npm semver calculator to see what specifiers match which versions.",
      "question_score": 7,
      "answer_score": 2,
      "created_at": "2022-10-15T23:06:45",
      "url": "https://stackoverflow.com/questions/74083042/npm-error-fix-the-upstream-dependency-conflict-installing-npm-packages-cloud-f"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 75637545,
      "title": "Accessing ChatGPT API through Firebase Cloud Function",
      "problem": "Below is some code for a simple Firebase Cloud Function that hits the ChatGPT API. Deploying this code and accessing it from my app results in a CORS error.\n`import * as functions from \"firebase-functions\";\nimport {defineString} from \"firebase-functions/v2/params\";\nimport {Configuration, OpenAIApi} from \"openai\";\n\nconst openAIKey = defineString(\"OPEN_API_KEY\");\n\nexport const getSummary = functions.https.onCall(async (data) => {\n  const configuration = new Configuration({\n    apiKey: openAIKey.value(),\n  });\n  const openai = new OpenAIApi(configuration);\n  const completion = await openai.createChatCompletion({\n    model: \"gpt-3.5-turbo\",\n    messages: [\n      {\n        role: \"user\",\n        content: data.prompt,\n      },\n    ],\n  });\n  const [choice] = completion.data.choices;\n  return {\n    response: choice.message ?? \"no response\",\n  };\n});\n`\nThis cloud function works perfectly when I access it from my app using the functions emulator. I only get the CORS error when I deploy it to the cloud and try to use it.\nAlso, I have a `helloWorld` function deployed alongside this one so that I can check that there's nothing wrong with my whole functions setup, and it works fine also. Furthermore, when I go into my Cloud Functions Console and test the function directly, it also works. So the issue clearly has to do with accessing the API specifically via the cloud function production environment and specifically from the app.\nUpdate: Here's the client code and the exact error:\n```\n`const getSummary = httpsCallable(functions, \"getSummary\");\nasync function askGPT() {\n    const result = await getSummary({\n      prompt: \"Please summarize the question in the following text. Phrase your response in the form of a question, and use Markdown for any formatting you might need.\\n\\n\" + question.text\n    });\n    question.question_summary = (\n      (question.question_summary ?? \"\") // @ts-ignore\n      + (result?.data?.response?.content || \"\").trim()\n    );\n  }\n`\n```\nerror:\n\nAccess to fetch at 'https://us-central1-my-documentation.cloudfunctions.net/getSummary' from origin 'http://localhost:5173' has been blocked by CORS policy: Response to preflight request doesn't pass access control check: No 'Access-Control-Allow-Origin' header is present on the requested resource. If an opaque response serves your needs, set the request's mode to 'no-cors' to fetch the resource with CORS disabled.",
      "solution": "The best solution to this problem is to avoid using `onCall` when you have CORS issues. Instead:\n\nCome up with a name for a new Firestore collection.\nSet up a cloud function that is triggered by the creation of a new document in that collection.\nPut the logic that talks to the OpenAI API in the cloud function. When you have the response, write it to a collection in Firestore (wherever you want).\nFrom the client:\n\nWrite to the aforementioned collection when you want to make a request.\nSubscribe to the collection that the cloud function writes to.\n\nAll together, a new request from the client causes this cascade:\n\nA write to the Firestore collection for requests.\nA cloud function read of that document, plus an API call and any other necessary logic.\nA write to a Firestore collection for responses.\nAn update on the client in response to that document update.\n\nThe reason this alternative approach is required is that an `onCall` cloud function has different CORS behavior than a cloud function triggered by a Firestore event.",
      "question_score": 7,
      "answer_score": 3,
      "created_at": "2023-03-04T18:54:28",
      "url": "https://stackoverflow.com/questions/75637545/accessing-chatgpt-api-through-firebase-cloud-function"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 67764536,
      "title": "Can&#39;t use ES modules in Google Cloud Functions",
      "problem": "I'm trying to deploy a very basic Google Cloud serverless application using Node.js, but it keeps showing the following error on Google Cloud Console:\n`Provided module can't be loaded.\nIs there a syntax error in your code?\nDetailed stack trace: Error [ERR_REQUIRE_ESM]: Must use import to load ES Module: /workspace/index.js\nrequire() of ES modules is not supported.\nrequire() of /workspace/index.js from /layers/google.nodejs.functions-framework/functions-framework/node_modules/@google-cloud/functions-framework/build/src/loader.js is an ES module file as it is a .js file whose nearest parent package.json contains \"type\": \"module\" which defines all .js files in that package scope as ES modules.\nInstead rename index.js to end in .cjs, change the requiring code to use import(), or remove \"type\": \"module\" from /workspace/package.json.\n at Object.Module._extensions..js (internal/modules/cjs/loader.js:1080:13)\n at Module.load (internal/modules/cjs/loader.js:928:32)\n at Function.Module._load (internal/modules/cjs/loader.js:769:14)\n at Module.require (internal/modules/cjs/loader.js:952:19)\n at require (internal/modules/cjs/helpers.js:88:18)\n at Object.getUserFunction (/layers/google.nodejs.functions-framework/functions-framework/node_modules/@google-cloud/functions-framework/build/src/loader.js:29:32)\n at Object. (/layers/google.nodejs.functions-framework/functions-framework/node_modules/@google-cloud/functions-framework/build/src/index.js:77:32)\n at Module._compile (internal/modules/cjs/loader.js:1063:30)\n at Object.Module._extensions..js (internal/modules/cjs/loader.js:1092:10)\n at Module.load (internal/modules/cjs/loader.js:928:32)\nCould not load the function, shutting down.\n`\nHere's my index.js file:\n`export function foobar(req, res) {\n    res.status(200).send(\"Hello World!\");\n}\n`\nHere's my package.json file:\n`{\n  ...\n  \"main\": \"index.js\",\n  \"type\": \"module\",\n  ...\n}\n`\nI'm running it using\n`gcloud functions deploy foobar --region europe-west1 --runtime nodejs14 --trigger-http --allow-unauthenticated\n`\nI've already tried the following:\n\nrenaming index.js to index.mjs and change `\"main\": \"index.js\"` to `\"main\": \"index.mjs\"` in package.json (the error persists)\n\nsolution 1 plus removing `\"type\": \"module\"` from package.json (the error persists)\n\nremoving `\"type\": \"module\"` from package.json (raises a SyntaxError: Unexpected token 'export')\n\nrenaming index.js to index.cjs and change `\"main\": \"index.js\"` to `\"main\": \"index.cjs\"` in package.json (raises a SyntaxError: Unexpected token 'export')\n\nsolution 4 plus removing `\"type\": \"module\"` from package.json (raises a SyntaxError: Unexpected token 'export')\n\nadding the `--experimental-modules` flag in `gcloud functions deploy` (shows unrecognized arguments: --experimental-modules)\n\nsolution 6 but replacing `gcloud functions deploy` with `gcloud beta functions deploy` (shows unrecognized arguments: --experimental-modules)\n\nuninstalling Google Cloud SDK completely and trying all above solutions again\n\nThe only solution that worked was solution 3 plus using\n`exports.foobar = (req, res) => {\n    res.status(200).send(\"Hello World!\");\n}\n`\nHowever, I want to use it for a Telegram bot with the `node-telegram-bot-api` module, but because I removed `\"type\": \"module\"` from package.json, it raises \"SyntaxError: Cannot use import statement outside a module\" when I import the Telegram API.\nThe project structure was created using `npm init`, as shown below.\n`.\n\u251c\u2500\u2500 index.js\n\u2514\u2500\u2500 package.json\n`\nAny thoughts? My Node.js version is v14.17.0 and my Google Cloud SDK version is v342.0.0.",
      "solution": "Google Cloud Functions (and therefore Firebase Functions) does not support ESM modules yet:\nhttps://github.com/GoogleCloudPlatform/functions-framework-nodejs/issues/233\nhttps://github.com/firebase/firebase-tools/issues/2994\nNow I think you are conflating using and packaging ES modules. You don't have to package your functions code as an ES module in order to use `node-telegram-bot-api` module. Just import it using `require` syntax instead of using the `import` syntax.",
      "question_score": 7,
      "answer_score": 2,
      "created_at": "2021-05-30T20:25:27",
      "url": "https://stackoverflow.com/questions/67764536/cant-use-es-modules-in-google-cloud-functions"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 68064370,
      "title": "GCP Alert Filters Don&#39;t Affect Open Incidents",
      "problem": "I have an alert that I have configured to send email when the sum of executions of cloud functions that have finished in status other than 'error' or 'ok' is above 0 (grouped by the function name).\nThe way I defined the alert is:\n\nAnd the secondary aggregator is `delta`.\nThe problem is that once the alert is open, it looks like the filters don't matter any more, and the alert stays open because it sees that the cloud function is triggered and finishes with any status (even 'ok' status keeps it open as long as its triggered enough).\nATM the only solution I can think of is to define a log based metric that will count it itself and then the alert will be based on that custom metric instead of on the built in one.\nIs there something that I'm missing?\nEdit:\nAdding another image to show what I think might be the problem:\n\nFrom the image above we see that the graph wont go down to 0 but will stay at 1, which is not the way other normal incidents work",
      "solution": "According to the official documentation:\n\n\"Monitoring automatically closes an incident when it observes that the condition is no longer met or when 7 days have passed without an observation that the condition is still being met.\"\n\nThat made me think that there are times where the condition is not relevant to make it close the incident. Which is confirmed here:\n\n\"If measurements are missing (for example, if there are no HTTP requests for a couple of minutes), the policy uses the last recorded value to evaluate conditions.\"\n\nThe lack of HTTP requests aren't a reason to close the metric as it keeps using the last recorded value (that triggered the metric).\nSo, using alerts for Http Requests is fine but you need to close them by yourself. Although I think it would be better to use a custom metric instead if you want them to be disabled automatically.",
      "question_score": 7,
      "answer_score": 1,
      "created_at": "2021-06-21T10:00:37",
      "url": "https://stackoverflow.com/questions/68064370/gcp-alert-filters-dont-affect-open-incidents"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 71185253,
      "title": "Permission denied to google cloud secret on firebase function deploy",
      "problem": "I have a firebase project with a google cloud function like this:\n`export const myFun = functions.region(\"europe-west1\")\n    .runWith({ timeoutSeconds: 10, secrets: ['MY_SECRET'] })\n    .https.onCall((data, context) => {/*doStuff()*/});\n`\nThe function uses MY_SECRET to access a db. Everything works perfectly fine when I build and deploy this function from my local machine to google cloud. I can access it and i get the results from the db, all good.\nHowever, I setup a github action to deploy this function to the cloud for me. For this i setup a service account as a github secret so I can run `npx firebase-tools deploy` inside the github action. This always worked, UNTIL I added the `secrets: ['MY_SECRET']` to the cloud function.\nLocally I can still sucessfully deploy, but the github action fails:\n```\n`Error: Failed to validate secret versions:\n- FirebaseError HTTP Error: 403, Permission 'secretmanager.versions.get' denied for resource 'projects/my-project/secrets/MY_SECRET/versions/latest' (or it may not exist).\n`\n```\nI made sure the secret actually exists in the correct google cloud project, and the service account I use in github DOES have the role `Secret Manager Secret Accessor `, but I still get the error.\nOne thing I noticed though is that when I go to the secret manager in the browser and click on my secret, I see:\n`Resource ID    projects/123456789/secrets/MY_SECRET`\nand the error says `projects/my-project/secrets/MY_SECRET/versions/latest`\nSo in the build step, the project name is used, and in the secret manager i see the project id. Not sure if this is relevant, just something i noticed...\nWhy does this not work? I tried for hours and am getting desperate, pls help \ud83d\ude05",
      "solution": "...Ok, found the solution after wasting wayyy to much time...\nTurns out the `Secret Manager Secret Accessor` role is not enough, the `Secret Manager Viewer` role is also needed! \ud83e\udd26\u200d\u2642\ufe0f\ud83e\udd26\u200d\u2642\ufe0f\ud83e\udd26\u200d\u2642\ufe0f",
      "question_score": 6,
      "answer_score": 15,
      "created_at": "2022-02-19T14:01:17",
      "url": "https://stackoverflow.com/questions/71185253/permission-denied-to-google-cloud-secret-on-firebase-function-deploy"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 74391040,
      "title": "Firebase functions deploy problem (Failed to create/update function)",
      "problem": "I would like to ask for the help of people familiar with Firebase functions. I am struggling with the problem that uploading the code via firebase cli fails. It was working a few days ago, I didn't change anything in the world, I mean through the configuration. And it gets stuck at a part where not even a code change was made. I have had this problem ever since the client set the editor role to the owner role. But in theory this shouldn't be a problem.\n`firebase deploy --debug` returns this:\n```\n`{\"error\":{\"code\":403,\"message\":\"Unable to retrieve the repository metadata for projects/{projectname}/locations/us-central1/repositories/gcf-artifacts. Ensure that the Cloud Functions service account has 'artifactregistry.repositories.list' and 'artifactregistry.repositories.get' permissions. You can add the permissions by granting the role 'roles/artifactregistry.reader'.\",\"status\":\"PERMISSION_DENIED\"}}\n`\n```\nI set it up but it still doesn't work. Maybe in the wrong place or I don't know. I only encountered similar problems on the net, but none of them helped. I do not know what to do. Artifactregistry api is also enabled.\nfirebase functions:log :\n```\n`2022-11-09T22:15:55.891760Z E friendRequestNotification: {\"@type\":\"type.googleapis.com/google.cloud.audit.AuditLog\",\"status\":{\"code\":7,\"message\":\"Unable to retrieve the repository metadata for projects/{projectname}/locations/us-central1/repositories/gcf-artifacts. Ensure that the Cloud Functions service account has 'artifactregistry.repositories.list' and 'artifactregistry.repositories.get' permissions. You can add the permissions by granting the role 'roles/artifactregistry.reader'.\"},\"authenticationInfo\":{\"principalEmail\":\"{email}\"},\"requestMetadata\":{\"callerIp\":\"{ip}\",\"callerSuppliedUserAgent\":\"FirebaseCLI/11.16.0,gzip(gfe),gzip(gfe)\",\"requestAttributes\":{\"time\":\"2022-11-09T22:15:56.055987Z\",\"auth\":{}},\"destinationAttributes\":{}},\"serviceName\":\"cloudfunctions.googleapis.com\",\"methodName\":\"google.cloud.functions.v1.CloudFunctionsService.CreateFunction\",\"authorizationInfo\":[{\"resource\":\"projects/{projectname}/locations/us-central1/functions/friendRequestNotification\",\"permission\":\"cloudfunctions.functions.create\",\"granted\":true,\"authorizationLoggingOptions\":{\"permissionType\":\"ADMIN_WRITE\"},\"resourceAttributes\":{}}],\"resourceName\":\"projects/{projectname}/locations/us-central1/functions/friendRequestNotification\",\"request\":{\"function\":{\"sourceUploadUrl\":\"https://storage.googleapis.com/uploads-760418412171.us-central1.cloudfunctions.appspot.com/6d1f7217-7899-484f-911c-1dbcb4512d8d.zip?GoogleAccessId=service-{}@gcf-admin-robot.iam.gserviceaccount.com&Expires={}\",\"labels\":{\"deployment-tool\":\"cli-firebase\",\"firebase-functions-hash\":\"{hash}\"},\"runtime\":\"nodejs16\",\"dockerRegistry\":\"ARTIFACT_REGISTRY\",\"entryPoint\":\"friendRequestNotification\",\"name\":\"projects/{projectname}/locations/us-central1/functions/friendRequestNotification\",\"eventTrigger\":{\"eventType\":\"providers/cloud.firestore/eventTypes/document.create\",\"resource\":\"projects/{projectname}/databases/(default)/documents/users/{userId}/friends/{friendId}\"}},\"location\":\"projects/{projectname}/locations/us-central1\",\"@type\":\"type.googleapis.com/google.cloud.functions.v1.CreateFunctionRequest\"},\"resourceLocation\":{\"currentLocations\":[\"us-central1\"]}}\n`\n```\nI have already tried all options within the Google cloud iam&admin settings, but nothing.",
      "solution": "Yeah, right. Got an issue.\n```\n`PATCH https://cloudfunctions.googleapis.com/v1/projects/.../functions/... {\"error\":{\"code\":403,\"message\":\"Unable to retrieve the repository metadata for projects/.../repositories/gcf-artifacts. Ensure that the Cloud Functions service account has 'artifactregistry.repositories.list' and 'artifactregistry.repositories.get' permissions. You can add the permissions by granting the role 'roles/artifactregistry.reader'.\",\"status\":\"PERMISSION_DENIED\"}}\n`\n```\nAfter updating the plan from Spark (Free) to Blaze (Pay as you go) - the following command started working again as expected.\n```\n`    firebase deploy --debug --only functions\n`\n```",
      "question_score": 6,
      "answer_score": 1,
      "created_at": "2022-11-10T15:57:56",
      "url": "https://stackoverflow.com/questions/74391040/firebase-functions-deploy-problem-failed-to-create-update-function"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 69393612,
      "title": "CORS blocking access to resource: How to fix in firebase cloud function?",
      "problem": "Yes, I have the dreaded CORS issue (or, at least it appears so)....and I have searched and tried a few solutions, to no avail...\nI have no problems using firebase emulator and running my function locally, but when I deploy the function and try to send a POST request using fetch() on the local host client-side app, I get the following CORs browser console error (it won't even get to the server logs):\n```\n`Access to fetch at 'https://us-central1-XXXX.cloudfunctions.net/deleteAsset' from origin 'http://localhost:3000' has been blocked by CORS policy: Response to preflight request doesn't pass access control check: No 'Access-Control-Allow-Origin' header is present on the requested resource. If an opaque response serves your needs, set the request's mode to 'no-cors' to fetch the resource with CORS disabled.\n\nThe FetchEvent for \"https://us-central1-XXXXX.cloudfunctions.net/deleteAsset\" resulted in a network error response: the promise was rejected.\n\nPromise.then (async)\n(anonymous) @ firebase-auth-sw.js:77\nfirebase-auth-sw.js:77 \n        \n Uncaught (in promise) TypeError: Failed to fetch\n    at firebase-auth-sw.js:77\n`\n```\nHere's my client side fetch request:\n`UploadImage.vue`:\n```\n`async deleteFile() {\n      await fetch(\n        'https://us-central1-XXXX.cloudfunctions.net/deleteAsset',\n        {\n          method: 'POST',\n          headers: {\n            'Content-Type': 'application/json'\n          },\n          body: JSON.stringify({\n            public_id: this.imageSrc.public_id\n          })\n        }\n      )\n}\n`\n```\nAnd then my firebase cloud function is like so:\n`/deleteAsset.js`:\n```\n`import cloudinary from 'cloudinary'\nimport * as functions from 'firebase-functions'\n\nconst cors = require('cors')({ origin: true })\n\ncloudinary.config({\n  cloud_name: functions.config().cloudinary.cloud_name,\n  api_key: functions.config().cloudinary.api_key,\n  api_secret: functions.config().cloudinary.api_secret,\n  secure: true\n})\n\nexport const deleteAsset = functions.https.onRequest((req, res) => {\n  return cors(req, res, () => {\n    try {\n      functions.logger.log(req.body)\n      cloudinary.v2.uploader.destroy(\n        req.body.public_id,\n        {\n          invalidate: true\n        },\n        (error, result) => {\n          if (error) {\n            res.status(500).send(error)\n          }\n\n          res.status(200).send(result)\n        }\n      )\n    } catch (error) {\n      res.status(500).send('There was an error in deleteAsset function')\n    }\n  })\n})\n`\n```\nAnyone spot any issues or have any advice on how I can further troubleshoot this?",
      "solution": "Alright, so I fixed it....the CORS issue was due to the cloud function not having the right IAM permissions. I suspected it after looking at my dashboard (not shown on firebase console, had to go to Cloud Functions over at Google Cloud to see it!) and noticed that it was missing \"Allow Unauthenticated\". So, I manually deleted the function and redeployed. All good now!",
      "question_score": 6,
      "answer_score": 15,
      "created_at": "2021-09-30T16:03:33",
      "url": "https://stackoverflow.com/questions/69393612/cors-blocking-access-to-resource-how-to-fix-in-firebase-cloud-function"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 74944491,
      "title": "Husky prepare script failing firebase function deployment",
      "problem": "I have installed `husky` in my npm project as a `prepare` script like below\n```\n`{\n  \"name\": \"functions\",\n  \"scripts\": {\n    \"build\": \"tsc\",\n    \"start\": \"npm run serve\",\n    \"deploy\": \"firebase deploy --only functions\",\n    \"prepare\": \"husky install functions/.husky\"\n  }\n  \"dependencies\": {\n    \"firebase-admin\": \"^11.4.1\",\n    \"firebase-functions\": \"^4.1.1\",\n  },\n  \"devDependencies\": {\n    \"husky\": \"^8.0.2\",\n    \"typescript\": \"^4.9.4\"\n  }\n}\n`\n```\n`husky` is declared as `devDependencies` as this npm module is only required while local development and has no need in runtime app.\nSo when I run `npm run deploy`, I get the below error\n```\n`i  functions: updating Node.js 16 function funName(us-central1)...\nBuild failed:\n\n> prepare\n> husky install functions/.husky\n\nsh: 1: husky: not found\nnpm ERR! code 127\nnpm ERR! path /workspace\nnpm ERR! command failed\nnpm ERR! command sh -c -- husky install functions/.husky\n`\n```\nThis error clearly states that `husky` is not installed.\nOne possible solution is to create a `prepare.js` script which checks if the script is running while in local development or in the firebase server(to prepare the project) and then conditionally run the `husky` npm module command",
      "solution": "I just ran into this exact same issue but with `tsc`. I'm not sure why, but the `prepare` script is also run in the cloud function (not just locally) while deploying. However, considering you likely have the `node_modules` directory in the `functions.ignore` list in the firebase.json, the node_modules directory doesn't get uploaded as part of the deployment and so the `husky` package isn't visible to the script when it gets run in the cloud function environment.\nYou likely don't need the husky script to be run in the function environment either way, so you can add a condition to check for an environment variable that is usually set in the function environment (I am using the `GOOGLE_FUNCTION_TARGET` environment variable in my case), and only run the command if that environment is not set. You also need to wrap this in a bash script instead of adding it inline in the package.json because of how the prepare script is run.\nFor example, here's the content of my `scripts/prepare.sh` file.\n`#!/bin/bash\nset -o verbose\n\n# Only run if the GOOGLE_FUNCTION_TARGET is not set\nif [[ -z \"$GOOGLE_FUNCTION_TARGET\" ]]; then\n    npm run build\nfi\n\n`\nThen I use it in my package.json prepare script:\n`// ...\n\"prepare\": \"./scripts/prepare.sh\",\n// ...\n`\nThere's potentially a better solution to this, but this is how I got it to work for me. Hope this helps!",
      "question_score": 6,
      "answer_score": 11,
      "created_at": "2022-12-28T20:56:03",
      "url": "https://stackoverflow.com/questions/74944491/husky-prepare-script-failing-firebase-function-deployment"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 72494678,
      "title": "&#39;An error occurred when trying to authenticate to the FCM servers&#39; on Firebase Cloud Functions",
      "problem": "I'm trying to send a message to a topic with FCM in a Firebase Cloud Function triggered when a Firestore document (a message) is created. Subscribing to the topic (also done with Functions) and triggering the send function works fine, but the actual send fails with:\n```\n`Error: An error occurred when trying to authenticate to the FCM servers. Make sure the credential used to authenticate this SDK has the proper permissions. See https://firebase.google.com/docs/admin/setup for setup instructions.\n`\n```\nand some raw HTML containing `PROJECT_NOT_PERMITTED` and `PROJECT_NOT_PERMITTED `.\nHere is my code (index.ts):\n`import * as admin from 'firebase-admin';\n\nadmin.initializeApp({\n    credential: admin.credential.applicationDefault(),\n});\n\nexport * from './messages';\n`\nand (messages.ts):\n`import * as admin from 'firebase-admin';\nimport * as functions from 'firebase-functions';\n\nexport const publishMessage = functions\n  .firestore.document('/messages/{messageId}').onCreate(\n    (snapshot, context) => {\n      const data = snapshot.data();\n      const message = {\n        notification: {\n          title: `${data.sentBy} sent a message`,\n          body: data.message,\n        },\n      };\n\n      return admin.messaging().sendToTopic('messages', message);\n    },\n  );\n`\nAccording to https://firebase.google.com/docs/cloud-messaging/auth-server#provide-credentials-using-adc this should work. I have also tried doing it without any parameters (https://firebase.google.com/docs/admin/setup#initialize-without-parameters) but it fails all the same. What am I missing?",
      "solution": "Turns out that instead of this\n`      const message = {\n        notification: {\n          title: `${data.sentBy} sent a message`,\n          body: data.message,\n        },\n      };\n\n      return admin.messaging().sendToTopic('messages', message);\n`\nI needed this:\n`      const message = {\n        notification: {\n          title: `${data.sentBy} sent a message`,\n          body: data.message,\n        },\n        topic: 'messages',\n      };\n\n      return admin.messaging().send('messages', message);\n`",
      "question_score": 6,
      "answer_score": 5,
      "created_at": "2022-06-03T22:06:07",
      "url": "https://stackoverflow.com/questions/72494678/an-error-occurred-when-trying-to-authenticate-to-the-fcm-servers-on-firebase-c"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 71495810,
      "title": "Firebase deploy failed even without changing the function from onCallable to onRequest",
      "problem": "Error: [functionName(us-central1)] Changing from a callable function to an HTTPS function is not allowed. Please delete your function and create a new one instead.\n\nAny advice and insight is appreciated.",
      "solution": "Which version of Firebase CLI (`firebase --version`) are you using? Last night I updated `firebase-tools` package to 10.3.0 and functions deployment started giving me the error you mention. I downgraded to 10.2.2 and functions deployment started working as before.\nUpdate:\nFirebase team confirmed there is an issue with 10.3.0 firebase-tools. They are working on a fix:\nhttps://github.com/firebase/firebase-tools/issues/4307",
      "question_score": 6,
      "answer_score": 10,
      "created_at": "2022-03-16T11:53:59",
      "url": "https://stackoverflow.com/questions/71495810/firebase-deploy-failed-even-without-changing-the-function-from-oncallable-to-onr"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 70932654,
      "title": "Firebase Functions - FirebaseError: Missing required options (force) while running in non-interactive mode",
      "problem": "I have a Firebase Function that deletes a user's collection in a Firestore database when their account is deleted.\n```\n`const firebase_tools = require(\"firebase-tools\");\nconst functions = require(\"firebase-functions\");\nconst admin = require(\"firebase-admin\");\nadmin.initializeApp();\n\nexports.deleteUser = functions.auth.user().onDelete((user) => {\n  return firebase_tools.firestore\n      .delete(`users/${user.uid}`, {\n        project: process.env.GCLOUD_PROJECT,\n        token: functions.config().fb.token,\n        recursive: true,\n        yes: true\n      }).catch((error) => {\n        console.log(error);\n        throw new functions.https.HttpsError(\n            \"unknown\",\n            \"Error deleting user's data\"\n        );\n      });\n});\n`\n```\nWhenever a user is deleted and the function is executed, I get the following error in the Functions logs.\n```\n`FirebaseError: Missing required options (force) while running in non-interactive mode\n    at prompt (/workspace/node_modules/firebase-tools/lib/prompt.js:16:15)\n    at promptOnce (/workspace/node_modules/firebase-tools/lib/prompt.js:29:11)\n    at Command.actionFn (/workspace/node_modules/firebase-tools/lib/commands/firestore-delete.js:69:51)\n    at Object.delete (/workspace/node_modules/firebase-tools/lib/command.js:190:25)\n    at processTicksAndRejections (node:internal/process/task_queues:96:5)\n`\n```\nThe only information I could find related to this is regarding deploying/deleting functions to Firebase and there's not much documentation for firebase-tools that I could find.",
      "solution": "I've reproduced the error that you have encountered.\n\nThis error occurs on the latest `\"firebase-tools\": \"^10.1.3\"`.\nBased on the Delete data with a Callable Cloud Function, the documentation have sample code that still uses `\"firebase-tools\": \"9.18.0\"`.\nYou could downgrade your `firebase-tools` by modifying the `package.json`. E.g. below:\n```\n`  \"dependencies\": {\n    \"firebase\": \"^9.6.5\",\n    \"firebase-admin\": \"^9.12.0\",\n    \"firebase-functions\": \"^3.16.0\",\n    \"firebase-tools\": \"9.18.0\"\n  }\n`\n```\nAfter downgrading, I'm able to delete the specified document successfully.\n\nYou could also use what's @Renaud Tarnec answered by using Admin SDK.\nE.g. below:\n```\n`const functions = require(\"firebase-functions\");\nconst admin = require(\"firebase-admin\");\nadmin.initializeApp();\n\ndb = admin.firestore();\n\nexports.deleteUser = functions.auth.user().onDelete((user) => {\n    db.collection(\"users\").doc(user.uid).delete()\n      .then(function(user) {\n        console.log(\"Successfully Deleted User:\", user.uid)\n        })\n      .catch((error) => {\n        console.log(error);\n        throw new functions.https.HttpsError(\n            \"unknown\",\n            \"Error deleting user's data\"\n        );\n      });\n});\n`\n```",
      "question_score": 6,
      "answer_score": 1,
      "created_at": "2022-01-31T21:51:00",
      "url": "https://stackoverflow.com/questions/70932654/firebase-functions-firebaseerror-missing-required-options-force-while-runni"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 67385902,
      "title": "Service Account does not have storage.objects.get access to the Google Cloud Storage object",
      "problem": "I have set up a new Firestore environment, and when attempting to upload and then read data from storage via cloud functions, I get the following error:\n`project_id@appspot.gserviceaccount.com does not have storage.objects.get access to the Google Cloud Storage object`.\nI have looked online and within the Google cloud console I have added the IAM permission of `Storage Admin` to this service account, but I am still getting this same error.\nHas anyone had this issue before? I have also tried just giving `Owner` role to the service account, but still get the same error.",
      "solution": "Storage Admin role  has storage.objects.* permissions and thus should be more than enough.\nI recommend you to try the following options:\n\nCheck whether Cloud Storage API is enabled or not. If not, enable API.\n\nRemove the Storage Admin role from your service account and then add it again. Make sure to wait for a couple of minutes so that the roles could propagate properly.\n\nIf the above option doesn't work for you, try giving the service account the Storage Object Viewer role separately",
      "question_score": 6,
      "answer_score": 9,
      "created_at": "2021-05-04T15:35:49",
      "url": "https://stackoverflow.com/questions/67385902/service-account-does-not-have-storage-objects-get-access-to-the-google-cloud-sto"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 67158368,
      "title": "Deploy Firebase Functions on node 14 runtime with increased Memory",
      "problem": "We're currently using Firebase Functions with the node 14 public preview.\nWe need to increase the memory of a function past 1Gb.\nThe documentation of Google Cloud functions specifies that the max_old_space_size must be set for newest runtime, and the documentation shows  :\n```\n`gcloud functions deploy envVarMemory \\\n--runtime nodejs12 \\\n--set-env-vars NODE_OPTIONS=\"--max_old_space_size=8Gi\" \\\n--memory 8Gi \\\n--trigger-http\n`\n```\nHowever, the `set-env-vars` options does not exist in the `firebase deploy`\nUsing\n`firebase deploy --only functions:myFunction --set-env-vars NODE_OPTIONS=\"--max_old_space_size=4Gi\"` Yields the `error: unknown option '--set-env-vars'` error.\nWhile deploying a heavy function, i logically get a heap out of memory error :\n```\n`[1:0x29c51e07b7a0]   120101 ms: Mark-sweep (reduce) 1017.1 (1028.5) -> 1016.2 (1028.7) MB, 928.7 / 0.1 ms  (average mu = 0.207, current mu = 0.209) allocation failure scavenge might not succeed \n[1:0x29c51e07b7a0]   119169 ms: Scavenge (reduce) 1016.9 (1025.2) -> 1016.2 (1026.5) MB, 3.6 / 0.0 ms  (average mu = 0.205, current mu = 0.191) allocation failure  \n`\n```\nAnd we can see the function only has 1028Mb of ram, not 4.\nWe did ask it to deploy with 4Gb:\n```\n`functions\n    .runWith({ memory: '4GB', timeoutSeconds: 300 ,})\n`\n```\nWhat is the key here?",
      "solution": "We had exactly the same issue. It seems to happen when deploying function with node 12 or more.\nHere is the solution to solve this:\n\nFind your function on GCP web app\nClick on \"edit\"\nScroll down and find \"Runtime environment variables\"\nAdd key NODE_OPTIONS with value : --max_old_space_size=4096\n\nHere is a picture of the setting :\n\nThis is really annoying and I did not find any solution to set this setting while deploying in command line.",
      "question_score": 6,
      "answer_score": 12,
      "created_at": "2021-04-19T10:19:41",
      "url": "https://stackoverflow.com/questions/67158368/deploy-firebase-functions-on-node-14-runtime-with-increased-memory"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 67079859,
      "title": "Firebase pubsub function schedule invalid schedule or timezone",
      "problem": "When I try to deploy my newly created Firebase PubSub function, I get the error `Error: Failed to create scheduler job projects/workouts-uat/locations/europe-west1/jobs/firebase-schedule-dailyTasks-eur3: HTTP Error: 400, Schedule or time zone is invalid.`\nMy function is declared as follows:\n```\n`exports.dailyTasks = functions\n  .region('europe-west2')\n  .pubsub\n  .schedule('every 24 hours 00:00')\n  .timeZone('Africa/Johannesburg')\n  .onRun(...);\n`\n```\nFrom my research I found that that the region used should be the same as the region found in your project settings, which in my case is `eur3 (europe-west)`. All my existing `onCall` functions use `europe-west2` which is why I tried to go with that first, but after finding my project settings region I updated to `.region('eur3')` and `.region('europe-west')`, but the error persists. So how do I successfully deploy this function?\nAnother hint I came across while googling the error was that a \"Google App Engine (GAE) app\" has to be created for pubsub functions to work, but I'm assuming that happens automatically on function deploy, right? Otherwise how does one create that? I have zero experience with GCP.",
      "solution": "I finally figured this out. It had nothing to do with the `.timeZone()`, as I originally thought - it was an error with the `.schedule()`. And yes, I just saw that the error message does include the possibility that it could be the \"Schedule or time zone\", but for some reason I thought it was referring to the function as a whole and not the `.schedule()` call.\nIn any case, once I updated my `every 24 hours 00:00` to `every day 00:00`, it all of a sudden deployed just fine. For clarity, this is what my working function signature now looks like:\n```\n`exports.dailyTasks = functions\n.region('europe-west2')\n.pubsub\n.schedule('every day 00:00')\n.timeZone('Africa/Johannesburg')\n`\n```\nAs it turns out, `.region()` also had nothing to do with it, and `europe-west2` works just fine.\nI'd like to thank the author of this article, because he linked an example project that also calls a pubsub function every day at midnight, and without said article, I'd have never found the answer.",
      "question_score": 6,
      "answer_score": 10,
      "created_at": "2021-04-13T19:42:38",
      "url": "https://stackoverflow.com/questions/67079859/firebase-pubsub-function-schedule-invalid-schedule-or-timezone"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 74174146,
      "title": "lib/logger/compat is not defined",
      "problem": "After updating my Firebase dependencies (need new features) i've been trying to deploy them without success.\n```\n`firebase deploy --only functions\n`\n```\nBefore the update everything worked fine. Here is the Firebase documentation page for logging.\nhttps://firebase.google.com/docs/functions/writing-and-viewing-logs#custom-logs\n```\n`require(\"firebase-functions/lib/logger/compat\");\n`\n```\nNow the logging fails and i cant understand why.\n```\n`Error: Failed to load function definition from source: Failed to generate manifest from function source: Error [ERR_PACKAGE_PATH_NOT_EXPORTED]: Package subpath './lib/logger/compat' is not defined by \"exports\" in /Users/admin/Desktop/Xxxxxxx Xcode/functions/node_modules/firebase-functions/package.json\n`\n```\nI've tried to delete and reinstall. Current versions are now.\n```\n`Node = v16.18.0  \nfirebase tools = 11.15.0\n`\n```\nWhat could i be missing?",
      "solution": "I got it.\nmaybe you use `firebase-functions@v4.0.1(latest)` now.\nfrom version 4, we can use\n```\n`require(\"firebase-functions/logger/compat\");\n`\n```\nEdit like this, it worked.",
      "question_score": 6,
      "answer_score": 7,
      "created_at": "2022-10-23T21:25:37",
      "url": "https://stackoverflow.com/questions/74174146/lib-logger-compat-is-not-defined"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 76667175,
      "title": "How to use Secret Manager with 2nd gen Cloud Firestore Triggers?",
      "problem": "I am migrating a 1st generation Cloud Firestore Trigger to the second generation.\nHowever, I cannot figure out how to access Google's Secret Manager from within the second generation Cloud Trigger.\nDocumentation exists for accessing secrets within second generation cloud functions by making use of a `defineSecret` utility which is passed into the function's dependency array. However, this approach does not work with the second generation cloud trigger as there is no options parameter to pass the dependency array.\nTo explain with a snippet what I am trying to do:\n`import { onDocumentCreated } from 'firebase-functions/v2/firestore';\nimport { defineSecret } from 'firebase-functions/params';\n\nconst apiKey = defineSecret('API_KEY');\n\nconst onUserCreated = onDocumentCreated(\n  'users/{userId}',\n  async (event) => {\n    // \ud83d\udc49 access apiKey secret \ud83d\udc48\n  }\n);\n`\nAny help would be greatly appreciated. Thank you.",
      "solution": "You can pass a DocumentOptions object as the first parameter, which is basically an extension of EventHandlerOptions to set the secrets with the below code:\n`import * as admin from \"firebase-admin\";\nadmin.initializeApp();\nimport { onDocumentWritten } from \"firebase-functions/v2/firestore\";\nimport { defineSecret } from \"firebase-functions/params\";\n\nconst discordApiKey = defineSecret(\"DISCORD_API_KEY\");\n\nexport const writetofirestore = onDocumentWritten({\n  document: \"users/{userId}\",\n  secrets: [discordApiKey] // you can provide secrets like this \n}, (event) => { \n  const apiKey = discordApiKey.value(); // use secret like this\n});\n`\nReference : firestore.onDocumentWritten()",
      "question_score": 6,
      "answer_score": 9,
      "created_at": "2023-07-12T06:34:20",
      "url": "https://stackoverflow.com/questions/76667175/how-to-use-secret-manager-with-2nd-gen-cloud-firestore-triggers"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 71708885,
      "title": "Cannot use import statement outside a module, TypeScript NodeJS Firebase Functions project",
      "problem": "I recently set up a Firebase Functions project using TypeScript. As far as I can tell I am using the same setup I have before in the past. This time, however, when I run `firebase emulators:start` I receive the following error:\n```\n`Error: Error occurred while parsing your function triggers.\n\n/firebase/functions/src/index.ts:3\nimport functions = require(\"firebase-functions\");\n^^^^^^\n\nSyntaxError: Cannot use import statement outside a module\n    at Object.compileFunction (node:vm:352:18)\n    at wrapSafe (node:internal/modules/cjs/loader:1031:15)\n    at Module._compile (node:internal/modules/cjs/loader:1065:27)\n    at Object.Module._extensions..js (node:internal/modules/cjs/loader:1153:10)\n    at Module.load (node:internal/modules/cjs/loader:981:32)\n    at Function.Module._load (node:internal/modules/cjs/loader:822:12)\n    at Module.require (node:internal/modules/cjs/loader:1005:19)\n    at require (node:internal/modules/cjs/helpers:102:18)\n    at loadModule (/opt/homebrew/lib/node_modules/firebase-tools/lib/deploy/functions/runtimes/node/triggerParser.js:10:16)\n    at /opt/homebrew/lib/node_modules/firebase-tools/lib/deploy/functions/runtimes/node/triggerParser.js:34:21\n`\n```\nAs far as I can tell I have not changed anything from past projects, where I did not come across this issue. The only potential difference I can think of is that I updated my firebase tools using `npm install -g firebase-tools@latest`. Below are some of the files from my project:\nsrc/index.ts\n```\n`/* eslint-disable max-len */\nimport functions = require(\"firebase-functions\");\nimport admin = require(\"firebase-admin\");\n\nadmin.initializeApp();\n\n...\n`\n```\n.eslintrc.js\n```\n`module.exports = {\n  root: true,\n  env: {\n    es6: true,\n    node: true,\n  },\n  extends: [\n    \"eslint:recommended\",\n    \"plugin:import/errors\",\n    \"plugin:import/warnings\",\n    \"plugin:import/typescript\",\n    \"google\",\n    \"plugin:@typescript-eslint/recommended\",\n  ],\n  parser: \"@typescript-eslint/parser\",\n  parserOptions: {\n    project: [\"tsconfig.json\", \"tsconfig.dev.json\"],\n    sourceType: \"module\",\n    tsconfigRootDir: __dirname,\n  },\n  ignorePatterns: [\n    \"/lib/**/*\", // Ignore built files.\n  ],\n  plugins: [\n    \"@typescript-eslint\",\n    \"import\",\n  ],\n  rules: {\n    \"indent\": [\"error\", 2],\n    \"object-curly-spacing\": [\"error\", \"always\"],\n    \"quotes\": [\"error\", \"double\"],\n    \"import/no-unresolved\": 0,\n  },\n};\n`\n```\npackage.json\n```\n`{\n  \"name\": \"functions\",\n  \"scripts\": {\n    \"lint\": \"eslint --ext .js,.ts .\",\n    \"build\": \"tsc\",\n    \"serve\": \"npm run build && firebase emulators:start --only functions\",\n    \"shell\": \"npm run build && firebase functions:shell\",\n    \"start\": \"npm run shell\",\n    \"deploy\": \"firebase deploy --only functions\",\n    \"logs\": \"firebase functions:log\"\n  },\n  \"engines\": {\n    \"node\": \"16\"\n  },\n  \"main\": \"./src/index.ts\",\n  \"dependencies\": {\n    \"firebase-admin\": \"^10.0.2\",\n    \"firebase-functions\": \"^3.18.0\"\n  },\n  \"devDependencies\": {\n    \"@typescript-eslint/eslint-plugin\": \"^5.12.0\",\n    \"@typescript-eslint/parser\": \"^5.12.0\",\n    \"eslint\": \"^8.9.0\",\n    \"eslint-config-google\": \"^0.14.0\",\n    \"eslint-plugin-import\": \"^2.25.4\",\n    \"firebase-functions-test\": \"^0.2.0\",\n    \"typescript\": \"^4.5.4\"\n  },\n  \"private\": true,\n  \"type\": \"module\",\n  \"module\": \"ES2020\"\n}\n`\n```\npackage.dev.json\n```\n`{\n  \"include\": [\n    \".eslintrc.js\"\n  ]\n}\n`\n```\ntsconfig.json\n```\n`{\n  \"compilerOptions\": {\n    \"module\": \"ES2020\",\n    \"noImplicitReturns\": true,\n    \"noUnusedLocals\": true,\n    \"outDir\": \"lib\",\n    \"sourceMap\": true,\n    \"strict\": true,\n    \"target\": \"ES2020\"\n  },\n  \"compileOnSave\": true,\n  \"include\": [\n    \"src\"\n  ]\n}\n`\n```\nfirebase.json\n```\n`{\n  ...\n  \"functions\": {\n    \"predeploy\": [\n      \"npm --prefix \\\"$RESOURCE_DIR\\\" run lint\",\n      \"npm --prefix \\\"$RESOURCE_DIR\\\" run build\"\n    ]\n  },\n  ...\n}\n`\n```\nMy `src/index.ts` import syntax matches what Google outlined in the Firestore documentation: Import the required modules and initialize an app.\nI have searched around for an answer but I have not come across one yet. Here are a few of the resources I have tried:\nSyntaxError: Cannot use import statement outside a module Firebase Functions\nNode.js v13.14.0 Documentation\nTypescript: Cannot use import statement outside a module\nThanks in advance for the help!",
      "solution": "Turns out the problem has a simple solution! The firebase emulators do not support typescript. To run functions in the emulator you first need to compile to JS (reference the docs). Both `npm run serve` and `firebase deploy` will automatically transpile your code, `firebase emulators:start` does NOT do this automatically. When using the emulators you must first run `npm run build` in your functions folder.\nAlso note, in your package.json you need your `main` argument to reference the compiled JS code. I needed to update both my `package.json` and `tsconfig.json`. Below are the final versions:\npackage.json\n```\n`{\n  \"name\": \"functions\",\n  \"scripts\": {\n    \"lint\": \"eslint --ext .js,.ts .\",\n    \"build\": \"tsc\",\n    \"serve\": \"npm run build && firebase emulators:start --only functions\",\n    \"shell\": \"npm run build && firebase functions:shell\",\n    \"start\": \"npm run shell\",\n    \"deploy\": \"firebase deploy --only functions\",\n    \"logs\": \"firebase functions:log\"\n  },\n  \"engines\": {\n    \"node\": \"16\"\n  },\n  \"main\": \"./lib/index.js\",\n  \"dependencies\": {\n    \"firebase-admin\": \"^10.0.2\",\n    \"firebase-functions\": \"^3.18.0\"\n  },\n  \"devDependencies\": {\n    \"@typescript-eslint/eslint-plugin\": \"^5.12.0\",\n    \"@typescript-eslint/parser\": \"^5.12.0\",\n    \"eslint\": \"^8.9.0\",\n    \"eslint-config-google\": \"^0.14.0\",\n    \"eslint-plugin-import\": \"^2.25.4\",\n    \"firebase-functions-test\": \"^0.2.0\",\n    \"typescript\": \"^4.5.4\"\n  },\n  \"private\": true\n}\n`\n```\ntsconfig.json\n```\n`{\n  \"compilerOptions\": {\n    \"module\": \"commonjs\",\n    \"noImplicitReturns\": true,\n    \"noUnusedLocals\": true,\n    \"outDir\": \"lib\",\n    \"sourceMap\": true,\n    \"strict\": true,\n    \"target\": \"es2017\"\n  },\n  \"compileOnSave\": true,\n  \"include\": [\n    \"src\"\n  ]\n}\n`\n```",
      "question_score": 6,
      "answer_score": 8,
      "created_at": "2022-04-01T16:54:10",
      "url": "https://stackoverflow.com/questions/71708885/cannot-use-import-statement-outside-a-module-typescript-nodejs-firebase-functio"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 69131840,
      "title": "How to invoke a cloud function from google cloud composer?",
      "problem": "For a requirement I want to call/invoke a cloud function from inside a cloud composer pipeline but I cant find much info on it, I tried using SimpleHTTP airflow operator but I get this error:\n```\n`[2021-09-10 10:35:46,649] {taskinstance.py:1503} ERROR - Task failed with exception\nTraceback (most recent call last):\nFile \"/opt/python3.8/lib/python3.8/site-packages/airflow/models/taskinstance.py\", line 1158, in \n_run_raw_task\nself._prepare_and_execute_task_with_callbacks(context, task)\nFile \"/opt/python3.8/lib/python3.8/site-packages/airflow/models/taskinstance.py\", line 1333, in \n_prepare_and_execute_task_with_callbacks\nresult = self._execute_task(context, task_copy)\nFile \"/opt/python3.8/lib/python3.8/site-packages/airflow/models/taskinstance.py\", line 1363, in \n_execute_task\nresult = task_copy.execute(context=context)\nFile \"/home/airflow/gcs/dags/to_gcf.py\", line 51, in execute\nif not self.response_check(response):\nFile \"/home/airflow/gcs/dags/to_gcf.py\", line 83, in \nresponse_check=lambda response: False if len(response.json()) == 0 else True,\nFile \"/opt/python3.8/lib/python3.8/site-packages/requests/models.py\", line 900, in json\nreturn complexjson.loads(self.text, **kwargs)\nFile \"/opt/python3.8/lib/python3.8/json/__init__.py\", line 357, in loads\nreturn _default_decoder.decode(s)\nFile \"/opt/python3.8/lib/python3.8/json/decoder.py\", line 337, in decode\nobj, end = self.raw_decode(s, idx=_w(s, 0).end())\nFile \"/opt/python3.8/lib/python3.8/json/decoder.py\", line 355, in raw_decode\nraise JSONDecodeError(\"Expecting value\", s, err.value) from None\n`\n```\nThanks in advance!!",
      "solution": "I faced the same issue as you, but I managed to figure it out by studying the Airflow 2.0 provider packages for Google and using a PythonOperator instead.\n```\n`from airflow.providers.google.common.utils import id_token_credentials as id_token_credential_utils\nimport google.auth.transport.requests\nfrom google.auth.transport.requests import AuthorizedSession\n\ndef invoke_cloud_function():\n\n  url = \"\" #the url is also the target audience. \n  request = google.auth.transport.requests.Request()  #this is a request for obtaining the the credentials\n  id_token_credentials = id_token_credential_utils.get_default_id_token_credentials(url, request=request) # If your cloud function url has query parameters, remove them before passing to the audience \n\n  resp = AuthorizedSession(id_token_credentials).request(\"GET\", url=url) # the authorized session object is used to access the Cloud Function\n\n  print(resp.status_code) # should return 200\n  print(resp.content) # the body of the HTTP response\n\n`\n```\nThus, invoke the function as below:\n```\n`    task = PythonOperator(task_id=\"invoke_cf\", python_callable=invoke_cloud_function)\n`\n```\nFrom my understanding, accessing an authenticated HTTP Cloud Function strictly requires a credential based on ID Tokens. Thus to obtain the required type of credentials, `get_default_id_token_credentials()` executes the Application Default Credentials(ADC) authorization flow, which is a process that obtains credentials from environment variables, known locations. or the Compute Engine metadata server. Composer should have the associated service account keyfile made avaliable via environment variables (probably `GOOGLE_APPLICATION_CREDENTIALS`).\nOnce you have the right type of credentials, you can use the AuthorizedSessions object to authenticate your requests to the cloud function.",
      "question_score": 6,
      "answer_score": 7,
      "created_at": "2021-09-10T13:55:47",
      "url": "https://stackoverflow.com/questions/69131840/how-to-invoke-a-cloud-function-from-google-cloud-composer"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 77896808,
      "title": "Firebase onCall functions and Google Cloud Functions authentication showing call is unauthenticated",
      "problem": "A few months ago I deployed some onCall Firebase functions, and they worked fine.\nToday I deployed a new onCall Firebase function, and I cannot call it, I get the following errors:\nIn Google Cloud Functions logs:\n\nThe request was not authenticated. Either allow unauthenticated invocations or set the proper Authorization header. Read more at https://cloud.google.com/run/docs/securing/authenticating Additional troubleshooting documentation can be found at: https://cloud.google.com/run/docs/troubleshooting#unauthorized-client\n\nOn the client (web browser):\n\nAccess to fetch at 'https://asia-northeast1-myprojectname.cloudfunctions.net/myfunctionname' from origin 'http://localhost:5173' has been blocked by CORS policy: Response to preflight request doesn't pass access control check: No 'Access-Control-Allow-Origin' header is present on the requested resource. If an opaque response serves your needs, set the request's mode to 'no-cors' to fetch the resource with CORS disabled.\n\nI suspect the CORS error is just a consequence of the authentication failure though, and not directly a due to CORS settings.\nInvestigating this further, although I see no difference between the old and new functions in the Firebase console, going to the Google Cloud Functions Console I can see a difference between the old and new functions:\n\nIn the Authentication column, the old ones were set to \"Allow unauthenticated\" while the new ones are set to \"Require authentication\".\nI do not remember ever changing the setting on the old ones, so I'm not sure why the new ones have a different setting. However, my understanding was that one of the purposes of the `onCall` functions (as opposed to `onRequest`) is they handle authentication automatically. I can see the user's auth data in `request.auth`, so it is clearly being provided. In that case, why I am getting the error saying the request was not authenticated? How can I get the client to authenticate itself properly when calling the cloud function?\nTo be clear, the user is already logged in/authenticated with Firebase authentication before the function call is made.",
      "solution": "That authentication column in the console has nothing to do with the fact that it's a callable function and also has nothing to do with Firebase Authentication.  It's referring to something entirely different, which you will find out if you click the provided link in the error message.  It has to do with GCP IAM authentication.\nThe other functions that are set to \"allow unauthenticated\" are indeed correctly set.  Callable functions need to have that IAM setting so that they work from web and mobile applications.  If you want to fix this right now, change the setting allow unauthenticated access like your older functions.  This involves adding the \"allUsers\" role to the function.  Read more in the GCP documentation: https://cloud.google.com/run/docs/authenticating/public\nThe Firebase CLI should set this automatically on deployment.  If it's not adding allUsers, then something is wrong in the system, and you should reach out to Firebase support to report this as a problem with the Firebase CLI.  Or file an issue on firebase-tools.\nSee also:\n\nDoes \"allUsers\" in Cloud Functions allow only the users of my app?\nCloud Function Error: Forbidden unless I open function to allUsers\nConfiguring Correct Permissions in Firebase Cloud Function to Avoid CORS Error\nNewly Created Firebase Functions Throwing UNAUTHENTICATED Error",
      "question_score": 6,
      "answer_score": 7,
      "created_at": "2024-01-29T00:37:14",
      "url": "https://stackoverflow.com/questions/77896808/firebase-oncall-functions-and-google-cloud-functions-authentication-showing-call"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 71021490,
      "title": "Flutter - Firebase callable functions with region not working",
      "problem": "we currently facing the problem in our flutter app, that httpsCallable functions which are defined with a region, in our case \"europe-west1\", is throwing an exception:\n```\n`[ERROR:flutter/lib/ui/ui_dart_state.cc(209)] Unhandled Exception: [firebase_functions/internal] Response is not valid JSON object.\n\n#0      StandardMethodCodec.decodeEnvelope (package:flutter/src/services/message_codecs.dart:607:7)\n#1      MethodChannel._invokeMethod (package:flutter/src/services/platform_channel.dart:167:18)\n\n#2      MethodChannelHttpsCallable.call (package:cloud_functions_platform_interface/src/method_channel/method_channel_https_callable.dart:23:24)\n\n#3      HttpsCallable.call (package:cloud_functions/src/https_callable.dart:35:37)\n\n#0      MethodChannelHttpsCallable.call (package:cloud_functions_platform_interface/src/method_channel/method_channel_https_callable.dart:39:7)\n\n#1      HttpsCallable.call (package:cloud_functions/src/https_callable.dart:35:37)\n\n`\n```\nAs far, as we tested, it makes no difference, if we define the region in flutter accordingly to docs:\n```\n`final result = await FirebaseFunctions.instanceFor(region: 'europe-west1').httpsCallable('myCallableFunction').call();\n`\n```\nOr directly in the cloud function:\n```\n`exports.myCallableFunction = functions.region(\"europe-west1\").https.onCall((data, context) => {\n   // ... return result\n});\n`\n```\nIf we remove the region on both lines above, the callable function is working and returning the expected result.\nIs there something, what I miss or is there currently an issue in flutter itself?",
      "solution": "After some testing and the comment from Peter Koltai, the solution is defining the region in both, flutter and cloud functions:\nflutter:\n```\n`final result = await FirebaseFunctions.instanceFor(region: 'europe-west1').httpsCallable('myCallableFunction').call();\n`\n```\ncloud functions:\n```\n`exports.myCallableFunction = functions.region(\"europe-west1\").https.onCall((data, context) => {\n   // ... return result\n});\n`\n```\nOf course, this makes sense, because as the caller, I want to call the callable functions out of the \"europe-west1\" region. This gives, if required, the flexibility to call different functions for different regions.",
      "question_score": 6,
      "answer_score": 8,
      "created_at": "2022-02-07T16:54:26",
      "url": "https://stackoverflow.com/questions/71021490/flutter-firebase-callable-functions-with-region-not-working"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 72322523,
      "title": "Importing AuthError in TypeScript using Firebase",
      "problem": "Is there a way I can check if an error is of type \"AuthError\" in TypeScript when using Firebase.\nI have an Https Callable function that has a try/catch block that includes the following:\n```\n`try{\n    await admin.auth().getUser(data.uid); // will throw error if user does not exist\n    await admin.auth().deleteUser(data.uid);\n} catch (error) {\n    if(error instanceof AuthError) { // error here\n        if (error.code === \"auth/user-not-found\") {\n            logger.error(`Error: auth/user-not-found, Given user ID does not exist in Firebase Authentication`);\n            throw new https.HttpsError(\"not-found\", \"Given user ID does not exist in Firebase Authentication\");\n        }\n    }\n}\n`\n```\nBut I will get an error in my IDE at the if statement:\n\n'AuthError' only refers to a type, but is being used as a value here.ts(2693)\n\nI am using `import { AuthError } from \"firebase/auth\";` to import AuthError.\nIs there a way to check if the error is an instance of AuthError? Are my imports correct? I could find no helpful information in the documentation\nThank you",
      "solution": "The API docs you linked to are for the node.js client SDK.  That's not the same as the Firebase Admin SDK for node.js that you're code is calling into.  The admin SDK just doesn't throw that AuthError, nor does it declare anywhere the type of the error object, even in its own API docs for getUser.\nIf you look at the Admin SDK documentation, it says:\n\nIf the provided email does not belong to an existing user or the user cannot be fetched for any other reason, the Admin SDK throws an error. For a full list of error codes, including descriptions and resolution steps, see Admin Authentication API Errors.\n\nIf you dig further into the source code, you'll see that it actually throws a FirebaseAuthError object, which doesn't appear in the public API documentation.  So, it appears you're on your own if you want a typesafe error.  But you can always just reach into the properties of that object to find out more.",
      "question_score": 6,
      "answer_score": 6,
      "created_at": "2022-05-20T19:09:34",
      "url": "https://stackoverflow.com/questions/72322523/importing-autherror-in-typescript-using-firebase"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 75498837,
      "title": "How to track when Firebase Functions Task Queues have exhausted all retries?",
      "problem": "I am using Firebase Functions Task Queues to call numerous API endpoints.\nEach function is set to retry 5 times if it encounters an error.\nNow I want to track if the function completes successfully or fails completely (i.e. all retries are exhausted and the function still throws an error).\nI want to track this so that if it fails completely I can update a Firestore document or send an alert of some kind.\nFor example, here is a Task Queue function, how do I add the above functionality?\n`export const someTask = functions.tasks\n  .taskQueue({\n    retryConfig: {\n      maxAttempts: 5,\n      minBackoffSeconds: 60,\n    },\n    rateLimits: {\n      maxConcurrentDispatches: 1,\n    },\n  })\n  .onDispatch(\n    async () => {\n      try {\n        // Call the API\n        await apiCall();\n        return;\n      } catch (error) {\n        // Throw error so that the Task Queue will retry\n        throw new functions.https.HttpsError(\n          'unknown',\n          'someTask error'\n        );\n      }\n    }\n  );\n`",
      "solution": "This cannot be done natively with because the `context` argument provided by `onDispatch` only holds authentication details  :(\nSo we have to track it outside the function.\nThis example updates a Firestore document for each retry, success, or complete failure. There is a Firstore collection named `functionStatus` and the ID of each document is equal to the name of the function you want to track. In this example the document ID would be `someTask`. This naming convention makes it easier to update the status.\n`import * as functions from 'firebase-functions';\nimport * as admin from 'firebase-admin';\n\nconst increment = admin.firestore.FieldValue.increment;\n\nexport const someTask = functions.tasks\n  .taskQueue({\n    retryConfig: {\n      maxAttempts: 5,\n      minBackoffSeconds: 60,\n    },\n    rateLimits: {\n      maxConcurrentDispatches: 1,\n    },\n  })\n  .onDispatch(async () => {\n    try {\n      // Call the API\n      await apiCall();\n    } catch (error) {\n      // Get the functionStatus someTask doc\n      const someTaskDoc = await admin\n        .firestore()\n        .collection('functionStatus')\n        .doc('someTask')\n        .get();\n\n      if (someTaskDoc.data()?.retries",
      "question_score": 6,
      "answer_score": 5,
      "created_at": "2023-02-19T09:21:57",
      "url": "https://stackoverflow.com/questions/75498837/how-to-track-when-firebase-functions-task-queues-have-exhausted-all-retries"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 76706249,
      "title": "Firebase Cloud Functions auth onCreate &quot;Cannot read properties of undefined (reading &#39;auth&#39;)&quot;",
      "problem": "I'm using cloud functions V2 with Node.js (not Typescript) and I need user creation trigger. In docs I've found something, but it doesn't work. By the link I also can see the following hint there:\n\nCloud Functions for Firebase (2nd gen) does not provide support for\nthe events and triggers described in this guide.\n\nAuth triggers doesn't work anymore for V2 cloud functions? If so, what is an alternative for this functionality? Below my code and deploy error:\n```\n`const {functions} = require(\"firebase-functions\");\ninitializeApp();\nconst db = getFirestore();\n\nexports.authUserCreated = functions.auth.user().onCreate({region: \"europe-west1\"}, (user) => {\n  const userEmail = user.email;\n  const userId = user.uid;\n  return db.collection(\"users\").doc(userId).update({createdAt: FieldValue.serverTimestamp(), email: userEmail});\n});\n`\n```\nDeploy error: `TypeError: Cannot read properties of undefined (reading 'auth')`",
      "solution": "Your `require` statement is wrong.  It should be:\n```\n`const functions = require('firebase-functions');\n`\n```",
      "question_score": 6,
      "answer_score": 2,
      "created_at": "2023-07-17T18:01:00",
      "url": "https://stackoverflow.com/questions/76706249/firebase-cloud-functions-auth-oncreate-cannot-read-properties-of-undefined-rea"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 75779120,
      "title": "Firebase ID token has incorrect &quot;aud&quot; (audience) claim when authenticating Firebase function to function calls",
      "problem": "I'm attempting to have one GCP Firebase Cloud Function call another.  I am using https://cloud.google.com/functions/docs/securing/authenticating#authenticating_function_to_function_calls as the guide.  I have everything working but when the function is invoked, it throws the following error:\n```\n`Failed to validate auth token. FirebaseAuthError: Firebase ID token has incorrect \"aud\" (audience) claim. Expected \"{projectId}\" but got \"https://{project-region}-{projectId}.cloudfunctions.net/{functionName}\". Make sure the ID token comes from the same Firebase project as the service account used to authenticate this SDK.\n`\n```\nI attempted to set the `targetAudience` to `{projectId}`, but then `auth.getIdTokenClient(targetAudience);` failed with a `401 Unauthorized` response.\nThe called/invoked function is using `functions.https.onCall` to authenticate the request.  If I switch it to `functions.https.onRequest`, it works, but I don't know how to validate the request and I think that's a pretty poor workaround anyway as it should be working with the `onCall` method.\nFor the `functions.https.onRequest` method, it passes through a Google Auth signed JWT Authorization header, but `const decodedToken = await admin.auth().verifyIdToken(req.headers.authorization ?? '');` (source) fails with the error:\n```\n`Error: Decoding Firebase ID token failed. Make sure you passed the entire string JWT which represents an ID token.\n`\n```",
      "solution": "I needed to use Google Auth's `OAuth2Client.verifyIdToken`.  This is not well documented.  I had to find the solution in a sample code file; even then, it wasn't clear how you should verify the token's payload (their example verification method seemed rather weak).  So here is my example of handling the full request:\n`\nimport { OAuth2Client } from 'google-auth-library';\n\nexport const exampleFunction = functions.https.onRequest(async (req, res) => {\n  // Note that I couldn't find a way to get the function name from the request object. :(\n  const functionName = 'exampleFunction';\n  // Note that you may have a different service account email if your Cloud Function \n  // is managed by a different account than the default.\n  const expectedServiceAccountEmail = `your-project-id@appspot.gserviceaccount.com`;\n  const parts = req.headers.authorization?.split(' ');\n  if (!parts || parts.length !== 2 || parts[0] !== 'Bearer' || !parts[1]) {\n    console.error('Bad header format: Authorization header not formated as \\'Bearer [token]\\'', req.headers);\n    throw new functions.https.HttpsError('unauthenticated', 'user not authenticated');\n  }\n  try {\n  const audience = `${req.protocol}://${req.hostname}/${functionName}`;\n  const googleOAuth2Client = new OAuth2Client();\n    const decodedToken = await googleOAuth2Client.verifyIdToken({\n      idToken: parts[1],\n      audience,\n    });\n    const payload = decodedToken.getPayload();\n    if (!payload) {\n      console.error('unpexpected state; missing payload', decodedToken);\n      throw new Error('no payload');\n    }\n    if (payload.aud !== audience) {\n      console.error('bad audience', payload);\n      throw new functions.https.HttpsError('permission-denied', 'bad audience');\n    }\n    if (payload.iss !== 'https://accounts.google.com') {\n      console.error('bad issuer', payload);\n      throw new functions.https.HttpsError('permission-denied', 'bad issuer');\n    }\n    if (payload.exp",
      "question_score": 6,
      "answer_score": 4,
      "created_at": "2023-03-18T23:51:36",
      "url": "https://stackoverflow.com/questions/75779120/firebase-id-token-has-incorrect-aud-audience-claim-when-authenticating-fireb"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 65535732,
      "title": "Separating Firebase Functions Into Self-Contained &quot;Packages&quot; (Not separate files - entirely separate packages)",
      "problem": "Original Question - See Answer at End\nWe've been using Firebase Functions for 2+ years and have amassed well over 120 HTTP, callable, triggered, and scheduled functions, all being called from a single `functions/index` and managed by a single `package.json`, probably like a lot of you. As you can imagine, we have some old dependencies in there that we're hesitant to update because it's an awful lot of code to go through, test, etc. So it got me thinking, and I'm asking if any of you have done this or know why this wouldn't work...\nLooking at the GCP dashboard, each function is a separate, stand-alone service. But if you download the code from there, you end up with the full build of all 120+ functions, node modules, etc. So if I run `npm deploy` on my single `functions` directory (if quotas weren't an issue), it looks like\n\nFirebase Tools grabs my single build on my machine or CI tool\ncopies it 120+ times, and then\npushes one full copy of the entire build into each of those functions\n\nThat got me thinking - considering I can't and don't want to build my entire project and deploy all functions at once, do I have to have them all in a single `functions` directory, sharing a single `package.json` and dependencies, and exported from a single `functions/index`?\nIs there any reason I couldn't have, for example:\n`- functions\n\n  - functionSingleA (running on node 10)\n    - lib/index.js\n    - package.json (stripe 8.92 and joi)\n    - src/index.ts\n    - node_modules\n\n  - functionGroupB (running on node 12)\n    - lib/index.js\n    - package.json (stripe 8.129 and @hapi/joi)\n    - src/index.ts\n    - node_modules\n`\nI know that I lose the ability to deploy all at once, but I don't have that luxury any more due to quotas. Beyond that, is there any reason this wouldn't work? After all, as best as I can tell, Firebase Functions are just individual serverless Cloud Functions with Firebase credentials built in. Am I missing something, or do you do this and it works fine (or breaks everything)?\nAnswer from Google Firebase Team\nA Firebase engineer through support confirms that this is absolutely possible, but also check out the discussion between me and @samthecodingman. You can break up your functions into completely self-contained modules or groups with different `package.json` files and dependencies, and deploy each one (individually or as groups) without affecting other functions.\nWhat you lose in return is the ability to deploy all with the `firebase functions deploy` command (though @samthecodingman presented a solution), and you lose the ability to emulate functions locally. I don't have a workaround for that yet.",
      "solution": "It should be possible by tweaking the file structure to this:\n```\n`- functionProjects\n  - deployAll.sh\n  - node10\n    - deploy.sh\n    - firebase.json\n    - functions\n      - lib/index.js\n      - package.json (stripe 8.92 and joi)\n      - src/index.ts\n      - node_modules\n  - node12\n    - deploy.sh\n    - firebase.json\n    - functions\n      - lib/index.js\n      - package.json (stripe 8.129 and @hapi/joi)\n      - src/index.ts\n      - node_modules\n`\n```\nAs a rough idea, you should be able to use a script to perform targeted deployments. Using the targeted deploy commands, it should leave the other functions untouched (i.e. it won't ask you to delete missing functions).\nEach `deploy.sh` should change the working directory to where it is located, and then execute a targeted deploy command.\n```\n`#!/bin/bash \n# update current working directory to where the script resides\nSCRIPTPATH=$(readlink -f \"$0\")\nSCRIPTPARENT=$(dirname \"$SCRIPTPATH\")\npushd $SCRIPTPARENT\nfirebase deploy --only functions:function1InThisFolder,functions:function2InThisFolder,functions:function3InThisFolder,...\npopd\n`\n```\nThe `deployAll.sh` file just executes each 'child' folder's `deploy.sh`.\n```\n`#!/bin/bash\n/bin/bash ./node10/deploy.sh\n/bin/bash ./node12/deploy.sh\n`\n```\nThis requires maintaining the list of functions in `deploy.sh`, but I don't think that's too tall of an ask. You could mock the `firebase-functions` library so that calls to `functions.https.onRequest()` (along with the other function exports) just return `true` and use that to get a dynamic list of functions if you so desire.\nYou could also flatten the file structure so that `./node10` and `./node12` are the deployed function directories (instead of the nested `functions` folders) by adding `\"functions\": { { \"source\": \".\" } }` to their respective `firebase.json` files.",
      "question_score": 6,
      "answer_score": 3,
      "created_at": "2021-01-02T04:15:53",
      "url": "https://stackoverflow.com/questions/65535732/separating-firebase-functions-into-self-contained-packages-not-separate-files"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 69785868,
      "title": "Installing private GitHub npm package in Firebase Cloud Functions",
      "problem": "I am new to Firebase Cloud Functions and have been struggling with adding private npm packages to make my functions work. I understand Firebase will treat them all as public unless specified and will install with npm / yarn what I have in package.json.\nThe only way for me to tell Firebase that it's a private repository on Github is to add a\n.npmrc (containing) - The key I am using is a Personal Access Token from Github-Developers that has all the need it permissions\n```\n`//npm.pkg.github.com/:_authToken=\n# @calugarul:registry=git+https://npm.pkg.github.com/calugarul\n@calugarul:registry=git+https://@github.com/kroitor/ccxt.pro.git\n`\n```\nIn my index.js file I am calling the private repo (ccxt.pro) but it will not install on the Firebase Cloud Functions server.\nindex.js\n```\n`const functions = require(\"firebase-functions\");\n\nvar num = 1;\n\nconst admin = require('firebase-admin');\nadmin.initializeApp();\n\nconst db = admin.firestore();\n\nvar moment = require('moment'); \nconst ccxtpro = require ('ccxt.pro');\n\nconst coinbaseEx = new ccxtpro.coinbasepro({'enableRateLimit': true});\n`\n```\npackage.json (contains)\n```\n`{\n  \"name\": \"functions\",\n  \"description\": \"Cloud Functions for Firebase\",\n  \"scripts\": {\n    \"lint\": \"eslint\",\n    \"serve\": \"firebase emulators:start --only functions\",\n    \"shell\": \"firebase functions:shell\",\n    \"start\": \"npm run shell\",\n    \"deploy\": \"firebase deploy --only functions\",\n    \"logs\": \"firebase functions:log\"\n  },\n  \"engines\": {\n    \"node\": \"14\"\n  },\n  \"main\": \"index.js\",\n  \"dependencies\": {\n    \"@calugarul/ccxt.pro\": \"git+https://github.com/kroitor/ccxt.pro.git\",\n    \"firebase-admin\": \"^9.8.0\",\n    \"firebase-functions\": \"^3.16.0\",\n    \"moment\": \"^2.29.1\"\n  },\n  \"devDependencies\": {\n    \"eslint\": \"^7.6.0\",\n    \"eslint-config-google\": \"^0.14.0\",\n    \"firebase-functions-test\": \"^0.2.0\"\n  },\n  \"private\": true\n}\n`\n```\nUsual Error:\n```\n` E getTickersAndSendRequestToStart: {\"@type\":\"type.googleapis.com/google.cloud.audit.AuditLog\",\"status\":{\"code\":3,\"message\":\"Build failed: npm ERR! Error while executing:\\nnpm ERR! /usr/bin/git ls-remote -h -t https://github.com/kroitor/ccxt.pro.git\\nnpm ERR! \\nnpm ERR! remote: Support for password authentication was removed on August 13, 2021. Please use a personal access token instead.\\nnpm ERR! remote: Please see https://github.blog/2020-12-15-token-authentication-requirements-for-git-operations/ for more information.\\nnpm ERR! fatal: Authentication failed for 'https://github.com/kroitor/ccxt.pro.git/'\\nnpm ERR! \\nnpm ERR! exited with error code: 128\\n\\nnpm ERR! A complete log of this run can be found in:\\nnpm ERR!     /www-data-home/.npm/_logs/2021-10-31T02_03_39_782Z-debug.log; Error ID: beaf8772\"},\"authenticationInfo\":{\"principalEmail\":\"my email\"},\"serviceName\":\"cloudfunctions.googleapis.com\",\"methodName\":\"google.cloud.functions.v1.CloudFunctionsService.UpdateFunction\",\"resourceName\":\"projects/smart-trader-cd29f/locations/us-central1/functions/getTickersAndSendRequestToStart\"}\n    macbookpro:functions cleo$ \n`\n```\nDon't know what to do, been searching for days and no results. Please help me find a way to be able to tell Firebase to install the private repository upon deployment of a function!",
      "solution": "After the rest of the day searching for the answer it was the most simple but not the wisest solution that worked:\nCompletely ignored the .npmrc file and in package.json under dependencies just added the personal access token like so: @github.com\n```\n`\"dependencies\": {\n    \"@calugarul/ccxt.pro\": \"git+https://@github.com/kroitor/ccxt.pro.git\",\n    \"dotenv\": \"^10.0.0\",\n    \"firebase-admin\": \"^9.8.0\",\n    \"firebase-functions\": \"^3.16.0\",\n    \"moment\": \"^2.29.1\"\n  },\n`\n```\nIt works and Cloud functions download the private repo but it's not the secure approach. If anybody has a more secure approach please let me know.",
      "question_score": 6,
      "answer_score": 1,
      "created_at": "2021-10-31T11:16:53",
      "url": "https://stackoverflow.com/questions/69785868/installing-private-github-npm-package-in-firebase-cloud-functions"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 68679583,
      "title": "Firebase App Check + Cloud functions : Failed to validate AppCheck token",
      "problem": "I have recently added Firebase App Check in my Flutter app.\nI am calling Cloud functions with the `https.onCall()` method. However i am receiving this error :\n```\n`>  {\"severity\":\"WARNING\",\"message\":\"Failed to validate AppCheck token. FirebaseAppCheckError: Decoding App Check token failed. Make sure you passed the entire string JWT which represents the Firebase App Check token.\n    at FirebaseAppCheckError.FirebaseError [as constructor] (/Users/foxtom/Desktop/Cloud Functions/functions/node_modules/firebase-admin/lib/utils/error.js:44:28)\n    at FirebaseAppCheckError.PrefixedFirebaseError [as constructor] (/Users/foxtom/Desktop/Cloud Functions/functions/node_modules/firebase-admin/lib/utils/error.js:90:28)\n    at new FirebaseAppCheckError (/Users/foxtom/Desktop/Cloud Functions/functions/node_modules/firebase-admin/lib/app-check/app-check-api-client-internal.js:187:28)\n    at /Users/foxtom/Desktop/Cloud Functions/functions/node_modules/firebase-admin/lib/app-check/token-verifier.js:82:19\n    at processTicksAndRejections (node:internal/process/task_queues:96:5) {\n  errorInfo: {\n    code: 'app-check/invalid-argument',\n    message: 'Decoding App Check token failed. Make sure you passed the entire string JWT which represents the Firebase App Check token.'\n  },\n codePrefix: 'app-check'\n}\"}\n>  {\"verifications\":{\"app\":\"INVALID\",\"auth\":\"MISSING\"},\"logging.googleapis.com/labels\":{\"firebase-log-type\":\"callable-request-verification\"},\"severity\":\"WARNING\",\"message\":\"Callable request verification failed: AppCheck token was rejected.\"}\n`\n```\nThe severity seems to be WARNING only but it doesn't execute the function. My function only contains a `console.log()`\nIn my app i have this error :\n```\n`W/FirebaseContextProvider( 6788): Error getting App Check token; using placeholder token instead. Error: com.google.firebase.FirebaseException: Error returned from API. code: 403 body: App attestation failed.\nI/flutter ( 6788): Error is : [firebase_functions/unauthenticated] Unauthenticated\n`\n```\nI have not enforced anything like suggested in the documentation\nThis is preventing me from using Cloud Functions and i can disable App Check for my app anymore...\nEDIT :\nI add that `Firebase Storage` and `RealTime Database` are working fine without any debug AppCheck token when it's not enforced.\nWhat can I do ?",
      "solution": "I created a reproducible code sample, which you see here: https://github.com/nilsreichardt/playground/tree/firebase-app-check-cloud-function-unauthenticated-issue/firebase-app-check-cloud-functions-unauthentificated\nTherefore, I created a detailed issue in the FlutterFire repository: https://github.com/FirebaseExtended/flutterfire/issues/6794\nA first workaround already posted as a comment and I'm sure that more workarounds or solutions will follow.",
      "question_score": 6,
      "answer_score": 2,
      "created_at": "2021-08-06T11:49:59",
      "url": "https://stackoverflow.com/questions/68679583/firebase-app-check-cloud-functions-failed-to-validate-appcheck-token"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 79732826,
      "title": "Why can anyone call my Firebase callable Cloud Function via its HTTPS URL, outside my mobile app?",
      "problem": "I have a Firebase project with a mobile app (Flutter) that uses callable Cloud Functions. From the app, I invoke them with the Firebase SDK like this:\n```\n`const functions = firebase.functions();\nconst callable = functions.httpsCallable('myFunctionName');\ncallable({ someData: 'value' }).then(result => {\n  console.log(result.data);\n});\n`\n```\nI assumed that onCall functions could only be invoked by my app\u2019s authenticated clients.\nHowever, I discovered I can call the underlying HTTPS endpoint directly from a browser or Postman using the URL. It still executes\u2014even when not called from my app.\nIn my Python Cloud Function, I\u2019ve already added this authentication check:\n```\n`if not req.auth or not req.auth.uid:\n    return {\"status\": 400, \"message\": \"User not authenticated\"}\n`\n```\nBecause of this check, an unauthenticated caller can\u2019t run the actual function logic.\nHowever, the request still causes the function instance to spin up (cold start), meaning it consumes resources and could be abused for a DoS-style cost attack.\nI\u2019ve also enabled App Check, so legitimate app clients must pass verification \u2014 but the HTTPS endpoint still remains publicly reachable.\nConfig observation:\nIn Google Cloud Console \u2192 Cloud Functions \u2192 Permissions, I see:\n```\n`Authentication: Require authentication\nWarning: This service is publicly accessible because 'allUsers' has been granted permission on the service.\n`\n```\nDoes this IAM setting explain why the endpoint is still publicly accessible?\nIf I remove allUsers from the IAM policy, will it block all external requests before they spin up the function, so only authenticated users from my app can call it?\nAn onCall request includes a Firebase Authentication user ID token for the logged-in user making the request. The backend automatically verifies this token and provides it in the handler's context. If the token is invalid, the request is rejected. However, I find it odd that my billable container instance count still reaches 1.\nDid see a similar question here : but does not have a solution : How do you make a HTTPS onCall Cloud Function deployed via Firebase private",
      "solution": "I assumed that onCall functions could only be invoked by my app\u2019s authenticated clients.\n\nThat's definitely not true.\n\nthe request still causes the function instance to spin up (cold start), meaning it consumes resources and could be abused for a DoS-style cost attack.\n\nThat's always the risk when providing services that are accessible from anywhere in the world.  There is no 100% reliable way of eliminating this risk.\n\nI\u2019ve also enabled App Check, so legitimate app clients must pass verification \u2014 but the HTTPS endpoint still remains publicly reachable.\n\nApp Check doesn't shut down access to a function from the public, nor does it provide 100% accurate protection.  This is even stated in the documentation:\n\nApp Check relies on the strength of its attestation providers to determine app or device authenticity. It prevents some, but not all, abuse vectors directed towards your backends. Using App Check does not guarantee the elimination of all abuse, but by integrating with App Check, you are taking an important step towards abuse protection for your backend resources.\n\nUnfortunately, if you want to run a public app with a backend, you're going to have to accept that an attacker can incur some costs, as is the case with all public apps and services running on any cloud platform.\n\nDoes this IAM setting explain why the endpoint is still publicly accessible? If I remove allUsers from the IAM policy, will it block all external requests before they spin up the function, so only authenticated users from my app can call it?\n\nNo.  If you remove allUsers, your app will not be able to invoke your callable function at all, and your client will always receive an authentication error.  Your function must have allUsers in order to function correctly.  GCP IAM setting are for controlling access to cloud resources using GCP service accounts, not Firebase users.  Firebase users have nothing at all to do with GCP allUsers - they refer to completely different things.\nIf you want strongly enterprise-grade protection, you'll have to look into paying for and configuring a product such as Cloud Armor, which can further help with abuse.\nSee also:\n\nDoes \"allUsers\" in Cloud Functions allow only the users of my app?\n\nFirebase Callable Functions: Permission settings to restrict calling\n\nIs it possible to only accept function calls from my app in Firebase?",
      "question_score": 6,
      "answer_score": 1,
      "created_at": "2025-08-12T09:29:42",
      "url": "https://stackoverflow.com/questions/79732826/why-can-anyone-call-my-firebase-callable-cloud-function-via-its-https-url-outsi"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 71734646,
      "title": "Backing up Firestore data incrementally",
      "problem": "I'm trying to think of the best (read automated, cheapest and easy to use) way to back up Firestore data for a production app.\nI'm aware I could automate exports through a scheduled cloud function and send them over to a gcloud bucket. The problem I have with this approach is that it does not allow for \"incremental updates of the new and updated documents\" but only for backing up entire collections. This means that most of the data will be backed up each and every time, even though it hasn't even changed since the last backup, skyrocketing the cost up for no reason.\nThe approach that came to mind was having a cloud function in \"my-app\" project that would listen to each and every change in the Firestore, and perform the same change in the Firestore of the \"my-app-backup\" project.\nThis way, I only back up the changed data. Furthermore, backed up data would never become stale (as it's backed up in real-time), unlike the first approach where automated backups happen e.g. daily or weekly.\nIs this even possible, having a single cloud function in the first Firebase project writing data into another Firebase project? If not, perhaps write the data elsewhere(not in another Firebase project)? Does the approach even make sense, or do you have a better suggestion?",
      "solution": "If you want to export updated documents only then you can store a field `updatedAt` and query documents `where(\"updatedAt\", \">\", \"lastExportTime\")`. Then you can periodically run a Cloud function to export these documents. This should only cost N reads (N = number of updated documents) every time the function runs.\n\nFurthermore, backed up data would never become stale (as it's backed up in real-time)\n\nThis works too but can also get expensive if the document updates are too frequent.",
      "question_score": 6,
      "answer_score": 1,
      "created_at": "2022-04-04T11:08:49",
      "url": "https://stackoverflow.com/questions/71734646/backing-up-firestore-data-incrementally"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 67233470,
      "title": "Google Secret Manager Permissions For Local Emulating of Functions",
      "problem": "I've given the service account for the functions the necessary permissions ('Secret Manager Secret Accessor') and when deployed, the firebase functions are able to access the secrets without any problems.\nHowever, when using `firebase serve` or `firebase emulators:start --only functions` in local development, I'm getting the following error\n\nUnhandled error Error: 7 PERMISSION_DENIED: Permission 'secretmanager.versions.access' denied for resource\n\nI've found in the documentation that setting `export GOOGLE_APPLICATION_CREDENTIALS=pathtoserviceaccount.json` is needed to be entered in the terminal, though this did also not work for me.\nI would be thankful for all pointers. Cheers.",
      "solution": "I've found the answer myself:\nWhen the functions are emulated locally, they do not get run by the `App Engine default service account` per default, this needs to be enabled as well.\nSo I had to follow this tutorial https://firebase.google.com/docs/functions/local-shell\nThe `App Engine default service account` needs a key which can be created in the Service Accounts settings in the Google Cloud, and then\nI had to enter\n`export GOOGLE_APPLICATION_CREDENTIALS=\"path/to/key.json\"`\nin the terminal. By running then `firebase emulators:start` they also got permission to access the Secret Manager.\nSo while I was on the right track, I was exporting the wrong Service Account key, and not the one that was allowed to run access the Secret Manager.",
      "question_score": 5,
      "answer_score": 5,
      "created_at": "2021-04-23T18:12:18",
      "url": "https://stackoverflow.com/questions/67233470/google-secret-manager-permissions-for-local-emulating-of-functions"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 74982023,
      "title": "&quot;Error: Queue does not exist&quot; when queue-ing task in Firebase Functions",
      "problem": "I have got one functions that triggers on db update:\n```\n`exports.eventAddedTrigger = functions\n  .region('europe-west6')\n  .firestore\n  .document('users/{user_id}/events/{event_id}')\n  .onCreate(async (snap, context) => {\n    const event = snap.data();\n\n    if (event) {\n      const { user_id, event_id } = context.params;\n      const queue = getFunctions().taskQueue('enrol');\n      const signupDate = DateTime.fromSeconds(event.signupDate.seconds).minus({minutes: 2});\n      const now = DateTime.local({zone: 'Europe/Zurich'})\n      let scheduleDelaySeconds = Math.floor(signupDate.diff(now, 'seconds').seconds);\n      if (scheduleDelaySeconds This function triggers fine, but when it comes to enqueue-ing, I always get the following error\n```\n`Error: Queue does not exist\n`\n```\nregardless of whether I run the function emulated or in production.\nThe enrol function looks like this:\n```\n`exports.enrol = functions\n  .region('europe-west6')\n  .runWith({\n    timeoutSeconds: 540,\n    memory: '1GB',\n  })\n  .tasks\n  .taskQueue()\n  .onDispatch(async (data) => {\n    const { user_id, event_id } = data.body;\n    await _enrol(user_id, event_id, db);\n    functions.logger.info(`Enrolled user ${user_id} to event ${event_id}`);\n  });\n`\n```\nI have initialised my app correctly to my best knowledge:\n```\n`initializeApp({\n  serviceAccountId: process.env.FIREBASE_SERVICE_ACCOUNT_ID,\n});\n`\n```\nDo I have to register the queue somewhere else?",
      "solution": "I figured this out for me. If you're using non default region for your function (i.e. not us-central1) than you need to specify your queue name including your target region.\nThe schema is defined here https://github.com/firebase/firebase-admin-node/blob/master/src/utils/index.ts#L293\nSo use your enqueue function like this:\n`await\u00a0this.functions \n    .taskQueue[]>(`locations/${region}/functions/${queueName}`) \n    .enqueue(data);\n`",
      "question_score": 5,
      "answer_score": 16,
      "created_at": "2023-01-02T12:44:46",
      "url": "https://stackoverflow.com/questions/74982023/error-queue-does-not-exist-when-queue-ing-task-in-firebase-functions"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 72653548,
      "title": "How to list all firebase functions configs that are currently saved?",
      "problem": "Is there any way to list all the keys that I saved using the following command to check for the ones that I want to remove in case of any issue or just for reference ?\n```\n`firebase functions:config:set\n`\n```",
      "solution": "Please review the documentation:\n\nTo inspect what's currently stored in environment config for your project, you can use `firebase functions:config:get`.",
      "question_score": 5,
      "answer_score": 18,
      "created_at": "2022-06-17T03:48:19",
      "url": "https://stackoverflow.com/questions/72653548/how-to-list-all-firebase-functions-configs-that-are-currently-saved"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 71840853,
      "title": "Error Deploying Cloud Function from gitlab",
      "problem": "I am trying to deploy a cloud function via gitlab using a new service account (Not using default service account). It has the cloud functions developer role but it is still failing with below error:\nThe error below includes a user as cloud-functions-mixer. I haven't configured anything like that in my repo and not sure why it is coming up.\nFirst of all, running the suggested command doesn't even work because the suggested syntax is bad . I have tried running the below command but it\u2019s not right\n\nError: googleapi: Error 403: Missing necessary permission iam.serviceAccounts.actAs for cloud-functions-mixer on the service account project-test-tf-02@appspot.gserviceaccount.com.\nGrant the role 'roles/iam.serviceAccountUser' to cloud-functions-mixer on the service account project-test-tf-02@appspot.gserviceaccount.com.\nYou can do that by running 'gcloud iam service-accounts add-iam-policy-binding project-test-tf-02@appspot.gserviceaccount.com --member=cloud-functions-mixer --role=roles/iam.serviceAccountUser'.",
      "solution": "Google's instructions about the `cloud-functions-mixer` are wrong. What you actually need to do is replace the string `cloud-functions-mixer` with the name of the service account that is building or deploying your function.\nThe following user-defined service accounts will be used in an example:\n\n`my-cloud-function@my-project.iam.gserviceaccount.com` is the service account that your function runs as.\n`build-service-account@my-project.iam.gserviceaccount.com` is the service account that builds/deploys your Cloud Function\n\nThe command to run is:\n`gcloud iam service-accounts add-iam-policy-binding \n  my-cloud-function@my-project.iam.gserviceaccount.com \n   --member=serviceAccount:build-service-account@my-project.iam.gserviceaccount.com \n    --role=roles/iam.serviceAccountUser\n`\nDocs\nOr, in Terraform, you would need a resource like this:\n```\n`resource \"google_service_account_iam_member\" \"opentok_webhook_mixer\" {\n  service_account_id = google_service_account.my_cloud_function.id\n  role               = \"roles/iam.serviceAccountUser\"\n  member             = \"serviceAccount:${google_service_account.build_service_account.email}\"\n}\n`\n```\nYou'll have to update the names of the service account resources.\nThis approach also works for Google Cloud Build.",
      "question_score": 5,
      "answer_score": 18,
      "created_at": "2022-04-12T12:13:54",
      "url": "https://stackoverflow.com/questions/71840853/error-deploying-cloud-function-from-gitlab"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 67770785,
      "title": "Why do I get a Parsing error: Unexpected token =&gt; when creating firebase cloud function?",
      "problem": "I'm fairly new to firebase cloud functions and I'm trying to create a cloud function that will send an send an email to a newly created user on firebase (i will be using a log to test it first) but I keep having an error Parsing error: Unexpected token =>\nthis is the index.js code\n```\n`const functions = require(\"firebase-functions\");\nconst admin = require('firebase-admin');\nadmin.initializeApp();\n\nconst db = admin.firestore();\n\n// // Create and Deploy Your First Cloud Functions\n// // https://firebase.google.com/docs/functions/write-firebase-functions\n//\n\nexports.onUserCreate = functions.firestore.document('users/{usersId}').onCreate(async (snap, context) => {\n    const values = snap.data();\n\n    //send email\n    await db.collection('logging').add({ description: `Email was sent to user with nickname:${values.username}` });\n})\n`\n```\nthis is the .eslintrc.js\n```\n`module.exports = {\n  root: true,\n  parserOptions: {\n    parser: 'babel-eslint',\n    ecmaVersion: 6,\n  },\n  env: {\n    es6: true,\n    node: true,\n  },\n  extends: [\n    'eslint:recommended',\n    'google',\n  ],\n  rules: {\n    'generator-star-spacing': 'off',\n    'no-debugger': process.env.NODE_ENV === 'production' ? 'error' : 'off',\n  },\n};\n`\n```\nthis is the package.json\n```\n`{\n  \"name\": \"functions\",\n  \"description\": \"Cloud Functions for Firebase\",\n  \"scripts\": {\n    \"lint\": \"eslint .\",\n    \"serve\": \"firebase emulators:start --only functions\",\n    \"shell\": \"firebase functions:shell\",\n    \"start\": \"npm run shell\",\n    \"deploy\": \"firebase deploy --only functions\",\n    \"logs\": \"firebase functions:log\"\n  },\n  \"engines\": {\n    \"node\": \"14\"\n  },\n  \"main\": \"index.js\",\n  \"dependencies\": {\n    \"firebase-admin\": \"^9.8.0\",\n    \"firebase-functions\": \"^3.14.1\"\n  },\n  \"devDependencies\": {\n    \"eslint\": \"^7.6.0\",\n    \"eslint-config-google\": \"^0.14.0\",\n    \"firebase-functions-test\": \"^0.2.0\"\n  },\n  \"private\": true\n}\n`\n```\nany help is greatly appreciated. thanks!",
      "solution": "Two possible solutions to the issue you are facing would be to change in your `package.json` the scripts section to the following:\n```\n`\"scripts\": {\n    \"lint\": \"eslint\",\n    ...\n},\n`\n```\nSo, removing the ` .` from there, which is auto-generated but might cause this kind of issues.\nAlso you could change the `ecmaVersion` of the parser to version 8, so changing in your `.eslintrc.js` file to this:\n```\n`parserOptions: {\n    parser: 'babel-eslint',\n    ecmaVersion: 8,\n},\n`\n```\nThis might be needed for your eslint to understand the `async/await` notation.",
      "question_score": 5,
      "answer_score": 18,
      "created_at": "2021-05-31T11:04:07",
      "url": "https://stackoverflow.com/questions/67770785/why-do-i-get-a-parsing-error-unexpected-token-when-creating-firebase-cloud-f"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 66587246,
      "title": "Deploy a Cloud Function to different Firebase projects",
      "problem": "I have two firebase projects, one for development (project-dev) and one for production (project-prod).\nI initialized firebase CLI for writing and deploying cloud functions linked to project-prod. Then I ran `firebase use --add` and added project-dev so that I can deploy the same function to both projects; I don't want to rewrite the same function twice.\nAt this point, I faced the problem. Into the function, I have to write something to the realtime database, but when I deploy the function to project-dev it writes to project-prod's database.\nWhat I want to achieve is that the function has to refer to the database of the project that it is deployed to. So that I have one function and when it is deployed to project-dev it writes to project-dev's database and when it is deployed to project-prod it writes to project-prod's database.\nIs it possible to achieve that? If not, what's the way to go?\nEDIT\nFunction code:\n```\n`exports.AddOrders= functions.https.onRequest(async (req, res) => {\n    async function addOrder(key, value) {\n        var ref = app.database().ref(\"/orders\");\n        return ref.child(key).set(value);\n    }\n\n    var orders = req.body['orders'];\n    var promises = [];\n    for (var key in orders) {\n        promises.push(addOrder(key, orders[key]));\n    }\n    Promise.all(promises).then(\n        _ => {\n            res.sendStatus(200);\n            return null;\n        }\n    ).catch(err => {\n        res.send(err);\n    })\n});\n`\n```\n(This function works fine, the problem is that it writes on the wrong database)",
      "solution": "This is the answer for anyone wondering what the problem was.\nHe was initialising the admin sdk with the credentials of a service account tied to the production project. The solution was to change it to `admin.initializeApp()` without passing any arguments. This made Firebase use the default credentials for each project.\nI know its a common mistake but again, here is the link to the corresponding documentation https://firebase.google.com/docs/admin/setup",
      "question_score": 5,
      "answer_score": 1,
      "created_at": "2021-03-11T18:10:04",
      "url": "https://stackoverflow.com/questions/66587246/deploy-a-cloud-function-to-different-firebase-projects"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 65647010,
      "title": "Read file with firebase function",
      "problem": "I've got a Firebase Function and want to read a html file, stored in a subfolder.\n```\n`\u2500\u2500 functions\n    \u251c\u2500\u2500 src\n    |   \u251c\u2500\u2500 index.ts // Here is the function\n    |   \u251c\u2500\u2500 html\n    |   |   \u251c\u2500\u2500 template.html // the file I want to read\n`\n```\nI tried to read the file via  `const html = fs.readFileSync('./html/template.html');` but it always tells me\n\nError: ENOENT: no such file or directory, open './html/template.html'\nat Object.openSync\n\nI worked trough almost every question on stackoverflow concerning this topic but nothing worked for me. I also tried placing the file in the same folder as the function but it always gives me the error.\nDo I need to install some package or import the file in some way to access it?",
      "solution": "You need to use the `path.resolve()` method, which will resolve your path into an absolute path, before passing it to the `fs.readdirSync()` method. So the following should do the trick:\n```\n`    const path = require('path');\n\n    // ...\n\n    let content = fs.readFileSync(\n        path.resolve('./html/template.html')\n    );\n`\n```",
      "question_score": 5,
      "answer_score": 4,
      "created_at": "2021-01-09T20:57:12",
      "url": "https://stackoverflow.com/questions/65647010/read-file-with-firebase-function"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 70851906,
      "title": "Cannot use &quot;undefined&quot; as a Firestore value. If you want to ignore undefined values, enable `ignoreUndefinedProperties`",
      "problem": "Im getting this error\n```\n`error is here: Error: Value for argument \"value\" is not a valid query constraint. Cannot use \"undefined\" as a Firestore value. If you want to ignore undefined values, enable `ignoreUndefinedProperties`. \n`\n```\nin my firebase javascript function and I have no idea how to solve it . I found out that in typescript this error happens . But what does this error means in javascript ?\nThis is my function\n```\n`import * as functions from \"firebase-functions\";\nimport admin from \"firebase-admin\";\n\nimport {\n  deleteCollection,\n  deleteQuery,\n  deleteUserData,\n} from \"../utils/deletion\";\nexport default functions.firestore\n  .document(\"deletions/{userUid}\")\n  .onDelete(async (snap, context) => {\n    const db = admin.firestore();\n    const { userUid } = context.params;\n    const { uid } = userUid;\n//dont forget to delete storage files\n    try {\n      await db.doc(`deletions/${userUid}/meinprofilsettings/${uid}`).delete();\n\n        await deleteQuery(db, db.collection(`deletions/${userUid}/videos`).where(\"uid\", \"==\", uid));\n`\n```\nThe line is the await deleteQuery...\nThe deleteQuery functions looks like this\n```\n`async function deleteQuery(db, query, batchSize = 100) {\n  const q = query.limit(batchSize);\n  return new Promise((resolve, reject) => {\n    deleteQueryBatch(db, q, resolve).catch(reject);\n  });\n}\nasync function deleteQueryBatch(db, query, resolve) {\n  const snapshot = await query.get();\n  const batchSize = snapshot.size;\n  if (batchSize === 0) {\n    // When there are no documents left, we are done\n    resolve();\n    return;\n  }\n  // Delete documents in a batch\n  const batch = db.batch();\n  snapshot.docs.forEach((doc) => {\n    batch.delete(doc.ref);\n  });\n  await batch.commit();\n  // Recurse on the next process tick, to avoid\n  // exploding the stack.\n  process.nextTick(() => {\n    deleteQueryBatch(db, query, resolve);\n  });\n}\n\n`\n```",
      "solution": "Error: Value for argument \"value\" is not a valid query constraint.\nCannot use \"undefined\" as a Firestore value. If you want to ignore\nundefined values, enable `ignoreUndefinedProperties`.\n\nThis means that you have an \u2018undefined\u2019 value in your list. Firebase does not support undefined values in your list, check your list and see if something is `undefined`.  You can avoid this by using `ignoreUndefinedProperties`.  Here is an example of how you can achieve this:\n```\n`import * as firestore from \"firebase-admin\";\n`\n```\nThen just need to have something like this:\n```\n`const db = firestore.firestore(); \ndb.settings({ ignoreUndefinedProperties: true })\n`\n```",
      "question_score": 5,
      "answer_score": 14,
      "created_at": "2022-01-25T17:07:47",
      "url": "https://stackoverflow.com/questions/70851906/cannot-use-undefined-as-a-firestore-value-if-you-want-to-ignore-undefined-val"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 66814094,
      "title": "Why can&#39;t I use `allAuthenticatedUsers` for my Firebase Cloud Function?",
      "problem": "When deploying Firebase Functions using the Firebase CLI, they are configured so that the Cloud Functions Invoker permission is granted to `allUsers`. With such a setting the code below functions as expected.\nThe Cloud Functions Invoker permission can also be granted to `allAuthenticatedUsers`. However, when I implement this change for `addMessage`, I only ever get a `UNAUTHENTICATED` error response using the code below.\nWhy won't `allAuthenticatedUsers` work for this Firebase Cloud Function?\n\nNote: This Q&A is a result of a now-deleted question posted by Furkan Yurdakul, regarding why `allAuthenticatedUsers` wasn't working with his Firebase Callable Function for his Firebase app\n\nMWE based on the documentation, with `addMessage` defined here:\n`firebase.auth().signInAnonymously() // for the sake of the MWE, this will normally be Facebook, Google, etc\n  .then((credential) => {\n    // logged in successfully, call my function\n    const addMessage = firebase.functions().httpsCallable('addMessage');\n    return addMessage({ text: messageText });\n  })\n  .then((result) => {\n    // Read result of the Cloud Function.\n    const sanitizedMessage = result.data.text;\n    alert('The sanitized message is: ' + sanitizedMessage);\n  })\n  .catch((error) => {\n    // something went wrong, keeping it simple for the MWE\n    const errorCode = error.code;\n    const errorMessage = error.message;\n\n    if (errorCode === 'auth/operation-not-allowed') {\n      alert('You must enable Anonymous auth in the Firebase Console.');\n    } else {\n      console.error(error);\n    }\n  });\n`",
      "solution": "Simply put, if the ID token passed to a Cloud Function represents a Google account (that used Google Sign-In through Firebase or Google itself), it works, otherwise, it doesn't.\nThink of `allAuthenticatedUsers` as `allAuthenticatedGoogleUsers` instead of `allAuthenticatedFirebaseUsers`.\nBackground Information\nFor Callable Firebase Functions used with the Firebase Client SDKs, you will normally grant `allUsers` the permission to call it (the default setting Firebase CLI deployed functions).\nA valid authenticated client request for a Google Cloud Functions must have an `Authorization: Bearer ID_TOKEN` header (preferred) or `?access_token=ID_TOKEN`. Here, `ID_TOKEN` is a signed-in Google user's ID token as a JWT.\nWhen Firebase Client SDKs call a Callable Function, they set the `Authorization` header for you with the current user's ID token (if the user is signed in, here). This is done so that the user's authentication token can be used in the `context` parameter of `onCall()` functions. Importantly though, a Firebase user's ID token doesn't always represent a Google user which makes it incompatible with `allAuthenticatedUsers`.\nBecause of this, you will have to gate your callable function in your code by checking `context.auth` and it's properties like below.\n`export const addMessage = functions.https.onCall((data, context) => {\n  if (!context.auth) {\n    // Throwing a HttpsError so that the client gets the error details.\n    throw new functions.https.HttpsError(\n      'failed-precondition',\n      'The function must be called while authenticated.'\n    );\n  }\n\n  // a valid user is logged in\n\n  // do work\n});\n`\n\nAddendum on 403 Forbidden Errors\nIf your function is consistently throwing a 403 error after being deployed, this is likely because you are using an outdated copy of the Firebase CLI, as highlighted in the documentation:\n\nCaution: New HTTP and HTTP callable functions deployed with any Firebase CLI lower than version 7.7.0 are private by default and throw HTTP 403 errors when invoked. Either explicitly make these functions public or update your Firebase CLI before you deploy any new functions.",
      "question_score": 5,
      "answer_score": 14,
      "created_at": "2021-03-26T10:20:18",
      "url": "https://stackoverflow.com/questions/66814094/why-cant-i-use-allauthenticatedusers-for-my-firebase-cloud-function"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 71353766,
      "title": "How to properly connect to MongoDB using Cloud functions?",
      "problem": "I would like to connect to my Atlas cluster only once per instance running Cloud Functions.\nHere is my code for an instance :\n```\n`const MongoClient = require(\"mongodb\").MongoClient;\n\nconst client = new MongoClient(\"myUrl\", {\n  useNewUrlParser: true,\n  useUnifiedTopology: true,\n});\n\nexports.myHttpMethod = functions.region(\"europe-west1\").runWith({\n  memory: \"128MB\",\n  timeoutSeconds: 20,\n}).https.onCall((data, context) => {\n  console.log(\"Data is: \", data);\n  client.connect(() => {\n    const testCollection = client.db(\"myDB\").collection(\"test\");\n    testCollection.insertOne(data);\n  });\n});\n`\n```\nAnd i would like to avoid the `client.connect()` in each function call that seems to be really too much.\nI would like to do something like this :\n```\n`const MongoClient = require(\"mongodb\").MongoClient;\n\nconst client = await MongoClient.connect(\"myUrl\", {\n  useNewUrlParser: true,\n  useUnifiedTopology: true,\n});\n\nconst db = client.db(\"myDB\");\n\nexports.myHttpMethod = functions.region(\"europe-west1\").runWith({\n  memory: \"128MB\",\n  timeoutSeconds: 20,\n}).https.onCall((data, context) => {\n  console.log(\"Data is: \", data);\n  const testCollection = db.collection(\"test\");\n  testCollection.insertOne(data);\n});\n`\n```\nBut i can't `await` like this.\nIn my AWS Lambda functions (running in python) i have not this issue and i am able to connect only once per instance, so i guess there is an equivalent but i don't know much JS / Node JS.",
      "solution": "You can store your database client as a global variable. From the documentation,\n\nCloud Functions often recycles the execution environment of a previous invocation. If you declare a variable in global scope, its value can be reused in subsequent invocations without having to be recomputed.\n\nTry refactoring the code as shown below:\n```\n`import * as functions from \"firebase-functions\";\n\nimport { MongoClient } from \"mongodb\"; \n\nlet client: MongoClient | null;\n\nconst getClient = async () => {\n  if (!client) {\n    const mClient = new MongoClient(\"[MONGODB_URI]\", {});\n    client = await mClient.connect();\n    functions.logger.log(\"Connected to MongoDB\");\n  } else {\n    functions.logger.log(\"Using existing MongoDB connection\");\n  }\n  functions.logger.log(\"Returning client\");\n  return client;\n};\n\nexport const helloWorld = functions.https.onRequest(\n  async (request, response) => {\n    const db = (await getClient()).db(\"[DATABASE]\");\n    const result = await db.collection(\"[COLLECTION]\").findOne({});\n    response.send(\"Hello from Firebase!\");\n  }\n);\n`\n```\nThis should reuse the connection for that instance.",
      "question_score": 5,
      "answer_score": 9,
      "created_at": "2022-03-04T16:41:11",
      "url": "https://stackoverflow.com/questions/71353766/how-to-properly-connect-to-mongodb-using-cloud-functions"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 70183270,
      "title": "Error when import firebase-functions-test when testing with mocha",
      "problem": "I am trying to setup a Firebase Cloud Functions repo to run mocha test. However, it throws the following error when I use `import * as firebase from \"firebase-functions-test\";` or `const firebase = require(\"firebase-functions-test\")();`. You can see in my code that I haven't even called the actual firebase functions yet so I think this a setup issue.\nQuestion: What change do I need to make mocha test running for Firebase Functions testing using import syntax?\nWorking test code\n```\n`import { assert } from \"chai\";\ndescribe(\"Sanity Check\", () => {\n  it(\"should pass\", () => {\n    assert.equal(0, 0);\n  });\n});\n\n`\n```\nFailed Test Code using require\n```\n`const test = require(\"firebase-functions-test\")();\nimport { assert } from \"chai\";\ndescribe(\"Sanity Check\", () => {\n  it(\"should pass\", () => {\n    assert.equal(0, 0);\n    test.cleanup();\n  });\n});\n`\n```\nFailed code using import\n```\n`import * as firebase from \"firebase-functions-test\";\nimport { assert } from \"chai\";\n\nconst test = firebase();\ndescribe(\"Sanity Check\", () => {\n  it(\"should pass\", () => {\n    assert.equal(0, 0);\n    test.cleanup();\n  });\n});\n`\n```\nError for the failure\n```\n`> functions@ test /Users/cupidchan/temp/functions\n> mocha -r ts-node/register test/**/*.spec.ts\n\nError [ERR_PACKAGE_PATH_NOT_EXPORTED]: Package subpath './lib/encoder' is not defined by \"exports\" in /Users/cupidchan/temp/functions/node_modules/firebase-functions/package.json\n    at new NodeError (internal/errors.js:322:7)\n    at throwExportsNotFound (internal/modules/esm/resolve.js:322:9)\n    at packageExportsResolve (internal/modules/esm/resolve.js:545:3)\n    at resolveExports (internal/modules/cjs/loader.js:450:36)\n    at Function.Module._findPath (internal/modules/cjs/loader.js:490:31)\n    at Function.Module._resolveFilename (internal/modules/cjs/loader.js:888:27)\n    at Function.Module._resolveFilename.sharedData.moduleResolveFilenameHook.installedValue [as _resolveFilename] (/Users/cupidchan/temp/functions/node_modules/@cspotcode/source-map-support/source-map-support.js:679:30)\n    at Function.Module._load (internal/modules/cjs/loader.js:746:27)\n    at Module.require (internal/modules/cjs/loader.js:974:19)\n    at require (internal/modules/cjs/helpers.js:93:18)\n    at Object. (/Users/cupidchan/temp/functions/node_modules/firebase-functions-test/lib/providers/firestore.js:26:19)\n    at Module._compile (internal/modules/cjs/loader.js:1085:14)\n    at Object.Module._extensions..js (internal/modules/cjs/loader.js:1114:10)\n    at Module.load (internal/modules/cjs/loader.js:950:32)\n    at Function.Module._load (internal/modules/cjs/loader.js:790:12)\n    at Module.require (internal/modules/cjs/loader.js:974:19)\n    at require (internal/modules/cjs/helpers.js:93:18)\n    at Object. (/Users/cupidchan/temp/functions/node_modules/firebase-functions-test/lib/features.js:9:19)\n    at Module._compile (internal/modules/cjs/loader.js:1085:14)\n    at Object.Module._extensions..js (internal/modules/cjs/loader.js:1114:10)\n    at Module.load (internal/modules/cjs/loader.js:950:32)\n    at Function.Module._load (internal/modules/cjs/loader.js:790:12)\n    at Module.require (internal/modules/cjs/loader.js:974:19)\n    at require (internal/modules/cjs/helpers.js:93:18)\n    at module.exports (/Users/cupidchan/temp/functions/node_modules/firebase-functions-test/lib/index.js:30:20)\n    at Object. (/Users/cupidchan/temp/functions/test/index.spec.ts:9:14)\n    at Module._compile (internal/modules/cjs/loader.js:1085:14)\n    at Module.m._compile (/Users/cupidchan/temp/functions/node_modules/ts-node/src/index.ts:1371:23)\n    at Module._extensions..js (internal/modules/cjs/loader.js:1114:10)\n    at Object.require.extensions. [as .ts] (/Users/cupidchan/temp/functions/node_modules/ts-node/src/index.ts:1374:12)\n    at Module.load (internal/modules/cjs/loader.js:950:32)\n    at Function.Module._load (internal/modules/cjs/loader.js:790:12)\n    at Module.require (internal/modules/cjs/loader.js:974:19)\n    at require (internal/modules/cjs/helpers.js:93:18)\n    at Object.exports.requireOrImport (/Users/cupidchan/temp/functions/node_modules/mocha/lib/nodejs/esm-utils.js:56:20)\n    at async Object.exports.loadFilesAsync (/Users/cupidchan/temp/functions/node_modules/mocha/lib/nodejs/esm-utils.js:88:20)\n    at async singleRun (/Users/cupidchan/temp/functions/node_modules/mocha/lib/cli/run-helpers.js:125:3)\n    at async Object.exports.handler (/Users/cupidchan/temp/functions/node_modules/mocha/lib/cli/run.js:374:5)\nnpm ERR! Test failed.  See above for more details.\n`\n```\npackage.json\n```\n`{\n  \"name\": \"functions\",\n  \"scripts\": {\n    \"lint\": \"eslint --ext .js,.ts .\",\n    \"build\": \"tsc\",\n    \"serve\": \"npm run build && firebase emulators:start --only functions\",\n    \"shell\": \"npm run build && firebase functions:shell\",\n    \"start\": \"npm run shell\",\n    \"deploy\": \"firebase deploy --only functions\",\n    \"logs\": \"firebase functions:log\",\n    \"test\": \"mocha -r ts-node/register test/**/*.spec.ts --reporter spec\"\n  },\n  \"engines\": {\n    \"node\": \"14\"\n  },\n  \"main\": \"lib/index.js\",\n  \"dependencies\": {\n    \"firebase-admin\": \"^9.8.0\",\n    \"firebase-functions\": \"^3.14.1\"\n  },\n  \"devDependencies\": {\n    \"@types/chai\": \"^4.2.22\",\n    \"@types/mocha\": \"^9.0.0\",\n    \"@types/node\": \"^16.11.11\",\n    \"@typescript-eslint/eslint-plugin\": \"^3.9.1\",\n    \"@typescript-eslint/parser\": \"^3.8.0\",\n    \"chai\": \"^4.3.4\",\n    \"eslint\": \"^7.6.0\",\n    \"eslint-config-google\": \"^0.14.0\",\n    \"eslint-config-prettier\": \"^8.3.0\",\n    \"eslint-plugin-import\": \"^2.22.0\",\n    \"eslint-plugin-prettier\": \"^4.0.0\",\n    \"esm\": \"^3.2.25\",\n    \"firebase-functions-test\": \"^0.2.3\",\n    \"mocha\": \"^9.1.3\",\n    \"prettier\": \"^2.5.0\",\n    \"ts-node\": \"^10.4.0\",\n    \"typescript\": \"^3.8.0\"\n  },\n  \"private\": true\n}\n`\n```\ntsconfig.json\n```\n`{\n  \"compilerOptions\": {\n    \"module\": \"commonjs\",\n    \"noImplicitReturns\": true,\n    \"noUnusedLocals\": true,\n    \"outDir\": \"lib\",\n    \"sourceMap\": true,\n    \"strict\": true,\n    \"target\": \"es2017\"\n  },\n  \"compileOnSave\": true,\n  \"include\": [\"src\", \"test\"]\n}\n`\n```",
      "solution": "This error should be resolved after specifying the latest version of the\n\n`firebase-functions`, v3.16.0, and\n\n`firebase-functions-test`, v0.3.3.",
      "question_score": 5,
      "answer_score": 7,
      "created_at": "2021-12-01T12:19:58",
      "url": "https://stackoverflow.com/questions/70183270/error-when-import-firebase-functions-test-when-testing-with-mocha"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 68554074,
      "title": "Pass parameters to Firebase Cloud Functions",
      "problem": "I'm trying to write a Firebase Cloud function to show a push notification to the user. First, I create the notification in the Firestore Database, and then I call a Firebase Cloud Function to send a push notification to the user. The problem is that I don't really know how to pass parameters to the Cloud Function\nI call the function like this:\n```\n`import { sendNotificationToUser, sendNotificationToNonUser } from '../../api/PushNotification';\n\nexport function createNotification(values, callback) {\n  return async dispatch => {\n    try {\n      ...\n      const newNotificationRef = firestore().collection('notifications').doc(notId);\n\n      const newNotification = await firestore().runTransaction(async transaction => {\n        const snapshot = await transaction.get(newNotificationRef);\n        const data = snapshot.data();\n\n        return transaction.set(newNotificationRef, {\n          ...data,\n          ...values,\n          id: notId,\n          viewed: false\n        });\n      });\n\n      if (newNotification) {\n        if (values.reciever === null) {\n          sendNotificationToNonUser(values.title, values.message);\n          ...\n        } else {\n          sendNotificationToUser(values.title, values.message, values.reciever);\n          ...\n        }\n      } else {\n        ...\n      }\n    } catch (e) {\n      ...\n    }\n  };\n}\n`\n```\nThen, on the `PushNotification` doc, I have this:\n```\n`import axios from 'axios';\n\nconst URL_BASE = 'https://>.cloudfunctions.net';\nconst Api = axios.create({\n  baseURL: URL_BASE\n});\n\nexport function sendNotificationToNonUser(title, body) {\n  Api.get('sendNotificationToNonUser', {\n    params: { title, body }\n  }).catch(error => { console.log(error.response); });\n}\n\nexport function sendNotificationToUser(title, body, user) {\n  Api.get('sendNotificationToUser', {\n    params: { title, body, user }\n  }).catch(error => { console.log(error.response); });\n}\n`\n```\nAnd on my Cloud Functions `index.js`\n```\n`exports.sendNotificationToUser = functions.https.onRequest((data, response) => {\n  console.log('Params:');\n});\n`\n```\nHow can I pass the params I send from the `PushNotifications` file to the respective Cloud Functions? I'm fairly new to Functions myself",
      "solution": "The `request, response` parameters (data, response in your case) are essentially Express Request and Response objects. You can use  `query` property of request to get those query parameters as shown.\n`exports.sendNotificationToUser = functions.https.onRequest((request, response) => {\n  console.log('Query Params:', request.query);\n  // This will log the params objects passed from frontend\n});\n`\nYou can also pass information in the request body and then access it by `request.body` in Cloud function, but then you would have to use a POST request.",
      "question_score": 5,
      "answer_score": 7,
      "created_at": "2021-07-28T05:35:52",
      "url": "https://stackoverflow.com/questions/68554074/pass-parameters-to-firebase-cloud-functions"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 67718406,
      "title": "What is the proper type for a request handler of a firebase HttpsFunction?",
      "problem": "I need to split my functions into multiple files.\nThis is my `index.ts`\n```\n`export const helloWorld = functions.https.onRequest((request, response) => {\n  functions.logger.info(\"Hello logs!\", {structuredData: true});\n  response.send(\"Hello from Firebase!\");\n});\n`\n```\nI need it to be something like:\n```\n`import helloWorldHandler from \"./handlers/helloWorldHandler\"\n\nexport const helloWorld = functions.https.onRequest(helloWorldHandler);\n`\n```\nSo what should I type the `helloWorldHandler` request handler?\n```\n`const helloWorldHandler : ??? = async (req,res) => {\n  const result = await someApi();\n  functions.logger.info(\"Hello logs!\", {structuredData: true});\n  res.send(\"Hello from Firebase!\");\n};\n\nexport default helloWorldHandler;\n`\n```\nI tried:\n```\n`import * as functions from \"firebase-functions\";\n\nconst helloWorldHandler : functions.HttpsFunction = async (req,res) => { ... };\n`\n```\nBut I'm getting this error:\n\nType '(req: Request, res: Response) => Promise' is not assignable to type 'HttpsFunction'.\nProperty '__trigger' is missing in type '(req: Request, res: Response) => Promise' but required in type 'TriggerAnnotated'.\n\nThe `onRequest()` method, the one that should take the handler as a parameter, does not seem to give it a proper type name, rather than a function signature. Do I need to create an alias for that?",
      "solution": "The type `functions.HttpsFunction` is the return type of `functions.https.onRequest()`, and not the argument to it. A function of this type is exported by your code and defines what needs to be deployed by the Firebase CLI (the region, memory size and so on are stored in the `__trigger` property).\nAs you want the type of the first argument to `functions.https.onRequest()`, you are instead looking for the type:\n`type HttpsOnRequestHandler = (req: functions.https.Request, resp: functions.Response) => void | Promise\n`\nBut rather than hard-code this, you can extract it from the Firebase Functions library using either:\n`import * as functions from \"firebase-functions\";\n\ntype HttpsOnRequestHandler = Parameters[0];\n`\nor\n`import { https } from \"firebase-functions\";\n\ntype HttpsOnRequestHandler = Parameters[0]\n`\nNote: If your code doesn't use the `firebase-functions` library itself, you can tell TypeScript that you only want its types using `import type * as functions from \"firebase-functions\";` and `import type { https } from \"firebase-functions\";` as appropriate; this removes the import from the compiled JavaScript as its not needed for running the code.",
      "question_score": 5,
      "answer_score": 5,
      "created_at": "2021-05-27T10:24:30",
      "url": "https://stackoverflow.com/questions/67718406/what-is-the-proper-type-for-a-request-handler-of-a-firebase-httpsfunction"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 76845486,
      "title": "How to get notified when there is an &quot;ERROR&quot; log in Google Cloud Logging for each running Cloud Function?",
      "problem": "I would simply like to know if it's possible and how to get notified when my Cloud Function is logging an entry using `logging.error` in python.\nThe field actually appears as : \"ERROR:root:the custom log\"\nI have checked about Alert policies but i don't understand the queries / filters and the \"time between notification\", since i only want to push for example to a Slack channel + email only once a notification when an error is logged.\nHow can i simply do that for each of my Functions ? or having : policies number = Alert policies x function number, in the worst case scenario..\nThanks in advance",
      "solution": "You can do something like :\n```\n`(resource.type=\u201ccloud_function\u201d resource.labels.function_name=(\u201cfunction_name\u201d) resource.labels.region=\u201ceurope-west1\u201d)) AND\ntextPayload:ERROR AND timestamp > \u201c2023-08-01\"\n`\n```",
      "question_score": 5,
      "answer_score": 1,
      "created_at": "2023-08-06T12:41:22",
      "url": "https://stackoverflow.com/questions/76845486/how-to-get-notified-when-there-is-an-error-log-in-google-cloud-logging-for-eac"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 74545030,
      "title": "How to set up per-seat subscription with Stripe?",
      "problem": "Good day!\nI am trying build a per-seat subscription model for my web site.\nBefore, I tried use Checkout session to provide \"place\", to write personal data, but the max what I reached - is creating default Subscription that you could cancel for instance.\nWhat I wanna do?\n\u2022 Figure out, if possible, how to create per-seat subscription through Checkout session and after, on event \"User joined\" automatically replace new price of the subscription.\n\u2022 If through Checkout session that is not possible, how to implement it through code?\nHow to get a user's card and how to upgrade later the subscription.\nI read the article on Stripe, but from that I can't figure how I get client data as card.\nIn addition, I use Stripe Extension on firebase with cloud functions.\nHope, this problem will be solved :)",
      "solution": "You can create a per-seat type subscription with Stripe Checkout. For that, you need to have a recurring price, just as the article suggests. Then pass the price into `line_items` property when creating a subscription, along with the initial quantity. For example, a customer might want to purchase 5 seats/licenses right away. You can also enable adjustable_quantity setting to allow the user to change the quantity on the Checkout Session page.\nWhen a new user joins, use the Update Subscription API to change the quantity of the item. For example, if the new total number of users you are serving is 11, set the quantity of the price to 11. More info here.\nIf you don\u2019t want to use Stripe Checkout, you can create a subscription via API, by following this guide. The upgrade step looks the same as in the other solution.\nBesides that, Stripe has a Billing Customer Portal that allows users to modify their subscriptions, it\u2019s worth taking a look.",
      "question_score": 5,
      "answer_score": 6,
      "created_at": "2022-11-23T11:07:58",
      "url": "https://stackoverflow.com/questions/74545030/how-to-set-up-per-seat-subscription-with-stripe"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 72306979,
      "title": "client.get_bucket() returns error: api_request() got an unexpected keyword argument &#39;extra_api_info&#39;",
      "problem": "I'm trying to store a newline-delimited JSON string in a GCS bucket using a cloud function, but seeing an error. I start by converting a dataframe to ndjson, then attempt to upload this to my GCS bucket as below. There is more code above this, but not relevant to my problem:\n```\n`import pandas as pd\nfrom google.cloud import storage\nfrom google.cloud.storage import blob\n\ndf = df.to_json(orient=\"records\", lines=True)\n\nstorage_client = storage.Client(project='my-project')\nbucket = storage_client.get_bucket('my-bucket')\nblob = bucket.blob('my-blob')\nblob.upload_from_string(df)\n`\n```\nWhen running this, I find the error below in the logs:\n```\n`Exception on / [POST] Traceback (most recent call last): \nFile \"/layers/google.python.pip/pip/lib/python3.7/site-packages/flask/app.py\", line 2073, in wsgi_app response = self.full_dispatch_request() \nFile \"/layers/google.python.pip/pip/lib/python3.7/site-packages/flask/app.py\", line 1518, in full_dispatch_request rv = self.handle_user_exception(e) \nFile \"/layers/google.python.pip/pip/lib/python3.7/site-packages/flask/app.py\", line 1516, in full_dispatch_request rv = self.dispatch_request() \nFile \"/layers/google.python.pip/pip/lib/python3.7/site-packages/flask/app.py\", line 1502, in dispatch_request return self.ensure_sync(self.view_functions[rule.endpoint])(**req.view_args) \nFile \"/layers/google.python.pip/pip/lib/python3.7/site-packages/functions_framework/__init__.py\", line 99, in view_func return function(request._get_current_object()) \nFile \"/workspace/main.py\", line 66, in my_request bucket = storage_client.get_bucket('my-bucket') \nFile \"/layers/google.python.pip/pip/lib/python3.7/site-packages/google/cloud/storage/client.py\", line 787, in get_bucket retry=retry, \nFile \"/layers/google.python.pip/pip/lib/python3.7/site-packages/google/cloud/storage/bucket.py\", line 1037, in reload retry=retry, \nFile \"/layers/google.python.pip/pip/lib/python3.7/site-packages/google/cloud/storage/_helpers.py\", line 244, in reload _target_object=self, \nFile \"/layers/google.python.pip/pip/lib/python3.7/site-packages/google/cloud/storage/client.py\", line 373, in _get_resource _target_object=_target_object, \nFile \"/layers/google.python.pip/pip/lib/python3.7/site-packages/google/cloud/storage/_http.py\", line 73, in api_request return call() \nFile \"/layers/google.python.pip/pip/lib/python3.7/site-packages/google/api_core/retry.py\", line 288, in retry_wrapped_func on_error=on_error, \nFile \"/layers/google.python.pip/pip/lib/python3.7/site-packages/google/api_core/retry.py\", line 190, in retry_target return target() TypeError: api_request() got an unexpected keyword argument 'extra_api_info'\n`\n```\nThis 'extra_api_info' argument appears to be the culprit, but I have no idea what this means, and never used to get this error when following exactly the same approach, so I wonder whether this is down to some change between different versions of the 'google.cloud' Python module.",
      "solution": "I worked out the answer to my own question. It was indeed a module version issue as I suspected.\nSpecifying `google.cloud.storage==1.44.0` in my `requirements.txt` file solved the problem, as my code is seemingly not compatible with the latest version of that module (for reasons that escape me).",
      "question_score": 5,
      "answer_score": 5,
      "created_at": "2022-05-19T17:08:14",
      "url": "https://stackoverflow.com/questions/72306979/client-get-bucket-returns-error-api-request-got-an-unexpected-keyword-argum"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 72208902,
      "title": "Firebase secrets not defined in process.env",
      "problem": "I'm writing a Firebase function with Cloud Storage trigger. Like this\n`const functions = require('firebase-functions')\nconst doSomethingWithSecrets = require('./doSomethingWithSecrets')\n\nconst doSomethingWhenUploaded = functions.runWith({\n  secrets: [\"MY_SECRET_1\", \"MY_SECRET_2\", \"MY_SECRET_3\"]\n}).storage.object().onFinalize(o => {\n  functions.logger.debug([\n    process.env.MY_SECRET_1  // undefined\n    process.env.MY_SECRET_2  // undefined\n    process.env.MY_SECRET_3  // undefined\n  ])\n\n  doSomethingWithSecrets(process.env.MY_SECRET_1, process.env.MY_SECRET_2, process.env.MY_SECRET_3)\n  // Error: Invalid secret.\n})\n`\nAll three of them returns undefined. I've made sure that they're properly set. They show up both when using `firebase functions:secret:accesss MY_SECRET_1` and in Google Cloud Console.\nWhat's wrong?\nAdditional info\nI previously used it with only one secret and it worked. I don't know what happened, I'm using nvm and lost track of which Node version I used when it worked, so it may be a clue.\n`process.env` returns all the env like normal and none of my secrets shows up.",
      "solution": "Update your firebase-tools and the issue will resolve itself. I was dealing with this issue all day today and found a git hub issue that fixed the problem in the latest release of 10.9.2.\n```\n`npm install -g firebase-tools\n`\n```\nhttps://github.com/firebase/firebase-tools/issues/4540\nhttps://github.com/firebase/firebase-tools/issues/4459",
      "question_score": 5,
      "answer_score": 4,
      "created_at": "2022-05-12T02:27:56",
      "url": "https://stackoverflow.com/questions/72208902/firebase-secrets-not-defined-in-process-env"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 74379026,
      "title": "Error on Firebase generateEmailVerificationLink in GCP Cloud Function",
      "problem": "I'm trying to set up a GCP Cloud Function to generate the email verification link using `admin.auth().generateEmailVerificationLink`, but it throws the error:\n```\n`Error: Credential implementation provided to initializeApp() via the \"credential\" property has insufficient permission to access the requested resource. See https://firebase.google.com/docs/admin/setup for details on how to authenticate this SDK with appropriate permissions.\n`\n```\nI was able to reproduce this error with the following Cloud Function code:\nindex.js:\n```\n`const admin = require('firebase-admin');\n\nadmin.initializeApp();\n\nexports.helloWorld = (req, res) => {\n  execute(res);\n};\n\nconst execute = async (res) => {\n  const email = 'test@test.com';\n  const url = 'https://example.firebaseapp.com';\n  const link = await admin.auth().generateEmailVerificationLink(email, { url });\n  console.log(link);\n  res.status(200).send(link);\n};\n\n`\n```\npackage.json:\n```\n`{\n  \"name\": \"sample-http\",\n  \"version\": \"0.0.1\",\n  \"dependencies\": {\n    \"firebase-admin\": \"^10.0.2\"\n  }\n}\n\n`\n```\nMy Firebase Admin Service Account (`firebase-adminsdk-XXX@example.iam.gserviceaccount.com`) has the roles:\n\nFirebase Admin SDK Administrator Service Agent\nService Account Token Creator\n\nI also viewed the API Key in Firebase Console, found it in GCP (`Browser key (auto created by Firebase)`, and see that it has the following APIs selected:\n\nCloud Firestore API\nCloud Functions API\nFirebase Installations API\nToken Service API\nIdentity Toolkit API\n\nI tried following the provided link (https://firebase.google.com/docs/admin/setup), but it seems specific to setting up `admin` outside of a GCP Cloud Function (see https://firebase.google.com/docs/admin/setup#initialize-without-parameters).  I also read through  https://firebase.google.com/docs/auth/admin/email-action-links, but there were no helpful details that I could find.\nI tried using `functions.https.onCall` instead of the regular GCP exports.\nI tried setting `FIREBASE_CONFIG={\"projectId\":\"example\",\"storageBucket\":\"example.appspot.com\",\"locationId\":\"\"}` and `GCLOUD_PROJECT=example` as runtime env vars.",
      "solution": "The issue was that because I was deploying the function on GCP (and not through Firebase), the actual service account that runs the function is not the one specified in the Firebase console (`firebase-adminsdk-XXX@example.iam.gserviceaccount.com`), it is instead the App Engine default service account:\n\nAt runtime, Cloud Functions defaults to using the App Engine default\nservice account (PROJECT_ID@appspot.gserviceaccount.com)\nSource:\nhttps://cloud.google.com/functions/docs/concepts/iam#access_control_for_service_accounts\n\nSo in this case, the call worked once I gave my account `example@appspot.gserviceaccount.com` the roles:\n\nFirebase Admin SDK Administrator Service Agent\nService Account Token Creator\n\nAs a side note, no additional options were needed for `admin.initializeApp()` nor were the Runtime Environment Variables needed.",
      "question_score": 5,
      "answer_score": 5,
      "created_at": "2022-11-09T18:28:40",
      "url": "https://stackoverflow.com/questions/74379026/error-on-firebase-generateemailverificationlink-in-gcp-cloud-function"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 73693752,
      "title": "Is it possible to specify input and return types for httpsCallable functions?",
      "problem": "I am using a callable function in app to update user claims. The firebase functions are in Typescript and there is an interface to describe the shape of data that the function requires.\nI would like to do the same thing client side, so that any developer on the team can quickly find out what the requirements for the cloud function are, without looking at the code in the functions directory.\nThe Firebase Cloud Function in `functions/src/index.ts`:\n`\n// This is the required data, that I would like to specify client side\ninterface GivePermissionsParams {\n  uid: string;\n  email: string;\n  newClaim: Partial;\n}\n\n/**\n * Callable function to give a user new permissions in the form\n * of custom claims and firestore properties\n */\nexports.givePermission = functions.https.onCall(\n  async (data: GivePermissionsParams, context) => {\n    if (!context.auth?.token.admin) {\n      throw new HttpsError(\n        'permission-denied',\n        `Non admin user ${context.auth?.uid} attempted to update permissions`\n      );\n    }\n    return grantPermission(data.uid, data.newClaim).then(() => {\n      log(`Successfully updated permissions for ${data.email}`);\n      return {\n        result: `Successfully updated permissions for ${data.email}`,\n      };\n    });\n  }\n);\n`\nClient side usage:\n`// firebase.ts\n\n// I would like to specify the function param and return types here.\n// eg: httpsCallable\nexport const givePermission = httpsCallable(functions, 'givePermission');\n`\n`// in my reactComponent.tsx\n\n  const changePermission = async (permission: string, value: boolean) => {\n\n    // This payload should match the GivePermissionsParams type as defined in the functions index.ts file. \n    const functionParams = {\n      uid: user.uid,\n      email: user.email,\n      newClaim: {[permission]: value}\n    }\n\n    const functionRes = await givePermission(functionParams);\n  };\n`",
      "solution": "It seems the solution is what you are trying to do. You can specify types for for request data and response like this:\n```\n`interface ReqInterface {\n  uid: string;\n  email: string;\n  newClaim: Partial;\n}\n\ninterface ResInterface {\n  result: string;\n}\n\nconst givePermission = httpsCallable(functions, 'givePermission')\n\nconst { data } = await givePermission({ url })\n// data is ResInterface\n`\n```",
      "question_score": 5,
      "answer_score": 5,
      "created_at": "2022-09-12T20:28:55",
      "url": "https://stackoverflow.com/questions/73693752/is-it-possible-to-specify-input-and-return-types-for-httpscallable-functions"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 72685175,
      "title": "Application Default Credentials http trigger GCP function from local nodejs application",
      "problem": "I want to trigger a GCP cloud function from a simple nodejs app running locally.\nReading the documentation it should be simple:\n\nrun `gcloud auth application-default login` to write ADC to file used by client libraries.\nuse google-auth-library to get a http client to use to trigger the function.\n\n`/**\n * TODO(developer): Uncomment these variables before running the sample.\n */\n// Example: https://my-cloud-run-service.run.app/books/delete/12345\n// const url = 'https://TARGET_HOSTNAME/TARGET_URL';\n\n// Example (Cloud Functions): https://project-region-projectid.cloudfunctions.net/myFunction\nconst targetAudience = 'https://-.cloudfunctions.net/';\nconst { GoogleAuth } = require('google-auth-library');\n\nconst auth = new GoogleAuth();\nconst payload = {\"prop1\": \"prop1Value\"};\n\nasync function request() {\n  const client = await auth.getIdTokenClient(targetAudience);\n  const resp = await client.request({ url: targetAudience, method: 'POST', data: payload });\n  console.info(`Resp status: ${resp.status}; resp.data: ${resp.data}`);\n}\n\n(async () => {\n  await request();\n})();\n`\nMy understanding was that the google-auth-library would pick up the ADC from the file setup from running `gcloud auth application-default login` and everything would work.\nMy user has permission to invoke GCP functions as I can trigger the function using CURL with the header `-H \"Authorization:bearer $(gcloud auth print-identity-token)\" \\`\nHowever when I run this, it doesn't get past the line:\n```\n`const client = await auth.getIdTokenClient(targetAudience);\n`\n```\nFailing with:\n\nCannot fetch ID token in this environment, use GCE or set the GOOGLE_APPLICATION_CREDENTIALS environment variable t\no a service account credentials JSON file.\n\nUsing PubSub library works fine so expect ADC does work just not sure what am I missing when trying to trigger the GCP function.\nAm I using the google-auth-library correctly here ?\nThanks",
      "solution": "There are, of course, ways around your issue but fundamentally this library seems to be missing the support of `application-default` credentials locally - a feature that many other google libs support out of the box. Requiring service account JSON files locally is an older approach.\nThis appears like a bug or at least missing feature and I've raised an issue in the client lib: https://github.com/googleapis/google-auth-library-nodejs/issues/1543\nLocal development without this support means that all developers need to export a privileged service account JSON key, store it locally, and configure an environment variable. GCS, secrets manager, etc, etc all just work \"out-of-the-box\" with locally authenticated `application-default` credentials if you use `gcloud auth application-default login`. It would be a shame to force this burden, (and potential security issues with JSON keys floating around the place), on our local developers for one library so hopefully the issue gains traction.",
      "question_score": 5,
      "answer_score": 4,
      "created_at": "2022-06-20T11:51:30",
      "url": "https://stackoverflow.com/questions/72685175/application-default-credentials-http-trigger-gcp-function-from-local-nodejs-appl"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 67826093,
      "title": "How to handle Firebase Cloud Functions infinite loops?",
      "problem": "I have a Firebase Cloud functions which is triggered by an update to some data in a Firebase Realtime Database. When the data is updated, I want to read the data, perform some calculations on that data, and then save the results of the calculations back to the Realtime Database. It looks like this:\n```\n`exports.onUpdate = functions.database.ref(\"/some/path\").onUpdate((change) => {\n    const values = change.after.val();\n    const newValues = performCalculations(value);\n    return change.after.ref.update(newValues);\n});\n`\n```\nMy concern is that this may create an indefinite loop of updates. I saw a note on the Cloud Firestore Triggers that says:\n\n\"Any time you write to the same document that triggered a function,\nyou are at risk of creating an infinite loop. Use caution and ensure\nthat you safely exit the function when no change is needed.\"\n\nSo my first question is: Does this same problem apply to the Firebase Realtime Database?\nIf it does, what is the best way to prevent the infinite looping?\nShould I be comparing before/after snapshots, the key/value pairs, etc.?\nMy idea so far:\n```\n`exports.onUpdate = functions.database.ref(\"/some/path\").onUpdate((change) => {\n\n    // Get old values\n    const beforeValues = change.before.val();\n\n    // Get current values\n    const afterValues = change.after.val();\n\n    // Something like this???\n    if (beforeValues === afterValues) return null;\n    \n    const newValues = performCalculations(afterValues);\n    return change.after.ref.update(newValues);\n});\n`\n```\nThanks",
      "solution": "Does this same problem apply to the Firebase Realtime Database?\n\nYes, the chance of infinite loops occurs whenever you write back to the same location that triggered your Cloud Function to run, no matter what trigger type was used.\nTo prevent an infinite loop, you have to detect its condition in the code. You can:\n\neither flag the node/document after processing it by writing a value into it, and check for that flag at the start of the Cloud Function.\nor you can detect whether the Cloud Function code made any effective change/improvement to the data, and not write it back to the database when there was no change/improvement.\n\nEither of these can work, and which one to use depends on your use-case. Your `if (beforeValues === afterValues) return null` is a form of the second approach, and can indeed work - but that depends on details about the data that you haven't shared.",
      "question_score": 5,
      "answer_score": 5,
      "created_at": "2021-06-03T19:29:42",
      "url": "https://stackoverflow.com/questions/67826093/how-to-handle-firebase-cloud-functions-infinite-loops"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 71506858,
      "title": "What is the Correct Way to Use Secret Manager in Firebase Cloud Function?",
      "problem": "I started to add Secret Manager with a SECRET_NAME contains a certain password inside Cloud Function using Node.js. I tried both ways. First, adding Secret Manager using Console and another, adding Secret Manager directly through Firebase CLI. Unfortunately, both ways give an empty Secret value in Cloud Function variable of Secret as shown in picture below.\n\nI used parameter runWith({secret: [\"SECRET_NAME\"]}) as shown in code below.\n\r\n\r\n`exports.Auth = functions.runWith(secret).https.onCall(async (data, context) => {\n  const response = 129132;\n  return Promise.resolve(response);\n});\n\nconst secret = {\n  timeoutSeconds: 120,\n  memory: \"1GB\",\n  secret: [\"SECRET_NAME\"],\n  minInstances: 0,\n};`\r\n\r\n\r\n\nI followed the documentation written in Firebase Cloud Function (https://firebase.google.com/docs/functions/config-env).\nI had wasted almost a week to figure out where I got wrong and found nothing. I also added a role \"Secret Manager Secret Accessor\" to current SECRET_NAME in Google Cloud Console, and it gave result to nothing at all. As written in this stackoverflow post reference\nMy question is should I add SECRET_NAME manually in Cloud Function Console in Google Cloud Platform? If so, then what is the use of documentation written above? Is Firebase trying to lead us to nowhere?",
      "solution": "I am sorry guys.\n\nApparently, I was wrong in writing the code:\n\r\n\r\n`const secret = {\n  timeoutSeconds: 120,\n  memory: \"1GB\",\n  secrets: [\"SECRET_NAME\"], //Must use secrets not secret\n  minInstances: 0,\n};`\r\n\r\n\r\n\nThank you, @MichaelBleigh.\n\nProblem is now solved.",
      "question_score": 5,
      "answer_score": 4,
      "created_at": "2022-03-17T04:45:19",
      "url": "https://stackoverflow.com/questions/71506858/what-is-the-correct-way-to-use-secret-manager-in-firebase-cloud-function"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 70868056,
      "title": "Is deploying a Google Cloud Function with Artifact Registry faster than using Container Registry?",
      "problem": "An email I recently received from GCP mentions the transition to Artifact Registry for Cloud Functions.\nIt claims:\n\nCloud Functions for Firebase and Firebase Extensions have historically\nused Container Registry for packaging functions and managing their\ndeployment, yet with the change to Artifact Registry, you\u2019ll have the\nfollowing benefits:\nYour functions will deploy faster.\nYou\u2019ll have access to more regions.\n\nI cannot find any more information regarding faster deployments, either from official documentation or from user experiences.\nIs there any reason to believe Cloud Function deployment will actually be faster, by an appreciable margin? Currently function deployment is glacial, so even a small speedup in percentage terms would shave minutes off deployment times.",
      "solution": "I'm personally surprise of that \"faster\" deployment mention, because, in reality, it won't.\nTo explain that, you simply have to review the deployment process:\n\nYou submit your code\nYour code is packaged in a container (with Cloud Build and Buildpack) and stored somewhere (in container registry or artifact registry)\nThe code is deployed on the target service.\n\nIf you take the duration of each step, in percentage you can have:\n\n0.5% (depend on your network)\n99% (depend on the build to perform, can take long minutes to compile/minify,...)\n0.5% (Even if the container is \"big\", the petabyte network is wonderful).\n\nSo, yes, you have more regions, and, by the way, if you have a large container to deploy, in a non supported region, the data transfer with take more ms, even a few seconds.\n\nAll of that to say, yes, you can save few seconds, but it's not always the case.",
      "question_score": 5,
      "answer_score": 4,
      "created_at": "2022-01-26T18:51:55",
      "url": "https://stackoverflow.com/questions/70868056/is-deploying-a-google-cloud-function-with-artifact-registry-faster-than-using-co"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 68680888,
      "title": "Deploy Next.js app to Firebase Functions with existing functions",
      "problem": "Following a few tutorials like this one, I was able to deploy my `Next.js` app to Firebase Hosting/Functions. Here's the reduced folder structure of my project:\n```\n`myApp\n  node_modules // Next.js app dependencies\n  .next // destination of npm run build of the Next.js project\n  pages // Next.js pages\n  functions\n    lib // this is where the functions compile\n    node_modules // functions' dependencies\n    src\n      app.ts\n      index.ts\n      other.ts\n    package.json\n  public // Next.js app public files\n  firebase.json\n  package.json\n  ...\n`\n```\nWhen I run `firebase deploy`, it compiles `functions` into its `lib` folder and deploys everything successfully.\nHowever, when I try to visit the url of my web app, I get `Error: could not handle the request`. When I looked into the Firebase Functions log, I can see an error saying `Error: Cannot find module 'react'`. So, I added `react` to `functions/package.json`, `npm install` and `firebase deploy` successfully again. However, another error, but this time it says `Cannot find module 'react-dom'`.\nMy understanding is that the reason for this is that my Next.js app relies on all the dependencies listed in my `package.json` in my `root` folder (a reduced list below):\n```\n`\"dependencies\": {\n    \"babel-plugin-prismjs\": \"^2.1.0\",\n    \"firebase\": \"^8.8.1\",\n    \"firebase-admin\": \"^9.11.0\",\n    \"formik\": \"^2.2.9\",\n    \"js-cookie\": \"^3.0.0\",\n    \"next\": \"11.0.1\",\n    \"next-i18next\": \"^8.5.5\",\n    \"nookies\": \"^2.5.2\",\n    \"prismjs\": \"^1.24.1\",\n    \"react\": \"17.0.2\",\n    \"react-dom\": \"17.0.2\",\n    \"react-ga\": \"^3.3.0\",\n    \"react-select\": \"^4.3.1\",\n    \"react-spinners\": \"^0.11.0\",\n    \"react-transition-group\": \"^4.4.2\",\n    \"recharts\": \"^2.0.10\",\n    \"styled-components\": \"^5.3.0\",\n    \"yup\": \"^0.32.9\"\n  }\n`\n```\nwhile my `functions/package.json` was, obviously, much shorter:\n```\n`\"dependencies\": {\n    \"@sendgrid/mail\": \"^7.4.4\",\n    \"firebase-admin\": \"^9.8.0\",\n    \"firebase-functions\": \"^3.14.1\",\n    \"next\": \"^11.0.1\",\n    \"react\": \"^17.0.2\"\n  }\n`\n```\nI assume the errors would have asked me to duplicate all the dependencies into the `functions/package.json` file, which I obviously don't want.\nAlternatively, I can add `firebase-functions` and other relevant dependencies from `functions/package.json` to `root/package.json`, and have the next server setup in the root folder as well. However, how do I include the other existing cloud functions?\nHere's how the contents of my `functions/src` files look:\nindex.ts\n```\n`import * as admin from 'firebase-admin'\n\nadmin.initializeApp()\n\nexport * from './app' // next.js\nexport * from './other'\n`\n```\nother.ts\n```\n`import * as functions from 'firebase-functions'\nimport * as admin from 'firebase-admin'\n\nexports.onUserCreate = functions.database.ref...\n// my other cloud functions, such as db triggers, https functions, etc.\n`\n```\napp.ts\n```\n`import next from 'next'\nimport {https} from 'firebase-functions'\n\nconst server = next({\n  dev: process.env.NODE_ENV !== 'production',\n  conf: {distDir: '.next'}\n})\n\nconst nextjsHandler = server.getRequestHandler()\n\nexports.app = https.onRequest(async (req, res) => {\n  await server.prepare()\n  return await nextjsHandler(req, res)\n})\n`\n```\nAlso, here's my `firebase.json`:\n```\n`{\n  \"functions\": {\n    \"predeploy\": [\n      \"npm --prefix \\\"$RESOURCE_DIR\\\" run lint\",\n      \"npm --prefix \\\"$RESOURCE_DIR\\\" run build\"\n    ]\n  },\n  \"hosting\": {\n    \"target\": \"myApp\",\n    \"public\": \"public\",\n    \"ignore\": [\"firebase.json\", \"**/.*\", \"**/node_modules/**\"],\n    \"rewrites\": [{\n      \"source\": \"**\",\n      \"function\": \"app\"\n    }]\n  }\n}\n`\n```\nHow do I deploy the `Next.js` app correctly and make sure my other cloud functions continue to work?\nThanks!",
      "solution": "While you could lift the `/functions` directory to your project's root directory, this would mean that your other functions are unnecessarily bloated with all of your `Next.js` app's dependencies.\nWhen you deploy a Cloud Function, everything in the configured deployment directory (`/functions` by default) is deployed - even if you don't use it. It is also deployed once for each function. So if you had a 10MB file to your functions directory, every function's deployed size would increase by 10MB. This is what I mean by \"bloating\" a function.\nFYI: You can change the deployed functions directory by adding the following to your `firebase.json` file (docs):\n`\"functions\": {\n  \"source\": \".\" // \nInstead, you should make use of partial deployment. In this situation you'd move `app.ts` to your Next project's directory and leave the other functions that don't depend on your Next deployment in the `/functions` folder. If you deploy your entire project directory, you will end up deploying `/functions` too even though you don't use it, so you should instead move your Next front-end and functions into its own folder.\nThe file structure will look similar to (details below):\n`/myApp\n  /functions\n    /lib            // this is where the functions compile\n    /node_modules   // functions' dependencies\n    /src\n      index.ts\n      other.ts\n    firebase.json   // configuration for this folder's contents\n    package.json    // configuration for this folder's contents\n  /next\n    /node_modules   // Next.js app dependencies\n    /.next          // compiled Next.js project\n    /pages          // Next.js pages\n    /public         // Next.js app public files\n    /functions-lib\n      app.js        // compiled Next function\n    /functions-src\n      app.ts        // source of Next function\n    firebase.json   // configuration for this folder's contents\n    package.json    // configuration for this folder's contents\n    ...\n`\nYour `/functions` folder's `firebase.json` file would look similar to:\n`{\n  \"functions\": {\n    \"source\": \".\",\n    \"predeploy\": [\n      \"npm --prefix \\\"$RESOURCE_DIR\\\" run lint\",\n      \"npm --prefix \\\"$RESOURCE_DIR\\\" run build\"\n    ]\n  }\n}\n`\nTo get the above structure to behave properly, you need to tweak the configuration of the `firebase.json` and `package.json` files.\nAdd the following to the `/function` folder's `package.json` file:\n`\"scripts\": {\n  \"deploy\": \"firebase deploy --only functions:otherFunc1,functions:otherFunc2\",\n  ...\n}\n`\nYou could also export these functions as a single group for easy deployment, but the functions will be named like `otherFunctions-otherFunc1`, `otherFunctions-otherFunc2`, and so on:\n`// index.ts\nimport * as otherFunctions from './other.ts';\nexport { otherFunctions };\n`\n`// package.json\n\"scripts\": {\n  \"deploy\": \"firebase deploy --only functions:otherFunctions\",\n  ...\n}\n`\nYour `/next` folder's `firebase.json` file would look similar to:\n`{\n  \"functions\": {\n    \"source\": \".\",\n    \"predeploy\": [\n      \"npm --prefix \\\"$RESOURCE_DIR\\\" run lint\",\n      \"npm --prefix \\\"$RESOURCE_DIR\\\" run build\"\n    ]\n  },\n  \"hosting\": {\n    \"target\": \"myApp\",\n    \"public\": \"public\",\n    \"ignore\": [\"firebase.json\", \"**/.*\", \"**/node_modules/**\"],\n    \"rewrites\": [{\n      \"source\": \"**\",\n      \"function\": \"app\"\n    }]\n  }\n}\n`\nAdd the following (or similar) to the `/next` folder's `package.json` file:\n`\"scripts\": {\n  \"build\": \"npm run build:next && npm run build:functions\",\n  \"build:functions\": \"tsc\",\n  \"build:next\": \"next build\",\n  \"lint\": \"npm run lint:next && npm run lint:functions\",\n  \"lint:functions\": \"eslint --ext .js,.ts ./functions-src\",\n  \"lint:next\": \"...\",\n  \"deploy\": \"firebase deploy --only hosting,functions:app\"\n}\n`\nWith the above changes, drop into the appropriate directory and use `npm run deploy` whenever you would use `firebase deploy`.\n\nNote: Don't forget that Cloud Functions consume the bodies of requests automatically, so make sure you account for this when using body parsers in Next projects and testing locally with `next start`.",
      "question_score": 5,
      "answer_score": 4,
      "created_at": "2021-08-06T13:32:00",
      "url": "https://stackoverflow.com/questions/68680888/deploy-next-js-app-to-firebase-functions-with-existing-functions"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 76153948,
      "title": "Firebase Functions with sub path imports outside of project",
      "problem": "I have a firebase functions project with typescript. In this project I use types outside of the project with sub path imports, the result of this is that the build files are skewed.\ninstead of `main:lib/index.js` I have `main:lib/functions/src/index.js`\n`functions/lib`:\n```\n`petertoth@Peters-MBP-2 lib % tree .\n.\n\u251c\u2500\u2500 functions\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 src\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 index.js\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 index.js.map\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 journalLogs.type.js\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 journalLogs.type.js.map\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 util\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 audiFiles.js\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 audiFiles.js.map\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 db.js\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 db.js.map\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 getJournalSettings.js\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 getJournalSettings.js.map\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 prompt.js\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 prompt.js.map\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 storage.js\n\u2502\u00a0\u00a0         \u2514\u2500\u2500 storage.js.map\n\u2514\u2500\u2500 types\n    \u251c\u2500\u2500 firebase\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 CreateFullRecording.request.js\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 CreateFullRecording.request.js.map\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 generateJournal.requests.js\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 generateJournal.requests.js.map\n    \u251c\u2500\u2500 firestore\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 JournalSettingsDoc.js\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 JournalSettingsDoc.js.map\n    \u2514\u2500\u2500 openai\n        \u251c\u2500\u2500 language.js\n        \u2514\u2500\u2500 language.js.map\n`\n```\npackage.json:\n```\n`{\n  \"name\": \"functions\",\n ...\n \"imports\": {\n    \"#types/*\": [\"../types/*\"]\n  },\n  \"engines\": {\n    \"node\": \"16\"\n  },\n  \"main\": \"lib/functions/src/index.js\",\n  \"private\": true\n  ...\n}\n`\n```\ntsconfig:\n```\n`{\n  \"compilerOptions\": {\n    \"module\": \"commonjs\",\n    \"noImplicitReturns\": true,\n    \"noUnusedLocals\": true,\n    \"outDir\": \"lib\",\n    \"sourceMap\": true,\n    \"strict\": true,\n    \"target\": \"es2017\",\n    \"moduleResolution\": \"nodenext\",\n    \"paths\": {\n      \"#types/*\": [\"../types/*\"]\n    }\n  },\n  \"baseUrl\": \".\",\n  \"compileOnSave\": true,\n  \"include\": [\n    \"src\"\n  ]\n}\n`\n```\nthis works well locally, and I can serve it wonderfully. However when I try to deploy:\n` firebase deploy --only functions` I'm getting the following error:\n```\n`i  deploying functions\nRunning command: npm --prefix \"$RESOURCE_DIR\" run build\n\n> build\n> tsc\n\n\u2714  functions: Finished running predeploy script.\ni  functions: preparing codebase default for deployment\ni  functions: ensuring required API cloudfunctions.googleapis.com is enabled...\ni  functions: ensuring required API cloudbuild.googleapis.com is enabled...\ni  artifactregistry: ensuring required API artifactregistry.googleapis.com is enabled...\n\u2714  functions: required API cloudfunctions.googleapis.com is enabled\n\u2714  artifactregistry: required API artifactregistry.googleapis.com is enabled\n\u2714  functions: required API cloudbuild.googleapis.com is enabled\ni  functions: preparing ./functions/ directory for uploading...\ni  functions: packaged XX (98.13 KB) for uploading\n\u2714  functions: ./functions/ folder uploaded successfully\ni  functions: updating Node.js 16 function generateJournal(europe-west1)...\ni  functions: updating Node.js 16 function migrageJournal(europe-west1)...\ni  functions: updating Node.js 16 function getCollectionNames(europe-west1)...\ni  functions: updating Node.js 16 function createFullRecording(europe-west1)...\nBuild failed: > build\n> tsc\n\nsrc/index.ts(22,44): error TS2307: Cannot find module '#types/firebase/CreateFullRecording.request' or its corresponding type declarations.\nsrc/index.ts(25,40): error TS2307: Cannot find module '#types/firebase/generateJournal.requests' or its corresponding type declarations.\nsrc/util/getJournalSettings.ts(1,36): error TS2307: Cannot find module '#types/firestore/JournalSettingsDoc' or its corresponding type declarations.\nsrc/util/prompt.ts(1,36): error TS2307: Cannot find module '#types/firestore/JournalSettingsDoc' or its corresponding type declarations.\nsrc/util/prompt.ts(2,30): error TS2307: Cannot find module '#types/openai/language' or its corresponding type declarations.\nsrc/util/prompt.ts(15,31): error TS18046: 'journalBullet' is of type 'unknown'.\nsrc/util/prompt.ts(16,34): error TS18046: 'journalBullet' is of type 'unknown'.\nsrc/util/prompt.ts(48,10): error TS7053: Element implicitly has an 'any' type because expression of type 'LanguageCode' can't be used to index type '{ en: string; da: string; sv: string; no: string; de: string; }'.; Error ID: 1a2262f3\n`\n```\nI think there is something wrong with the config of the packaging of files to upload. Just a hunch though and I do not know how to debug this more.",
      "solution": "Update your Firebase CLI: `npm install -g firebase-tools`\nGoogle Cloud Functions now runs `npm run build` during deployment, which seems to cause the issue.\nHowever, as per this comment by colerogers, the Firebase team patched their CLI to disable this feature in firebase-tools v11.27.0.\n\nIf you upgrade to at least that version, this error should go away without any additional work.",
      "question_score": 5,
      "answer_score": 2,
      "created_at": "2023-05-02T12:28:18",
      "url": "https://stackoverflow.com/questions/76153948/firebase-functions-with-sub-path-imports-outside-of-project"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "google-cloud-functions",
      "question_id": 75883705,
      "title": "Why are we suddenly getting &quot;Function execution took 60000 ms, finished with status: &#39;error&#39;&quot; on Firebase callable functions / google cloud functions?",
      "problem": "Our app is in production since the March 22nd. It makes use of Firebase callable functions. The execution of the functions were flawless until suddenly on March 28th, we started to observe many failed executions get the following debug messages in the cloud functions logs `Function execution took 60000 ms, finished with status: 'error'`.\nWhile some executions of the same function failed, others worked as usual and completed in on average 250ms.\nWhen checking the cloud platform infrastructure status, there were no indicators, of any outages or any other issues.\nFor instance we have this very simple function:\n```\n` exports.getAccommodations = functions\n  .region('asia-east1')\n  .https.onCall(async (data, context) => {\n    functions.logger.warn(`V3 STARTED`);\n    // load some collection from firestore here and return it\n  });\n`\n```\nNote that the very first line of the function is creating a log warn entry. We were able to observe the following:\n\nFor failed executions with the above error message, there was no log.warn entry.\nFor succeeded executions, there was always an according log.warn entry.\n\nIn other words, it looks like failed executions, do not even bother to execute the functions body. All we see in the failure case is `Function Execution started` followed by `Function execution took 60000 ms, finished with status: 'error'`.\nThese are our logs, corroborating our observations:\n\nWhen filtering for the failure case: `(resource.type=\"cloud_function\"  ) OR (resource.type=\"cloud_run_revision\"  ) textPayload=\"Function execution took 60000 ms, finished with status: 'error'` we get: \n\nWhen listing all executions `(resource.type=\"cloud_function\"  ) OR (resource.type=\"cloud_run_revision\"  )` we get: \n\nNote that we have not deployed anything around the time the issues started to show.",
      "solution": "Seeing the same issue, most calls end up with status code `408` and have much higher latency than we usually see.\nThink it's related to this, which is very broadly phrased:\n\nSummary: Google Cloud Networking experiencing elevated latency in multiple regions\n\nhttps://status.cloud.google.com/incidents/nieR2aLyg1rwFKq1aWZU#2c2sBHWU84yPDJ8y1ar4",
      "question_score": 5,
      "answer_score": 3,
      "created_at": "2023-03-30T04:44:05",
      "url": "https://stackoverflow.com/questions/75883705/why-are-we-suddenly-getting-function-execution-took-60000-ms-finished-with-sta"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "bigquery",
      "question_id": 72511979,
      "title": "ValueError: install DBtypes to use this function",
      "problem": "I'm using BigQuery for the first time.\n`client.list_rows(table, max_results = 5).to_dataframe();\n`\nWhenever I use `to_dataframe()` it raises this error:\n\nValueError: Please install the 'db-dtypes' package to use this function.\n\nI found this similar problem (almost exactly the same), but I can't understand how to implement their proposed solution.",
      "solution": "I was able to replicate your use case as shown below.\n\nEasiest solution is to `pip install db-dtypes` as mentioned by @MattDMo.\nOr you can specify previous version of `google-cloud-bigquery` by creating a `requirements.txt` with below contents:\n```\n`google-cloud-bigquery==2.34.3\n`\n```\nAnd then pip install by using command as shown below:\n```\n`pip install -r /path/to/requirements.txt\n`\n```\nOutput of my sample replication:",
      "question_score": 37,
      "answer_score": 20,
      "created_at": "2022-06-06T02:26:04",
      "url": "https://stackoverflow.com/questions/72511979/valueerror-install-dbtypes-to-use-this-function"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "bigquery",
      "question_id": 71563696,
      "title": "Pandas to_gbq() TypeError &quot;Expected bytes, got a &#39;int&#39; object",
      "problem": "I am using the `pandas_gbq` module to try and append a dataframe to a table in Google BigQuery.\nI keep getting this error:\n\nArrowTypeError: Expected bytes, got a 'int' object.\n\nI can confirm the data types of the dataframe match the schema of the BQ table.\nI found this post regarding Parquet files not being able to have mixed datatypes: Pandas to parquet file\nIn the error message I'm receiving, I see there is a reference to a Parquet file, so I'm assuming the `df.to_gbq()` call is creating a Parquet file and I have a mixed data type column, which is causing the error. The error message doesn't specify.\nI think that my challenge is that I can't see to find which column has the mixed datatype - I've tried casting them all as strings and then specifying the table schema parameter, but that hasn't worked either.\nThis is the full error traceback:\n```\n`In [76]: df.to_gbq('Pricecrawler.Daily_Crawl_Data', project_id=project_id, if_exists='append')\nArrowTypeError                            Traceback (most recent call last)\n in \n----> 1 df.to_gbq('Pricecrawler.Daily_Crawl_Data', project_id=project_id, if_exists='append')\n\n~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py in to_gbq(self, destination_table, \nproject_id, chunksize, reauth, if_exists, auth_local_webserver, table_schema, location, \nprogress_bar, credentials)\n   1708         from pandas.io import gbq\n   1709\n-> 1710         gbq.to_gbq(\n   1711             self,\n   1712             destination_table,\n\n~\\Anaconda3\\lib\\site-packages\\pandas\\io\\gbq.py in to_gbq(dataframe, destination_table, project_id, chunksize, reauth, if_exists, auth_local_webserver, table_schema, location, progress_bar, credentials)\n    209 ) -> None:\n    210     pandas_gbq = _try_import()\n--> 211     pandas_gbq.to_gbq(\n    212         dataframe,\n    213         destination_table,\n\n~\\Anaconda3\\lib\\site-packages\\pandas_gbq\\gbq.py in to_gbq(dataframe, destination_table, project_id, chunksize, reauth, if_exists, auth_local_webserver, table_schema, location, progress_bar, credentials, api_method, verbose, private_key)\n   1191         return\n   1192\n-> 1193     connector.load_data(\n   1194         dataframe,\n   1195         destination_table_ref,\n\n~\\Anaconda3\\lib\\site-packages\\pandas_gbq\\gbq.py in load_data(self, dataframe, destination_table_ref, chunksize, schema, progress_bar, api_method, billing_project)\n    584\n    585         try:\n--> 586             chunks = load.load_chunks(\n    587                 self.client,\n    588                 dataframe,\n\n~\\Anaconda3\\lib\\site-packages\\pandas_gbq\\load.py in load_chunks(client, dataframe, destination_table_ref, chunksize, schema, location, api_method, billing_project)\n    235 ):\n    236     if api_method == \"load_parquet\":\n--> 237         load_parquet(\n    238             client,\n    239             dataframe,\n\n~\\Anaconda3\\lib\\site-packages\\pandas_gbq\\load.py in load_parquet(client, dataframe, destination_table_ref, location, schema, billing_project)\n    127\n    128     try:\n--> 129         client.load_table_from_dataframe(\n    130             dataframe,\n    131             destination_table_ref,\n\n~\\Anaconda3\\lib\\site-packages\\google\\cloud\\bigquery\\client.py in load_table_from_dataframe(self, dataframe, destination, num_retries, job_id, job_id_prefix, location, project, job_config, parquet_compression, timeout)\n   2669                         parquet_compression = parquet_compression.upper()\n   2670\n-> 2671                     _pandas_helpers.dataframe_to_parquet(\n   2672                         dataframe,\n   2673                         job_config.schema,\n\n~\\Anaconda3\\lib\\site-packages\\google\\cloud\\bigquery\\_pandas_helpers.py in dataframe_to_parquet(dataframe, bq_schema, filepath, parquet_compression, parquet_use_compliant_nested_type)\n    584\n    585     bq_schema = schema._to_schema_fields(bq_schema)\n--> 586     arrow_table = dataframe_to_arrow(dataframe, bq_schema)\n    587     pyarrow.parquet.write_table(\n    588         arrow_table, filepath, compression=parquet_compression, **kwargs,\n\n~\\Anaconda3\\lib\\site-packages\\google\\cloud\\bigquery\\_pandas_helpers.py in dataframe_to_arrow(dataframe, bq_schema)\n    527         arrow_names.append(bq_field.name)\n    528         arrow_arrays.append(\n--> 529             bq_to_arrow_array(get_column_or_index(dataframe, bq_field.name), bq_field)\n    530         )\n    531         arrow_fields.append(bq_to_arrow_field(bq_field, arrow_arrays[-1].type))\n\n~\\Anaconda3\\lib\\site-packages\\google\\cloud\\bigquery\\_pandas_helpers.py in bq_to_arrow_array(series, bq_field)\n    288     if field_type_upper in schema._STRUCT_TYPES:\n    289         return pyarrow.StructArray.from_pandas(series, type=arrow_type)\n--> 290     return pyarrow.Array.from_pandas(series, type=arrow_type)\n    291\n    292\n\n~\\Anaconda3\\lib\\site-packages\\pyarrow\\array.pxi in pyarrow.lib.Array.from_pandas()\n\n~\\Anaconda3\\lib\\site-packages\\pyarrow\\array.pxi in pyarrow.lib.array()\n\n~\\Anaconda3\\lib\\site-packages\\pyarrow\\array.pxi in pyarrow.lib._ndarray_to_array()\n\n~\\Anaconda3\\lib\\site-packages\\pyarrow\\error.pxi in pyarrow.lib.check_status()\n\nArrowTypeError: Expected bytes, got a 'int' object\n`\n```",
      "solution": "Had this same issue - solved it simply with\n```\n`df = df.astype(str)\n`\n```\nand doing `to_gbq` on that instead.\nCaveat is that all your fields will now be strings...",
      "question_score": 16,
      "answer_score": 12,
      "created_at": "2022-03-21T21:24:01",
      "url": "https://stackoverflow.com/questions/71563696/pandas-to-gbq-typeerror-expected-bytes-got-a-int-object"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "bigquery",
      "question_id": 70013949,
      "title": "BigQuery: 404 &quot;Table is truncated.&quot; when insert right after truncate",
      "problem": "I truncate my table by executing a queryJob described here: https://cloud.google.com/bigquery/docs/quickstarts/quickstart-client-libraries\n```\n`\"truncate table \" + PROJECT_ID + \".\" + datasetName + \".\" + tableName;\n`\n```\ni wait until the job finishes via\n```\n`queryJob = queryJob.waitFor();\n`\n```\nTruncate works fine.\nAnyway, if i do an insert right after the truncate operation via\n```\n`InsertAllResponse response = table.insert(rows);\n`\n```\nit results in a\n```\n`com.google.cloud.bigquery.BigQueryException: Table is truncated.\n`\n```\nwith following log:\n```\n`Caused by: com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found\n    POST https://www.googleapis.com/bigquery/v2/projects/[MYPROJECTID]/datasets/[MYDATASET]/tables/[MYTABLE]/insertAll?prettyPrint=false\n    {\n      \"code\" : 404,\n      \"errors\" : [ {\n        \"domain\" : \"global\",\n        \"message\" : \"Table is truncated.\",\n        \"reason\" : \"notFound\"\n      } ],\n      \"message\" : \"Table is truncated.\",\n      \"status\" : \"NOT_FOUND\"\n    }\n`\n```\nSometimes i have even to wait more than 5 Minutes between truncate and insert.\nI would like to check if my table is still in the state \"Table is truncated.\" periodically until this state is gone.\nHow can i request bigquery api in order to check if the table is ready for inserts?\nHow can i request bigquery api for get the status of the table?\nEdit\nexample for reproduce can be found here",
      "solution": "Unfortunately the api does not (yet?) provide an endpoint to check the truncated state of the table.\nIn order to avoid this issue, one can use a load job via gc storage.\nIt looks like the load job respects this state, as i have no issues with truncate/load multiple times in a row.\n```\n`public void load(String datasetName, String tableName, String sourceUri) throws InterruptedException {\n    Table table = getTable(datasetName, tableName);\n\n    Job job = table.load(FormatOptions.json(), sourceUri);\n    // Wait for the job to complete\n\n    Job completedJob = job.waitFor(RetryOption.initialRetryDelay(Duration.ofSeconds(1)),\n            RetryOption.totalTimeout(Duration.ofMinutes(3)));\n    if (completedJob != null && completedJob.getStatus().getError() == null) {\n        // Job completed successfully\n    } else {\n        // Handle error case\n    }\n}\n`\n```",
      "question_score": 15,
      "answer_score": 3,
      "created_at": "2021-11-18T03:41:44",
      "url": "https://stackoverflow.com/questions/70013949/bigquery-404-table-is-truncated-when-insert-right-after-truncate"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "bigquery",
      "question_id": 75113742,
      "title": "Improving performance for a nested for loop iterating over dates",
      "problem": "I am looking to learn how to improve the performance of code over a large dataframe (10 million rows) and my solution loops over multiple dates `(2023-01-10, 2023-01-20, 2023-01-30)` for different combinations of `category_a` and `category_b`.\nThe working approach is shown below, which iterates over the dates for different pairings of the two-category data by first locating a subset of a particular pair. However, I would want to refactor it to see if there is an approach that is more efficient.\nMy input (`df`) looks like:\n\ndate\ncategory_a\ncategory_b\noutflow\nopen\ninflow\nmax\nclose\nbuy\nrandom_str\n\n0\n2023-01-10\n4\n1\n1\n0\n0\n10\n0\n0\na\n\n1\n2023-01-20\n4\n1\n2\n0\n0\n20\nnan\nnan\na\n\n2\n2023-01-30\n4\n1\n10\n0\n0\n20\nnan\nnan\na\n\n3\n2023-01-10\n4\n2\n2\n0\n0\n10\n0\n0\nb\n\n4\n2023-01-20\n4\n2\n2\n0\n0\n20\nnan\nnan\nb\n\n5\n2023-01-30\n4\n2\n0\n0\n0\n20\nnan\nnan\nb\n\nwith 2 pairs `(4, 1)` and `(4,2)` over the days and my expected output (`results`) looks like this:\n\ndate\ncategory_a\ncategory_b\noutflow\nopen\ninflow\nmax\nclose\nbuy\nrandom_str\n\n0\n2023-01-10\n4\n1\n1\n0\n0\n10\n-1\n23\na\n\n1\n2023-01-20\n4\n1\n2\n-1\n23\n20\n20\n10\na\n\n2\n2023-01-30\n4\n1\n10\n20\n10\n20\n20\nnan\na\n\n3\n2023-01-10\n4\n2\n2\n0\n0\n10\n-2\n24\nb\n\n4\n2023-01-20\n4\n2\n2\n-2\n24\n20\n20\n0\nb\n\n5\n2023-01-30\n4\n2\n0\n20\n0\n20\n20\nnan\nb\n\nI have a working solution using pandas dataframes to take a subset then loop over it to get a solution but I would like to see how I can improve the performance of this using perhaps ;`numpy`, `numba`, `pandas-multiprocessing` or `dask`. Another great idea was to rewrite it in BigQuery SQL.\nI am not sure what the best solution would be and I would appreciate any help in improving the performance.\nMinimum working example\nThe code below generates the input dataframe.\n```\n`import pandas as pd\nimport numpy as np\n\n# prepare the input  df\ndf = pd.DataFrame({\n'date' : ['2023-01-10', '2023-01-20','2023-01-30', '2023-01-10', '2023-01-20','2023-01-30'] ,\n'category_a' : [4, 4,4,4, 4, 4] ,\n'category_b' : [1, 1,1, 2, 2,2] ,\n'outflow' : [1.0, 2.0,10.0, 2.0, 2.0, 0.0],\n'open' : [0.0, 0.0, 0.0, 0.0, 0.0, 0.0] ,\n'inflow' : [0.0, 0.0, 0.0, 0.0, 0.0, 0.0] ,\n'max' : [10.0, 20.0, 20.0 , 10.0, 20.0,  20.0] ,\n'close' : [0.0, np.nan,np.nan, 0.0, np.nan, np.nan] ,\n'buy' : [0.0, np.nan,np.nan, 0.0, np.nan,np.nan],\n'random_str' : ['a', 'a', 'a', 'b', 'b', 'b'] \n})\n\ndf['date'] = pd.to_datetime(df['date'])\n\n# get unique pairs of category_a and category_b in a dictionary\nunique_pairs = df.groupby(['category_a', 'category_b']).size().reset_index().rename(columns={0:'count'})[['category_a', 'category_b']].to_dict('records')\nunique_dates = np.sort(df['date'].unique())\n`\n```\nUsing this input dataframe and Numpy, the code below is what I am trying to optmizize.\n```\n`df = df.set_index('date')\nday_0 = unique_dates[0] # first date\n\n# Using Dictionary comprehension\n\nlist_of_numbers = list(range(len(unique_pairs)))\nmyset  = {key: None for key in list_of_numbers}\n\nfor count_pair, value in enumerate(unique_pairs):\n    \n    # pair of category_a and category_b\n    category_a = value['category_a']\n    category_b = value['category_b']\n\n    # subset the dataframe for the pair\n    df_subset = df.loc[(df['category_a'] == category_a) & (df['category_b'] == category_b)]\n\n    log.info(f\" running for {category_a} and {category_b}\")\n\n    # day 0\n    df_subset.loc[day_0, 'close'] = df_subset.loc[day_0, 'open'] + df_subset.loc[day_0, 'inflow'] - df_subset.loc[day_0, 'outflow']\n    \n    \n    # loop over single pair using date\n    for count, date in enumerate(unique_dates[1:], start=1):\n        previous_date = unique_dates[count-1]\n\n        df_subset.loc[date, 'open'] = df_subset.loc[previous_date, 'close']\n        df_subset.loc[date, 'close'] = df_subset.loc[date, 'open'] + df_subset.loc[date, 'inflow'] - df_subset.loc[date, 'outflow']\n\n        # check if closing value is negative, if so, set inflow to buy for next weeks deficit\n\n        if df_subset.loc[date, 'close']  df_subset.loc[date, 'max']:\n            df_subset.loc[previous_date, 'buy'] = 0\n        else:\n            df_subset.loc[previous_date, 'buy'] = df_subset.loc[date, 'inflow']\n        \n        df_subset.loc[date, 'inflow'] = df_subset.loc[previous_date, 'buy']\n        df_subset.loc[date, 'close'] = df_subset.loc[date, 'open'] + df_subset.loc[date, 'inflow'] - df_subset.loc[date, 'outflow']\n    \n    # store all the dataframes in a container myset\n    myset[count_pair] = df_subset\n    \n# make myset into a dataframe\nresult = pd.concat(myset.values()).reset_index(drop=False)\nresult\n\n`\n```\nAfter which we can check that the solution is the same as what we expected.\n```\n`from pandas.testing import assert_frame_equal\n\nexpected = pd.DataFrame({\n'date' : [pd.Timestamp('2023-01-10 00:00:00'), pd.Timestamp('2023-01-20 00:00:00'), pd.Timestamp('2023-01-30 00:00:00'), pd.Timestamp('2023-01-10 00:00:00'), pd.Timestamp('2023-01-20 00:00:00'), pd.Timestamp('2023-01-30 00:00:00')] ,\n'category_a' : [4, 4, 4, 4, 4, 4] ,\n'category_b' : [1, 1, 1, 2, 2, 2] ,\n'outflow' : [1, 2, 10, 2, 2, 0] ,\n'open' : [0.0, -1.0, 20.0, 0.0, -2.0, 20.0] ,\n'inflow' : [0.0, 23.0, 10.0, 0.0, 24.0, 0.0] ,\n'max' : [10, 20, 20, 10, 20, 20] ,\n'close' : [-1.0, 20.0, 20.0, -2.0, 20.0, 20.0] ,\n'buy' : [23.0, 10.0, np.nan, 24.0, 0.0, np.nan] ,\n'random_str' : ['a', 'a', 'a', 'b', 'b', 'b'] \n})\n\n# check that the result is the same as expected\nassert_frame_equal(result, expected)\n`\n```\nSQL to create first table\nThe solution can also be in sql, if so you can use the following code to create the initial table.\nI am busy trying to implement a solution in big query sql using a user defined function to keep the logic going too. This would be a nice approach to solving the problem too.\n```\n`WITH data AS (\n  SELECT \n    DATE '2023-01-10' as date, 4 as category_a, 1 as category_b, 1 as outflow, 0 as open, 0 as inflow, 10 as max, 0 as close, 0 as buy, 'a' as random_str\n  UNION ALL\n  SELECT \n    DATE '2023-01-20' as date, 4 as category_a, 1 as category_b, 2 as outflow, 0 as open, 0 as inflow, 20 as max, NULL as close, NULL as buy, 'a' as random_str\n  UNION ALL\n  SELECT \n    DATE '2023-01-30' as date, 4 as category_a, 1 as category_b, 10 as outflow, 0 as open, 0 as inflow, 20 as max, NULL as close, NULL as buy, 'a' as random_str\n  UNION ALL\n  SELECT \n    DATE '2023-01-10' as date, 4 as category_a, 2 as category_b, 2 as outflow, 0 as open, 0 as inflow, 10 as max, 0 as close, 0 as buy, 'b' as random_str\n  UNION ALL\n  SELECT \n    DATE '2023-01-20' as date, 4 as category_a, 2 as category_b, 2 as outflow, 0 as open, 0 as inflow, 20 as max, NULL as close, NULL as buy, 'b' as random_str\n  UNION ALL\n  SELECT \n    DATE '2023-01-30' as date, 4 as category_a, 2 as category_b, 0 as outflow, 0 as open, 0 as inflow, 20 as max, NULL as close, NULL as buy, 'b' as random_str\n)\n\nSELECT \n  ROW_NUMBER() OVER (ORDER BY date) as \" \",\n  date,\n  category_a,\n  category_b,\n  outflow,\n  open,\n  inflow,\n  max,\n  close,\n  buy,\n  random_str\nFROM data\n`\n```",
      "solution": "Efficient algorithm\nFirst of all, the complexity of the algorithm can be improved. Indeed, `(df['category_a'] == category_a) & (df['category_b'] == category_b)` travels the whole dataframe and this is done for each item in `unique_pairs`. The running time is `O(U R)` where `U = len(unique_pairs)` and `R = len(df)`.\nAn efficient solution is to perform a groupby, that is, to split the dataframe in `M` groups each sharing the same pair of category. This operation can be done in `O(R)` time where `R` is the number of rows in the dataframe. In practice, Pandas may implement this using a (comparison-based) sort running in `O(R log R)` time.\n\nFaster access & Conversion to Numpy\nMoreover, accessing a dataframe item per item using `loc` is very slow. Indeed, Pandas needs to locate the location of the column using an internal dictionary, find the row based on the provided date, extract the value in the dataframe based on the ith row and jth column, create a new object and return it, not to mention the several check done (eg. types and bounds). On top of that, Pandas introduces a significant overhead partially due to its code being interpreted using typically CPython.\nA faster solution is to extract the columns ahead of time, and to iterate over the row using integers instead of values (like dates). The thing is the order of the sorted date may not be the one in the dataframe subset. I guess it is the case for your input dataframe in practice, but if it is not, then you can sort the dataframe of each precomputed groups by date. I assume all the dates are present in all subset dataframe (but again, if this not the case, you can correct the result of the `groupby`). Each column can be converted to Numpy so to the can be faster. The result is a pure-Numpy code, not using Pandas anymore. Computationally-intensive Numpy codes are great since they can often be heavily optimized, especially when the target arrays contains native numerical types.\nHere is the implementation so far:\n`df = df.set_index('date')\nday_0 = unique_dates[0] # first date\n\n# Using Dictionary comprehension\n\nlist_of_numbers = list(range(len(unique_pairs)))\nmyset = {key: None for key in list_of_numbers}\n\ngroups = dict(list(df.groupby(['category_a', 'category_b'])))\n\nfor count_pair, value in enumerate(unique_pairs):\n    \n    # pair of category_a and category_b\n    category_a = value['category_a']\n    category_b = value['category_b']\n\n    # subset the dataframe for the pair\n    df_subset = groups[(category_a, category_b)]\n\n    # Extraction of the Pandas columns and convertion to Numpy ones\n    col_open = df_subset['open'].to_numpy()\n    col_close = df_subset['close'].to_numpy()\n    col_inflow = df_subset['inflow'].to_numpy()\n    col_outflow = df_subset['outflow'].to_numpy()\n    col_max = df_subset['max'].to_numpy()\n    col_buy = df_subset['buy'].to_numpy()\n\n    # day 0\n    col_close[0] = col_open[0] + col_inflow[0] - col_outflow[0]\n\n    # loop over single pair using date\n    for i in range(1, len(unique_dates)):\n        col_open[i] = col_close[i-1]\n        col_close[i] = col_open[i] + col_inflow[i] - col_outflow[i]\n\n        # check if closing value is negative, if so, set inflow to buy for next weeks deficit\n        if col_close[i]  col_max[i]:\n            col_buy[i-1] = 0\n        else:\n            col_buy[i-1] = col_inflow[i]\n\n        col_inflow[i] = col_buy[i-1]\n        col_close[i] = col_open[i] + col_inflow[i] - col_outflow[i]\n    \n    # store all the dataframes in a container myset\n    myset[count_pair] = df_subset\n\n# make myset into a dataframe\nresult = pd.concat(myset.values()).reset_index(drop=False)\nresult\n`\nThis code is not only faster, but also a bit easier to read.\n\nFast execution using Numba\nAt this point, the general solution is to use vectorized functions but this is really not easy to do that efficiently (if even possible) here due to the loop dependencies and the conditionals. A fast solution is to use a JIT compiler like Numba so to generate a very-fast implementation. Numba is designed to work efficiently on natively-typed Numpy arrays so this is the perfect use-case. Note that Numba need the input parameter to have a well-defined (native) type. Providing the types manually cause Numba to generate the code eagerly (during the definition of the function) instead of lazily (during the first execution).\nHere is the final resulting code:\n`import numba as nb\n\n@nb.njit('(float64[:], float64[:], float64[:], int64[:], int64[:], float64[:], int64)')\ndef compute(col_open, col_close, col_inflow, col_outflow, col_max, col_buy, n):\n    # Important checks to avoid out-of bounds that are \n    # not checked by Numba for sake of performance. \n    # If they are not true and not done, then \n    # the function can simply cause a crash.\n    assert col_open.size == n and col_close.size == n\n    assert col_inflow.size == n and col_outflow.size == n\n    assert col_max.size == n and col_buy.size == n\n\n    # day 0\n    col_close[0] = col_open[0] + col_inflow[0] - col_outflow[0]\n\n    # loop over single pair using date\n    for i in range(1, n):\n        col_open[i] = col_close[i-1]\n        col_close[i] = col_open[i] + col_inflow[i] - col_outflow[i]\n\n        # check if closing value is negative, if so, set inflow to buy for next weeks deficit\n        if col_close[i]  col_max[i]:\n            col_buy[i-1] = 0\n        else:\n            col_buy[i-1] = col_inflow[i]\n\n        col_inflow[i] = col_buy[i-1]\n        col_close[i] = col_open[i] + col_inflow[i] - col_outflow[i]\n\ndf = df.set_index('date')\nday_0 = unique_dates[0] # first date\n\n# Using Dictionary comprehension\n\nlist_of_numbers = list(range(len(unique_pairs)))\nmyset = {key: None for key in list_of_numbers}\n\ngroups = dict(list(df.groupby(['category_a', 'category_b'])))\n\nfor count_pair, value in enumerate(unique_pairs):\n    # pair of category_a and category_b\n    category_a = value['category_a']\n    category_b = value['category_b']\n\n    # subset the dataframe for the pair\n    df_subset = groups[(category_a, category_b)]\n\n    # Extraction of the Pandas columns and convertion to Numpy ones\n    col_open = df_subset['open'].to_numpy()\n    col_close = df_subset['close'].to_numpy()\n    col_inflow = df_subset['inflow'].to_numpy()\n    col_outflow = df_subset['outflow'].to_numpy()\n    col_max = df_subset['max'].to_numpy()\n    col_buy = df_subset['buy'].to_numpy()\n\n    # Numba-accelerated computation\n    compute(col_open, col_close, col_inflow, col_outflow, col_max, col_buy, len(unique_dates))\n\n    # store all the dataframes in a container myset\n    myset[count_pair] = df_subset\n\n# make myset into a dataframe\nresult = pd.concat(myset.values()).reset_index(drop=False)\nresult\n`\nFeel free to change the type of the parameters if they do not match with the real-world input data-type (eg. int32 vs int64 or float64 vs int64). Note that you can replace things like `float64[:]` by `float64[::1]` if you know that the input array is contiguous which is likely the case. This generates a faster code.\nAlso please note that `myset` can be a list since `count_pair` is an increasing integer. This is simpler and faster but it might be useful in your real-world code.\n\nPerformance results\nThe Numba function call runs in about 1 \u00b5s on my machine as opposed to 7.1 ms for the initial code. This means the hot part of the code is 7100 times faster just on the tiny example. That being said, Pandas takes some time to convert the columns to Numpy, to create groups and to merge the dataframes. The former takes a small constant time negligible for large arrays. The two later operations take more time on bigger input dataframes and they are actually the main bottleneck on my machine (both takes 1 ms on the small example). Overall, the whole initial code takes 16.5 ms on my machine for the tiny example dataframe, while the new one takes 3.1 ms. This means a 5.3 times faster code just for this small input. On bigger input dataframes the speed-up should be significantly better. Finally, please not that `df.groupby(['category_a', 'category_b'])` was actually already precomputed so I am not even sure we should include it in the benchmark ;) .",
      "question_score": 14,
      "answer_score": 7,
      "created_at": "2023-01-13T20:32:44",
      "url": "https://stackoverflow.com/questions/75113742/improving-performance-for-a-nested-for-loop-iterating-over-dates"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "bigquery",
      "question_id": 72260516,
      "title": "Unable to alter column data type in Big Query",
      "problem": "We imported database into BigQuery but a lot of columns are not in correct data types, many of them are stored as STRING. I want to fix them by change the data type in BigQuery\n```\n`ALTER TABLE my.data_set.my_table ALTER COLUMN create_date SET DATA TYPE DATE;\n`\n```\nBut I got\n```\n`ALTER TABLE ALTER COLUMN SET DATA TYPE requires that the existing column type (STRING) is assignable to the new type (DATE)\n`\n```\nHow to solve it?",
      "solution": "Unfortunately, as far as I know there is no way to convert data type from `STRING` to `DATE` using `ALTER TABLE` but to create it again with the schema you want.\n`CREATE OR REPLACE TABLE testset.tbl AS \nSELECT 'a' AS col1, '2022-05-16' AS create_date\n UNION ALL\nSELECT 'a' AS col1, '2022-05-14' AS create_date\n;\n\n-- ALTER TABLE testset.tbl ALTER COLUMN create_date SET DATA TYPE DATE;\n\n-- Query error: ALTER TABLE ALTER COLUMN SET DATA TYPE requires that\n-- the existing column type (STRING) is assignable to the new type (DATE) at [7:25]\n\n-- Create it again. \nCREATE OR REPLACE TABLE testset.tbl AS\nSELECT * REPLACE(SAFE_CAST(create_date AS DATE) AS create_date) \n  FROM testset.tbl;\n`",
      "question_score": 13,
      "answer_score": 8,
      "created_at": "2022-05-16T16:05:16",
      "url": "https://stackoverflow.com/questions/72260516/unable-to-alter-column-data-type-in-big-query"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "bigquery",
      "question_id": 75665455,
      "title": "Linking GA4 to BigQuery not working after 24 hours",
      "problem": "I'm trying to link a recently-created Google Analytics GA4 data stream to a BigQuery account. I've checked every tutorial I can find and I've set up everything correctly as far as I can tell. However, more than 24 hours later, there is no data connection. I can't find any information on how to debug this - does anybody have any ideas?\nBelow are pictures showing what I've set up. The BigQuery account is paid, not sandbox.\nGA4 report showing there is some data in Google Analytics:\n\nGA4 link to BigQuery:\n\nBigQuery service account:\n\nBigQuery project with no data flowing:\n\nI have blanked out names but I only have one BigQuery project so I've definitely linked GA4 to the correct one.\nThere is only one data stream, web, and it is selected in the link.\nCan anyone think of any other configuration or setting which I might need?",
      "solution": "Two days later and BigQuery has decided to connect to GA4. I changed nothing. It seems that it can take three or four days to connect after you've set everything up, and there's no way to get any feedback while waiting.",
      "question_score": 11,
      "answer_score": 11,
      "created_at": "2023-03-07T18:39:57",
      "url": "https://stackoverflow.com/questions/75665455/linking-ga4-to-bigquery-not-working-after-24-hours"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "bigquery",
      "question_id": 66053579,
      "title": "Bigquery INSERT after WITH AS statement not working",
      "problem": "I have a CTE like this:\n```\n`WITH source1 as (\nSELECT blah FROM blah\n), source2 as (\nSELECT moreblah FROM source1)\n\nSELECT * FROM source2;\n`\n```\nI want to insert the results of this query into a table, but when I write this:\n```\n`WITH source1 as (\nSELECT blah FROM blah\n), source2 as (\nSELECT moreblah FROM source1)\n\nINSERT INTO newtable SELECT * FROM source2;\n`\n```\nIt says I have a syntax error `Expected \"(\" or \",\" or keyword SELECT but got keyword INSERT`. I'm wondering if this is a Bigquery issue b/c I've looked at other places like this which say my INSERT INTO should work. Any help would be appreciated!",
      "solution": "In BigQuery, the `WITH` goes with the `SELECT`:\n```\n`INSERT INTO newtable \n    WITH source1 as (\n          SELECT blah FROM blah\n         ),\n         source2 as (\n          SELECT moreblah FROM source1\n         )\n    SELECT *\n    FROM source2;\n`\n```",
      "question_score": 9,
      "answer_score": 31,
      "created_at": "2021-02-04T21:47:57",
      "url": "https://stackoverflow.com/questions/66053579/bigquery-insert-after-with-as-statement-not-working"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "bigquery",
      "question_id": 66097351,
      "title": "Is there a wait method for Google BigQuery SQL",
      "problem": "I'm scheduling a query to run every day that picks up data from a table that get's written to Google BigQuery automatically.\nUsually the table is there, but I'd like to be sure before I execute the query depending on this table.\nI'm looking into other ways as well, but the simplest way seems to be to use just SQL and integrate the checking and retrying into the scheduled query.\nI'm able to check if the table exists and retry if it doesn't. I can't seem to find a way to not do this immediately and have the query wait for 30 minutes before the next retry.\nIs there something available, possibly similar to 'WAITFOR' that will achieve this?\nCurrent SQL;\n```\n`DECLARE retry_count INT64;\nDECLARE success BOOL;\nDECLARE size_bytes INT64;\nDECLARE row_count INT64;\nSET retry_count = 1;\nSET success = FALSE;\n\nWHILE retry_count  0  THEN\n    SELECT 'Table Exists!' as message, retry_count as retries;\n    SET success = TRUE;\n  ELSE\n    SELECT 'Table does not exist' as message, retry_count as retries, row_count;\n    SET retry_count = retry_count + 1;\n--      WAITFOR DELAY '00:30:00';\n  END IF;\nEND;\nEND WHILE;\n`\n```",
      "solution": "I hope you found ways to get the time delay added. I too recently came across such a situation and this is how I handled it -\n```\n`DECLARE retry_count INT64;\nDECLARE success BOOL;\nDECLARE size_bytes INT64;\nDECLARE row_count INT64;\nDECLARE DELAY_TIME DATETIME;\nDECLARE WAIT STRING;\nSET retry_count = 1;\nSET success = FALSE;\n\nWHILE retry_count  0  THEN\n    SELECT 'Table Exists!' as message, retry_count as retries;\n    SET success = TRUE;\n  ELSE\n    SELECT 'Table does not exist' as message, retry_count as retries, row_count;\n    SET retry_count = retry_count + 1;\n--      WAITFOR DELAY '00:30:00';\n    SET WAIT = 'TRUE';\n    SET DELAY_TIME = DATETIME_ADD(CURRENT_DATETIME,INTERVAL 30 MINUTE);\n    WHILE WAIT = 'TRUE' DO\n      IF (DELAY_TIME Thanks,\nAnusha",
      "question_score": 8,
      "answer_score": 12,
      "created_at": "2021-02-08T08:23:13",
      "url": "https://stackoverflow.com/questions/66097351/is-there-a-wait-method-for-google-bigquery-sql"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "bigquery",
      "question_id": 69759838,
      "title": "What is the BigQuery equivalent of SELECT * FROM (VALUES ...) AS t",
      "problem": "I'm trying to setup a CTE with a constant table using this PosgreSQL syntax:\n```\n`WITH rates AS (\n  SELECT * FROM (\n    VALUES\n      (2017, 2018, 0.1),\n      (2016, 2017, 0.1),\n      (2015, 2016, 0.2),\n      (2014, 2015, 0.3)\n  ) AS t(init, end, rate)\n)\nSELECT * FROM my_bq_table as my\n  JOIN rates as r ON my.year = r.init;\n`\n```\nBut I'm getting `Syntax error: Expected keyword JOIN but got \",\"` because apparently BigQuery doesn't recognize the syntax `VALUES [(tuple)]` as a `from-clause` exactly like PostgreSQL.\nWhat is the easiest way to achieve a similar CTE?",
      "solution": "Consider below approach\n```\n`WITH rates AS (\n  SELECT * FROM UNNEST([\n      STRUCT\n      (2017, 2018, 0.1),\n      (2016, 2017, 0.1),\n      (2015, 2016, 0.2),\n      (2014, 2015, 0.3)\n  ])\n)\nSELECT * FROM rates     \n`\n```\nwith output",
      "question_score": 7,
      "answer_score": 11,
      "created_at": "2021-10-28T21:29:03",
      "url": "https://stackoverflow.com/questions/69759838/what-is-the-bigquery-equivalent-of-select-from-values-as-t"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "bigquery",
      "question_id": 66277165,
      "title": "How to choose the latest partition of a bigquery table in DBT without scanning the whole table?",
      "problem": "I'm trying to select the latest partition from a BigQuery table without scanning the whole table in a DBT model in order to save query costs.\nDBT doesnt allow using semicolons in a data model so using the `DECLARE`+`SET` scripting statements doesn't work as suggested here.\nDBT has a sql_header macro which allows setting some variables in the header but that header doesn't accept references to a data model or at least the following code is not compiling:\n```\n`{{ config(\n  sql_header=\"  DECLARE latest_partition_date DATE;\n  DECLARE latest_load_timestamp TIMESTAMP;\n  SET latest_partition_date = (SELECT MAX(_PARTITIONDATE) FROM {{ ref(\"model\") }} );\n  SET latest_load_timestamp = (SELECT MAX(loaded_at) FROM {{ ref(\"model\") }} WHERE _PARTITIONDATE = latest_partition_date);\"\n) }}\n\n-- set the main query\nSELECT * FROM {{ ref(\"model\") }}\nWHERE \n-- Select the latest partition to reduce 'Bytes processed' for loading the query.\n_PARTITIONDATE = latest_partition_date\n-- Select the latest load within the latest partition to get only one duplicate of data.\nAND loaded_at = latest_load_timestamp\n`\n```\nI need to solve this in standard SQL.\nOther methods that were suggested included setting `WHERE _PARTITIONDATE = CURRENT_DATE()` or using `DATE_SUB(CURRENT_DATE(), 3)` but those don't satisfy because data load breakages are unpredictable and only dynamically selecting the latest would work here. Is that possible?",
      "solution": "Since the original question was working with dates, the right datatype conversion was the missing piece.\nIn the end I figured that the conversion to the right datatype needs to be done within jinja and not with SQL for the queries to accept right variables. Also, `{{ max_date }}` needed quotes.\nThe final solution that I got working was this:\n```\n`\n{%- call statement('max_partition_date_query', True) -%}\n  SELECT MAX(_PARTITIONDATE) as max_partition_date FROM {{ ref('model') }}\n{%- endcall -%}\n\n{%- set max_timestamp = load_result('max_partition_date_query')['data'][0][0] -%}\n{%- set max_date = max_timestamp.strftime('%Y-%m-%d') -%}\n\nselect * FROM {{ ref('model') }}\nWHERE _PARTITIONDATE = '{{ max_date }}'\n`\n```",
      "question_score": 7,
      "answer_score": 3,
      "created_at": "2021-02-19T12:58:02",
      "url": "https://stackoverflow.com/questions/66277165/how-to-choose-the-latest-partition-of-a-bigquery-table-in-dbt-without-scanning-t"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "bigquery",
      "question_id": 74807026,
      "title": "Vertex AI feature store vs BigQuery",
      "problem": "I was trying to figure out key differences between using GCP Vertex AI feature store and Saving preprocessed features to BigQuery and loading whenever it gets necessary.\nI still cannot understand why to choose the first option, rather than the second option, which seems to be easier and more accessible.\nIs there any good reason to use feature store in Vertex AI, rather than storing features in BigQuery tables formats?",
      "solution": "Vertex AI Feature Store and BigQuery, both can be used to store the features as mentioned by you. But Vertex AI Feature Store has several advantages over BigQuery that makes it favorable for storing features.\nAdvantages of Vertex AI Feature Store over BigQuery :\n\nVertex AI Feature Store is designed to create and manage featurestores, entity types, and features whereas BigQuery is a data warehouse where you can perform analysis on data.\nVertex AI Feature Store can be used for batch and online storage but BigQuery is not a solution for storage.\nVertex AI Feature Store can be used for sharing the features across the organization from the central repository which BigQuery does not provide.\nVertex AI Feature Store is a managed solution for online feature serving which is not supported by BigQuery.\nFor more information, you can check this link.",
      "question_score": 6,
      "answer_score": 5,
      "created_at": "2022-12-15T06:05:49",
      "url": "https://stackoverflow.com/questions/74807026/vertex-ai-feature-store-vs-bigquery"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "bigquery",
      "question_id": 66887015,
      "title": "What permissions are required to execute a BigQuery stored procedure?",
      "problem": "I need to grant someone the permission to execute a bigquery stored procedure but I don't want to grant them roles/bigquery.admin, I would rather grant only the required permissions via a custom role.\nThis led me to question the who a stored procedure executes as. I have a background in SQL Server and there is an option there to have a stored procedure execute as owner which means that it runs as the owner of the stored procedure and thus the permissions assigned to that owner...but I don't think there's anything similar in BigQuery.\nIn short, how do I grant someone permission to execute a stored procedure and do I need to grant the person executing the stored procedure the appropriate permissions on the datasets affected by the stored procedure?\nI've pored over the documentation, mainly https://cloud.google.com/iam/docs/understanding-roles#bigquery-roles, but I can't find anything that provides clarity on this issue.",
      "solution": "What you describe \"it runs as the owner of the stored procedure and thus the permissions assigned to that owner\" is similar to the concept of \"Authorized Routines\". As of September 2023, authorized routines support stored procedures.\nRegarding the permissions needed, the `bigquery.routines.get` permission is required to execute a stored procedure or UDF. This is provided by `roles/bigquery.metadataViewer` and/or `roles/bigquery.dataViewer`. In addition, if it is not an authorized routine, then yes the query user also requires the appropriate permissions on the datasets affected by the stored procedure.\nResources:\n\nBigQuery permissions\nBigQuery roles",
      "question_score": 6,
      "answer_score": 9,
      "created_at": "2021-03-31T13:30:34",
      "url": "https://stackoverflow.com/questions/66887015/what-permissions-are-required-to-execute-a-bigquery-stored-procedure"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "bigquery",
      "question_id": 68021003,
      "title": "Is it possible to pull column descriptions from BigQuery metadata",
      "problem": "I'm trying to get the column descriptions from the BigQuery metadata but I can't seem to find it, I've tried using INFORMATION_SCHEMA.COLUMNS but it isn't contained in here, does anyone know where it is please?\nThanks!",
      "solution": "You can have access to all the columns metadata by querying the `COLUMN_FIELD_PATHS` table:\n```\n`SELECT * FROM `yourdataset.INFORMATION_SCHEMA.COLUMN_FIELD_PATHS`\n`\n```\nDoc reference here\nYou get information about column names and types within all your dataset.",
      "question_score": 6,
      "answer_score": 8,
      "created_at": "2021-06-17T16:06:16",
      "url": "https://stackoverflow.com/questions/68021003/is-it-possible-to-pull-column-descriptions-from-bigquery-metadata"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "bigquery",
      "question_id": 77438318,
      "title": "Connecting Datastream to Cloud SQL postgres over private IP: Do I need a reverse proxy even if they are in the same project?",
      "problem": "My goal should be quite straight forward: I have a PostgreSQL running in Cloud SQL, and want to use Datastream to transfer data to Big Query, within the same project, preferably through private IP connectivity.\nI have set up the database with private IP, created a private connectivity in Datastream with an available IP range on the database's VPC, and set up a connection profile towards the private IP with the correct credentials.\nAll I get for now is a timeout when I test the connection, being a bit hard to debug.\nI have also tried enabling firewall rules to accept the traffic, with the same result. (Is it necessary to create firewall rules in this case?)\nAt first I thought the reverse proxy would only be needed when connecting from outside the GCP project or from other networks. Do I really need it in this case as well? Shouldn't they be accessible when they are within the same GCP project?\nThe amount of work and config needed to make this work leads me to believe that I am doing something the wrong way or not following best practice here. As this is assumably a central part of Datastream/GCP functionality, I assume there is a simpler and more easy to maintain way? I am setting up several databases to transfer analytics data to Big Query, so minimizing the overhead for each of them is a big advantage.\nWhat would be the preferred way to acheive this?\nSide question: Apart from Datastream, is there another preferred way / best practice to transfer data to Big Query from Cloud SQL? In the Cloud SQL config I see the option \"Enable private path: Allows other Google Cloud services like BigQuery to access data and make queries over Private IP\" but I can't find much documentation on what this is and how to use it.",
      "solution": "Do I need a reverse proxy even if they are in the same project?\n\nYes unfortunately I do believe you need to setup a \"reverse proxy\" or intermediary VM even within the same project for the VPC network.\n\nThe reason being for this stated in the docs is:\n\nWhen you configure a Cloud SQL for PostgreSQL instance to use private IP addresses, you use a VPC peering connection between your VPC network and the VPC network of the underlying Google services where your Cloud SQL instance resides. (Cloud SQL blue square in above digram)\n\nBecause Datastream's network (Datastream blue rectangle in diagram) can't be peered directly with Cloud SQL's private services network, and because VPC peering isn't transitive, a reverse proxy for Cloud SQL is required to bridge the connection from Datastream to your Cloud SQL instance.\"\n\nBasically what this means is because Cloud SQL is a managed service and uses its own \"Cloud SQL\" protected VPC and Datastream is a managed service with its own \"Datastream\" VPC it results in a transitive VPC peering conflict. To resolve this transitivity issue, a middle client/proxy must be established to forward the traffic.\n\nApart from Datastream, is there another preferred way / best practice to transfer data to BigQuery from Cloud SQL?\n\nThis depends on the use-case or meaning of \"transfer data\" so let me try and showcase the options/products.\nFirst ask yourself which of the below use-cases are you?\na) You want to transfer data in real-time from Cloud SQL Postgres to BigQuery so that the data also resides in BigQuery\nb) You want to read Cloud SQL data into BigQuery and use it for analysis?\nIf you are Option a) than yes this is where Datastream comes in as the product/feature for you. Datastream is a change data capture (CDC) and replication service that lets you synchronize data reliably, and with minimal latency. Basically as data is added to Cloud SQL Postgres it will be synced and replicated into BigQuery in almost real-time. The benefit here is that BigQuery data will be consistently updated without you needing to do extra work.\nIf you are Option b) and you want to quickly read data from Cloud SQL into BigQuery for analysis and are fine with the data residing purely in Cloud SQL than Datastream might be too much overhead. Cloud SQL Federated Queries allow you to read/query Cloud SQL data into BigQuery and may be a possible option. Selecting the Private path for Google Cloud services checkbox is what enables these federated queries for Private IP Cloud SQL instances as discussed in Connect Cloud SQL to BigQuery docs.",
      "question_score": 6,
      "answer_score": 7,
      "created_at": "2023-11-07T13:46:18",
      "url": "https://stackoverflow.com/questions/77438318/connecting-datastream-to-cloud-sql-postgres-over-private-ip-do-i-need-a-reverse"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "bigquery",
      "question_id": 75108436,
      "title": "BigQuery &quot;Schr&#246;dingers Row&quot; or why ROW_NUMBER() is not a good identifier",
      "problem": "Situation\nWe have a fairly complex internal logic to allocate marketing spend to various channels and had currently started to rework some of our queries to simplify the setup. We recently came across a really puzzling case where using `ROW_NUMBER() OVER()` to identify unique rows lead to very strange results.\nProblem\nIn essence, using `ROW_NUMBER() OVER()` resulted in what I call Schr\u00f6dingers Rows. As they appear to be matched and unmatched at the same time (please find replicable query below). In the attached screenshot (which is a result of the query) it can be clearly seen that\n`german_spend + non_german_spend > total_spend`\nWhich should not be the case.\n\nQuery\nPlease note that execution of the query will give you different results each time you run it as it relies on RAND() to generate dummy data. Also please be aware that the query is a very dumbed down version of what we are doing. For reasons beyond the scope of this post, we needed to uniquely identify the buckets.\n`###################\n# CREATE Dummy Data\n###################\nDECLARE NUMBER_OF_DUMMY_RECORDS DEFAULT 1000000;\n\nWITH data AS (\n  SELECT\n    num as campaign_id, \n    RAND() as rand_1,\n    RAND() as rand_2\n  FROM\n    UNNEST(GENERATE_ARRAY(1, NUMBER_OF_DUMMY_RECORDS)) AS num\n),\n\nspend_with_categories AS (\n  SELECT\n    campaign_id,\n    CASE \n      WHEN rand_1 \nSolution\nWe were actually able to solve the problem by using a hash of the key instead of the `ROW_NUMBER() OVER()` identifier, but out of curiosity I would still love to understand what causes this.\nAdditional Notes\n\nUsing `GENERATE_UUID() AS identifier` instead of `CONCAT(\"row_\", ROW_NUMBER() OVER()) AS identifier` leads to almost 0 matches. I.e. entire spend is classified as non-german.\n\nWriting spend_buckets to a table also solves the problem, which leads me to believe that maybe `ROW_NUMBER() OVER()` is lazily executed or so?\n\nusing a small number for the dummy data also produces non-matching results regardless of the method of generating a \"unique\" id",
      "solution": "Hash functions are a way better for marking rows than generating a  rownumber, which is changing each day.\nThe CTE (`with` tables) are not persistent, but calculated for each time used in your query.\nRunning the same CTE several times within a query, results in different results:\n`With test as (Select rand() as x)\n\nSelect * from test\nunion all Select * from test\nunion all Select * from test\n`\nA good solution is the use of `temp table`. A workaround is to use search for CTE table, which creates a row_number or generates random number and are used more than once in following. These CTE are to rename and be used in a recursive CTE and then the later CTE is used. In your example it is the `spend_buckets`:\n`WITH recursive\n...\nspend_buckets_ as (\n...),\nspend_buckets as\n(select * from spend_buckets_\nunion all select * from spend_buckets_\nwhere false\n),\n`\nThen the values will match.",
      "question_score": 6,
      "answer_score": 1,
      "created_at": "2023-01-13T12:11:24",
      "url": "https://stackoverflow.com/questions/75108436/bigquery-schr%c3%b6dingers-row-or-why-row-number-is-not-a-good-identifier"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "bigquery",
      "question_id": 69147748,
      "title": "Expected type &#39;_SpecialForm[str]&#39;, got &#39;str&#39; instead",
      "problem": "I'm trying to create a BQ table schema, as seen on this page\nBut I get a compilation error for all the `mode=\"REQUIRED\"`\nI didn't see anything special to import but the bq module.\n```\n`Expected type '_SpecialForm[str]', got 'str' instead \n`\n```\nThe code:\n```\n`    bqServiceWrapper.create_table(_ADS_TO_REMOVE_TABLE_NAME,\n                                  [\n                                      bigquery.SchemaField(\"add_id\", \"STRING\", mode=\"REQUIRED\"),\n                                      bigquery.SchemaField(\"timestamp_str\", \"TIMESTAMP\", mode=\"REQUIRED\"),\n                                      bigquery.SchemaField(\"timestamp\", \"TIMESTAMP\", mode=\"REQUIRED\")\n                                  ])\n`\n```\nBTW does the python BQ library allows creating a table without a schema (like Java does?). If so - how can the type to implied as `\"TIMESTAMP\"` and not `\"STRING\"`?",
      "solution": "There appears to be an extra `mode=\u201dREQUIRED\u201d` in your code. Also, your code is not creating a table as mentioned in the doc `table = bigquery.Table(table_id, schema=schema)` . Rewriting your code as follows :\n```\n`from google.cloud import bigquery\n\n# Construct a BigQuery client object.\nclient = bigquery.Client()\n\nclient.create_table(bigquery.Table(\"ProjectID.Dataset.Table\", schema= [\n                                   bigquery.SchemaField(\"add_id\", \"STRING\", mode=\"REQUIRED\"),\n                                   bigquery.SchemaField(\"timestamp_str\", \"TIMESTAMP\", mode=\"REQUIRED\"),\n                                    bigquery.SchemaField(\"timestamp\", \"TIMESTAMP\", mode=\"REQUIRED\")\n                                ]))\n\n`\n```\nThis creates the table in BigQuery with the required schema :\nFor creating a schemaless table using the Python client library you can simply run the above code without the schema : `client.create_table(bigquery.Table(\"ProjectID.Dataset.Table\"))` or directly `client.create_table(\"ProjectID.Dataset.Table\")`.\nBut if we are creating a schemaless table we need to define the schema either by auto-detect or manually and only then we can add data to it. Assuming you are trying to load data from a CSV file into an empty table with auto-detect schema, you need to have the Timestamp data in the supported format as specified in this doc.",
      "question_score": 6,
      "answer_score": 1,
      "created_at": "2021-09-12T03:08:30",
      "url": "https://stackoverflow.com/questions/69147748/expected-type-specialformstr-got-str-instead"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "bigquery",
      "question_id": 68638749,
      "title": "What does &quot;EXCEPT distinct select * &quot; in SQL language mean?",
      "problem": "I am following a tutorial on Qwiklabs on Bigquery and financial fraud detection and came across a query, below, that I am failing to understand\n```\n`CREATE OR REPLACE TABLE finance.fraud_data_model AS\nSELECT\n*\nFROM finance.fraud_data_sample  \nEXCEPT distinct select * from finance.fraud_data_test  \n`\n```\nI am trying to understand what is being retrieved or ignored from the `finance.fraud_data_test` table\nor what is the main `finance.fraud_data_model` made up of when it comes to the two other tables?",
      "solution": "The SQL EXCEPT statement is used to filter records based on the intersection of records returned via two SELECT statements.\nThe records that are common between the two tables are filtered from the table on the left side of the SQL EXCEPT statement and the remaining records are returned.\n```\n`SELECT * FROM finance.fraud_data_sample  \nEXCEPT distinct select * from finance.fraud_data_test\n`\n```\nIn your case, the query returns every record in `finance.fraud_data_sample`\nthat are not present in the unique list obtained from`finance.fraud_data_test`",
      "question_score": 5,
      "answer_score": 12,
      "created_at": "2021-08-03T17:22:30",
      "url": "https://stackoverflow.com/questions/68638749/what-does-except-distinct-select-in-sql-language-mean"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "bigquery",
      "question_id": 72104562,
      "title": "BigQuery SQL, Obtain Median, Grouped by Date?",
      "problem": "When trying to obtain, say the median using the partition by window function, I receive an error message \"SELECT list expression references column seller_stock which is neither grouped nor aggregated\", why is this, how must i write this SQL differently? I have many records per day, and i want to return the median for each day ...\n```\n`SELECT date(snapshot_date) AS period, \n  PERCENTILE_DISC(**seller_stock**, 0.5) OVER (PARTITION BY snapshot_date) AS median_stock\nFROM `table.name`  \nWHERE snapshot_date >= \"2022-04-01\" \nGROUP BY snapshot_date\n`\n```",
      "solution": "The thing is that you cannot group by an AGG function, since you are getting already the median there over by your rows, you will need just the top row of that statement.\nYou can use an intermediate table or aux.\nThis is an example:\n```\n`with median_data as (\n  select \n    date(snapshot_date) AS period,\n    PERCENTILE_DISC(seller_stock, 0.5) OVER (PARTITION BY snapshot_date) AS median_stock,\n    row_number() over(order by snapshot_date) as r\n  from `table.name`\n  where snapshot_date >= \"2022-04-01\" \n)\n\nselect period,median_stock from median_data where r =  1\n`\n```",
      "question_score": 5,
      "answer_score": 8,
      "created_at": "2022-05-03T21:29:17",
      "url": "https://stackoverflow.com/questions/72104562/bigquery-sql-obtain-median-grouped-by-date"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "bigquery",
      "question_id": 66360948,
      "title": "How to add primary key and foreign key to a BigQuery?",
      "problem": "I am trying to learn bigquery but I noticed I cannot add Primary key or Foreign Keys at all.\nHere is an example:\n```\n`CREATE TABLE db.VENDOR \n(\n    V_CODE      INT64,\n    V_NAME      String NOT NULL,\n    V_CONTACT   String NOT NULL,\n    V_AREACODE  String NOT NULL,\n    V_PHONE     String NOT NULL,\n    V_STATE     String NOT NULL,\n    V_ORDER     String NOT NULL,\n    PRIMARY KEY(V_Code)\n);\n\nCREATE TABLE db.PRODUCT \n(\n    P_CODE     string Not Null,\n    P_DESCRIPT string NOT NULL,\n    P_INDATE   DATETIME NOT NULL,\n    P_QOH      int64 NOT NULL,\n    P_MIN      int64 NOT NULL,\n    P_PRICE    NUMERIC NOT NULL,\n    P_DISCOUNT NUMERIC NOT NULL,\n    V_CODE     int64,\n\n    CONSTRAINT PRODUCT_V_CODE_FK \n        FOREIGN KEY (V_CODE) REFERENCES VENDOR (V_CODE)\n);\n`\n```\nWhen I use primary key, I get an error:\n\nPrimary Key is not supported\n\nand for the foreign key I get:\n\nTable name \"VENDOR\" missing dataset while no default dataset is set in the request\n\nIs there anyway to use PK or FK in BigQuery? if yes, How?",
      "solution": "Check out the docs for Primary keys and Foreign keys\nThe following example adds the primary key constraint of x and y to the pk_table table:\n```\n`ALTER TABLE pk_table ADD PRIMARY KEY (x,y) NOT ENFORCED;\n`\n```\nThe following example adds the my_fk_name foreign key constraint to the fk_table table. This example depends on an existing table, pk_table.\n```\n`ALTER TABLE fk_table\nADD CONSTRAINT my_fk_name FOREIGN KEY (u, v)\nREFERENCES pk_table(x, y) NOT ENFORCED;\n`\n```\nNote, key constraints are not enforced, so they will not prevent duplicates. They are used primarily to help optimizer - Why create Primary Keys and Foreign Keys?",
      "question_score": 5,
      "answer_score": 11,
      "created_at": "2021-02-25T01:51:55",
      "url": "https://stackoverflow.com/questions/66360948/how-to-add-primary-key-and-foreign-key-to-a-bigquery"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "bigquery",
      "question_id": 72976543,
      "title": "Google BigQuery query in Python works when using result(), but Permission issue when using to_dataframe()",
      "problem": "I've run into a problem after upgrades of my pip packages and my bigquery connector that returns query results suddenly stopped working with following error message\n```\n`from google.cloud import bigquery\nfrom google.oauth2 import service_account\n\ncredentials = service_account.Credentials.from_service_account_file('path/to/file', scopes=['https://www.googleapis.com/auth/cloud-platform',\n    'https://www.googleapis.com/auth/drive',\n    'https://www.googleapis.com/auth/bigquery'\n])\n\nclient = bigquery.Client(credentials=credentials)\ndata = client.query('select * from dataset.table').to_dataframe()\n`\n```\n\nPermissionDenied: 403 request failed: the user does not have\nbigquery.readsessions.create' permission\n\nBut! If you switched the code to\n```\n`data = client.query('select * from dataset.table').result()\n`\n```\n(dataframe -> result) you received the data in RowIterator format and were able to properly read them.\nThe same script using to_dataframe with the same credentials was working on the server. Therefore I set my bigquery package to the same version 2.28.0, which still did not help.\nI could not find any advices on this error / topic anywhere, so I just want to share if any of you faced the same thing.",
      "solution": "There are different ways of receiving data from bigquery. Using the BQ Storage API is considered more efficient for larger result sets compared to the other options:\n\nThe BigQuery Storage Read API provides a third option that represents an improvement over prior options. When you use the Storage Read API, structured data is sent over the wire in a binary serialization format. This allows for additional parallelism among multiple consumers for a set of results\n\nThe Python BQ library internally determines whether it can use the BQ Storage API or not. For the result method, it uses the tradtional tabledata.list method internally, whereas the to_dataframe method uses the BQ Storage API if the according package is installed.\nHowever, using the BQ Storage API requires you to have the bigquery.readSessionUser Role respectively the readsessions.create right which in your case seems to be lacking.\nBy uninstalling the google-cloud-bigquery-storage, the google-cloud-bigquery package was falling back to the list method. Hence, by de-installing this package, you were working around the lack of rights.\nSee the BQ Python Libary Documentation for details.",
      "question_score": 5,
      "answer_score": 6,
      "created_at": "2022-07-14T09:08:30",
      "url": "https://stackoverflow.com/questions/72976543/google-bigquery-query-in-python-works-when-using-result-but-permission-issue"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "bigquery",
      "question_id": 72337471,
      "title": "Bigquery : how to declare an array variable and set data with a select statement?",
      "problem": "I am trying to declare an array variable on BigQuery but I don't manage to put a SQL statement in my variable. I couldn't find any topic about that.\nI want to put in my variable all the column names of a table so I tried this :\n```\n`DECLARE my_array ARRAY ;\n\nSET my_array = (\n  SELECT column_name\n  FROM my_project.my_dataset.INFORMATION_SCHEMA.COLUMNS\n  WHERE table_name = 'my_table'\n);\n\nSELECT my_array\n`\n```\nI think I have a syntax problem because the error is :\n```\n`Query error: Cannot coerce expression (\n  SELECT column_name\n  FROM my_project.my_dataset.INFORMATION_SCHEMA.COLUMNS\n  WHERE table_name = 'my_table'\n) to type ARRAY at [3:16]\n`\n```\nThanks,",
      "solution": "Would you try this one ?\n`DECLARE my_array ARRAY ;\n\nSET my_array = ARRAY(\n  SELECT column_name\n  FROM my_project.my_dataset.INFORMATION_SCHEMA.COLUMNS\n  WHERE table_name = 'my_table'\n);\n\nSELECT my_array;\n`\nOr, this will work also.\n`SET my_array = (\n  SELECT ARRAY_AGG(column_name)\n  FROM my_project.my_dataset.INFORMATION_SCHEMA.COLUMNS\n  WHERE table_name = 'my_table'\n);\n`",
      "question_score": 5,
      "answer_score": 7,
      "created_at": "2022-05-22T14:13:50",
      "url": "https://stackoverflow.com/questions/72337471/bigquery-how-to-declare-an-array-variable-and-set-data-with-a-select-statement"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "bigquery",
      "question_id": 69316124,
      "title": "bigQuery: PartialFailureError on table insert",
      "problem": "I'm trying to insert data row to bigQuery table as follows:\n```\n`await bigqueryClient\n          .dataset(DATASET_ID)\n          .table(TABLE_ID)\n          .insert(row);\n`\n```\nBut I get a `PartialFailureError` when deploying the cloud function.\nThe table schem has a name (string) and campaigns (record/repeated) fields which I created manually from the console.\n```\n`hotel_name  STRING  NULLABLE    \ncampaigns   RECORD  REPEATED    \n  campaign_id   STRING  NULLABLE    \n  platform_id   NUMERIC NULLABLE    \n  platform_name STRING  NULLABLE    \n  reporting_id  STRING  NULLABLE\n`\n```\nAnd the data I'm inserting is an object like this:\n```\n`      const row = {\n        hotel_name: hotel_name,//string\n        campaigns: {\n            id: item.id,//string\n            platform_id: item.platform_id,//int\n            platform_name: item.platform_name,//string\n            reporting_id: item.reporting_id,//string\n          },\n      };\n`\n```\nThe errors logged don't give much clue about the issue.",
      "solution": "According to my test it seems that there are 2 errors here. First is that you have `campaign_id` in schema while `id` in JSON.\n2nd thing is related with format of REPEATED mode data in JSON. The documentation mentions following:\n\n. Notice that the `addresses` column contains an array of values (indicated by `[ ]`). The multiple addresses in the array are the repeated data. The multiple fields within each address are the nested data.\n\nIt's not so straight in mentioned document (probably can be found somewhere else) however when you use REPEATED mode you should use brackets `[]`.\nI tested it shortly on my side and it seems that it should work like this:\n```\n`const row = {\n        hotel_name: hotel_name,//string\n        campaigns: [ {\n            campaign_id: item.id,//string\n            platform_id: item.platform_id,//int\n            platform_name: item.platform_name,//string\n            reporting_id: item.reporting_id,//string\n          }, ]\n      };\n`\n```",
      "question_score": 5,
      "answer_score": 3,
      "created_at": "2021-09-24T15:48:00",
      "url": "https://stackoverflow.com/questions/69316124/bigquery-partialfailureerror-on-table-insert"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "bigquery",
      "question_id": 66799366,
      "title": "ARRAY_AGG with STRUCT - IGNORE NULLS",
      "problem": "I need to aggregate `STRUCT`s into an array where the two fields within the `STRUCT` can be null (they will either both be null nor neither). Using `IGNORE NULLS` in the `ARRAY_AGG` function doesn't work - is there a way to take these null rows out of the resulting array?\nMy table looks like this:\n\nAnd I want my output to look like this, but excluding the null values within the `STRUCT`s:\n\nWhere there is no value for label, language or dtk I still want to include the row but with a blank array - this means I can't prefilter the nulls out as this will result in rows being excluded.\nThe query I am currently using to get this output looks like this:\n```\n`SELECT\n  ARRAY_AGG(STRUCT(label, language_name) IGNORE NULLS) AS label,\n  ARRAY_AGG(DISTINCT dtk IGNORE NULLS) AS dtk,\n  country_name, category, age_group, gender\nFROM\n  categoryData\nGROUP BY\n  country_name, category, age_group, gender\n`\n```",
      "solution": "Try IF:\n```\n`SELECT\n  ARRAY_AGG(if(label is not null or language_name is not null, struct(label, language_name), null) IGNORE NULLS) AS label,\n  ARRAY_AGG(DISTINCT dtk IGNORE NULLS) AS dtk,\n  country_name, category, age_group, gender\nFROM\n  categoryData\nGROUP BY\n  country_name, category, age_group, gender\n`\n```",
      "question_score": 5,
      "answer_score": 9,
      "created_at": "2021-03-25T13:19:23",
      "url": "https://stackoverflow.com/questions/66799366/array-agg-with-struct-ignore-nulls"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "bigquery",
      "question_id": 66990400,
      "title": "BigQuery timestamp current date minus x days",
      "problem": "I have a TIMESTAMP column with dates that I need to filter. I need to grab the data that is 5 days old. So `current date - 5 days`. My data is in BigQuery. I tried the following query:\n```\n`where created_time >= (TIMESTAMP_TRUNC(CURRENT_TIMESTAMP(), DAY), INTERVAL -5 DAY)\n`\n```\nI got an error: `Unexpected INTERVAL expression`",
      "solution": "You are missing `TIMESTAMP_ADD()`:\n```\n`where created_time > TIMESTAMP_ADD(TIMESTAMP_TRUNC(CURRENT_TIMESTAMP(), DAY), INTERVAL -5 DAY)\n`\n```",
      "question_score": 5,
      "answer_score": 5,
      "created_at": "2021-04-07T18:39:49",
      "url": "https://stackoverflow.com/questions/66990400/bigquery-timestamp-current-date-minus-x-days"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "bigquery",
      "question_id": 73250933,
      "title": "How do I run SQL model in dbt multiple times by looping through variables?",
      "problem": "I have a model in dbt (test_model) that accepts a geography variable (zip, state, region) in the configuration. I would like to run the model three times by looping through the variables, each time running it with a different variable.\nHere's the catch: I have a macro shown below that appends the variable to the end of the output table name (i.e., running test_model with zip as the variable outputs a table called test_model_zip). This is accomplished by adding `{{ config(alias=var('geo')) }}` at the top of the model.\nWhether I define the variable within dbt_project.yml, the model itself, or on the CLI, I've been unable to find a way to loop through these variables, each time passing the new variable to the configuration, and successfully create three tables. Do any of you have an idea how to accomplish this? FWIW, I'm using BigQuery SQL.\nThe macro:\n```\n`{% macro generate_alias_name(custom_alias_name=none, node=none) -%}\n\n    {%- if custom_alias_name is none -%}\n\n        {{ node.name }}\n\n    {%- else -%}\n\n        {% set node_name = node.name ~ '_' ~ custom_alias_name %}\n        {{ node_name | trim }}\n\n    {%- endif -%}\n\n{%- endmacro %}\n`\n```\nThe model, run by entering `dbt run --select test_model.sql --vars '{\"geo\": \"zip\"}'` in the CLI:\n```\n`{{ config(materialized='table', alias=var('geo')) }}\n\nwith query as (select 1 as id)\n\nselect * from query\n`\n```\nThe current output: a single table called test_model_zip.\nThe desired output: three tables called test_model_zip, test_model_state, and test_model_region.",
      "solution": "I would flip this on its head.\ndbt doesn't really have a concept for parameterized models, so if you materialize a single model in multiple places, you'll lose lineage (the DAG relationship) and docs/etc. will get all confused.\nMuch better to create multiple model files that simply call a macro with a different parameter, like this:\n`geo_model_macro.sql`\n`{% macro geo_model_macro(grain) %}\nselect\n    {{ grain }},\n    count(*)\nfrom {{ ref('my_upstream_table') }}\ngroup by 1\n{% endmacro %}\n`\n`my_model_zip.sql`\n`{{ geo_model_macro('zip') }}\n`\n`my_model_state.sql`\n`{{ geo_model_macro('state') }}\n`\n`my_model_region.sql`\n`{{ geo_model_macro('region') }}\n`\nIf I needed to do this hundreds of times (instead of 3), I would either:\n\nCreate a script to generate all of these .sql files for me\nCreate a new materialization that accepted a list of parameters, but this would be a super-advanced, here-be-dragons approach that is probably only appropriate when you've maxed out your other options.",
      "question_score": 5,
      "answer_score": 6,
      "created_at": "2022-08-05T15:57:06",
      "url": "https://stackoverflow.com/questions/73250933/how-do-i-run-sql-model-in-dbt-multiple-times-by-looping-through-variables"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "bigquery",
      "question_id": 65626971,
      "title": "GCP BigQuery message &quot;Not found: Connection&quot; when I use a connection in scripting",
      "problem": "I am using GCP bigquery. I have an external connection \"myconnectionid\" defined. This connection is of connection type  \"Cloud SQL - MySQL\" and it works fine for a running fedarated query from bigquery to the cloudsql mysql instance such as below.\n```\n`SELECT * FROM \nEXTERNAL_QUERY(\"myconnectionid\",\"SELECT CURDATE() from Dual;\")\n`\n```\nHowever when I attempt to declare a SQL variable and assign the return value from federated query to that variable ( I suppose that amounts to bigquery scripting) I get a message \"Not found: Connection myconnectionid\".\nPlease see example of error causing snippet of code below\n```\n`DECLARE MYDATE DATE;\nSET MYDATE = (SELECT * FROM \nEXTERNAL_QUERY(\"myconnectionid\",\"SELECT CURDATE() from Dual;\"))\n`\n```\nThis snippet gives me an error \"Not found: Connection myconnectionid at [2:14]\"\nDo gcp bigquery external connections need a special handling in scripting ?\nOr more simply ; do you know how I could overcome this error ?\nI am reading through google documentation on connections https://cloud.google.com/bigquery/docs/working-with-connections#federated_query_syntax\nand also the google documentation on big-query scripting\nhttps://cloud.google.com/bigquery/docs/reference/standard-sql/scripting\nNo luck yet.\nYour help much appreciated\nThanks !!",
      "solution": "I created a new database and connection almost two years after I originally encountered this and ran into exact same problem\nThe only definitive way I could get this to work is to ensure that the BQ database , the external connection and the cloud SQL database are in exact same region.\nSo take away is => if you plan to use BQ \"federated Queries\"  then ensure that the BQ database, the external connection and the cloud SQL database are in exact same region!",
      "question_score": 5,
      "answer_score": 1,
      "created_at": "2021-01-08T11:05:28",
      "url": "https://stackoverflow.com/questions/65626971/gcp-bigquery-message-not-found-connection-when-i-use-a-connection-in-scriptin"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "bigquery",
      "question_id": 78249901,
      "title": "SELECT AS STRUCT/VALUES",
      "problem": "I am wondering what the possible use cases are for the `SELECT` `AS STRUCT|VALUES` modifier for GoogleSQL beyond the sort of textbook examples in the documentation.\nThe `AS STRUCT` reduces a non-scalar row into a scalar `STRUCT, so something like:\n```\n`SELECT \"David\" AS name, 20 AS age\n`\n```\nCould be converted into a subquery-able (scalar) item by doing something like:\n```\n`SELECT  (\n    SELECT AS STRUCT \"David\" AS name, 20 AS age\n)\n`\n```\nOr adding a bunch of debug info into a single (scalar) column by doing something like:\n```\n`SELECT \"David\" AS name, 20 AS age, \n    (SELECT AS STRUCT 1 AS a, 2 AS b) debug\nFROM (SELECT NULL)\n`\n```\nBut beyond that I can't see too much use, and I've never used it outside of trivial debugging queries myself. I was wondering if hopefully someone on the BigQuery team can explain:\n\nWhat are some actual examples when you use or find these two modifier keywords being used?\nDo any other SQL dialects use that construction (or a similar construction) and if not, why did GoogleSQL need to support it?",
      "solution": "The `STRUCT` type is a container of ordered fields where each field has a type and name (optional). It can be used to combine multiple fields under one name.\n\nBut beyond that I can't see too much use, and I've never used it\noutside of trivial debugging queries myself.\n\nStruct can be used to structure your data. For example:\n```\n`CREATE TEMPORARY TABLE rectangles (\n  name string,\n  coordinates STRUCT,\n    bottomRight STRUCT\n  >\n);\n\nINSERT INTO rectangles (name, coordinates) VALUES\n('rectangle 1', ((10, 10), (90, 90)));\n`\n```\nHere is the resulting table schema and data.\n\nWhat are some actual examples when you use or find these two modifier\nkeywords being used?\n\n`SELECT AS STRUCT` (and other struct constructors) create a struct from multiple values. This could be used to bypass some restrictions, for example, when the data must consist of 1 column x n rows:\n```\n`SELECT ARRAY(\n  SELECT AS STRUCT gender, name\n  FROM bigquery-public-data.usa_names.usa_1910_2013\n  WHERE year = 2001 AND STate = 'WA'\n  ORDER BY number\n  LIMIT 5\n)\n`\n```\nWithout struct, you will get the following error:\n`ARRAY subquery cannot have more than one column unless\nusing SELECT AS STRUCT to build STRUCT values\n`\nAnother example where the IN clause subquery needs to return more than one column for tuple comparison:\n```\n`SELECT *\nFROM bigquery-public-data.usa_names.usa_1910_2013\nWHERE (state, year, number) IN (\n  SELECT AS STRUCT state, year, MAX(number)\n  FROM bigquery-public-data.usa_names.usa_1910_2013\n  WHERE year BETWEEN 2001 AND 2003\n  GROUP BY state, year\n)\n`\n```\nWithout struct, you will get the following error:\n`Subquery of type IN must have only one output column\n`\nNote that an IN clause subquery CAN return more than one column according to SQL standard (more on this below).\n`SELECT AS VALUE` operates on a one column select and that column must be a struct.\nWhile the input for `AS STRUCT` and `AS VALUE` is different, the result for both  is a value table \u2014 a one column table where that column is a struct type and has no name. You may substitute `SELECT AS STRUCT` in the previous section with `SELECT AS VALUE` and make necessary changes to the columns.\n\nDo any other SQL dialects use that construction (or a similar\nconstruction) and if not, why did GoogleSQL need to support it?\n\nThe answer depends on what you're trying to do.\n\nYou can think of struct as a user defined type (see the coordinates type in rectangles example above). Only PostgreSQL CREATE TYPE with Composite Type comes close.\nYou may use JSON functions to convert arbitrary data to a JSON object. All major RDBMS vendors support JSON functions.\nYou may use XML functions to convert arbitrary data to an XML document. SQL Server supports XML.\nYou may use Array functions to combine multiple values into one. But most RDBMS require same data type for all items in the array.\nIf it is just tuple comparison, the SQL standard allows you to compare tuples like `SELECT (1, 1, 0)",
      "question_score": 5,
      "answer_score": 3,
      "created_at": "2024-03-31T01:03:59",
      "url": "https://stackoverflow.com/questions/78249901/select-as-struct-values"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "bigquery",
      "question_id": 67397911,
      "title": "Export Data in Standard SQL Bigquery : EXPORT DATA statement cannot reference meta tables in the queries",
      "problem": "I want to export data to csv and save it to Google Storage using EXPORT DATA in Standard Query. It will be saved as scheduled query. Then, I set the table suffix into dynamic according to yesterday's date. Unfortunately, Bigquery didn't allow using the _TABLE_SUFFIX and resulted a warning of\n```\n`\"EXPORT DATA statement cannot reference meta tables in the queries.\" \n`\n```\nIt might mean I should use a static table name. But, in this case, I can only use table name with changing name according to yesterday date.\nDo you have any idea how to work around with this problem? Thank you.\n```\n`EXPORT DATA OPTIONS(\n  uri=CONCAT('gs://my_data//table1_', CONCAT(FORMAT_DATE(\"%Y%m%d\",CURRENT_DATE()-1),'*.csv')),\n  format='CSV',\n  overwrite=true,\n  header=true,\n  field_delimiter=',') AS\nSELECT *\nFROM `mybigquery.123456.ga_sessions_*`\nWHERE\n_TABLE_SUFFIX = FORMAT_DATE(\"%Y%m%d\",CURRENT_DATE()-1)\n`\n```",
      "solution": "I separated the query into 2 jobs. First job is to run query and save it to a table. Second job is to export the table to GCS. All can be done using scheduled query.\nFirst job\n```\n`SELECT *\nFROM `mybigquery.123456.ga_sessions_*`\nWHERE\n_TABLE_SUFFIX = FORMAT_DATE(\"%Y%m%d\",CURRENT_DATE()-1)\n`\n```\nThen, schedule it to make a new table called table1.\nSecond job\n```\n`EXPORT DATA OPTIONS(\n  uri=CONCAT('gs://my_data//table1_', CONCAT(FORMAT_DATE(\"%Y%m%d\",CURRENT_DATE()-1),'*.csv')),\n  format='CSV',\n  overwrite=true,\n  header=true,\n  field_delimiter=',') AS\nSELECT *\nFROM `mybigquery.123456.table1`\n`\n```",
      "question_score": 5,
      "answer_score": 1,
      "created_at": "2021-05-05T10:43:39",
      "url": "https://stackoverflow.com/questions/67397911/export-data-in-standard-sql-bigquery-export-data-statement-cannot-reference-me"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "bigquery",
      "question_id": 72339579,
      "title": "API call to bigquery.jobs.query failed with error: Access Denied: Project ppc-big-query: User does not have bigquery.jobs.create permission",
      "problem": "I created a Google Sheet that extracts data from Big Query via a sidebar. I am able to get the data when I run the function via a custom menu or if I execute the function from the Editor. The function simply queries a table in BQ and outputs the data in the sheet.\nHowever, I'm getting an error when I try to run the function via a submit button in the sidebar. I am using google.script.run in the HTML file to execute the Apps Script function. Here's the error that I'm getting:\nError:\nGoogleJsonResponseException: API call to bigquery.jobs.query failed with error: Access Denied: Project xxx: User does not have bigquery.jobs.create permission in project xxx.\nAgain, I am able to access BQ from a custom menu or from the Editor which means I have access to BQ, I believe. So I am not sure how to resolve this kind of error. I have been trying to rummage the internet for a solution but to no avail. I hope someone can help. Thanks in advance!",
      "solution": "This error tells you that you are missing a role that is required by Apps Script.\nHow to resolve this issue:\nYou need to tell the administrator to `Edit your existing role` in Google Cloud IAM and add the `bigquery.jobs.create`. There are other roles that you can add such as `bigquery.jobUser` and `bigquery.user`, these two roles have `bigquery.jobs.create` and other roles.\nWhy does this happen?\nThere are different BigQuery roles, maybe you can see the data and you probably have the Bigquery DataViewer role or the BigQuery Editor role, these two roles allow you to read data of your project but they does not have the specific role that Apps Script needs to use.\nYou can see here all the BigQuery roles.",
      "question_score": 5,
      "answer_score": 4,
      "created_at": "2022-05-22T18:52:33",
      "url": "https://stackoverflow.com/questions/72339579/api-call-to-bigquery-jobs-query-failed-with-error-access-denied-project-ppc-bi"
    },
    {
      "tech": "gcp",
      "source": "stackoverflow",
      "tag": "bigquery",
      "question_id": 67564533,
      "title": "BigQury Storage Read API, the user does not have &#39;bigquery.readsessions.create&#39;",
      "problem": "I'm trying to use BigQuery Storage Read API. As far as I can tell, the local script is using the an account, that has Owner role, BigQuery user, and BigQuery read session on the entire project. However, running the code from the local machine yields this error:\n\ngoogle.api_core.exceptions.PermissionDenied: 403 request failed: the user does not have 'bigquery.readsessions.create' permission for 'projects/xyz'\n\nAccording to the GCP documentation the API is enabled by default. So the only reason I can think of is my script is using the wrong account.\nHow would you go debugging this issue? Is there a way to know for sure which user/account is running a python code on run time, something like `print(user.user_name)`",
      "solution": "There is a `gcloud` command to get the current user permissions\n`$ gcloud projects get-iam-policy [PROJECT_ID]\n`\nYou can also check the `user_email` field of your job to find out which user it is using to execute your query.\nExample:\n`{\n  # ...\n  \"user_email\": \"myemail@company.com\",\n  \"configuration\": {\n  # ...\n  \"jobType\": QUERY\n    },\n  },\n  \"jobReference\": {\n    \"projectId\": \"my-project\",\n  # ...\n}\n`",
      "question_score": 5,
      "answer_score": 3,
      "created_at": "2021-05-17T07:45:09",
      "url": "https://stackoverflow.com/questions/67564533/bigqury-storage-read-api-the-user-does-not-have-bigquery-readsessions-create"
    }
  ]
}